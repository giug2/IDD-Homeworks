<article class="ltx_document ltx_authors_1line" lang="en">
 <h1 class="ltx_title ltx_title_document">
  A Review of Large Language Models and Autonomous Agents in Chemistry
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0001-5336-2847" target="_blank" title="">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="5" id="id1.1.1.g1" src="/html/2407.01603/assets/x1.png" width="5"/>
     Mayk Caldas Ramos
    </a>
    <br class="ltx_break"/>
    Department of Chemical Engineering
    <br class="ltx_break"/>
    University of Rochester, Rochester, NY
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id4.4.id1">
     mcaldasr@ur.rochester.edu
    </span>
    <br class="ltx_break"/>
    &amp;
    <a class="ltx_ref ltx_href" href="https://orcid.org/" target="_blank" title="">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="5" id="id2.2.2.g1" src="/html/2407.01603/assets/x1.png" width="5"/>
     Christopher J. Collison
    </a>
    <br class="ltx_break"/>
    School of Chemistry and Materials Science
    <br class="ltx_break"/>
    Rochester Institute of Technology, Rochester, NY
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id5.5.id2">
     cjcscha@rit.edu
    </span>
    <br class="ltx_break"/>
    &amp;
    <a class="ltx_ref ltx_href" href="https://orcid.org/0000-0002-6647-3965" target="_blank" title="">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="5" id="id3.3.3.g1" src="/html/2407.01603/assets/x1.png" width="5"/>
     Andrew D. White
    </a>
    <br class="ltx_break"/>
    FutureHouse Inc., San Francisco, CA
    <br class="ltx_break"/>
    Department of Chemical Engineering
    <br class="ltx_break"/>
    University of Rochester, Rochester, NY
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id6.6.id3">
     andrew@futurehouse.org
    </span>
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    Corresponding author
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id7.id1">
   <span class="ltx_text" id="id7.id1.1">
    Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization.
This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation.
We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains.
This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry.
Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods.
Due to the quick pace of this field, a repository has been built to keep track of the latest studies:
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ur-whitelab/LLMs-in-science" target="_blank" title="">
     https://github.com/ur-whitelab/LLMs-in-science
    </a>
    .
   </span>
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <p class="ltx_p" id="p1.1">
   <em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">
    Keywords
   </em>
   Large Language Model, LLM, LLM agent, agent, science, chemistry
  </p>
 </div>
 <nav class="ltx_TOC ltx_list_toc ltx_toc_toc">
  <h6 class="ltx_title ltx_title_contents">
   Contents
  </h6>
  <ol class="ltx_toclist">
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S1" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       1
      </span>
      Introduction
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S1.SS1" title="In 1 Introduction ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         1.1
        </span>
        Challenges in chemistry
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S2" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       2
      </span>
      Large Language Models
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S2.SS1" title="In 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         2.1
        </span>
        The Transformer
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S2.SS2" title="In 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         2.2
        </span>
        Model training
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S2.SS3" title="In 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         2.3
        </span>
        Model types
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S2.SS3.SSS1" title="In 2.3 Model types ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           2.3.1
          </span>
          Encoder-only models
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S2.SS3.SSS2" title="In 2.3 Model types ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           2.3.2
          </span>
          Decoder-only models
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S2.SS3.SSS3" title="In 2.3 Model types ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           2.3.3
          </span>
          Encoder-decoder models
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S2.SS3.SSS4" title="In 2.3 Model types ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           2.3.4
          </span>
          Multi-task and multi-modal models
         </span>
        </a>
       </li>
      </ol>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S3" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       3
      </span>
      LLMs for chemistry and biochemistry
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS1" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.1
        </span>
        Molecular Representations, Datasets, and Benchmarks
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS2" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.2
        </span>
        Property Prediction and Encoder-only mol-LLMs
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS2.SSS1" title="In 3.2 Property Prediction and Encoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.2.1
          </span>
          Property Prediction
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS2.SSS2" title="In 3.2 Property Prediction and Encoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.2.2
          </span>
          Encoder-only Mol-LLMs
         </span>
        </a>
       </li>
      </ol>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS3" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.3
        </span>
        Property Directed Inverse Design and Decoder-only mol-LLMs
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS3.SSS1" title="In 3.3 Property Directed Inverse Design and Decoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.3.1
          </span>
          Property Directed Inverse Design
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS3.SSS2" title="In 3.3 Property Directed Inverse Design and Decoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.3.2
          </span>
          Decoder-only mol-LLMs
         </span>
        </a>
       </li>
      </ol>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS4" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.4
        </span>
        Synthesis Prediction and Encoder-decoder Mol-LLMs
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS4.SSS1" title="In 3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.4.1
          </span>
          Synthesis Prediction
         </span>
        </a>
       </li>
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS4.SSS2" title="In 3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.4.2
          </span>
          Encoder-decoder mol-LLMs
         </span>
        </a>
       </li>
      </ol>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS5" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.5
        </span>
        Multi-Modal LLMs
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS6" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.6
        </span>
        Textual scientific LLMs
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS7" title="In 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.7
        </span>
        The use of ChatGPT in Chemistry
       </span>
      </a>
      <ol class="ltx_toclist ltx_toclist_subsection">
       <li class="ltx_tocentry ltx_tocentry_subsubsection">
        <a class="ltx_ref" href="#S3.SS7.SSS1" title="In 3.7 The use of ChatGPT in Chemistry ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
         <span class="ltx_text ltx_ref_title">
          <span class="ltx_tag ltx_tag_ref">
           3.7.1
          </span>
          Automation
         </span>
        </a>
       </li>
      </ol>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S4" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       4
      </span>
      LLM-based autonomous agents
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS1" title="In 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.1
        </span>
        Memory module
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS2" title="In 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.2
        </span>
        Planning and reasoning modules
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS3" title="In 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.3
        </span>
        Profiling module
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS4" title="In 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.4
        </span>
        Perception
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS5" title="In 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.5
        </span>
        Tools
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S5" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       5
      </span>
      LLM-Based autonomous agents in scientific research
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS1" title="In 5 LLM-Based autonomous agents in scientific research ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.1
        </span>
        Agents for literature review
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS2" title="In 5 LLM-Based autonomous agents in scientific research ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.2
        </span>
        Agents for chemical innovation
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS3" title="In 5 LLM-Based autonomous agents in scientific research ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.3
        </span>
        Agents for experiments planning
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS4" title="In 5 LLM-Based autonomous agents in scientific research ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.4
        </span>
        Agents for automating cheminformatics tasks
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS5" title="In 5 LLM-Based autonomous agents in scientific research ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.5
        </span>
        Agents for hypothesis creation
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S6" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       6
      </span>
      Challenges and opportunities
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S7" title="In A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       7
      </span>
      Conclusions
     </span>
    </a>
   </li>
  </ol>
 </nav>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The integration of Machine Learning (ML) and Artificial Intelligence (AI) into chemistry has spanned several decades
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.1.1">
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib3" title="">
       3
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib4" title="">
       4
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib5" title="">
       5
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib6" title="">
       6
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib7" title="">
       7
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib8" title="">
       8
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib9" title="">
       9
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib10" title="">
       10
      </a>
     </sup>
    </cite>
    . Although applications of computational methods in quantum chemistry and molecular modeling from the 1950s-1970s were not considered AI, they laid the groundwork. Subsequently in the 1980s expert systems like DENDRAL
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.2.1">
      <a class="ltx_ref" href="#bib.bib11" title="">
       11
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
     </sup>
    </cite>
    were expanded to infer molecular structures from mass spectrometry data.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.3.1">
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
     </sup>
    </cite>
    At the same time, Quantitative Structure-Activity Relationship (QSAR) Models were developed
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.4.1">
      <a class="ltx_ref" href="#bib.bib5" title="">
       5
      </a>
     </sup>
    </cite>
    that would use statistical methods to predict the effects of chemical structure on activity.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.5.1">
      <a class="ltx_ref" href="#bib.bib14" title="">
       14
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
     </sup>
    </cite>
    In the 1990s, neural networks, and associated Kohonen Self-Organizing Maps were introduced to domains such as drug design,
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.6.1">
      <a class="ltx_ref" href="#bib.bib18" title="">
       18
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib19" title="">
       19
      </a>
     </sup>
    </cite>
    as summarized well by
    <cite class="ltx_cite ltx_citemacro_citet">
     Yang et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib5" title="">
       5
      </a>
     </sup>
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Goldman and Walters
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib20" title="">
       20
      </a>
     </sup>
    </cite>
    , although they were limited by the computational resources of the time. With an explosion of data from High-Throughput Screening (HTS)
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.7.1">
      <a class="ltx_ref" href="#bib.bib21" title="">
       21
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib22" title="">
       22
      </a>
     </sup>
    </cite>
    , models then started to benefit from vast datasets of molecular structures and their biological activities. Furthermore, ML algorithms such as Support Vector Machines and Random Forests became popular for classification and regression tasks in cheminformatics,
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.8.1">
      <a class="ltx_ref" href="#bib.bib1" title="">
       1
      </a>
     </sup>
    </cite>
    offering improved performance over traditional statistical methods.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p1.1.9.1">
      <a class="ltx_ref" href="#bib.bib23" title="">
       23
      </a>
     </sup>
    </cite>
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Deep learning transformed the landscape of ML in chemistry and materials science in the 2010s.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.1.1">
      <a class="ltx_ref" href="#bib.bib24" title="">
       24
      </a>
     </sup>
    </cite>
    Recurrent Neural Networks (RNNs),
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.2.1">
      <a class="ltx_ref" href="#bib.bib25" title="">
       25
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib26" title="">
       26
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib28" title="">
       28
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib29" title="">
       29
      </a>
     </sup>
    </cite>
    Convolutional Neural Networks (CNNs)
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.3.1">
      <a class="ltx_ref" href="#bib.bib30" title="">
       30
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib31" title="">
       31
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib32" title="">
       32
      </a>
     </sup>
    </cite>
    and later, Graph Neural Networks (GNNs),
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.4.1">
      <a class="ltx_ref" href="#bib.bib33" title="">
       33
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib35" title="">
       35
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib36" title="">
       36
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       37
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
     </sup>
    </cite>
    made great gains in their application to molecular property prediction, drug discovery,
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.5.1">
      <a class="ltx_ref" href="#bib.bib39" title="">
       39
      </a>
     </sup>
    </cite>
    and synthesis prediction.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.6.1">
      <a class="ltx_ref" href="#bib.bib40" title="">
       40
      </a>
     </sup>
    </cite>
    Such methods were able to capture complex patterns in data, and therefore enabled the identification of novel materials for high-impact needs such as energy storage and conversion.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p2.1.7.1">
      <a class="ltx_ref" href="#bib.bib41" title="">
       41
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib42" title="">
       42
      </a>
     </sup>
    </cite>
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In this review, we explore the next phase of AI in chemistry, namely the use of Large Language Models (LLMs) and autonomous agents. Inspired by successes in natural language processing (NLP), LLMs
were adapted for chemical language (e.g., SMILES
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p3.1.1.1">
      <a class="ltx_ref" href="#bib.bib43" title="">
       43
      </a>
     </sup>
    </cite>
    ) to tackle tasks from synthesis prediction to molecule generation
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p3.1.2.1">
      <a class="ltx_ref" href="#bib.bib44" title="">
       44
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
     </sup>
    </cite>
    . We will then explore the integration of LLMs into autonomous agents as illustrated by
    <cite class="ltx_cite ltx_citemacro_citet">
     M. Bran et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib47" title="">
       47
      </a>
     </sup>
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Boiko et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
     </sup>
    </cite>
    , which may be used for data interpretation or, for example, to experiment with robotic systems.
We are at a crossroads where AI enables chemists to solve major global problems faster and streamline routine lab tasks. This enables, for instance, the development of larger, consistent experimental datasets and shorter lead times for drug and material commercialization.
As such, language has been the preferred mechanism for describing and disseminating research results and protocols in chemistry for hundreds of years.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S1.p3.1.3.1">
      <a class="ltx_ref" href="#bib.bib49" title="">
       49
      </a>
     </sup>
    </cite>
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    The review structure is as follows: Section 1.1 discusses current challenges that need to be addressed to increase the impact of AI in chemistry. Section 2 introduces transformers, covering encoder-only, decoder-only, and encoder-decoder architectures. Section 3 surveys work with LLMs, linking each transformer architecture to suitable chemistry areas. Section 4 describes autonomous agents and their applications in chemistry research. Section 5 surveys LLM-based agents in chemistry. Section 6 discusses future challenges and opportunities, followed by the conclusion in Section 7. We distinguish between “text-based” and “mol-based” inputs and outputs, with “text” referring to natural language and “mol” referring to the chemical syntax for material structures, as introduced by
    <cite class="ltx_cite ltx_citemacro_citet">
     Zhang et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
     </sup>
    </cite>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     1.1
    </span>
    Challenges in chemistry
   </h3>
   <div class="ltx_para ltx_noindent" id="S1.SS1.p1">
    <p class="ltx_p" id="S1.SS1.p1.1">
     With the potential to accelerate scientific discovery through the use of AI, it makes sense to connect this potential to some of the more pressing opportunities in the chemical sciences as may be categorized under three broad umbrellas: Property Prediction, Property-Directed Molecule Generation, and Synthesis Prediction.
The first challenge is to predict a property for a given compound to decide if it should be synthesized for a specific application, such as an indicator,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib51" title="">
        51
       </a>
      </sup>
     </cite>
     light harvester,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib52" title="">
        52
       </a>
      </sup>
     </cite>
     or catalyst.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib53" title="">
        53
       </a>
      </sup>
     </cite>
     To have better models for property prediction, good quality data is crucial. We discuss the caveats and issues with the current datasets in Section
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Molecular Representations, Datasets, and Benchmarks ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     and illustrate state-of-the-art findings in Section
     <a class="ltx_ref" href="#S3.SS2" title="3.2 Property Prediction and Encoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3.2
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S1.SS1.p2">
    <p class="ltx_p" id="S1.SS1.p2.1">
     The second challenge is to generate novel chemical structures that meet desired chemical profiles or specific properties.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib54" title="">
        54
       </a>
      </sup>
     </cite>
     Success would accelerate progress in various chemical applications, but reliable reverse engineering (inverse design)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib55" title="">
        55
       </a>
      </sup>
     </cite>
     is not yet feasible over the vast chemical space.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p2.1.3.1">
       <a class="ltx_ref" href="#bib.bib56" title="">
        56
       </a>
      </sup>
     </cite>
     For instance, inverse design coupled with automatic selection of novel structures (
     <span class="ltx_text ltx_font_italic" id="S1.SS1.p2.1.4">
      de novo
     </span>
     molecular design) could develop a drug targeting a specific protein while retaining properties like solubility, toxicity, and blood-brain barrier permeability.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p2.1.5.1">
       <a class="ltx_ref" href="#bib.bib57" title="">
        57
       </a>
      </sup>
     </cite>
     The complexity of connecting
     <span class="ltx_text ltx_font_italic" id="S1.SS1.p2.1.6">
      de novo
     </span>
     design with property prediction is high and raises ethical questions that need to be addressed to guide the development of such models. We show how state-of-the-art models currently perform in Section
     <a class="ltx_ref" href="#S3.SS3" title="3.3 Property Directed Inverse Design and Decoder-only mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3.3
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S1.SS1.p3">
    <p class="ltx_p" id="S1.SS1.p3.1">
     The third major challenge is predicting its optimal synthesis using inexpensive, readily available, and non-toxic starting materials. There will always likely be a solution within a large chemical space: an alternative molecule with similar properties that is easier to synthesize. Exploring this space to find a new molecule with the right properties and a high-yield synthesis route unites these challenges. The number of possible stable chemicals is estimated to be up to
     <math alttext="10^{180}" class="ltx_Math" display="inline" id="S1.SS1.p3.1.m1.1">
      <semantics id="S1.SS1.p3.1.m1.1a">
       <msup id="S1.SS1.p3.1.m1.1.1" xref="S1.SS1.p3.1.m1.1.1.cmml">
        <mn id="S1.SS1.p3.1.m1.1.1.2" xref="S1.SS1.p3.1.m1.1.1.2.cmml">
         10
        </mn>
        <mn id="S1.SS1.p3.1.m1.1.1.3" xref="S1.SS1.p3.1.m1.1.1.3.cmml">
         180
        </mn>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S1.SS1.p3.1.m1.1b">
        <apply id="S1.SS1.p3.1.m1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1">
         <csymbol cd="ambiguous" id="S1.SS1.p3.1.m1.1.1.1.cmml" xref="S1.SS1.p3.1.m1.1.1">
          superscript
         </csymbol>
         <cn id="S1.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S1.SS1.p3.1.m1.1.1.2">
          10
         </cn>
         <cn id="S1.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S1.SS1.p3.1.m1.1.1.3">
          180
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S1.SS1.p3.1.m1.1c">
        10^{180}
       </annotation>
      </semantics>
     </math>
     .
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib58" title="">
        58
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib59" title="">
        59
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib60" title="">
        60
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib61" title="">
        61
       </a>
      </sup>
     </cite>
     Exploring this vast space requires significant acceleration beyond current methods.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib62" title="">
        62
       </a>
      </sup>
     </cite>
     <cite class="ltx_cite ltx_citemacro_citet">
      Restrepo
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib58" title="">
        58
       </a>
      </sup>
     </cite>
     emphasizes the need to catalog data on failed syntheses to build a comprehensive dataset of chemical features. Autonomous chemical resources can accelerate database growth and tackle this challenge. Thus, automation is considered a fourth major challenge in chemistry.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S1.SS1.p3.1.3.1">
       <a class="ltx_ref" href="#bib.bib63" title="">
        63
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib64" title="">
        64
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib65" title="">
        65
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib66" title="">
        66
       </a>
      </sup>
     </cite>
     The following discussion explores how LLMs and autonomous agents can provide the most value. Relevant papers are discussed in Section
     <a class="ltx_ref" href="#S3.SS4" title="3.4 Synthesis Prediction and Encoder-decoder Mol-LLMs ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3.4
      </span>
     </a>
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Large Language Models
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    The prior state-of-the-art for seq2seq had been the Recurrent Neural Network (RNN),
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S2.p1.1.1.1">
      <a class="ltx_ref" href="#bib.bib67" title="">
       67
      </a>
     </sup>
    </cite>
    typically as implemented in
    <cite class="ltx_cite ltx_citemacro_citet">
     Hochreiter and Schmidhuber
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib68" title="">
       68
      </a>
     </sup>
    </cite>
    . The RNN retains “memory” of previous steps in a sequence to predict later parts. However, as sequence length increases, gradients can become vanishingly small or explosively large
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S2.p1.1.2.1">
      <a class="ltx_ref" href="#bib.bib69" title="">
       69
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib70" title="">
       70
      </a>
     </sup>
    </cite>
    , preventing effective use of earlier information in long sequences. RNNs have thus fallen behind Large Language Models (LLMs).
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    LLMs are deep neural networks (NN) with billions of parameters. Mostly, LLMs implement a transformer architecture, introduced by
    <cite class="ltx_cite ltx_citemacro_citet">
     Vaswani et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib71" title="">
       71
      </a>
     </sup>
    </cite>
    , although other architectures for longer input sequences
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       These longer sequence models are more important in biology, where very long genetic sequences are relevant
      </span>
     </span>
    </span>
    are being actively explored
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S2.p2.1.1.1">
      <a class="ltx_ref" href="#bib.bib72" title="">
       72
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib73" title="">
       73
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib74" title="">
       74
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib75" title="">
       75
      </a>
     </sup>
    </cite>
    . Transformers are well-developed in chemistry and are the dominant paradigm behind nearly all state-of-the-art sequence modeling results and, thus, the focus.
More detailed discussion on current LLMs applied to general purposes can be found elsewhere.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S2.p2.1.2.1">
      <a class="ltx_ref" href="#bib.bib76" title="">
       76
      </a>
     </sup>
    </cite>
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    The Transformer
   </h3>
   <figure class="ltx_figure" id="S2.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S2.F1.g1" src="/html/2407.01603/assets/figs/transformer_all_combined.png" width="598"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     a) The generalized encoder-decoder transformer: The encoder on the left converts an input into a vector, while the decoder on the right predicts the next token in a sequence. b) Encoder-decoder transformers are traditionally used for translation tasks and, in chemistry, for reaction prediction, translating reactants into products. c) Encoder-only transformers provide a vector output and are typically used for sentiment analysis. In chemistry, they are used for property prediction or classification tasks. d) Decoder-only transformers generate likely next tokens in a sequence. In chemistry, they are used to generate new molecules given an instruction and description of molecules.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The transformer was introduced in, “Attention is all you need” by
     <cite class="ltx_cite ltx_citemacro_citet">
      Vaswani et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib71" title="">
        71
       </a>
      </sup>
     </cite>
     in 2017. A careful line-by-line review of the model can be found in “The Annotated Transformer”
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib77" title="">
        77
       </a>
      </sup>
     </cite>
     . The transformer was the first sequence-to-sequence (seq2seq) model based entirely on attention, although attention had been a feature for Recurrent Neural Networks (RNNs) some years prior.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib78" title="">
        78
       </a>
      </sup>
     </cite>
     Here, a seq2seq model processes a text input, such as an English paragraph, and produces a different text sequence, such as a French translation of that paragraph. The concept of “attention” is a focus applied to certain words of the input, which would convey the most importance, or the context of the passage, and thereby would allow for better decision-making and greater accuracy. However, in a practical sense, “attention” is implemented simply as the dot-product between token embeddings with some non-linear learned function, and is described further below.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     The transformer architecture implements two main modules: the encoder and the decoder. Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 The Transformer ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     a provides a cartoon diagram of the general encoder-decoder transformer architecture.
The input is first tokenized, from the model’s vocabulary,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib79" title="">
        79
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib80" title="">
        80
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib81" title="">
        81
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib82" title="">
        82
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib83" title="">
        83
       </a>
      </sup>
     </cite>
     into computable integers that are then converted into numerical vectors using embedding layers. A single encoder stack is shown here, where in reality there are many, with six in the original model.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib71" title="">
        71
       </a>
      </sup>
     </cite>
     The later stacks each use outputs from the prior stack, typically as a
     <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1">
      <semantics id="S2.SS1.p2.1.m1.1a">
       <mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b">
        <csymbol cd="latexml" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     512 per-token vector.
Each encoder applies positional encoding typically using sine and cosine functions, with a frequency depending on that word’s position, to handle sequences of any length. Other functions can also be used.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.3.1">
       <a class="ltx_ref" href="#bib.bib84" title="">
        84
       </a>
      </sup>
     </cite>
     The encoder stack then comprises a multi-headed self-attention mechanism, which relates each “word” to others in the sequence by computing attention scores based on queries, keys, and values. This is followed by normalization and residual connections. Residual connection is represented in Figure 1a by the “by-passing” arrows, which indicate how each sub-layer output is added to its input, which helps mitigate the vanishing gradient problem.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.4.1">
       <a class="ltx_ref" href="#bib.bib69" title="">
        69
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib70" title="">
        70
       </a>
      </sup>
     </cite>
     Lastly, the output is further refined through a pointwise feed-forward network with an activation function (such as ReLU,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.5.1">
       <a class="ltx_ref" href="#bib.bib85" title="">
        85
       </a>
      </sup>
     </cite>
     SwiGLU,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.6.1">
       <a class="ltx_ref" href="#bib.bib86" title="">
        86
       </a>
      </sup>
     </cite>
     GELU,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS1.p2.1.7.1">
       <a class="ltx_ref" href="#bib.bib87" title="">
        87
       </a>
      </sup>
     </cite>
     etc) resulting in a set of vectors representing the input sequence with rich contextual understanding, ready for the decoder.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     Similarly, the decoder workflow begins with converting the initial input token into numerical vectors, known as the output embedding step, as this initial input serves as the start token for the model’s final output. Positional encodings are then added, followed by a stack of approximately six identical decoder layers. Each decoder layer includes a Masked Self-Attention Mechanism to prevent positions from attending to subsequent ones and an Encoder-Decoder Multi-Head Attention to align encoder outputs with decoder attention layer outputs (indicated by the square connecting arrows in Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 The Transformer ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     a), focusing on relevant parts of the input. A fully connected Feed-Forward Neural Network refines the output further. The final layer acts as a linear classifier, mapping the output to the vocabulary size, and a softmax layer converts this output into probabilities, with the highest indicating the predicted next word. Each sub-layer is followed by normalization and includes a residual connection. The decoder generates output sequences autoregressively, incorporating each newly predicted token until an end token signals completion.
Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 The Transformer ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     b shows a cartoon illustration of the encoder-decoder model’s strength in translating between languages or from atoms in reactants mapped into new positions in associated products. Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 The Transformer ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     c depicts the ability of an encoder-only model to detect an additional meaning, or property, from an input sequence, represented as a number description. Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 The Transformer ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     d presents a cartoon depiction of a decoder-only architecture inferring and generating a consequence, based on an original input. Hence, a new peptide sequence has been inferred after the combined input of two smaller amino acids, indicating how the decoder-only approach can be used to generate potentially new compounds.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Model training
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     The common lifetime of an LLM consists of being first pretrained using unsupervised or self-supervised techniques, generating what is called a base model.
This base model is then fine-tuned for specific applications using supervised techniques. Finally, the supervised fine-tuned model is further tuned with reward models to improve human preference or some other non-differentiable and sparse desired character
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS2.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib88" title="">
        88
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
    <h5 class="ltx_title ltx_title_paragraph">
     Unsupervised pretraining
    </h5>
    <div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">
      A significant benefit implied in all the transformer models described in this review is that unsupervised learning takes place with a vast corpus of text. Thus, the algorithm learns patterns from untagged data, which opens up the model to larger datasets that may not have been explicitly annotated by humans. The advantage is to discover underlying structures or distributions without being provided with explicit instructions on what to predict, nor with labels that might indicate the correct answer.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
    <h5 class="ltx_title ltx_title_paragraph">
     Supervised fine-tuning
    </h5>
    <div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">
      After this pretraining, many models described herein are fine-tuned on specific downstream tasks (e.g., text classification, question answering) using supervised learning. In supervised learning, models learn from labeled data, and map inputs to known outputs. Such fine-tuning allows the model to be adjusted with a smaller, task-specific dataset to perform well on that downstream task.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
    <h5 class="ltx_title ltx_title_paragraph">
     LLM alignment
    </h5>
    <div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px3.p1">
     <p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">
      A key task after model training is aligning the output with what human users prefer. This can be done so as to improve the LLM output’s style and tone, or
to reduce harmful outputs because the training objectives used during the pretraining and the fine-tuning may not include human values.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib89" title="">
         89
        </a>
       </sup>
      </cite>
      Reinforcement learning (RL) techniques are among the most common alignment techniques. RL is a type of machine learning where actions are performed and feedback is received through rewards, the goal being to learn a policy that maximizes the cumulative reward. However, human preferences can only be evaluated on complete outputs from models, preventing supervised fine-tuning (which requires token-by-token losses).
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib90" title="">
         90
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib91" title="">
         91
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib92" title="">
         92
        </a>
       </sup>
      </cite>
      To apply RL, sampling tokens from the model is recast as a Markov Decision Process (MDP) - actions are tokens and the reward is zero until the EOS (end of sequence) token, at which point the human preference reward is applied
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib93" title="">
         93
        </a>
       </sup>
      </cite>
      Reinforcement Learning with Human Feedback (RLHF)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.4.1">
        <a class="ltx_ref" href="#bib.bib94" title="">
         94
        </a>
       </sup>
      </cite>
      is the most used alignment technique. However, alternative strategies include reinforcement learning with synthetic feedback (RLSF)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.5.1">
        <a class="ltx_ref" href="#bib.bib95" title="">
         95
        </a>
       </sup>
      </cite>
      , proximal policy optimization (PPO)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.6.1">
        <a class="ltx_ref" href="#bib.bib96" title="">
         96
        </a>
       </sup>
      </cite>
      , REINFORCE
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.7.1">
        <a class="ltx_ref" href="#bib.bib97" title="">
         97
        </a>
       </sup>
      </cite>
      , and others.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p1.1.8.1">
        <a class="ltx_ref" href="#bib.bib90" title="">
         90
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib93" title="">
         93
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib98" title="">
         98
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib99" title="">
         99
        </a>
       </sup>
      </cite>
      .
A general discussion about RL potential for LLM fine-tuning was published by
      <cite class="ltx_cite ltx_citemacro_citet">
       Cao et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib100" title="">
         100
        </a>
       </sup>
      </cite>
      and
      <cite class="ltx_cite ltx_citemacro_citet">
       Shen et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib89" title="">
         89
        </a>
       </sup>
      </cite>
      .
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px3.p2">
     <p class="ltx_p" id="S2.SS2.SSS0.Px3.p2.1">
      There are ways to reformulate the RLHF process into a direct optimization problem with a different loss. This is known as direct preference optimization (DPO)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p2.1.1.1">
        <a class="ltx_ref" href="#bib.bib101" title="">
         101
        </a>
       </sup>
      </cite>
      and a popular competitor to PPO (the most commonly used approach) due to its simplicity. It overcomes the lack of token-by-token loss signal by comparing two completions at a time.
The discussions about which technique is superior remain very active in the literature.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS2.SSS0.Px3.p2.1.2.1">
        <a class="ltx_ref" href="#bib.bib102" title="">
         102
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px3.p3">
     <p class="ltx_p" id="S2.SS2.SSS0.Px3.p3.1">
      Finally, the alignment may not be to human preferences but to downstream tasks that do not provide token-by-token rewards. For example,
      <cite class="ltx_cite ltx_citemacro_citet">
       Bou et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib103" title="">
         103
        </a>
       </sup>
      </cite>
      and
      <cite class="ltx_cite ltx_citemacro_citet">
       Hayes et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib104" title="">
         104
        </a>
       </sup>
      </cite>
      both use RL on a language model for improving its outputs on a downstream scientific task.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Model types
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     While the Vaswani Transformer
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S2.SS3.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib71" title="">
        71
       </a>
      </sup>
     </cite>
     employed an encoder-decoder structure for sequence-to-sequence tasks, the encoder and decoder were ultimately seen as independent models, leading to “encoder-only”, and “decoder-only” models described below.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S2.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.1
     </span>
     Encoder-only models
    </h4>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p1">
     <p class="ltx_p" id="S2.SS3.SSS1.p1.1">
      Beyond Vaswani’s transformer,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS1.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib71" title="">
         71
        </a>
       </sup>
      </cite>
      used for sequence-to-sequence tasks, another significant evolutionary step forward came in the guise of the Bidirectional Encoder Representations from Transformers, or “BERT”, described in October 2018 by
      <cite class="ltx_cite ltx_citemacro_citet">
       Devlin et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib105" title="">
         105
        </a>
       </sup>
      </cite>
      BERT utilized only the encoder component, achieving state-of-the-art performance on sentence-level and token-level tasks, outperforming prior task-specific architectures.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS1.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib105" title="">
         105
        </a>
       </sup>
      </cite>
      The key difference was BERT’s bidirectional transformer pretraining on unlabeled text, meaning the model looks at the context both to the left and right of the word in question, facilitated by a Masked Language Model (MLM). This encoder-only design meant BERT focused on generating a deeper "understanding" of the input sequence, rather than mapping input sequences to output sequences.
In pretraining, BERT also uses Next Sentence Prediction (NSP). “Sentence” here means an arbitrary span of contiguous text. The MLM task randomly masks tokens and predicts them by looking at preceding and following contexts simultaneously, inspired by Taylor
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS1.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib106" title="">
         106
        </a>
       </sup>
      </cite>
      . NSP predicts whether one sentence logically follows another, training the model to understand sentence relationships. This bidirectional approach allows BERT to recognize greater nuance and richness in the input data.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS1.p2">
     <p class="ltx_p" id="S2.SS3.SSS1.p2.1">
      Subsequent evolutions of BERT include, for example, RoBERTa, (Robustly optimized BERT approach), described in 2019 by Facebook AI’s
      <cite class="ltx_cite ltx_citemacro_citet">
       Liu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib107" title="">
         107
        </a>
       </sup>
      </cite>
      . RoBERTa was trained on a larger corpus, for more iterations, with larger mini-batches, and longer sequences, improving model understanding and generalization. By removing the NSP task and focusing on the MLM task, performance improved. RoBERTa dynamically changed masked positions during training and used different hyperparameters. Evolutions of BERT also include domain-specific pretraining and creating specialist LLMs for fields like chemistry, as described below (see Section
      <a class="ltx_ref" href="#S3" title="3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      ).
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S2.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.2
     </span>
     Decoder-only models
    </h4>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS2.p1">
     <p class="ltx_p" id="S2.SS3.SSS2.p1.1">
      In June 2018,
      <cite class="ltx_cite ltx_citemacro_citet">
       Radford et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib108" title="">
         108
        </a>
       </sup>
      </cite>
      from OpenAI proposed the Generative Pretrained Transformer (GPT) in their paper, “Improving Language Understanding by Generative Pretraining”. GPT used a decoder-only, left-to-right unidirectional language model to predict the next word in a sequence based on previous words, without an encoder. Unlike the Vaswani Transformer’s decoder, GPT could predict the next sequence, applying general language understanding to specific tasks with smaller annotated datasets.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS2.p2">
     <p class="ltx_p" id="S2.SS3.SSS2.p2.1">
      GPT employed positional encodings to maintain word order in its predictions. Unlike the Vaswani Transformer, GPT’s self-attention mechanism prevented tokens from attending to future tokens, ensuring each word prediction depended only on preceding words.
Hence a decoder-only architecture represents a so-called causal language model, one that generates each item in a sequence based on the previous items; the generation of each subsequent output is causally linked to the history of generated outputs and nothing ahead of the current word affects its generation.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S2.SS3.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.3
     </span>
     Encoder-decoder models
    </h4>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS3.p1">
     <p class="ltx_p" id="S2.SS3.SSS3.p1.1">
      Evolving further, BART (Bidirectional and Auto-Regressive Transformers) was introduced by
      <cite class="ltx_cite ltx_citemacro_citeauthor">
       <a class="ltx_ref" href="#bib.bib109" title="">
        Lewis et al.
       </a>
      </cite>
      from Facebook AI in 2019.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS3.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib109" title="">
         109
        </a>
       </sup>
      </cite>
      BART combined the context learning strengths of the bidirectional BERT, and the autoregressive capabilities of models like GPT, which excel at generating coherent text. BART was thus a hybrid seq2seq model, consisting of a BERT-like bidirectional encoder and a GPT-like autoregressive decoder. This is nearly the same architecture as
      <cite class="ltx_cite ltx_citemacro_citet">
       Vaswani et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib71" title="">
         71
        </a>
       </sup>
      </cite>
      ; the differences are in the pretraining. BART was pretrained using a task that corrupted text by, for example, deleting tokens, and shuffling sentences. It then learned to reconstruct the original text with left-to-right autoregressive decoding as in GPT models.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S2.SS3.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.3.4
     </span>
     Multi-task and multi-modal models
    </h4>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS4.p1">
     <p class="ltx_p" id="S2.SS3.SSS4.p1.1">
      In previous sections, we discussed LLMs that take natural language text as input and then they output either a learned representation or another text sequence. These models traditionally perform tasks like translation, summarization, and classification.
      <cite class="ltx_cite ltx_citemacro_citet">
       Raffel et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib110" title="">
         110
        </a>
       </sup>
      </cite>
      developed the Text-to-Text Transfer Transformer (T5) to demonstrate that various tasks can be reframed as text-to-text tasks, allowing the same model architecture and training procedure to be used with the same set of weights across different tasks. This approach has inspired the development of multi-task models that can adapt to different tasks at inference time. For example, Flan-T5
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS4.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib111" title="">
         111
        </a>
       </sup>
      </cite>
      used instruction fine-tuning with chain-of-thought prompts, enabling it to generalize to unseen tasks, such as generating rationales before answering. More complex approaches have been proposed for robust multi-task models.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS4.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib112" title="">
         112
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib113" title="">
         113
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib114" title="">
         114
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib115" title="">
         115
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS3.SSS4.p2">
     <p class="ltx_p" id="S2.SS3.SSS4.p2.1">
      Additionally, LLMs have been extended to understand different input modalities, despite initially receiving only text. For instance, Fuyu
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS4.p2.1.1.1">
        <a class="ltx_ref" href="#bib.bib116" title="">
         116
        </a>
       </sup>
      </cite>
      uses linear projection to adapt image representations into the token space of an LLM, enabling a decoder-only model to write figure captions. Scaling up, next-GPT
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S2.SS3.SSS4.p2.1.2.1">
        <a class="ltx_ref" href="#bib.bib117" title="">
         117
        </a>
       </sup>
      </cite>
      was developed as an “any-to-any” model, processing multiple modalities—text, audio, image, and video—through modality-specific encoders. The encoded representation is projected into a decoder-only token space, and the LLM’s output is processed by a domain-specific diffusion model to generate each modality’s output. Multitask or multimodel methods are further described below as these methods start to connect LLMs with autonomous agents.
     </p>
    </div>
    <figure class="ltx_figure" id="S2.F2">
     <div class="ltx_inline-block ltx_transformed_outer" id="S2.F2.1" style="width:854.8pt;height:338.1pt;vertical-align:-331.2pt;">
      <span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
       <p class="ltx_p" id="S2.F2.1.1">
        <span class="ltx_text" id="S2.F2.1.1.1">
         <span class="ltx_ERROR undefined" id="S2.F2.1.1.1.1">
          {forest}
         </span>
         for tree=
forked edges,
grow’=0,
draw,
rounded corners,
node options=align=center,
text width=2.7cm,
s sep=6pt,
calign=edge midpoint,
,
[SciLLMs, fill=blue, root
[Text2Text, task
[Question-answering, application
[BioMedLM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.2.1">
           <a class="ltx_ref" href="#bib.bib118" title="">
            118
           </a>
          </sup>
         </cite>
         , BioMistral
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.3.1">
           <a class="ltx_ref" href="#bib.bib119" title="">
            119
           </a>
          </sup>
         </cite>
         , BiMediX
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.4.1">
           <a class="ltx_ref" href="#bib.bib120" title="">
            120
           </a>
          </sup>
         </cite>
         , EpilepsyLLM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.5.1">
           <a class="ltx_ref" href="#bib.bib121" title="">
            121
           </a>
          </sup>
         </cite>
         , CheXagent
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.6.1">
           <a class="ltx_ref" href="#bib.bib122" title="">
            122
           </a>
          </sup>
         </cite>
         , BioMedGPT-LM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.7.1">
           <a class="ltx_ref" href="#bib.bib123" title="">
            123
           </a>
          </sup>
         </cite>
         , Darwin
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.8.1">
           <a class="ltx_ref" href="#bib.bib124" title="">
            124
           </a>
          </sup>
         </cite>
         , PMC-LLaMA
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.9.1">
           <a class="ltx_ref" href="#bib.bib125" title="">
            125
           </a>
          </sup>
         </cite>
         , Galactica
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.10.1">
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
          </sup>
         </cite>
         , BioGPT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.11.1">
           <a class="ltx_ref" href="#bib.bib127" title="">
            127
           </a>
          </sup>
         </cite>
         , nacho0
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.12.1">
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
          </sup>
         </cite>
         , etc., study]
]
]
[Text2Mol, task
[Conditional de-novo generation, application
[BioT5+
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.13.1">
           <a class="ltx_ref" href="#bib.bib129" title="">
            129
           </a>
          </sup>
         </cite>
         , Darwin
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.14.1">
           <a class="ltx_ref" href="#bib.bib124" title="">
            124
           </a>
          </sup>
         </cite>
         , Text+Chem T5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.15.1">
           <a class="ltx_ref" href="#bib.bib130" title="">
            130
           </a>
          </sup>
         </cite>
         , MolT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.16.1">
           <a class="ltx_ref" href="#bib.bib131" title="">
            131
           </a>
          </sup>
         </cite>
         , etc., study]
]
]
[Text2Number, task
[Document classification, application
[MatSciBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.17.1">
           <a class="ltx_ref" href="#bib.bib132" title="">
            132
           </a>
          </sup>
         </cite>
         , Galactica
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.18.1">
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
          </sup>
         </cite>
         , PubMedBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.19.1">
           <a class="ltx_ref" href="#bib.bib133" title="">
            133
           </a>
          </sup>
         </cite>
         , SciBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.20.1">
           <a class="ltx_ref" href="#bib.bib134" title="">
            134
           </a>
          </sup>
         </cite>
         , BlueBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.21.1">
           <a class="ltx_ref" href="#bib.bib135" title="">
            135
           </a>
          </sup>
         </cite>
         , etc, study]
]
[Property prediction, application
[EpilepsyLLM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.22.1">
           <a class="ltx_ref" href="#bib.bib121" title="">
            121
           </a>
          </sup>
         </cite>
         , CatBERTa
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.23.1">
           <a class="ltx_ref" href="#bib.bib136" title="">
            136
           </a>
          </sup>
         </cite>
         , ScholarBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.24.1">
           <a class="ltx_ref" href="#bib.bib137" title="">
            137
           </a>
          </sup>
         </cite>
         , ClinicalBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.25.1">
           <a class="ltx_ref" href="#bib.bib138" title="">
            138
           </a>
          </sup>
         </cite>
         , etc, study]
]
[Named entity recognition, application
[nacho0
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.26.1">
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
          </sup>
         </cite>
         , MaterialsBert
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.27.1">
           <a class="ltx_ref" href="#bib.bib139" title="">
            139
           </a>
          </sup>
         </cite>
         , MatBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.28.1">
           <a class="ltx_ref" href="#bib.bib140" title="">
            140
           </a>
          </sup>
         </cite>
         , ChemBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.29.1">
           <a class="ltx_ref" href="#bib.bib141" title="">
            141
           </a>
          </sup>
         </cite>
         , PubMedBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.30.1">
           <a class="ltx_ref" href="#bib.bib133" title="">
            133
           </a>
          </sup>
         </cite>
         , BioMegatron
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.31.1">
           <a class="ltx_ref" href="#bib.bib142" title="">
            142
           </a>
          </sup>
         </cite>
         , BioBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.32.1">
           <a class="ltx_ref" href="#bib.bib143" title="">
            143
           </a>
          </sup>
         </cite>
         , SciBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.33.1">
           <a class="ltx_ref" href="#bib.bib134" title="">
            134
           </a>
          </sup>
         </cite>
         , BlueBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.34.1">
           <a class="ltx_ref" href="#bib.bib135" title="">
            135
           </a>
          </sup>
         </cite>
         , etc, study]
]
]
[Mol2Text, task
[Molecule captioning, application
[BioT5+
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.35.1">
           <a class="ltx_ref" href="#bib.bib129" title="">
            129
           </a>
          </sup>
         </cite>
         , BioT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.36.1">
           <a class="ltx_ref" href="#bib.bib144" title="">
            144
           </a>
          </sup>
         </cite>
         , Text+Chem T5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.37.1">
           <a class="ltx_ref" href="#bib.bib130" title="">
            130
           </a>
          </sup>
         </cite>
         , Galactica
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.38.1">
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
          </sup>
         </cite>
         , MolT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.39.1">
           <a class="ltx_ref" href="#bib.bib131" title="">
            131
           </a>
          </sup>
         </cite>
         , etc., study]
]
]
[Mol2Mol, task
[Molecule tuning, application
[nacho0
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.40.1">
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
          </sup>
         </cite>
         , Regression Transformer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.41.1">
           <a class="ltx_ref" href="#bib.bib145" title="">
            145
           </a>
          </sup>
         </cite>
         , ChemFormer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.42.1">
           <a class="ltx_ref" href="#bib.bib146" title="">
            146
           </a>
          </sup>
         </cite>
         , etc., study]
]
[Synthesis prediction, application
[ReactionT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.43.1">
           <a class="ltx_ref" href="#bib.bib147" title="">
            147
           </a>
          </sup>
         </cite>
         ,
Galactica
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.44.1">
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
          </sup>
         </cite>
         , ChemFormer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.45.1">
           <a class="ltx_ref" href="#bib.bib146" title="">
            146
           </a>
          </sup>
         </cite>
         , T5Chem
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.46.1">
           <a class="ltx_ref" href="#bib.bib148" title="">
            148
           </a>
          </sup>
         </cite>
         , etc., study]
]
[Retrosynthesis, application
[Text+Chem T5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.47.1">
           <a class="ltx_ref" href="#bib.bib130" title="">
            130
           </a>
          </sup>
         </cite>
         , T5Chem
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.48.1">
           <a class="ltx_ref" href="#bib.bib148" title="">
            148
           </a>
          </sup>
         </cite>
         , etc., study]
]
]
[Mol2Number, task
[Property prediction, application
[nacho0
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.49.1">
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
          </sup>
         </cite>
         , Regression Transformer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.50.1">
           <a class="ltx_ref" href="#bib.bib145" title="">
            145
           </a>
          </sup>
         </cite>
         , CatBERTa
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.51.1">
           <a class="ltx_ref" href="#bib.bib136" title="">
            136
           </a>
          </sup>
         </cite>
         , Darwin
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.52.1">
           <a class="ltx_ref" href="#bib.bib124" title="">
            124
           </a>
          </sup>
         </cite>
         , SELFormer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.53.1">
           <a class="ltx_ref" href="#bib.bib149" title="">
            149
           </a>
          </sup>
         </cite>
         , Galactica
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.54.1">
           <a class="ltx_ref" href="#bib.bib126" title="">
            126
           </a>
          </sup>
         </cite>
         , ChemBERTa-2
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.55.1">
           <a class="ltx_ref" href="#bib.bib150" title="">
            150
           </a>
          </sup>
         </cite>
         , MolFormer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.56.1">
           <a class="ltx_ref" href="#bib.bib151" title="">
            151
           </a>
          </sup>
         </cite>
         , Mol-BERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.57.1">
           <a class="ltx_ref" href="#bib.bib152" title="">
            152
           </a>
          </sup>
         </cite>
         , MTL-BERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.58.1">
           <a class="ltx_ref" href="#bib.bib153" title="">
            153
           </a>
          </sup>
         </cite>
         , ChemBERTa
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.59.1">
           <a class="ltx_ref" href="#bib.bib44" title="">
            44
           </a>
          </sup>
         </cite>
         , MolBERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.60.1">
           <a class="ltx_ref" href="#bib.bib154" title="">
            154
           </a>
          </sup>
         </cite>
         , SMILES-BERT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.61.1">
           <a class="ltx_ref" href="#bib.bib155" title="">
            155
           </a>
          </sup>
         </cite>
         , SMILES transformer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S2.F2.1.1.1.62.1">
           <a class="ltx_ref" href="#bib.bib156" title="">
            156
           </a>
          </sup>
         </cite>
         etc, study]
]
]
]
        </span>
       </p>
      </span>
     </div>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 2:
      </span>
      Classification of LLMs in chemistry and biochemistry according to their application.
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   LLMs for chemistry and biochemistry
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Integrating LLMs into chemistry and biochemistry has revolutionized molecular design, synthesis prediction, and property analysis. These models, trained on vast datasets, interpret chemical languages like SMILES and InChI to predict molecular behavior accurately. This section explores the importance of trustworthy datasets and the necessity of good benchmarks and LLM applications in molecular representations, property prediction, inverse design, and synthesis prediction. Figure
    <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2.3.4 Multi-task and multi-modal models ‣ 2.3 Model types ‣ 2 Large Language Models ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    illustrates the capabilities of different LLMs available currently, and Figure
    <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.1 Molecular Representations, Datasets, and Benchmarks ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    shows a chronological map of LLMs in chemistry and biology.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Molecular Representations, Datasets, and Benchmarks
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Given the diversity of data in chemistry-based machine learning, and the many different ways that a compound can be described, from molecular structure to property description, there are multiple ways a molecule can be represented, underlining this heterogeneity.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib157" title="">
        157
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib158" title="">
        158
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib159" title="">
        159
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib160" title="">
        160
       </a>
      </sup>
     </cite>
     Some common forms include molecular graphs
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib161" title="">
        161
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib162" title="">
        162
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib163" title="">
        163
       </a>
      </sup>
     </cite>
     , 3D point clouds
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib164" title="">
        164
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib165" title="">
        165
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib166" title="">
        166
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib167" title="">
        167
       </a>
      </sup>
     </cite>
     , and quantitative feature descriptors.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.4.1">
       <a class="ltx_ref" href="#bib.bib168" title="">
        168
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib169" title="">
        169
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib170" title="">
        170
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib171" title="">
        171
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib158" title="">
        158
       </a>
      </sup>
     </cite>
     In this review, we will focus on string-based representations of molecules, given the interest in language models. Among the known string representations, we can cite IUPAC names, SMILES,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.5.1">
       <a class="ltx_ref" href="#bib.bib43" title="">
        43
       </a>
      </sup>
     </cite>
     DeepSMILES,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.6.1">
       <a class="ltx_ref" href="#bib.bib172" title="">
        172
       </a>
      </sup>
     </cite>
     SELFIES,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.7.1">
       <a class="ltx_ref" href="#bib.bib173" title="">
        173
       </a>
      </sup>
     </cite>
     and InChI,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p1.1.8.1">
       <a class="ltx_ref" href="#bib.bib174" title="">
        174
       </a>
      </sup>
     </cite>
     as recently reviewed by
     <cite class="ltx_cite ltx_citemacro_citet">
      Das et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib175" title="">
        175
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F3">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="153" id="S3.F3.g1" src="/html/2407.01603/assets/x2.png" width="276"/>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Number of training tokens (on log scale) available from various chemical sources compared with typical LLM training runs. The numbers are drawn from ZINC
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.F3.4.1.1">
       <a class="ltx_ref" href="#bib.bib176" title="">
        176
       </a>
      </sup>
     </cite>
     , PubChem
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.F3.5.2.1">
       <a class="ltx_ref" href="#bib.bib177" title="">
        177
       </a>
      </sup>
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Touvron et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib178" title="">
        178
       </a>
      </sup>
     </cite>
     , ChEMBL
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.F3.6.3.1">
       <a class="ltx_ref" href="#bib.bib179" title="">
        179
       </a>
      </sup>
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Kinney et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib180" title="">
        180
       </a>
      </sup>
     </cite>
    </figcaption>
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F3.7">
       .
      </p>
     </div>
    </div>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     Regarding datasets, there are two types of data used for training LLMs: training data and evaluation data. Training data should be grounded on real molecules to build the correct “prior belief” of a molecule definition during the LLM training process. Similar care is taken when natural language training data used in GPT-4, for instance, is built from real sentences or code; it is important not to use random combinations of words, even if they are grammatically correct.
Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.1 Molecular Representations, Datasets, and Benchmarks ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     shows the number of tokens on common chemistry datasets in comparison with the number of tokens used to train LLaMA2, based on literature information.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib176" title="">
        176
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib177" title="">
        177
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib178" title="">
        178
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib179" title="">
        179
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib180" title="">
        180
       </a>
      </sup>
     </cite>
     With this in mind, we note the largest chemical training corpus, which largely comprises hypothetical chemical structures, amounts to billions of tokens, almost two orders of magnitude fewer than the trillions of tokens used to train LLaMA2. When not including the large number of hypothetical structures found on ZINC
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib176" title="">
        176
       </a>
      </sup>
     </cite>
     , (Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.1 Molecular Representations, Datasets, and Benchmarks ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ), that number of tokens associated with verifiably synthesized compounds is over five orders of magnitude lower than the LLaMA2 training data.
Nonetheless,
     <cite class="ltx_cite ltx_citemacro_citet">
      Segler et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib181" title="">
        181
       </a>
      </sup>
     </cite>
     demonstrated that even using the Reaxys dataset, a very small human-curated set of chemical reactions, is enough to enable state-of-the-art retrosynthesis results. Therefore, the lack of data is not the only problem. Instead, the lack of good-quality data may be the pivotal factor holding back the development of better scientific LLMs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     On the other hand, benchmark data evaluates the success of these models. Numerous datasets, curated by the scientific community, are available for this purpose
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib182" title="">
        182
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib183" title="">
        183
       </a>
      </sup>
     </cite>
     . Among them, MoleculeNet
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib57" title="">
        57
       </a>
      </sup>
     </cite>
     , first published in 2017, is the most commonly used labeled dataset for chemistry. However, MoleculeNet has several issues: it is small, contains errors and inconsistencies, and lacks relevance to real-world chemistry problems
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p3.1.3.1">
       <a class="ltx_ref" href="#bib.bib184" title="">
        184
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib146" title="">
        146
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib185" title="">
        185
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib128" title="">
        128
       </a>
      </sup>
     </cite>
     . Pat Walters, a leader in ML for drug discovery, stated, “I think the best way to make progress on applications of machine learning to drug discovery is to fund a large public effort that will generate high-quality data and make this data available to the community”
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p3.1.4.1">
       <a class="ltx_ref" href="#bib.bib186" title="">
        186
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     Walters makes several constructive critiques such as, for example, how the QM7, QM8, and QM9 datasets, intended for predicting quantum properties from 3D structures, are often misused with predictions based incorrectly on 1D SMILES strings, which inadequately represent 3D molecular conformations.
Walters also makes suggestions for more relevant benchmarks and also datasets with more valid entries. He suggests the Absorption, Distribution, Metabolism, and Excretion (ADME) data curated by
     <cite class="ltx_cite ltx_citemacro_citet">
      Fang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib187" title="">
        187
       </a>
      </sup>
     </cite>
     , and the Therapeutic Data Commons (TDC)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib188" title="">
        188
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib189" title="">
        189
       </a>
      </sup>
     </cite>
     and TDC-2
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib190" title="">
        190
       </a>
      </sup>
     </cite>
     . These datasets contain measurements of real compounds, making them grounded in reality. Moreover, ADME is crucial for determining a drug candidate’s success, while therapeutic results in diverse modalities align with metrics used in drug development.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     Here, we hypothesize that the lack of easily accessible, high-quality data in the correct format for training foundational chemical language models is a major bottleneck to the development of the highly desired “super-human” AI-powered digital chemist. A more optimistic view is presented by
     <cite class="ltx_cite ltx_citemacro_citet">
      Rich and Birnbaum
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib191" title="">
        191
       </a>
      </sup>
     </cite>
     . They argue that we do not need to wait for the creation of new benchmarks. Instead, the messy, available public data can be used to curate benchmarks carefully curated for specific real-world approximation.
In addition, we argue that extracting data from scientific chemistry papers might be an interesting commitment to generating data of high quality, grounded to the truth, and on a large scale.
Recently, a benchmark following these ideas — LAB-Bench
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS1.p5.1.1.1">
       <a class="ltx_ref" href="#bib.bib192" title="">
        192
       </a>
      </sup>
     </cite>
     — was created
for evaluating LLMs’ performance in biology.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="663" id="S3.F4.g1" src="/html/2407.01603/assets/figs/llm_ldt.png" width="336"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Illustration of how Large Language Models (LLMs) evolved chronologically.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Property Prediction and Encoder-only mol-LLMs
   </h3>
   <figure class="ltx_table" id="S3.T1">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Encoder-only scientific LLMs.
     <math alttext="a" class="ltx_Math" display="inline" id="S3.T1.1.m1.1">
      <semantics id="S3.T1.1.m1.1b">
       <mi id="S3.T1.1.m1.1.1" xref="S3.T1.1.m1.1.1.cmml">
        a
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T1.1.m1.1c">
        <ci id="S3.T1.1.m1.1.1.cmml" xref="S3.T1.1.m1.1.1">
         𝑎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T1.1.m1.1d">
        a
       </annotation>
      </semantics>
     </math>
     :“Model Size” is reported as the number of parameters.
     <math alttext="b" class="ltx_Math" display="inline" id="S3.T1.2.m2.1">
      <semantics id="S3.T1.2.m2.1b">
       <mi id="S3.T1.2.m2.1.1" xref="S3.T1.2.m2.1.1.cmml">
        b
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T1.2.m2.1c">
        <ci id="S3.T1.2.m2.1.1.cmml" xref="S3.T1.2.m2.1.1">
         𝑏
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T1.2.m2.1d">
        b
       </annotation>
      </semantics>
     </math>
     : The authors report they not used as many encoder layers as it was used in the original BERT paper. But the total number of parameters was not reported.
    </figcaption>
    <table class="ltx_tabular" id="S3.T1.17">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S3.T1.5.1">
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S3.T1.5.1.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.2.1">
         <span class="ltx_p" id="S3.T1.5.1.2.1.1">
          LLM
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T1.5.1.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.1.1">
         <span class="ltx_p" id="S3.T1.5.1.1.1.1">
          Model Size
          <sup class="ltx_sup" id="S3.T1.5.1.1.1.1.1">
           <span class="ltx_text ltx_font_italic" id="S3.T1.5.1.1.1.1.1.1">
            a
           </span>
          </sup>
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T1.5.1.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.3.1">
         <span class="ltx_p" id="S3.T1.5.1.3.1.1">
          Training Data
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T1.5.1.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.4.1">
         <span class="ltx_p" id="S3.T1.5.1.4.1.1">
          Architecture
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T1.5.1.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.5.1">
         <span class="ltx_p" id="S3.T1.5.1.5.1.1">
          Application
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T1.5.1.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.1.6.1">
         <span class="ltx_p" id="S3.T1.5.1.6.1.1">
          Release date
         </span>
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S3.T1.17.14.1">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T1.17.14.1.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.1.1">
         <span class="ltx_p" id="S3.T1.17.14.1.1.1.1">
          CatBERTa
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.14.1.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib136" title="">
             136
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.17.14.1.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.2.1">
         <span class="ltx_p" id="S3.T1.17.14.1.2.1.1">
          355M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.17.14.1.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.3.1">
         <span class="ltx_p" id="S3.T1.17.14.1.3.1.1">
          OpenCatalyst2020 (OC20)
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.17.14.1.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.4.1">
         <span class="ltx_p" id="S3.T1.17.14.1.4.1.1">
          RoBERTa
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.17.14.1.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.5.1">
         <span class="ltx_p" id="S3.T1.17.14.1.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.17.14.1.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.14.1.6.1">
         <span class="ltx_p" id="S3.T1.17.14.1.6.1.1">
          2023.11
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.7.3">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.7.3.3" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.3.3.1">
         <span class="ltx_p" id="S3.T1.7.3.3.1.1">
          SELFormer
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.7.3.3.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib149" title="">
             149
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.6.2.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.6.2.1.1">
         <span class="ltx_p" id="S3.T1.6.2.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.6.2.1.1.1.m1.1">
           <semantics id="S3.T1.6.2.1.1.1.m1.1a">
            <mo id="S3.T1.6.2.1.1.1.m1.1.1" xref="S3.T1.6.2.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.6.2.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.6.2.1.1.1.m1.1.1.cmml" xref="S3.T1.6.2.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.6.2.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          86M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.7.3.2" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.3.2.1">
         <span class="ltx_p" id="S3.T1.7.3.2.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.7.3.2.1.1.m1.1">
           <semantics id="S3.T1.7.3.2.1.1.m1.1a">
            <mo id="S3.T1.7.3.2.1.1.m1.1.1" xref="S3.T1.7.3.2.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.7.3.2.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.7.3.2.1.1.m1.1.1.cmml" xref="S3.T1.7.3.2.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.7.3.2.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          2M compounds from ChEMBL
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.7.3.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.3.4.1">
         <span class="ltx_p" id="S3.T1.7.3.4.1.1">
          RoBERTa
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.7.3.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.3.5.1">
         <span class="ltx_p" id="S3.T1.7.3.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.7.3.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.3.6.1">
         <span class="ltx_p" id="S3.T1.7.3.6.1.1">
          2023.05
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.15.2">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.15.2.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.1.1">
         <span class="ltx_p" id="S3.T1.17.15.2.1.1.1">
          MaterialsBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.15.2.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib193" title="">
             193
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.15.2.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.2.1">
         <span class="ltx_p" id="S3.T1.17.15.2.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.15.2.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.3.1">
         <span class="ltx_p" id="S3.T1.17.15.2.3.1.1">
          2.4M material science abstracts + 750 annotated abstract for NER
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.15.2.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.4.1">
         <span class="ltx_p" id="S3.T1.17.15.2.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.15.2.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.5.1">
         <span class="ltx_p" id="S3.T1.17.15.2.5.1.1">
          NER and property extraction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.15.2.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.15.2.6.1">
         <span class="ltx_p" id="S3.T1.17.15.2.6.1.1">
          2023.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.16.3">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.16.3.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.1.1">
         <span class="ltx_p" id="S3.T1.17.16.3.1.1.1">
          SolvBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.16.3.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib194" title="">
             194
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.16.3.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.2.1">
         <span class="ltx_p" id="S3.T1.17.16.3.2.1.1">
          <span class="ltx_text ltx_font_italic" id="S3.T1.17.16.3.2.1.1.1">
           b
          </span>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.16.3.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.3.1">
         <span class="ltx_p" id="S3.T1.17.16.3.3.1.1">
          1M SMILES of solute-solvent pairs from CombiSolv-QM and LogS from
          <cite class="ltx_cite ltx_citemacro_citet">
           Boobier et al.
           <sup class="ltx_sup">
            <a class="ltx_ref" href="#bib.bib195" title="">
             195
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.16.3.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.4.1">
         <span class="ltx_p" id="S3.T1.17.16.3.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.16.3.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.5.1">
         <span class="ltx_p" id="S3.T1.17.16.3.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.16.3.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.16.3.6.1">
         <span class="ltx_p" id="S3.T1.17.16.3.6.1.1">
          2023.01
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.8.4">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.8.4.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.2.1">
         <span class="ltx_p" id="S3.T1.8.4.2.1.1">
          MolFormer
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.8.4.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib196" title="">
             196
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.8.4.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.1.1">
         <span class="ltx_p" id="S3.T1.8.4.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T1.8.4.1.1.1.m1.1">
           <semantics id="S3.T1.8.4.1.1.1.m1.1a">
            <mi id="S3.T1.8.4.1.1.1.m1.1.1" xref="S3.T1.8.4.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T1.8.4.1.1.1.m1.1b">
             <ci id="S3.T1.8.4.1.1.1.m1.1.1.cmml" xref="S3.T1.8.4.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.8.4.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.8.4.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.3.1">
         <span class="ltx_p" id="S3.T1.8.4.3.1.1">
          PubChem and ZINC
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.8.4.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.4.1">
         <span class="ltx_p" id="S3.T1.8.4.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.8.4.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.5.1">
         <span class="ltx_p" id="S3.T1.8.4.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.8.4.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.4.6.1">
         <span class="ltx_p" id="S3.T1.8.4.6.1.1">
          2022.12
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.17.4">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.17.4.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.1.1">
         <span class="ltx_p" id="S3.T1.17.17.4.1.1.1">
          ChemBERTa-2
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.17.4.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib150" title="">
             150
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.17.4.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.2.1">
         <span class="ltx_p" id="S3.T1.17.17.4.2.1.1">
          5M - 46M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.17.4.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.3.1">
         <span class="ltx_p" id="S3.T1.17.17.4.3.1.1">
          77M SMILES from PubChem
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.17.4.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.4.1">
         <span class="ltx_p" id="S3.T1.17.17.4.4.1.1">
          RoBERTa
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.17.4.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.5.1">
         <span class="ltx_p" id="S3.T1.17.17.4.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.17.4.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.17.4.6.1">
         <span class="ltx_p" id="S3.T1.17.17.4.6.1.1">
          2022.09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.18.5">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.18.5.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.1.1">
         <span class="ltx_p" id="S3.T1.17.18.5.1.1.1">
          ScholarBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.18.5.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib137" title="">
             137
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.18.5.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.2.1">
         <span class="ltx_p" id="S3.T1.17.18.5.2.1.1">
          340M, 770M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.18.5.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.3.1">
         <span class="ltx_p" id="S3.T1.17.18.5.3.1.1">
          Public.Resource.Org, Inc
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.18.5.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.4.1">
         <span class="ltx_p" id="S3.T1.17.18.5.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.18.5.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.5.1">
         <span class="ltx_p" id="S3.T1.17.18.5.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.18.5.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.18.5.6.1">
         <span class="ltx_p" id="S3.T1.17.18.5.6.1.1">
          2022.05
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.10.6">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.10.6.3" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.10.6.3.1">
         <span class="ltx_p" id="S3.T1.10.6.3.1.1">
          BatteryBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.10.6.3.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib197" title="">
             197
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.9.5.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.9.5.1.1">
         <span class="ltx_p" id="S3.T1.9.5.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.9.5.1.1.1.m1.1">
           <semantics id="S3.T1.9.5.1.1.1.m1.1a">
            <mo id="S3.T1.9.5.1.1.1.m1.1.1" xref="S3.T1.9.5.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.9.5.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.9.5.1.1.1.m1.1.1.cmml" xref="S3.T1.9.5.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.9.5.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.10.6.2" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.10.6.2.1">
         <span class="ltx_p" id="S3.T1.10.6.2.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.10.6.2.1.1.m1.1">
           <semantics id="S3.T1.10.6.2.1.1.m1.1a">
            <mo id="S3.T1.10.6.2.1.1.m1.1.1" xref="S3.T1.10.6.2.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.10.6.2.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.10.6.2.1.1.m1.1.1.cmml" xref="S3.T1.10.6.2.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.10.6.2.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          400k papers from RSC, Elsevier and Springer
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.10.6.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.10.6.4.1">
         <span class="ltx_p" id="S3.T1.10.6.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.10.6.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.10.6.5.1">
         <span class="ltx_p" id="S3.T1.10.6.5.1.1">
          Document classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.10.6.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.10.6.6.1">
         <span class="ltx_p" id="S3.T1.10.6.6.1.1">
          2022.05
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.11.7">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.11.7.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.2.1">
         <span class="ltx_p" id="S3.T1.11.7.2.1.1">
          MatSciBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.11.7.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib132" title="">
             132
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.11.7.3" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.3.1">
         <span class="ltx_p" id="S3.T1.11.7.3.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.11.7.1" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.1.1">
         <span class="ltx_p" id="S3.T1.11.7.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.11.7.1.1.1.m1.1">
           <semantics id="S3.T1.11.7.1.1.1.m1.1a">
            <mo id="S3.T1.11.7.1.1.1.m1.1.1" xref="S3.T1.11.7.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.11.7.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.11.7.1.1.1.m1.1.1.cmml" xref="S3.T1.11.7.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.11.7.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          150K material science paper downloaded from Elsevier
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.11.7.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.4.1">
         <span class="ltx_p" id="S3.T1.11.7.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.11.7.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.5.1">
         <span class="ltx_p" id="S3.T1.11.7.5.1.1">
          NER and text classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.11.7.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.11.7.6.1">
         <span class="ltx_p" id="S3.T1.11.7.6.1.1">
          2022.05
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.19.6">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.19.6.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.1.1">
         <span class="ltx_p" id="S3.T1.17.19.6.1.1.1">
          MatBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.19.6.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib140" title="">
             140
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.19.6.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.2.1">
         <span class="ltx_p" id="S3.T1.17.19.6.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.19.6.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.3.1">
         <span class="ltx_p" id="S3.T1.17.19.6.3.1.1">
          Abstracts from solid state articles and abstracts and methods from gold nanoparticle articles
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.19.6.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.4.1">
         <span class="ltx_p" id="S3.T1.17.19.6.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.19.6.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.5.1">
         <span class="ltx_p" id="S3.T1.17.19.6.5.1.1">
          NER
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.19.6.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.19.6.6.1">
         <span class="ltx_p" id="S3.T1.17.19.6.6.1.1">
          2022.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.20.7">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.20.7.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.1.1">
         <span class="ltx_p" id="S3.T1.17.20.7.1.1.1">
          PubMedBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.20.7.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib133" title="">
             133
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.20.7.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.2.1">
         <span class="ltx_p" id="S3.T1.17.20.7.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.20.7.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.3.1">
         <span class="ltx_p" id="S3.T1.17.20.7.3.1.1">
          14M abstracts from PubMed
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.20.7.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.4.1">
         <span class="ltx_p" id="S3.T1.17.20.7.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.20.7.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.5.1">
         <span class="ltx_p" id="S3.T1.17.20.7.5.1.1">
          NER, QA, and document classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.20.7.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.20.7.6.1">
         <span class="ltx_p" id="S3.T1.17.20.7.6.1.1">
          2021.10
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.12.8">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.12.8.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.2.1">
         <span class="ltx_p" id="S3.T1.12.8.2.1.1">
          Mol-BERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.12.8.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib152" title="">
             152
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.12.8.3" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.3.1">
         <span class="ltx_p" id="S3.T1.12.8.3.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.12.8.1" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.1.1">
         <span class="ltx_p" id="S3.T1.12.8.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.12.8.1.1.1.m1.1">
           <semantics id="S3.T1.12.8.1.1.1.m1.1a">
            <mo id="S3.T1.12.8.1.1.1.m1.1.1" xref="S3.T1.12.8.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.12.8.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.12.8.1.1.1.m1.1.1.cmml" xref="S3.T1.12.8.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.12.8.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          4B SMILES from ZINC15 and ChEMBL27
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.12.8.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.4.1">
         <span class="ltx_p" id="S3.T1.12.8.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.12.8.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.5.1">
         <span class="ltx_p" id="S3.T1.12.8.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.12.8.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.12.8.6.1">
         <span class="ltx_p" id="S3.T1.12.8.6.1.1">
          2021.09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.13.9">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.13.9.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.2.1">
         <span class="ltx_p" id="S3.T1.13.9.2.1.1">
          ChemBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.13.9.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib141" title="">
             141
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.13.9.3" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.3.1">
         <span class="ltx_p" id="S3.T1.13.9.3.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.13.9.1" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.1.1">
         <span class="ltx_p" id="S3.T1.13.9.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.13.9.1.1.1.m1.1">
           <semantics id="S3.T1.13.9.1.1.1.m1.1a">
            <mo id="S3.T1.13.9.1.1.1.m1.1.1" xref="S3.T1.13.9.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.13.9.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.13.9.1.1.1.m1.1.1.cmml" xref="S3.T1.13.9.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.13.9.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          200k extracted using ChemDataExtractor
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.13.9.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.4.1">
         <span class="ltx_p" id="S3.T1.13.9.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.13.9.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.5.1">
         <span class="ltx_p" id="S3.T1.13.9.5.1.1">
          NER
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.13.9.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.13.9.6.1">
         <span class="ltx_p" id="S3.T1.13.9.6.1.1">
          2021.06
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.14.10">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.14.10.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.2.1">
         <span class="ltx_p" id="S3.T1.14.10.2.1.1">
          MolBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.14.10.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib154" title="">
             154
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.14.10.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.1.1">
         <span class="ltx_p" id="S3.T1.14.10.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.14.10.1.1.1.m1.1">
           <semantics id="S3.T1.14.10.1.1.1.m1.1a">
            <mo id="S3.T1.14.10.1.1.1.m1.1.1" xref="S3.T1.14.10.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.14.10.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.14.10.1.1.1.m1.1.1.cmml" xref="S3.T1.14.10.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.14.10.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          85M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.14.10.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.3.1">
         <span class="ltx_p" id="S3.T1.14.10.3.1.1">
          ChemBench
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.14.10.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.4.1">
         <span class="ltx_p" id="S3.T1.14.10.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.14.10.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.5.1">
         <span class="ltx_p" id="S3.T1.14.10.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.14.10.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.14.10.6.1">
         <span class="ltx_p" id="S3.T1.14.10.6.1.1">
          2020.11
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.21.8">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.21.8.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.21.8.1.1">
         <span class="ltx_p" id="S3.T1.17.21.8.1.1.1">
          ChemBERTa
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.21.8.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib44" title="">
             44
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_middle" id="S3.T1.17.21.8.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.21.8.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.21.8.3.1">
         <span class="ltx_p" id="S3.T1.17.21.8.3.1.1">
          10M SMILES from PubChem
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.21.8.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.21.8.4.1">
         <span class="ltx_p" id="S3.T1.17.21.8.4.1.1">
          RoBERTa
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.21.8.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.21.8.5.1">
         <span class="ltx_p" id="S3.T1.17.21.8.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.21.8.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.21.8.6.1">
         <span class="ltx_p" id="S3.T1.17.21.8.6.1.1">
          2020.10
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.22.9">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.22.9.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.1.1">
         <span class="ltx_p" id="S3.T1.17.22.9.1.1.1">
          BioMegatron
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.22.9.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib142" title="">
             142
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.22.9.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.2.1">
         <span class="ltx_p" id="S3.T1.17.22.9.2.1.1">
          345M, 800M, 1.2B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.22.9.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.3.1">
         <span class="ltx_p" id="S3.T1.17.22.9.3.1.1">
          Wikipedia, CC-Stories, Real-News, and OpenWebtext
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.22.9.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.4.1">
         <span class="ltx_p" id="S3.T1.17.22.9.4.1.1">
          Megatron-LM
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.22.9.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.5.1">
         <span class="ltx_p" id="S3.T1.17.22.9.5.1.1">
          NER and QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.22.9.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.22.9.6.1">
         <span class="ltx_p" id="S3.T1.17.22.9.6.1.1">
          2020-10
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.23.10">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.23.10.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.1.1">
         <span class="ltx_p" id="S3.T1.17.23.10.1.1.1">
          BioBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.23.10.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib143" title="">
             143
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.23.10.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.2.1">
         <span class="ltx_p" id="S3.T1.17.23.10.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.23.10.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.3.1">
         <span class="ltx_p" id="S3.T1.17.23.10.3.1.1">
          PubMed and PMC
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.23.10.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.4.1">
         <span class="ltx_p" id="S3.T1.17.23.10.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.23.10.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.5.1">
         <span class="ltx_p" id="S3.T1.17.23.10.5.1.1">
          NER and QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.23.10.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.23.10.6.1">
         <span class="ltx_p" id="S3.T1.17.23.10.6.1.1">
          2020.02
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.15.11">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.15.11.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.2.1">
         <span class="ltx_p" id="S3.T1.15.11.2.1.1">
          Molecule Attention Transformer
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.15.11.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib198" title="">
             198
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.15.11.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.1.1">
         <span class="ltx_p" id="S3.T1.15.11.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T1.15.11.1.1.1.m1.1">
           <semantics id="S3.T1.15.11.1.1.1.m1.1a">
            <mi id="S3.T1.15.11.1.1.1.m1.1.1" xref="S3.T1.15.11.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T1.15.11.1.1.1.m1.1b">
             <ci id="S3.T1.15.11.1.1.1.m1.1.1.cmml" xref="S3.T1.15.11.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.15.11.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.15.11.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.3.1">
         <span class="ltx_p" id="S3.T1.15.11.3.1.1">
          ZINC15
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.15.11.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.4.1">
         <span class="ltx_p" id="S3.T1.15.11.4.1.1">
          Encoder with GCN features
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.15.11.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.5.1">
         <span class="ltx_p" id="S3.T1.15.11.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.15.11.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.15.11.6.1">
         <span class="ltx_p" id="S3.T1.15.11.6.1.1">
          2020.02
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.24.11">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.24.11.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.1.1">
         <span class="ltx_p" id="S3.T1.17.24.11.1.1.1">
          SciBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.24.11.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib134" title="">
             134
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.24.11.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.2.1">
         <span class="ltx_p" id="S3.T1.17.24.11.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.24.11.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.3.1">
         <span class="ltx_p" id="S3.T1.17.24.11.3.1.1">
          1.14M papers from Semantic Scholar
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.24.11.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.4.1">
         <span class="ltx_p" id="S3.T1.17.24.11.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.24.11.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.5.1">
         <span class="ltx_p" id="S3.T1.17.24.11.5.1.1">
          NER and sentence classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.24.11.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.24.11.6.1">
         <span class="ltx_p" id="S3.T1.17.24.11.6.1.1">
          2019.09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.13">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.13.3" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.13.3.1">
         <span class="ltx_p" id="S3.T1.17.13.3.1.1">
          SMILES-BERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.13.3.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib199" title="">
             199
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.16.12.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.16.12.1.1">
         <span class="ltx_p" id="S3.T1.16.12.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T1.16.12.1.1.1.m1.1">
           <semantics id="S3.T1.16.12.1.1.1.m1.1a">
            <mi id="S3.T1.16.12.1.1.1.m1.1.1" xref="S3.T1.16.12.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T1.16.12.1.1.1.m1.1b">
             <ci id="S3.T1.16.12.1.1.1.m1.1.1.cmml" xref="S3.T1.16.12.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.16.12.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.13.2" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.13.2.1">
         <span class="ltx_p" id="S3.T1.17.13.2.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T1.17.13.2.1.1.m1.1">
           <semantics id="S3.T1.17.13.2.1.1.m1.1a">
            <mo id="S3.T1.17.13.2.1.1.m1.1.1" xref="S3.T1.17.13.2.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T1.17.13.2.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T1.17.13.2.1.1.m1.1.1.cmml" xref="S3.T1.17.13.2.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T1.17.13.2.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          18M SMILES from ZINC
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.13.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.13.4.1">
         <span class="ltx_p" id="S3.T1.17.13.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.13.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.13.5.1">
         <span class="ltx_p" id="S3.T1.17.13.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.13.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.13.6.1">
         <span class="ltx_p" id="S3.T1.17.13.6.1.1">
          2019.09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.25.12">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T1.17.25.12.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.1.1">
         <span class="ltx_p" id="S3.T1.17.25.12.1.1.1">
          BlueBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.25.12.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib135" title="">
             135
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.25.12.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.2.1">
         <span class="ltx_p" id="S3.T1.17.25.12.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.25.12.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.3.1">
         <span class="ltx_p" id="S3.T1.17.25.12.3.1.1">
          PubMed and MIMIC-III
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.25.12.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.4.1">
         <span class="ltx_p" id="S3.T1.17.25.12.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.25.12.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.5.1">
         <span class="ltx_p" id="S3.T1.17.25.12.5.1.1">
          NER, and document classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.17.25.12.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.25.12.6.1">
         <span class="ltx_p" id="S3.T1.17.25.12.6.1.1">
          2019.06
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.17.26.13">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S3.T1.17.26.13.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.1.1">
         <span class="ltx_p" id="S3.T1.17.26.13.1.1.1">
          ClinicalBERT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T1.17.26.13.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib138" title="">
             138
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T1.17.26.13.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.2.1">
         <span class="ltx_p" id="S3.T1.17.26.13.2.1.1">
          110M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T1.17.26.13.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.3.1">
         <span class="ltx_p" id="S3.T1.17.26.13.3.1.1">
          MIMIC-III
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T1.17.26.13.4" style="width:51.2pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.4.1">
         <span class="ltx_p" id="S3.T1.17.26.13.4.1.1">
          BERT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T1.17.26.13.5" style="width:85.4pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.5.1">
         <span class="ltx_p" id="S3.T1.17.26.13.5.1.1">
          Patient readmission probability
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T1.17.26.13.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.17.26.13.6.1">
         <span class="ltx_p" id="S3.T1.17.26.13.6.1.1">
          2019.04
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Encoder-only transformer architectures solely comprise an encoder. Chemists can leverage the simpler encoder-only architecture for tasks where the primary goal is classification, efficient exploration of chemical space, and property prediction. Since encoder-only architectures are mostly applied to property prediction, we describe here the relative importance of this principal chemical challenge.
     <cite class="ltx_cite ltx_citemacro_citet">
      Sultan et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib200" title="">
        200
       </a>
      </sup>
     </cite>
     also discussed the high importance of this task, the knowledge obtained in the last years, and the remaining challenges regarding molecular property prediction using LLMs.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1
     </span>
     Property Prediction
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      The universal value of chemistry lies in identifying and understanding the properties of compounds to optimize their practical applications. In the pharmaceutical industry, therapeutic molecules interact with the body in profound ways.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib201" title="">
         201
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib202" title="">
         202
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib203" title="">
         203
        </a>
       </sup>
      </cite>
      Understanding these interactions and modifying molecular structures to enhance those therapeutic benefits can lead to significant medical advancements.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib204" title="">
         204
        </a>
       </sup>
      </cite>
      Similarly, in polymer science, material properties depend on chemical structure, polymer chain length, and packing,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib205" title="">
         205
        </a>
       </sup>
      </cite>
      and a protein’s function similarly depends on its structure and folding.
Historically, chemists have identified new molecules from natural products
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.4.1">
        <a class="ltx_ref" href="#bib.bib206" title="">
         206
        </a>
       </sup>
      </cite>
      and screened them against potential targets
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.5.1">
        <a class="ltx_ref" href="#bib.bib207" title="">
         207
        </a>
       </sup>
      </cite>
      to test their properties for diseases. Once a natural product shows potential, chemists synthesize scaled-up quantities for further testing or derivatization,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.6.1">
        <a class="ltx_ref" href="#bib.bib208" title="">
         208
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib209" title="">
         209
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib210" title="">
         210
        </a>
       </sup>
      </cite>
      a costly and labor-intensive process.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.7.1">
        <a class="ltx_ref" href="#bib.bib211" title="">
         211
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib212" title="">
         212
        </a>
       </sup>
      </cite>
      Traditionally, chemists have used their expertise to hypothesize the properties of new molecules derived from those natural products, hence aiming for the best investment of synthesis time and labor.
To support the chemical industry in more accurate property prediction, computational chemistry has evolved.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.8.1">
        <a class="ltx_ref" href="#bib.bib213" title="">
         213
        </a>
       </sup>
      </cite>
      Quantum theoretical calculations predict certain properties reliably, while force-field-based Molecular Dynamics (MD)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.9.1">
        <a class="ltx_ref" href="#bib.bib214" title="">
         214
        </a>
       </sup>
      </cite>
      predict packing and crystal structures of large molecular ensembles, though both require substantial computational resources.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.10.1">
        <a class="ltx_ref" href="#bib.bib215" title="">
         215
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib216" title="">
         216
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib217" title="">
         217
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib218" title="">
         218
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib219" title="">
         219
        </a>
       </sup>
      </cite>
      Property prediction can now be enhanced through machine learning tools,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS1.p1.1.11.1">
        <a class="ltx_ref" href="#bib.bib220" title="">
         220
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib170" title="">
         170
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib221" title="">
         221
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib222" title="">
         222
        </a>
       </sup>
      </cite>
      and more recent advancements in LLMs lead to effective property prediction without the extensive computational demands of quantum mechanics and MD calculations. Combined with human insight, AI can revolutionize material development, enabling the synthesis of new materials with a high likelihood of possessing desired properties for specific applications.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2
     </span>
     Encoder-only Mol-LLMs
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      Encoder-only models are exemplified by the BERT architecture, which is commonly applied in natural language sentiment analysis to extract deeper patterns from prose.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib223" title="">
         223
        </a>
       </sup>
      </cite>
      The human chemist has been taught to look at a 2D image of a molecular structure and to recognize its chemical properties or classify the compound. Therefore, encoder-only models would ideally convert SMILES strings, empty of inherent chemical essence, into a vector representation, or latent space, reflecting those chemical properties. This vector representation can then be used directly for various downstream tasks.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
     <p class="ltx_p" id="S3.SS2.SSS2.p2.1">
      <cite class="ltx_cite ltx_citemacro_citet">
       Schwaller et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib224" title="">
         224
        </a>
       </sup>
      </cite>
      used a BERT model to more accurately classify complex synthesis reactions by generating reaction fingerprints from raw SMILES strings, without the need to separate reactants from reagents in the input data, thereby simplifying data preparation. The BERT model achieved higher accuracy (98.2%) compared to the encoder-decoder model (95.2%) for classifying reactions. Accurate classification aids in understanding reaction mechanisms, vital for reaction design, optimization, and retrosynthesis.
      <cite class="ltx_cite ltx_citemacro_citet">
       Toniato et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib225" title="">
         225
        </a>
       </sup>
      </cite>
      also used a BERT architecture to classify reaction types for downstream retrosynthesis tasks that enable manufacture of any molecular target. Further examples of BERT use include unsupervised reaction atom-to-atom mapping
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p2.1.1.1">
        <a class="ltx_ref" href="#bib.bib226" title="">
         226
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib227" title="">
         227
        </a>
       </sup>
      </cite>
      . These chemical classifications would accelerate research and development in organic synthesis, described further below.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p3">
     <p class="ltx_p" id="S3.SS2.SSS2.p3.1">
      Due to limited labeled data for molecular property prediction,
      <cite class="ltx_cite ltx_citemacro_citeauthor">
       <a class="ltx_ref" href="#bib.bib155" title="">
        Wang et al.
       </a>
      </cite>
      proposed a semi-supervised SMILES-BERT model, pretrained on a large unlabeled dataset with a Masked SMILES Recovery task.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p3.1.1.1">
        <a class="ltx_ref" href="#bib.bib155" title="">
         155
        </a>
       </sup>
      </cite>
      The model was then fine-tuned for various molecular property prediction tasks, outperforming state-of-the-art methods in 2019 on three chosen datasets varying in size and property. This marked a shift from using BERT for reaction classification towards property prediction and drug discovery.
      <cite class="ltx_cite ltx_citemacro_citet">
       Maziarka et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib198" title="">
         198
        </a>
       </sup>
      </cite>
      also claimed state-of-the-art performance in property prediction after self-supervised pretraining in their Molecule Attention Transformer (MAT), which adapted BERT to chemical molecules by augmenting the self-attention with inter-atomic distances and molecular graph structure.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p4">
     <p class="ltx_p" id="S3.SS2.SSS2.p4.1">
      <cite class="ltx_cite ltx_citemacro_citet">
       Zhang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib228" title="">
         228
        </a>
       </sup>
      </cite>
      also tackled the issue of limited property-labeled data and the lack of correlation between any two datasets labeled for different properties, hindering generalizability. They introduced multitask learning BERT (MTL-BERT), which used large-scale pretraining and multitask learning with unlabeled SMILES strings from ChEMBL.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p4.1.1.1">
        <a class="ltx_ref" href="#bib.bib179" title="">
         179
        </a>
       </sup>
      </cite>
      This approach mined contextual information and extracted key patterns from complex SMILES strings, improving model interpretability. The model was fine-tuned for relevant downstream tasks, achieving better performance than state-of-the-art methods in 2022 on 60 molecular datasets from ADMETlab
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p4.1.2.1">
        <a class="ltx_ref" href="#bib.bib229" title="">
         229
        </a>
       </sup>
      </cite>
      and MoleculeNet.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p4.1.3.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p5">
     <p class="ltx_p" id="S3.SS2.SSS2.p5.1">
      In 2021,
      <cite class="ltx_cite ltx_citemacro_citeauthor">
       <a class="ltx_ref" href="#bib.bib152" title="">
        Li and Jiang
       </a>
      </cite>
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p5.1.1.1">
        <a class="ltx_ref" href="#bib.bib152" title="">
         152
        </a>
       </sup>
      </cite>
      introduced Mol-BERT, pretrained on four million unlabeled drug SMILES from the ZINC15
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p5.1.2.1">
        <a class="ltx_ref" href="#bib.bib230" title="">
         230
        </a>
       </sup>
      </cite>
      and ChEMBL27
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p5.1.3.1">
        <a class="ltx_ref" href="#bib.bib179" title="">
         179
        </a>
       </sup>
      </cite>
      databases to capture molecular substructure information for property prediction. Their work leveraged the underutilized potential of large unlabeled datasets like ZINC. Mol-BERT consisted of three components: PretrainingExtractor, Pretraining Mol-BERT, and Fine-Tuning Mol-BERT. It treated Morgan fingerprint fragments as “words” and compounds as “sentences,” using RDKit and the Morgan algorithm for canonicalization and substructure identification. This approach generated comprehensive molecular fingerprints from SMILES strings, used in a Masked Language Model (MLM) task for pretraining. Mol-BERT was fine-tuned on labeled samples, providing outputs as binary values or continuous scores for classification or regression, and it outperformed existing sequence and graph-based methods by at least 2% in ROC-AUC scores on Tox21, SIDER, and ClinTox datasets.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p5.1.4.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p6">
     <p class="ltx_p" id="S3.SS2.SSS2.p6.1">
      <cite class="ltx_cite ltx_citemacro_citet">
       Ross et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib151" title="">
         151
        </a>
       </sup>
      </cite>
      introduced MoLFormer, a large-scale unsupervised BERT model for more accurate and faster molecular property predictions than Density Functional Theory calculations or wet-lab experiments. They trained MoLFormer with rotary positional embeddings on SMILES sequences of 1.1 billion unlabeled molecules from PubChem
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p6.1.1.1">
        <a class="ltx_ref" href="#bib.bib177" title="">
         177
        </a>
       </sup>
      </cite>
      and ZINC
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p6.1.2.1">
        <a class="ltx_ref" href="#bib.bib230" title="">
         230
        </a>
       </sup>
      </cite>
      . Rotary positional encoding captures token positions more effectively than traditional methods,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p6.1.3.1">
        <a class="ltx_ref" href="#bib.bib71" title="">
         71
        </a>
       </sup>
      </cite>
      improving modeling of sequence relationships. MoLFormer outperformed state-of-the-art GNNs on several classification and regression tasks from ten MoleculeNet
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p6.1.4.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      datasets, while performing competitively on two others. It effectively learned spatial relationships between atoms, predicting various molecular properties, including quantum-chemical properties. Additionally, the authors stated how MoLFormer represents an efficient and environment-friendly use of computational resources, reducing GPU usage in training by a factor of 60 (16 GPUs instead of 1000).
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p7">
     <p class="ltx_p" id="S3.SS2.SSS2.p7.1">
      With ChemBERTa,
      <cite class="ltx_cite ltx_citemacro_citet">
       Chithrananda et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib44" title="">
         44
        </a>
       </sup>
      </cite>
      explored the impact of pretraining dataset size, tokenization strategy, and the use of SMILES or SELFIES, distinguishing their work from other BERT studies. They used HuggingFace’s RoBERTa transformer,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.1.1">
        <a class="ltx_ref" href="#bib.bib231" title="">
         231
        </a>
       </sup>
      </cite>
      and referenced a DeepChem
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.2.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      tutorial for accessibility. Their results showed improved performance on downstream tasks (BBBP, ClinTox, HIV, Tox21 from MoleculeNet
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.3.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      ) as the pretraining dataset size increased from 100K to 10M. Although ChemBERTa did not surpass state-of-the-art GNN-based baselines like Chemprop (which used 2048-bit Morgan Fingerprints from rdkit),
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.4.1">
        <a class="ltx_ref" href="#bib.bib232" title="">
         232
        </a>
       </sup>
      </cite>
      the authors suggested that with expansion to larger datasets they would eventually beat those baselines. The authors compared Byte-Pair Encoder (BPE) with a custom SmilesTokenizer and its regex developed by
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.5.1">
        <a class="ltx_ref" href="#bib.bib233" title="">
         233
        </a>
       </sup>
      </cite>
      while exploring tokenization strategies. They found the SmilesTokenizer slightly outperformed BPE, suggesting more relevant sub-word tokenization is beneficial. No difference was found between SMILES and SELFIES, but the paper highlighted how attention heads in transformers could be visualized with BertViz,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p7.1.6.1">
        <a class="ltx_ref" href="#bib.bib234" title="">
         234
        </a>
       </sup>
      </cite>
      showing certain neurons selective for functional groups. This study underscored the importance of appropriate benchmarking and addresses the carbon footprint of AI in molecular property prediction.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p8">
     <p class="ltx_p" id="S3.SS2.SSS2.p8.1">
      In ChemBERTa-2,
      <cite class="ltx_cite ltx_citemacro_citet">
       Ahmad et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib150" title="">
         150
        </a>
       </sup>
      </cite>
      aimed to create a foundational model applicable across various tasks. They addressed a criticism that LLMs were not so generalizable because the training data was biased or non-representative. They addressed this criticism by training on 77M samples and adding a Multi-Task Regression component to the pretraining. ChemBERTa-2 matched state-of-the-art architectures on MoleculeNet.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p8.1.1.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      As with ChemBERTa, the work was valuable because of additional exploration, in this case into how pretraining improvements affected certain downstream tasks more than others, depending on the type of fine-tuning task, the structural features of the molecules in the fine-tuning task data set, or the size of that fine-tuning dataset. The result was that pretraining the encoder-only model is important, but gains could be made by considering the chemical application itself, and the associated fine-tuning dataset.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p9">
     <p class="ltx_p" id="S3.SS2.SSS2.p9.1">
      In June 2023,
      <cite class="ltx_cite ltx_citemacro_citet">
       Yuksel et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib149" title="">
         149
        </a>
       </sup>
      </cite>
      introduced SELFormer, building on ideas from ChemBERTa2
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p9.1.1.1">
        <a class="ltx_ref" href="#bib.bib150" title="">
         150
        </a>
       </sup>
      </cite>
      and using SELFIES for large data input.
      <cite class="ltx_cite ltx_citemacro_citet">
       Yuksel et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib149" title="">
         149
        </a>
       </sup>
      </cite>
      argue that SMILES strings have validity and robustness issues, hindering effective chemical interpretation of the data, although this perspective is not universally held.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p9.1.2.1">
        <a class="ltx_ref" href="#bib.bib235" title="">
         235
        </a>
       </sup>
      </cite>
      SELFormer uses SELFIES and is pretrained on two million drug-like compounds, fine-tuned for diverse molecular property prediction tasks (BBBP, SIDER, Tox21, HIV, BACE, FreeSolv, ESOL, PDBbind from MoleculeNet).
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p9.1.3.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      SELFormer outperformed all competing methods for some tasks and produced comparable results for the rest. It could also discriminate molecules with different structural properties. The paper suggests future directions in multimodal models combining structural data with other types of molecular information, including text-based annotations. We will discuss such multimodal models below.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p10">
     <p class="ltx_p" id="S3.SS2.SSS2.p10.1">
      Also in 2023,
      <cite class="ltx_cite ltx_citemacro_citet">
       Yu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib194" title="">
         194
        </a>
       </sup>
      </cite>
      published SolvBERT, a multi-task BERT-based regression model that could predict both solvation free energy and solubility from the SMILES notations of solute-solvent complexes. It was trained on the CombiSolv-QM dataset,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p10.1.1.1">
        <a class="ltx_ref" href="#bib.bib236" title="">
         236
        </a>
       </sup>
      </cite>
      a curation of experimental solvent free energy data called CombiSolv-Exp-8780,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p10.1.2.1">
        <a class="ltx_ref" href="#bib.bib237" title="">
         237
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib238" title="">
         238
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib239" title="">
         239
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib240" title="">
         240
        </a>
       </sup>
      </cite>
      and the solubility dataset from
      <cite class="ltx_cite ltx_citemacro_citet">
       Boobier et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib195" title="">
         195
        </a>
       </sup>
      </cite>
      . SolvBERT’s performance was benchmarked against advanced graph-based models
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p10.1.3.1">
        <a class="ltx_ref" href="#bib.bib241" title="">
         241
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib242" title="">
         242
        </a>
       </sup>
      </cite>
      This work is powerful because there is an expectation that solvation free energy depends on 3-dimensional conformational properties of the molecules, or at least 2D properties that would be well characterized by graph-based molecular representations. It shows an overachieving utility of using SMILES strings in property prediction, and aligns with other work by
      <cite class="ltx_cite ltx_citemacro_citet">
       Winter et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib243" title="">
         243
        </a>
       </sup>
      </cite>
      , regarding activity coefficients. SolvBERT showed comparable performance to DMPNN in predicting solvation free energy, largely due to its effective clustering feature in the pretraining phase as shown by TMAP visualizations. T Furthermore, SolvBERT outperformed GROVER in predicting experimentally evaluated solubility data for new solute-solvent combinations. This underscores the significance of SolvBERT’s ability to capture the dynamic and spatial complexities of solvation interactions in a text-based model.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p11">
     <p class="ltx_p" id="S3.SS2.SSS2.p11.1">
      Despite significant progress,
      <cite class="ltx_cite ltx_citemacro_citet">
       Jiang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib244" title="">
         244
        </a>
       </sup>
      </cite>
      highlighted the limitations of labeled data in 2024 and introduced INTransformer,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS2.SSS2.p11.1.1.1">
        <a class="ltx_ref" href="#bib.bib244" title="">
         244
        </a>
       </sup>
      </cite>
      a transformer-based method for predicting molecule properties. INTransformer enhances the capture of global molecular information by adding perturbing noise and employing contrastive learning to artificially augment smaller datasets. This method achieved higher performance, and ongoing work continues to explore various transformer strategies for smaller datasets.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Property Directed Inverse Design and Decoder-only mol-LLMs
   </h3>
   <figure class="ltx_table" id="S3.T2">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Decoder-only scientific LLMs.
     <math alttext="a" class="ltx_Math" display="inline" id="S3.T2.1.m1.1">
      <semantics id="S3.T2.1.m1.1b">
       <mi id="S3.T2.1.m1.1.1" xref="S3.T2.1.m1.1.1.cmml">
        a
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T2.1.m1.1c">
        <ci id="S3.T2.1.m1.1.1.cmml" xref="S3.T2.1.m1.1.1">
         𝑎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T2.1.m1.1d">
        a
       </annotation>
      </semantics>
     </math>
     :“Model Size” is reported as the number of parameters. “PubMed” refer to the PubMed abstracts dataset, while PMC (PubMed Corpus) refers to the full-text corpus dataset.
     <math alttext="b" class="ltx_Math" display="inline" id="S3.T2.2.m2.1">
      <semantics id="S3.T2.2.m2.1b">
       <mi id="S3.T2.2.m2.1.1" xref="S3.T2.2.m2.1.1.cmml">
        b
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T2.2.m2.1c">
        <ci id="S3.T2.2.m2.1.1.cmml" xref="S3.T2.2.m2.1.1">
         𝑏
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T2.2.m2.1d">
        b
       </annotation>
      </semantics>
     </math>
     : The total number of parameters was not reported.
    </figcaption>
    <table class="ltx_tabular" id="S3.T2.11">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S3.T2.5.1">
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S3.T2.5.1.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.2.1">
         <span class="ltx_p" id="S3.T2.5.1.2.1.1">
          LLM
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T2.5.1.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.1.1">
         <span class="ltx_p" id="S3.T2.5.1.1.1.1">
          Model Size
          <sup class="ltx_sup" id="S3.T2.5.1.1.1.1.1">
           <span class="ltx_text ltx_font_italic" id="S3.T2.5.1.1.1.1.1.1">
            a
           </span>
          </sup>
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T2.5.1.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.3.1">
         <span class="ltx_p" id="S3.T2.5.1.3.1.1">
          Training Data
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T2.5.1.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.4.1">
         <span class="ltx_p" id="S3.T2.5.1.4.1.1">
          Architecture
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T2.5.1.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.5.1">
         <span class="ltx_p" id="S3.T2.5.1.5.1.1">
          Application
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S3.T2.5.1.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.5.1.6.1">
         <span class="ltx_p" id="S3.T2.5.1.6.1.1">
          Release date
         </span>
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S3.T2.6.2">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T2.6.2.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.2.1">
         <span class="ltx_p" id="S3.T2.6.2.2.1.1">
          Tx-LLM
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.6.2.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib245" title="">
             245
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.6.2.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.1.1">
         <span class="ltx_p" id="S3.T2.6.2.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T2.6.2.1.1.1.m1.1">
           <semantics id="S3.T2.6.2.1.1.1.m1.1a">
            <mi id="S3.T2.6.2.1.1.1.m1.1.1" xref="S3.T2.6.2.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T2.6.2.1.1.1.m1.1b">
             <ci id="S3.T2.6.2.1.1.1.m1.1.1.cmml" xref="S3.T2.6.2.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.6.2.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.6.2.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.3.1">
         <span class="ltx_p" id="S3.T2.6.2.3.1.1">
          TDC datasets
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.6.2.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.4.1">
         <span class="ltx_p" id="S3.T2.6.2.4.1.1">
          PaLM-2
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.6.2.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.5.1">
         <span class="ltx_p" id="S3.T2.6.2.5.1.1">
          Property prediction and retrosynthesis
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T2.6.2.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.6.2.6.1">
         <span class="ltx_p" id="S3.T2.6.2.6.1.1">
          2024.06
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.7.3">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.7.3.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.2.1">
         <span class="ltx_p" id="S3.T2.7.3.2.1.1">
          LlasMol
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.7.3.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib246" title="">
             246
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.7.3.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.1.1">
         <span class="ltx_p" id="S3.T2.7.3.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T2.7.3.1.1.1.m1.1">
           <semantics id="S3.T2.7.3.1.1.1.m1.1a">
            <mo id="S3.T2.7.3.1.1.1.m1.1.1" xref="S3.T2.7.3.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T2.7.3.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T2.7.3.1.1.1.m1.1.1.cmml" xref="S3.T2.7.3.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.7.3.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.7.3.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.3.1">
         <span class="ltx_p" id="S3.T2.7.3.3.1.1">
          SMolInstruct
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.7.3.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.4.1">
         <span class="ltx_p" id="S3.T2.7.3.4.1.1">
          Galactica, LLaMa, Mistral
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.7.3.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.5.1">
         <span class="ltx_p" id="S3.T2.7.3.5.1.1">
          Property prediction, molecule captioning, molecule generation, retrosynthesis, name conversion
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.7.3.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.7.3.6.1">
         <span class="ltx_p" id="S3.T2.7.3.6.1.1">
          2024.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.8.1">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.8.1.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.1.1">
         <span class="ltx_p" id="S3.T2.11.8.1.1.1.1">
          BioMedLM
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.8.1.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib118" title="">
             118
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.8.1.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.2.1">
         <span class="ltx_p" id="S3.T2.11.8.1.2.1.1">
          2.7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.8.1.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.3.1">
         <span class="ltx_p" id="S3.T2.11.8.1.3.1.1">
          PubMed abstracts and full articles
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.8.1.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.4.1">
         <span class="ltx_p" id="S3.T2.11.8.1.4.1.1">
          GPT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.8.1.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.5.1">
         <span class="ltx_p" id="S3.T2.11.8.1.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.8.1.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.8.1.6.1">
         <span class="ltx_p" id="S3.T2.11.8.1.6.1.1">
          2024.03
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.9.2">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.9.2.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.1.1">
         <span class="ltx_p" id="S3.T2.11.9.2.1.1.1">
          BioMistral
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.9.2.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib119" title="">
             119
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.9.2.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.2.1">
         <span class="ltx_p" id="S3.T2.11.9.2.2.1.1">
          7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.9.2.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.3.1">
         <span class="ltx_p" id="S3.T2.11.9.2.3.1.1">
          PubMed Central (PMC)
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.9.2.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.4.1">
         <span class="ltx_p" id="S3.T2.11.9.2.4.1.1">
          Mistral
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.9.2.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.5.1">
         <span class="ltx_p" id="S3.T2.11.9.2.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.9.2.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.9.2.6.1">
         <span class="ltx_p" id="S3.T2.11.9.2.6.1.1">
          2024.02
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.10.3">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.10.3.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.1.1">
         <span class="ltx_p" id="S3.T2.11.10.3.1.1.1">
          BiMediX
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.10.3.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib120" title="">
             120
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.10.3.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.2.1">
         <span class="ltx_p" id="S3.T2.11.10.3.2.1.1">
          8x7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.10.3.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.3.1">
         <span class="ltx_p" id="S3.T2.11.10.3.3.1.1">
          1.3M Arabic-English instructions (BiMed)
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.10.3.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.4.1">
         <span class="ltx_p" id="S3.T2.11.10.3.4.1.1">
          Mixtral
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.10.3.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.5.1">
         <span class="ltx_p" id="S3.T2.11.10.3.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.10.3.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.10.3.6.1">
         <span class="ltx_p" id="S3.T2.11.10.3.6.1.1">
          2024.02
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.11.4">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.11.4.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.1.1">
         <span class="ltx_p" id="S3.T2.11.11.4.1.1.1">
          EpilepsyLLM
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.11.4.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib121" title="">
             121
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.11.4.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.2.1">
         <span class="ltx_p" id="S3.T2.11.11.4.2.1.1">
          7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.11.4.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.3.1">
         <span class="ltx_p" id="S3.T2.11.11.4.3.1.1">
          Data from the Japan Epilepsy Association, Epilepsy Information Center, and Tenkan Net
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.11.4.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.4.1">
         <span class="ltx_p" id="S3.T2.11.11.4.4.1.1">
          LLaMa
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.11.4.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.5.1">
         <span class="ltx_p" id="S3.T2.11.11.4.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.11.4.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.11.4.6.1">
         <span class="ltx_p" id="S3.T2.11.11.4.6.1.1">
          2024.01
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.8.4">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.8.4.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.2.1">
         <span class="ltx_p" id="S3.T2.8.4.2.1.1">
          ChemSpaceAL
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.8.4.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib247" title="">
             247
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.8.4.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.1.1">
         <span class="ltx_p" id="S3.T2.8.4.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T2.8.4.1.1.1.m1.1">
           <semantics id="S3.T2.8.4.1.1.1.m1.1a">
            <mi id="S3.T2.8.4.1.1.1.m1.1.1" xref="S3.T2.8.4.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T2.8.4.1.1.1.m1.1b">
             <ci id="S3.T2.8.4.1.1.1.m1.1.1.cmml" xref="S3.T2.8.4.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.8.4.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.8.4.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.3.1">
         <span class="ltx_p" id="S3.T2.8.4.3.1.1">
          ChEMBL 33, GuacaMol v1, MOSES, and BindingDB 08-2023
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.8.4.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.4.1">
         <span class="ltx_p" id="S3.T2.8.4.4.1.1">
          GPT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.8.4.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.5.1">
         <span class="ltx_p" id="S3.T2.8.4.5.1.1">
          Molecule Generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.8.4.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.8.4.6.1">
         <span class="ltx_p" id="S3.T2.8.4.6.1.1">
          2024.01
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.12.5">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.12.5.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.1.1">
         <span class="ltx_p" id="S3.T2.11.12.5.1.1.1">
          CheXagent
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.12.5.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib122" title="">
             122
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.12.5.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.2.1">
         <span class="ltx_p" id="S3.T2.11.12.5.2.1.1">
          7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.12.5.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.3.1">
         <span class="ltx_p" id="S3.T2.11.12.5.3.1.1">
          28 publicly available datasets, including PMC, MIMIC, wikipedia, PadChest, and BIMCV-COVID-19
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.12.5.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.4.1">
         <span class="ltx_p" id="S3.T2.11.12.5.4.1.1">
          Mistral
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.12.5.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.5.1">
         <span class="ltx_p" id="S3.T2.11.12.5.5.1.1">
          QA, Image understanding
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.12.5.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.12.5.6.1">
         <span class="ltx_p" id="S3.T2.11.12.5.6.1.1">
          2024.01
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.9.5">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.9.5.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.2.1">
         <span class="ltx_p" id="S3.T2.9.5.2.1.1">
          ChemGPT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.9.5.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib248" title="">
             248
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.9.5.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.1.1">
         <span class="ltx_p" id="S3.T2.9.5.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T2.9.5.1.1.1.m1.1">
           <semantics id="S3.T2.9.5.1.1.1.m1.1a">
            <mo id="S3.T2.9.5.1.1.1.m1.1.1" xref="S3.T2.9.5.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T2.9.5.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T2.9.5.1.1.1.m1.1.1.cmml" xref="S3.T2.9.5.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.9.5.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          1B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.9.5.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.3.1">
         <span class="ltx_p" id="S3.T2.9.5.3.1.1">
          10M molecules from PubChem
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.9.5.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.4.1">
         <span class="ltx_p" id="S3.T2.9.5.4.1.1">
          GPT-neo
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.9.5.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.5.1">
         <span class="ltx_p" id="S3.T2.9.5.5.1.1">
          Molecule generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.9.5.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.9.5.6.1">
         <span class="ltx_p" id="S3.T2.9.5.6.1.1">
          2023.11
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.13.6">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.13.6.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.1.1">
         <span class="ltx_p" id="S3.T2.11.13.6.1.1.1">
          BioMedGPT-LM
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.13.6.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib123" title="">
             123
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.13.6.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.2.1">
         <span class="ltx_p" id="S3.T2.11.13.6.2.1.1">
          7B and 10B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.13.6.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.3.1">
         <span class="ltx_p" id="S3.T2.11.13.6.3.1.1">
          5.5M biomedical papers from S2ORC
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.13.6.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.4.1">
         <span class="ltx_p" id="S3.T2.11.13.6.4.1.1">
          LLaMA2
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.13.6.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.5.1">
         <span class="ltx_p" id="S3.T2.11.13.6.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.13.6.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.13.6.6.1">
         <span class="ltx_p" id="S3.T2.11.13.6.6.1.1">
          2023.08
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.14.7">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.14.7.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.1.1">
         <span class="ltx_p" id="S3.T2.11.14.7.1.1.1">
          Darwin
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.14.7.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib124" title="">
             124
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.14.7.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.2.1">
         <span class="ltx_p" id="S3.T2.11.14.7.2.1.1">
          7B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.14.7.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.3.1">
         <span class="ltx_p" id="S3.T2.11.14.7.3.1.1">
          SciQ and Web of Science
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.14.7.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.4.1">
         <span class="ltx_p" id="S3.T2.11.14.7.4.1.1">
          LLaMA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.14.7.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.5.1">
         <span class="ltx_p" id="S3.T2.11.14.7.5.1.1">
          QA, Property prediction, NER, and Molecule Generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.14.7.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.14.7.6.1">
         <span class="ltx_p" id="S3.T2.11.14.7.6.1.1">
          2023.08
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.10.6">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.10.6.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.2.1">
         <span class="ltx_p" id="S3.T2.10.6.2.1.1">
          cMolGPT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.10.6.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib46" title="">
             46
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.10.6.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.1.1">
         <span class="ltx_p" id="S3.T2.10.6.1.1.1">
          <math alttext="b" class="ltx_Math" display="inline" id="S3.T2.10.6.1.1.1.m1.1">
           <semantics id="S3.T2.10.6.1.1.1.m1.1a">
            <mi id="S3.T2.10.6.1.1.1.m1.1.1" xref="S3.T2.10.6.1.1.1.m1.1.1.cmml">
             b
            </mi>
            <annotation-xml encoding="MathML-Content" id="S3.T2.10.6.1.1.1.m1.1b">
             <ci id="S3.T2.10.6.1.1.1.m1.1.1.cmml" xref="S3.T2.10.6.1.1.1.m1.1.1">
              𝑏
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.10.6.1.1.1.m1.1c">
             b
            </annotation>
           </semantics>
          </math>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.10.6.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.3.1">
         <span class="ltx_p" id="S3.T2.10.6.3.1.1">
          MOSES
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.10.6.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.4.1">
         <span class="ltx_p" id="S3.T2.10.6.4.1.1">
          GPT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.10.6.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.5.1">
         <span class="ltx_p" id="S3.T2.10.6.5.1.1">
          Molecule Generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.10.6.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.10.6.6.1">
         <span class="ltx_p" id="S3.T2.10.6.6.1.1">
          2023.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.15.8">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.15.8.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.1.1">
         <span class="ltx_p" id="S3.T2.11.15.8.1.1.1">
          PMC-LLaMA
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.15.8.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib125" title="">
             125
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.15.8.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.2.1">
         <span class="ltx_p" id="S3.T2.11.15.8.2.1.1">
          7B and 13B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.15.8.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.3.1">
         <span class="ltx_p" id="S3.T2.11.15.8.3.1.1">
          MedC-k and MedC-I
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.15.8.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.4.1">
         <span class="ltx_p" id="S3.T2.11.15.8.4.1.1">
          LLaMA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.15.8.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.5.1">
         <span class="ltx_p" id="S3.T2.11.15.8.5.1.1">
          QA
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.15.8.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.15.8.6.1">
         <span class="ltx_p" id="S3.T2.11.15.8.6.1.1">
          2023.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.7">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.7.2" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.2.1">
         <span class="ltx_p" id="S3.T2.11.7.2.1.1">
          Regression Transformer
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.7.2.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib145" title="">
             145
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.7.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.1.1">
         <span class="ltx_p" id="S3.T2.11.7.1.1.1">
          <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T2.11.7.1.1.1.m1.1">
           <semantics id="S3.T2.11.7.1.1.1.m1.1a">
            <mo id="S3.T2.11.7.1.1.1.m1.1.1" xref="S3.T2.11.7.1.1.1.m1.1.1.cmml">
             ∼
            </mo>
            <annotation-xml encoding="MathML-Content" id="S3.T2.11.7.1.1.1.m1.1b">
             <csymbol cd="latexml" id="S3.T2.11.7.1.1.1.m1.1.1.cmml" xref="S3.T2.11.7.1.1.1.m1.1.1">
              similar-to
             </csymbol>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S3.T2.11.7.1.1.1.m1.1c">
             \sim
            </annotation>
           </semantics>
          </math>
          27M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.7.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.3.1">
         <span class="ltx_p" id="S3.T2.11.7.3.1.1">
          ChEMBL, MoleculeNet, USPTO, etc
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.7.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.4.1">
         <span class="ltx_p" id="S3.T2.11.7.4.1.1">
          XLNet
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.7.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.5.1">
         <span class="ltx_p" id="S3.T2.11.7.5.1.1">
          Property prediction, Molecule tuning, Molecule generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.7.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.7.6.1">
         <span class="ltx_p" id="S3.T2.11.7.6.1.1">
          2023.04
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.16.9">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.16.9.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.1.1">
         <span class="ltx_p" id="S3.T2.11.16.9.1.1.1">
          GPTChem
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.16.9.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib249" title="">
             249
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.16.9.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.2.1">
         <span class="ltx_p" id="S3.T2.11.16.9.2.1.1">
          175B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.16.9.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.3.1">
         <span class="ltx_p" id="S3.T2.11.16.9.3.1.1">
          Curation of multiple classification and regression benchmarks
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.16.9.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.4.1">
         <span class="ltx_p" id="S3.T2.11.16.9.4.1.1">
          GPT-3
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.16.9.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.5.1">
         <span class="ltx_p" id="S3.T2.11.16.9.5.1.1">
          Property prediction and inverse design
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.16.9.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.16.9.6.1">
         <span class="ltx_p" id="S3.T2.11.16.9.6.1.1">
          2023.02
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.17.10">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.17.10.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.1.1">
         <span class="ltx_p" id="S3.T2.11.17.10.1.1.1">
          Galactica
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.17.10.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib126" title="">
             126
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.17.10.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.2.1">
         <span class="ltx_p" id="S3.T2.11.17.10.2.1.1">
          125M, 1.3B, 6.7B, 30B, 120B
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.17.10.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.3.1">
         <span class="ltx_p" id="S3.T2.11.17.10.3.1.1">
          The galactica corpus, a curation with  62B scientific documents
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.17.10.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.4.1">
         <span class="ltx_p" id="S3.T2.11.17.10.4.1.1">
          Decoder-only
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.17.10.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.5.1">
         <span class="ltx_p" id="S3.T2.11.17.10.5.1.1">
          QA, NER, Document Summarization, Property Prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.17.10.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.17.10.6.1">
         <span class="ltx_p" id="S3.T2.11.17.10.6.1.1">
          2022.11
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.18.11">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.18.11.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.1.1">
         <span class="ltx_p" id="S3.T2.11.18.11.1.1.1">
          BioGPT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.18.11.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib127" title="">
             127
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.18.11.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.2.1">
         <span class="ltx_p" id="S3.T2.11.18.11.2.1.1">
          355M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.18.11.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.3.1">
         <span class="ltx_p" id="S3.T2.11.18.11.3.1.1">
          15M of Title and abstract from PubMed
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.18.11.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.4.1">
         <span class="ltx_p" id="S3.T2.11.18.11.4.1.1">
          GPT-2
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.18.11.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.5.1">
         <span class="ltx_p" id="S3.T2.11.18.11.5.1.1">
          QA, NER, and Document Classification
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.18.11.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.18.11.6.1">
         <span class="ltx_p" id="S3.T2.11.18.11.6.1.1">
          2022-09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.19.12">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.19.12.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.1.1">
         <span class="ltx_p" id="S3.T2.11.19.12.1.1.1">
          SPT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.19.12.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib243" title="">
             243
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.19.12.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.2.1">
         <span class="ltx_p" id="S3.T2.11.19.12.2.1.1">
          6.5M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.19.12.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.3.1">
         <span class="ltx_p" id="S3.T2.11.19.12.3.1.1">
          Synthetic data generated with the thermodynamic model COSMO-RS
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.19.12.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.4.1">
         <span class="ltx_p" id="S3.T2.11.19.12.4.1.1">
          GPT-3
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.19.12.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.5.1">
         <span class="ltx_p" id="S3.T2.11.19.12.5.1.1">
          Property prediction
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.19.12.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.19.12.6.1">
         <span class="ltx_p" id="S3.T2.11.19.12.6.1.1">
          2022.09
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.20.13">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T2.11.20.13.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.1.1">
         <span class="ltx_p" id="S3.T2.11.20.13.1.1.1">
          MolGPT
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.20.13.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib250" title="">
             250
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.20.13.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.2.1">
         <span class="ltx_p" id="S3.T2.11.20.13.2.1.1">
          6M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.20.13.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.3.1">
         <span class="ltx_p" id="S3.T2.11.20.13.3.1.1">
          MOSES and GuacaMol
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.20.13.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.4.1">
         <span class="ltx_p" id="S3.T2.11.20.13.4.1.1">
          GPT
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.20.13.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.5.1">
         <span class="ltx_p" id="S3.T2.11.20.13.5.1.1">
          Molecule Generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T2.11.20.13.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.20.13.6.1">
         <span class="ltx_p" id="S3.T2.11.20.13.6.1.1">
          2021.10
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T2.11.21.14">
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S3.T2.11.21.14.1" style="width:71.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.1.1">
         <span class="ltx_p" id="S3.T2.11.21.14.1.1.1">
          Adilov2021
          <cite class="ltx_cite ltx_citemacro_cite">
           <sup class="ltx_sup" id="S3.T2.11.21.14.1.1.1.1.1">
            <a class="ltx_ref" href="#bib.bib251" title="">
             251
            </a>
           </sup>
          </cite>
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T2.11.21.14.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.2.1">
         <span class="ltx_p" id="S3.T2.11.21.14.2.1.1">
          13.4M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T2.11.21.14.3" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.3.1">
         <span class="ltx_p" id="S3.T2.11.21.14.3.1.1">
          5M SMILES from ChemBERTa’s PubChem-10M.
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T2.11.21.14.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.4.1">
         <span class="ltx_p" id="S3.T2.11.21.14.4.1.1">
          GPT-2
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T2.11.21.14.5" style="width:108.1pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.5.1">
         <span class="ltx_p" id="S3.T2.11.21.14.5.1.1">
          Property prediction and molecule generation
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S3.T2.11.21.14.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
        <span class="ltx_inline-block ltx_align_top" id="S3.T2.11.21.14.6.1">
         <span class="ltx_p" id="S3.T2.11.21.14.6.1.1">
          2021.09
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     Decoder-only GPT-like architectures offer significant value for property directed molecule generation and
     <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">
      de novo
     </span>
     chemistry applications because they excel in generating novel molecular structures by learning from vast datasets of chemical compounds, and capture intricate patterns and relationships within molecular sequences. They thus propose viable new compounds that adhere to desired chemical properties and constraints, enabling rapid exploration and innovation within an almost infinite chemical space. In addition, such large general-purpose models have been proven to enable specific applications by fine-tuning with a small amount of scientific data.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS3.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib249" title="">
        249
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib252" title="">
        252
       </a>
      </sup>
     </cite>
     We first describe property-directed inverse design from a chemistry perspective and then provide examples of how decoder-only LLMs have propelled inverse design forward.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.3.1
     </span>
     Property Directed Inverse Design
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
     <p class="ltx_p" id="S3.SS3.SSS1.p1.1">
      Nature has been a significant source of molecules that inhibit disease proliferation, as organisms have evolved chemicals to protect themselves. This has led to most pharmaceuticals being derived from natural products,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib253" title="">
         253
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib254" title="">
         254
        </a>
       </sup>
      </cite>
      which offer advantages such as cell permeability, target specificity, and a vast chemical diversity.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib255" title="">
         255
        </a>
       </sup>
      </cite>
      However, despite these benefits, there are high costs and complexities associated with high-throughput screening and synthesis of natural products.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib255" title="">
         255
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib253" title="">
         253
        </a>
       </sup>
      </cite>
      While building on the chemical diversity and efficacy of natural products, AI also opens new avenues for synthesizing unique compounds efficiently. Advanced
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.1.4">
       in-silico
      </span>
      molecular design allows for rapid mutation
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.5.1">
        <a class="ltx_ref" href="#bib.bib256" title="">
         256
        </a>
       </sup>
      </cite>
      towards valid
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.1.6">
       de-novo
      </span>
      molecular structures that are synthesizable,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.7.1">
        <a class="ltx_ref" href="#bib.bib222" title="">
         222
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib257" title="">
         257
        </a>
       </sup>
      </cite>
      streamlining the iterative process of drug development. Yet, the true innovation lies in the potential of LLM-driven target-directed molecular design or “Inverse Design,” where we can start with a desired property and directly generate molecules that manifest this attribute, bypassing traditional stepwise modifications.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS1.p1.1.8.1">
        <a class="ltx_ref" href="#bib.bib258" title="">
         258
        </a>
       </sup>
      </cite>
      This capability dramatically accelerates the pathway from concept to viable therapeutic agents and aligns well with decoder-only LLM architectures.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.3.2
     </span>
     Decoder-only mol-LLMs
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
     <p class="ltx_p" id="S3.SS3.SSS2.p1.1">
      In 2021,
      <cite class="ltx_cite ltx_citemacro_citet">
       Adilov
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib251" title="">
         251
        </a>
       </sup>
      </cite>
      presented “Generative pretraining from Molecules,” one of the first applications of decoder-only models to SMILES strings. It pretrained a GPT-2-like causal transformer for self-supervised learning, introducing “adapters” between attention blocks for task-specific fine-tuning.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib259" title="">
         259
        </a>
       </sup>
      </cite>
      This method, requiring minimal architectural changes, offered versatility in molecule generation and property prediction, aiming to surpass ChemBERTa’s encoder-only performance with a more scalable and resource-efficient approach. Another early decoder-only SMILES-based model was Bagal et al’s MolGPT model,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib250" title="">
         250
        </a>
       </sup>
      </cite>
      . MolGPT, with a mere 6 million parameters, advanced GPT-type LLMs for molecular generation. Its decoder-only architecture with masked self-attention facilitated learning long-range dependencies, enabling chemically valid SMILES representations that met complex structural rules involving valency and ring closures. The paper also used salience measures for interpretability in predicting SMILES tokens. MolGPT outperformed many existing Variational Auto-Encoder (VAE) based approaches
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib260" title="">
         260
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib261" title="">
         261
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib262" title="">
         262
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib263" title="">
         263
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib264" title="">
         264
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib265" title="">
         265
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib266" title="">
         266
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib267" title="">
         267
        </a>
       </sup>
      </cite>
      in predicting novel molecules with specified properties, being trained on datasets like MOSES
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.4.1">
        <a class="ltx_ref" href="#bib.bib268" title="">
         268
        </a>
       </sup>
      </cite>
      and GuacaMol.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.5.1">
        <a class="ltx_ref" href="#bib.bib269" title="">
         269
        </a>
       </sup>
      </cite>
      It showed good performance with metrics like validity, uniqueness, Frechet ChemNet Distance (FCD),
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.6.1">
        <a class="ltx_ref" href="#bib.bib270" title="">
         270
        </a>
       </sup>
      </cite>
      and KL divergence,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p1.1.7.1">
        <a class="ltx_ref" href="#bib.bib269" title="">
         269
        </a>
       </sup>
      </cite>
      . While MolGPT’s computational demands might be higher than traditional VAEs, its ability to generate high-quality, novel molecules justifies this trade-off, but future research could likely optimize model efficiency or explore lighter versions. A brief summary of advancements in transformer-based models for
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.8">
       de-novo
      </span>
      molecule generation from 2023 and 2024 follows.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
     <p class="ltx_p" id="S3.SS3.SSS2.p2.1">
      <cite class="ltx_cite ltx_citemacro_citet">
       Haroon et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib271" title="">
         271
        </a>
       </sup>
      </cite>
      further developed a GPT-based model with relative attention for
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">
       de novo
      </span>
      drug design, showing improved validity, uniqueness, and novelty.
      <cite class="ltx_cite ltx_citemacro_citet">
       Frey et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib248" title="">
         248
        </a>
       </sup>
      </cite>
      introduced ChemGPT and explored the challenges of hyperparameter tuning at scale in new domains, along with a consideration of how the scale of the pretraining dataset will influence neural architecture selection, relative to ”typical" chemical generative models. Both
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib272" title="">
         272
        </a>
       </sup>
      </cite>
      and
      <cite class="ltx_cite ltx_citemacro_citet">
       Mao et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib273" title="">
         273
        </a>
       </sup>
      </cite>
      presented work that surpassed MolGPT, with Mao et al.’s T5-type model
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p2.1.2.1">
        <a class="ltx_ref" href="#bib.bib274" title="">
         274
        </a>
       </sup>
      </cite>
      generating novel compounds using IUPAC names directly.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p2.1.3.1">
        <a class="ltx_ref" href="#bib.bib275" title="">
         275
        </a>
       </sup>
      </cite>
      Although T5-based, we include this here because of its relevance to
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.4">
       de novo
      </span>
      drug design.
In a similar vein,
      <cite class="ltx_cite ltx_citemacro_citet">
       Zhang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib276" title="">
         276
        </a>
       </sup>
      </cite>
      proposed including target 3D structural information in molecular generative models, even though their approach is not LLM-based. However, we nonetheless note its value for future structure-based LLM drug design. They demonstrated that integrating additional biological data significantly enhances the relevance and specificity of generated molecules for targeted drug discovery.
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib277" title="">
         277
        </a>
       </sup>
      </cite>
      discussed PETrans, a deep learning method for generating target-specific ligands using protein-specific encoding and transfer learning. This study emphasized the application of transformer models for generating molecules with a high binding affinity to specific protein targets.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p3">
     <p class="ltx_p" id="S3.SS3.SSS2.p3.1">
      In 2024,
      <cite class="ltx_cite ltx_citemacro_citet">
       Yoshikai et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib278" title="">
         278
        </a>
       </sup>
      </cite>
      discussed the limitations of transformer architectures in recognizing chirality from SMILES representations, highlighting the challenges in learning overall molecular structures with, in particular, chirality impacting the prediction accuracy of molecular properties. They coupled a transformer with a VAE to address this.
      <cite class="ltx_cite ltx_citemacro_citet">
       Qian et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib279" title="">
         279
        </a>
       </sup>
      </cite>
      introduced CONSMI, motivated by contrastive learning in NLP, to generate new molecules using multiple SMILES representations, improving molecular novelty and validity.
      <cite class="ltx_cite ltx_citemacro_citet">
       Kyro et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib247" title="">
         247
        </a>
       </sup>
      </cite>
      presented ChemSpaceAL, an active learning method for protein-specific molecular generation, efficiently discovering molecules with desired characteristics without prior knowledge of existing inhibitors.
      <cite class="ltx_cite ltx_citemacro_citet">
       Yan et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib280" title="">
         280
        </a>
       </sup>
      </cite>
      proposed the GMIA framework, featuring a graph mutual interaction attention decoder for drug-drug interaction prediction, enhancing prediction accuracy and interpretability. Lastly,
      <cite class="ltx_cite ltx_citemacro_citet">
       Shen et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib281" title="">
         281
        </a>
       </sup>
      </cite>
      reported on AutoMolDesigner, an AI-based open-source software for automated design of small-molecule antibiotics.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p4">
     <p class="ltx_p" id="S3.SS3.SSS2.p4.1">
      For a deeper dive into decoder-only transformer architecture in chemistry, we highlight the May 2023 “Taiga” model by
      <cite class="ltx_cite ltx_citemacro_citet">
       Mazuz et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib92" title="">
         92
        </a>
       </sup>
      </cite>
      , and cMolGPT by
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib46" title="">
         46
        </a>
       </sup>
      </cite>
      . Taiga first learns to map SMILES strings to a vector space, and then refines that space using a smaller dataset of labeled molecules to produce molecules with targeted attributes. It employs an autoregressive mechanism, sequentially predicting each SMILES character based on the previous ones. For optimizing molecular properties, Taiga uses the REINFORCE algorithm
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.1.1">
        <a class="ltx_ref" href="#bib.bib97" title="">
         97
        </a>
       </sup>
      </cite>
      , which develops a strategy to enhance desired molecular features. Although this fine-tuning with reinforcement learning (RL) slightly reduces molecule validity, it significantly boosts their practical applicability. Initially evaluated using the Quantitative Estimate of Drug-likeness (QED) metric,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.2.1">
        <a class="ltx_ref" href="#bib.bib282" title="">
         282
        </a>
       </sup>
      </cite>
      Taiga has also demonstrated promising results in targeting IC50 values,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.3.1">
        <a class="ltx_ref" href="#bib.bib179" title="">
         179
        </a>
       </sup>
      </cite>
      the BACE protein,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.4.1">
        <a class="ltx_ref" href="#bib.bib283" title="">
         283
        </a>
       </sup>
      </cite>
      and anti-cancer activities they collected from a variety of sources. This work underscores the importance of using new models to address applications that require a higher level of chemical sophistication, to illustrate how such models could ultimately be applied outside of the available benchmark datasets. It also builds on the necessary use of standardized datasets and train-validation-test splitting, to demonstrate progress, as explained by
      <cite class="ltx_cite ltx_citemacro_citet">
       Wu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      . Yet, even the MoleculeNet benchmarks
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.5.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      are flawed, and we point the reader here to a more detailed discussion on benchmarking,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p4.1.6.1">
        <a class="ltx_ref" href="#bib.bib186" title="">
         186
        </a>
       </sup>
      </cite>
      given that a significant portion of molecules in the BACE dataset have undefined stereo centers, which, at a deeper level, complicates the modeling and prediction accuracy.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p5">
     <p class="ltx_p" id="S3.SS3.SSS2.p5.1">
      <cite class="ltx_cite ltx_citemacro_citet">
       Wang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib46" title="">
         46
        </a>
       </sup>
      </cite>
      introduced “cMolGPT: A Conditional Generative Pre-Trained Transformer for Target-Specific
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p5.1.1">
       de novo
      </span>
      Molecular generation,” which underscores the importance of incorporating chemical domain knowledge to effectively navigate the vast landscape of drug-like molecules. Using unsupervised learning and an auto-regressive approach, cMolGPT generates SMILES guided by predefined conditions based on target proteins and binding molecules. Initially trained on the MOSES dataset
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p5.1.2.1">
        <a class="ltx_ref" href="#bib.bib268" title="">
         268
        </a>
       </sup>
      </cite>
      without target information, the model is fine-tuned with embeddings of protein-binder pairs, focusing on generating compound libraries and target-specific molecules for the EGFR, HTR1A, and S1PR1 protein datasets.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p5.1.3.1">
        <a class="ltx_ref" href="#bib.bib284" title="">
         284
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib285" title="">
         285
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib286" title="">
         286
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib287" title="">
         287
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p6">
     <p class="ltx_p" id="S3.SS3.SSS2.p6.1">
      Their approach employs a QSAR model
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p6.1.1.1">
        <a class="ltx_ref" href="#bib.bib5" title="">
         5
        </a>
       </sup>
      </cite>
      to predict the activity of generated compounds, achieving a Pearson correlation coefficient over 0.75. However, reliance on this QSAR model, which has its own limitations, underscores the necessity for more extensive experimental datasets. Despite its capabilities, the cMolGPT model tends to generate molecules within the confines of the same sub-chemical space as the original dataset, indicating that while it successfully identifies potential binders, it faces challenges in broadly exploring the chemical space to discover novel solutions. This limitation highlights the difficulty of generating molecules from significantly different chemical families that are effective in binding or possess a combination of desired properties, suggesting a need for approaches that can more effectively expand the exploration of chemical diversity.
Regardless, models capable of this kind of conditional generation are typically trained on large datasets of known protein-ligand interactions. Through this training, they learn the complex relationship between protein structures and the chemical structures of binding ligands. The model learns to predict which molecular features or structures are important for interacting with proteins that have certain characteristics (as captured in the embeddings).
While both “Taiga” and “cMolGPT” use transformer architectures and decoder-only setups for molecule generation, they differ in approach. Taiga integrates reinforcement learning to optimize its generative model, while cMolGPT uses target-specific embeddings to guide the generation process.
      <cite class="ltx_cite ltx_citemacro_citet">
       Yu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib246" title="">
         246
        </a>
       </sup>
      </cite>
      follow a different approach. LlaSMol
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p6.1.2.1">
        <a class="ltx_ref" href="#bib.bib246" title="">
         246
        </a>
       </sup>
      </cite>
      starts from pretrained models (for instance Galactica, LlaMa2, and Mistral) and performs parameter efficient fine-tuning (PEFT)
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p6.1.3.1">
        <a class="ltx_ref" href="#bib.bib288" title="">
         288
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib289" title="">
         289
        </a>
       </sup>
      </cite>
      . Specifically, they used LoRa
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p6.1.4.1">
        <a class="ltx_ref" href="#bib.bib290" title="">
         290
        </a>
       </sup>
      </cite>
      . They show that PEFT has the potential to achieve state-of-the-art performance in property prediction when fine-tuning on MoleculeNet
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p6.1.5.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      .
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p7">
     <p class="ltx_p" id="S3.SS3.SSS2.p7.1">
      Outside of training on SMILES strings,
      <cite class="ltx_cite ltx_citemacro_citet">
       Mao et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib275" title="">
         275
        </a>
       </sup>
      </cite>
      introduce iupacGPT, which also uses a decoder-only architecture, modeled after GPT-2. It addresses the limitations of SMILES strings by using IUPAC names to integrate human-readable chemical semantics. It focuses on pretraining with a vast IUPAC dataset and fine-tuning with lightweight networks, excelling in molecule generation, classification, and regression tasks.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p8">
     <p class="ltx_p" id="S3.SS3.SSS2.p8.1">
      We also refer the reader to several other good reviews,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS3.SSS2.p8.1.1.1">
        <a class="ltx_ref" href="#bib.bib291" title="">
         291
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib292" title="">
         292
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib293" title="">
         293
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib175" title="">
         175
        </a>
       </sup>
      </cite>
      with
      <cite class="ltx_cite ltx_citemacro_citet">
       Goel et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib294" title="">
         294
        </a>
       </sup>
      </cite>
      exploring the efficiency of modern machine learning methods in sampling drug-like chemical space for virtual screening and molecular design. The work of
      <cite class="ltx_cite ltx_citemacro_citet">
       Goel et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib294" title="">
         294
        </a>
       </sup>
      </cite>
      discussed generative models, including LLM approaches, for approximating the entire drug-like chemical space and highlighted models conditioned on specific properties or receptor structures.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p9">
     <p class="ltx_p" id="S3.SS3.SSS2.p9.1">
      We provide a segue from this section by introducing the work by
      <cite class="ltx_cite ltx_citemacro_citet">
       Jablonka et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib249" title="">
         249
        </a>
       </sup>
      </cite>
      , which showcases a decoder-only GPT model that, despite its training on natural language rather than specialized chemical languages, competes effectively with decoder-only LLMs tailored to chemical languages. The authors use finetuned GPT-3 to answer complex chemistry questions and therefore highlight its potential as a foundational tool in the field. This sets the stage for integrating a natural language decoder-only LLM, like ChatGPT, into chemical research, where it could serve as a central hub. This foreshadows future developments to pair the LLM with specialized tools to enhance capabilities, paving the way for the creation of autonomous agents that leverage deep language understanding in scientific domains.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Synthesis Prediction and Encoder-decoder Mol-LLMs
   </h3>
   <figure class="ltx_table" id="S3.T3">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Encoder-decoder scientific LLMs.
     <math alttext="a" class="ltx_Math" display="inline" id="S3.T3.3.m1.1">
      <semantics id="S3.T3.3.m1.1b">
       <mi id="S3.T3.3.m1.1.1" xref="S3.T3.3.m1.1.1.cmml">
        a
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T3.3.m1.1c">
        <ci id="S3.T3.3.m1.1.1.cmml" xref="S3.T3.3.m1.1.1">
         𝑎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T3.3.m1.1d">
        a
       </annotation>
      </semantics>
     </math>
     :“Model Size” is reported as the number of parameters.
     <math alttext="b" class="ltx_Math" display="inline" id="S3.T3.4.m2.1">
      <semantics id="S3.T3.4.m2.1b">
       <mi id="S3.T3.4.m2.1.1" xref="S3.T3.4.m2.1.1.cmml">
        b
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.T3.4.m2.1c">
        <ci id="S3.T3.4.m2.1.1.cmml" xref="S3.T3.4.m2.1.1">
         𝑏
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.T3.4.m2.1d">
        b
       </annotation>
      </semantics>
     </math>
     : The total number of parameters was not reported.
    </figcaption>
    <table class="ltx_tabular ltx_align_middle" id="S3.T3.8">
     <tr class="ltx_tr" id="S3.T3.5.1">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.5.1.2" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.2.1">
        <span class="ltx_p" id="S3.T3.5.1.2.1.1">
         LLM
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.5.1.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.1.1">
        <span class="ltx_p" id="S3.T3.5.1.1.1.1">
         Model Size
         <sup class="ltx_sup" id="S3.T3.5.1.1.1.1.1">
          <span class="ltx_text ltx_font_italic" id="S3.T3.5.1.1.1.1.1.1">
           a
          </span>
         </sup>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.5.1.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.3.1">
        <span class="ltx_p" id="S3.T3.5.1.3.1.1">
         Training Data
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.5.1.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.4.1">
        <span class="ltx_p" id="S3.T3.5.1.4.1.1">
         Architecture
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.5.1.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.5.1">
        <span class="ltx_p" id="S3.T3.5.1.5.1.1">
         Application
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.5.1.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.5.1.6.1">
        <span class="ltx_p" id="S3.T3.5.1.6.1.1">
         Release date
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.6.2">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.6.2.2" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.2.1">
        <span class="ltx_p" id="S3.T3.6.2.2.1.1">
         MOLGEN
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.6.2.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib295" title="">
            295
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T3.6.2.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.1.1">
        <span class="ltx_p" id="S3.T3.6.2.1.1.1">
         <math alttext="b" class="ltx_Math" display="inline" id="S3.T3.6.2.1.1.1.m1.1">
          <semantics id="S3.T3.6.2.1.1.1.m1.1a">
           <mi id="S3.T3.6.2.1.1.1.m1.1.1" xref="S3.T3.6.2.1.1.1.m1.1.1.cmml">
            b
           </mi>
           <annotation-xml encoding="MathML-Content" id="S3.T3.6.2.1.1.1.m1.1b">
            <ci id="S3.T3.6.2.1.1.1.m1.1.1.cmml" xref="S3.T3.6.2.1.1.1.m1.1.1">
             𝑏
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T3.6.2.1.1.1.m1.1c">
            b
           </annotation>
          </semantics>
         </math>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T3.6.2.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.3.1">
        <span class="ltx_p" id="S3.T3.6.2.3.1.1">
         ZINC15
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T3.6.2.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.4.1">
        <span class="ltx_p" id="S3.T3.6.2.4.1.1">
         BART
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T3.6.2.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.5.1">
        <span class="ltx_p" id="S3.T3.6.2.5.1.1">
         Molecule Generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T3.6.2.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.6.2.6.1">
        <span class="ltx_p" id="S3.T3.6.2.6.1.1">
         2024.03
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.5">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.5.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.1.1">
        <span class="ltx_p" id="S3.T3.8.5.1.1.1">
         BioT5+
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.5.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib129" title="">
            129
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.5.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.2.1">
        <span class="ltx_p" id="S3.T3.8.5.2.1.1">
         252M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.5.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.3.1">
        <span class="ltx_p" id="S3.T3.8.5.3.1.1">
         ZINC20, UniRef50, 33M PubMed articles, 339K mol-text pairs from PubChem, 569K FASTA-text pairs from Swiss-prot
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.5.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.4.1">
        <span class="ltx_p" id="S3.T3.8.5.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.5.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.5.1">
        <span class="ltx_p" id="S3.T3.8.5.5.1.1">
         Molecule Captioning, Molecule Generation, Property Prediction,
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.5.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.5.6.1">
        <span class="ltx_p" id="S3.T3.8.5.6.1.1">
         2024-02
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.6">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.6.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.1.1">
        <span class="ltx_p" id="S3.T3.8.6.1.1.1">
         BioT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.6.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib144" title="">
            144
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.6.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.2.1">
        <span class="ltx_p" id="S3.T3.8.6.2.1.1">
         252M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.6.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.3.1">
        <span class="ltx_p" id="S3.T3.8.6.3.1.1">
         ZINC20, UniRef50, full-articles from BioRxiv and PubMed, mol-text-IUPAC information from PubChem
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.6.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.4.1">
        <span class="ltx_p" id="S3.T3.8.6.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.6.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.5.1">
        <span class="ltx_p" id="S3.T3.8.6.5.1.1">
         Molecule Captioning, Property Prediction
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.6.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.6.6.1">
        <span class="ltx_p" id="S3.T3.8.6.6.1.1">
         2023-12
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.7">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.7.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.1.1">
        <span class="ltx_p" id="S3.T3.8.7.1.1.1">
         nach0
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.7.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib128" title="">
            128
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.7.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.2.1">
        <span class="ltx_p" id="S3.T3.8.7.2.1.1">
         250M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.7.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.3.1">
        <span class="ltx_p" id="S3.T3.8.7.3.1.1">
         MoleculeNet, USPTO, ZINC
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.7.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.4.1">
        <span class="ltx_p" id="S3.T3.8.7.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.7.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.5.1">
        <span class="ltx_p" id="S3.T3.8.7.5.1.1">
         Property prediction, Molecule generation, Question answering, NER
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.7.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.7.6.1">
        <span class="ltx_p" id="S3.T3.8.7.6.1.1">
         2023.11
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.8">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.8.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.1.1">
        <span class="ltx_p" id="S3.T3.8.8.1.1.1">
         ReactionT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.8.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib147" title="">
            147
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.8.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.2.1">
        <span class="ltx_p" id="S3.T3.8.8.2.1.1">
         220M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.8.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.3.1">
        <span class="ltx_p" id="S3.T3.8.8.3.1.1">
         ZINC and ORD
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.8.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.4.1">
        <span class="ltx_p" id="S3.T3.8.8.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.8.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.5.1">
        <span class="ltx_p" id="S3.T3.8.8.5.1.1">
         Property prediction and Reaction prediction
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.8.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.8.6.1">
        <span class="ltx_p" id="S3.T3.8.8.6.1.1">
         2023.11
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.9">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.9.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.1.1">
        <span class="ltx_p" id="S3.T3.8.9.1.1.1">
         Text+Chem T5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.9.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib130" title="">
            130
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.9.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.2.1">
        <span class="ltx_p" id="S3.T3.8.9.2.1.1">
         60M, 220M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.9.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.3.1">
        <span class="ltx_p" id="S3.T3.8.9.3.1.1">
         11.5M or 33.5M samples curated from
         <cite class="ltx_cite ltx_citemacro_citet">
          Vaucher et al.
          <sup class="ltx_sup">
           <a class="ltx_ref" href="#bib.bib296" title="">
            296
           </a>
          </sup>
         </cite>
         ,
         <cite class="ltx_cite ltx_citemacro_citet">
          Toniato et al.
          <sup class="ltx_sup">
           <a class="ltx_ref" href="#bib.bib225" title="">
            225
           </a>
          </sup>
         </cite>
         , and CheBI-20
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.9.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.4.1">
        <span class="ltx_p" id="S3.T3.8.9.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.9.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.5.1">
        <span class="ltx_p" id="S3.T3.8.9.5.1.1">
         Molecule Captioning, Product Prediction, Retrosynthesis, Molecule Generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.9.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.9.6.1">
        <span class="ltx_p" id="S3.T3.8.9.6.1.1">
         2023.01
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.10">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.10.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.1.1">
        <span class="ltx_p" id="S3.T3.8.10.1.1.1">
         MolT5
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.10.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib131" title="">
            131
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.10.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.2.1">
        <span class="ltx_p" id="S3.T3.8.10.2.1.1">
         60M, 770M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.10.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.3.1">
        <span class="ltx_p" id="S3.T3.8.10.3.1.1">
         C4 dataset
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.10.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.4.1">
        <span class="ltx_p" id="S3.T3.8.10.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.10.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.5.1">
        <span class="ltx_p" id="S3.T3.8.10.5.1.1">
         Molecule Captioning and Molecule Generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.10.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.10.6.1">
        <span class="ltx_p" id="S3.T3.8.10.6.1.1">
         2022.11
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.11">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.11.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.1.1">
        <span class="ltx_p" id="S3.T3.8.11.1.1.1">
         T5Chem
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.11.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib148" title="">
            148
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.11.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.2.1">
        <span class="ltx_p" id="S3.T3.8.11.2.1.1">
         220M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.11.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.3.1">
        <span class="ltx_p" id="S3.T3.8.11.3.1.1">
         USPTO
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.11.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.4.1">
        <span class="ltx_p" id="S3.T3.8.11.4.1.1">
         T5
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.11.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.5.1">
        <span class="ltx_p" id="S3.T3.8.11.5.1.1">
         Product Prediction, Retrosynthesis, Property Prediction
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.11.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.11.6.1">
        <span class="ltx_p" id="S3.T3.8.11.6.1.1">
         2022.03
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.12">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.12.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.1.1">
        <span class="ltx_p" id="S3.T3.8.12.1.1.1">
         ChemFormer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.12.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib146" title="">
            146
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.12.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.2.1">
        <span class="ltx_p" id="S3.T3.8.12.2.1.1">
         45M, 230M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.12.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.3.1">
        <span class="ltx_p" id="S3.T3.8.12.3.1.1">
         100M SMILES from ZINC-15
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.12.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.4.1">
        <span class="ltx_p" id="S3.T3.8.12.4.1.1">
         BART
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.12.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.5.1">
        <span class="ltx_p" id="S3.T3.8.12.5.1.1">
         Product Prediction, Property Prediction, Molecular Generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.12.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.12.6.1">
        <span class="ltx_p" id="S3.T3.8.12.6.1.1">
         2022.01
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.7.3">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.7.3.2" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.2.1">
        <span class="ltx_p" id="S3.T3.7.3.2.1.1">
         Text2Mol
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.7.3.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib297" title="">
            297
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.7.3.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.1.1">
        <span class="ltx_p" id="S3.T3.7.3.1.1.1">
         <math alttext="b" class="ltx_Math" display="inline" id="S3.T3.7.3.1.1.1.m1.1">
          <semantics id="S3.T3.7.3.1.1.1.m1.1a">
           <mi id="S3.T3.7.3.1.1.1.m1.1.1" xref="S3.T3.7.3.1.1.1.m1.1.1.cmml">
            b
           </mi>
           <annotation-xml encoding="MathML-Content" id="S3.T3.7.3.1.1.1.m1.1b">
            <ci id="S3.T3.7.3.1.1.1.m1.1.1.cmml" xref="S3.T3.7.3.1.1.1.m1.1.1">
             𝑏
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T3.7.3.1.1.1.m1.1c">
            b
           </annotation>
          </semantics>
         </math>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.7.3.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.3.1">
        <span class="ltx_p" id="S3.T3.7.3.3.1.1">
         CheBI-20
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.7.3.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.4.1">
        <span class="ltx_p" id="S3.T3.7.3.4.1.1">
         SciBERT w/ decoder
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.7.3.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.5.1">
        <span class="ltx_p" id="S3.T3.7.3.5.1.1">
         Molecule captioning and conditional molecule generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.7.3.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.7.3.6.1">
        <span class="ltx_p" id="S3.T3.7.3.6.1.1">
         2021.
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.4">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.4.2" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.2.1">
        <span class="ltx_p" id="S3.T3.8.4.2.1.1">
         SMILES transformer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.4.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib156" title="">
            156
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.4.1" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.1.1">
        <span class="ltx_p" id="S3.T3.8.4.1.1.1">
         <math alttext="b" class="ltx_Math" display="inline" id="S3.T3.8.4.1.1.1.m1.1">
          <semantics id="S3.T3.8.4.1.1.1.m1.1a">
           <mi id="S3.T3.8.4.1.1.1.m1.1.1" xref="S3.T3.8.4.1.1.1.m1.1.1.cmml">
            b
           </mi>
           <annotation-xml encoding="MathML-Content" id="S3.T3.8.4.1.1.1.m1.1b">
            <ci id="S3.T3.8.4.1.1.1.m1.1.1.cmml" xref="S3.T3.8.4.1.1.1.m1.1.1">
             𝑏
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S3.T3.8.4.1.1.1.m1.1c">
            b
           </annotation>
          </semantics>
         </math>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.4.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.3.1">
        <span class="ltx_p" id="S3.T3.8.4.3.1.1">
         ChEMBL24
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.4.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.4.1">
        <span class="ltx_p" id="S3.T3.8.4.4.1.1">
         Transformer
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.4.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.5.1">
        <span class="ltx_p" id="S3.T3.8.4.5.1.1">
         Property prediction
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.4.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.4.6.1">
        <span class="ltx_p" id="S3.T3.8.4.6.1.1">
         2019.11
        </span>
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S3.T3.8.13">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.8.13.1" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.1.1">
        <span class="ltx_p" id="S3.T3.8.13.1.1.1">
         Molecular Transformer
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S3.T3.8.13.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib233" title="">
            233
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.13.2" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.2.1">
        <span class="ltx_p" id="S3.T3.8.13.2.1.1">
         12M
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.13.3" style="width:128.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.3.1">
        <span class="ltx_p" id="S3.T3.8.13.3.1.1">
         USPTO
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.13.4" style="width:42.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.4.1">
        <span class="ltx_p" id="S3.T3.8.13.4.1.1">
         Transformer
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.13.5" style="width:99.6pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.5.1">
        <span class="ltx_p" id="S3.T3.8.13.5.1.1">
         Product prediction
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T3.8.13.6" style="width:56.9pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S3.T3.8.13.6.1">
        <span class="ltx_p" id="S3.T3.8.13.6.1.1">
         2019.08
        </span>
       </span>
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     The encoder-decoder architecture is designed for tasks involving the translation of one sequence into another, making it ideal for predicting chemical reaction outcomes or generating synthesis pathways from given reactants. We begin with a background on optimal synthesis prediction and describe how earlier machine learning has approached this challenge. Following that, we explain how LLMs have enhanced chemical synthesis prediction and optimization. Although, our context below is aptly chosen to be synthesis prediction, other applications exist. For example, SMILES Transformer (ST)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS4.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib156" title="">
        156
       </a>
      </sup>
     </cite>
     is worth a mention, historically, because it explored the benefits of unsupervised pretraining to produce continuous, data-driven molecular fingerprints from large SMILES-based datasets.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS4.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.4.1
     </span>
     Synthesis Prediction
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
     <p class="ltx_p" id="S3.SS4.SSS1.p1.1">
      Once a molecule has been identified through property-directed inverse design, the next challenge is to predict its optimal synthesis, including yield.
      <cite class="ltx_cite ltx_citemacro_citet">
       Shenvi
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib298" title="">
         298
        </a>
       </sup>
      </cite>
      describe how the demanding and elegant syntheses of natural products has contributed greatly to organic chemistry. However, in the past 20 years, the focus has shifted away from complex natural product synthesis towards developing reactions applicable for a broader range of compounds, especially in reaction catalysis.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS1.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib298" title="">
         298
        </a>
       </sup>
      </cite>
      Yet, complex synthesis is becoming relevant again as it can be digitally encoded, mined by LLMs,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS1.p1.1.2.1">
        <a class="ltx_ref" href="#bib.bib299" title="">
         299
        </a>
       </sup>
      </cite>
      and applied to new challenges. Unlike property prediction, reaction prediction is challenging due to the involvement of multiple molecules. Modifying one reactant requires adjusting all others, with different synthesis mechanisms or conditions likely involved. Higher-level challenges exist for catalytic reactions and complex natural product synthesis.
Synthesis can be approached in two ways. Forward synthesis involves building complex target molecules from simple, readily available substances, planning the steps progressively. Retrosynthesis, introduced by E.J. Corey in 1988,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS1.p1.1.3.1">
        <a class="ltx_ref" href="#bib.bib300" title="">
         300
        </a>
       </sup>
      </cite>
      is more common. It involves working backward from the target molecule, breaking it into smaller fragments whose re-connection is most effective. Chemists choose small, inexpensive, and readily available starting materials to achieve the greatest yield and cost-effectiveness. For example, the first total synthesis of discodermolide
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS1.p1.1.4.1">
        <a class="ltx_ref" href="#bib.bib301" title="">
         301
        </a>
       </sup>
      </cite>
      involved 36 such steps, a 24-step longest linear sequence, and a 3.2% yield. There are many possible combinations for the total synthesis of a target molecule, and the synthetic chemist must choose the most sensible approach based on their expertise and knowledge. However, this approach to total synthesis takes many years.
LLMs can now transform synthesis such that structure-activity relationship predictions can be coupled in lock-step with molecule selection based on easier synthetic routes. This third challenge of predicting the optimal synthesis can also lead to the creation of innovative, non-natural compounds, chosen because of such an easier predicted sytnthesis but for which the properties are still predicted to meet the needs of the application. Thus, these three challenges introduced above are interconnected.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS4.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.4.2
     </span>
     Encoder-decoder mol-LLMs
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
     <p class="ltx_p" id="S3.SS4.SSS2.p1.1">
      Before we focus on transformer use, some description is provided on the evolution from RNN and Gated Recurrent Unit (GRU) approaches in concert with the move from template-based to semi-template-based to template-free models.
      <cite class="ltx_cite ltx_citemacro_citet">
       Nam and Kim
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib302" title="">
         302
        </a>
       </sup>
      </cite>
      pioneered forward synthesis prediction using a GRU-based translation model, while
      <cite class="ltx_cite ltx_citemacro_citet">
       Liu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib303" title="">
         303
        </a>
       </sup>
      </cite>
      reported retro-synthesis prediction with a Long Short-Term Memory (LSTM) based seq2seq model incorporating an attention mechanism, achieving 37.4% accuracy on the USPTO-50K dataset.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib304" title="">
         304
        </a>
       </sup>
      </cite>
      The reported accuracies of these early models highlighted the challenges of synthesis prediction, particularly retrosynthesis.
      <cite class="ltx_cite ltx_citemacro_citet">
       Schneider et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib304" title="">
         304
        </a>
       </sup>
      </cite>
      further advanced retrosynthesis by assigning reaction roles to reagents and reactants based on the product.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
     <p class="ltx_p" id="S3.SS4.SSS2.p2.1">
      Evolving from RNNs and GRUs, the field progressed with the introduction of template-based models. As a parallel to the separate development of the Chematica
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p2.1.1.1">
        <a class="ltx_ref" href="#bib.bib305" title="">
         305
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib306" title="">
         306
        </a>
       </sup>
      </cite>
      synthesis mapping tool,
      <cite class="ltx_cite ltx_citemacro_citet">
       Segler and Waller
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib307" title="">
         307
        </a>
       </sup>
      </cite>
      identified that computational “rule-based systems” often failed because they ignored the molecular context, resulting in “reactivity conflicts.” They prioritized suitable transformation rules describing how atoms and bonds change during reactions, applied in reverse for retrosynthesis. They trained a model on 3.5 million reactions, achieving 95% top-10 accuracy in retrosynthesis and 97% for reaction prediction on a validation set of nearly 1 million reactions from the Reaxys database (1771-2015). Though not transformer-based, their work paved the way for LLMs in synthesis applications. However, template-based models rely on explicit reaction templates from known reactions, which limits their ability to predict novel reactions and they require manual updates for learning new data.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p3">
     <p class="ltx_p" id="S3.SS4.SSS2.p3.1">
      Semi-template-based models offered a balance between rigid template-based methods and flexible template-free approaches. They used interpolation or extrapolation within template-defined spaces to predict a wider range of reactions and to adjust based on new data. In 2021,
      <cite class="ltx_cite ltx_citemacro_citet">
       Somnath et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib308" title="">
         308
        </a>
       </sup>
      </cite>
      introduced a graph-based approach recognizing that precursor molecule topology is largely unchanged during reactions. Their model broke the product molecule into “synthons” and added relevant leaving groups, making results more interpretable.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p3.1.1.1">
        <a class="ltx_ref" href="#bib.bib309" title="">
         309
        </a>
       </sup>
      </cite>
      Training on the USPTO-50k dataset,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p3.1.2.1">
        <a class="ltx_ref" href="#bib.bib304" title="">
         304
        </a>
       </sup>
      </cite>
      they achieved a top-1 accuracy of 53.7%, outperforming previous methods.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p4">
     <p class="ltx_p" id="S3.SS4.SSS2.p4.1">
      It is, however, the template-free approaches that align well with transformer-based learning approaches because they learn retrosynthetic rules from raw training data. This provides significant flexibility and generalizability across various types of chemistry. Template-free models are not constrained by template libraries and so can uncover novel synthetic routes that are undocumented or not obvious from existing reaction templates.
To pave the way for transformer use in synthesis,
      <cite class="ltx_cite ltx_citemacro_citet">
       Cadeddu et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib310" title="">
         310
        </a>
       </sup>
      </cite>
      drew an analogy between fragments in a compound and words in a sentence due to their similar rank distributions.
      <cite class="ltx_cite ltx_citemacro_citet">
       Schwaller et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib311" title="">
         311
        </a>
       </sup>
      </cite>
      further advanced this with an LSTM network augmented by an attention-mechanism-based encoder-decoder architecture, using the USPTO dataset.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p4.1.1.1">
        <a class="ltx_ref" href="#bib.bib304" title="">
         304
        </a>
       </sup>
      </cite>
      They introduced a commonly used “regular expression” (or “regex”) for tokenizing molecules, framing synthesis (or retrosynthesis) predictions as translation problems with a data-driven, template-free sequence-to-sequence model. They tracked which starting materials were actual reactants, distinguishing them from other reagents like solvents or catalysts, and used the regex to uniquely tokenize recurring reagents, as their atoms were not mapped to products in the core reaction.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p5">
     <p class="ltx_p" id="S3.SS4.SSS2.p5.1">
      In 2019, going beyond the ”neural machine" work of
      <cite class="ltx_cite ltx_citemacro_citet">
       Nam and Kim
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib302" title="">
         302
        </a>
       </sup>
      </cite>
      ,
      <cite class="ltx_cite ltx_citemacro_citet">
       Schwaller et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib233" title="">
         233
        </a>
       </sup>
      </cite>
      first applied a transformer for synthesis prediction, framing the task as translating reactants and reagents into the final product. Their model inferred correlations between chemical motifs in reactants, reagents, and products in the dataset (USPTO-MIT,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p5.1.1.1">
        <a class="ltx_ref" href="#bib.bib312" title="">
         312
        </a>
       </sup>
      </cite>
      USPTO-LEF,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p5.1.2.1">
        <a class="ltx_ref" href="#bib.bib313" title="">
         313
        </a>
       </sup>
      </cite>
      USPTO-STEREO
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p5.1.3.1">
        <a class="ltx_ref" href="#bib.bib311" title="">
         311
        </a>
       </sup>
      </cite>
      ). It required no handcrafted rules and accurately predicted subtle chemical transformations, outperforming all prior algorithms on a common benchmark dataset. The model handled inputs without a reactant-reagent split, following their previous work,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p5.1.4.1">
        <a class="ltx_ref" href="#bib.bib311" title="">
         311
        </a>
       </sup>
      </cite>
      and accounted for stereochemistry, making it valuable for universal application.
Then, in 2020, for automated retrosynthesis,
      <cite class="ltx_cite ltx_citemacro_citet">
       Schwaller et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib314" title="">
         314
        </a>
       </sup>
      </cite>
      developed an advanced Molecular Transformer model with a hyper-graph exploration strategy. The model set a standard for predicting reactants and other entities, evaluated with four new metrics: coverage, class diversity, round-trip accuracy, and Jensen–Shannon divergence. Constructed dynamically, the hypergraph allowed for efficient expansion based on Bayesian-like probability scores, showing high performance despite training data limitations. Notably, accuracy was improved after the re-synthesis of the target product from the newly generated molecular precursors was included in the model. This round-trip accuracy concept was also used by
      <cite class="ltx_cite ltx_citemacro_citet">
       Chen and Jung
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib315" title="">
         315
        </a>
       </sup>
      </cite>
      and
      <cite class="ltx_cite ltx_citemacro_citet">
       Westerlund et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib316" title="">
         316
        </a>
       </sup>
      </cite>
      . Also in 2020,
      <cite class="ltx_cite ltx_citemacro_citet">
       Zheng et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib317" title="">
         317
        </a>
       </sup>
      </cite>
      developed a “template-free self-corrected retrosynthesis predictor” (SCROP) using transformer networks and a neural network-based syntax corrector, achieving 59.0% accuracy on a benchmark dataset.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p5.1.5.1">
        <a class="ltx_ref" href="#bib.bib318" title="">
         318
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib304" title="">
         304
        </a>
       </sup>
      </cite>
      This approach outperformed other deep learning methods by over 2% and template-based methods by over 6%.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p6">
     <p class="ltx_p" id="S3.SS4.SSS2.p6.1">
      We now highlight advancements in synthesis prediction using the BART Encoder-Decoder architecture, starting with Chemformer by
      <cite class="ltx_cite ltx_citemacro_citet">
       Irwin et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib146" title="">
         146
        </a>
       </sup>
      </cite>
      . This paper emphasized the computational expense of training transformers on SMILES and the importance of pretraining for efficiency. It showed that models pretrained on task-specific datasets or using only the encoder stack were limited for sequence-to-sequence tasks. After transfer learning, Chemformer achieved state-of-the-art results in both sequence-to-sequence synthesis tasks and discriminative tasks, such as optimizing molecular structures for specific properties. They studied the effects of small changes on molecular properties using pairs of molecules from the ChEMBL database
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p6.1.1.1">
        <a class="ltx_ref" href="#bib.bib179" title="">
         179
        </a>
       </sup>
      </cite>
      with a single structural modification. Chemformer’s performance was tested on the ESOL, Lipophilicity, and Free Solvation datasets.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p6.1.2.1">
        <a class="ltx_ref" href="#bib.bib57" title="">
         57
        </a>
       </sup>
      </cite>
      <cite class="ltx_cite ltx_citemacro_citet">
       Irwin et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib146" title="">
         146
        </a>
       </sup>
      </cite>
      also described their use of an in-house property prediction model, but when models train on calculated data for ease of access and uniformity, they abstract away from real-world chemical properties. We again emphasize the importance of incorporating experimentally derived data into Chemistry LLM research to create more robust and relevant models. Continuously curating new, relevant datasets that better represent real-world chemical complexities will enhance the applicability and transferability of these models.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p7">
     <p class="ltx_p" id="S3.SS4.SSS2.p7.1">
      In 2023,
      <cite class="ltx_cite ltx_citemacro_citet">
       Toniato et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib225" title="">
         225
        </a>
       </sup>
      </cite>
      also applied LLMs to single-step retrosynthesis as a translation problem, but increased retrosynthesis prediction diversity by adding classification tokens, or “prompt tokens,” to the target molecule’s language representation, guiding the model towards different disconnection strategies. Increased prediction diversity has high value by providing out-of-the-box synthetic strategies to complement the human chemist’s work.
To measure retrosynthesis accuracy,
      <cite class="ltx_cite ltx_citemacro_citet">
       Li et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib319" title="">
         319
        </a>
       </sup>
      </cite>
      introduced Retro-BLEU, a metric adapted from the BLEU score used in machine translation.
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p7.1.1.1">
        <a class="ltx_ref" href="#bib.bib320" title="">
         320
        </a>
       </sup>
      </cite>
      Despite progress in computer-assisted synthesis planning (CASP), not all generated routes are chemically feasible due to steps like protection and deprotection needed for product formation. Widely accepted NLP metrics like BLEU
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p7.1.2.1">
        <a class="ltx_ref" href="#bib.bib320" title="">
         320
        </a>
       </sup>
      </cite>
      and ROUGE
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p7.1.3.1">
        <a class="ltx_ref" href="#bib.bib321" title="">
         321
        </a>
       </sup>
      </cite>
      focus on precision and recall by computing n-gram overlaps between generated and reference texts. Similarly, in retrosynthesis, reactant-product pairs can be treated as overlapping bigrams. Retro-BLEU uses a modified BLEU score, emphasizing precision over recall, as there is no absolute best route for retrosynthesis. Although not yet applied to LLM-based predictions, this approach has value by allowing future performance comparison with a single standard.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p8">
     <p class="ltx_p" id="S3.SS4.SSS2.p8.1">
      Finally, by expanding the use of encoder-decoder architectures outside synthesis prediction into molecular generation,
      <cite class="ltx_cite ltx_citemacro_citet">
       Fang et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib295" title="">
         295
        </a>
       </sup>
      </cite>
      introduced MOLGEN, a BART-based pretrained molecular language model, in a 2023 preprint updated in 2024. MOLGEN addressed three key challenges: generating valid SMILES strings, avoiding an observed bias that existed against natural product-like molecules, and preventing hallucinations of molecules that didn’t retain the intended properties. Pretrained on 100 million molecules using SELFIES
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS4.SSS2.p8.1.1.1">
        <a class="ltx_ref" href="#bib.bib173" title="">
         173
        </a>
       </sup>
      </cite>
      and a masked language model approach, MOLGEN predicts missing tokens to internalize chemical grammar. An additional highlight of this work is how MOLGEN uses “domain-agnostic molecular prefix tuning.” This technique integrates domain knowledge directly into the model’s attention mechanisms by adding molecule-specific prefixes, trained simultaneously with the main model across various molecular domains. The model’s parameters would thus be adjusted to better capture the complexities and diversities of molecular structures, and domain-specific insights would be seamlessly integrated. To prevent molecular hallucinations, MOLGEN employs a chemical feedback mechanism, to autonomously evaluate generated molecules for appropriate properties, to guide learning and optimization. Such feedback foreshadows a core aspect of autonomous agents, which is their capacity for reflection. We will explore this further below.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.5
    </span>
    Multi-Modal LLMs
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS5.p1">
    <p class="ltx_p" id="S3.SS5.p1.1">
     Multitask or multimodal methods applied to chemistry are now described. In 2021,
     <cite class="ltx_cite ltx_citemacro_citet">
      Edwards et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib297" title="">
        297
       </a>
      </sup>
     </cite>
     proposed Text2Mol, which retrieved molecules using natural language descriptions as queries. This task required integrating both molecular and natural language modalities, making it a cross-lingual retrieval problem due to the distinct grammar of molecular representations. The researchers built a paired dataset of molecules with corresponding text descriptions, and developed an aligned semantic embedding space for retrieval. This was enhanced with a cross-modal attention-based model for explainability and reranking. One stated aim was to improve retrieval metrics, which would further advance the ability for machines to learn from chemical literature.
In their follow-up work in 2022,
     <cite class="ltx_cite ltx_citemacro_citet">
      Edwards et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib131" title="">
        131
       </a>
      </sup>
     </cite>
     continued using SMILES string representations along with text descriptions. The two key applications were first, to generate captions for molecules from SMILES strings and second, to output molecules based on textual descriptions of desired properties.
With this in mind, several important challenges remain. First, molecules can have many possible descriptions based on their diverse properties. For example, aspirin can be described by its pain-relieving effects, its use in preventing heart attacks, its chemical structure (an ester and a carboxylic acid connected to a benzene ring in ortho geometry), or its degradation into salicylic acid and ethanoic acid in moist conditions
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS5.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib322" title="">
        322
       </a>
      </sup>
     </cite>
     . Describing such a molecule therefore requires expertise in different chemistry domains, depending on its functions and properties. In contrast, image captioning for common objects like cats and dogs needs much less domain expertise. Thus, obtaining a large, paired dataset of chemical representations and textual descriptions is challenging. Another issue is that standard evaluation metrics, like BLEU, are inadequate for molecule-language tasks. To address this,
     <cite class="ltx_cite ltx_citemacro_citet">
      Edwards et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib131" title="">
        131
       </a>
      </sup>
     </cite>
     developed MolT5, which pretrained a model on a large corpus of unlabeled natural language text and molecule strings using a denoising objective. They fine-tuned the model on gold standard annotations and improved the metric based on their prior Text2Mol work
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS5.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib297" title="">
        297
       </a>
      </sup>
     </cite>
     . MolT5 effectively generated both molecule and caption outputs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS5.p2">
    <p class="ltx_p" id="S3.SS5.p2.1">
     In other work,
     <cite class="ltx_cite ltx_citemacro_citet">
      Seidl et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib323" title="">
        323
       </a>
      </sup>
     </cite>
     developed CLAMP. Trained on large biochemical datasets, CLAMP adapts to new tasks using separate modules for chemical and language inputs, and predicts biochemical activity for drug discovery.
     <cite class="ltx_cite ltx_citemacro_citet">
      Xu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib324" title="">
        324
       </a>
      </sup>
     </cite>
     presented BioTranslator, a method for translating user-written text descriptions into non-text biological data, facilitating the discovery of novel cell types and applications in protein function prediction and drug target identification. This method enabled data exploration beyond controlled vocabularies using free text. ChatDrug, by
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib325" title="">
        325
       </a>
      </sup>
     </cite>
     , integrates multimodal capabilities through a prompt module, a retrieval and domain feedback module, and a conversation module for systematic drug editing. It identifies and manipulates molecular structures for better interpretability in pharmaceutical research.
     <cite class="ltx_cite ltx_citemacro_citet">
      Christofidellis et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib130" title="">
        130
       </a>
      </sup>
     </cite>
     introduced a multi-domain, multi-task language model capable of handling tasks across chemical and natural language domains without specific pretraining for each.
     <cite class="ltx_cite ltx_citemacro_citet">
      Shoghi et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib326" title="">
        326
       </a>
      </sup>
     </cite>
     describe Joint Multi-domain Pre-training (JMP), based on the hypothesis that pre-training on a diverse set of chemical domains should lead to better
generalization towards a foundational model. The pre-training task is framed as a multi-task supervised learning
problem, where each label of each pre-training dataset is treated as a separate task.
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib327" title="">
        327
       </a>
      </sup>
     </cite>
     developed MolXPT, a unified model that integrates text and molecules for enhanced molecular property prediction and translation, demonstrating robust zero-shot molecular generation capabilities.
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib328" title="">
        328
       </a>
      </sup>
     </cite>
     introduced PremuNet, a pretrained multi-representation fusion network that enhances molecular property prediction by integrating various molecular data representations.
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib329" title="">
        329
       </a>
      </sup>
     </cite>
     developed GIT-Mol, integrating graph, image, and text data, significantly improving molecular property prediction and molecule generation validity.
     <cite class="ltx_cite ltx_citemacro_citet">
      Gao et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib330" title="">
        330
       </a>
      </sup>
     </cite>
     advanced targeted molecule generation with DockingGA, combining transformer neural networks with genetic algorithms and docking simulations for optimal molecule generation, utilizing Self-referencing Chemical Structure Strings to represent and optimize molecules.
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhou et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib331" title="">
        331
       </a>
      </sup>
     </cite>
     developed TSMMG, a teacher-student LLM for multi-constraint molecular generation, learning from a large set of text-molecule pairs and generating molecules that meet complex property requirements.
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib332" title="">
        332
       </a>
      </sup>
     </cite>
     proposed 3M-Diffusion, a multi-modal molecular graph generation method that uses natural language descriptions to generate diverse, novel molecular graphs.
     <cite class="ltx_cite ltx_citemacro_citet">
      Gong et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib333" title="">
        333
       </a>
      </sup>
     </cite>
     introduced TGM-DLM, a diffusion model for text-guided molecule generation that overcomes limitations of autoregressive models in generating precise molecules from textual descriptions.
     <cite class="ltx_cite ltx_citemacro_citet">
      Fang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib334" title="">
        334
       </a>
      </sup>
     </cite>
     developed MolTC, a multimodal framework that effectively integrates graphical information of molecules for improved prediction of molecular interactions.
     <cite class="ltx_cite ltx_citemacro_citet">
      Soares et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib335" title="">
        335
       </a>
      </sup>
     </cite>
     describe MULTIMODAL-MOLFORMER, a multimodal language model that predicts molecular properties by integrating chemical language representations with physicochemical features. It uses a causal multistage feature selection method to identify physicochemical features directly impacting target properties. These features are combined with molecular embeddings from MOLFORMER
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS5.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib196" title="">
        196
       </a>
      </sup>
     </cite>
     , significantly enhancing prediction accuracy for complex tasks like biodegradability and PFAS toxicity.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS6">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.6
    </span>
    Textual scientific LLMs
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS6.p1">
    <p class="ltx_p" id="S3.SS6.p1.1">
     LLMs are large deep neural network models. Therefore, their superior performance against other smaller models in classical machine learning tasks is somewhat expected.
Addressing tasks that seemed impossible to tackle in the past better illustrates LLM’s outstanding capabilities.
That is, despite accurately predicting properties given well-structured information such as molecular features and descriptors, LLMs can access less structured data and extract information from simple natural language.
For instance, instead of developing a restricted representation,
     <cite class="ltx_cite ltx_citemacro_citet">
      Huang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib138" title="">
        138
       </a>
      </sup>
     </cite>
     used clinical annotations from the MIMIC-III
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib336" title="">
        336
       </a>
      </sup>
     </cite>
     dataset to predict patient readmission. ClinicalBERT implements a pretrained BERT architecture using a combination of masked language modeling and next-sentence prediction training schemes followed by supervised fine-tuning on readmission prediction.
Or, instead of pretraining,
     <cite class="ltx_cite ltx_citemacro_citet">
      Zhao et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib121" title="">
        121
       </a>
      </sup>
     </cite>
     fine-tuned LLaMA on epilepsy data using an instruction-following approach to create EpilepsyLLM.
Similarly, SciBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib134" title="">
        134
       </a>
      </sup>
     </cite>
     and ScholarBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib137" title="">
        137
       </a>
      </sup>
     </cite>
     adapted BERT to scientific literature.
     <cite class="ltx_cite ltx_citemacro_citet">
      Beltagy et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib134" title="">
        134
       </a>
      </sup>
     </cite>
     created a tokenizer specific for scientific texts extracted from scientific articles from semantic scholar
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.4.1">
       <a class="ltx_ref" href="#bib.bib180" title="">
        180
       </a>
      </sup>
     </cite>
     and showed that SciBERT outperforms fine-tuned
     <span class="ltx_text ltx_font_typewriter" id="S3.SS6.p1.1.5">
      BERT-base
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS6.p1.1.5.1.1">
        <a class="ltx_ref" href="#bib.bib105" title="">
         105
        </a>
       </sup>
      </cite>
     </span>
     in every task investigated. It shows that vocabulary quality plays an important role in the model’s performance.
A few years later,
     <cite class="ltx_cite ltx_citemacro_citet">
      Hong et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib137" title="">
        137
       </a>
      </sup>
     </cite>
     pretrained BERT using RoBERTa
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.6.1">
       <a class="ltx_ref" href="#bib.bib337" title="">
        337
       </a>
      </sup>
     </cite>
     optimizations to improve pretraining performance. ScholarBERT was pretrained on scientific articles from
     <span class="ltx_text ltx_font_typewriter" id="S3.SS6.p1.1.7">
      Public.Resource.Org, Inc
     </span>
     and further fine-tuned on the tasks used for evaluation.
Despite using a much larger dataset, ScholarBERT did not outperform other LLMs trained with narrower domain datasets. Even against SciBERT, another LLM trained on multi-domain articles, ScholarBERT could not perform relatively better in tasks from biomedical, computer science, and material domains. However, ScholarBERT showed significant improvement in the ScienceExamCER
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p1.1.8.1">
       <a class="ltx_ref" href="#bib.bib338" title="">
        338
       </a>
      </sup>
     </cite>
     dataset, which contains semantic class annotations in a training corpus of 3rd to 9th grade science exam questions for named entity recognition (NER) tasks.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS6.p2">
    <p class="ltx_p" id="S3.SS6.p2.6">
     On the other hand, instead of pretraining LLMs on a large domain scope, models focused on only one field were also developed.
     <cite class="ltx_cite ltx_citemacro_citet">
      Guo et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib141" title="">
        141
       </a>
      </sup>
     </cite>
     argue that manually curating structured datasets is a sub-optimal, time-consuming, and labor-intensive task. Therefore, they automated data extraction and annotation from scientific papers using
     <span class="ltx_text ltx_font_typewriter" id="S3.SS6.p2.6.1">
      ChemDataExtractor
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS6.p2.6.1.1.1">
        <a class="ltx_ref" href="#bib.bib339" title="">
         339
        </a>
       </sup>
      </cite>
     </span>
     and their in-house annotation tool
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.2.1">
       <a class="ltx_ref" href="#bib.bib340" title="">
        340
       </a>
      </sup>
     </cite>
     .
ChemBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.3.1">
       <a class="ltx_ref" href="#bib.bib141" title="">
        141
       </a>
      </sup>
     </cite>
     was pretrained using a BERT model to encode chemical reaction information, followed by fine-tuning a NER head. ChemBERT outperformed other models such as BERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.4.1">
       <a class="ltx_ref" href="#bib.bib105" title="">
        105
       </a>
      </sup>
     </cite>
     and BioBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.5.1">
       <a class="ltx_ref" href="#bib.bib143" title="">
        143
       </a>
      </sup>
     </cite>
     in the product extraction task, presenting an improvement of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.1.m1.1">
      <semantics id="S3.SS6.p2.1.m1.1a">
       <mo id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     6% in precision. For the product role labeling task – given the extracted compound, which role does it play in the reaction? –, it increases precision in
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.2.m2.1">
      <semantics id="S3.SS6.p2.2.m2.1a">
       <mo id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     5% when compared to the same models.
The increase in performance might be due to training in a narrower domain, which allows the model to learn specific traits from the data instead of being more generalist.
MatSciBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.6.1">
       <a class="ltx_ref" href="#bib.bib132" title="">
        132
       </a>
      </sup>
     </cite>
     and MaterialsBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.7.1">
       <a class="ltx_ref" href="#bib.bib139" title="">
        139
       </a>
      </sup>
     </cite>
     followed a similar strategy, and BatteryBERT,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.8.1">
       <a class="ltx_ref" href="#bib.bib197" title="">
        197
       </a>
      </sup>
     </cite>
     similarly outperformed original BERT models with application to specific battery tasks.
     <cite class="ltx_cite ltx_citemacro_citet">
      Gupta et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib132" title="">
        132
       </a>
      </sup>
     </cite>
     developed MatSciBERT by fine-tuning SciBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.9.1">
       <a class="ltx_ref" href="#bib.bib134" title="">
        134
       </a>
      </sup>
     </cite>
     on the Material Science Corpus (MSC), their curated dataset focused on materials extracted from Elsevier’s scientific papers. MatSciBERT shows an accuracy improvement of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.3.m3.1">
      <semantics id="S3.SS6.p2.3.m3.1a">
       <mo id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     3% compared to SciBERT on classifying an article subject based on its abstract.
Differently, MaterialsBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.10.1">
       <a class="ltx_ref" href="#bib.bib139" title="">
        139
       </a>
      </sup>
     </cite>
     fine-tuned PubMedBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p2.6.11.1">
       <a class="ltx_ref" href="#bib.bib133" title="">
        133
       </a>
      </sup>
     </cite>
     on 2.4M abstracts extracted from material science articles using a masked training approach. MaterialsBERT was then used to generate a contextual encoded representation of the abstract to train a named entity relationship (NER) model. Reporting an increase in precision on their test dataset of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.4.m4.1">
      <semantics id="S3.SS6.p2.4.m4.1a">
       <mo id="S3.SS6.p2.4.m4.1.1" xref="S3.SS6.p2.4.m4.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.4.m4.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.4.m4.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.4.m4.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     1%,
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.5.m5.1">
      <semantics id="S3.SS6.p2.5.m5.1a">
       <mo id="S3.SS6.p2.5.m5.1.1" xref="S3.SS6.p2.5.m5.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.5.m5.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.5.m5.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.5.m5.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     2%, and
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p2.6.m6.1">
      <semantics id="S3.SS6.p2.6.m6.1a">
       <mo id="S3.SS6.p2.6.m6.1.1" xref="S3.SS6.p2.6.m6.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p2.6.m6.1b">
        <csymbol cd="latexml" id="S3.SS6.p2.6.m6.1.1.cmml" xref="S3.SS6.p2.6.m6.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p2.6.m6.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     10% for PubMedBERT, MatBERT, and ChemBERT, respectively.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS6.p3">
    <p class="ltx_p" id="S3.SS6.p3.1">
     Considerable effort was also employed to develop such models focused on biology tasks.
They follow a similar trend of training an LLM model on a large training corpus like Wikipedia, scientific data extracted from a scientific database, and textbooks, followed by fine-tuning specific downstream tasks.
     <cite class="ltx_cite ltx_citemacro_citet">
      Shin et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib142" title="">
        142
       </a>
      </sup>
     </cite>
     pretrained different Megatron-LM
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib341" title="">
        341
       </a>
      </sup>
     </cite>
     , another BERT-like LLM, models with different sizes, originating the BioMegatron family of models. Considering models with 345M, 800M, and 1.2B parameters and vocabularies with 30k and 50k tokens, they compared the influence of model size on its performance. These models were pretrained using abstracts from the PubMed dataset and full-text scientific articles from PubMed Central (PMC), similar to BioBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib143" title="">
        143
       </a>
      </sup>
     </cite>
     . The largest 1.2B training started from a
     <span class="ltx_text ltx_font_typewriter" id="S3.SS6.p3.1.3">
      BERT-uncased
     </span>
     checkpoint to save time. Surprisingly, the largest model did not perform better than the smaller ones. In fact, the 345M parameters using the 50k tokens vocabulary consistently outperformed other models in NER and relation extraction (RE) tasks. Interestingly, the model size seems relevant for the SQuAD
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.4.1">
       <a class="ltx_ref" href="#bib.bib342" title="">
        342
       </a>
      </sup>
     </cite>
     dataset, showing that LLMs trained on small, domain-specific datasets may lack generalization abilities.
BioBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.5.1">
       <a class="ltx_ref" href="#bib.bib143" title="">
        143
       </a>
      </sup>
     </cite>
     used data from Wikipedia, textbooks, PubMed abstracts, and PMC full-text corpus to pretrain a BERT architecture. Outperforming the original BERT in all benchmarks tested and even SOTA models in some benchmarks (NCBI disease, 2010 i2b2/VA, BC5CDR, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800).
     <cite class="ltx_cite ltx_citemacro_citet">
      Peng et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib343" title="">
        343
       </a>
      </sup>
     </cite>
     developed a multi-task BERT evaluated on the Biomedical Language Understanding Evaluation (BLUE)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.6.1">
       <a class="ltx_ref" href="#bib.bib135" title="">
        135
       </a>
      </sup>
     </cite>
     benchmark called BlueBERT. The model was pretrained on PubMed abstracts and MIMIC-III
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.7.1">
       <a class="ltx_ref" href="#bib.bib336" title="">
        336
       </a>
      </sup>
     </cite>
     and fine-tuned on BLUE tasks, achieving similar results as BioBERT in multiple benchmarks.
PubMedBERT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p3.1.8.1">
       <a class="ltx_ref" href="#bib.bib133" title="">
        133
       </a>
      </sup>
     </cite>
     , following the approach adopted in SciBERT, developed a domain-specific vocabulary. They used 14M abstracts extracted from PubChem to pretrain PubMedBERT. In addition, they curated and grouped multiple biomedical datasets to develop BLURB, their comprehensive benchmark for biomedical NLP tasks, such as NER, sentence similarity, document classification, and question-answering.
     <cite class="ltx_cite ltx_citemacro_citet">
      Gu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib133" title="">
        133
       </a>
      </sup>
     </cite>
     showed that PubMedChem considerably outperformed other LLMs in the BLURB benchmark, mainly in PubMedQA and BioQSA datasets. The second best-evaluated model in these datasets was BioBERT, evidencing the importance of domain-specific knowledge to high-performance LLMs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS6.p4">
    <p class="ltx_p" id="S3.SS6.p4.1">
     The models discussed previously in the section use an encoder-only LLM to encode information from scientific data. The learned encoded representation is then further used to make predictions by feeding it to simpler models. A different approach involves training a decoder model. Using a decoder model aims to address the lack of generation ability of the encoder-only models. By implementing a decoder, the model can address a different group of generative tasks, such as question-answering and document classification, using generated labels instead of following a pre-defined set of possible classes.
Darwin
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib124" title="">
        124
       </a>
      </sup>
     </cite>
     aims to create domain-specific LLMs for natural science by first fine-tuning LLaMA-7B on FAIR, a general Question-Answering (QA) dataset, and then fine-tuning it on scientific QA datasets. Scientific QA instructions were obtained from SciQ
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib344" title="">
        344
       </a>
      </sup>
     </cite>
     and generated by the authors using the Scientific Instruction Generation (SIG) model also developed by the authors. Their SIG model is fine-tuned from Vicuna-7B to convert full-text scientific papers into a set of QA pairs. This approach presented an improved performance of DARWIN in several regression and classification benchmarks. Interestingly, Using LLaMA-7B fine-tuned on FAIR only – with no scientific-specific QA fine-tuning – led to similar results in six out of nine benchmarks with metrics available for both models.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS6.p5">
    <p class="ltx_p" id="S3.SS6.p5.1">
     In biology, BioGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p5.1.1.1">
       <a class="ltx_ref" href="#bib.bib127" title="">
        127
       </a>
      </sup>
     </cite>
     pretrained a GPT-2 model architecture using 15M abstracts from PubChem corpus. BioGPT was evaluated on four tasks across five benchmarks: end-to-end relation extraction on BC5CDR, KD-DTI, and DDI, question answering on PubMedQA, document classification on HoC, and text generation on the previously listed benchmarks. fine-tuning BioGPT on downstream tasks (except text generation) showed its superiority against other encoder-only LLMs in the biomedical domain, such as BioBERT and PubMedBERT. Focusing on text generation, the authors qualitatively compared BioGPT and GPT-2 generations. They conclude that BioGPT demonstrates better text generation. However, no metric was used, making the comparison difficult to measure.
With a similar idea,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib125" title="">
        125
       </a>
      </sup>
     </cite>
     first pretrained an LLaMA2 model with the MedC-k dataset, consisting of 4.8M academic papers and 30K textbooks. In the sequence, the model was aligned through instruction tuning using the MedC-I dataset, a set of medical QA problems. PMC-LLaMA
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p5.1.2.1">
       <a class="ltx_ref" href="#bib.bib125" title="">
        125
       </a>
      </sup>
     </cite>
     outperforms LLaMa-2 and ChatGPT in multiple biomedical QA benchmarks despite its size
     <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS6.p5.1.m1.1">
      <semantics id="S3.SS6.p5.1.m1.1a">
       <mo id="S3.SS6.p5.1.m1.1.1" xref="S3.SS6.p5.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.SS6.p5.1.m1.1b">
        <csymbol cd="latexml" id="S3.SS6.p5.1.m1.1.1.cmml" xref="S3.SS6.p5.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS6.p5.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     10 times smaller. Interestingly, the authors show that the performance of PMC-LLaMA on MedQA
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p5.1.3.1">
       <a class="ltx_ref" href="#bib.bib345" title="">
        345
       </a>
      </sup>
     </cite>
     , MedMCQA
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p5.1.4.1">
       <a class="ltx_ref" href="#bib.bib346" title="">
        346
       </a>
      </sup>
     </cite>
     , and PubMedQA
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS6.p5.1.5.1">
       <a class="ltx_ref" href="#bib.bib347" title="">
        347
       </a>
      </sup>
     </cite>
     benchmarks progressively increases as more knowledge is injected, the model size is increased, and more instructions are included in instruction tuning.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS7">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.7
    </span>
    The use of ChatGPT in Chemistry
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS7.p1">
    <p class="ltx_p" id="S3.SS7.p1.1">
     With the rise of ChatGPT, we review here how many researchers have wanted to test the capability of such an accessible decoder-only LLM.
     <cite class="ltx_cite ltx_citemacro_citet">
      Castro Nascimento and Pimentel
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib348" title="">
        348
       </a>
      </sup>
     </cite>
     wrote the first notable paper on ChatGPT’s impact on Chemistry. The authors emphasize that LLMs, trained on extensive, uncurated datasets potentially containing errors or secondary sources, may include inaccuracies limiting their ability to predict chemical properties or trends. The paper highlighted that while LLMs could generate seemingly valid responses, they lacked true reasoning or comprehension abilities and would perpetuate existing errors from their training data. However, the authors suggested that these limitations could be addressed in the future.
The work serves as a benchmark to qualitatively assess improvements in generative pretrained transformers. For example, five tasks were given to ChatGPT (GPT-3). The accuracy for converting compound names to SMILES representations and vice versa was about 27%, with issues in differentiating alkanes and alkenes, benzene and cyclohexene, or
     <span class="ltx_text ltx_font_italic" id="S3.SS7.p1.1.1">
      cis
     </span>
     and
     <span class="ltx_text ltx_font_italic" id="S3.SS7.p1.1.2">
      trans
     </span>
     isomers. ChatGPT found reasonable octanol-water partition coefficients with a 31% mean relative error, and a 58% hit rate for coordination compounds’ structural information. It had a 100% hit rate for polymer water solubility and a 60% hit rate for molecular point groups. Understandably, the best accuracies were achieved with widely recognized topics. The authors concluded that neither experimental nor computational chemists should fear the development of LLMs, nor task automation. Instead, they encouraged the improvement of dedicated AI tools for specific problems, with their integration into research, as facilitators.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS7.p2">
    <p class="ltx_p" id="S3.SS7.p2.1">
     The sole use of ChatGPT for chemistry and any apparent advancement in chemistry seems somewhat limited.
     <cite class="ltx_cite ltx_citemacro_citet">
      Humphry and Fuller
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib349" title="">
        349
       </a>
      </sup>
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Emenike and Emenike
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib350" title="">
        350
       </a>
      </sup>
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Fergus et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib351" title="">
        351
       </a>
      </sup>
     </cite>
     address the use of ChatGPT in chemical education. There are also a few papers that describe the use of ChatGPT on a specific area of chemical research, namely with the synthesis and functional optimization of Metal Organic Framewoks (MOFs), merging computational modeling with empirical chemistry research.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS7.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib352" title="">
        352
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib353" title="">
        353
       </a>
      </sup>
     </cite>
     There is also some overlap with fine-tuned GPT models,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS7.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib354" title="">
        354
       </a>
      </sup>
     </cite>
     within this field.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS7.p2.1.3.1">
       <a class="ltx_ref" href="#bib.bib355" title="">
        355
       </a>
      </sup>
     </cite>
     <cite class="ltx_cite ltx_citemacro_citet">
      Deb et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib356" title="">
        356
       </a>
      </sup>
     </cite>
     provide a thorough assessment of ChatGPT’s effectiveness in material science through various tasks in computational material science, though the perspective is subjective. ChatGPT assisted in computational materials science by providing crystal space groups for simple compounds, suggesting inputs for simulations, refining analysis with specific rules, and identifying resources. The authors highlight ChatGPT’s potential to write code for enhancing processes and as a tool for those without prior expertise, particularly in catalyst development for CO2 capture.
We highlight three points about using ChatGPT alone. First, reliable outputs require detailed and accurate input, as
     <cite class="ltx_cite ltx_citemacro_citet">
      Deb et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib356" title="">
        356
       </a>
      </sup>
     </cite>
     showed, where ChatGPT struggled to mine or predict crystal structures. Second, standardized methods for reproducing and evaluating GPT work are still unclear. Third, this work naturally leads to the inference that additional chemical tools or agents are needed to achieve more complex thinking, as described by Bloom’s Taxonomy.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S3.SS7.p2.1.4.1">
       <a class="ltx_ref" href="#bib.bib357" title="">
        357
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib358" title="">
        358
       </a>
      </sup>
     </cite>
     Bloom’s taxonomy categorizes educational goals into hierarchical levels: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. Remembering involves recalling facts, Understanding means restating a concept, and Applying uses a theory in a new situation. Analyzing breaks down a mechanism into parts to understand connections and differences. Evaluating involves making value judgments, and Creating entails building new structures from diverse elements.
Currently, LLMs and autonomous agents are limited in replicating higher-level thinking compared to human understanding. To better quantify their capabilities in this area, we therefore recommend adopting a quality metric based on Bloom’s taxonomy. This educational framework can serve as an effective tool for evaluating the sophistication of LLMs and autonomous agents, especially when tackling complex chemical challenges. Bloom’s taxonomy would thereby provide a structured approach to assess higher-order thinking and reasoning.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S3.SS7.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.7.1
     </span>
     Automation
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS7.SSS1.p1">
     <p class="ltx_p" id="S3.SS7.SSS1.p1.1">
      The evolution of artificial intelligence in chemistry has fueled the potential for automating scientific processes. For example, in 2019,
      <cite class="ltx_cite ltx_citemacro_citet">
       Coley et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib359" title="">
         359
        </a>
       </sup>
      </cite>
      developed a flow-based synthesis robot proposing synthetic routes and assembling flow reaction systems, tested on medically relevant molecules, and in 2020,
      <cite class="ltx_cite ltx_citemacro_citet">
       Gromski et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib360" title="">
         360
        </a>
       </sup>
      </cite>
      provided a useful exploration on how chemical robots could outperform humans when executing chemical reactions and analyses. They developed the Chemputer, a programmable batch synthesis robot handling reactions like peptide synthesis and Suzuki coupling. In 2021,
      <cite class="ltx_cite ltx_citemacro_citet">
       Grisoni et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib361" title="">
         361
        </a>
       </sup>
      </cite>
      combined deep learning-based molecular generation with on-chip synthesis and testing. The Automated Chemical Design (ACD) framework by
      <cite class="ltx_cite ltx_citemacro_citet">
       Goldman et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib362" title="">
         362
        </a>
       </sup>
      </cite>
      provides a useful taxonomy for automation and experimental integration levels.
Thus, automation promises to enhance productivity through increased efficiency, error reduction, and the ability to handle complex problems, as described in several excellent reviews regarding automation in chemistry,
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS7.SSS1.p1.1.1.1">
        <a class="ltx_ref" href="#bib.bib363" title="">
         363
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib364" title="">
         364
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib365" title="">
         365
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib366" title="">
         366
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib367" title="">
         367
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib368" title="">
         368
        </a>
        ,
        <a class="ltx_ref" href="#bib.bib369" title="">
         369
        </a>
       </sup>
      </cite>
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS7.SSS1.p2">
     <p class="ltx_p" id="S3.SS7.SSS1.p2.1">
      This increased productivity may be the only possible approach to exploring the vastness of all chemical space.
To fully leverage AI in property prediction, inverse design, and synthesis prediction, it must be integrated with automated synthesis, purification, and testing. This automation should be high-throughput and driven by AI-based autonomous decision-making (sometimes called “lights-out” automation).
      <cite class="ltx_cite ltx_citemacro_citet">
       Janet et al.
       <sup class="ltx_sup">
        <a class="ltx_ref" href="#bib.bib364" title="">
         364
        </a>
       </sup>
      </cite>
      highlighted challenges in multi-step reactions with intermediate purifications, quantifying uncertainty, and the need for standardized recipe formats. They also stated the limitations of automated decision-making.
Organa
      <cite class="ltx_cite ltx_citemacro_cite">
       <sup class="ltx_sup" id="S3.SS7.SSS1.p2.1.1.1">
        <a class="ltx_ref" href="#bib.bib370" title="">
         370
        </a>
       </sup>
      </cite>
      addresses some of these challenges. It can significantly reduce physical workload and improve users’ lab experience by automating diverse common lab routine tasks such as solubility assessment, pH measurement, and recrystallization. Organa interacts with the user through text and audio. The commands are converted into a detailed LLM prompt and used to map the goal to the robot’s instructions. Interestingly, Organa is also capable of reasoning over the instructions, giving feedback about the experiments, and producing a written report with the results.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS7.SSS1.p3">
     <p class="ltx_p" id="S3.SS7.SSS1.p3.1">
      Other limitations exist, like machines’ restriction to pre-defined instructions, their inability to originate new materials, and the lower likelihood of lucky discoveries. Yet, when dedicated tools can be connected to address each step of an automated chemical design, these limitations can be systematically addressed through advancements in LLMs and autonomous agents, discussed in the next section.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   LLM-based autonomous agents
  </h2>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    The term “agent” originates in philosophy, referring to entities capable of making decisions
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S4.p1.1.1.1">
      <a class="ltx_ref" href="#bib.bib371" title="">
       371
      </a>
     </sup>
    </cite>
    . Hence, in artificial intelligence, an “agent” is a system that can perceive its environment, make decisions, and act upon them in response to external stimuli
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S4.p1.1.2.1">
      <a class="ltx_ref" href="#bib.bib372" title="">
       372
      </a>
     </sup>
    </cite>
    . Language has enabled humans to decide and act to make progress in response to the environment and its stimuli, and so LLMs are naturally ideal for serving as the core of autonomous agents. Thus, in agreement with
    <cite class="ltx_cite ltx_citemacro_citet">
     Gao et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib373" title="">
       373
      </a>
     </sup>
    </cite>
    , we define a “language agent” as a model or program (typically based on LLMs) that receives an observation from its environment and executes an action in this environment. Here, environment means a set of tools and a task.
Hence, “LLM-based autonomous agents” refer to language agents whose core is based on an LLM model.
Comprehensive analyses of these agents are available in the literature
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S4.p1.1.3.1">
      <a class="ltx_ref" href="#bib.bib374" title="">
       374
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib372" title="">
       372
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib373" title="">
       373
      </a>
     </sup>
    </cite>
    , but this section highlights key aspects to prepare the reader for future discussions.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    There is no agreed definition of the nomenclature to be used to discuss agents.
For instance,
    <cite class="ltx_cite ltx_citemacro_citet">
     Gao et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib373" title="">
       373
      </a>
     </sup>
    </cite>
    created a classification scheme that aims to group agents by their autonomy in biological research. This means a level 0 agent has no autonomy and can only be used as a tool, while a level 3 agent can independently create hypotheses, design experiments, and reason.
Another previously suggested framework highlights agents’ construction, application, and evaluation as a parallel between agents and neural networks.
Following this perspective,
    <cite class="ltx_cite ltx_citemacro_citet">
     Wang et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib374" title="">
       374
      </a>
     </sup>
    </cite>
    categorizes agent components into four modules: profiling, memory, planning, and action. In contrast,
    <cite class="ltx_cite ltx_citemacro_citet">
     Weng
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib375" title="">
       375
      </a>
     </sup>
    </cite>
    also identifies four elements — memory, planning, action, and tools — but with a different emphasis. Meanwhile,
    <cite class="ltx_cite ltx_citemacro_citet">
     Xi et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib372" title="">
       372
      </a>
     </sup>
    </cite>
    proposes a division into three components: brain, perception, and action, integrating profiling, memory, and planning within the brain component, where the brain is typically an LLM.
Recently,
    <cite class="ltx_cite ltx_citemacro_citet">
     Sumers et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib376" title="">
       376
      </a>
     </sup>
    </cite>
    proposed Cognitive Architectures for Language Agents (CoALA), a conceptual framework to generalize and ease the design of general-purpose cognitive language agents. In their framework, a larger cognitive architecture composed of modules and processes is defined. CoALA defines a memory, decision-making, and core processing module, in addition to an action space composed of both internal and external tools. While internal tools mainly interact with the memory to support decision-making, external tools compose the environment, as illustrated in Figure
    <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    .
Given a task that initiates the environment, the “decision process” runs continuously in a loop, receiving observations and executing actions until the task is completed. For more details, read Reference [
    <cite class="ltx_cite ltx_citemacro_citenum">
     <a class="ltx_ref" href="#bib.bib376" title="">
      376
     </a>
    </cite>
    ].
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F5">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="S4.F5.g1" src="/html/2407.01603/assets/figs/agents3.png" width="592"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 5:
    </span>
    Agent’s architecture as defined in this review. According to our definition, an agent is composed of a central program (typically an LLM and the code to implement the agent’s dynamic behavior) and the agent modules. The agent continuously receives observations from the environment and decides which action should be executed to complete the task given to it. Here, we define the agent as the set of elements whose decision is trainable, that is, the LLM, the agent code, the decision process, and the agent modules. Given a task, the agent uses the agent modules (memory, reasoning, planning, profiling) and the LLM to decide which action should be executed. This action is executed by calling a tool from the environment. After the action is executed, an observation is produced and fed back to the agent. The agent can use perception to receive inputs in different modalities from the environment. A) Description of agent modules, B) illustration of the agent architecture, C) illustration of the environment components, D) description of tools elements present in the environment.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    In this review, we define an autonomous agent system as a model (typically an LLM) that continuously receives observations from the environment and executes actions to complete a provided task, as described by
    <cite class="ltx_cite ltx_citemacro_citet">
     Gao et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib373" title="">
       373
      </a>
     </sup>
    </cite>
    . Nevertheless, in contrast to CoALA
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S4.p3.1.1.1">
      <a class="ltx_ref" href="#bib.bib373" title="">
       373
      </a>
     </sup>
    </cite>
    , we will rename “internal tools” as “agent modules” and “external tools” simply as “tools”, for clarity. The agent consists of trainable decision-making components such as the LLM itself, policy, memory, and reasoning scheme. In contrast, the environment comprises non-trainable elements like the task to be completed, Application Programming Interface (API) access, interfaces with self-driving labs, dataset access, and execution of external code.
By referring to decision-making components as agent modules, we emphasize their inclusion as parts of the agent. By referring to non-trainable elements as tools, we highlight their role as part of the environment. We discuss six main types of actions. As shown in Figure
    <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    , four of the six, memory, planning, reasoning, and profiling are agent modules. The remaining two actions (or tools) and perception are part of the environment. Since the perception is how the agent interacts with the environment and is not a trainable decision, we therefore included it as part of the environment.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Memory module
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     The role of the memory module is to store and recall information from past interactions and experiences to inform future decisions and actions. There are multiple types of memory in agents, namely sensory memory, short-term memory, and long-term memory.
A major challenge in using agents is the limited context window, which restricts the amount of in-context information and can lead to information loss, thereby impacting the effectiveness of short-term and long-term memory. Solutions involve summarizing memory content
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib377" title="">
        377
       </a>
      </sup>
     </cite>
     , compressing memories into vectors
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib378" title="">
        378
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib379" title="">
        379
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib380" title="">
        380
       </a>
      </sup>
     </cite>
     , and utilizing vector databases
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib381" title="">
        381
       </a>
      </sup>
     </cite>
     or combinations thereof
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.4.1">
       <a class="ltx_ref" href="#bib.bib382" title="">
        382
       </a>
      </sup>
     </cite>
     , with various databases available such as ChromaDB, FAISS, Pinecone, Weaviate, Annoy, and ScaNN.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.5.1">
       <a class="ltx_ref" href="#bib.bib383" title="">
        383
       </a>
      </sup>
     </cite>
     Addressing these challenges to enhance agent memory continues to be a significant area of research
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.6.1">
       <a class="ltx_ref" href="#bib.bib384" title="">
        384
       </a>
      </sup>
     </cite>
     .
Sensory, or procedural, memory is knowledge embedded into the model’s parameters during pretraining and/or in heuristics implemented into the agent’s code.
Short-term, or working, memory includes the agent’s finite knowledge during a task, incorporating interaction history and techniques like in-context learning
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.7.1">
       <a class="ltx_ref" href="#bib.bib385" title="">
        385
       </a>
      </sup>
     </cite>
     (ICL), which leverages the limited input’s context length for information retention.
Long-term memory involves storing information externally, typically through an embedded vector representation in an external database. In the original CoALA
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS1.p1.1.8.1">
       <a class="ltx_ref" href="#bib.bib373" title="">
        373
       </a>
      </sup>
     </cite>
     paper, long-term memory is further divided into two different types of long-term memory: episodic, which registers previous experiences, and semantic, which stores general information about the world.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Planning and reasoning modules
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     The planning and reasoning module is made of two components. Planning involves identifying a sequence of actions required to achieve a specified goal. In the context of language agents, this means generating steps or strategies that the model can follow to solve a problem or answer a question, which can be enhanced with retrieval from previous experiences,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib386" title="">
        386
       </a>
      </sup>
     </cite>
     and from feedback from post-execution reasoning
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib387" title="">
        387
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib388" title="">
        388
       </a>
      </sup>
     </cite>
     .
Reasoning refers to the process of drawing conclusions or making decisions based on available information and logical steps. For example, there are studies that demonstrate the benefits of LLM reasoning for question answering, where new context tokens can be integrated in a step-by-step way to guide the model towards more accurate answers
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib389" title="">
        389
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib390" title="">
        390
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib391" title="">
        391
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib392" title="">
        392
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib394" title="">
        394
       </a>
      </sup>
     </cite>
     . One popular reasoning strategy is Chain-of-Thought (CoT),
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.4.1">
       <a class="ltx_ref" href="#bib.bib391" title="">
        391
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib395" title="">
        395
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib396" title="">
        396
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib98" title="">
        98
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib397" title="">
        397
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib398" title="">
        398
       </a>
      </sup>
     </cite>
     a reasoning strategy which substantially boosts QA performance by generating intermediate reasoning steps. CoT thereby reduces hallucinations and enhances interpretability, as demonstrated by improved results in models like PaLM
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.5.1">
       <a class="ltx_ref" href="#bib.bib399" title="">
        399
       </a>
      </sup>
     </cite>
     and GPT-3 with benchmarks like GSM8K
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.6.1">
       <a class="ltx_ref" href="#bib.bib400" title="">
        400
       </a>
      </sup>
     </cite>
     , SVAMPs
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.7.1">
       <a class="ltx_ref" href="#bib.bib401" title="">
        401
       </a>
      </sup>
     </cite>
     , and MAWPS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p1.1.8.1">
       <a class="ltx_ref" href="#bib.bib402" title="">
        402
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     In advanced reasoning, final tasks are often decomposed into intermediary ones using a cascading approach, similar to Zero-shot-CoT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib390" title="">
        390
       </a>
      </sup>
     </cite>
     and RePrompt
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib387" title="">
        387
       </a>
      </sup>
     </cite>
     .
However, while CoT is considered as single-path reasoning, CoT extensions like Tree-of-Thoughts
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.3.1">
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
      </sup>
     </cite>
     , Graph-of-Thoughts
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.4.1">
       <a class="ltx_ref" href="#bib.bib403" title="">
        403
       </a>
      </sup>
     </cite>
     , Self-consistent CoT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.5.1">
       <a class="ltx_ref" href="#bib.bib392" title="">
        392
       </a>
      </sup>
     </cite>
     , and Algorithm-of-Thoughts
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.6.1">
       <a class="ltx_ref" href="#bib.bib404" title="">
        404
       </a>
      </sup>
     </cite>
     offer multi-path reasoning. Furthermore, other models have pitted multiple agents against each other to debate or discuss various reasoning paths,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.7.1">
       <a class="ltx_ref" href="#bib.bib405" title="">
        405
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib406" title="">
        406
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib407" title="">
        407
       </a>
      </sup>
     </cite>
     while others use external planners to create plans
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.8.1">
       <a class="ltx_ref" href="#bib.bib408" title="">
        408
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib409" title="">
        409
       </a>
      </sup>
     </cite>
     .
A feedback step during the execution of the plan was a further extension of the CoT ideas; this enables agents to refine their actions based on environmental responses adaptively, which is crucial for complex tasks.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p2.1.9.1">
       <a class="ltx_ref" href="#bib.bib410" title="">
        410
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib411" title="">
        411
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     Another interesting reasoning scheme is the Chain-of-Verification(CoVe),
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib388" title="">
        388
       </a>
      </sup>
     </cite>
     where once an answer is generated, another LLM is prompted to generate a set of verification questions to check for agreement between the original answer and the answers to the verification questions such that the final answer can be refined.
The ReAct
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
      </sup>
     </cite>
     – Reason+Act – model proposes adding an observation step after acting. This means the LLM first reasons about the task and determines the necessary step for its execution, it performs the action and then observes the action’s result. Reasoning on that result, it can subsequently perform the following step.
Similarly, Reflexion
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p3.1.3.1">
       <a class="ltx_ref" href="#bib.bib98" title="">
        98
       </a>
      </sup>
     </cite>
     also implements a reasoning step after executing an action. However, Reflexion implements an evaluator and self-reflection LLMs to not only reason about each step but also to evaluate the current trajectory the agent is following using a long-term memory module.
As the context increases, it may become challenging for agents to deal with the long prompt. Aiming to solve this issue, the Chain-of-Agents (CoA)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS2.p3.1.4.1">
       <a class="ltx_ref" href="#bib.bib412" title="">
        412
       </a>
      </sup>
     </cite>
     extends reasoning schemes that leverage multi-agent collaboration to reason over long contexts. This framework employs workers and manager agents to process and synthesize information to generate the final response. CoA demonstrated improvements of up to 10% when compared against an RAG baseline.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     ReAct and Reflexion are closed-ended approaches where the agent starts with all the tools and must determine which to use. To address more open-world challenges,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib413" title="">
        413
       </a>
      </sup>
     </cite>
     introduced the Describe, Explain, Plan, and Select (DEPS) method, which extends this approach. Lastly, human inputs can also be used to provide feedback to the agent. Providing feedback using a human-in-the-loop approach is particularly interesting in fields where safety is a main concern.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Profiling module
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     LLMs can be configured to perform in specific roles, such as coders, professors, students, and domain experts, through a process known as profiling. Language agents can thus incorporate the profile through the LLM or through the agent code. The profiling approach involves inputting psychological characteristics to the agent, significantly impacting its decision-making process
     <cite class="ltx_cite ltx_citemacro_citep">
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib414" title="">
        414
       </a>
      </sup>
      ,
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib415" title="">
        415
       </a>
      </sup>
      ,
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib416" title="">
        416
       </a>
      </sup>
      ,
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib417" title="">
        417
       </a>
      </sup>
     </cite>
     . Profiling enables the creation of multi-agent systems that simulate societal interactions, with each agent embodying a unique persona within the group
     <cite class="ltx_cite ltx_citemacro_citep">
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib386" title="">
        386
       </a>
      </sup>
      ,
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib418" title="">
        418
       </a>
      </sup>
     </cite>
     . The most prevalent technique for profiling, called “handcrafting”, requires manually defining the agent’s profile, often through prompts or system messages
     <cite class="ltx_cite ltx_citemacro_citep">
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib419" title="">
        419
       </a>
      </sup>
      ,
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib420" title="">
        420
       </a>
      </sup>
     </cite>
     . While profiling can also be automated with LLMs
     <cite class="ltx_cite ltx_citemacro_citep">
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib421" title="">
        421
       </a>
      </sup>
     </cite>
     , that automation method may only be suited for generating large numbers of agents since it offers less control over their overall behavior. An interesting application of profiling is the development of agent sets that reflect demographic distributions
     <cite class="ltx_cite ltx_citemacro_citep">
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib422" title="">
        422
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Perception
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     Perception is an analog to the human sensory system, which interprets multimodal information such as text, images, or auditory data, transforming it into a format comprehensible by LLMs, as demonstrated by SAM
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib423" title="">
        423
       </a>
      </sup>
     </cite>
     , GPT4-V
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib424" title="">
        424
       </a>
      </sup>
     </cite>
     , LLaVa
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib425" title="">
        425
       </a>
      </sup>
     </cite>
     , Fuyu8B
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.4.1">
       <a class="ltx_ref" href="#bib.bib116" title="">
        116
       </a>
      </sup>
     </cite>
     , and BuboGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.5.1">
       <a class="ltx_ref" href="#bib.bib426" title="">
        426
       </a>
      </sup>
     </cite>
     . In our proposed architecture, the perception is responsible for converting the task and the observations to a data representation that can be understood by the agent. Moreover, advancements in LLMs have led to the development of even more versatile models, such as the any-to-any Next-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.6.1">
       <a class="ltx_ref" href="#bib.bib117" title="">
        117
       </a>
      </sup>
     </cite>
     and the any-to-text Macaw-LLM
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.7.1">
       <a class="ltx_ref" href="#bib.bib427" title="">
        427
       </a>
      </sup>
     </cite>
     . Employing multimodal LLMs, which are introduced above, in decision-making processes can simplify perception tasks for agents, with several studies exploring their use in autonomous systems
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS4.p1.1.8.1">
       <a class="ltx_ref" href="#bib.bib428" title="">
        428
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib429" title="">
        429
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.5
    </span>
    Tools
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p1">
    <p class="ltx_p" id="S4.SS5.p1.1">
     In our proposed definition (see Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4 LLM-based autonomous agents ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     b), tools or actions are part of the environment. The agent can interact with this environment by deciding which action to execute through the decision-making process. The set of all possible actions that can be selected is also known as the “action space”.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p2">
    <p class="ltx_p" id="S4.SS5.p2.1">
     The decision process is composed of three main steps: proposal, evaluation, and selection. During the proposal, one or more action candidates are selected using reasoning
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
      </sup>
     </cite>
     , code structures
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib430" title="">
        430
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib386" title="">
        386
       </a>
      </sup>
     </cite>
     , or simply by selecting every tool available
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p2.1.3.1">
       <a class="ltx_ref" href="#bib.bib431" title="">
        431
       </a>
      </sup>
     </cite>
     .
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p2.1.4.1">
       <a class="ltx_ref" href="#bib.bib389" title="">
        389
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib432" title="">
        432
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib392" title="">
        392
       </a>
      </sup>
     </cite>
     The evaluation process consists of evaluating each selected action according to some metric to predict which action would bring more value to the agent. Lastly, the action is selected and executed.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p3">
    <p class="ltx_p" id="S4.SS5.p3.1">
     Given that pretrained parameters (sensory memory) are limited, the model must use tools for complex tasks in order to provide reliable answers.
However, LLMs need to learn how to interact with the action space and how and when to use those tools most accurately.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib433" title="">
        433
       </a>
      </sup>
     </cite>
     LLMs can be pretrained or fine-tuned with examples of tool use, enabling them to operate tools and directly retrieve tool calls from sensory memory during a zero-shot generation.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib434" title="">
        434
       </a>
      </sup>
     </cite>
     Recent studies investigate this approach, particularly focusing on open-source LLMs.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p3.1.3.1">
       <a class="ltx_ref" href="#bib.bib435" title="">
        435
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib436" title="">
        436
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib437" title="">
        437
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p4">
    <p class="ltx_p" id="S4.SS5.p4.1">
     As foundational AI models become more advanced, their abilities can be expanded. It was shown that general-purpose foundation models can reason and select tools even with no fine-tuning. For example, MRKL
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib438" title="">
        438
       </a>
      </sup>
     </cite>
     (pronounced “miracle”) implements an extendable set of specialized tools known as neuro-symbolic modules and a smart “router” system to retrieve the best module based on the textual input. These neuro-symbolic modules are designed to handle specific tasks or types of information, equipped with built-in capabilities and task-relevant knowledge. This pre-specialization allows the model to perform domain-specific tasks without needing a separate, domain-specific dataset.
This design addresses the problem of LLMs lacking domain-specific knowledge and eliminates the need for the costly and time-consuming LLM fine-tuning step, using specialized data annotation.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib439" title="">
        439
       </a>
      </sup>
     </cite>
     The router can receive support from a reasoning strategy to help select the tools
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.3.1">
       <a class="ltx_ref" href="#bib.bib439" title="">
        439
       </a>
      </sup>
     </cite>
     or follow a previously created plan
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.4.1">
       <a class="ltx_ref" href="#bib.bib413" title="">
        413
       </a>
      </sup>
     </cite>
     . Recent advances have shown that LLMs can develop new tools of their own
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.5.1">
       <a class="ltx_ref" href="#bib.bib440" title="">
        440
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib441" title="">
        441
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib442" title="">
        442
       </a>
      </sup>
     </cite>
     , enabling agents to operate, as needed, in dynamic and unpredictable “open-worlds”, on unseen problems as illustrated by Voyager
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S4.SS5.p4.1.6.1">
       <a class="ltx_ref" href="#bib.bib430" title="">
        430
       </a>
      </sup>
     </cite>
     . This capability allows agents to evolve and improve continually.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   LLM-Based autonomous agents in scientific research
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    The previous section introduced key concepts relevant to any description of the development of autonomous agents. Here, we now focus on which agents were developed for scientific purposes, and ultimately for chemistry. Previous sections of this review have discussed how LLMs could be powerful in addressing challenges in molecular property prediction, inverse design, and synthesis prediction. When we consider the value of agents in chemistry and the ability to combine tools that, for example, search the internet for established synthetic procedures, look up experimental properties, and control robotic synthesis and characterization systems, we can see how autonomous agents powerfully align with the broader theme of automation, which will lead to an acceleration of chemical research and application.
   </p>
  </div>
  <figure class="ltx_table" id="S5.T4">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_table">
     Table 4:
    </span>
    Scientific LLM systems and agents. We identify the studies we classified as an agent with the icon
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.T4.113.1.1">
      <a class="ltx_ref" href="#bib.bib443" title="">
       443
      </a>
     </sup>
    </cite>
    and multi-agent systems with the icon
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.2.g2" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.3.g3" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
    .
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.4.g4" src="/html/2407.01603/assets/figs/letter-w.png" width="14"/>
    ,
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.5.g5" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
    , and
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.6.g6" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.T4.114.2.1">
      <a class="ltx_ref" href="#bib.bib444" title="">
       444
      </a>
     </sup>
    </cite>
    mean the agent bases his behavior on sensory, short, and long memory components, respectively. Besides the textual capabilities of LLM-based agents,
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.7.g7" src="/html/2407.01603/assets/figs/ear.png" width="14"/>
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.T4.115.3.1">
      <a class="ltx_ref" href="#bib.bib445" title="">
       445
      </a>
     </sup>
    </cite>
    and
    <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.8.g8" src="/html/2407.01603/assets/figs/view.png" width="14"/>
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.T4.116.4.1">
      <a class="ltx_ref" href="#bib.bib446" title="">
       446
      </a>
     </sup>
    </cite>
    mean the agent has additional audio and visual perception, respectively.
   </figcaption>
   <table class="ltx_tabular" id="S5.T4.112">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S5.T4.112.97.1">
      <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S5.T4.112.97.1.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.112.97.1.1.1">
        <span class="ltx_p" id="S5.T4.112.97.1.1.1.1">
         Agent
        </span>
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.112.97.1.2" style="padding-top:2pt;padding-bottom:2pt;">
       Memory
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.112.97.1.3" style="padding-top:2pt;padding-bottom:2pt;">
       Planning
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.112.97.1.4" style="padding-top:2pt;padding-bottom:2pt;">
       Reasoning
      </th>
      <th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S5.T4.112.97.1.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.112.97.1.5.1">
        <span class="ltx_p" id="S5.T4.112.97.1.5.1.1">
         Action
        </span>
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.112.97.1.6" style="padding-top:2pt;padding-bottom:2pt;">
       Release
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S5.T4.20.4">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T4.17.1.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.17.1.1.1">
        <span class="ltx_p" id="S5.T4.17.1.1.1.1">
         LLaMP
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.17.1.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib447" title="">
            447
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.17.1.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.19.3.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.18.2.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.19.3.3.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_border_t" id="S5.T4.20.4.5" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.20.4.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.20.4.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.20.4.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S5.T4.20.4.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.20.4.6.1">
        <span class="ltx_p" id="S5.T4.20.4.6.1.1">
         Tools for database access, literature search, and atomistic simulations
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.20.4.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.06
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.22.6">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.21.5.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.21.5.1.1">
        <span class="ltx_p" id="S5.T4.21.5.1.1.1">
         SGA
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.21.5.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib448" title="">
            448
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.21.5.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.22.6.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.22.6.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.22.6.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.22.6.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.22.6.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.22.6.5.1">
        <span class="ltx_p" id="S5.T4.22.6.5.1.1">
         Employ the LLMs in a optimization loop
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.22.6.6" style="padding-top:2pt;padding-bottom:2pt;">
       2024.05
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.27.11">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.24.8.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.24.8.2.2">
        <span class="ltx_p" id="S5.T4.24.8.2.2.2">
         CRISPR-GPT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.24.8.2.2.2.1.1">
           <a class="ltx_ref" href="#bib.bib449" title="">
            449
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.23.7.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.24.8.2.2.2.g2" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.25.9.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.25.9.3.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.26.10.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.26.10.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.26.10.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.27.11.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.27.11.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.27.11.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.27.11.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.27.11.6.1">
        <span class="ltx_p" id="S5.T4.27.11.6.1.1">
         Tool for gene editing experiments design
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.27.11.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.04
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.32.16">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.29.13.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.29.13.2.2">
        <span class="ltx_p" id="S5.T4.29.13.2.2.2">
         TAIS
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.29.13.2.2.2.1.1">
           <a class="ltx_ref" href="#bib.bib450" title="">
            450
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.28.12.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.29.13.2.2.2.g2" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.30.14.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.30.14.3.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.31.15.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.31.15.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.31.15.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.32.16.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.32.16.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.32.16.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.32.16.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.32.16.6.1">
        <span class="ltx_p" id="S5.T4.32.16.6.1.1">
         Tools for gene expression data analysis
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.32.16.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.02
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.37.21">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.33.17.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.33.17.1.1">
        <span class="ltx_p" id="S5.T4.33.17.1.1.1">
         ChemReasoner
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.33.17.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib451" title="">
            451
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.33.17.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.35.19.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.34.18.2.g1" src="/html/2407.01603/assets/figs/letter-w.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.35.19.3.g2" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.36.20.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.36.20.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.36.20.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.37.21.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.37.21.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.37.21.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.37.21.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.37.21.6.1">
        <span class="ltx_p" id="S5.T4.37.21.6.1.1">
         Tools for heuristic search, 3D structure generation, and prediction using GNNs
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.37.21.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.02
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.42.26">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.38.22.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.38.22.1.1">
        <span class="ltx_p" id="S5.T4.38.22.1.1.1">
         SciAgent
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.38.22.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib452" title="">
            452
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.38.22.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.40.24.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.39.23.2.g1" src="/html/2407.01603/assets/figs/letter-w.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.40.24.3.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.41.25.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.41.25.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.41.25.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.42.26.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.42.26.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.42.26.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.42.26.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.42.26.6.1">
        <span class="ltx_p" id="S5.T4.42.26.6.1.1">
         Trained Mistral for tool usage. Evaluated it using MathToolBench’s tools
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.42.26.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.02
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.46.30">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.46.30.5" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.46.30.5.1">
        <span class="ltx_p" id="S5.T4.46.30.5.1.1">
         STORM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.46.30.5.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib453" title="">
            453
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.44.28.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.43.27.1.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.44.28.2.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.45.29.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.45.29.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.45.29.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.46.30.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.46.30.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.46.30.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.46.30.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.46.30.6.1">
        <span class="ltx_p" id="S5.T4.46.30.6.1.1">
         Article writing using retrieval from multi-LLM conversations and pre-generated outline
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.46.30.7" style="padding-top:2pt;padding-bottom:2pt;">
       2024.02
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.47.31">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.47.31.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.47.31.2.1">
        <span class="ltx_p" id="S5.T4.47.31.2.1.1">
         <cite class="ltx_cite ltx_citemacro_citet">
          Völker et al.
          <sup class="ltx_sup">
           <a class="ltx_ref" href="#bib.bib454" title="">
            454
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.47.31.1" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.47.31.1.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.47.31.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.47.31.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.47.31.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.47.31.5.1">
        <span class="ltx_p" id="S5.T4.47.31.5.1.1">
         Regression with ICL and text retrieval
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.47.31.6" style="padding-top:2pt;padding-bottom:2pt;">
       2024.02
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.53.37">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.49.33.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.49.33.2.2">
        <span class="ltx_p" id="S5.T4.49.33.2.2.2">
         ProtAgent
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.49.33.2.2.2.1.1">
           <a class="ltx_ref" href="#bib.bib455" title="">
            455
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.48.32.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.49.33.2.2.2.g2" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.51.35.4" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.50.34.3.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.51.35.4.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.52.36.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.52.36.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.52.36.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.53.37.6" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.53.37.6.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.53.37.6.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.53.37.7" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.53.37.7.1">
        <span class="ltx_p" id="S5.T4.53.37.7.1.1">
         Tools for proteins information retrieval, analyzing,
         <span class="ltx_text ltx_font_italic" id="S5.T4.53.37.7.1.1.1">
          de novo
         </span>
         design, and 3D folded structure generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.53.37.8" style="padding-top:2pt;padding-bottom:2pt;">
       2024.01
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.60.44">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.56.40.3" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.56.40.3.3">
        <span class="ltx_p" id="S5.T4.56.40.3.3.3">
         Organa
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.56.40.3.3.3.1.1">
           <a class="ltx_ref" href="#bib.bib370" title="">
            370
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.54.38.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.55.39.2.2.2.g2" src="/html/2407.01603/assets/figs/ear.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.56.40.3.3.3.g3" src="/html/2407.01603/assets/figs/view.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.58.42.5" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.57.41.4.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.58.42.5.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.59.43.6" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.59.43.6.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.59.43.6.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.60.44.7" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.60.44.7.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.60.44.7.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.60.44.8" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.60.44.8.1">
        <span class="ltx_p" id="S5.T4.60.44.8.1.1">
         Tools for common lab procedures, reasoning about experimental results, and report writing
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.60.44.9" style="padding-top:2pt;padding-bottom:2pt;">
       2024.01
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.65.49">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.61.45.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.61.45.1.1">
        <span class="ltx_p" id="S5.T4.61.45.1.1.1">
         PaperQA
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.61.45.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib456" title="">
            456
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.61.45.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.63.47.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.62.46.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.63.47.3.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.64.48.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.64.48.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.64.48.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.65.49.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.65.49.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.65.49.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.65.49.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.65.49.6.1">
        <span class="ltx_p" id="S5.T4.65.49.6.1.1">
         Tools to search the scientific literature, gather evidence, and answer questions
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.65.49.7" style="padding-top:2pt;padding-bottom:2pt;">
       2023.12
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.70.54">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.66.50.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.66.50.1.1">
        <span class="ltx_p" id="S5.T4.66.50.1.1.1">
         WikiCrow
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.66.50.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib457" title="">
            457
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.66.50.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.68.52.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.67.51.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.68.52.3.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.69.53.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.69.53.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.69.53.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.70.54.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.70.54.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.70.54.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.70.54.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.70.54.6.1">
        <span class="ltx_p" id="S5.T4.70.54.6.1.1">
         Uses PaperQA as a tool
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.70.54.7" style="padding-top:2pt;padding-bottom:2pt;">
       2023.12
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.74.58">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.71.55.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.71.55.1.1">
        <span class="ltx_p" id="S5.T4.71.55.1.1.1">
         Coscientist
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.71.55.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib48" title="">
            48
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.71.55.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.72.56.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.72.56.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.73.57.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.73.57.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.73.57.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.74.58.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.74.58.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.74.58.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.74.58.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.74.58.5.1">
        <span class="ltx_p" id="S5.T4.74.58.5.1.1">
         Tools for running Python code, web-searching, and interacting with lab equipment
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.74.58.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.12
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.77.61">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.75.59.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.75.59.1.1">
        <span class="ltx_p" id="S5.T4.75.59.1.1.1">
         Eunomia
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.75.59.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib458" title="">
            458
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.75.59.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.76.60.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.76.60.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.77.61.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.77.61.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.77.61.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.77.61.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.77.61.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.77.61.5.1">
        <span class="ltx_p" id="S5.T4.77.61.5.1.1">
         Tools for literature and dataset searching and a chain-of–verification loop
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.77.61.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.12
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.79.63">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.78.62.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.78.62.1.1">
        <span class="ltx_p" id="S5.T4.78.62.1.1.1">
         CALMS
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.78.62.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib459" title="">
            459
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.78.62.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.79.63.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.79.63.2.g1" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.79.63.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.79.63.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.79.63.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.79.63.5.1">
        <span class="ltx_p" id="S5.T4.79.63.5.1.1">
         Tools for using the Materials Project API, designing experiments, and using a hardware API to perform the experiment
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.79.63.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.12
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.82.66">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.80.64.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.80.64.1.1">
        <span class="ltx_p" id="S5.T4.80.64.1.1.1">
         CoQuest
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.80.64.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib460" title="">
            460
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.80.64.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.81.65.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.81.65.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.82.66.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.82.66.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.82.66.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.82.66.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.82.66.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.82.66.5.1">
        <span class="ltx_p" id="S5.T4.82.66.5.1.1">
         Research question generations and tools for literature visualization using a graph organization
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.82.66.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.10
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.85.69">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.83.67.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.83.67.1.1">
        <span class="ltx_p" id="S5.T4.83.67.1.1.1">
         eXpertAI
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.83.67.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib309" title="">
            309
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.83.67.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.84.68.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.84.68.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.85.69.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.85.69.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.85.69.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.85.69.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.85.69.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.85.69.5.1">
        <span class="ltx_p" id="S5.T4.85.69.5.1.1">
         Tools for applying XAI methods
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.85.69.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.11
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.89.73">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.86.70.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.86.70.1.1">
        <span class="ltx_p" id="S5.T4.86.70.1.1.1">
         BioPlanner
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.86.70.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib461" title="">
            461
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.86.70.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.87.71.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.87.71.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.88.72.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.88.72.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.88.72.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.89.73.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.89.73.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.89.73.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.89.73.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.89.73.5.1">
        <span class="ltx_p" id="S5.T4.89.73.5.1.1">
         Tool for protocol searching in the BioProt dataset
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.89.73.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.10
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.91.75">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.90.74.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.90.74.1.1">
        <span class="ltx_p" id="S5.T4.90.74.1.1.1">
         IBM ChatChem
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.90.74.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib462" title="">
            462
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.90.74.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.91.75.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.91.75.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.91.75.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.91.75.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.91.75.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.91.75.5.1">
        <span class="ltx_p" id="S5.T4.91.75.5.1.1">
         Tools for cheminformatics and accessing GT4SD and HuggingFace models
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.91.75.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.09
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.95.79">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.92.76.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.92.76.1.1">
        <span class="ltx_p" id="S5.T4.92.76.1.1.1">
         ChatMOF
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.92.76.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib463" title="">
            463
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.92.76.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.94.78.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.93.77.2.g1" src="/html/2407.01603/assets/figs/letter-w.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.94.78.3.g2" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.95.79.5" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.95.79.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.95.79.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.95.79.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.95.79.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.95.79.6.1">
        <span class="ltx_p" id="S5.T4.95.79.6.1.1">
         Tools for database search, property prediction, and MOF’s structure generation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.95.79.7" style="padding-top:2pt;padding-bottom:2pt;">
       2023.08
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.100.84">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.97.81.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.97.81.2.2">
        <span class="ltx_p" id="S5.T4.97.81.2.2.2">
         AmadeusGPT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.97.81.2.2.2.1.1">
           <a class="ltx_ref" href="#bib.bib47" title="">
            47
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.96.80.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.97.81.2.2.2.g2" src="/html/2407.01603/assets/figs/view.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.99.83.4" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.98.82.3.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.99.83.4.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.100.84.6" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.100.84.5" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.100.84.5.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.100.84.5.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.100.84.7" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.100.84.7.1">
        <span class="ltx_p" id="S5.T4.100.84.7.1.1">
         Tools for writing and executing code for computer vision, machine learning, and spatial-temporal reasoning
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.100.84.8" style="padding-top:2pt;padding-bottom:2pt;">
       2023.07
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.102.86">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.101.85.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.101.85.1.1">
        <span class="ltx_p" id="S5.T4.101.85.1.1.1">
         i-Digest
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.101.85.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib464" title="">
            464
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.101.85.1.1.1.g1" src="/html/2407.01603/assets/figs/ear.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.102.86.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.102.86.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.102.86.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.102.86.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.102.86.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.102.86.5.1">
        <span class="ltx_p" id="S5.T4.102.86.5.1.1">
         Uses the whisper model to process audio transcription from classes and write summaries and following up questions
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.102.86.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.06
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.103.87">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.103.87.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.103.87.2.1">
        <span class="ltx_p" id="S5.T4.103.87.2.1.1">
         BOLLaMa
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.103.87.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib465" title="">
            465
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.103.87.1" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.103.87.1.g1" src="/html/2407.01603/assets/figs/letter-w.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.103.87.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.103.87.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.103.87.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.103.87.5.1">
        <span class="ltx_p" id="S5.T4.103.87.5.1.1">
         Implements an LLM interface to ease the usage of their BO code
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.103.87.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.06
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.104.88">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.104.88.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.104.88.2.1">
        <span class="ltx_p" id="S5.T4.104.88.2.1.1">
         text2concrete
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.104.88.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib466" title="">
            466
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.104.88.1" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.104.88.1.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.104.88.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.104.88.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.104.88.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.104.88.5.1">
        <span class="ltx_p" id="S5.T4.104.88.5.1.1">
         Uses ICL to predict compressive strength from concrete formulation
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.104.88.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.06
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.108.92">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.105.89.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.105.89.1.1">
        <span class="ltx_p" id="S5.T4.105.89.1.1.1">
         MAPI_LLM
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.105.89.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib467" title="">
            467
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.105.89.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.107.91.3" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.106.90.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.107.91.3.g2" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.108.92.5" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.108.92.4" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.108.92.4.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.108.92.4.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.108.92.6" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.108.92.6.1">
        <span class="ltx_p" id="S5.T4.108.92.6.1.1">
         Database access and LLM prediction using ICL
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.108.92.7" style="padding-top:2pt;padding-bottom:2pt;">
       2023.06
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.109.93">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S5.T4.109.93.2" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.109.93.2.1">
        <span class="ltx_p" id="S5.T4.109.93.2.1.1">
         BO-LIFT
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.109.93.2.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib468" title="">
            468
           </a>
          </sup>
         </cite>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.109.93.1" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.109.93.1.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td" id="S5.T4.109.93.3" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td" id="S5.T4.109.93.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle" id="S5.T4.109.93.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.109.93.5.1">
        <span class="ltx_p" id="S5.T4.109.93.5.1.1">
         Regression using ICL and text retrieval
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T4.109.93.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.04
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.112.96">
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S5.T4.110.94.1" style="width:91.0pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.110.94.1.1">
        <span class="ltx_p" id="S5.T4.110.94.1.1.1">
         ChemCrow
         <cite class="ltx_cite ltx_citemacro_cite">
          <sup class="ltx_sup" id="S5.T4.110.94.1.1.1.1.1">
           <a class="ltx_ref" href="#bib.bib47" title="">
            47
           </a>
          </sup>
         </cite>
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.110.94.1.1.1.g1" src="/html/2407.01603/assets/figs/robot.png" width="14"/>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.111.95.2" style="padding-top:2pt;padding-bottom:2pt;">
       <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="S5.T4.111.95.2.g1" src="/html/2407.01603/assets/figs/letter-s.png" width="14"/>
      </td>
      <td class="ltx_td ltx_border_b" id="S5.T4.112.96.4" style="padding-top:2pt;padding-bottom:2pt;">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.112.96.3" style="padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_text" id="S5.T4.112.96.3.1" style="color:#00FF00;">
        <svg class="ltx_picture" height="11.02" id="S5.T4.112.96.3.1.pic1" overflow="visible" version="1.1" width="15.75">
         <g fill="#00FF00" stroke="#00FF00" stroke-width="0.4pt" transform="translate(0,11.02) matrix(1 0 0 -1 0 0)">
          <path d="M 0 5.51 L 3.94 0 L 15.75 11.02 L 3.94 2.36 Z" style="stroke:none">
          </path>
         </g>
        </svg>
       </span>
      </td>
      <td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S5.T4.112.96.5" style="width:170.7pt;padding-top:2pt;padding-bottom:2pt;">
       <span class="ltx_inline-block ltx_align_top" id="S5.T4.112.96.5.1">
        <span class="ltx_p" id="S5.T4.112.96.5.1.1">
         Molecular, cheminformatics, search and critique tools
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.112.96.6" style="padding-top:2pt;padding-bottom:2pt;">
       2023.04
      </td>
     </tr>
    </tbody>
   </table>
  </figure>
  <div class="ltx_para ltx_noindent" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    It was
    <cite class="ltx_cite ltx_citemacro_citet">
     Hocky and White
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib469" title="">
       469
      </a>
     </sup>
    </cite>
    who discussed the early stages of models that could automate programming and, hence, the expected impacts in chemistry.
Then, early work by
    <cite class="ltx_cite ltx_citemacro_citet">
     White et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib470" title="">
       470
      </a>
     </sup>
    </cite>
    applied LLMs that could generate code to a benchmark set of chemical problems. In that case, not only were LLMs demonstrated to possess a notable understanding of chemistry, based on accurate question answering, but
    <cite class="ltx_cite ltx_citemacro_citet">
     White et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib470" title="">
       470
      </a>
     </sup>
    </cite>
    imagined a potential to use them as base models to control knowledge augmentation and a variety of other tools. Thus, these LLMs could be used to execute routine tasks, optimize procedures, and enhance the retrieval of information from scientific literature across a range of scientific domains. To the best of our knowledge, this is the first review of autonomous agents in chemistry that have evolved since these two visionary conceptual perspectives. A deeper exploration follows below.
One driving motivation for the need to augment LLMs with a more pertinent and dedicated knowledge base is the need to circumvent problems of a limited context prompt window, and the restriction that once an LLM is trained, any new information is beyond it’s reach since it necessarily has fallen outside its corpus of training data. Furthermore, LLMs are also known to hallucinate. Their predictions are probabilistic and, in science, if experimental evidence is available, then there is great value in building from known domain-specific information. Some improved prompt engineering can aid in the generation of results that are more likely to be accurate, but the use of autonomous agents may solve such problems completely in this next phase of AI in chemistry.
In fact, even adding one or two components when building an agent, as opposed to a whole suite, has shown some significant gains.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p3">
   <p class="ltx_p" id="S5.p3.1">
    Building on this foundation,
    <cite class="ltx_cite ltx_citemacro_citet">
     Ramos et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib468" title="">
       468
      </a>
     </sup>
    </cite>
    illustrated that LLMs could directly predict experimental outcomes from natural language descriptions, a technique they incorporated into a Bayesian optimization (BO) algorithm to streamline chemical processes. Because ICL was used, their approach did not require additional model training or finetuning, thereby greatly simplifying the optimization algorithm and removing the need to re-train or update models as data is gathered.
Recently,
    <cite class="ltx_cite ltx_citemacro_citet">
     Kristiadi et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib471" title="">
       471
      </a>
     </sup>
    </cite>
    demonstrated that similar results could also be achieved with a much smaller domain-specific model while performing parameter-efficient fine-tuning (PEFT) instead of ICL.
    <cite class="ltx_cite ltx_citemacro_citet">
     Ranković and Schwaller
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib472" title="">
       472
      </a>
     </sup>
    </cite>
    also explored BO using natural language. They used an LLM to encode chemical reaction procedures, described using natural language, and then trained a Gaussian process (GP) head to predict the reaction yield from the latent encoded representation of the procedure. This effective transfer-learning approach minimized training time by keeping the LLM frozen while updating only the MLP. Extending these ideas,
    <cite class="ltx_cite ltx_citemacro_citet">
     Völker et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib454" title="">
       454
      </a>
     </sup>
    </cite>
    suggested sampling multiple model completions and adding a verifier model to select the next best procedure to continue the BO algorithm. They also used ICL and a short-term memory component to optimize alkali-activated concrete mix design. The authors thus underscored how augmenting with knowledge-driven design can outperform traditional data-driven design. These are examples of how agent-based approaches can execute complex optimization algorithms in a step-by-step approach, directly contributing to automation and leading to faster and more efficient experimental design.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p4">
   <p class="ltx_p" id="S5.p4.1">
    To better promote new ideas regarding AI in scientific research,
    <cite class="ltx_cite ltx_citemacro_citet">
     Jablonka et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib473" title="">
       473
      </a>
     </sup>
    </cite>
    organized a hackathon in March 2023.
During this one-day hackathon, 14 innovative projects were developed to address chemical problems centered around predictive modeling, automation, knowledge extraction, and education. Below, we highlight agent-based approaches from that Hackathon. First, MAPI_LLM
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.1.1">
      <a class="ltx_ref" href="#bib.bib467" title="">
       467
      </a>
     </sup>
    </cite>
    is an agent with access to the Materials Project API (MAPI) database that receives a query asking for a property of a material and then retrieves the relevant information from the dataset. If the material is not available on MAPI, the agent can search for similar materials and use in-context learning (ICL) to provide a prediction of the requested property. Additionally, MAPI_LLM also has a reaction module for synthesis proposal. Second,
    <cite class="ltx_cite ltx_citemacro_citet">
     Rankovic et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib465" title="">
       465
      </a>
     </sup>
    </cite>
    used LLMs to make BO algorithms more accessible to a broader group of scientists; BOLLaMa implements a natural language interface to easily interact with BO software developed by their group
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.2.1">
      <a class="ltx_ref" href="#bib.bib474" title="">
       474
      </a>
     </sup>
    </cite>
    . Third, and similar to
    <cite class="ltx_cite ltx_citemacro_citet">
     Ramos et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib468" title="">
       468
      </a>
     </sup>
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Ranković and Schwaller
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib472" title="">
       472
      </a>
     </sup>
    </cite>
    who employed LLMs in BO,
    <cite class="ltx_cite ltx_citemacro_citet">
     Weiser et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib475" title="">
       475
      </a>
     </sup>
    </cite>
    focused on genetic algorithms (GA), a different optimization algorithm. In GA, pieces of information are stochastically combined and evaluated to guide the algorithm during the optimization. For chemistry, these pieces are often molecular fragments that are combined to compose a final whole molecular structure. Thus,
    <cite class="ltx_cite ltx_citemacro_citet">
     Weiser et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib475" title="">
       475
      </a>
     </sup>
    </cite>
    used LLMs to implement common GA operators under the hypothesis that LLMs can generate new combined molecules better than random cross-over due to their sensory memory. Fourth, InsightGraph
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.3.1">
      <a class="ltx_ref" href="#bib.bib476" title="">
       476
      </a>
     </sup>
    </cite>
    can draw general relationships between materials and their properties from JSON files.
    <cite class="ltx_cite ltx_citemacro_citet">
     Circi and Badhwar
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib476" title="">
       476
      </a>
     </sup>
    </cite>
    showed that LLMs can understand the structured data from a JSON format and reorganize the information in a knowledge graph. Further refinement of this tool could automate the process of describing relationships between materials across various scientific reports, a task that remains labor-intensive today. Fifth,
    <cite class="ltx_cite ltx_citemacro_citet">
     Kruschwitz et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib466" title="">
       466
      </a>
     </sup>
    </cite>
    used ICL and LLMs to accurately predict the compressive strength of concrete formulations; Text2Concrete achieved predictive accuracy comparable with a Gaussian process regression (GPR) model, with the advantage that design principles can be easily added as context. This model was successfully applied in a BO algorithm following
    <cite class="ltx_cite ltx_citemacro_citet">
     Ramos et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib468" title="">
       468
      </a>
     </sup>
    </cite>
    approach.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.4.1">
      <a class="ltx_ref" href="#bib.bib454" title="">
       454
      </a>
     </sup>
    </cite>
    For education purposes, multiple authors have raised the discussion about how LLMs can be used to support educators’ and instructors’ daily work.
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.5.1">
      <a class="ltx_ref" href="#bib.bib477" title="">
       477
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib478" title="">
       478
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib479" title="">
       479
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib480" title="">
       480
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib481" title="">
       481
      </a>
     </sup>
    </cite>
    Finally, in this direction,
    <cite class="ltx_cite ltx_citemacro_citet">
     Mouriño et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib464" title="">
       464
      </a>
     </sup>
    </cite>
    developed i-Digest, an agent whose perception module can understand audio tracks and video recordings. These audio recordings are transcribed to text using the Whisper
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p4.1.6.1">
      <a class="ltx_ref" href="#bib.bib482" title="">
       482
      </a>
     </sup>
    </cite>
    model, and therefore, i-Digest is a digital tutor that generates questions to help students test their knowledge about the course material. These are just a few examples to showcase the capabilities of AI systems to innovate and generate solutions rapidly.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S5.p5">
   <p class="ltx_p" id="S5.p5.1">
    More recently,
    <cite class="ltx_cite ltx_citemacro_citet">
     Ma et al.
     <sup class="ltx_sup">
      <a class="ltx_ref" href="#bib.bib452" title="">
       452
      </a>
     </sup>
    </cite>
    showed that agents can be trained to use tools. SciAgent
    <cite class="ltx_cite ltx_citemacro_cite">
     <sup class="ltx_sup" id="S5.p5.1.1.1">
      <a class="ltx_ref" href="#bib.bib452" title="">
       452
      </a>
     </sup>
    </cite>
    was developed under the premise that finetuning LLMs for domain-specific applications is often impractical. Nevertheless, the agent can be fine-tuned with a set of tools that will enable them to perform well in a domain-specific task. These tools, typically Python functions, enable SciAgent to plan, retrieve, and use these tools to facilitate reasoning and answer domain-related questions effectively. The benchmark developed for SciAgent, known as
    <span class="ltx_text ltx_font_typewriter" id="S5.p5.1.2">
     SciToolBench
    </span>
    , includes five distinct domains, each equipped with a set of questions and corresponding tools. The development of its retrieval and planning modules involved finetuning different LLMs on the MathFunc benchmark, resulting in a notable performance improvement of approximately
    <math alttext="\sim" class="ltx_Math" display="inline" id="S5.p5.1.m1.1">
     <semantics id="S5.p5.1.m1.1a">
      <mo id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml">
       ∼
      </mo>
      <annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b">
       <csymbol cd="latexml" id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1">
        similar-to
       </csymbol>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">
       \sim
      </annotation>
     </semantics>
    </math>
    20% across all domains in
    <span class="ltx_text ltx_font_typewriter" id="S5.p5.1.3">
     SciToolBench
    </span>
    compared to other LLMs.
   </p>
  </div>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Agents for literature review
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     Another fantastic opportunity for automation in the sciences is associated with high-quality literature review, a pivotal aspect of scientific research that requires reading and selecting relevant information from large numbers of papers, and thereby distilling the current state of knowledge relevant to a particular research direction. This extremely time-consuming task is being revolutionized by advanced AI tools designed to automate and enhance such analysis and summarization.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     PaperQA introduces a robust model that significantly reduces misinformation while improving the efficiency of information retrieval. This agent retrieves papers from online scientific databases, reasons about their content, and performs question-answering (QA) tasks. Its mechanism involves three primary components—“
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.1">
      search
     </span>
     ”, “
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.2">
      gather_evidence
     </span>
     ”, and “
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.3">
      answer_question
     </span>
     ” and the authors adapted the Retrieval-Augmented Generation (RAG)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p2.1.4.1">
       <a class="ltx_ref" href="#bib.bib483" title="">
        483
       </a>
      </sup>
     </cite>
     algorithm to include inner loops on each step. For instance, PaperQA can perform multiple rounds of
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.5">
      search
     </span>
     and
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.6">
      gather_evidence
     </span>
     if, with reflection, not have enough evidence has been acquired to successfully
     <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.7">
      answer_question
     </span>
     .
Further validating its capabilities, a new benchmark called LitQA demonstrates that PaperQA not only meets but exceeds human performance in solving complex, real-world scientific questions. By applying the RAG technique to full-text scientific papers, PaperQA sets a new standard in QA capabilities, achieving human-like performance in curated datasets without hallucination or selecting irrelevant citations.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p2.1.8.1">
       <a class="ltx_ref" href="#bib.bib456" title="">
        456
       </a>
      </sup>
     </cite>
     To further investigate PaperQA performance, the authors developed a new benchmark called LitQA, which designs tasks that mimic the complexity of real scientific inquiry. The benchmark comprises 50 multiple-choice questions sourced from biomedical papers published post-September 2021, ensuring they were not part of the LLM training data. In this setting, PaperQA demonstrated a precision rate of 87.9% and surpassed typical human accuracy, scoring 69.5% compared to 66.8%.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p2.1.9.1">
       <a class="ltx_ref" href="#bib.bib456" title="">
        456
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p3">
    <p class="ltx_p" id="S5.SS1.p3.1">
     Building on top of PaperQA, WikiCrow exemplifies the practical application of AI in generating concise and relevant Wikipedia-style summaries. The authors show that while 16% of a human-created Wikipedia article comprises irrelevant statements, WikiCrow displays irrelevant information only 3% of the time. Their system also added 5% more correct citations when compared with original articles. Moreover, thanks to its foundation in the PaperQA framework
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib456" title="">
        456
       </a>
      </sup>
     </cite>
     , WikiCrow achieves remarkable cost-efficiency. The authors estimate that WikiCrow can accomplish in a few days what would take humans approximately 60,000 hours, or about 6.8 years, thereby underscoring its ability to rapidly produce extensive scientific content. This efficiency exemplifies the reliability and transformative potential of AI in content creation
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib457" title="">
        457
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p4">
    <p class="ltx_p" id="S5.SS1.p4.4">
     Following a different approach, the STORM model also addressed the problem of writing Wikipedia-like summaries, where the STORM acronym represents the Synthesis of Topic Outlines through Retrieval and Multi-perspective questions.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS1.p4.4.1.1">
       <a class="ltx_ref" href="#bib.bib453" title="">
        453
       </a>
      </sup>
     </cite>
     This approach implements a two-step procedure. First, STORM retrieves multiple articles on a topic and uses an LLM to integrate various perspectives into a cohesive outline. Second, this outline is used to write each section of the Wikipedia-like summary individually.
To create the outline, multiple articles discussing the topic of interest are retrieved by an “expert” LLM, which processes each one to create
     <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.1">
      <semantics id="S5.SS1.p4.1.m1.1a">
       <mi id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b">
        <ci id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">
        N
       </annotation>
      </semantics>
     </math>
     perspectives. Each perspective is then fed to a “writer” LLM, and a conversation is initiated between writer and expert. Finally, the
     <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2.1">
      <semantics id="S5.SS1.p4.2.m2.1a">
       <mi id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b">
        <ci id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">
        N
       </annotation>
      </semantics>
     </math>
     conversations are used to design the final outline.
The outline and the set of references, accessed by RAG, are given to the writer LLM. The writer LLM is prompted to use these inputs to generate each section of the article sequentially. Following this, all sections are merged and refined to eliminate redundancies and enhance coherence. Upon human evaluation, STORM is reported to be
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p4.3.m3.1">
      <semantics id="S5.SS1.p4.3.m3.1a">
       <mo id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b">
        <csymbol cd="latexml" id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     25% more organized and present
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p4.4.m4.1">
      <semantics id="S5.SS1.p4.4.m4.1a">
       <mo id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b">
        <csymbol cd="latexml" id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     10% better coverage when compared to a pure RAG approach. However, it was also less informative than human-written Wikipedia pages, and STORM presented a transfer of internet-borne biases, producing emotional articles, which is a major concern.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Agents for chemical innovation
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     Transitioning from literature synthesis to practical chemistry applications, we next explore how LLM-based agents have proven their capabilities to revolutionize routine chemical tasks towards an acceleration of molecular discovery and scientific research. Agents are flexible entities capable of developing prompt-specific workflows and executing a plan toward accomplishing a specific task. ChemCrow
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     introduced a significant shift in how LLMs would be applied in chemistry, given that LLMs alone do not access information outside of their training data nor can they directly perform chemistry-related tasks.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     By augmenting LLMs with common chemical tools, computational or robotic, ChemCrow automates a broad spectrum of routine chemical tasks, demonstrating a significant leap in LLM applicability. Under human evaluation, ChemCrow consistently outperformed GPT-4, achieving an accuracy score of 9.24/10 compared to 4.79/10.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     The developers of ChemCrow have also considered the ethical implications and potential risks associated with its capabilities. ChemCrow’s high potential could be misused and exploited for malicious objectives, and therefore the authors have implemented safety checks and guidelines to prevent such misuse, or “dual usage”. Additionally, they acknowledge that ChemCrow, relying on an LLM, may not always provide completely accurate answers due to gaps in its chemical knowledge. As such, they recommend careful and responsible use of the tool, along with thorough scrutiny of its outputs. In summary, while ChemCrow presents a powerful new chemical assistant
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     , oversight of its use is required, and this agent’s access to tools has been deliberately limited to enhance security and avoid misuse.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     Similarly to ChemCrow
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     , Chemist-X
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib484" title="">
        484
       </a>
      </sup>
     </cite>
     uses RAG to get up-to-date literature information and use it to reliably solve user’s questions. Nevertheless, Chemist-X focuses on designing chemical reactions to achieve a given molecule. It works in three phases: (1) First, the agent searches molecule databases for similar molecules, then (2) there is a look-up for online literature searching for chemical reactions capable of converting the list of similar molecules in the target. Lastly, (3) machine learning models are used to propose the reaction conditions. To validate their agent, the authors used Chemist-X to design an HTS experiment aiming to produce 6-(1-methyl-1H-indazol-4-yl), resulting in a maximum yield of 98.6%.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p4">
    <p class="ltx_p" id="S5.SS2.p4.1">
     On the other hand, the Coscientist
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     system exemplifies the integration of semi-autonomous robots in planning, conceiving, and performing chemical reactions with minimal human intervention. At its core, the system features a main module named ‘PLANNER’, which is supported by four submodules. These submodules, or tools, are responsible for performing actions such as searching the web for organic synthesis, executing Python code, searching the hardware documentation, and performing a reaction in an automated lab.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     Utilizing this framework, the Coscientist successfully conducted two types of chemical coupling reactions, Suzuki-Miyaura and Sonogashira, in a semi-automated fashion, with manual handling of initial reagents and solvents. Additionally, Coscientist was also used to optimize reaction conditions.
In contrast to
     <cite class="ltx_cite ltx_citemacro_citet">
      Ramos et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib468" title="">
        468
       </a>
      </sup>
     </cite>
     , who used LLMs within a Bayesian Optimization (BO) algorithm as a surrogate model,
     <cite class="ltx_cite ltx_citemacro_citet">
      Boiko et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     approached the optimization task as a strategic “game” aimed at maximizing reaction yield by selecting optimal reaction conditions. This demonstrates the ability of GPT-4 to effectively reason about popular chemical reactions – possibly via comprehensive coverage in pretraining.
The authors have indicated that the code for their agent will be released following changes in U.S. regulations on AI and its scientific applications. At the time of writing, the code remains unreleased, but a simple example that calculates the square roots of random numbers has been provided to illustrate their approach.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p4.1.3.1">
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     .
These examples underscore the transformative role of LLMs in enhancing and automating chemical processes, which will likely accelerate chemical discovery.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p5">
    <p class="ltx_p" id="S5.SS2.p5.1">
     Automated workflows in protein research have also been explored. ProtAgent
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p5.1.1.1">
       <a class="ltx_ref" href="#bib.bib455" title="">
        455
       </a>
      </sup>
     </cite>
     is a multi-agent system designed to automate and optimize protein design with minimal human intervention. This system comprises three primary agents:
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.2">
      Planner
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.3">
      Assistant
     </span>
     , and
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.4">
      Critic
     </span>
     . The
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.5">
      Planner
     </span>
     is tasked with devising a strategy to address the given problem, the
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.6">
      Assistant
     </span>
     executes the plan using specialized tools and API calls, and the
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.7">
      Critic
     </span>
     supervises the entire process, providing feedback and analyzing outcomes. These agents collaborate through a dynamic group chat managed by a fourth agent, the
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p5.1.8">
      Chat Manager
     </span>
     . Tasks executed by this team include protein retrieval and analysis,
     <span class="ltx_text ltx_font_italic" id="S5.SS2.p5.1.9">
      de novo
     </span>
     protein design, and conditioned protein design using Chroma
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p5.1.10.1">
       <a class="ltx_ref" href="#bib.bib485" title="">
        485
       </a>
      </sup>
     </cite>
     and OmegaFold
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p5.1.11.1">
       <a class="ltx_ref" href="#bib.bib486" title="">
        486
       </a>
      </sup>
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p6">
    <p class="ltx_p" id="S5.SS2.p6.1">
     Similarly to ProtAgent,
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib450" title="">
        450
       </a>
      </sup>
     </cite>
     created a team of AI-made scientists (TAIS) to scientific discovery without human intervention. However, their agents have roles analog to human roles in scientific discovery, for instance, project manager, data engineer, code reviewer, statistician, and domain expert. While in ProtAgent
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p6.1.1.1">
       <a class="ltx_ref" href="#bib.bib455" title="">
        455
       </a>
      </sup>
     </cite>
     agents interact through the
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p6.1.2">
      Chat Manager
     </span>
     only, TAIS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p6.1.3.1">
       <a class="ltx_ref" href="#bib.bib450" title="">
        450
       </a>
      </sup>
     </cite>
     enables AI scientists to interact between themselves directly using pre-defined collaboration pipelines. To evaluate TAIS, the authors curated the Genetic Question Exploration (GenQEX) benchmark, which consists of 457 selected genetic data questions. As a case study, the authors show TAIS’s answer to the prompt
     <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p6.1.4">
      ‘‘What genes are associated with Pancreatic Cancer when considering conditions related to Vitamin D Levels?’’
     </span>
     . The system identified 20+ genes with a prediction accuracy of 80%.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p7">
    <p class="ltx_p" id="S5.SS2.p7.1">
     Innovation can also be achieved by looking into data from a different point-of-view to get new insights. Automating querying databases was investigated by
     <cite class="ltx_cite ltx_citemacro_citet">
      Ramos et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib467" title="">
        467
       </a>
      </sup>
     </cite>
     with a ReAct agent with access to the MAPI dataset. This concept was extended by
     <cite class="ltx_cite ltx_citemacro_citet">
      Chiang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib447" title="">
        447
       </a>
      </sup>
     </cite>
     . LLaMP
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p7.1.1.1">
       <a class="ltx_ref" href="#bib.bib447" title="">
        447
       </a>
      </sup>
     </cite>
     is a RAG-based ReAct agent that can interact with MAPI, arXiv, Wikipedia, and has access to atomistic simulation tools. The authors showed that grounding the responses on high-fidelity information (a well-known dataset) enabled the agent to perform inferences without fine-tuning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p8">
    <p class="ltx_p" id="S5.SS2.p8.1">
     The agents in chemistry, as exemplified by ChemCrow
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p8.1.1.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     and Coscientist
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p8.1.2.1">
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     , highlight a significant shift towards automation and enhanced efficiency in molecular discovery and scientific research. These systems demonstrate the potential of integrating LLMs with chemical tools and automation frameworks, achieving impressive accuracy and effectiveness in tasks ranging from routine chemical operations to complex reaction optimizations. Similarly, ProtAgent
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p8.1.3.1">
       <a class="ltx_ref" href="#bib.bib455" title="">
        455
       </a>
      </sup>
     </cite>
     and TAIS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p8.1.4.1">
       <a class="ltx_ref" href="#bib.bib450" title="">
        450
       </a>
      </sup>
     </cite>
     systems showcase the versatility of multi-agent frameworks in automating protein design and genetic research, pushing the boundaries of what AI-driven scientific discovery can achieve. These studies collectively showcase the incredible potential of agents in chemical and biological research, promising automation of routine tasks, easing the application of advanced techniques and analyses, and accelerating discoveries. However, they also underscore the necessity for meticulous oversight and responsible development to harness their full potential while mitigating risks.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS2.p8.1.5.1">
       <a class="ltx_ref" href="#bib.bib487" title="">
        487
       </a>
      </sup>
     </cite>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Agents for experiments planning
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     Building on the capabilities of ChemCrow and Coscientist in automating chemistry-related tasks, recent advances have focused on bridging the gap between virtual agents and physical laboratory environments.
For example, Context-Aware Language Models for Science (CALMS)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib459" title="">
        459
       </a>
      </sup>
     </cite>
     , BioPlanner
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib461" title="">
        461
       </a>
      </sup>
     </cite>
     , and CRISPR-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib449" title="">
        449
       </a>
      </sup>
     </cite>
     focus on giving support to researchers with wet-lab experimental design and data analysis.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     CALMS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib459" title="">
        459
       </a>
      </sup>
     </cite>
     focuses on improving laboratory efficiency through the operation of instruments and management of complex experiments, employing conversational LLMs to interact with scientists during experiments. In addition, this agent can perform actions using lab equipment after lab equipment APIs have been provided to the agent as tools.
CALMS was designed to enhance instrument usability and speed up scientific discovery, providing on-the-spot assistance for complex experimental setups, such as tomography scans, and enabling fully automated experiments. For instance, its capability was showcased through the operation of a real-world diffractometer. Although CALMS excelled in several tasks, a comparison between GPT-3.5 and Vicuna 1.5 revealed Vicuna’s limitations in handling tools.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p3">
    <p class="ltx_p" id="S5.SS3.p3.1">
     In contrast, BioPlanner
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib461" title="">
        461
       </a>
      </sup>
     </cite>
     significantly improves the efficiency of scientific experimentation by creating pseudocode representations of experimental procedures, showcasing AI’s capacity to streamline scientific workflows. Therefore, Rather than interacting directly with lab equipment through APIs, BioPlanner creates innovative experimental protocols that can be expanded upon within a laboratory setting.
The initial step in BioPlanner’s process involves assessing the capability of LLMs to produce structured pseudocode based on detailed natural language descriptions of experimental procedures.
In testing, BioPlanner successfully generated correct pseudocode for 59 out of 100 procedures using GPT-4, although the most common errors involved omitted units.
Afterward, the authors used BioPlanned to generate a procedure for culturing an E.coli bacteria colony and storing it with cryopreservation, which ran successfully.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p4">
    <p class="ltx_p" id="S5.SS3.p4.1">
     Focusing on gene editing experiments, CRISPR-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib449" title="">
        449
       </a>
      </sup>
     </cite>
     is an agent developed to design experiments iteratively with constant human feedback.
CRISPR-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib449" title="">
        449
       </a>
      </sup>
     </cite>
     aims to bridge the gap for non-experts by simplifying this process into manageable steps solvable by an LLM with access to useful tools. This agent operates in three modes based on user prompts: “Meta mode” provides predefined pipelines for common gene-editing scenarios; “Auto mode” uses the LLM to plan a sequence of tasks; and “Q&amp;A mode” answers general questions about the experimental design. The authors demonstrate that based on human evaluations, CRISPR-GPT outperforms GPT-3.5 and GPT-4 in accuracy, reasoning, completeness, and conciseness. Additionally, they applied CRISPR-GPT to design real-world experiments for knocking out TGFBR1, SNAI1, BAX, and BCL2L1 in the human A375 cell line, achieving an editing efficiency of approximately 70% for each gene.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p5">
    <p class="ltx_p" id="S5.SS3.p5.1">
     Following the ideas of developing agents for automating experimental protocol generation,
     <cite class="ltx_cite ltx_citemacro_citet">
      Ruan et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib488" title="">
        488
       </a>
      </sup>
     </cite>
     created a multi-agents system composed of 6 agents:
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.1">
      Literature Scouter
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.2">
      Experiment Designer
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.3">
      Hardware Executor
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.4">
      Spectrum Analyzer
     </span>
     ,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.5">
      Separation Instructor
     </span>
     , and
     <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p5.1.6">
      Result Interpreter
     </span>
     . The Large Language Models-based Reaction Development Framework (LLM-RDF)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p5.1.7.1">
       <a class="ltx_ref" href="#bib.bib488" title="">
        488
       </a>
      </sup>
     </cite>
     automates every step of the synthesis workflow. While other studies focus on the literature review
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p5.1.8.1">
       <a class="ltx_ref" href="#bib.bib489" title="">
        489
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib490" title="">
        490
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib491" title="">
        491
       </a>
      </sup>
     </cite>
     , HTS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p5.1.9.1">
       <a class="ltx_ref" href="#bib.bib484" title="">
        484
       </a>
      </sup>
     </cite>
     , and reaction optimization
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p5.1.10.1">
       <a class="ltx_ref" href="#bib.bib492" title="">
        492
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     , LLM-RDF can support researchers from literature search until the product purification. Using this system, the authors showed they could design a copper/TEMPO catalyzed alcohol oxidation reaction, optimize reaction conditions, engineer a scale-up, and purify the products, obtaining a yield of 86% and a purity &gt;98% while producing 1 gram of product.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p6">
    <p class="ltx_p" id="S5.SS3.p6.1">
     Interestingly, despite acting on different fields and presenting different goals (while CALMS
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.1.1">
       <a class="ltx_ref" href="#bib.bib459" title="">
        459
       </a>
      </sup>
     </cite>
     and LLM-RDF
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.2.1">
       <a class="ltx_ref" href="#bib.bib488" title="">
        488
       </a>
      </sup>
     </cite>
     can autonomously use laboratory equipment, BioPlanner
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.3.1">
       <a class="ltx_ref" href="#bib.bib461" title="">
        461
       </a>
      </sup>
     </cite>
     and CRISPR-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.4.1">
       <a class="ltx_ref" href="#bib.bib449" title="">
        449
       </a>
      </sup>
     </cite>
     propose a protocol that needs to be carried out by a human), all studies implemented a “human-in-the-loop” approach. Maintaining the researcher as a vital piece of the development is a way to enhance reliability and mitigate agent limitations, such as errors and hallucinations. This approach also addresses risk and dual usage problems once a human can decide whether the agent’s suggestions are safe.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.5.1">
       <a class="ltx_ref" href="#bib.bib374" title="">
        374
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib493" title="">
        493
       </a>
      </sup>
     </cite>
     On a slightly different direction, Organa
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS3.p6.1.6.1">
       <a class="ltx_ref" href="#bib.bib370" title="">
        370
       </a>
      </sup>
     </cite>
     fully automates the laboratory workload while providing feedback to the researcher and producing reports with the results, as discussed on Section
     <a class="ltx_ref" href="#S3.SS7.SSS1" title="3.7.1 Automation ‣ 3.7 The use of ChatGPT in Chemistry ‣ 3 LLMs for chemistry and biochemistry ‣ A Review of Large Language Models and Autonomous Agents in Chemistry">
      <span class="ltx_text ltx_ref_tag">
       3.7.1
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.4
    </span>
    Agents for automating cheminformatics tasks
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     Cheminformatics consists of applying information technology techniques to convert physicochemical information into knowledge. The process of solving cheminformatics problems commonly involves retrieving, processing, and analyzing chemical data.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib494" title="">
        494
       </a>
      </sup>
     </cite>
     Getting inspiration from ChemCrow
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
      </sup>
     </cite>
     ideas, Chemistry Agent Connecting Tool Usage to Science (CACTUS)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib495" title="">
        495
       </a>
      </sup>
     </cite>
     focused on assisting scientists by automating cheminformatics tasks. CACTUS automates the applications of multiple cheminformatics tools, such as property prediction and calculation, while maintaining the human-in-the-loop for molecular discovery. The authors investigated the performance of a diverse set of open-source LLMs, where Gemma-7B and Mistral-7B demonstrated superior performance against LLaMA-7B and Falcon-7B. In addition, the authors reported that adding domain-specific information in the prompt to align the agent to chemistry problems considerably increases model’s performance. For instance, predicting drug-likeness with a Gemma-7B agent improves the accuracy of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1">
      <semantics id="S5.SS4.p1.1.m1.1a">
       <mo id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b">
        <csymbol cd="latexml" id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     60% when aligning the agent. Prompt alignment improved the prediction of all properties they studied.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p2">
    <p class="ltx_p" id="S5.SS4.p2.5">
     Further illustrating the versatility of AI in scientific research and domain-specific tools usage is ChatMOF,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p2.5.1.1">
       <a class="ltx_ref" href="#bib.bib463" title="">
        463
       </a>
      </sup>
     </cite>
     which focuses on the prediction and generation of Metal-Organic Frameworks (MOFs). ChatMOF integrates MOF databases with its MOFTransformer
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p2.5.2.1">
       <a class="ltx_ref" href="#bib.bib496" title="">
        496
       </a>
      </sup>
     </cite>
     predictor module, thereby showcasing the innovative use of genetic algorithms in guiding generative tasks from associated predictions. The authors showed that ChatMOF achieved an accuracy of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1">
      <semantics id="S5.SS4.p2.1.m1.1a">
       <mo id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b">
        <csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     90% in search and prediction tasks while generative tasks have an accuracy of
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS4.p2.2.m2.1">
      <semantics id="S5.SS4.p2.2.m2.1a">
       <mo id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b">
        <csymbol cd="latexml" id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     70%. The genetic algorithm used by ChatMOF allows for the generation of a diverse array of MOF structures, which can be further refined based on specific properties requested by users. For instance, when prompted to, “generate structures with the largest surface area”, the system initially generated a broad distribution of structures with surface area centered in 3784 m
     <sup class="ltx_sup" id="S5.SS4.p2.5.3">
      2
     </sup>
     /g, and the GA evolves it to a narrower distribution with a peak at 5554 m
     <sup class="ltx_sup" id="S5.SS4.p2.5.4">
      2
     </sup>
     /g after only three generations. It is important to note that even though ChatMOF has access to a dataset of experimental values for MOFs, language model predictions guide their GA, and no further validation has been made.
Lastly,
     <cite class="ltx_cite ltx_citemacro_citet">
      Ansari and Moosavi
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib458" title="">
        458
       </a>
      </sup>
     </cite>
     developed Eunomia, another domain-specific autonomous AI agent that leverages existing knowledge to answer questions about materials. Eunomia
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p2.5.5.1">
       <a class="ltx_ref" href="#bib.bib458" title="">
        458
       </a>
      </sup>
     </cite>
     can use chemistry tools to
access a variety of datasets, scientific papers and unstructured texts to extract and reason about material science information. The authors implemented a CoVe
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p2.5.6.1">
       <a class="ltx_ref" href="#bib.bib388" title="">
        388
       </a>
      </sup>
     </cite>
     (Consistency Verification) scheme to evaluate the model’s answer and minimize hallucination. The authors showed that including CoVe increased the model’s precision by
     <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS4.p2.5.m5.1">
      <semantics id="S5.SS4.p2.5.m5.1a">
       <mo id="S5.SS4.p2.5.m5.1.1" xref="S5.SS4.p2.5.m5.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS4.p2.5.m5.1b">
        <csymbol cd="latexml" id="S5.SS4.p2.5.m5.1.1.cmml" xref="S5.SS4.p2.5.m5.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS4.p2.5.m5.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     20% when compared to previous methods such as an agent using ReAct only.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p2.5.7.1">
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p3">
    <p class="ltx_p" id="S5.SS4.p3.1">
     Promoting molecular discovery is a topic with great attention in the literature devoted to it and, as described extensively above, LLMs have leveraged a large amount of unstructured data to accelerate that discovery.
     <cite class="ltx_cite ltx_citemacro_citet">
      Janakarajan et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib462" title="">
        462
       </a>
      </sup>
     </cite>
     discuss the advantages of using LLMs in fields such as
     <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.1">
      de novo
     </span>
     drug design, reaction chemistry, and property prediction, but they augment the LLM in
     <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p3.1.2">
      IBM ChemChat
     </span>
     , a chatbot with the capability of using common APIs and python packages commonly used daily by a cheminformatics researcher to access molecular information.
     <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p3.1.3">
      ChemChat
     </span>
     has access to tools such as Generative Toolkit for Scientific Discovery (GT4SD)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p3.1.4.1">
       <a class="ltx_ref" href="#bib.bib497" title="">
        497
       </a>
      </sup>
     </cite>
     , a package with dozens of trained models generative models for science, rxn4chemistry
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p3.1.5.1">
       <a class="ltx_ref" href="#bib.bib498" title="">
        498
       </a>
      </sup>
     </cite>
     , a package for computing chemistry reactions tasks, HuggingMolecules
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p3.1.6.1">
       <a class="ltx_ref" href="#bib.bib499" title="">
        499
       </a>
      </sup>
     </cite>
     , a package developed to aggregate molecular property prediction LMs, and RDKit
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p3.1.7.1">
       <a class="ltx_ref" href="#bib.bib500" title="">
        500
       </a>
      </sup>
     </cite>
     , a package to manipulating molecules. Since
     <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p3.1.8">
      ChemChat
     </span>
     implements an agent in a chat-like environment, users can interactively refine design ideas. Despite being developed to target
     <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.9">
      de novo
     </span>
     drug design,
     <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p3.1.10">
      ChemChat
     </span>
     nonetheless is a multi-purpose platform that can be more broadly used for molecular discovery.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p4">
    <p class="ltx_p" id="S5.SS4.p4.1">
     In addition to the capabilities described above, LLM-based agents can empower users to tackle tasks that typically require extensive technical knowledge. In previous work,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wellawatte and Schwaller
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
      </sup>
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Gandhi and White
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib257" title="">
        257
       </a>
      </sup>
     </cite>
     showed that including natural language explanations (NLE) in explainable AI (XAI) analysis can improve user understanding. More recently,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wellawatte and Schwaller
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
      </sup>
     </cite>
     developed XpertAI
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
      </sup>
     </cite>
     to seamlessly integrate XAI techniques with LLMs to interpret and explain raw chemical data autonomously. Applying XAI techniques is usually restricted to technical experts but by integrating such techniques with an LLM-based agent to automate the workflow, the authors made XAI accessible to a wider audience.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p5">
    <p class="ltx_p" id="S5.SS4.p5.1">
     Their system receives raw data with labels for physicochemical properties. The raw data is used to compute human-interpretable descriptors and then calculate SHAP values or Z-scores for Local Interpretable Model-agnostic Explanations (LIME). By calculating SHAP values (or SHapley Addictive exPlanations) a value can be assigned to each feature, indicating its contribution to a model’s output. LIME interprets a model by making a local approximation, around a particular prediction, to indicate what factors have contributed to that prediction in the model. It may use, for example, a surrogate local linear regression fit to recognized features.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p5.1.1.1">
       <a class="ltx_ref" href="#bib.bib257" title="">
        257
       </a>
      </sup>
     </cite>
     In addition to XAI tools, XpertAI can search and leverage scientific literature to provide accessible NLEs.
While ChatGPT provides scientific justifications with similar accuracy, its explanation is often too broad. On the other hand, XpertAI provides data-specific explanations and visual XAI plots to support its explanations.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS4.p5.1.2.1">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
      </sup>
     </cite>
     With a similar goal,
     <cite class="ltx_cite ltx_citemacro_citet">
      Zheng et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib501" title="">
        501
       </a>
      </sup>
     </cite>
     prompted the LLM to generate explanatory rules from data.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS4.p6">
    <p class="ltx_p" id="S5.SS4.p6.1">
     These developments signify a growing trend in the integration of tools and LLMs in autonomous AI within scientific research. By automating routine tasks, enhancing information retrieval and analysis, and facilitating experimentation, AI is expanding the capabilities of researchers and accelerating the pace of scientific discovery. This review underscores the transformative impact of AI across various scientific domains, heralding a new era of innovation and efficiency in chemical research.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.5
    </span>
    Agents for hypothesis creation
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS5.p1">
    <p class="ltx_p" id="S5.SS5.p1.1">
     Following the agent’s classification proposed by
     <cite class="ltx_cite ltx_citemacro_citet">
      Gao et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib373" title="">
        373
       </a>
      </sup>
     </cite>
     , the studies we discussed previously lie mainly in level 1 — AI agents as a research assistant. That is, such agents can support researchers in executing predefined tasks, but they lack the autonomy to propose, test, and refine new scientific hypotheses. New research has been focusing on making agents able to refine scientists’ initial hypotheses collaboratively, which is a required skill to achieve level 2 in
     <cite class="ltx_cite ltx_citemacro_citet">
      Gao et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib373" title="">
        373
       </a>
      </sup>
     </cite>
     classification.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS5.p2">
    <p class="ltx_p" id="S5.SS5.p2.1">
     the idea of an “AI scientist” who can generate new, relevant research questions (RQ) has been pursued.
     <cite class="ltx_cite ltx_citemacro_citet">
      Wang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib502" title="">
        502
       </a>
      </sup>
     </cite>
     developed a framework called Scientific Inspiration Machines Optimized for Novelty (SciMON). It uses LLMs to produce new scientific ideas grounded in existing literature. SciMON retrieves inspirations from past papers and iteratively refines generated ideas to optimize novelty by comparing them with prior work. Extending these ideas,
     <cite class="ltx_cite ltx_citemacro_citet">
      Gu and Krenn
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib503" title="">
        503
       </a>
      </sup>
     </cite>
     used LLMs to search over a knowledge graph for inspiration to propose new personalized research ideas.
Aligned with this vision,
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib460" title="">
        460
       </a>
      </sup>
     </cite>
     developed CoQuest, partially automating the brainstorming for new RQs process. This system uses a human-computing interface (HCI) to allow the agent to create new RQs that can be further enhanced by human feedback. They developed two strategies for RQ generation: breadth-first, where the agent generates multiple RQs simultaneously following the original user’s prompt, and depth-first, where multiple RQs are created sequentially, building on the top of the previously generated RQ. For each RQ generation, the agent implements a ReAct
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS5.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib393" title="">
        393
       </a>
      </sup>
     </cite>
     framework with tools for literature discovery, hypothesis proposition, refinement, and evaluation. Upon evaluation of 20 HCI doctoral researchers by a post-interaction survey, the breadth-first approach was preferred by 60% of the evaluators. Interestingly, despite the evaluators’ report that the breadth-first approach gave them more control and resulted in more trustworthy RQs, the depth-first had better scores for novelty and surprise. This difference might be caused by the fact that the depth-first uses its own RQ to iterate. This process can introduce new keywords that users have not considered.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS5.p3">
    <p class="ltx_p" id="S5.SS5.p3.1">
     Focusing on generating and testing hypotheses, ChemReasoner
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS5.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib451" title="">
        451
       </a>
      </sup>
     </cite>
     uses a domain-specific reward function and computational chemistry feedback to validate agent responses. The authors combined a Monte Carlo thought search
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS5.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib504" title="">
        504
       </a>
      </sup>
     </cite>
     for catalysis with a reward function from atomistic GNNs trained to predict adsorption energy or reaction energy barriers. While the search is responsible for exploiting literature information and allowing the model to propose new materials, the hypothetic material is further tested by the GNN. This framework was applied to suggest materials for adsorbates, biofuel catalysts, and catalysts for CO
     <sub class="ltx_sub" id="S5.SS5.p3.1.3">
      2
     </sub>
     to methanol conversion. The LLM generated the top five catalysts for each task, with ChemReasoner significantly outperforming GPT-4 based on the reward score.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS5.p4">
    <p class="ltx_p" id="S5.SS5.p4.1">
     Similarly,
     <cite class="ltx_cite ltx_citemacro_citet">
      Ma et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib448" title="">
        448
       </a>
      </sup>
     </cite>
     developed Scientific Generative Agent (SGA) to generate hypotheses and iteratively refine them through computational simulations. Initially, the LLM generates a hypothesis. In the use cases considered, it can be a code snippet or a molecule. In the sequence, a search algorithm is used to find a better initial hypothesis for solving the initial query. Finally, this hypothesis — code or molecule — is optimized using a gradient-based algorithm. Lastly, the optimization output serves as feedback to the LLM to iterate. In their molecule design task, the goal was to generate a molecule with a specified HOMO-LUMO gap. The hypothesis is a molecule, that is, a SMILES string and a set of atomic coordinates. The gap is predicted by employing UniMol
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S5.SS5.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib505" title="">
        505
       </a>
      </sup>
     </cite>
     . They showed that SGA could generate molecules based on quantum mechanical properties, but the results were not validated.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Challenges and opportunities
  </h2>
  <div class="ltx_para ltx_noindent" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    LLMs hold great potential in chemistry due to their ability to both predict properties and orchestrate existing computational and experimental tools. These capabilities enhance the accuracy and efficiency of chemical research and open up new avenues for discovery and innovation. By encapsulating AI models, data analysis software, and laboratory equipment within agent-based frameworks, researchers can harness these sophisticated tools through a unified interface. This approach not only simplifies the interaction with complex systems but also democratizes the immense capabilities of modern computational tools, thereby maximizing their utility in advancing chemical research and development.
   </p>
  </div>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
   <h5 class="ltx_title ltx_title_paragraph">
    Data Quality and Availability
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">
     Quality and availability of data are critical factors that influence the efficacy of LLMs. Indeed, scaling both the model size and the amount of training data used has proven to improve capabilities.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib506" title="">
        506
       </a>
      </sup>
     </cite>
     However, current AI models are not trained on large amounts of chemical data, which limits their capabilities to reason about advanced chemical concepts.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib183" title="">
        183
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px1.p2">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">
     There are two types of datasets commonly used to train LLMs: unlabeled and labeled datasets.
Unlabeled datasets, or pretraining data, are used during the semi-supervised training, which focuses on creating a “prior belief” about a molecule. Currently, we have huge datasets composed of hypothetical and/or theoretical data. When a model is trained on data that is not grounded in real chemical information, this might cause the model to learn a wrong prior belief.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib507" title="">
        507
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px1.p3">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p3.1">
     Labeled datasets, often used in benchmarks, also suffer from their inclusion of hypothetical and calculated data. Benchmarks are necessary for quantifying improvements in AI modeling and prediction within a competitive field. However, dominant benchmarks like MoleculeNet,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib57" title="">
        57
       </a>
      </sup>
     </cite>
     have significant limitations that may restrict the generalizability and applicability of evolving models. In his blog,
     <cite class="ltx_cite ltx_citemacro_citet">
      Walters
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib186" title="">
        186
       </a>
      </sup>
     </cite>
     brings to light numerous errors and inconsistencies within the MoleculeNet data, which substantially impact model performance and reliability.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p3.1.2.1">
       <a class="ltx_ref" href="#bib.bib184" title="">
        184
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib146" title="">
        146
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib185" title="">
        185
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib128" title="">
        128
       </a>
      </sup>
     </cite>
     . Walters also argues that the properties present in these benchmarks do not directly correlate with real chemistry improvement. As such, new benchmarks need to translate to practical chemistry problems directly. For instance, increasing accuracy in predicting LogP is not necessarily mapped to drugs with greater bioavailability. Some promising work has come from the Therapeutic Common Data (TDC)
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p3.1.3.1">
       <a class="ltx_ref" href="#bib.bib189" title="">
        189
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib188" title="">
        188
       </a>
      </sup>
     </cite>
     includes data from actual therapeutic essays, providing a more practical foundation for model training.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px1.p4">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p4.1">
     The community continues to work to organize and curate datasets to make data ready for LLMs training and evaluation. Scientific benchmarks
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p4.1.1.1">
       <a class="ltx_ref" href="#bib.bib135" title="">
        135
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib456" title="">
        456
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib508" title="">
        508
       </a>
      </sup>
     </cite>
     , repositories with curated datasets
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p4.1.2.1">
       <a class="ltx_ref" href="#bib.bib182" title="">
        182
       </a>
      </sup>
     </cite>
     , and packages for model evaluation
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p4.1.3.1">
       <a class="ltx_ref" href="#bib.bib183" title="">
        183
       </a>
      </sup>
     </cite>
     have been developed.
However, the challenges concerning grounded truth and consistent datasets remain.
With advancements in scientific document processing,
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px1.p4.1.4.1">
       <a class="ltx_ref" href="#bib.bib339" title="">
        339
       </a>
      </sup>
     </cite>
     there is now the opportunity to obtain new datasets from peer-reviewed scientific papers.
Due to the multi-modal capabilities of such AI models, these new benchmarks can comprise multiple data types, potentially enhancing the applicability and transferability of these models. The continual curation of new, relevant datasets that represent the complexities of real-world chemical problems will further enhance the robustness and relevance of LLMs in chemistry.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
   <h5 class="ltx_title ltx_title_paragraph">
    Model Interpretability
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">
     Model interpretability is a significant challenge for LLMs due to their “black-box” nature, which obscures the understanding of how predictions are made.
However, innovative approaches are being developed to enhance LLMs’ interpretability. For instance,
     <cite class="ltx_cite ltx_citemacro_citet">
      Schwaller et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib509" title="">
        509
       </a>
      </sup>
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Schilter et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib510" title="">
        510
       </a>
      </sup>
     </cite>
     used information from the different multi-attention heads. While
     <cite class="ltx_cite ltx_citemacro_citet">
      Schwaller et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib509" title="">
        509
       </a>
      </sup>
     </cite>
     connected atoms from reactants to atoms in the products,
     <cite class="ltx_cite ltx_citemacro_citet">
      Schilter et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib510" title="">
        510
       </a>
      </sup>
     </cite>
     assigned H-NMR peaks to specific hydrogens in a molecule to indicate how spectra were comprehended, or structures deduced. Additionally, since the LLMs use language, which is intrinsically interpretable, LLMs may be incrementally modified to explain their reasoning processes directly, exemplified with tools like eXpertAI
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px2.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
      </sup>
     </cite>
     and or simply adjusting prompting
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px2.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib391" title="">
        391
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib439" title="">
        439
       </a>
      </sup>
     </cite>
     . These methods address the critical need for transparency in the mechanism of understanding for a good prediction beyond the good prediction itself.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
   <h5 class="ltx_title ltx_title_paragraph">
    Integration with Domain Knowledge and Cross-disciplinary Applications
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">
     While LLMs excel at pattern recognition, integrating explicit chemical rules and domain knowledge into these systems remains challenging. This integration is essential to make predictions that are not only statistically valid but also chemically reasonable. It was shown by
     <cite class="ltx_cite ltx_citemacro_citet">
      Beltagy et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib134" title="">
        134
       </a>
      </sup>
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Gu et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib133" title="">
        133
       </a>
      </sup>
     </cite>
     that better performance on common NLP tasks can be achieved by developing a vocabulary and pretraining on a domain-specific training corpus. While, pretrainng with domain-specific datasets that include chemical properties, reaction mechanisms, and experimental results may better capture the nuances of chemistry, the challenge to use AI to promote multi-disciplinary research remains. The Galactica LLM
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px3.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib126" title="">
        126
       </a>
      </sup>
     </cite>
     also used special tokens for delineating chemical information, to relatively good success on chemistry tasks.
     <cite class="ltx_cite ltx_citemacro_citet">
      Aryal et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib511" title="">
        511
       </a>
      </sup>
     </cite>
     also progress by creating an ensemble of specialist agents with different domains of knowledge, allowing them to interact to better answer the user query. Specifically,
     <cite class="ltx_cite ltx_citemacro_citet">
      Aryal et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib511" title="">
        511
       </a>
      </sup>
     </cite>
     used agents with chemistry, physics, electrochemistry, and materials knowledge.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px4">
   <h5 class="ltx_title ltx_title_paragraph">
    Tool development
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px4.p1.1">
     The effectiveness of a combined LLM/autonomous agent approach hinges significantly on the availability and quality of the tools, as well as on the complexity and diversity of the chemical tasks at hand.
Some emphasis should be placed on refining standalone tools, with the confidence that overarching frameworks, like a GPT-4-type wrapper, or “assistant”, will eventually integrate these tools seamlessly. Developers should stay informed about existing tools and design their tools to interface effectively with such a wrapper. This ensures that each tool is ready to contribute its unique capabilities to a cohesive agent system.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px5">
   <h5 class="ltx_title ltx_title_paragraph">
    Reinforcement learning
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px5.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px5.p1.1">
     RL has been successfully used in LLMs
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px5.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib512" title="">
        512
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib513" title="">
        513
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib94" title="">
        94
       </a>
      </sup>
     </cite>
     , with a few applications also proposed for use in agents
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px5.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib514" title="">
        514
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib515" title="">
        515
       </a>
      </sup>
     </cite>
     . The next frontier is applying RL to agents directly, to improve their ability on specific tasks.
     <cite class="ltx_cite ltx_citemacro_citet">
      Bou et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib103" title="">
        103
       </a>
      </sup>
     </cite>
     provided a recent framework and example for generative molecular design when viewed as an RL problem (similar to RLHF) and some early success has been seen in applying the RLHF algorithm directly to protein language models where the reward model comes from scientific tasks.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px5.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib104" title="">
        104
       </a>
      </sup>
     </cite>
     Neither of these are direct RL on language model agents, but are a step towards this goal.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px6">
   <h5 class="ltx_title ltx_title_paragraph">
    Agent evaluation
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px6.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px6.p1.1">
     Comparing different agent systems is challenging due to the lack of robust benchmarks and evaluation schemes. Consequently, it is difficult to define what constitutes a “superhuman” digital chemist and reach a consensus on the criteria for success. This issue is similar to the ongoing discussions about defining artificial general intelligence (AGI) and the expected capabilities of cognitive architectures.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px6.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib516" title="">
        516
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib517" title="">
        517
       </a>
      </sup>
     </cite>
     Once a reliable metric for evaluating such AI systems is established, it is crucial for the AI scientific community to set clear guidelines for conducting research. Currently, assessing success is challenging because the goals are not well defined. Building on this, we propose using Bloom’s taxonomy as a reference point for developing a metric to evaluate more complex reasoning and tool use in autonomous agents. This educational framework categorizes cognitive skills in a hierarchical manner, from basic recall to creative construction, providing a structured approach to assess higher-order thinking and reasoning capabilities in these systems. This adaptation could significantly enhance the evaluation of LLMs and autonomous agents, especially when tackling complex chemical challenges.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px7">
   <h5 class="ltx_title ltx_title_paragraph">
    Ethical and Safety Concerns
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px7.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px7.p1.1">
     As with all AI technologies, there are ethical considerations in deploying LLMs, including bias in model predictions and the potential misuse of AI-generated chemical knowledge.
     <cite class="ltx_cite ltx_citemacro_citet">
      Ruan et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib518" title="">
        518
       </a>
      </sup>
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Tang et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib487" title="">
        487
       </a>
      </sup>
     </cite>
     pointed out the necessity of different levels of regulation in agent development, noting that current alignment methods may be insufficient for ensuring safety,
and using human evaluation to address the risks of such systems is not scalable. Therefore, the lack of specialized models for risk control and effective safety evaluations poses perhaps the greatest challenges when assuring the safety of LLMs with the ability to use tools. This underscores the urgency, and the opportunity, in automating red-teaming of agents to enhance safety protocols in AI technologies.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px8">
   <h5 class="ltx_title ltx_title_paragraph">
    Human-AI Collaboration in Chemical Research
   </h5>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px8.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px8.p1.1">
     LLMs are poised to transform fields such as drug discovery, materials science, and environmental chemistry due to their ability to predict chemical properties and reactions with remarkable accuracy. Models based on architectures like BERT have demonstrated their capability to achieve state-of-the-art performance in various property prediction tasks.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p1.1.1.1">
       <a class="ltx_ref" href="#bib.bib45" title="">
        45
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib155" title="">
        155
       </a>
      </sup>
     </cite>
     Furthermore, studies by
     <cite class="ltx_cite ltx_citemacro_citet">
      Jablonka et al.
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib249" title="">
        249
       </a>
      </sup>
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Born and Manica
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib145" title="">
        145
       </a>
      </sup>
     </cite>
     have showcased the predictive power of LLMs by reformulating traditional regression and classification tasks as generative tasks, opening up new avenues for chemical modeling. However, as emphasized by
     <cite class="ltx_cite ltx_citemacro_citet">
      Weng
      <sup class="ltx_sup">
       <a class="ltx_ref" href="#bib.bib375" title="">
        375
       </a>
      </sup>
     </cite>
     , maintaining the reliability of LLM outputs is essential, as inaccuracies in formatting, logical reasoning, or content can significantly impede their practical utility.
Hallucination is also an intrinsic issue with LLMs.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p1.1.2.1">
       <a class="ltx_ref" href="#bib.bib519" title="">
        519
       </a>
      </sup>
     </cite>
     Though agents can deal with hallucinations to some extent by implementing sanity-checking tools, it does not make the response hallucination-proof. A possible approach to address this issue is to use a human-in-the-loop approach, where steps of human-agent interaction are added to the workflow to check if the agent is in the correct pathway to solve the request.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p1.1.3.1">
       <a class="ltx_ref" href="#bib.bib520" title="">
        520
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib521" title="">
        521
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib522" title="">
        522
       </a>
      </sup>
     </cite>
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px8.p2">
    <p class="ltx_p" id="S6.SS0.SSS0.Px8.p2.1">
     The potential of LLMs to design novel molecules and materials was highlighted by the AI-powered robotic lab assistant, A-Lab, which synthesized 41 new materials within just 17 days.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p2.1.1.1">
       <a class="ltx_ref" href="#bib.bib523" title="">
        523
       </a>
      </sup>
     </cite>
     Nonetheless, this achievement has sparked debates about the experimental methods and the actual integration of atoms into new crystalline materials, raising questions about the authenticity of the synthesized structures.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p2.1.2.1">
       <a class="ltx_ref" href="#bib.bib524" title="">
        524
       </a>
      </sup>
     </cite>
     These controversies underline the necessity for rigorous standards and the critical role of human expertise in validating AI-generated results. Again, the integration of advanced AI tools with the oversight of seasoned chemists is crucial, suggesting that a hybrid approach could significantly enhance both the innovation and integrity of materials science research.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px8.p3">
    <p class="ltx_p" id="S6.SS0.SSS0.Px8.p3.1">
     In parallel, we have seen how LLM-based agents are increasingly capable of automating routine tasks in chemical research, which traditionally consume significant time and resources. These models excel in real-time data processing, managing vast datasets, and even conducting comprehensive literature reviews with minimal human intervention. Advances in AI technology now allow agents not only to perform predefined tasks but also to adapt and develop new tools for automating additional processes. For instance, tasks such as data analysis, literature review, and elements of experimental design are now being automated.
     <cite class="ltx_cite ltx_citemacro_cite">
      <sup class="ltx_sup" id="S6.SS0.SSS0.Px8.p3.1.1.1">
       <a class="ltx_ref" href="#bib.bib309" title="">
        309
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib525" title="">
        525
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib526" title="">
        526
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib456" title="">
        456
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib461" title="">
        461
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib459" title="">
        459
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib47" title="">
        47
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib48" title="">
        48
       </a>
      </sup>
     </cite>
     This automation liberates chemists to focus on more innovative and intellectually engaging aspects of their work, and the opportunity is to expand productivity and creativity in their science.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusions
  </h2>
  <div class="ltx_para ltx_noindent" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    Since this review is targeted in part to an audience of chemists, who may not have yet embraced AI technology, we consider it valuable to point out our perspective that AI in chemistry is definitely here to stay.
We predict that its use will only grow as a necessary tool that will inevitably lead to more jobs and greater progress. We hope to facilitate the change by connecting the technology to the chemical problems that our readership is already addressing through more traditional methods.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S7.p2">
   <p class="ltx_p" id="S7.p2.1">
    Large Language Models (LLMs) have demonstrated remarkable potential in reshaping chemical research and development workflows.
These models have facilitated significant advancements in molecular simulation, reaction prediction, and materials discovery.
In this review, we discussed the evolution of LLMs in chemistry and biochemistry.
Successful cases where LLMs have proven their potential in promoting scientific discovery were shown with caveats of such models.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S7.p3">
   <p class="ltx_p" id="S7.p3.1">
    Adopting LLM-based autonomous agents in chemistry has enhanced the accuracy and efficiency of traditional research methodologies and introduced innovative approaches to solving complex chemical problems.
Looking forward, the continued integration of LLMs promises to accelerate the field’s evolution further, driving forward the frontiers of scientific discovery and technological innovation in chemistry.
We have shown how agents have been used in chemistry and proposed a framework for thinking about agents as a central LLM followed by interchangeable components.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S7.p4">
   <p class="ltx_p" id="S7.p4.1">
    However, despite the community’s astonishing advances in this field, many challenges still require solutions.
We identified the main challenges and opportunities that need to be addressed to promote the further development of agents in chemistry.
Addressing the challenges related to model transparency, data biases, and computational demands will be crucial for maximizing their utility and ensuring their responsible use in future scientific endeavors.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S7.p5">
   <p class="ltx_p" id="S7.p5.1">
    While there are significant challenges to be addressed, the opportunities presented by LLMs in chemistry are vast and have the potential to fundamentally alter how chemical research and development are conducted. Effectively addressing these challenges will be crucial for realizing the full potential of LLMs in this exciting field.
To keep pace with the ever-growing, we will maintain a repository with an organized structure listing new studies regarding LLMs and LLM-based agents focused on scientific purposes.
The repository can be found in
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ur-whitelab/LLMs-in-science" target="_blank" title="">
     https://github.com/ur-whitelab/LLMs-in-science
    </a>
   </p>
  </div>
  <section class="ltx_subsection" id="S7.SSx1">
   <h3 class="ltx_title ltx_title_subsection">
    Author contribution
   </h3>
   <div class="ltx_para ltx_noindent" id="S7.SSx1.p1">
    <p class="ltx_p" id="S7.SSx1.p1.1">
     All authors contributed to writing this review article.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S7.SSx2">
   <h3 class="ltx_title ltx_title_subsection">
    Competing interests
   </h3>
   <div class="ltx_para ltx_noindent" id="S7.SSx2.p1">
    <p class="ltx_p" id="S7.SSx2.p1.1">
     The authors have no conflicts to declare.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S7.SSx2.SSSx1">
    <h4 class="ltx_title ltx_title_subsubsection">
     Acknowledgments
    </h4>
    <div class="ltx_para ltx_noindent" id="S7.SSx2.SSSx1.p1">
     <p class="ltx_p" id="S7.SSx2.SSSx1.p1.1">
      M.C.R. and A.D.W. acknowledge the U.S. Department of Energy, Grant No. DE-SC0023354, and C.J.C. gratefully acknowledges the Jane King Harris Endowed Professorship at Rochester Institute of Technology for the support provided for this publication. We are grateful for feedback from early drafts of this review from the following colleagues: Kevin Jablonka, Philippe Schwaller, Michael Pieler, Ryan-Rhys Griffiths, Geemi Wellawatte, and Mario Krenn.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Willett 2011
    </span>
    <span class="ltx_bibblock">
     Peter Willett.
    </span>
    <span class="ltx_bibblock">
     Chemoinformatics: a history.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Wiley Interdisciplinary Reviews: Computational Molecular Science
     </em>
     , 1(1):46–56, 2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Griffen et al. 2020
    </span>
    <span class="ltx_bibblock">
     Edward J. Griffen, Alexander G. Dossetter, and Andrew G. Leach.
    </span>
    <span class="ltx_bibblock">
     Chemists: AI Is Here; Unite To Get the Benefits.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Journal of Medicinal Chemistry
     </em>
     , 63(16):8695–8704, August 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 0022-2623.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jmedchem.0c00163" target="_blank" title="">
      10.1021/acs.jmedchem.0c00163
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jmedchem.0c00163" target="_blank" title="">
      https://doi.org/10.1021/acs.jmedchem.0c00163
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Baum et al. 2021
    </span>
    <span class="ltx_bibblock">
     Zachary J. Baum, Xiang Yu, Philippe Y. Ayala, Yanan Zhao, Steven P. Watkins, and Qiongqiong Zhou.
    </span>
    <span class="ltx_bibblock">
     Artificial Intelligence in Chemistry: Current Trends and Future Directions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 61(7):3197–3212, July 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.1c00619" target="_blank" title="">
      10.1021/acs.jcim.1c00619
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.1c00619" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.1c00619
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ayres et al. 2021
    </span>
    <span class="ltx_bibblock">
     Lucas B. Ayres, Federico J.V. Gomez, Jeb R. Linton, Maria F. Silva, and Carlos D. Garcia.
    </span>
    <span class="ltx_bibblock">
     Taking the leap between analytical chemistry and artificial intelligence: A tutorial review.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Analytica Chimica Acta
     </em>
     , 1161:338403, May 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 00032670.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.aca.2021.338403" target="_blank" title="">
      10.1016/j.aca.2021.338403
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://linkinghub.elsevier.com/retrieve/pii/S0003267021002294" target="_blank" title="">
      https://linkinghub.elsevier.com/retrieve/pii/S0003267021002294
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. 2019a
    </span>
    <span class="ltx_bibblock">
     Xin Yang, Yifei Wang, Ryan Byrne, Gisbert Schneider, and Shengyong Yang.
    </span>
    <span class="ltx_bibblock">
     Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      Chemical Reviews
     </em>
     , 119(18):10520–10594, September 2019a.
    </span>
    <span class="ltx_bibblock">
     ISSN 0009-2665.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.chemrev.8b00728" target="_blank" title="">
      10.1021/acs.chemrev.8b00728
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.chemrev.8b00728" target="_blank" title="">
      https://doi.org/10.1021/acs.chemrev.8b00728
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mater and Coote 2019
    </span>
    <span class="ltx_bibblock">
     Adam C Mater and Michelle L Coote.
    </span>
    <span class="ltx_bibblock">
     Deep learning in chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Journal of chemical information and modeling
     </em>
     , 59(6):2545–2559, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yun-Fei Shi, Zheng-Xin Yang, Sicong Ma, Pei-Lin Kang, Cheng Shang, P Hu, and Zhi-Pan Liu.
    </span>
    <span class="ltx_bibblock">
     Machine learning for chemistry: basics and applications.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      Engineering
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Keith et al. 2021
    </span>
    <span class="ltx_bibblock">
     John A Keith, Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger, Klaus-Robert Muller, and Alexandre Tkatchenko.
    </span>
    <span class="ltx_bibblock">
     Combining machine learning and computational chemistry for predictive insights into chemical systems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Chemical reviews
     </em>
     , 121(16):9816–9872, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kuntz and Wilson 2022
    </span>
    <span class="ltx_bibblock">
     David Kuntz and Angela K Wilson.
    </span>
    <span class="ltx_bibblock">
     Machine learning, artificial intelligence, and chemistry: How smart algorithms are reshaping simulation and the laboratory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Pure and Applied Chemistry
     </em>
     , 94(8):1019–1054, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Meuwly 2021
    </span>
    <span class="ltx_bibblock">
     Markus Meuwly.
    </span>
    <span class="ltx_bibblock">
     Machine learning for chemical reactions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Chemical Reviews
     </em>
     , 121(16):10218–10239, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lederberg et al. 1969
    </span>
    <span class="ltx_bibblock">
     Joshua Lederberg, Georgia L Sutherland, Bruce G Buchanan, Edward A Feigenbaum, Alexander V Robertson, Alan M Duffield, and Carl Djerassi.
    </span>
    <span class="ltx_bibblock">
     Applications of artificial intelligence for chemical inference. i. number of possible organic compounds. acyclic structures containing carbon, hydrogen, oxygen, and nitrogen.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Journal of the American Chemical Society
     </em>
     , 91(11):2973–2976, 1969.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lindsay et al. 1993
    </span>
    <span class="ltx_bibblock">
     Robert K Lindsay, Bruce G Buchanan, Edward A Feigenbaum, and Joshua Lederberg.
    </span>
    <span class="ltx_bibblock">
     Dendral: a case study of the first expert system for scientific hypothesis formation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Artificial intelligence
     </em>
     , 61(2):209–261, 1993.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Buchanan et al. 1976
    </span>
    <span class="ltx_bibblock">
     B. G. Buchanan, D. H. Smith, W. C. White, R. J. Gritter, E. A. Feigenbaum, J. Lederberg, and Carl Djerassi.
    </span>
    <span class="ltx_bibblock">
     Applications of artificial intelligence for chemical inference. 22. Automatic rule formation in mass spectrometry by means of the meta-DENDRAL program.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Journal of the American Chemical Society
     </em>
     , 98(20):6168–6178, September 1976.
    </span>
    <span class="ltx_bibblock">
     ISSN 0002-7863.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/ja00436a017" target="_blank" title="">
      10.1021/ja00436a017
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/ja00436a017" target="_blank" title="">
      https://doi.org/10.1021/ja00436a017
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hansch et al. 1962
    </span>
    <span class="ltx_bibblock">
     Corwin Hansch, Peyton P Maloney, Toshio Fujita, and Robert M Muir.
    </span>
    <span class="ltx_bibblock">
     Correlation of biological activity of phenoxyacetic acids with hammett substituent constants and partition coefficients.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Nature
     </em>
     , 194(4824):178–180, 1962.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hansch and Fujita 1964
    </span>
    <span class="ltx_bibblock">
     Corwin Hansch and Toshio Fujita.
    </span>
    <span class="ltx_bibblock">
     p-
     <math alttext="\sigma" class="ltx_Math" display="inline" id="bib.bib15.1.m1.1">
      <semantics id="bib.bib15.1.m1.1a">
       <mi id="bib.bib15.1.m1.1.1" xref="bib.bib15.1.m1.1.1.cmml">
        σ
       </mi>
       <annotation-xml encoding="MathML-Content" id="bib.bib15.1.m1.1b">
        <ci id="bib.bib15.1.m1.1.1.cmml" xref="bib.bib15.1.m1.1.1">
         𝜎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="bib.bib15.1.m1.1c">
        \sigma
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="\pi" class="ltx_Math" display="inline" id="bib.bib15.2.m2.1">
      <semantics id="bib.bib15.2.m2.1a">
       <mi id="bib.bib15.2.m2.1.1" xref="bib.bib15.2.m2.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="bib.bib15.2.m2.1b">
        <ci id="bib.bib15.2.m2.1.1.cmml" xref="bib.bib15.2.m2.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="bib.bib15.2.m2.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     analysis. a method for the correlation of biological activity and chemical structure.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">
      Journal of the American Chemical Society
     </em>
     , 86(8):1616–1626, 1964.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hansch and Leo 1995
    </span>
    <span class="ltx_bibblock">
     Corwin Hansch and Albert Leo.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Exploring QSAR.: Fundamentals and applications in chemistry and biology
     </em>
     , volume 1.
    </span>
    <span class="ltx_bibblock">
     American Chemical Society, 1995.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Topliss and Costello 1972
    </span>
    <span class="ltx_bibblock">
     John G Topliss and Robert J Costello.
    </span>
    <span class="ltx_bibblock">
     Chance correlations in structure-activity studies using multiple regression analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      Journal of Medicinal Chemistry
     </em>
     , 15(10):1066–1068, 1972.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weinstein et al. 1992
    </span>
    <span class="ltx_bibblock">
     John N. Weinstein, Kurt W. Kohn, Michael R. Grever, Vellarkad N. Viswanadhan, Lawrence V. Rubinstein, Anne P. Monks, Dominic A. Scudiero, Lester Welch, Antonis D. Koutsoukos, August J. Chiausa, and Kenneth D. Paull.
    </span>
    <span class="ltx_bibblock">
     Neural Computing in Cancer Drug Development: Predicting Mechanism of Action.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Science
     </em>
     , 258(5081):447–451, October 1992.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/science.1411538" target="_blank" title="">
      10.1126/science.1411538
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/10.1126/science.1411538" target="_blank" title="">
      https://www.science.org/doi/10.1126/science.1411538
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Van Osdol et al. 1994
    </span>
    <span class="ltx_bibblock">
     William W Van Osdol, Timothy G Myers, Kenneth D Paull, Kurt W Kohn, and John N Weinstein.
    </span>
    <span class="ltx_bibblock">
     Use of the kohonen self-organizing map to study the mechanisms of action of chemotherapeutic agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      JNCI: Journal of the National Cancer Institute
     </em>
     , 86(24):1853–1859, 1994.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goldman and Walters 2006
    </span>
    <span class="ltx_bibblock">
     Brian B Goldman and W Patrick Walters.
    </span>
    <span class="ltx_bibblock">
     Machine learning in computational chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Annual Reports in Computational Chemistry
     </em>
     , 2:127–140, 2006.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pereira and Williams 2007
    </span>
    <span class="ltx_bibblock">
     DA Pereira and JA Williams.
    </span>
    <span class="ltx_bibblock">
     Origin and evolution of high throughput screening.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      British journal of pharmacology
     </em>
     , 152(1):53–61, 2007.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Medina-Franco et al. 2013
    </span>
    <span class="ltx_bibblock">
     José L. Medina-Franco, Marc A. Giulianotti, Gregory S. Welmaker, and Richard A. Houghten.
    </span>
    <span class="ltx_bibblock">
     Shifting from the single to the multitarget paradigm in drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Drug Discovery Today
     </em>
     , 18(9):495–501, May 2013.
    </span>
    <span class="ltx_bibblock">
     ISSN 1359-6446.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.drudis.2013.01.008" target="_blank" title="">
      10.1016/j.drudis.2013.01.008
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1359644613000251" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1359644613000251
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Butler et al. 2018
    </span>
    <span class="ltx_bibblock">
     Keith T. Butler, Daniel W. Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh.
    </span>
    <span class="ltx_bibblock">
     Machine learning for molecular and materials science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Nature
     </em>
     , 559(7715):547–555, July 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-018-0337-2" target="_blank" title="">
      10.1038/s41586-018-0337-2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41586-018-0337-2" target="_blank" title="">
      https://www.nature.com/articles/s41586-018-0337-2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rupp et al. 2012
    </span>
    <span class="ltx_bibblock">
     Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Müller, and O. Anatole von Lilienfeld.
    </span>
    <span class="ltx_bibblock">
     Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Physical Review Letters
     </em>
     , 108(5):058301, January 2012.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1103/PhysRevLett.108.058301" target="_blank" title="">
      10.1103/PhysRevLett.108.058301
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.aps.org/doi/10.1103/PhysRevLett.108.058301" target="_blank" title="">
      https://link.aps.org/doi/10.1103/PhysRevLett.108.058301
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Physical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Olivecrona et al. 2017
    </span>
    <span class="ltx_bibblock">
     Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen.
    </span>
    <span class="ltx_bibblock">
     Molecular de-novo design through deep reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      J. Cheminform.
     </em>
     , 9(1):48, September 2017.
    </span>
    <span class="ltx_bibblock">
     ISSN 1758-2946,1758-2946.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1186/s13321-017-0235-x" target="_blank" title="">
      10.1186/s13321-017-0235-x
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0235-x" target="_blank" title="">
      https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0235-x
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Segler et al. 2018a
    </span>
    <span class="ltx_bibblock">
     Marwin H S Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller.
    </span>
    <span class="ltx_bibblock">
     Generating focused molecule libraries for drug discovery with recurrent neural networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      ACS Cent. Sci.
     </em>
     , 4(1):120–131, January 2018a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943,2374-7951.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.7b00512" target="_blank" title="">
      10.1021/acscentsci.7b00512
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00512" target="_blank" title="">
      https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00512
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Segler et al. 2018b
    </span>
    <span class="ltx_bibblock">
     Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller.
    </span>
    <span class="ltx_bibblock">
     Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural Networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      ACS Central Science
     </em>
     , 4(1):120–131, January 2018b.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.7b00512" target="_blank" title="">
      10.1021/acscentsci.7b00512
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acscentsci.7b00512" target="_blank" title="">
      https://doi.org/10.1021/acscentsci.7b00512
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gupta et al. 2018
    </span>
    <span class="ltx_bibblock">
     Anvita Gupta, Alex T Muller, Berend J H Huisman, Jens A Fuchs, Petra Schneider, and Gisbert Schneider.
    </span>
    <span class="ltx_bibblock">
     Generative recurrent networks for de novo drug design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      Mol. Inform.
     </em>
     , 37(1-2):1700111, January 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 1868-1751,1868-1743.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/minf.201700111" target="_blank" title="">
      10.1002/minf.201700111
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700111" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700111
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Karpov et al. 2020
    </span>
    <span class="ltx_bibblock">
     Pavel Karpov, Guillaume Godin, and Igor V Tetko.
    </span>
    <span class="ltx_bibblock">
     Transformer-CNN: Swiss knife for QSAR modeling and interpretation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      J. Cheminform.
     </em>
     , 12(1):17, March 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1758-2946,1758-2946.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1186/s13321-020-00423-w" target="_blank" title="">
      10.1186/s13321-020-00423-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/articles/10.1186/s13321-020-00423-w" target="_blank" title="">
      https://link.springer.com/articles/10.1186/s13321-020-00423-w
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schütt et al. 2018
    </span>
    <span class="ltx_bibblock">
     K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Müller.
    </span>
    <span class="ltx_bibblock">
     SchNet – A deep learning architecture for molecules and materials.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      The Journal of Chemical Physics
     </em>
     , 148(24):241722, March 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 0021-9606.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1063/1.5019779" target="_blank" title="">
      10.1063/1.5019779
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1063/1.5019779" target="_blank" title="">
      https://doi.org/10.1063/1.5019779
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hirohara et al. 2018
    </span>
    <span class="ltx_bibblock">
     Maya Hirohara, Yutaka Saito, Yuki Koda, Kengo Sato, and Yasubumi Sakakibara.
    </span>
    <span class="ltx_bibblock">
     Convolutional neural network based on SMILES representation of compounds for detecting chemical motif.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      BMC Bioinformatics
     </em>
     , 19(Suppl 19):526, December 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 1471-2105,1471-2105.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1186/s12859-018-2523-5" target="_blank" title="">
      10.1186/s12859-018-2523-5
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2523-5" target="_blank" title="">
      https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2523-5
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Coley et al. 2019a
    </span>
    <span class="ltx_bibblock">
     Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola, William H Green, Regina Barzilay, and Klavs F Jensen.
    </span>
    <span class="ltx_bibblock">
     A graph-convolutional neural network model for the prediction of chemical reactivity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Chem. Sci.
     </em>
     , 10(2):370–377, January 2019a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6520,2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/c8sc04228d" target="_blank" title="">
      10.1039/c8sc04228d
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc04228d" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc04228d
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dwivedi et al. 2023
    </span>
    <span class="ltx_bibblock">
     Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
    </span>
    <span class="ltx_bibblock">
     Benchmarking graph neural networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      Journal of Machine Learning Research
     </em>
     , 24(43):1–48, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sanchez-Lengeling et al. 2021
    </span>
    <span class="ltx_bibblock">
     Benjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B Wiltschko.
    </span>
    <span class="ltx_bibblock">
     A gentle introduction to graph neural networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      Distill
     </em>
     , 6(9):e33, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bronstein et al. 2017
    </span>
    <span class="ltx_bibblock">
     Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
    </span>
    <span class="ltx_bibblock">
     Geometric deep learning: going beyond euclidean data.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      IEEE Signal Processing Magazine
     </em>
     , 34(4):18–42, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2020
    </span>
    <span class="ltx_bibblock">
     Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip.
    </span>
    <span class="ltx_bibblock">
     A comprehensive survey on graph neural networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      IEEE transactions on neural networks and learning systems
     </em>
     , 32(1):4–24, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gilmer et al. 2017
    </span>
    <span class="ltx_bibblock">
     Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.
    </span>
    <span class="ltx_bibblock">
     Neural message passing for quantum chemistry.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      International conference on machine learning
     </em>
     , pages 1263–1272. PMLR, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gómez-Bombarelli et al. 2018
    </span>
    <span class="ltx_bibblock">
     Rafael Gómez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alán Aspuru-Guzik.
    </span>
    <span class="ltx_bibblock">
     Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      ACS Central Science
     </em>
     , 4(2):268–276, February 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.7b00572" target="_blank" title="">
      10.1021/acscentsci.7b00572
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acscentsci.7b00572" target="_blank" title="">
      https://doi.org/10.1021/acscentsci.7b00572
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gaudelet et al. 2021
    </span>
    <span class="ltx_bibblock">
     Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian Tang, et al.
    </span>
    <span class="ltx_bibblock">
     Utilizing graph machine learning within drug discovery and development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      Briefings in bioinformatics
     </em>
     , 22(6):bbab159, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Choudhary et al. 2022
    </span>
    <span class="ltx_bibblock">
     Kamal Choudhary, Brian DeCost, Chi Chen, Anubhav Jain, Francesca Tavazza, Ryan Cohn, Cheol Woo Park, Alok Choudhary, Ankit Agrawal, Simon J. L. Billinge, Elizabeth Holm, Shyue Ping Ong, and Chris Wolverton.
    </span>
    <span class="ltx_bibblock">
     Recent advances and applications of deep learning methods in materials science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      npj Computational Materials
     </em>
     , 8(1):1–26, April 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2057-3960.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41524-022-00734-6" target="_blank" title="">
      10.1038/s41524-022-00734-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41524-022-00734-6" target="_blank" title="">
      https://www.nature.com/articles/s41524-022-00734-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fung et al. 2021
    </span>
    <span class="ltx_bibblock">
     Victor Fung, Jiaxin Zhang, Eric Juarez, and Bobby G Sumpter.
    </span>
    <span class="ltx_bibblock">
     Benchmarking graph neural networks for materials chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Npj Comput. Mater.
     </em>
     , 7(1):1–8, June 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 2057-3960,2057-3960.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41524-021-00554-0" target="_blank" title="">
      10.1038/s41524-021-00554-0
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41524-021-00554-0" target="_blank" title="">
      https://www.nature.com/articles/s41524-021-00554-0
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reiser et al. 2022
    </span>
    <span class="ltx_bibblock">
     Patrick Reiser, Marlen Neubert, André Eberhard, Luca Torresi, Chen Zhou, Chen Shao, Houssam Metni, Clint van Hoesel, Henrik Schopmans, Timo Sommer, and Pascal Friederich.
    </span>
    <span class="ltx_bibblock">
     Graph neural networks for materials science and chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      Commun. Mater.
     </em>
     , 3(1):93, November 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2662-4443,2662-4443.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s43246-022-00315-6" target="_blank" title="">
      10.1038/s43246-022-00315-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s43246-022-00315-6" target="_blank" title="">
      https://www.nature.com/articles/s43246-022-00315-6
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weininger 1988
    </span>
    <span class="ltx_bibblock">
     David Weininger.
    </span>
    <span class="ltx_bibblock">
     SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      Journal of Chemical Information and Computer Sciences
     </em>
     , 28(1):31–36, February 1988.
    </span>
    <span class="ltx_bibblock">
     ISSN 0095-2338.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/ci00057a005" target="_blank" title="">
      10.1021/ci00057a005
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/ci00057a005" target="_blank" title="">
      https://doi.org/10.1021/ci00057a005
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chithrananda et al. 2020
    </span>
    <span class="ltx_bibblock">
     Seyone Chithrananda, Gabe Grand, and Bharath Ramsundar.
    </span>
    <span class="ltx_bibblock">
     ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv [cs.LG]
     </em>
     , October 2020.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2010.09885" target="_blank" title="">
      10.48550/arXiv.2010.09885
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2010.09885" target="_blank" title="">
      http://arxiv.org/abs/2010.09885
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li and Jiang 2021a
    </span>
    <span class="ltx_bibblock">
     Juncai Li and Xiaofei Jiang.
    </span>
    <span class="ltx_bibblock">
     Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      WIRELESS COMMUNICATIONS &amp; MOBILE COMPUTING
     </em>
     , 2021, September 2021a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1530-8669.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1155/2021/7181815" target="_blank" title="">
      10.1155/2021/7181815
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Ye Wang, Honggang Zhao, Simone Sciabola, and Wenlu Wang.
    </span>
    <span class="ltx_bibblock">
     cMolGPT: A conditional generative Pre-Trained transformer for Target-Specific de novo molecular generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Molecules
     </em>
     , 28(11), May 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1420-3049.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/molecules28114430" target="_blank" title="">
      10.3390/molecules28114430
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.3390/molecules28114430" target="_blank" title="">
      http://dx.doi.org/10.3390/molecules28114430
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     M. Bran et al. 2024
    </span>
    <span class="ltx_bibblock">
     Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     Augmenting large language models with chemistry tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      Nature Machine Intelligence
     </em>
     , pages 1–11, May 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839, 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-024-00832-8" target="_blank" title="">
      10.1038/s42256-024-00832-8
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-024-00832-8" target="_blank" title="">
      https://www.nature.com/articles/s42256-024-00832-8
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Boiko et al. 2023
    </span>
    <span class="ltx_bibblock">
     Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes.
    </span>
    <span class="ltx_bibblock">
     Autonomous chemical research with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      Nature
     </em>
     , 624(7992):570–578, December 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0028-0836,1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-023-06792-0" target="_blank" title="">
      10.1038/s41586-023-06792-0
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41586-023-06792-0" target="_blank" title="">
      https://www.nature.com/articles/s41586-023-06792-0
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     White 2023
    </span>
    <span class="ltx_bibblock">
     Andrew D White.
    </span>
    <span class="ltx_bibblock">
     The future of chemistry is language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      Nature Reviews Chemistry
     </em>
     , 7(7):457–458, July 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2397-3358.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41570-023-00502-0" target="_blank" title="">
      10.1038/s41570-023-00502-0
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41570-023-00502-0" target="_blank" title="">
      https://www.nature.com/articles/s41570-023-00502-0
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang Chen, Xiaohui Fan, Huabin Xing, and Huajun Chen.
    </span>
    <span class="ltx_bibblock">
     Scientific large language models: A survey on biological &amp; chemical domains.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      arXiv [cs.CL]
     </em>
     , January 2024a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.14656" target="_blank" title="">
      http://arxiv.org/abs/2401.14656
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Collison et al. 2008
    </span>
    <span class="ltx_bibblock">
     Christopher J. Collison, Marc J. O’Donnell, and Jessica L. Alexander.
    </span>
    <span class="ltx_bibblock">
     Complexation between Rhodamine 101 and Single-Walled Carbon Nanotubes Indicative of Solvent-Nanotube Interaction Strength.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      The Journal of Physical Chemistry C
     </em>
     , 112(39):15144–15150, October 2008.
    </span>
    <span class="ltx_bibblock">
     ISSN 1932-7447.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jp804359j" target="_blank" title="">
      10.1021/jp804359j
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/jp804359j" target="_blank" title="">
      https://doi.org/10.1021/jp804359j
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wiegand et al. 2024
    </span>
    <span class="ltx_bibblock">
     Tyler J. Wiegand, Juan S. Sandoval, Jeremy A. Cody, David W. McCamant, and Christopher J. Collison.
    </span>
    <span class="ltx_bibblock">
     Directional Exciton Diffusion, Measured by Subpicosecond Transient Absorption as an Explanation for Squaraine Solar Cell Performance.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      The Journal of Physical Chemistry C
     </em>
     , 128(11):4616–4630, March 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1932-7447, 1932-7455.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jpcc.3c06361" target="_blank" title="">
      10.1021/acs.jpcc.3c06361
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/10.1021/acs.jpcc.3c06361" target="_blank" title="">
      https://pubs.acs.org/doi/10.1021/acs.jpcc.3c06361
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahmadov et al. 2024
    </span>
    <span class="ltx_bibblock">
     Rashad Ahmadov, Shane S. Michtavy, and Marc D. Porosoff.
    </span>
    <span class="ltx_bibblock">
     Dual Functional Materials: At the Interface of Catalysis and Separations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      Langmuir
     </em>
     , 40(19):9833–9841, May 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 0743-7463.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.langmuir.3c03888" target="_blank" title="">
      10.1021/acs.langmuir.3c03888
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.langmuir.3c03888" target="_blank" title="">
      https://doi.org/10.1021/acs.langmuir.3c03888
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fischer et al. 2019
    </span>
    <span class="ltx_bibblock">
     Thomas Fischer, Silvia Gazzola, and Rainer Riedl.
    </span>
    <span class="ltx_bibblock">
     Approaching Target Selectivity by De Novo Drug Design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      Expert Opinion on Drug Discovery
     </em>
     , 14(8):791–803, August 2019.
    </span>
    <span class="ltx_bibblock">
     ISSN 1746-0441.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1080/17460441.2019.1615435" target="_blank" title="">
      10.1080/17460441.2019.1615435
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/17460441.2019.1615435" target="_blank" title="">
      https://doi.org/10.1080/17460441.2019.1615435
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Taylor &amp; Francis _eprint: https://doi.org/10.1080/17460441.2019.1615435.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2022a
    </span>
    <span class="ltx_bibblock">
     Zhuo Wang, Zhehao Sun, Hang Yin, Xinghui Liu, Jinlan Wang, Haitao Zhao, Cheng Heng Pang, Tao Wu, Shuzhou Li, Zongyou Yin, and Xue-Feng Yu.
    </span>
    <span class="ltx_bibblock">
     Data-Driven Materials Innovation and Applications.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      Advanced Materials
     </em>
     , 34(36):2104113, 2022a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1521-4095.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/adma.202104113" target="_blank" title="">
      10.1002/adma.202104113
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.202104113" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.202104113
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/adma.202104113.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sridharan et al. 2022
    </span>
    <span class="ltx_bibblock">
     Bhuvanesh Sridharan, Manan Goel, and U. Deva Priyakumar.
    </span>
    <span class="ltx_bibblock">
     Modern machine learning for tackling inverse problems in chemistry: molecular design to realization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      Chemical Communications
     </em>
     , 58(35):5316–5331, April 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1364-548X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D1CC07035E" target="_blank" title="">
      10.1039/D1CC07035E
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2022/cc/d1cc07035e" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2022/cc/d1cc07035e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2018
    </span>
    <span class="ltx_bibblock">
     Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande.
    </span>
    <span class="ltx_bibblock">
     MoleculeNet: a benchmark for molecular machine learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      Chemical Science
     </em>
     , 9(2):513–530, 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6520, 2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/C7SC02664A" target="_blank" title="">
      10.1039/C7SC02664A
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://xlink.rsc.org/?DOI=C7SC02664A" target="_blank" title="">
      http://xlink.rsc.org/?DOI=C7SC02664A
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Restrepo 2022
    </span>
    <span class="ltx_bibblock">
     Guillermo Restrepo.
    </span>
    <span class="ltx_bibblock">
     Chemical space: limits, evolution and modelling of an object bigger than our universal library.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">
      Digital Discovery
     </em>
     , 1(5):568–585, October 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D2DD00030J" target="_blank" title="">
      10.1039/D2DD00030J
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00030j" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00030j
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: RSC.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kirkpatrick and Ellis 2004
    </span>
    <span class="ltx_bibblock">
     Peter Kirkpatrick and Clare Ellis.
    </span>
    <span class="ltx_bibblock">
     Chemical space.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">
      Nature
     </em>
     , 432(7019):823–824, 2004.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mullard et al. 2017
    </span>
    <span class="ltx_bibblock">
     Asher Mullard et al.
    </span>
    <span class="ltx_bibblock">
     The drug-maker’s guide to the galaxy.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">
      Nature
     </em>
     , 549(7673):445–447, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Llanos et al. 2019
    </span>
    <span class="ltx_bibblock">
     Eugenio J Llanos, Wilmer Leal, Duc H Luu, Jürgen Jost, Peter F Stadler, and Guillermo Restrepo.
    </span>
    <span class="ltx_bibblock">
     Exploration of the chemical space and its three historical regimes.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">
      Proceedings of the National Academy of Sciences
     </em>
     , 116(26):12660–12665, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schrier et al. 2023
    </span>
    <span class="ltx_bibblock">
     Joshua Schrier, Alexander J. Norquist, Tonio Buonassisi, and Jakoah Brgoch.
    </span>
    <span class="ltx_bibblock">
     In Pursuit of the Exceptional: Research Directions for Machine Learning in Chemical and Materials Science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">
      Journal of the American Chemical Society
     </em>
     , 145(40):21699–21716, October 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0002-7863.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jacs.3c04783" target="_blank" title="">
      10.1021/jacs.3c04783
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/jacs.3c04783" target="_blank" title="">
      https://doi.org/10.1021/jacs.3c04783
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gromski et al. 2019
    </span>
    <span class="ltx_bibblock">
     Piotr S Gromski, Alon B Henson, Jarosław M Granda, and Leroy Cronin.
    </span>
    <span class="ltx_bibblock">
     How to explore chemical space using algorithms and automation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">
      Nature Reviews Chemistry
     </em>
     , 3(2):119–128, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Steiner et al. 2019
    </span>
    <span class="ltx_bibblock">
     Sebastian Steiner, Jakob Wolf, Stefan Glatzel, Anna Andreou, Jarosław M Granda, Graham Keenan, Trevor Hinkley, Gerardo Aragon-Camarasa, Philip J Kitson, Davide Angelone, et al.
    </span>
    <span class="ltx_bibblock">
     Organic synthesis in a modular robotic system driven by a chemical programming language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">
      Science
     </em>
     , 363(6423):eaav2211, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Burger et al. 2020
    </span>
    <span class="ltx_bibblock">
     Benjamin Burger, Phillip M Maffettone, Vladimir V Gusev, Catherine M Aitchison, Yang Bai, Xiaoyan Wang, Xiaobo Li, Ben M Alston, Buyi Li, Rob Clowes, et al.
    </span>
    <span class="ltx_bibblock">
     A mobile robotic chemist.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">
      Nature
     </em>
     , 583(7815):237–241, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     MacLeod et al. 2020
    </span>
    <span class="ltx_bibblock">
     Benjamin P MacLeod, Fraser GL Parlane, Thomas D Morrissey, Florian Häse, Loïc M Roch, Kevan E Dettelbach, Raphaell Moreira, Lars PE Yunker, Michael B Rooney, Joseph R Deeth, et al.
    </span>
    <span class="ltx_bibblock">
     Self-driving laboratory for accelerated discovery of thin-film materials.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">
      Science Advances
     </em>
     , 6(20):eaaz8867, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rumelhart et al. 1986
    </span>
    <span class="ltx_bibblock">
     David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    </span>
    <span class="ltx_bibblock">
     Learning internal representations by error propagation, parallel distributed processing, explorations in the microstructure of cognition, ed. de rumelhart and j. mcclelland. vol. 1. 1986.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">
      Biometrika
     </em>
     , 71:599–607, 1986.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib68">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hochreiter and Schmidhuber 1997
    </span>
    <span class="ltx_bibblock">
     Sepp Hochreiter and Jürgen Schmidhuber.
    </span>
    <span class="ltx_bibblock">
     Long short-term memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">
      Neural computation
     </em>
     , 9(8):1735–1780, 1997.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib69">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ribeiro et al. 2020
    </span>
    <span class="ltx_bibblock">
     Antônio H. Ribeiro, Koen Tiels, Luis A. Aguirre, and Thomas B. Schön.
    </span>
    <span class="ltx_bibblock">
     Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness, March 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1906.08482" target="_blank" title="">
      http://arxiv.org/abs/1906.08482
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1906.08482 [cs, math, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib70">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Or 2023
    </span>
    <span class="ltx_bibblock">
     Dr Barak Or.
    </span>
    <span class="ltx_bibblock">
     The Exploding and Vanishing Gradients Problem in Time Series, December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/metaor-artificial-intelligence/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22" target="_blank" title="">
      https://medium.com/metaor-artificial-intelligence/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib71">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al. 2017
    </span>
    <span class="ltx_bibblock">
     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.
    </span>
    <span class="ltx_bibblock">
     Attention is all you need.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">
      arXiv [cs.CL]
     </em>
     , June 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1706.03762" target="_blank" title="">
      http://arxiv.org/abs/1706.03762
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib72">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gu and Dao 2023
    </span>
    <span class="ltx_bibblock">
     Albert Gu and Tri Dao.
    </span>
    <span class="ltx_bibblock">
     Mamba: Linear-time sequence modeling with selective state spaces.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">
      arXiv [cs.LG]
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.00752" target="_blank" title="">
      http://arxiv.org/abs/2312.00752
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib73">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jelassi et al. 2024
    </span>
    <span class="ltx_bibblock">
     Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach.
    </span>
    <span class="ltx_bibblock">
     Repeat after me: Transformers are better than state space models at copying.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">
      arXiv [cs.LG]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.01032" target="_blank" title="">
      http://arxiv.org/abs/2402.01032
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib74">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.
    </span>
    <span class="ltx_bibblock">
     Rwkv: Reinventing rnns for the transformer era.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">
      arXiv preprint arXiv:2305.13048
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib75">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Beck et al. 2024
    </span>
    <span class="ltx_bibblock">
     Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.
    </span>
    <span class="ltx_bibblock">
     xLSTM: Extended Long Short-Term Memory, May 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.04517" target="_blank" title="">
      http://arxiv.org/abs/2405.04517
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2405.04517 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib76">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Minaee et al. 2024
    </span>
    <span class="ltx_bibblock">
     Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao.
    </span>
    <span class="ltx_bibblock">
     Large language models: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.06196" target="_blank" title="">
      http://arxiv.org/abs/2402.06196
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib77">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     ann 2022
    </span>
    <span class="ltx_bibblock">
     The Annotated Transformer, 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nlp.seas.harvard.edu/annotated-transformer/" target="_blank" title="">
      https://nlp.seas.harvard.edu/annotated-transformer/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib78">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bahdanau et al. 2016
    </span>
    <span class="ltx_bibblock">
     Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
    </span>
    <span class="ltx_bibblock">
     Neural Machine Translation by Jointly Learning to Align and Translate, May 2016.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1409.0473" target="_blank" title="">
      http://arxiv.org/abs/1409.0473
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1409.0473 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib79">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kudo 2018
    </span>
    <span class="ltx_bibblock">
     Taku Kudo.
    </span>
    <span class="ltx_bibblock">
     Subword regularization: Improving neural network translation models with multiple subword candidates.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">
      arXiv [cs.CL]
     </em>
     , April 2018.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1804.10959" target="_blank" title="">
      http://arxiv.org/abs/1804.10959
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib80">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kudo and Richardson 2018
    </span>
    <span class="ltx_bibblock">
     Taku Kudo and John Richardson.
    </span>
    <span class="ltx_bibblock">
     SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">
      arXiv [cs.CL]
     </em>
     , August 2018.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1808.06226" target="_blank" title="">
      http://arxiv.org/abs/1808.06226
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib81">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al. 2020
    </span>
    <span class="ltx_bibblock">
     Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Fast WordPiece tokenization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">
      arXiv [cs.CL]
     </em>
     , December 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2012.15524" target="_blank" title="">
      http://arxiv.org/abs/2012.15524
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib82">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rust et al. 2020
    </span>
    <span class="ltx_bibblock">
     Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna Gurevych.
    </span>
    <span class="ltx_bibblock">
     How good is your tokenizer? on the monolingual performance of multilingual language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">
      arXiv [cs.CL]
     </em>
     , December 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2012.15613" target="_blank" title="">
      http://arxiv.org/abs/2012.15613
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib83">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Berglund and van der Merwe 2023
    </span>
    <span class="ltx_bibblock">
     Martin Berglund and Brink van der Merwe.
    </span>
    <span class="ltx_bibblock">
     Formalizing BPE tokenization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">
      arXiv [cs.FL]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.08715" target="_blank" title="">
      http://arxiv.org/abs/2309.08715
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib84">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gehring et al. 2017
    </span>
    <span class="ltx_bibblock">
     Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.
    </span>
    <span class="ltx_bibblock">
     Convolutional sequence to sequence learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">
      arXiv [cs.CL]
     </em>
     , pages 1243–1252, May 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v70/gehring17a.html" target="_blank" title="">
      https://proceedings.mlr.press/v70/gehring17a.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib85">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nair and Hinton 2010
    </span>
    <span class="ltx_bibblock">
     Vinod Nair and Geoffrey E Hinton.
    </span>
    <span class="ltx_bibblock">
     Rectified linear units improve restricted boltzmann machines.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">
      Proceedings of the 27th international conference on machine learning (ICML-10)
     </em>
     , pages 807–814, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib86">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shazeer 2020
    </span>
    <span class="ltx_bibblock">
     Noam Shazeer.
    </span>
    <span class="ltx_bibblock">
     GLU Variants Improve Transformer, February 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2002.05202" target="_blank" title="">
      http://arxiv.org/abs/2002.05202
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2002.05202 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib87">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hendrycks and Gimpel 2023
    </span>
    <span class="ltx_bibblock">
     Dan Hendrycks and Kevin Gimpel.
    </span>
    <span class="ltx_bibblock">
     Gaussian Error Linear Units (GELUs), June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1606.08415" target="_blank" title="">
      http://arxiv.org/abs/1606.08415
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1606.08415 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib88">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stiennon et al. 2020
    </span>
    <span class="ltx_bibblock">
     Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano.
    </span>
    <span class="ltx_bibblock">
     Learning to summarize with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 33:3008–3021, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib89">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. 2023
    </span>
    <span class="ltx_bibblock">
     Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong.
    </span>
    <span class="ltx_bibblock">
     Large language model alignment: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">
      arXiv [cs.CL]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.15025" target="_blank" title="">
      http://arxiv.org/abs/2309.15025
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib90">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Duan et al. 2016
    </span>
    <span class="ltx_bibblock">
     Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel.
    </span>
    <span class="ltx_bibblock">
     RL
     <sup class="ltx_sup" id="bib.bib90.2.1">
      <span class="ltx_text ltx_font_italic" id="bib.bib90.2.1.1">
       2
      </span>
     </sup>
     : Fast reinforcement learning via slow reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">
      arXiv [cs.AI]
     </em>
     , November 2016.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1611.02779" target="_blank" title="">
      http://arxiv.org/abs/1611.02779
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib91">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ziegler et al. 2019
    </span>
    <span class="ltx_bibblock">
     Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.
    </span>
    <span class="ltx_bibblock">
     Fine-tuning language models from human preferences.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">
      arXiv [cs.CL]
     </em>
     , September 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.08593" target="_blank" title="">
      http://arxiv.org/abs/1909.08593
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib92">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mazuz et al. 2023
    </span>
    <span class="ltx_bibblock">
     Eyal Mazuz, Guy Shtar, Bracha Shapira, and Lior Rokach.
    </span>
    <span class="ltx_bibblock">
     Molecule generation using transformers and policy gradient reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">
      Sci. Rep.
     </em>
     , 13(1):8799, May 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2045-2322.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41598-023-35648-w" target="_blank" title="">
      10.1038/s41598-023-35648-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41598-023-35648-w" target="_blank" title="">
      http://dx.doi.org/10.1038/s41598-023-35648-w
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib93">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Laskin et al. 2022
    </span>
    <span class="ltx_bibblock">
     Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, D J Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih.
    </span>
    <span class="ltx_bibblock">
     In-context reinforcement learning with algorithm distillation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">
      arXiv [cs.LG]
     </em>
     , October 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.14215" target="_blank" title="">
      http://arxiv.org/abs/2210.14215
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib94">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ope 2022
    </span>
    <span class="ltx_bibblock">
     Aligning language models to follow instructions.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/instruction-following" target="_blank" title="">
      https://openai.com/index/instruction-following
     </a>
     , 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/instruction-following" target="_blank" title="">
      https://openai.com/index/instruction-following
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-5-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib95">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. 2023
    </span>
    <span class="ltx_bibblock">
     Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo.
    </span>
    <span class="ltx_bibblock">
     Aligning large language models through synthetic feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.13735" target="_blank" title="">
      http://arxiv.org/abs/2305.13735
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib96">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schulman et al. 2017
    </span>
    <span class="ltx_bibblock">
     John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
    </span>
    <span class="ltx_bibblock">
     Proximal policy optimization algorithms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">
      arXiv [cs.LG]
     </em>
     , July 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.06347" target="_blank" title="">
      http://arxiv.org/abs/1707.06347
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib97">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2020
    </span>
    <span class="ltx_bibblock">
     Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd.
    </span>
    <span class="ltx_bibblock">
     Sample efficient reinforcement learning with REINFORCE.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">
      arXiv [cs.LG]
     </em>
     , October 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2010.11364" target="_blank" title="">
      http://arxiv.org/abs/2010.11364
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib98">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. 2023
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
    </span>
    <span class="ltx_bibblock">
     Reflexion: Language agents with verbal reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">
      arXiv [cs.AI]
     </em>
     , March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.11366" target="_blank" title="">
      http://arxiv.org/abs/2303.11366
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib99">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Akyurek et al. 2023
    </span>
    <span class="ltx_bibblock">
     Afra Feyza Akyurek, Ekin Akyurek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon.
    </span>
    <span class="ltx_bibblock">
     RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.08844" target="_blank" title="">
      http://arxiv.org/abs/2305.08844
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib100">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cao et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua Zhao, and Yun Li.
    </span>
    <span class="ltx_bibblock">
     Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">
      arXiv [cs.LG]
     </em>
     , March 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.00282" target="_blank" title="">
      http://arxiv.org/abs/2404.00282
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib101">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rafailov et al. 2023
    </span>
    <span class="ltx_bibblock">
     Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn.
    </span>
    <span class="ltx_bibblock">
     Direct preference optimization: Your language model is secretly a reward model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">
      arXiv [cs.LG]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.18290" target="_blank" title="">
      http://arxiv.org/abs/2305.18290
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib102">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. 2024
    </span>
    <span class="ltx_bibblock">
     Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu.
    </span>
    <span class="ltx_bibblock">
     Is DPO superior to PPO for LLM alignment? a comprehensive study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">
      arXiv [cs.CL]
     </em>
     , April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.10719" target="_blank" title="">
      http://arxiv.org/abs/2404.10719
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib103">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bou et al. 2024
    </span>
    <span class="ltx_bibblock">
     Albert Bou, Morgan Thomas, Sebastian Dittert, Carles Navarro Ramírez, Maciej Majewski, Ye Wang, Shivam Patel, Gary Tresadern, Mazen Ahmad, Vincent Moens, et al.
    </span>
    <span class="ltx_bibblock">
     Acegen: Reinforcement learning of generative chemical agents for drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">
      arXiv preprint arXiv:2405.04657
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib104">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hayes et al. 2024
    </span>
    <span class="ltx_bibblock">
     Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J. Bartie, Patrick D. Hsu, Tom Sercu, Salvatore Candido, and Alexander Rives.
    </span>
    <span class="ltx_bibblock">
     Simulating 500 million years of evolution with a language model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">
      self-hosted preprint
     </em>
     , 2024.
    </span>
    <span class="ltx_bibblock">
     Preprint.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib105">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Devlin et al. 2018
    </span>
    <span class="ltx_bibblock">
     Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    </span>
    <span class="ltx_bibblock">
     BERT: Pre-training of deep bidirectional Transformers for language understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">
      arXiv [cs.CL]
     </em>
     , October 2018.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1810.04805" target="_blank" title="">
      10.48550/arXiv.1810.04805
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1810.04805" target="_blank" title="">
      http://arxiv.org/abs/1810.04805
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib106">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Taylor 1953
    </span>
    <span class="ltx_bibblock">
     Wilson L. Taylor.
    </span>
    <span class="ltx_bibblock">
     “Cloze Procedure”: A New Tool for Measuring Readability.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">
      Journalism Quarterly
     </em>
     , 30(4):415–433, September 1953.
    </span>
    <span class="ltx_bibblock">
     ISSN 0022-5533.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1177/107769905303000401" target="_blank" title="">
      10.1177/107769905303000401
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1177/107769905303000401" target="_blank" title="">
      https://doi.org/10.1177/107769905303000401
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: SAGE Publications.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib107">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2019a
    </span>
    <span class="ltx_bibblock">
     Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
    </span>
    <span class="ltx_bibblock">
     RoBERTa: A Robustly Optimized BERT Pretraining Approach, July 2019a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1907.11692" target="_blank" title="">
      http://arxiv.org/abs/1907.11692
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1907.11692 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib108">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. 2018
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
    </span>
    <span class="ltx_bibblock">
     Improving language understanding with unsupervised learning.
    </span>
    <span class="ltx_bibblock">
     2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib109">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewis et al. 2019
    </span>
    <span class="ltx_bibblock">
     Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
    </span>
    <span class="ltx_bibblock">
     BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, October 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1910.13461" target="_blank" title="">
      http://arxiv.org/abs/1910.13461
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1910.13461 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib110">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Raffel et al. 2019
    </span>
    <span class="ltx_bibblock">
     Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.
    </span>
    <span class="ltx_bibblock">
     Exploring the limits of transfer learning with a unified text-to-text transformer.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">
      arXiv [cs.LG]
     </em>
     , October 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1910.10683" target="_blank" title="">
      http://arxiv.org/abs/1910.10683
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib111">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chung et al. 2022
    </span>
    <span class="ltx_bibblock">
     Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, and Jason Wei.
    </span>
    <span class="ltx_bibblock">
     Scaling instruction-finetuned language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">
      arXiv [cs.LG]
     </em>
     , October 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.11416" target="_blank" title="">
      http://arxiv.org/abs/2210.11416
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib112">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bowen Tan, Yun Zhu, Lijuan Liu, Eric Xing, Zhiting Hu, and Jindong Chen.
    </span>
    <span class="ltx_bibblock">
     Cappy: Outperforming and boosting large multi-task LMs with a small scorer.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">
      arXiv [cs.LG]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2311.06720" target="_blank" title="">
      10.48550/arXiv.2311.06720
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b860c0c546f4a3a786f9c9468228c99f-Paper-Conference.pdf" target="_blank" title="">
      https://proceedings.neurips.cc/paper_files/paper/2023/file/b860c0c546f4a3a786f9c9468228c99f-Paper-Conference.pdf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib113">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi.
    </span>
    <span class="ltx_bibblock">
     Tag-LLM: Repurposing general-purpose LLMs for specialized domains.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">
      arXiv [cs.LG]
     </em>
     , February 2024a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.05140" target="_blank" title="">
      http://arxiv.org/abs/2402.05140
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib114">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Son et al. 2024
    </span>
    <span class="ltx_bibblock">
     Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, and Seungone Kim.
    </span>
    <span class="ltx_bibblock">
     Multi-task inference: Can large language models follow multiple instructions at once?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.11597" target="_blank" title="">
      http://arxiv.org/abs/2402.11597
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib115">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al. 2024
    </span>
    <span class="ltx_bibblock">
     Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, and Hao Wang.
    </span>
    <span class="ltx_bibblock">
     Mixture-of-LoRAs: An efficient multitask tuning for large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">
      arXiv [cs.CL]
     </em>
     , March 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.03432" target="_blank" title="">
      http://arxiv.org/abs/2403.03432
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib116">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     FUY 2023
    </span>
    <span class="ltx_bibblock">
     Fuyu-8B: A multimodal architecture for AI agents.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adept.ai/blog/fuyu-8b" target="_blank" title="">
      https://www.adept.ai/blog/fuyu-8b
     </a>
     , 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adept.ai/blog/fuyu-8b" target="_blank" title="">
      https://www.adept.ai/blog/fuyu-8b
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2023-11-8.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib117">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.
    </span>
    <span class="ltx_bibblock">
     NExT-GPT: Any-to-any multimodal LLM.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">
      arXiv [cs.AI]
     </em>
     , September 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.05519" target="_blank" title="">
      http://arxiv.org/abs/2309.05519
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib118">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bolton et al. 2024
    </span>
    <span class="ltx_bibblock">
     Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, and Christopher D Manning.
    </span>
    <span class="ltx_bibblock">
     BioMedLM: A 2.7B parameter language model trained on biomedical text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">
      arXiv [cs.CL]
     </em>
     , March 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.18421" target="_blank" title="">
      http://arxiv.org/abs/2403.18421
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib119">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Labrak et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour.
    </span>
    <span class="ltx_bibblock">
     BioMistral: A collection of open-source pretrained large language models for medical domains.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.10373" target="_blank" title="">
      http://arxiv.org/abs/2402.10373
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib120">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pieri et al. 2024
    </span>
    <span class="ltx_bibblock">
     Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, and Hisham Cholakkal.
    </span>
    <span class="ltx_bibblock">
     BiMediX: Bilingual medical mixture of experts LLM.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.13253" target="_blank" title="">
      http://arxiv.org/abs/2402.13253
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib121">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. 2024
    </span>
    <span class="ltx_bibblock">
     Xuyang Zhao, Qibin Zhao, and Toshihisa Tanaka.
    </span>
    <span class="ltx_bibblock">
     EpilepsyLLM: Domain-specific large language model fine-tuned with epilepsy medical knowledge.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">
      arXiv [cs.CL]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.05908" target="_blank" title="">
      http://arxiv.org/abs/2401.05908
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib122">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. 2024
    </span>
    <span class="ltx_bibblock">
     Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S Chaudhari, and Curtis Langlotz.
    </span>
    <span class="ltx_bibblock">
     CheXagent: Towards a foundation model for chest X-ray interpretation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">
      arXiv [cs.CV]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.12208" target="_blank" title="">
      http://arxiv.org/abs/2401.12208
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib123">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie.
    </span>
    <span class="ltx_bibblock">
     BioMedGPT: Open multimodal generative pre-trained transformer for BioMedicine.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">
      arXiv [cs.CE]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/PharMolix/OpenBioMed" target="_blank" title="">
      https://github.com/PharMolix/OpenBioMed
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib124">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al. 2023
    </span>
    <span class="ltx_bibblock">
     Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, Imran Razzak, and Bram Hoex.
    </span>
    <span class="ltx_bibblock">
     DARWIN series: Domain specific large language models for natural science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">
      arXiv [cs.CL]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.13565" target="_blank" title="">
      http://arxiv.org/abs/2308.13565
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib125">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.
    </span>
    <span class="ltx_bibblock">
     PMC-LLaMA: Towards building open-source language models for medicine.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">
      arXiv [cs.CL]
     </em>
     , April 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.14454" target="_blank" title="">
      http://arxiv.org/abs/2304.14454
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib126">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Taylor et al. 2022
    </span>
    <span class="ltx_bibblock">
     Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
    </span>
    <span class="ltx_bibblock">
     Galactica: A large language model for science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">
      arXiv [cs.CL]
     </em>
     , November 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2211.09085" target="_blank" title="">
      http://arxiv.org/abs/2211.09085
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib127">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. 2022
    </span>
    <span class="ltx_bibblock">
     Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.
    </span>
    <span class="ltx_bibblock">
     BioGPT: generative pre-trained transformer for biomedical text generation and mining.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">
      Brief. Bioinform.
     </em>
     , 23(6), November 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1467-5463, 1477-4054.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1093/bib/bbac409" target="_blank" title="">
      10.1093/bib/bbac409
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1093/bib/bbac409" target="_blank" title="">
      http://dx.doi.org/10.1093/bib/bbac409
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib128">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Livne et al. 2023
    </span>
    <span class="ltx_bibblock">
     Micha Livne, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony Costa, Alex Aliper, Alán Aspuru-Guzik, and Alex Zhavoronkov.
    </span>
    <span class="ltx_bibblock">
     Nach0: Multimodal natural and chemical languages foundation model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">
      arXiv [cs.CL]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.12410" target="_blank" title="">
      http://arxiv.org/abs/2311.12410
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib129">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pei et al. 2024
    </span>
    <span class="ltx_bibblock">
     Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, and Rui Yan.
    </span>
    <span class="ltx_bibblock">
     BioT5+: Towards generalized biological understanding with IUPAC integration and multi-task tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">
      arXiv [q-bio.QM]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.17810" target="_blank" title="">
      http://arxiv.org/abs/2402.17810
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib130">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Christofidellis et al. 2023
    </span>
    <span class="ltx_bibblock">
     Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica.
    </span>
    <span class="ltx_bibblock">
     Unifying molecular and textual representations via multi-task language modelling.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">
      arXiv [cs.LG]
     </em>
     , January 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2301.12586" target="_blank" title="">
      http://arxiv.org/abs/2301.12586
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib131">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Edwards et al. 2022
    </span>
    <span class="ltx_bibblock">
     Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     Translation between molecules and natural language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">
      arXiv [cs.CL]
     </em>
     , April 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2204.11817" target="_blank" title="">
      http://arxiv.org/abs/2204.11817
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib132">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gupta et al. 2022
    </span>
    <span class="ltx_bibblock">
     Tanishq Gupta, Mohd Zaki, N M Anoop Krishnan, and Mausam.
    </span>
    <span class="ltx_bibblock">
     MatSciBERT: A materials domain language model for text mining and information extraction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">
      npj Computational Materials
     </em>
     , 8(1):1–11, May 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2057-3960, 2057-3960.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41524-022-00784-w" target="_blank" title="">
      10.1038/s41524-022-00784-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41524-022-00784-w" target="_blank" title="">
      https://www.nature.com/articles/s41524-022-00784-w
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib133">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gu et al. 2021
    </span>
    <span class="ltx_bibblock">
     Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.
    </span>
    <span class="ltx_bibblock">
     Domain-Specific language model pretraining for biomedical natural language processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">
      ACM Trans. Comput. Healthcare
     </em>
     , 3(1):1–23, October 2021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3458754" target="_blank" title="">
      10.1145/3458754
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3458754" target="_blank" title="">
      https://doi.org/10.1145/3458754
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib134">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Beltagy et al. 2019
    </span>
    <span class="ltx_bibblock">
     Iz Beltagy, Kyle Lo, and Arman Cohan.
    </span>
    <span class="ltx_bibblock">
     SciBERT: A pretrained language model for scientific text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">
      arXiv [cs.CL]
     </em>
     , March 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1903.10676" target="_blank" title="">
      http://arxiv.org/abs/1903.10676
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib135">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al. 2019
    </span>
    <span class="ltx_bibblock">
     Yifan Peng, Shankai Yan, and Zhiyong Lu.
    </span>
    <span class="ltx_bibblock">
     Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">
      arXiv [cs.CL]
     </em>
     , June 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1906.05474" target="_blank" title="">
      http://arxiv.org/abs/1906.05474
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib136">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ock et al. 2023
    </span>
    <span class="ltx_bibblock">
     Janghoon Ock, Chakradhar Guntuboina, and Amir Barati Farimani.
    </span>
    <span class="ltx_bibblock">
     Catalyst Energy Prediction with CatBERTa: Unveiling Feature Exploration Strategies through Large Language Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">
      ACS CATALYSIS
     </em>
     , 13(24):16032–16044, November 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2155-5435.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscatal.3c04956" target="_blank" title="">
      10.1021/acscatal.3c04956
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib137">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. 2022
    </span>
    <span class="ltx_bibblock">
     Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Kyle Chard, and Ian Foster.
    </span>
    <span class="ltx_bibblock">
     The diminishing returns of masked language models to science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">
      arXiv [cs.CL]
     </em>
     , May 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.11342" target="_blank" title="">
      http://arxiv.org/abs/2205.11342
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib138">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. 2019
    </span>
    <span class="ltx_bibblock">
     Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.
    </span>
    <span class="ltx_bibblock">
     ClinicalBERT: Modeling clinical notes and predicting hospital readmission.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">
      arXiv [cs.CL]
     </em>
     , April 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1904.05342" target="_blank" title="">
      http://arxiv.org/abs/1904.05342
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib139">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shetty et al. 2023
    </span>
    <span class="ltx_bibblock">
     Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti, Lauren Holm, Chao Zhang, and Rampi Ramprasad.
    </span>
    <span class="ltx_bibblock">
     A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">
      NPJ Comput Mater
     </em>
     , 9(1):52, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2057-3960.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41524-023-01003-w" target="_blank" title="">
      10.1038/s41524-023-01003-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41524-023-01003-w" target="_blank" title="">
      http://dx.doi.org/10.1038/s41524-023-01003-w
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib140">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Trewartha et al. 2022
    </span>
    <span class="ltx_bibblock">
     Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain.
    </span>
    <span class="ltx_bibblock">
     Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">
      Patterns (N Y)
     </em>
     , 3(4):100488, April 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2666-3899.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.patter.2022.100488" target="_blank" title="">
      10.1016/j.patter.2022.100488
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.patter.2022.100488" target="_blank" title="">
      http://dx.doi.org/10.1016/j.patter.2022.100488
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib141">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. 2022
    </span>
    <span class="ltx_bibblock">
     Jiang Guo, A Santiago Ibanez-Lopez, Hanyu Gao, Victor Quach, Connor W Coley, Klavs F Jensen, and Regina Barzilay.
    </span>
    <span class="ltx_bibblock">
     Automated chemical reaction extraction from scientific literature.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">
      J. Chem. Inf. Model.
     </em>
     , 62(9):2035–2045, May 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.1c00284" target="_blank" title="">
      10.1021/acs.jcim.1c00284
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jcim.1c00284" target="_blank" title="">
      http://dx.doi.org/10.1021/acs.jcim.1c00284
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib142">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shin et al. 2020
    </span>
    <span class="ltx_bibblock">
     Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav Mani.
    </span>
    <span class="ltx_bibblock">
     BioMegatron: Larger biomedical domain language model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">
      arXiv [cs.CL]
     </em>
     , October 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2010.06060" target="_blank" title="">
      http://arxiv.org/abs/2010.06060
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib143">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lee et al. 2020
    </span>
    <span class="ltx_bibblock">
     Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
    </span>
    <span class="ltx_bibblock">
     BioBERT: a pre-trained biomedical language representation model for biomedical text mining.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">
      Bioinformatics
     </em>
     , 36(4):1234–1240, February 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1367-4803, 1367-4811.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1093/bioinformatics/btz682" target="_blank" title="">
      10.1093/bioinformatics/btz682
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1093/bioinformatics/btz682" target="_blank" title="">
      http://dx.doi.org/10.1093/bioinformatics/btz682
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib144">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pei et al. 2023
    </span>
    <span class="ltx_bibblock">
     Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan.
    </span>
    <span class="ltx_bibblock">
     BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations.
    </span>
    <span class="ltx_bibblock">
     In Houda Bouamor, Juan Pino, and Kalika Bali, editors,
     <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 1102–1123, Stroudsburg, PA, USA, December 2023. Association for Computational Linguistics.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.70" target="_blank" title="">
      10.18653/v1/2023.emnlp-main.70
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.70" target="_blank" title="">
      https://aclanthology.org/2023.emnlp-main.70
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib145">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Born and Manica 2023
    </span>
    <span class="ltx_bibblock">
     Jannis Born and Matteo Manica.
    </span>
    <span class="ltx_bibblock">
     Regression Transformer enables concurrent sequence regression and generation for molecular language modelling.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">
      Nat. Mach. Intell.
     </em>
     , 5(4):432–444, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00639-z" target="_blank" title="">
      10.1038/s42256-023-00639-z
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00639-z" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00639-z
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib146">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Irwin et al. 2022
    </span>
    <span class="ltx_bibblock">
     Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum.
    </span>
    <span class="ltx_bibblock">
     Chemformer: a pre-trained transformer for computational chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">
      Machine Learning: Science and Technology
     </em>
     , 3(1):015022, January 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2632-2153.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1088/2632-2153/ac3ffb" target="_blank" title="">
      10.1088/2632-2153/ac3ffb
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dx.doi.org/10.1088/2632-2153/ac3ffb" target="_blank" title="">
      https://dx.doi.org/10.1088/2632-2153/ac3ffb
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: IOP Publishing.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib147">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sagawa and Kojima 2023
    </span>
    <span class="ltx_bibblock">
     Tatsuya Sagawa and Ryosuke Kojima.
    </span>
    <span class="ltx_bibblock">
     ReactionT5: a large-scale pre-trained model towards application of limited reaction data.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">
      arXiv [physics.chem-ph]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.06708" target="_blank" title="">
      http://arxiv.org/abs/2311.06708
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib148">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu and Zhang 2022
    </span>
    <span class="ltx_bibblock">
     Jieyu Lu and Yingkai Zhang.
    </span>
    <span class="ltx_bibblock">
     Unified Deep Learning Model for Multitask Reaction Predictions with Explanation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">
      J. Chem. Inf. Model.
     </em>
     , 62(6):1376–1387, March 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.1c01467" target="_blank" title="">
      10.1021/acs.jcim.1c01467
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.1c01467" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.1c01467
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib149">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yuksel et al. 2023
    </span>
    <span class="ltx_bibblock">
     Atakan Yuksel, Erva Ulusoy, Atabey Ünlü, and Tunca Doğan.
    </span>
    <span class="ltx_bibblock">
     SELFormer: Molecular representation learning via SELFIES language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">
      arXiv [q-bio.QM]
     </em>
     , April 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.04662" target="_blank" title="">
      http://arxiv.org/abs/2304.04662
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib150">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahmad et al. 2022
    </span>
    <span class="ltx_bibblock">
     Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar.
    </span>
    <span class="ltx_bibblock">
     ChemBERTa-2: Towards Chemical Foundation Models, September 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2209.01712" target="_blank" title="">
      http://arxiv.org/abs/2209.01712
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2209.01712 [cs, q-bio].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib151">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ross et al. 2022a
    </span>
    <span class="ltx_bibblock">
     Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das.
    </span>
    <span class="ltx_bibblock">
     Large-scale chemical language representations capture molecular structure and properties.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">
      Nature Machine Intelligence
     </em>
     , 4(12):1256–1264, December 2022a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839, 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-022-00580-7" target="_blank" title="">
      10.1038/s42256-022-00580-7
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-022-00580-7" target="_blank" title="">
      https://www.nature.com/articles/s42256-022-00580-7
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib152">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li and Jiang 2021b
    </span>
    <span class="ltx_bibblock">
     Juncai Li and Xiaofei Jiang.
    </span>
    <span class="ltx_bibblock">
     Mol-BERT: An effective molecular representation with BERT for molecular property prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">
      Proc. Int. Wirel. Commun. Mob. Comput. Conf.
     </em>
     , 2021, September 2021b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1530-8669, 1530-8669.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1155/2021/7181815" target="_blank" title="">
      10.1155/2021/7181815
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.hindawi.com/journals/wcmc/2021/7181815/" target="_blank" title="">
      https://www.hindawi.com/journals/wcmc/2021/7181815/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib153">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jia et al. 2021
    </span>
    <span class="ltx_bibblock">
     Qinjin Jia, Jialin Cui, Yunkai Xiao, Chengyuan Liu, Parvez Rashid, and Edward F. Gehringer.
    </span>
    <span class="ltx_bibblock">
     ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments, October 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2110.03895" target="_blank" title="">
      http://arxiv.org/abs/2110.03895
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2110.03895 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib154">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fabian et al. 2020
    </span>
    <span class="ltx_bibblock">
     Benedek Fabian, Thomas Edlich, Héléna Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed.
    </span>
    <span class="ltx_bibblock">
     Molecular representation learning with language models and domain-relevant auxiliary tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">
      arXiv [cs.LG]
     </em>
     , November 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2011.13230" target="_blank" title="">
      http://arxiv.org/abs/2011.13230
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib155">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2019a
    </span>
    <span class="ltx_bibblock">
     Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang.
    </span>
    <span class="ltx_bibblock">
     SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">
      Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics
     </em>
     , BCB ’19, pages 429–436, New York, NY, USA, September 2019a. Association for Computing Machinery.
    </span>
    <span class="ltx_bibblock">
     ISBN 978-1-4503-6666-3.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3307339.3342186" target="_blank" title="">
      10.1145/3307339.3342186
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3307339.3342186" target="_blank" title="">
      https://doi.org/10.1145/3307339.3342186
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib156">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Honda et al. 2019
    </span>
    <span class="ltx_bibblock">
     Shion Honda, Shoi Shi, and Hiroki R Ueda.
    </span>
    <span class="ltx_bibblock">
     SMILES transformer: Pre-trained molecular fingerprint for low data drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">
      arXiv [cs.LG]
     </em>
     , November 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1911.04738" target="_blank" title="">
      http://arxiv.org/abs/1911.04738
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib157">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lo et al. 2018
    </span>
    <span class="ltx_bibblock">
     Yu-Chen Lo, Stefano E Rensi, Wen Torng, and Russ B Altman.
    </span>
    <span class="ltx_bibblock">
     Machine learning in chemoinformatics and drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">
      Drug discovery today
     </em>
     , 23(8):1538–1546, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib158">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     David et al. 2020
    </span>
    <span class="ltx_bibblock">
     Laurianne David, Amol Thakkar, Rocío Mercado, and Ola Engkvist.
    </span>
    <span class="ltx_bibblock">
     Molecular representations in AI-driven drug discovery: a review and practical guide.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">
      Journal of Cheminformatics
     </em>
     , 12(1):56, September 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1758-2946.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1186/s13321-020-00460-5" target="_blank" title="">
      10.1186/s13321-020-00460-5
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s13321-020-00460-5" target="_blank" title="">
      https://doi.org/10.1186/s13321-020-00460-5
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib159">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Atz et al. 2021
    </span>
    <span class="ltx_bibblock">
     Kenneth Atz, Francesca Grisoni, and Gisbert Schneider.
    </span>
    <span class="ltx_bibblock">
     Geometric deep learning on molecular representations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">
      arXiv [physics.chem-ph]
     </em>
     , July 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-021-00418-8" target="_blank" title="">
      https://www.nature.com/articles/s42256-021-00418-8
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib160">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Walters and Barzilay 2021
    </span>
    <span class="ltx_bibblock">
     W Patrick Walters and Regina Barzilay.
    </span>
    <span class="ltx_bibblock">
     Applications of deep learning in molecule generation and molecular property prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">
      Acc. Chem. Res.
     </em>
     , 54(2):263–270, January 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 0001-4842,1520-4898.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.accounts.0c00699" target="_blank" title="">
      10.1021/acs.accounts.0c00699
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/abs/10.1021/acs.accounts.0c00699" target="_blank" title="">
      https://pubs.acs.org/doi/abs/10.1021/acs.accounts.0c00699
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib161">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. 2019
    </span>
    <span class="ltx_bibblock">
     Chi Chen, Weike Ye, Yunxing Zuo, Chen Zheng, and Shyue Ping Ong.
    </span>
    <span class="ltx_bibblock">
     Graph networks as a universal machine learning framework for molecules and crystals.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">
      Chemistry of Materials
     </em>
     , 31(9):3564–3572, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib162">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. 2020
    </span>
    <span class="ltx_bibblock">
     Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec.
    </span>
    <span class="ltx_bibblock">
     Open graph benchmark: Datasets for machine learning on graphs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">
      Advances in neural information processing systems
     </em>
     , 33:22118–22133, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib163">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kearnes et al. 2016
    </span>
    <span class="ltx_bibblock">
     Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley.
    </span>
    <span class="ltx_bibblock">
     Molecular graph convolutions: moving beyond fingerprints.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">
      Journal of computer-aided molecular design
     </em>
     , 30:595–608, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib164">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2022b
    </span>
    <span class="ltx_bibblock">
     Yeji Wang, Shuo Wu, Yanwen Duan, and Yong Huang.
    </span>
    <span class="ltx_bibblock">
     A point cloud-based deep learning strategy for protein–ligand binding affinity prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">
      Briefings in Bioinformatics
     </em>
     , 23(1):bbab474, 2022b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib165">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Thomas et al. 2018
    </span>
    <span class="ltx_bibblock">
     Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.
    </span>
    <span class="ltx_bibblock">
     Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">
      arXiv preprint arXiv:1802.08219
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib166">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2021
    </span>
    <span class="ltx_bibblock">
     Liguo Wang, Lu Zhao, Xian Liu, Jianjie Fu, and Aiqian Zhang.
    </span>
    <span class="ltx_bibblock">
     Seppcnet: deeping learning on a 3d surface electrostatic potential point cloud for enhanced toxicity classification and its application to suspected environmental estrogens.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">
      Environmental Science &amp; Technology
     </em>
     , 55(14):9958–9967, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib167">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahmadi et al. 2024
    </span>
    <span class="ltx_bibblock">
     Soroush Ahmadi, Mohammad Amin Ghanavati, and Sohrab Rohani.
    </span>
    <span class="ltx_bibblock">
     Machine learning-guided prediction of cocrystals using point cloud-based molecular representation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">
      Chemistry of Materials
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib168">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Singh and Sunoj 2023
    </span>
    <span class="ltx_bibblock">
     Sukriti Singh and Raghavan B. Sunoj.
    </span>
    <span class="ltx_bibblock">
     Molecular Machine Learning for Chemical Catalysis: Prospects and Challenges.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">
      Accounts of Chemical Research
     </em>
     , 56(3):402–412, February 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0001-4842.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.accounts.2c00801" target="_blank" title="">
      10.1021/acs.accounts.2c00801
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.accounts.2c00801" target="_blank" title="">
      https://doi.org/10.1021/acs.accounts.2c00801
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib169">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bran and Schwaller 2023
    </span>
    <span class="ltx_bibblock">
     Andres M Bran and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     Transformers and large language models for chemistry and drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">
      arXiv [cs.LG]
     </em>
     , October 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.06083" target="_blank" title="">
      http://arxiv.org/abs/2310.06083
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib170">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shilpa et al. 2023
    </span>
    <span class="ltx_bibblock">
     Shilpa Shilpa, Gargee Kashyap, and Raghavan B. Sunoj.
    </span>
    <span class="ltx_bibblock">
     Recent Applications of Machine Learning in Molecular Property and Chemical Reaction Outcome Predictions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">
      The Journal of Physical Chemistry A
     </em>
     , 127(40):8253–8271, October 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1089-5639.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jpca.3c04779" target="_blank" title="">
      10.1021/acs.jpca.3c04779
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jpca.3c04779" target="_blank" title="">
      https://doi.org/10.1021/acs.jpca.3c04779
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib171">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wigh et al. 2022
    </span>
    <span class="ltx_bibblock">
     Daniel S Wigh, Jonathan M Goodman, and Alexei A Lapkin.
    </span>
    <span class="ltx_bibblock">
     A review of molecular representation in the age of machine learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">
      Wiley Interdiscip. Rev. Comput. Mol. Sci.
     </em>
     , 12(5), September 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1759-0876,1759-0884.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/wcms.1603" target="_blank" title="">
      10.1002/wcms.1603
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.1603" target="_blank" title="">
      https://wires.onlinelibrary.wiley.com/doi/10.1002/wcms.1603
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib172">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     O’Boyle and Dalke 2018
    </span>
    <span class="ltx_bibblock">
     Noel O’Boyle and Andrew Dalke.
    </span>
    <span class="ltx_bibblock">
     DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of Chemical Structures, September 2018.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/60c73ed6567dfe7e5fec388d" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/60c73ed6567dfe7e5fec388d
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib173">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Krenn et al. 2020
    </span>
    <span class="ltx_bibblock">
     Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alán Aspuru-Guzik.
    </span>
    <span class="ltx_bibblock">
     Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">
      Machine Learning: Science and Technology
     </em>
     , 1(4):045024, December 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 2632-2153.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1088/2632-2153/aba947" target="_blank" title="">
      10.1088/2632-2153/aba947
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.13741" target="_blank" title="">
      http://arxiv.org/abs/1905.13741
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1905.13741 [physics, physics:quant-ph, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib174">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Heller et al. 2015
    </span>
    <span class="ltx_bibblock">
     Stephen R. Heller, Alan McNaught, Igor Pletnev, Stephen Stein, and Dmitrii Tchekhovskoi.
    </span>
    <span class="ltx_bibblock">
     InChI, the IUPAC International Chemical Identifier.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">
      Journal of Cheminformatics
     </em>
     , 7(1):23, May 2015.
    </span>
    <span class="ltx_bibblock">
     ISSN 1758-2946.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1186/s13321-015-0068-4" target="_blank" title="">
      10.1186/s13321-015-0068-4
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s13321-015-0068-4" target="_blank" title="">
      https://doi.org/10.1186/s13321-015-0068-4
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib175">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Das et al. 2024
    </span>
    <span class="ltx_bibblock">
     Manajit Das, Ankit Ghosh, and Raghavan B Sunoj.
    </span>
    <span class="ltx_bibblock">
     Advances in machine learning with chemical language models in molecular property and reaction outcome predictions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">
      J. Comput. Chem.
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 0192-8651, 1096-987X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/jcc.27315" target="_blank" title="">
      10.1002/jcc.27315
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1002/jcc.27315" target="_blank" title="">
      http://dx.doi.org/10.1002/jcc.27315
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib176">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Irwin et al. 2012
    </span>
    <span class="ltx_bibblock">
     John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman.
    </span>
    <span class="ltx_bibblock">
     Zinc: a free tool to discover chemistry for biology.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">
      Journal of chemical information and modeling
     </em>
     , 52(7):1757–1768, 2012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib177">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. 2016
    </span>
    <span class="ltx_bibblock">
     Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, et al.
    </span>
    <span class="ltx_bibblock">
     Pubchem substance and compound databases.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">
      Nucleic acids research
     </em>
     , 44(D1):D1202–D1213, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib178">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. 2023
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">
      arXiv preprint arXiv:2307.09288
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib179">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gaulton et al. 2012
    </span>
    <span class="ltx_bibblock">
     Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, and John P Overington.
    </span>
    <span class="ltx_bibblock">
     ChEMBL: a large-scale bioactivity database for drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">
      Nucleic Acids Res.
     </em>
     , 40(Database issue):D1100–7, January 2012.
    </span>
    <span class="ltx_bibblock">
     ISSN 0305-1048,1362-4962.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1093/nar/gkr777" target="_blank" title="">
      10.1093/nar/gkr777
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1093/nar/gkr777" target="_blank" title="">
      http://dx.doi.org/10.1093/nar/gkr777
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib180">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kinney et al. 2023
    </span>
    <span class="ltx_bibblock">
     Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen, and Daniel S Weld.
    </span>
    <span class="ltx_bibblock">
     The semantic scholar open data platform.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">
      arXiv [cs.DL]
     </em>
     , January 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2301.10140" target="_blank" title="">
      http://arxiv.org/abs/2301.10140
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib181">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Segler et al. 2018c
    </span>
    <span class="ltx_bibblock">
     Marwin HS Segler, Mike Preuss, and Mark P Waller.
    </span>
    <span class="ltx_bibblock">
     Planning chemical syntheses with deep neural networks and symbolic ai.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">
      Nature
     </em>
     , 555(7698):604–610, 2018c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib182">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     _ja 2023
    </span>
    <span class="ltx_bibblock">
     awesome-chemistry-datasets/code_of_conduct.md at main · kjappelbaum/awesome-chemistry-datasets, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kjappelbaum/awesome-chemistry-datasets/blob/main/code_of_conduct.md" target="_blank" title="">
      https://github.com/kjappelbaum/awesome-chemistry-datasets/blob/main/code_of_conduct.md
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib183">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mirza et al. 2024
    </span>
    <span class="ltx_bibblock">
     Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir Mohammad Elahi, Maximilian Greiner, Caroline T Holick, Tanya Gupta, Mehrdad Asgari, Christina Glaubitz, Lea C Klepsch, Yannik Koster, Jakob Meyer, Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole Roesner, Ulrich S Schubert, Leanne M Stafast, Dinga Wonanke, Michael Pieler, Philippe Schwaller, and Kevin Maik Jablonka.
    </span>
    <span class="ltx_bibblock">
     Are large language models superhuman chemists?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">
      arXiv [cs.LG]
     </em>
     , April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.01475" target="_blank" title="">
      http://arxiv.org/abs/2404.01475
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib184">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gloriam 2019
    </span>
    <span class="ltx_bibblock">
     David E Gloriam.
    </span>
    <span class="ltx_bibblock">
     Bigger is better in virtual drug screens.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">
      Nature
     </em>
     , 566(7743):193–194, February 2019.
    </span>
    <span class="ltx_bibblock">
     ISSN 0028-0836,1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/d41586-019-00145-6" target="_blank" title="">
      10.1038/d41586-019-00145-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/d41586-019-00145-6" target="_blank" title="">
      http://dx.doi.org/10.1038/d41586-019-00145-6
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib185">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar.
    </span>
    <span class="ltx_bibblock">
     Multi-modal molecule structure–text model for text-based retrieval and editing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">
      Nature Machine Intelligence
     </em>
     , 5(12):1447–1457, December 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00759-6" target="_blank" title="">
      10.1038/s42256-023-00759-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00759-6" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00759-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib186">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Walters 2023
    </span>
    <span class="ltx_bibblock">
     Pat Walters.
    </span>
    <span class="ltx_bibblock">
     We need better benchmarks for machine learning in drug discovery, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://practicalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html" target="_blank" title="">
      http://practicalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib187">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fang et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Cheng Fang, Ye Wang, Richard Grater, Sudarshan Kapadnis, Cheryl Black, Patrick Trapa, and Simone Sciabola.
    </span>
    <span class="ltx_bibblock">
     Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">
      J. Chem. Inf. Model.
     </em>
     , 63(11):3263–3274, June 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596,1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c00160" target="_blank" title="">
      10.1021/acs.jcim.3c00160
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/full/10.1021/acs.jcim.3c00160" target="_blank" title="">
      https://pubs.acs.org/doi/full/10.1021/acs.jcim.3c00160
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib188">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     188
    </span>
    <span class="ltx_bibblock">
     Therapeutics data commons.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://tdcommons.ai/" target="_blank" title="">
      https://tdcommons.ai/
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-6-13.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib189">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. 2021
    </span>
    <span class="ltx_bibblock">
     Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik.
    </span>
    <span class="ltx_bibblock">
     Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">
      arXiv [cs.LG]
     </em>
     , February 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2102.09548" target="_blank" title="">
      http://arxiv.org/abs/2102.09548
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib190">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Velez-Arce et al. 2024
    </span>
    <span class="ltx_bibblock">
     Alejandro Velez-Arce, Kexin Huang, Michelle M Li, Xiang Lin, Wenhao Gao, Tianfan Fu, Manolis Kellis, Bradley L Pentelute, and Marinka Zitnik.
    </span>
    <span class="ltx_bibblock">
     TDC-2: Multimodal foundation for therapeutic science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">
      bioRxiv
     </em>
     , page 2024.06.12.598655, June 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1101/2024.06.12.598655" target="_blank" title="">
      10.1101/2024.06.12.598655
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.biorxiv.org/content/10.1101/2024.06.12.598655v2.abstract" target="_blank" title="">
      https://www.biorxiv.org/content/10.1101/2024.06.12.598655v2.abstract
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib191">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rich and Birnbaum 2023
    </span>
    <span class="ltx_bibblock">
     Alex Rich and Ben Birnbaum.
    </span>
    <span class="ltx_bibblock">
     Building adme benchmark datasets that drive impact, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.inductive.bio/blog/building-better-benchmarks-for-adme-optimization" target="_blank" title="">
      https://www.inductive.bio/blog/building-better-benchmarks-for-adme-optimization
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib192">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Laurent et al. 2024
    </span>
    <span class="ltx_bibblock">
     Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, and Samuel G Rodriques.
    </span>
    <span class="ltx_bibblock">
     LAB-bench: Measuring capabilities of language models for biology research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">
      arXiv [cs.AI]
     </em>
     , July 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2407.10362" target="_blank" title="">
      http://arxiv.org/abs/2407.10362
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib193">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yoshitake et al. 2022
    </span>
    <span class="ltx_bibblock">
     Michiko Yoshitake, Fumitaka Sato, Hiroyuki Kawano, and Hiroshi Teraoka.
    </span>
    <span class="ltx_bibblock">
     MaterialBERT for natural language processing of materials science texts.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">
      Science and Technology of Advanced Materials: Methods
     </em>
     , 2(1):372–380, December 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2766-0400.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1080/27660400.2022.2124831" target="_blank" title="">
      10.1080/27660400.2022.2124831
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/27660400.2022.2124831" target="_blank" title="">
      https://doi.org/10.1080/27660400.2022.2124831
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib194">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. 2022a
    </span>
    <span class="ltx_bibblock">
     Jiahui Yu, Chengwei Zhang, Yingying Cheng, Yun-Fang Yang, Yuan-Bin She, Fengfan Liu, Weike Su, and An Su.
    </span>
    <span class="ltx_bibblock">
     SolvBERT for solvation free energy and solubility prediction: a demonstration of an NLP model for predicting the properties of molecular complexes.
    </span>
    <span class="ltx_bibblock">
     July 2022a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/62df4881a8e4dcc8f41cbadf" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/62df4881a8e4dcc8f41cbadf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib195">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Boobier et al. 2020
    </span>
    <span class="ltx_bibblock">
     Samuel Boobier, David RJ Hose, A John Blacker, and Bao N Nguyen.
    </span>
    <span class="ltx_bibblock">
     Machine learning with physicochemical relationships: solubility prediction in organic solvents and water.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">
      Nature communications
     </em>
     , 11(1):5753, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib196">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ross et al. 2022b
    </span>
    <span class="ltx_bibblock">
     Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das.
    </span>
    <span class="ltx_bibblock">
     Large-scale chemical language representations capture molecular structure and properties.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">
      Nat. Mach. Intell.
     </em>
     , 4(12):1256–1264, December 2022b.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-022-00580-7" target="_blank" title="">
      10.1038/s42256-022-00580-7
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s42256-022-00580-7" target="_blank" title="">
      http://dx.doi.org/10.1038/s42256-022-00580-7
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib197">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang and Cole 2022
    </span>
    <span class="ltx_bibblock">
     Shu Huang and Jacqueline M. Cole.
    </span>
    <span class="ltx_bibblock">
     BatteryBERT: A Pretrained Language Model for Battery Database Enhancement.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 62(24):6365–6377, December 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.2c00035" target="_blank" title="">
      10.1021/acs.jcim.2c00035
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.2c00035" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.2c00035
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib198">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Maziarka et al. 2020
    </span>
    <span class="ltx_bibblock">
     <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="bib.bib198.1.g1" src="/html/2407.01603/assets/figs/letter-l.png" width="14"/>
     ukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski.
    </span>
    <span class="ltx_bibblock">
     Molecule attention transformer.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib198.2.1">
      arXiv [cs.LG]
     </em>
     , February 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2002.08264" target="_blank" title="">
      http://arxiv.org/abs/2002.08264
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib199">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2019b
    </span>
    <span class="ltx_bibblock">
     Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang.
    </span>
    <span class="ltx_bibblock">
     SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">
      Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics
     </em>
     , BCB ’19, pages 429–436, Niagara Falls NY USA, September 2019b. Association for Computing Machinery.
    </span>
    <span class="ltx_bibblock">
     ISBN 9781450366663.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3307339.3342186" target="_blank" title="">
      10.1145/3307339.3342186
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3307339.3342186" target="_blank" title="">
      https://dl.acm.org/doi/10.1145/3307339.3342186
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib200">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sultan et al. 2024
    </span>
    <span class="ltx_bibblock">
     Afnan Sultan, Jochen Sieg, Miriam Mathea, and Andrea Volkamer.
    </span>
    <span class="ltx_bibblock">
     Transformers for molecular property prediction: Lessons learned from the past five years.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">
      arXiv [cs.LG]
     </em>
     , April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.03969" target="_blank" title="">
      http://arxiv.org/abs/2404.03969
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib201">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2023c
    </span>
    <span class="ltx_bibblock">
     Di Wu, Qi Chen, Xiaojie Chen, Feng Han, Zhong Chen, and Yi Wang.
    </span>
    <span class="ltx_bibblock">
     The blood–brain barrier: structure, regulation, and drug delivery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">
      Signal Transduction and Targeted Therapy
     </em>
     , 8(1):1–27, May 2023c.
    </span>
    <span class="ltx_bibblock">
     ISSN 2059-3635.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41392-023-01481-w" target="_blank" title="">
      10.1038/s41392-023-01481-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41392-023-01481-w" target="_blank" title="">
      https://www.nature.com/articles/s41392-023-01481-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib202">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bissantz et al. 2010
    </span>
    <span class="ltx_bibblock">
     Caterina Bissantz, Bernd Kuhn, and Martin Stahl.
    </span>
    <span class="ltx_bibblock">
     A medicinal chemist’s guide to molecular interactions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">
      Journal of medicinal chemistry
     </em>
     , 53(14):5061–5084, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib203">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Roughley and Jordan 2011
    </span>
    <span class="ltx_bibblock">
     Stephen D Roughley and Allan M Jordan.
    </span>
    <span class="ltx_bibblock">
     The medicinal chemist’s toolbox: an analysis of reactions used in the pursuit of drug candidates.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">
      Journal of medicinal chemistry
     </em>
     , 54(10):3451–3479, 2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib204">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Doytchinova 2022
    </span>
    <span class="ltx_bibblock">
     Irini Doytchinova.
    </span>
    <span class="ltx_bibblock">
     Drug Design—Past, Present, Future.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">
      Molecules
     </em>
     , 27(5):1496, February 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1420-3049.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/molecules27051496" target="_blank" title="">
      10.3390/molecules27051496
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8911833/" target="_blank" title="">
      https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8911833/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib205">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     205
    </span>
    <span class="ltx_bibblock">
     Introduction to Physical Polymer Science, 4th Edition | Wiley.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wiley.com/en-us/Introduction+to+Physical+Polymer+Science%2C+4th+Edition-p-9780471706069" target="_blank" title="">
      https://www.wiley.com/en-us/Introduction+to+Physical+Polymer+Science%2C+4th+Edition-p-9780471706069
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib206">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Newman and Cragg 2016
    </span>
    <span class="ltx_bibblock">
     David J. Newman and Gordon M. Cragg.
    </span>
    <span class="ltx_bibblock">
     Natural Products as Sources of New Drugs from 1981 to 2014.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">
      Journal of Natural Products
     </em>
     , 79(3):629–661, March 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 0163-3864.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jnatprod.5b01055" target="_blank" title="">
      10.1021/acs.jnatprod.5b01055
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jnatprod.5b01055" target="_blank" title="">
      https://doi.org/10.1021/acs.jnatprod.5b01055
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib207">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ferreira et al. 2023
    </span>
    <span class="ltx_bibblock">
     Paulo Michel Pinheiro Ferreira, Daniel Dias Rufino Arcanjo, and Ana Paula Peron.
    </span>
    <span class="ltx_bibblock">
     Drug development, Brazilian biodiversity and political choices: Where are we heading?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">
      Journal of Toxicology and Environmental Health, Part B
     </em>
     , 26(5):257–274, July 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1093-7404.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1080/10937404.2023.2193762" target="_blank" title="">
      10.1080/10937404.2023.2193762
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/10937404.2023.2193762" target="_blank" title="">
      https://doi.org/10.1080/10937404.2023.2193762
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Taylor &amp; Francis _eprint: https://doi.org/10.1080/10937404.2023.2193762.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib208">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kolb and Sharpless 2003
    </span>
    <span class="ltx_bibblock">
     Hartmuth C Kolb and K. Barry Sharpless.
    </span>
    <span class="ltx_bibblock">
     The growing impact of click chemistry on drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">
      Drug Discovery Today
     </em>
     , 8(24):1128–1137, December 2003.
    </span>
    <span class="ltx_bibblock">
     ISSN 1359-6446.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/S1359-6446(03)02933-7" target="_blank" title="">
      10.1016/S1359-6446(03)02933-7
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1359644603029337" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1359644603029337
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib209">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Castellino et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nathan J. Castellino, Andrew P. Montgomery, Jonathan J. Danon, and Michael Kassiou.
    </span>
    <span class="ltx_bibblock">
     Late-stage Functionalization for Improving Drug-like Molecular Properties.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">
      Chemical Reviews
     </em>
     , 123(13):8127–8153, July 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0009-2665.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.chemrev.2c00797" target="_blank" title="">
      10.1021/acs.chemrev.2c00797
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.chemrev.2c00797" target="_blank" title="">
      https://doi.org/10.1021/acs.chemrev.2c00797
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib210">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sharma et al. 2023
    </span>
    <span class="ltx_bibblock">
     Komal Sharma, Krishna K. Sharma, Anku Sharma, and Rahul Jain.
    </span>
    <span class="ltx_bibblock">
     Peptide-based drug discovery: Current status and recent advances.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">
      Drug Discovery Today
     </em>
     , 28(2):103464, February 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1359-6446.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.drudis.2022.103464" target="_blank" title="">
      10.1016/j.drudis.2022.103464
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1359644622004573" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1359644622004573
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib211">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reizman and Jensen 2016
    </span>
    <span class="ltx_bibblock">
     Brandon J. Reizman and Klavs F. Jensen.
    </span>
    <span class="ltx_bibblock">
     Feedback in Flow for Accelerated Reaction Development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">
      Accounts of Chemical Research
     </em>
     , 49(9):1786–1796, September 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 0001-4842.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.accounts.6b00261" target="_blank" title="">
      10.1021/acs.accounts.6b00261
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.accounts.6b00261" target="_blank" title="">
      https://doi.org/10.1021/acs.accounts.6b00261
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib212">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     DiMasi et al. 2016
    </span>
    <span class="ltx_bibblock">
     Joseph A. DiMasi, Henry G. Grabowski, and Ronald W. Hansen.
    </span>
    <span class="ltx_bibblock">
     Innovation in the pharmaceutical industry: New estimates of R&amp;D costs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">
      Journal of Health Economics
     </em>
     , 47:20–33, May 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 1879-1646.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.jhealeco.2016.01.012" target="_blank" title="">
      10.1016/j.jhealeco.2016.01.012
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib213">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewars 2024
    </span>
    <span class="ltx_bibblock">
     Errol G Lewars.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">
      Computational chemistry: Introduction to the theory and applications of molecular and quantum mechanics
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Springer International Publishing, Cham, Switzerland, 4 edition, April 2024.
    </span>
    <span class="ltx_bibblock">
     ISBN 9783031514425,9783031514432.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-031-51443-2" target="_blank" title="">
      10.1007/978-3-031-51443-2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/book/10.1007/978-3-031-51443-2" target="_blank" title="">
      https://link.springer.com/book/10.1007/978-3-031-51443-2
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib214">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brini et al. 2016
    </span>
    <span class="ltx_bibblock">
     Emiliano Brini, S. Shanaka Paranahewage, Christopher J. Fennell, and Ken A. Dill.
    </span>
    <span class="ltx_bibblock">
     Adapting the semi-explicit assembly solvation model for estimating water-cyclohexane partitioning with the SAMPL5 molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">
      Journal of Computer-Aided Molecular Design
     </em>
     , 30(11):1067–1077, November 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 1573-4951.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10822-016-9961-9" target="_blank" title="">
      10.1007/s10822-016-9961-9
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib215">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bidault and Chaudhuri 2023
    </span>
    <span class="ltx_bibblock">
     Xavier Bidault and Santanu Chaudhuri.
    </span>
    <span class="ltx_bibblock">
     How Accurate Can Crystal Structure Predictions Be for High-Energy Molecular Crystals?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">
      Molecules
     </em>
     , 28(11):4471, January 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1420-3049.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/molecules28114471" target="_blank" title="">
      10.3390/molecules28114471
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1420-3049/28/11/4471" target="_blank" title="">
      https://www.mdpi.com/1420-3049/28/11/4471
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Number: 11 Publisher: Multidisciplinary Digital Publishing Institute.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib216">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pyzer-Knapp et al. 2021
    </span>
    <span class="ltx_bibblock">
     Edward O. Pyzer-Knapp, Linjiang Chen, Graeme M. Day, and Andrew I. Cooper.
    </span>
    <span class="ltx_bibblock">
     Accelerating computational discovery of porous solids through improved navigation of energy-structure-function maps.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">
      Science Advances
     </em>
     , 7(33):eabi4763, August 2021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/sciadv.abi4763" target="_blank" title="">
      10.1126/sciadv.abi4763
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/10.1126/sciadv.abi4763" target="_blank" title="">
      https://www.science.org/doi/10.1126/sciadv.abi4763
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib217">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fredericks et al. 2021
    </span>
    <span class="ltx_bibblock">
     Scott Fredericks, Kevin Parrish, Dean Sayre, and Qiang Zhu.
    </span>
    <span class="ltx_bibblock">
     PyXtal: A Python library for crystal structure generation and symmetry analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">
      Computer Physics Communications
     </em>
     , 261:107810, April 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 0010-4655.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.cpc.2020.107810" target="_blank" title="">
      10.1016/j.cpc.2020.107810
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0010465520304057" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S0010465520304057
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib218">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Case et al. 2016
    </span>
    <span class="ltx_bibblock">
     David H. Case, Josh E. Campbell, Peter J. Bygrave, and Graeme M. Day.
    </span>
    <span class="ltx_bibblock">
     Convergence Properties of Crystal Structure Prediction by Quasi-Random Sampling.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">
      Journal of Chemical Theory and Computation
     </em>
     , 12(2):910–924, February 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9618.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jctc.5b01112" target="_blank" title="">
      10.1021/acs.jctc.5b01112
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jctc.5b01112" target="_blank" title="">
      https://doi.org/10.1021/acs.jctc.5b01112
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib219">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kazantsev et al. 2011
    </span>
    <span class="ltx_bibblock">
     A. V. Kazantsev, P. G. Karamertzanis, C. S. Adjiman, and C. C. Pantelides.
    </span>
    <span class="ltx_bibblock">
     Efficient Handling of Molecular Flexibility in Lattice Energy Minimization of Organic Crystals.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">
      Journal of Chemical Theory and Computation
     </em>
     , 7(6):1998–2016, June 2011.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9618.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/ct100597e" target="_blank" title="">
      10.1021/ct100597e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/ct100597e" target="_blank" title="">
      https://doi.org/10.1021/ct100597e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib220">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. 2023
    </span>
    <span class="ltx_bibblock">
     Guannan Huang, Yani Guo, Ye Chen, and Zhengwei Nie.
    </span>
    <span class="ltx_bibblock">
     Application of Machine Learning in Material Synthesis and Property Prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">
      Materials
     </em>
     , 16(17):5977, January 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1996-1944.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/ma16175977" target="_blank" title="">
      10.3390/ma16175977
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1996-1944/16/17/5977" target="_blank" title="">
      https://www.mdpi.com/1996-1944/16/17/5977
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Number: 17 Publisher: Multidisciplinary Digital Publishing Institute.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib221">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Martinez-Mayorga et al. 2024
    </span>
    <span class="ltx_bibblock">
     Karina Martinez-Mayorga, José G. Rosas-Jiménez, Karla Gonzalez-Ponce, Edgar López-López, Antonio Neme, and José L. Medina-Franco.
    </span>
    <span class="ltx_bibblock">
     The pursuit of accurate predictive models of the bioactivity of small molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">
      Chemical Science
     </em>
     , 15(6):1938–1952, February 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D3SC05534E" target="_blank" title="">
      10.1039/D3SC05534E
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2024/sc/d3sc05534e" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2024/sc/d3sc05534e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib222">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wellawatte et al. 2023
    </span>
    <span class="ltx_bibblock">
     Geemi P Wellawatte, Heta A Gandhi, Aditi Seshadri, and Andrew D White.
    </span>
    <span class="ltx_bibblock">
     A PERSPECTIVE ON EXPLANATIONS OF MOLECULAR PREDICTION MODELS.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">
      J. Chem. Theory Comput.
     </em>
     , 19(8):2149–2160, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9618, 1549-9626.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jctc.2c01235" target="_blank" title="">
      10.1021/acs.jctc.2c01235
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jctc.2c01235" target="_blank" title="">
      http://dx.doi.org/10.1021/acs.jctc.2c01235
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib223">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deng et al. 2023
    </span>
    <span class="ltx_bibblock">
     Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, and Michael Bendersky.
    </span>
    <span class="ltx_bibblock">
     LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">
      Companion Proceedings of the ACM Web Conference 2023
     </em>
     , WWW ’23 Companion, pages 1014–1019, New York, NY, USA, April 2023. Association for Computing Machinery.
    </span>
    <span class="ltx_bibblock">
     ISBN 978-1-4503-9419-2.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3543873.3587605" target="_blank" title="">
      10.1145/3543873.3587605
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3543873.3587605" target="_blank" title="">
      https://dl.acm.org/doi/10.1145/3543873.3587605
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib224">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2020a
    </span>
    <span class="ltx_bibblock">
     P Schwaller, Daniel Probst, A Vaucher, Vishnu H Nair, D Kreutter, T Laino, and J Reymond.
    </span>
    <span class="ltx_bibblock">
     Mapping the space of chemical reactions using attention-based neural networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib224.1.1">
      Nat. Mach. Intell.
     </em>
     , 3(2):144–152, August 2020a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-020-00284-w" target="_blank" title="">
      10.1038/s42256-020-00284-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-020-00284-w" target="_blank" title="">
      https://www.nature.com/articles/s42256-020-00284-w
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib225">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Toniato et al. 2023
    </span>
    <span class="ltx_bibblock">
     Alessandra Toniato, Alain C Vaucher, Philippe Schwaller, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Enhancing diversity in language based models for single-step retrosynthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">
      Digital Discovery
     </em>
     , 2(2):489–501, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D2DD00110A" target="_blank" title="">
      10.1039/D2DD00110A
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00110a" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00110a
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib226">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2021a
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Extraction of organic chemistry grammar from unsupervised learning of chemical reactions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">
      Science Advances
     </em>
     , 7(15):eabe4166, April 2021a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/sciadv.abe4166" target="_blank" title="">
      10.1126/sciadv.abe4166
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/10.1126/sciadv.abe4166" target="_blank" title="">
      https://www.science.org/doi/10.1126/sciadv.abe4166
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib227">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2021b
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Alain C Vaucher, Teodoro Laino, and Jean-Louis Reymond.
    </span>
    <span class="ltx_bibblock">
     Prediction of chemical reaction yields using deep learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">
      Mach. Learn. Sci. Technol.
     </em>
     , 2(1):015016, March 2021b.
    </span>
    <span class="ltx_bibblock">
     ISSN 2632-2153.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1088/2632-2153/abc81d" target="_blank" title="">
      10.1088/2632-2153/abc81d
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://iopscience.iop.org/article/10.1088/2632-2153/abc81d/meta" target="_blank" title="">
      https://iopscience.iop.org/article/10.1088/2632-2153/abc81d/meta
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib228">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2022
    </span>
    <span class="ltx_bibblock">
     Xiao-Chen Zhang, Cheng-Kun Wu, Jia-Cai Yi, Xiang-Xiang Zeng, Can-Qun Yang, Ai-Ping Lu, Hou T-j, Ting-Jun Hou, and Dong-Sheng Cao.
    </span>
    <span class="ltx_bibblock">
     Pushing the Boundaries of Molecular Property Prediction for Drug Discovery with Multitask Learning BERT Enhanced by SMILES Enumeration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">
      Research
     </em>
     , 2022:0004, December 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2096-5168.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.34133/research.0004" target="_blank" title="">
      10.34133/research.0004
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.34133/research.0004" target="_blank" title="">
      http://dx.doi.org/10.34133/research.0004
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib229">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. 2021
    </span>
    <span class="ltx_bibblock">
     Guoli Xiong, Zhenxing Wu, Jiacai Yi, Li Fu, Zhijiang Yang, Changyu Hsieh, Mingzhu Yin, Xiangxiang Zeng, Chengkun Wu, Aiping Lu, Xiang Chen, Tingjun Hou, and Dongsheng Cao.
    </span>
    <span class="ltx_bibblock">
     ADMETlab 2.0: an integrated online platform for accurate and comprehensive predictions of ADMET properties.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">
      Nucleic Acids Res.
     </em>
     , 49(W1):W5–W14, July 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 0305-1048, 1362-4962.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1093/nar/gkab255" target="_blank" title="">
      10.1093/nar/gkab255
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1093/nar/gkab255" target="_blank" title="">
      http://dx.doi.org/10.1093/nar/gkab255
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib230">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sterling and Irwin 2015
    </span>
    <span class="ltx_bibblock">
     Teague Sterling and John J. Irwin.
    </span>
    <span class="ltx_bibblock">
     ZINC 15 – Ligand Discovery for Everyone.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib230.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 55(11):2324–2337, November 2015.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.5b00559" target="_blank" title="">
      10.1021/acs.jcim.5b00559
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.5b00559" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.5b00559
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib231">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wolf et al. 2020
    </span>
    <span class="ltx_bibblock">
     Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.
    </span>
    <span class="ltx_bibblock">
     Huggingface’s transformers: State-of-the-art natural language processing, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib232">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. 2019b
    </span>
    <span class="ltx_bibblock">
     Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi Jaakkola, Klavs Jensen, and Regina Barzilay.
    </span>
    <span class="ltx_bibblock">
     Analyzing Learned Molecular Representations for Property Prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib232.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 59(8):3370–3388, August 2019b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.9b00237" target="_blank" title="">
      10.1021/acs.jcim.9b00237
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib233">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2019
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee.
    </span>
    <span class="ltx_bibblock">
     Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib233.1.1">
      ACS Central Science
     </em>
     , 5(9):1572–1583, September 2019.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943, 2374-7951.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.9b00576" target="_blank" title="">
      10.1021/acscentsci.9b00576
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acscentsci.9b00576" target="_blank" title="">
      https://doi.org/10.1021/acscentsci.9b00576
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib234">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vig 2019
    </span>
    <span class="ltx_bibblock">
     Jesse Vig.
    </span>
    <span class="ltx_bibblock">
     Bertviz: A tool for visualizing multihead self-attention in the bert model.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib234.1.1">
      ICLR workshop: Debugging machine learning models
     </em>
     , volume 3, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib235">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Skinnider 2024
    </span>
    <span class="ltx_bibblock">
     Michael A. Skinnider.
    </span>
    <span class="ltx_bibblock">
     Invalid SMILES are beneficial rather than detrimental to chemical language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib235.1.1">
      Nature Machine Intelligence
     </em>
     , 6(4):437–448, April 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-024-00821-x" target="_blank" title="">
      10.1038/s42256-024-00821-x
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-024-00821-x" target="_blank" title="">
      https://www.nature.com/articles/s42256-024-00821-x
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib236">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vermeire and Green 2021
    </span>
    <span class="ltx_bibblock">
     Florence H Vermeire and William H Green.
    </span>
    <span class="ltx_bibblock">
     Transfer learning for solvation free energies: From quantum chemistry to experiments.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">
      Chemical Engineering Journal
     </em>
     , 418:129307, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib237">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mobley and Guthrie 2014
    </span>
    <span class="ltx_bibblock">
     David L Mobley and J Peter Guthrie.
    </span>
    <span class="ltx_bibblock">
     FreeSolv: a database of experimental and calculated hydration free energies, with input files.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib237.1.1">
      J. Comput. Aided Mol. Des.
     </em>
     , 28(7):711–720, July 2014.
    </span>
    <span class="ltx_bibblock">
     ISSN 0920-654X,1573-4951.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10822-014-9747-x" target="_blank" title="">
      10.1007/s10822-014-9747-x
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.springer.com/article/10.1007/s10822-014-9747-x" target="_blank" title="">
      https://link.springer.com/article/10.1007/s10822-014-9747-x
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib238">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Marenich et al. 2020
    </span>
    <span class="ltx_bibblock">
     Aleksandr V Marenich, Casey P Kelly, Jason D Thompson, Gregory D Hawkins, Candee C Chambers, David J Giesen, Paul Winget, Christopher J Cramer, and Donald G Truhlar.
    </span>
    <span class="ltx_bibblock">
     Minnesota solvation database (mnsol) version 2012.
    </span>
    <span class="ltx_bibblock">
     2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib239">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Moine et al. 2017
    </span>
    <span class="ltx_bibblock">
     Edouard Moine, Romain Privat, Baptiste Sirjean, and Jean-Noël Jaubert.
    </span>
    <span class="ltx_bibblock">
     Estimation of solvation quantities from experimental thermodynamic data: Development of the comprehensive compsol databank for pure and mixed solutes.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">
      Journal of Physical and Chemical Reference Data
     </em>
     , 46(3), 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib240">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Grubbs et al. 2010
    </span>
    <span class="ltx_bibblock">
     Laura M Grubbs, Mariam Saifullah, E Nohelli, Shulin Ye, Sai S Achi, William E Acree Jr, and Michael H Abraham.
    </span>
    <span class="ltx_bibblock">
     Mathematical correlations for describing solute transfer into functionalized alkane solvents containing hydroxyl, ether, ester or ketone solvents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">
      Fluid phase equilibria
     </em>
     , 298(1):48–53, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib241">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. 2019c
    </span>
    <span class="ltx_bibblock">
     Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian P Kelley, Andrew Palmer, Volker Settels, et al.
    </span>
    <span class="ltx_bibblock">
     Are learned molecular representations ready for prime time?
    </span>
    <span class="ltx_bibblock">
     2019c.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv.7940594.v2" target="_blank" title="">
      10.26434/chemrxiv.7940594.v2
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib242">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rong et al. 2020
    </span>
    <span class="ltx_bibblock">
     Yu Rong, Yatao Bian, Tingyang Xu, Wei-Yang Xie, Ying Wei, Wen-Bing Huang, and Junzhou Huang.
    </span>
    <span class="ltx_bibblock">
     Self-Supervised Graph Transformer on Large-Scale Molecular Data.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">
      Adv. Neural Inf. Process. Syst.
     </em>
     , 33:12559–12571, June 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1049-5258.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/94aef38441efa3380a3bed3faf1f9d5d-Abstract.html" target="_blank" title="">
      https://proceedings.neurips.cc/paper_files/paper/2020/hash/94aef38441efa3380a3bed3faf1f9d5d-Abstract.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib243">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Winter et al. 2022
    </span>
    <span class="ltx_bibblock">
     Benedikt Winter, Clemens Winter, Johannes Schilling, and André Bardow.
    </span>
    <span class="ltx_bibblock">
     A smile is all you need: predicting limiting activity coefficients from SMILES with natural language processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">
      Digital Discovery
     </em>
     , 1(6):859–869, December 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D2DD00058J" target="_blank" title="">
      10.1039/D2DD00058J
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00058j" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2022/dd/d2dd00058j
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: RSC.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib244">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. 2024
    </span>
    <span class="ltx_bibblock">
     Jing Jiang, Yachao Li, Ruisheng Zhang, and Yunwu Liu.
    </span>
    <span class="ltx_bibblock">
     INTransformer: Data augmentation-based contrastive learning by injecting noise into transformer for molecular property prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">
      Journal of Molecular Graphics and Modelling
     </em>
     , 128:108703, May 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1093-3263.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.jmgm.2024.108703" target="_blank" title="">
      10.1016/j.jmgm.2024.108703
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1093326324000032" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1093326324000032
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib245">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chaves et al. 2024
    </span>
    <span class="ltx_bibblock">
     Juan Manuel Zambrano Chaves, Eric Wang, Tao Tu, Eeshit Dhaval Vaishnav, Byron Lee, S Sara Mahdavi, Christopher Semturs, David Fleet, Vivek Natarajan, and Shekoofeh Azizi.
    </span>
    <span class="ltx_bibblock">
     Tx-LLM: A large language model for therapeutics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">
      arXiv [cs.CL]
     </em>
     , June 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2406.06316" target="_blank" title="">
      http://arxiv.org/abs/2406.06316
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib246">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. 2024
    </span>
    <span class="ltx_bibblock">
     Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, and Huan Sun.
    </span>
    <span class="ltx_bibblock">
     LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset, April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.09391" target="_blank" title="">
      http://arxiv.org/abs/2402.09391
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2402.09391 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib247">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kyro et al. 2024
    </span>
    <span class="ltx_bibblock">
     Gregory W. Kyro, Anton Morgunov, Rafael I. Brent, and Victor S. Batista.
    </span>
    <span class="ltx_bibblock">
     ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 64(3):653–665, February 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c01456" target="_blank" title="">
      10.1021/acs.jcim.3c01456
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c01456" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c01456
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib248">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Frey et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nathan C. Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gómez-Bombarelli, Connor W. Coley, and Vijay Gadepally.
    </span>
    <span class="ltx_bibblock">
     Neural scaling of deep chemical models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">
      Nature Machine Intelligence
     </em>
     , 5(11):1297–1305, November 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00740-3" target="_blank" title="">
      10.1038/s42256-023-00740-3
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00740-3" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00740-3
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib249">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jablonka et al. 2024
    </span>
    <span class="ltx_bibblock">
     Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit.
    </span>
    <span class="ltx_bibblock">
     Leveraging large language models for predictive chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">
      Nat. Mach. Intell.
     </em>
     , 6(2):161–169, February 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00788-1" target="_blank" title="">
      10.1038/s42256-023-00788-1
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00788-1" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00788-1
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib250">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bagal et al. 2022
    </span>
    <span class="ltx_bibblock">
     Viraj Bagal, Rishal Aggarwal, P K Vinod, and U Deva Priyakumar.
    </span>
    <span class="ltx_bibblock">
     MolGPT: Molecular generation using a Transformer-Decoder model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib250.1.1">
      J. Chem. Inf. Model.
     </em>
     , 62(9):2064–2076, May 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.1c00600" target="_blank" title="">
      10.1021/acs.jcim.1c00600
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jcim.1c00600" target="_blank" title="">
      http://dx.doi.org/10.1021/acs.jcim.1c00600
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib251">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Adilov 2021
    </span>
    <span class="ltx_bibblock">
     Sanjar Adilov.
    </span>
    <span class="ltx_bibblock">
     Generative Pre-Training from molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib251.1.1">
      ChemRxiv
     </em>
     , September 2021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2021-5fwjd" target="_blank" title="">
      10.26434/chemrxiv-2021-5fwjd
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib252">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hocky 2024
    </span>
    <span class="ltx_bibblock">
     Glen M Hocky.
    </span>
    <span class="ltx_bibblock">
     Connecting molecular properties with plain language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib252.1.1">
      Nat. Mach. Intell.
     </em>
     , 6(3):249–250, March 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-024-00812-y" target="_blank" title="">
      10.1038/s42256-024-00812-y
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-024-00812-y" target="_blank" title="">
      https://www.nature.com/articles/s42256-024-00812-y
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib253">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li and Vederas 2009
    </span>
    <span class="ltx_bibblock">
     Jesse W.-H. Li and John C. Vederas.
    </span>
    <span class="ltx_bibblock">
     Drug Discovery and Natural Products: End of an Era or an Endless Frontier?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib253.1.1">
      Science
     </em>
     , 325(5937):161–165, July 2009.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/science.1168243" target="_blank" title="">
      10.1126/science.1168243
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/abs/10.1126/science.1168243" target="_blank" title="">
      https://www.science.org/doi/abs/10.1126/science.1168243
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib254">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Newman and Cragg 2012
    </span>
    <span class="ltx_bibblock">
     David J. Newman and Gordon M. Cragg.
    </span>
    <span class="ltx_bibblock">
     Natural Products As Sources of New Drugs over the 30 Years from 1981 to 2010.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib254.1.1">
      Journal of Natural Products
     </em>
     , 75(3):311–335, March 2012.
    </span>
    <span class="ltx_bibblock">
     ISSN 0163-3864.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/np200906s" target="_blank" title="">
      10.1021/np200906s
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/np200906s" target="_blank" title="">
      https://doi.org/10.1021/np200906s
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib255">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     A. Farha and D. Brown 2016
    </span>
    <span class="ltx_bibblock">
     Maya A. Farha and Eric D. Brown.
    </span>
    <span class="ltx_bibblock">
     Strategies for target identification of antimicrobial natural products.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib255.1.1">
      Natural Product Reports
     </em>
     , 33(5):668–680, 2016.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/C5NP00127G" target="_blank" title="">
      10.1039/C5NP00127G
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2016/np/c5np00127g" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2016/np/c5np00127g
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib256">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nigam et al. 2021
    </span>
    <span class="ltx_bibblock">
     AkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos Passos Gomes, and Alán Aspuru-Guzik.
    </span>
    <span class="ltx_bibblock">
     Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib256.1.1">
      Chemical Science
     </em>
     , 12(20):7079–7090, 2021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D1SC00231G" target="_blank" title="">
      10.1039/D1SC00231G
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc00231g" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc00231g
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib257">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gandhi and White 2022
    </span>
    <span class="ltx_bibblock">
     Heta A Gandhi and Andrew D White.
    </span>
    <span class="ltx_bibblock">
     Explaining molecular properties with natural language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib257.1.1">
      ChemRxiv
     </em>
     , October 2022.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2022-v5p6m-v3" target="_blank" title="">
      10.26434/chemrxiv-2022-v5p6m-v3
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/633731d1f764e6e535093041" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/633731d1f764e6e535093041
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib258">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yuanqi Du, Arian R Jamasb, Jeff Guo, Tianfan Fu, Charles Harris, Yingheng Wang, Chenru Duan, Pietro Liò, Philippe Schwaller, and Tom L Blundell.
    </span>
    <span class="ltx_bibblock">
     Machine learning-aided generative molecular design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib258.1.1">
      Nat. Mach. Intell.
     </em>
     , pages 1–16, June 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-024-00843-5" target="_blank" title="">
      10.1038/s42256-024-00843-5
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-024-00843-5" target="_blank" title="">
      https://www.nature.com/articles/s42256-024-00843-5
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib259">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Houlsby et al. 2019
    </span>
    <span class="ltx_bibblock">
     Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
    </span>
    <span class="ltx_bibblock">
     Parameter-efficient transfer learning for nlp.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib259.1.1">
      International conference on machine learning
     </em>
     , pages 2790–2799. PMLR, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib260">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fuhr and Sumpter 2022
    </span>
    <span class="ltx_bibblock">
     Addis S. Fuhr and Bobby G. Sumpter.
    </span>
    <span class="ltx_bibblock">
     Deep generative models for materials discovery and machine learning-accelerated innovation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib260.1.1">
      FRONTIERS IN MATERIALS
     </em>
     , 9, MAR 22 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2296-8016.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fmats.2022.865270" target="_blank" title="">
      10.3389/fmats.2022.865270
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib261">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Ri Han, Hongryul Yoon, Gahee Kim, Hyundo Lee, and Yoonji Lee.
    </span>
    <span class="ltx_bibblock">
     Revolutionizing medicinal chemistry: The application of artificial intelligence (ai) in early drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib261.1.1">
      PHARMACEUTICALS
     </em>
     , 16(9), SEP 2023a.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/ph16091259" target="_blank" title="">
      10.3390/ph16091259
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib262">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koutroumpa et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nikoletta-Maria Koutroumpa, Konstantinos D. Papavasileiou, Anastasios G. Papadiamantis, Georgia Melagraki, and Antreas Afantitis.
    </span>
    <span class="ltx_bibblock">
     A systematic review of deep learning methodologies used in the drug discovery process with emphasis on in vivo validation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib262.1.1">
      INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES
     </em>
     , 24(7), APR 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1661-6596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/ijms24076573" target="_blank" title="">
      10.3390/ijms24076573
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib263">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kell et al. 2020
    </span>
    <span class="ltx_bibblock">
     Douglas B. Kell, Soumitra Samanta, and Neil Swainston.
    </span>
    <span class="ltx_bibblock">
     Deep learning and generative methods in cheminformatics and chemical biology: navigating small molecule space intelligently.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib263.1.1">
      BIOCHEMICAL JOURNAL
     </em>
     , 477(23):4559–4580, DEC 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 0264-6021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1042/BCJ20200781" target="_blank" title="">
      10.1042/BCJ20200781
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib264">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bilodeau et al. 2022
    </span>
    <span class="ltx_bibblock">
     Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F. Jensen.
    </span>
    <span class="ltx_bibblock">
     Generative models for molecular discovery: Recent advances and challenges.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib264.1.1">
      WILEY INTERDISCIPLINARY REVIEWS-COMPUTATIONAL MOLECULAR SCIENCE
     </em>
     , 12(5), SEP 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1759-0876.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/wcms.1608" target="_blank" title="">
      10.1002/wcms.1608
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib265">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gangwal et al. 2024
    </span>
    <span class="ltx_bibblock">
     Amit Gangwal, Azim Ansari, Iqrar Ahmad, Abul Kalam Azad, Vinoth Kumarasamy, Vetriselvan Subramaniyan, and Ling Shing Wong.
    </span>
    <span class="ltx_bibblock">
     Generative artificial intelligence in drug discovery: basic framework, recent advances, challenges, and opportunities.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib265.1.1">
      FRONTIERS IN PHARMACOLOGY
     </em>
     , 15, FEB 7 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fphar.2024.1331062" target="_blank" title="">
      10.3389/fphar.2024.1331062
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib266">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vogt 2022
    </span>
    <span class="ltx_bibblock">
     Martin Vogt.
    </span>
    <span class="ltx_bibblock">
     Using deep neural networks to explore chemical space.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib266.1.1">
      EXPERT OPINION ON DRUG DISCOVERY
     </em>
     , 17(3):297–304, MAR 4 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 1746-0441.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1080/17460441.2022.2019704" target="_blank" title="">
      10.1080/17460441.2022.2019704
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib267">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Talluri et al. 2024
    </span>
    <span class="ltx_bibblock">
     Sekhar Talluri, Mohammad Amjad Kamal, and Rama Rao Malla.
    </span>
    <span class="ltx_bibblock">
     Novel computational methods for cancer drug design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib267.1.1">
      CURRENT MEDICINAL CHEMISTRY
     </em>
     , 31(5):554–572, 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 0929-8673.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.2174/0929867330666230403100008" target="_blank" title="">
      10.2174/0929867330666230403100008
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib268">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Polykovskiy et al. 2020
    </span>
    <span class="ltx_bibblock">
     Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alán Aspuru-Guzik, and Alex Zhavoronkov.
    </span>
    <span class="ltx_bibblock">
     Molecular sets (MOSES): A benchmarking platform for molecular generation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib268.1.1">
      Front. Pharmacol.
     </em>
     , 11:565644, December 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1663-9812.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fphar.2020.565644" target="_blank" title="">
      10.3389/fphar.2020.565644
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2020.565644/full" target="_blank" title="">
      https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2020.565644/full
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib269">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. 2019
    </span>
    <span class="ltx_bibblock">
     Nathan Brown, Marco Fiscato, Marwin H S Segler, and Alain C Vaucher.
    </span>
    <span class="ltx_bibblock">
     GuacaMol: Benchmarking models for de novo molecular design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib269.1.1">
      J. Chem. Inf. Model.
     </em>
     , 59(3):1096–1108, March 2019.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.8b00839" target="_blank" title="">
      10.1021/acs.jcim.8b00839
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jcim.8b00839" target="_blank" title="">
      http://dx.doi.org/10.1021/acs.jcim.8b00839
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib270">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Preuer et al. 2018
    </span>
    <span class="ltx_bibblock">
     Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer.
    </span>
    <span class="ltx_bibblock">
     Fréchet chemnet distance: a metric for generative models for molecules in drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib270.1.1">
      Journal of chemical information and modeling
     </em>
     , 58(9):1736–1741, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib271">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Haroon et al. 2023
    </span>
    <span class="ltx_bibblock">
     Suhail Haroon, Hafsath C.a., and Jereesh A.s.
    </span>
    <span class="ltx_bibblock">
     Generative Pre-trained Transformer (GPT) based model with relative attention for de novo drug design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib271.1.1">
      Computational Biology and Chemistry
     </em>
     , 106:107911, October 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1476-9271.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compbiolchem.2023.107911" target="_blank" title="">
      10.1016/j.compbiolchem.2023.107911
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1476927123001020" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1476927123001020
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib272">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Jianmin Wang, Jiashun Mao, Meng Wang, Xiangyang Le, and Yunyun Wang.
    </span>
    <span class="ltx_bibblock">
     Explore drug-like space with deep generative models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib272.1.1">
      Methods
     </em>
     , 210:52–59, February 2023b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1046-2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.ymeth.2023.01.004" target="_blank" title="">
      10.1016/j.ymeth.2023.01.004
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1046202323000129" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1046202323000129
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib273">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mao et al. 2024
    </span>
    <span class="ltx_bibblock">
     Jiashun Mao, Jianmin Wang, Amir Zeb, Kwang-Hwi Cho, Haiyan Jin, Jongwan Kim, Onju Lee, Yunyun Wang, and Kyoung Tai No.
    </span>
    <span class="ltx_bibblock">
     Transformer-Based Molecular Generative Model for Antiviral Drug Design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib273.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 64(7):2733–2745, April 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c00536" target="_blank" title="">
      10.1021/acs.jcim.3c00536
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c00536" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c00536
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib274">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Raffel et al. 2020
    </span>
    <span class="ltx_bibblock">
     Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
    </span>
    <span class="ltx_bibblock">
     Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib274.1.1">
      Journal of Machine Learning Research
     </em>
     , 21(140):1–67, 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1533-7928.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" target="_blank" title="">
      http://jmlr.org/papers/v21/20-074.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib275">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mao et al. 2023
    </span>
    <span class="ltx_bibblock">
     Jiashun Mao, Jianmin Wang, Kwang-Hwi Cho, and Kyoung Tai No.
    </span>
    <span class="ltx_bibblock">
     iupacGPT: IUPAC-based large-scale molecular pre-trained model for property prediction and molecule generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib275.1.1">
      ChemRxiv
     </em>
     , pages 1–13, May 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2023-5kjvh" target="_blank" title="">
      10.26434/chemrxiv-2023-5kjvh
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/public-dashboard" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/public-dashboard
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib276">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Wenyi Zhang, Kaiyue Zhang, and Jing Huang.
    </span>
    <span class="ltx_bibblock">
     A Simple Way to Incorporate Target Structural Information in Molecular Generative Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib276.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 63(12):3719–3730, June 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c00293" target="_blank" title="">
      10.1021/acs.jcim.3c00293
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c00293" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c00293
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib277">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023c
    </span>
    <span class="ltx_bibblock">
     Xun Wang, Changnan Gao, Peifu Han, Xue Li, Wenqi Chen, Alfonso Rodríguez Patón, Shuang Wang, and Pan Zheng.
    </span>
    <span class="ltx_bibblock">
     PETrans: De Novo Drug Design with Protein-Specific Encoding Based on Transfer Learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib277.1.1">
      International Journal of Molecular Sciences
     </em>
     , 24(2):1146, January 2023c.
    </span>
    <span class="ltx_bibblock">
     ISSN 1422-0067.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/ijms24021146" target="_blank" title="">
      10.3390/ijms24021146
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1422-0067/24/2/1146" target="_blank" title="">
      https://www.mdpi.com/1422-0067/24/2/1146
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Number: 2 Publisher: Multidisciplinary Digital Publishing Institute.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib278">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yoshikai et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, and Hiroyuki Kusuhara.
    </span>
    <span class="ltx_bibblock">
     A novel molecule generative model of VAE combined with Transformer for unseen structure generation, April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.11950" target="_blank" title="">
      http://arxiv.org/abs/2402.11950
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2402.11950 [physics, q-bio].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib279">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Ying Qian, Minghua Shi, and Qian Zhang.
    </span>
    <span class="ltx_bibblock">
     CONSMI: Contrastive Learning in the Simplified Molecular Input Line Entry System Helps Generate Better Molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib279.1.1">
      Molecules
     </em>
     , 29(2):495, January 2024a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1420-3049.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/molecules29020495" target="_blank" title="">
      10.3390/molecules29020495
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1420-3049/29/2/495" target="_blank" title="">
      https://www.mdpi.com/1420-3049/29/2/495
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Number: 2 Publisher: Multidisciplinary Digital Publishing Institute.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib280">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yan et al. 2024
    </span>
    <span class="ltx_bibblock">
     Xiaoying Yan, Chi Gu, Yuehua Feng, and Jiaxin Han.
    </span>
    <span class="ltx_bibblock">
     Predicting Drug-drug Interaction with Graph Mutual Interaction Attention Mechanism.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib280.1.1">
      Methods
     </em>
     , 223:16–25, March 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1046-2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.ymeth.2024.01.009" target="_blank" title="">
      10.1016/j.ymeth.2024.01.009
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1046202324000276" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1046202324000276
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib281">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Tao Shen, Jiale Guo, Zunsheng Han, Gao Zhang, Qingxin Liu, Xinxin Si, Dongmei Wang, Song Wu, and Jie Xia.
    </span>
    <span class="ltx_bibblock">
     AutoMolDesigner for Antibiotic Discovery: An AI-Based Open-Source Software for Automated Design of Small-Molecule Antibiotics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib281.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 64(3):575–583, February 2024b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c01562" target="_blank" title="">
      10.1021/acs.jcim.3c01562
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c01562" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c01562
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib282">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bickerton et al. 2012
    </span>
    <span class="ltx_bibblock">
     G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins.
    </span>
    <span class="ltx_bibblock">
     Quantifying the chemical beauty of drugs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib282.1.1">
      Nature chemistry
     </em>
     , 4(2):90–98, 2012.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib283">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Subramanian et al. 2016
    </span>
    <span class="ltx_bibblock">
     Govindan Subramanian, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny.
    </span>
    <span class="ltx_bibblock">
     Computational modeling of
     <math alttext="\beta" class="ltx_Math" display="inline" id="bib.bib283.1.m1.1">
      <semantics id="bib.bib283.1.m1.1a">
       <mi id="bib.bib283.1.m1.1.1" xref="bib.bib283.1.m1.1.1.cmml">
        β
       </mi>
       <annotation-xml encoding="MathML-Content" id="bib.bib283.1.m1.1b">
        <ci id="bib.bib283.1.m1.1.1.cmml" xref="bib.bib283.1.m1.1.1">
         𝛽
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="bib.bib283.1.m1.1c">
        \beta
       </annotation>
      </semantics>
     </math>
     -secretase 1 (bace-1) inhibitors using ligand based approaches.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib283.2.1">
      Journal of chemical information and modeling
     </em>
     , 56(10):1936–1949, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib284">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. 2008
    </span>
    <span class="ltx_bibblock">
     Guozhang Xu, Marta C Abad, Peter J Connolly, Michael P Neeper, Geoffrey T Struble, Barry A Springer, Stuart L Emanuel, Niranjan Pandey, Robert H Gruninger, Mary Adams, et al.
    </span>
    <span class="ltx_bibblock">
     4-amino-6-arylamino-pyrimidine-5-carbaldehyde hydrazones as potent erbb-2/egfr dual kinase inhibitors.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib284.1.1">
      Bioorganic &amp; medicinal chemistry letters
     </em>
     , 18(16):4615–4619, 2008.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib285">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. 2022b
    </span>
    <span class="ltx_bibblock">
     Leiye Yu, Licong He, Bing Gan, Rujuan Ti, Qingjie Xiao, Xin Yang, Hongli Hu, Lizhe Zhu, Sheng Wang, and Ruobing Ren.
    </span>
    <span class="ltx_bibblock">
     Structural insights into sphingosine-1-phosphate receptor activation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib285.1.1">
      Proceedings of the National Academy of Sciences
     </em>
     , 119(16):e2117716119, 2022b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib286">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. 2021
    </span>
    <span class="ltx_bibblock">
     Peiyu Xu, Sijie Huang, Huibing Zhang, Chunyou Mao, X Edward Zhou, Xi Cheng, Icaro A Simon, Dan-Dan Shen, Hsin-Yung Yen, Carol V Robinson, et al.
    </span>
    <span class="ltx_bibblock">
     Structural insights into the lipid and ligand regulation of serotonin receptors.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib286.1.1">
      Nature
     </em>
     , 592(7854):469–473, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib287">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. 2017
    </span>
    <span class="ltx_bibblock">
     Jiangming Sun, Nina Jeliazkova, Vladimir Chupakhin, Jose-Felipe Golib-Dzib, Ola Engkvist, Lars Carlsson, Jörg Wegner, Hugo Ceulemans, Ivan Georgiev, Vedrin Jeliazkov, et al.
    </span>
    <span class="ltx_bibblock">
     Excape-db: an integrated large scale dataset facilitating big data analysis in chemogenomics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib287.1.1">
      Journal of cheminformatics
     </em>
     , 9:1–9, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib288">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han et al. 2024
    </span>
    <span class="ltx_bibblock">
     Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang.
    </span>
    <span class="ltx_bibblock">
     Parameter-efficient fine-tuning for large models: A comprehensive survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib288.1.1">
      arXiv [cs.LG]
     </em>
     , March 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.14608" target="_blank" title="">
      http://arxiv.org/abs/2403.14608
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib289">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ding et al. 2023
    </span>
    <span class="ltx_bibblock">
     Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Parameter-efficient fine-tuning of large-scale pre-trained language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib289.1.1">
      Nat. Mach. Intell.
     </em>
     , 5(3):220–235, March 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839,2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00626-4" target="_blank" title="">
      10.1038/s42256-023-00626-4
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00626-4" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00626-4
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib290">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. 2021
    </span>
    <span class="ltx_bibblock">
     Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     LoRA: Low-rank adaptation of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib290.1.1">
      arXiv [cs.CL]
     </em>
     , June 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.09685" target="_blank" title="">
      http://arxiv.org/abs/2106.09685
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib291">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guzman-Pando et al. 2023
    </span>
    <span class="ltx_bibblock">
     Abimael Guzman-Pando, Graciela Ramirez-Alonso, Carlos Arzate-Quintana, and Javier Camarillo-Cisneros.
    </span>
    <span class="ltx_bibblock">
     Deep learning algorithms applied to computational chemistry.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib291.1.1">
      Molecular Diversity
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1573-501X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s11030-023-10771-y" target="_blank" title="">
      10.1007/s11030-023-10771-y
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11030-023-10771-y" target="_blank" title="">
      https://doi.org/10.1007/s11030-023-10771-y
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib292">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fromer and Coley 2023
    </span>
    <span class="ltx_bibblock">
     Jenna C. Fromer and Connor W. Coley.
    </span>
    <span class="ltx_bibblock">
     Computer-aided multi-objective optimization in small molecule discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib292.1.1">
      Patterns
     </em>
     , 4(2):100678, February 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2666-3899.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.patter.2023.100678" target="_blank" title="">
      10.1016/j.patter.2023.100678
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2666389923000016" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S2666389923000016
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib293">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vogt 2023
    </span>
    <span class="ltx_bibblock">
     Martin Vogt.
    </span>
    <span class="ltx_bibblock">
     Exploring chemical space — Generative models and their evaluation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib293.1.1">
      Artificial Intelligence in the Life Sciences
     </em>
     , 3:100064, December 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2667-3185.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.ailsci.2023.100064" target="_blank" title="">
      10.1016/j.ailsci.2023.100064
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2667318523000089" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S2667318523000089
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib294">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goel et al. 2023
    </span>
    <span class="ltx_bibblock">
     Manan Goel, Rishal Aggarwal, Bhuvanesh Sridharan, Pradeep Kumar Pal, and U. Deva Priyakumar.
    </span>
    <span class="ltx_bibblock">
     Efficient and enhanced sampling of drug-like chemical space for virtual screening and molecular design using modern machine learning methods.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib294.1.1">
      WIREs Computational Molecular Science
     </em>
     , 13(2):e1637, 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1759-0884.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/wcms.1637" target="_blank" title="">
      10.1002/wcms.1637
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1637" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1637
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcms.1637.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib295">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fang et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen.
    </span>
    <span class="ltx_bibblock">
     Domain-agnostic molecular generation with chemical feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib295.1.1">
      arXiv [cs.LG]
     </em>
     , January 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2301.11259" target="_blank" title="">
      http://arxiv.org/abs/2301.11259
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib296">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaucher et al. 2020
    </span>
    <span class="ltx_bibblock">
     Alain C Vaucher, Federico Zipoli, Joppe Geluykens, Vishnu H Nair, Philippe Schwaller, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Automated extraction of chemical synthesis actions from experimental procedures.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib296.1.1">
      Nat. Commun.
     </em>
     , 11(1):3601, July 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-1723.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41467-020-17266-6" target="_blank" title="">
      10.1038/s41467-020-17266-6
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41467-020-17266-6" target="_blank" title="">
      http://dx.doi.org/10.1038/s41467-020-17266-6
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib297">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Edwards et al. 2021
    </span>
    <span class="ltx_bibblock">
     Carl Edwards, ChengXiang Zhai, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     Text2Mol: Cross-modal molecule retrieval with natural language queries.
    </span>
    <span class="ltx_bibblock">
     In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors,
     <em class="ltx_emph ltx_font_italic" id="bib.bib297.1.1">
      Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 595–607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.47" target="_blank" title="">
      10.18653/v1/2021.emnlp-main.47
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.47" target="_blank" title="">
      https://aclanthology.org/2021.emnlp-main.47
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib298">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shenvi 2024
    </span>
    <span class="ltx_bibblock">
     Ryan A. Shenvi.
    </span>
    <span class="ltx_bibblock">
     Natural product synthesis in the 21st century: Beyond the mountain top.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib298.1.1">
      ACS Central Science
     </em>
     , 10(3):519–528, 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.3c01518" target="_blank" title="">
      10.1021/acscentsci.3c01518
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acscentsci.3c01518" target="_blank" title="">
      https://doi.org/10.1021/acscentsci.3c01518
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib299">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ai et al. 2024
    </span>
    <span class="ltx_bibblock">
     Qianxiang Ai, Fanwang Meng, Jiale Shi, Brenden Pelkie, and Connor W. Coley.
    </span>
    <span class="ltx_bibblock">
     Extracting Structured Data from Organic Synthesis Procedures Using a Fine-Tuned Large Language Model, April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/661064e921291e5d1d2bc860" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/661064e921291e5d1d2bc860
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib300">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Corey 1988
    </span>
    <span class="ltx_bibblock">
     E. J. Corey.
    </span>
    <span class="ltx_bibblock">
     Robert Robinson Lecture. Retrosynthetic thinking—essentials and examples.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib300.1.1">
      Chemical Society Reviews
     </em>
     , 17(0):111–133, January 1988.
    </span>
    <span class="ltx_bibblock">
     ISSN 1460-4744.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/CS9881700111" target="_blank" title="">
      10.1039/CS9881700111
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/1988/cs/cs9881700111" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/1988/cs/cs9881700111
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib301">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nerenberg et al. 1993
    </span>
    <span class="ltx_bibblock">
     Jennie B. Nerenberg, Deborah T. Hung, Patricia K. Somers, and Stuart L. Schreiber.
    </span>
    <span class="ltx_bibblock">
     Total synthesis of the immunosuppressive agent (-)-discodermolide.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib301.1.1">
      Journal of the American Chemical Society
     </em>
     , 115(26):12621–12622, 1993.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/ja00079a066" target="_blank" title="">
      10.1021/ja00079a066
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/ja00079a066" target="_blank" title="">
      https://doi.org/10.1021/ja00079a066
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://doi.org/10.1021/ja00079a066.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib302">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nam and Kim 2016
    </span>
    <span class="ltx_bibblock">
     Juno Nam and Jurae Kim.
    </span>
    <span class="ltx_bibblock">
     Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions, December 2016.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1612.09529" target="_blank" title="">
      http://arxiv.org/abs/1612.09529
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1612.09529 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib303">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2017
    </span>
    <span class="ltx_bibblock">
     Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande.
    </span>
    <span class="ltx_bibblock">
     Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib303.1.1">
      ACS Central Science
     </em>
     , 3(10):1103–1113, October 2017.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.7b00303" target="_blank" title="">
      10.1021/acscentsci.7b00303
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acscentsci.7b00303" target="_blank" title="">
      https://doi.org/10.1021/acscentsci.7b00303
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib304">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schneider et al. 2016
    </span>
    <span class="ltx_bibblock">
     Nadine Schneider, Nikolaus Stiefl, and Gregory A. Landrum.
    </span>
    <span class="ltx_bibblock">
     What’s what: The (nearly) definitive guide to reaction role assignment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib304.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 56(12):2336–2346, 2016.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.6b00564" target="_blank" title="">
      10.1021/acs.jcim.6b00564
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.6b00564" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.6b00564
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     PMID: 28024398.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib305">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     August  2016
    </span>
    <span class="ltx_bibblock">
     Matthew Gunther10 August 2016.
    </span>
    <span class="ltx_bibblock">
     Software could revolutionise chemistry.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.chemistryworld.com/news/software-could-revolutionise-chemistry/1017236.article" target="_blank" title="">
      https://www.chemistryworld.com/news/software-could-revolutionise-chemistry/1017236.article
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib306">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Klucznik et al. 2018
    </span>
    <span class="ltx_bibblock">
     Tomasz Klucznik, Barbara Mikulak-Klucznik, Michael P. McCormack, Heather Lima, Sara Szymkuć, Manishabrata Bhowmick, Karol Molga, Yubai Zhou, Lindsey Rickershauser, Ewa P. Gajewska, Alexei Toutchkine, Piotr Dittwald, Michał P. Startek, Gregory J. Kirkovits, Rafał Roszak, Ariel Adamski, Bianka Sieredzińska, Milan Mrksich, Sarah L. J. Trice, and Bartosz A. Grzybowski.
    </span>
    <span class="ltx_bibblock">
     Efficient Syntheses of Diverse, Medicinally Relevant Targets Planned by Computer and Executed in the Laboratory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib306.1.1">
      Chem
     </em>
     , 4(3):522–532, March 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 2451-9294, 2451-9308.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.chempr.2018.02.002" target="_blank" title="">
      10.1016/j.chempr.2018.02.002
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cell.com/chem/abstract/S2451-9294(18)30063-9" target="_blank" title="">
      https://www.cell.com/chem/abstract/S2451-9294(18)30063-9
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Elsevier.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib307">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Segler and Waller 2017
    </span>
    <span class="ltx_bibblock">
     Marwin H S Segler and Mark P Waller.
    </span>
    <span class="ltx_bibblock">
     Neural-symbolic machine learning for retrosynthesis and reaction prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib307.1.1">
      Chemistry
     </em>
     , 23(25):5966–5971, May 2017.
    </span>
    <span class="ltx_bibblock">
     ISSN 0947-6539, 1521-3765.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/chem.201605499" target="_blank" title="">
      10.1002/chem.201605499
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/chem.201605499" target="_blank" title="">
      https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/chem.201605499
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib308">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Somnath et al. 2021
    </span>
    <span class="ltx_bibblock">
     Vignesh Ram Somnath, Charlotte Bunne, Connor W. Coley, Andreas Krause, and Regina Barzilay.
    </span>
    <span class="ltx_bibblock">
     Learning Graph Models for Retrosynthesis Prediction, June 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2006.07038" target="_blank" title="">
      http://arxiv.org/abs/2006.07038
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2006.07038 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib309">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wellawatte and Schwaller 2023
    </span>
    <span class="ltx_bibblock">
     Geemi P Wellawatte and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     Extracting human interpretable structure-property relationships in chemistry using XAI and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib309.1.1">
      arXiv [physics.chem-ph]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.04047" target="_blank" title="">
      http://arxiv.org/abs/2311.04047
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib310">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cadeddu et al. 2014
    </span>
    <span class="ltx_bibblock">
     Andrea Cadeddu, Elizabeth K. Wylie, Janusz Jurczak, Matthew Wampler-Doty, and Bartosz A. Grzybowski.
    </span>
    <span class="ltx_bibblock">
     Organic Chemistry as a Language and the Implications of Chemical Linguistics for Structural and Retrosynthetic Analyses.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib310.1.1">
      Angewandte Chemie International Edition
     </em>
     , 53(31):8108–8112, 2014.
    </span>
    <span class="ltx_bibblock">
     ISSN 1521-3773.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/anie.201403708" target="_blank" title="">
      10.1002/anie.201403708
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201403708" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201403708
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201403708.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib311">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2018
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Théophile Gaudin, Dávid Lányi, Costas Bekas, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     “Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib311.1.1">
      Chemical Science
     </em>
     , 9(28):6091–6098, July 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/C8SC02339E" target="_blank" title="">
      10.1039/C8SC02339E
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib312">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jin et al. 2017
    </span>
    <span class="ltx_bibblock">
     Wengong Jin, Connor W. Coley, Regina Barzilay, and Tommi Jaakkola.
    </span>
    <span class="ltx_bibblock">
     Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network, December 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1709.04555" target="_blank" title="">
      http://arxiv.org/abs/1709.04555
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1709.04555 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib313">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bradshaw et al. 2019
    </span>
    <span class="ltx_bibblock">
     John Bradshaw, Matt J. Kusner, Brooks Paige, Marwin H. S. Segler, and José Miguel Hernández-Lobato.
    </span>
    <span class="ltx_bibblock">
     A Generative Model For Electron Paths, March 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1805.10970" target="_blank" title="">
      http://arxiv.org/abs/1805.10970
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1805.10970 [physics, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib314">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2020b
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H. Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib314.1.1">
      Chemical Science
     </em>
     , 11(12):3316–3325, March 2020b.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/C9SC05704H" target="_blank" title="">
      10.1039/C9SC05704H
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2020/sc/c9sc05704h" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2020/sc/c9sc05704h
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib315">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen and Jung 2021
    </span>
    <span class="ltx_bibblock">
     Shuan Chen and Yousung Jung.
    </span>
    <span class="ltx_bibblock">
     Deep Retrosynthetic Reaction Prediction using Local Reactivity and Global Attention.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib315.1.1">
      JACS Au
     </em>
     , 1(10):1612–1620, August 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 2691-3704.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jacsau.1c00246" target="_blank" title="">
      10.1021/jacsau.1c00246
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/jacsau.1c00246" target="_blank" title="">
      https://doi.org/10.1021/jacsau.1c00246
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib316">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Westerlund et al. 2024
    </span>
    <span class="ltx_bibblock">
     Annie M. Westerlund, Siva Manohar Koki, Supriya Kancharla, Alessandro Tibo, Lakshidaa Saigiridharan, Mikhail Kabeshov, Rocio Mercado, and Samuel Genheden.
    </span>
    <span class="ltx_bibblock">
     Do Chemformers Dream of Organic Matter? Evaluating a Transformer Model for Multistep Retrosynthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib316.1.1">
      JOURNAL OF CHEMICAL INFORMATION AND MODELING
     </em>
     , 64(8):3021–3033, April 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c01685" target="_blank" title="">
      10.1021/acs.jcim.3c01685
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcAuth=DynamicDOIArticle&amp;SrcApp=UA&amp;KeyAID=10.1021%2Facs.jcim.3c01685&amp;DestApp=DOI&amp;SrcAppSID=USW2EC0C6Dsgh6m89jbQCLUMIbTAg&amp;SrcJTitle=JOURNAL+OF+CHEMICAL+INFORMATION+AND+MODELING&amp;DestDOIRegistrantName=American+Chemical+Society" target="_blank" title="">
      https://www.webofscience.com/api/gateway?GWVersion=2&amp;SrcAuth=DynamicDOIArticle&amp;SrcApp=UA&amp;KeyAID=10.1021%2Facs.jcim.3c01685&amp;DestApp=DOI&amp;SrcAppSID=USW2EC0C6Dsgh6m89jbQCLUMIbTAg&amp;SrcJTitle=JOURNAL+OF+CHEMICAL+INFORMATION+AND+MODELING&amp;DestDOIRegistrantName=American+Chemical+Society
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Num Pages: 13 Place: Washington Publisher: Amer Chemical Soc Web of Science ID: WOS:001201284600001.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib317">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2020
    </span>
    <span class="ltx_bibblock">
     Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang.
    </span>
    <span class="ltx_bibblock">
     Predicting Retrosynthetic Reactions Using Self-Corrected Transformer Neural Networks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib317.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 60(1):47–55, January 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.9b00949" target="_blank" title="">
      10.1021/acs.jcim.9b00949
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.9b00949" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.9b00949
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib318">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lowe 2012
    </span>
    <span class="ltx_bibblock">
     Daniel Mark Lowe.
    </span>
    <span class="ltx_bibblock">
     Extraction of chemical structures and reactions from the literature.
    </span>
    <span class="ltx_bibblock">
     October 2012.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.dspace.cam.ac.uk/handle/1810/244727" target="_blank" title="">
      http://www.dspace.cam.ac.uk/handle/1810/244727
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib319">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. 2024
    </span>
    <span class="ltx_bibblock">
     Junren Li, Lei Fang, and Jian-Guang Lou.
    </span>
    <span class="ltx_bibblock">
     Retro-BLEU: quantifying chemical plausibility of retrosynthesis routes through reaction template sequence analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib319.1.1">
      Digital Discovery
     </em>
     , 3(3):482–490, 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D3DD00219E" target="_blank" title="">
      10.1039/D3DD00219E
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00219e" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00219e
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib320">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Papineni et al. 2002
    </span>
    <span class="ltx_bibblock">
     Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
    </span>
    <span class="ltx_bibblock">
     BLEU: a method for automatic evaluation of machine translation.
    </span>
    <span class="ltx_bibblock">
     In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors,
     <em class="ltx_emph ltx_font_italic" id="bib.bib320.1.1">
      Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL ’02
     </em>
     , pages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" target="_blank" title="">
      10.3115/1073083.1073135
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://portal.acm.org/citation.cfm?doid=1073083.1073135" target="_blank" title="">
      http://portal.acm.org/citation.cfm?doid=1073083.1073135
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib321">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lin 2004
    </span>
    <span class="ltx_bibblock">
     Chin-Yew Lin.
    </span>
    <span class="ltx_bibblock">
     ROUGE: A Package for Automatic Evaluation of Summaries.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib321.1.1">
      Text Summarization Branches Out
     </em>
     , pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W04-1013" target="_blank" title="">
      https://aclanthology.org/W04-1013
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib322">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Carstensen and Attarchi 1988
    </span>
    <span class="ltx_bibblock">
     J. T. Carstensen and Faraneh Attarchi.
    </span>
    <span class="ltx_bibblock">
     Decomposition of Aspirin in the Solid State in the Presence of Limited Amounts of Moisture III: Effect of Temperature and a Possible Mechanism.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib322.1.1">
      Journal of Pharmaceutical Sciences
     </em>
     , 77(4):318–321, April 1988.
    </span>
    <span class="ltx_bibblock">
     ISSN 0022-3549.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/jps.2600770407" target="_blank" title="">
      10.1002/jps.2600770407
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0022354915476658" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S0022354915476658
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib323">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Seidl et al. 2023
    </span>
    <span class="ltx_bibblock">
     Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Guenter Klambauer.
    </span>
    <span class="ltx_bibblock">
     Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib323.1.1">
      Arxiv
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib324">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Hanwen Xu, Addie Woicik, Hoifung Poon, Russ B. Altman, and Sheng Wang.
    </span>
    <span class="ltx_bibblock">
     Multilingual translation for zero-shot biomedical classification using BioTranslator.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib324.1.1">
      NATURE COMMUNICATIONS
     </em>
     , 14(1), February 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-1723.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41467-023-36476-2" target="_blank" title="">
      10.1038/s41467-023-36476-2
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib325">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao.
    </span>
    <span class="ltx_bibblock">
     ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib325.1.1">
      Arxiv
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib326">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shoghi et al. 2024
    </span>
    <span class="ltx_bibblock">
     Nima Shoghi, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick, and Brandon M. Wood.
    </span>
    <span class="ltx_bibblock">
     From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction, May 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.16802" target="_blank" title="">
      http://arxiv.org/abs/2310.16802
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2310.16802 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib327">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023c
    </span>
    <span class="ltx_bibblock">
     Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu.
    </span>
    <span class="ltx_bibblock">
     MolXPT: Wrapping Molecules with Text for Generative Pre-training.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib327.1.1">
      Arxiv
     </em>
     , 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib328">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Haohui Zhang, Juntong Wu, Shichao Liu, and Shen Han.
    </span>
    <span class="ltx_bibblock">
     A pre-trained multi-representation fusion network for molecular property prediction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib328.1.1">
      INFORMATION FUSION
     </em>
     , 103, March 2024b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1566-2535.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.inffus.2023.102092" target="_blank" title="">
      10.1016/j.inffus.2023.102092
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib329">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren.
    </span>
    <span class="ltx_bibblock">
     Git-mol: A multi-modal large language model for molecular science with graph, image, and text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib329.1.1">
      Computers in Biology and Medicine
     </em>
     , 171:108073, March 2024a.
    </span>
    <span class="ltx_bibblock">
     ISSN 0010-4825.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.compbiomed.2024.108073" target="_blank" title="">
      10.1016/j.compbiomed.2024.108073
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.compbiomed.2024.108073" target="_blank" title="">
      http://dx.doi.org/10.1016/j.compbiomed.2024.108073
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib330">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Changnan Gao, Wenjie Bao, Shuang Wang, Jianyang Zheng, Lulu Wang, Yongqi Ren, Linfang Jiao, Jianmin Wang, and Xun Wang.
    </span>
    <span class="ltx_bibblock">
     DockingGA: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib330.1.1">
      BRIEFINGS IN FUNCTIONAL GENOMICS
     </em>
     , April 2024a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-2649.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1093/bfgp/elae011" target="_blank" title="">
      10.1093/bfgp/elae011
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib331">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. 2024
    </span>
    <span class="ltx_bibblock">
     Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Siqi Sun, Jianxin Lin, Longyue Wang, and Xiangxiang Zeng.
    </span>
    <span class="ltx_bibblock">
     Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib331.1.1">
      Arxiv
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib332">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. 2024
    </span>
    <span class="ltx_bibblock">
     Huaisheng Zhu, Teng Xiao, and Vasant G Honavar.
    </span>
    <span class="ltx_bibblock">
     3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib332.1.1">
      Arxiv
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib333">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gong et al. 2024
    </span>
    <span class="ltx_bibblock">
     Haisong Gong, Qiang Liu, Shu Wu, and Liang Wang.
    </span>
    <span class="ltx_bibblock">
     Text-Guided Molecule Generation with Diffusion Language Model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib333.1.1">
      Arxiv
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib334">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fang et al. 2024
    </span>
    <span class="ltx_bibblock">
     Junfeng Fang, Shuai Zhang, Chang Wu, Zhiyuan Liu, Sihang Li, Kun Wang, Wenjie Du, and Xiang Wang.
    </span>
    <span class="ltx_bibblock">
     MolTC: Towards Molecular Relational Modeling In Language Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib334.1.1">
      Arxiv
     </em>
     , 2024.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib335">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Soares et al. 2023
    </span>
    <span class="ltx_bibblock">
     Eduardo Soares, Emilio Vital Brazil, Karen Fiorela Aquino Gutierrez, Renato Cerqueira, Dan Sanders, Kristin Schmidt, and Dmitry Zubarev.
    </span>
    <span class="ltx_bibblock">
     Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction, June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.14919" target="_blank" title="">
      http://arxiv.org/abs/2306.14919
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2306.14919 [physics, q-bio].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib336">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     mim 2021
    </span>
    <span class="ltx_bibblock">
     MIMIC-III documentation.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mimic.mit.edu/docs/iii/" target="_blank" title="">
      https://mimic.mit.edu/docs/iii/
     </a>
     , 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mimic.mit.edu/docs/iii/" target="_blank" title="">
      https://mimic.mit.edu/docs/iii/
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-3-25.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib337">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2019b
    </span>
    <span class="ltx_bibblock">
     Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
    </span>
    <span class="ltx_bibblock">
     RoBERTa: A robustly optimized BERT pretraining approach.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib337.1.1">
      arXiv [cs.CL]
     </em>
     , July 2019b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1907.11692" target="_blank" title="">
      http://arxiv.org/abs/1907.11692
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib338">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Smith et al. 2019
    </span>
    <span class="ltx_bibblock">
     Hannah Smith, Zeyu Zhang, John Culnan, and Peter Jansen.
    </span>
    <span class="ltx_bibblock">
     ScienceExamCER: A high-density fine-grained science-domain corpus for common entity recognition.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib338.1.1">
      arXiv [cs.CL]
     </em>
     , November 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1911.10436" target="_blank" title="">
      http://arxiv.org/abs/1911.10436
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib339">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Swain and Cole 2016
    </span>
    <span class="ltx_bibblock">
     Matthew C Swain and Jacqueline M Cole.
    </span>
    <span class="ltx_bibblock">
     ChemDataExtractor: A toolkit for automated extraction of chemical information from the scientific literature.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib339.1.1">
      J. Chem. Inf. Model.
     </em>
     , 56(10):1894–1904, October 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596, 1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.6b00207" target="_blank" title="">
      10.1021/acs.jcim.6b00207
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/acs.jcim.6b00207" target="_blank" title="">
      http://dx.doi.org/10.1021/acs.jcim.6b00207
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib340">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     340
    </span>
    <span class="ltx_bibblock">
     Santiago Ibanez.
    </span>
    <span class="ltx_bibblock">
     chemie-turk: Mechanical turk on your own machine for chemical literature annotation.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/asibanez/chemie-turk" target="_blank" title="">
      https://github.com/asibanez/chemie-turk
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib341">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shoeybi et al. 2019
    </span>
    <span class="ltx_bibblock">
     Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
    </span>
    <span class="ltx_bibblock">
     Megatron-LM: Training multi-billion parameter language models using model parallelism.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib341.1.1">
      arXiv [cs.CL]
     </em>
     , September 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.08053" target="_blank" title="">
      http://arxiv.org/abs/1909.08053
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib342">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rajpurkar et al. 2016
    </span>
    <span class="ltx_bibblock">
     Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
    </span>
    <span class="ltx_bibblock">
     SQuAD: 100,000+ questions for machine comprehension of text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib342.1.1">
      arXiv [cs.CL]
     </em>
     , June 2016.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1606.05250" target="_blank" title="">
      http://arxiv.org/abs/1606.05250
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib343">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al. 2020
    </span>
    <span class="ltx_bibblock">
     Yifan Peng, Qingyu Chen, and Zhiyong Lu.
    </span>
    <span class="ltx_bibblock">
     An empirical study of multi-task learning on BERT for biomedical text mining.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib343.1.1">
      arXiv [cs.CL]
     </em>
     , May 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2005.02799" target="_blank" title="">
      http://arxiv.org/abs/2005.02799
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib344">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Welbl et al. 2017
    </span>
    <span class="ltx_bibblock">
     Johannes Welbl, Nelson F Liu, and Matt Gardner.
    </span>
    <span class="ltx_bibblock">
     Crowdsourcing multiple choice science questions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib344.1.1">
      arXiv [cs.HC]
     </em>
     , July 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.06209" target="_blank" title="">
      http://arxiv.org/abs/1707.06209
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib345">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jin et al. 2021
    </span>
    <span class="ltx_bibblock">
     Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.
    </span>
    <span class="ltx_bibblock">
     What disease does this patient have? a large-scale open domain question answering dataset from medical exams.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib345.1.1">
      Preprints
     </em>
     , May 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2009.13081" target="_blank" title="">
      http://arxiv.org/abs/2009.13081
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib346">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pal et al. 2022
    </span>
    <span class="ltx_bibblock">
     Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.
    </span>
    <span class="ltx_bibblock">
     MedMCQA : A large-scale multi-subject multi-choice dataset for medical domain question answering.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib346.1.1">
      arXiv [cs.CL]
     </em>
     , March 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.14371" target="_blank" title="">
      http://arxiv.org/abs/2203.14371
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib347">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jin et al. 2019
    </span>
    <span class="ltx_bibblock">
     Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.
    </span>
    <span class="ltx_bibblock">
     PubMedQA: A dataset for biomedical research question answering.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib347.1.1">
      arXiv [cs.CL]
     </em>
     , September 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.06146" target="_blank" title="">
      http://arxiv.org/abs/1909.06146
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib348">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Castro Nascimento and Pimentel 2023
    </span>
    <span class="ltx_bibblock">
     Cayque Monteiro Castro Nascimento and André Silva Pimentel.
    </span>
    <span class="ltx_bibblock">
     Do Large Language Models Understand Chemistry? A Conversation with ChatGPT.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib348.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 63(6):1649–1655, March 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c00285" target="_blank" title="">
      10.1021/acs.jcim.3c00285
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c00285" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c00285
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib349">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Humphry and Fuller 2023
    </span>
    <span class="ltx_bibblock">
     Tim Humphry and Amy L. Fuller.
    </span>
    <span class="ltx_bibblock">
     Potential ChatGPT Use in Undergraduate Chemistry Laboratories.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib349.1.1">
      JOURNAL OF CHEMICAL EDUCATION
     </em>
     , 100(4):1434–1436, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0021-9584.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jchemed.3c00006" target="_blank" title="">
      10.1021/acs.jchemed.3c00006
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib350">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Emenike and Emenike 2023
    </span>
    <span class="ltx_bibblock">
     Mary E. Emenike and Bright U. Emenike.
    </span>
    <span class="ltx_bibblock">
     Was This Title Generated by ChatGPT? Considerations for Artificial Intelligence Text-Generation Software Programs for Chemists and Chemistry Educators.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib350.1.1">
      JOURNAL OF CHEMICAL EDUCATION
     </em>
     , 100(4):1413–1418, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0021-9584.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jchemed.3c00063" target="_blank" title="">
      10.1021/acs.jchemed.3c00063
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib351">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fergus et al. 2023
    </span>
    <span class="ltx_bibblock">
     Suzanne Fergus, Michelle Botha, and Mehrnoosh Ostovar.
    </span>
    <span class="ltx_bibblock">
     Evaluating Academic Answers Generated Using ChatGPT.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib351.1.1">
      JOURNAL OF CHEMICAL EDUCATION
     </em>
     , 100(4):1672–1675, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0021-9584.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jchemed.3c00087" target="_blank" title="">
      10.1021/acs.jchemed.3c00087
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib352">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Ali H. Alawadhi, Saumil Chheda, S. Ephraim Neumann, Nakul Rampal, Shengchao Liu, Ha L. Nguyen, Yen-hsu Lin, Zichao Rong, J. Ilja Siepmann, Laura Gagliardi, Anima Anandkumar, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi.
    </span>
    <span class="ltx_bibblock">
     Shaping the Water-Harvesting Behavior of Metal-Organic Frameworks Aided by Fine-Tuned GPT Models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib352.1.1">
      JOURNAL OF THE AMERICAN CHEMICAL SOCIETY
     </em>
     , 145(51):28284–28295, December 2023a.
    </span>
    <span class="ltx_bibblock">
     ISSN 0002-7863.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jacs.3c12086" target="_blank" title="">
      10.1021/jacs.3c12086
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib353">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi.
    </span>
    <span class="ltx_bibblock">
     ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib353.1.1">
      Journal of the American Chemical Society
     </em>
     , 145(Copyright © 2024 American Chemical Society (ACS). All Rights Reserved.; Copyright © 2024 U.S. National Library of Medicine.):18048–18062, 2023b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1520-5126.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jacs.3c05819" target="_blank" title="">
      10.1021/jacs.3c05819
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib354">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al. 2024
    </span>
    <span class="ltx_bibblock">
     Zikai Xie, Xenophon Evangelopoulos, Ömer H. Omar, Alessandro Troisi, Andrew I. Cooper, and Linjiang Chen.
    </span>
    <span class="ltx_bibblock">
     Fine-tuning GPT-3 for machine learning electronic and functional properties of organic molecules.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib354.1.1">
      Chemical Science
     </em>
     , 15(2):500–510, January 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-6539.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D3SC04610A" target="_blank" title="">
      10.1039/D3SC04610A
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2024/sc/d3sc04610a" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2024/sc/d3sc04610a
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib355">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023c
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Zichao Rong, Nakul Rampal, Christian Borgs, Jennifer T. Chayes, and Omar M. Yaghi.
    </span>
    <span class="ltx_bibblock">
     A GPT-4 Reticular Chemist for Guiding MOF Discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib355.1.1">
      ANGEWANDTE CHEMIE-INTERNATIONAL EDITION
     </em>
     , 62(46), November 2023c.
    </span>
    <span class="ltx_bibblock">
     ISSN 1433-7851.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/anie.202311983" target="_blank" title="">
      10.1002/anie.202311983
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib356">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deb et al. 2024
    </span>
    <span class="ltx_bibblock">
     Jyotirmoy Deb, Lakshi Saikia, Kripa Dristi Dihingia, and G. Narahari Sastry.
    </span>
    <span class="ltx_bibblock">
     Chatgpt in the material design: Selected case studies to assess the potential of chatgpt.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib356.1.1">
      Journal of Chemical Information and Modeling
     </em>
     , 64(3):799–811, 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.3c01702" target="_blank" title="">
      10.1021/acs.jcim.3c01702
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.jcim.3c01702" target="_blank" title="">
      https://doi.org/10.1021/acs.jcim.3c01702
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     PMID: 38237025.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib357">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bloom 1968
    </span>
    <span class="ltx_bibblock">
     Benjamin S Bloom.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib357.1.1">
      Taxonomy of Educational Objectives: The Classification of Educational Goals; Handbook. Cognitive Domain
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     McKay, 1968.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib358">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bloom 2010
    </span>
    <span class="ltx_bibblock">
     Benjamin Samuel Bloom.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib358.1.1">
      A taxonomy for learning, teaching, and assessing: A revision of Bloom’s taxonomy of educational objectives
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Longman, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib359">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Coley et al. 2019b
    </span>
    <span class="ltx_bibblock">
     Connor W. Coley, Dale A. Thomas, Justin A. M. Lummiss, Jonathan N. Jaworski, Christopher P. Breen, Victor Schultz, Travis Hart, Joshua S. Fishman, Luke Rogers, Hanyu Gao, Robert W. Hicklin, Pieter P. Plehiers, Joshua Byington, John S. Piotti, William H. Green, A. John Hart, Timothy F. Jamison, and Klavs F. Jensen.
    </span>
    <span class="ltx_bibblock">
     A robotic platform for flow synthesis of organic compounds informed by AI planning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib359.1.1">
      Science
     </em>
     , 365(6453):eaax1566, August 2019b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/science.aax1566" target="_blank" title="">
      10.1126/science.aax1566
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/full/10.1126/science.aax1566" target="_blank" title="">
      https://www.science.org/doi/full/10.1126/science.aax1566
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib360">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gromski et al. 2020
    </span>
    <span class="ltx_bibblock">
     Piotr S. Gromski, Jarosław M. Granda, and Leroy Cronin.
    </span>
    <span class="ltx_bibblock">
     Universal Chemical Synthesis and Discovery with ‘The Chemputer’.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib360.1.1">
      Trends in Chemistry
     </em>
     , 2(1):4–12, January 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 25895974.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.trechm.2019.07.004" target="_blank" title="">
      10.1016/j.trechm.2019.07.004
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://linkinghub.elsevier.com/retrieve/pii/S2589597419301868" target="_blank" title="">
      https://linkinghub.elsevier.com/retrieve/pii/S2589597419301868
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib361">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Grisoni et al. 2021
    </span>
    <span class="ltx_bibblock">
     Francesca Grisoni, Berend J. H. Huisman, Alexander L. Button, Michael Moret, Kenneth Atz, Daniel Merk, and Gisbert Schneider.
    </span>
    <span class="ltx_bibblock">
     Combining generative artificial intelligence and on-chip synthesis for de novo drug design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib361.1.1">
      Science Advances
     </em>
     , 7(24):eabg3338, June 2021.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/sciadv.abg3338" target="_blank" title="">
      10.1126/sciadv.abg3338
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.science.org/doi/full/10.1126/sciadv.abg3338" target="_blank" title="">
      https://www.science.org/doi/full/10.1126/sciadv.abg3338
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Association for the Advancement of Science.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib362">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goldman et al. 2022
    </span>
    <span class="ltx_bibblock">
     Brian Goldman, Steven Kearnes, Trevor Kramer, Patrick Riley, and W. Patrick Walters.
    </span>
    <span class="ltx_bibblock">
     Defining Levels of Automated Chemical Design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib362.1.1">
      Journal of Medicinal Chemistry
     </em>
     , 65(10):7073–7087, May 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 0022-2623, 1520-4804.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jmedchem.2c00334" target="_blank" title="">
      10.1021/acs.jmedchem.2c00334
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00334" target="_blank" title="">
      https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00334
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib363">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schneider 2018
    </span>
    <span class="ltx_bibblock">
     Gisbert Schneider.
    </span>
    <span class="ltx_bibblock">
     Automating drug discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib363.1.1">
      Nature Reviews Drug Discovery
     </em>
     , 17(2):97–113, February 2018.
    </span>
    <span class="ltx_bibblock">
     ISSN 1474-1784.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/nrd.2017.232" target="_blank" title="">
      10.1038/nrd.2017.232
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/nrd.2017.232" target="_blank" title="">
      https://doi.org/10.1038/nrd.2017.232
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib364">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Janet et al. 2023
    </span>
    <span class="ltx_bibblock">
     Jon Paul Janet, Lewis Mervin, and Ola Engkvist.
    </span>
    <span class="ltx_bibblock">
     Artificial intelligence in molecular
     <span class="ltx_text ltx_font_italic" id="bib.bib364.1.1">
      de novo
     </span>
     design: Integration with experiment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib364.2.1">
      Current Opinion in Structural Biology
     </em>
     , 80:102575, June 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0959-440X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.sbi.2023.102575" target="_blank" title="">
      10.1016/j.sbi.2023.102575
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0959440X23000490" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S0959440X23000490
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib365">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Coley et al. 2020a
    </span>
    <span class="ltx_bibblock">
     Connor W. Coley, Natalie S. Eyke, and Klavs F. Jensen.
    </span>
    <span class="ltx_bibblock">
     Autonomous Discovery in the Chemical Sciences Part I: Progress.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib365.1.1">
      Angewandte Chemie International Edition
     </em>
     , 59(51):22858–22893, 2020a.
    </span>
    <span class="ltx_bibblock">
     ISSN 1521-3773.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/anie.201909987" target="_blank" title="">
      10.1002/anie.201909987
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909987" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909987
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201909987.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib366">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Coley et al. 2020b
    </span>
    <span class="ltx_bibblock">
     Connor W. Coley, Natalie S. Eyke, and Klavs F. Jensen.
    </span>
    <span class="ltx_bibblock">
     Autonomous Discovery in the Chemical Sciences Part II: Outlook.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib366.1.1">
      Angewandte Chemie International Edition
     </em>
     , 59(52):23414–23436, 2020b.
    </span>
    <span class="ltx_bibblock">
     ISSN 1521-3773.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1002/anie.201909989" target="_blank" title="">
      10.1002/anie.201909989
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909989" target="_blank" title="">
      https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909989
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201909989.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib367">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Thakkar et al. 2021
    </span>
    <span class="ltx_bibblock">
     Amol Thakkar, Simon Johansson, Kjell Jorner, David Buttar, Jean-Louis Reymond, and Ola Engkvist.
    </span>
    <span class="ltx_bibblock">
     Artificial intelligence and automation in computer aided synthesis planning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib367.1.1">
      Reaction Chemistry &amp; Engineering
     </em>
     , 6(1):27–51, January 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 2058-9883.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D0RE00340A" target="_blank" title="">
      10.1039/D0RE00340A
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2021/re/d0re00340a" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2021/re/d0re00340a
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: The Royal Society of Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib368">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. 2021
    </span>
    <span class="ltx_bibblock">
     Yuning Shen, Julia E. Borowski, Melissa A. Hardy, Richmond Sarpong, Abigail G. Doyle, and Tim Cernak.
    </span>
    <span class="ltx_bibblock">
     Automation and computer-assisted planning for chemical synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib368.1.1">
      Nature Reviews Methods Primers
     </em>
     , 1(1):23, March 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 2662-8449.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s43586-021-00022-5" target="_blank" title="">
      10.1038/s43586-021-00022-5
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s43586-021-00022-5" target="_blank" title="">
      https://www.nature.com/articles/s43586-021-00022-5
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib369">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2021
    </span>
    <span class="ltx_bibblock">
     Yuxiao Liu, Lingyu Sun, Hui Zhang, Luoran Shang, and Yuanjin Zhao.
    </span>
    <span class="ltx_bibblock">
     Microfluidics for Drug Development: From Synthesis to Evaluation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib369.1.1">
      Chemical Reviews
     </em>
     , 121(13):7468–7529, July 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 0009-2665.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.chemrev.0c01289" target="_blank" title="">
      10.1021/acs.chemrev.0c01289
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acs.chemrev.0c01289" target="_blank" title="">
      https://doi.org/10.1021/acs.chemrev.0c01289
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: American Chemical Society.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib370">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Darvish et al. 2024
    </span>
    <span class="ltx_bibblock">
     Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Alán Aspuru-Guzik, Animesh Garg, and Florian Shkurti.
    </span>
    <span class="ltx_bibblock">
     ORGANA: A robotic assistant for automated chemistry experimentation and characterization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib370.1.1">
      arXiv [cs.RO]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.06949" target="_blank" title="">
      http://arxiv.org/abs/2401.06949
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib371">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Šalamon 2011
    </span>
    <span class="ltx_bibblock">
     Tomáš Šalamon.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib371.1.1">
      Design of Agent-based Models: Developing Computer Simulations for a Better Understanding of Social Processes
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Tomáš Bruckner, Repin, CZE, 2011.
    </span>
    <span class="ltx_bibblock">
     ISBN 9788090466111.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://play.google.com/store/books/details?id=2rCdKnltaH8C" target="_blank" title="">
      https://play.google.com/store/books/details?id=2rCdKnltaH8C
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib372">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xi et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.
    </span>
    <span class="ltx_bibblock">
     The rise and potential of large language model based agents: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib372.1.1">
      arXiv [cs.AI]
     </em>
     , September 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.07864" target="_blank" title="">
      http://arxiv.org/abs/2309.07864
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib373">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik.
    </span>
    <span class="ltx_bibblock">
     Empowering biomedical discovery with AI agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib373.1.1">
      arXiv [cs.AI]
     </em>
     , April 2024b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.02831" target="_blank" title="">
      http://arxiv.org/abs/2404.02831
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib374">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen.
    </span>
    <span class="ltx_bibblock">
     A survey on large language model based autonomous agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib374.1.1">
      Front. Comput. Sci.
     </em>
     , 18(6), December 2024a.
    </span>
    <span class="ltx_bibblock">
     ISSN 2095-2228,2095-2236.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s11704-024-40231-1" target="_blank" title="">
      10.1007/s11704-024-40231-1
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s11704-024-40231-1" target="_blank" title="">
      http://dx.doi.org/10.1007/s11704-024-40231-1
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib375">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weng 2023
    </span>
    <span class="ltx_bibblock">
     Lilian Weng.
    </span>
    <span class="ltx_bibblock">
     LLM powered autonomous agents.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank" title="">
      https://lilianweng.github.io/posts/2023-06-23-agent/
     </a>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank" title="">
      https://lilianweng.github.io/posts/2023-06-23-agent/
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-1-22.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib376">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sumers et al. 2023
    </span>
    <span class="ltx_bibblock">
     Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths.
    </span>
    <span class="ltx_bibblock">
     Cognitive architectures for language agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib376.1.1">
      arXiv [cs.AI]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.02427" target="_blank" title="">
      http://arxiv.org/abs/2309.02427
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib377">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023d
    </span>
    <span class="ltx_bibblock">
     Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li.
    </span>
    <span class="ltx_bibblock">
     Enhancing large language model with self-controlled memory framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib377.1.1">
      arXiv [cs.CL]
     </em>
     , April 2023d.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.13343" target="_blank" title="">
      http://arxiv.org/abs/2304.13343
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib378">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Yi Zhang, Zhongyang Yu, Wanqi Jiang, Yufeng Shen, and Jin Li.
    </span>
    <span class="ltx_bibblock">
     Long-term memory for large language models through topic-based vector database.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib378.1.1">
      2023 International Conference on Asian Language Processing (IALP)
     </em>
     . IEEE, November 2023b.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ialp61005.2023.10337079" target="_blank" title="">
      10.1109/ialp61005.2023.10337079
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10337079/" target="_blank" title="">
      https://ieeexplore.ieee.org/document/10337079/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib379">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. 2023
    </span>
    <span class="ltx_bibblock">
     Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai.
    </span>
    <span class="ltx_bibblock">
     Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib379.1.1">
      arXiv [cs.AI]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.17144" target="_blank" title="">
      http://arxiv.org/abs/2305.17144
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib380">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong et al. 2023
    </span>
    <span class="ltx_bibblock">
     Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang.
    </span>
    <span class="ltx_bibblock">
     MemoryBank: Enhancing large language models with long-term memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib380.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.10250" target="_blank" title="">
      http://arxiv.org/abs/2305.10250
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib381">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Yikun Han, Chunjiang Liu, and Pengfei Wang.
    </span>
    <span class="ltx_bibblock">
     A comprehensive survey on vector database: Storage and retrieval technique, challenge.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib381.1.1">
      arXiv [cs.DB]
     </em>
     , October 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.11703" target="_blank" title="">
      http://arxiv.org/abs/2310.11703
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib382">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang.
    </span>
    <span class="ltx_bibblock">
     ExpeL: LLM agents are experiential learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib382.1.1">
      arXiv [cs.LG]
     </em>
     , August 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.10144" target="_blank" title="">
      http://arxiv.org/abs/2308.10144
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib383">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     383
    </span>
    <span class="ltx_bibblock">
     ANN-benchmarks.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ann-benchmarks.com/" target="_blank" title="">
      https://ann-benchmarks.com/
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ann-benchmarks.com/" target="_blank" title="">
      https://ann-benchmarks.com/
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-2-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib384">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hatalis et al. 2023
    </span>
    <span class="ltx_bibblock">
     Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.
    </span>
    <span class="ltx_bibblock">
     Memory matters: The need to improve Long-Term memory in LLM-Agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib384.1.1">
      AAAI-SS
     </em>
     , 2(1):277–280, 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2994-4317, 2994-4317.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaaiss.v2i1.27688" target="_blank" title="">
      10.1609/aaaiss.v2i1.27688
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI-SS/article/view/27688" target="_blank" title="">
      https://ojs.aaai.org/index.php/AAAI-SS/article/view/27688
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib385">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. 2020
    </span>
    <span class="ltx_bibblock">
     Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Language Models are Few-Shot Learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib385.1.1">
      arXiv [cs.CL]
     </em>
     , May 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2005.14165" target="_blank" title="">
      http://arxiv.org/abs/2005.14165
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib386">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. 2023
    </span>
    <span class="ltx_bibblock">
     Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.
    </span>
    <span class="ltx_bibblock">
     Generative agents: Interactive simulacra of human behavior.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib386.1.1">
      arXiv [cs.HC]
     </em>
     , April 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.03442" target="_blank" title="">
      http://arxiv.org/abs/2304.03442
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib387">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Raman et al. 2022
    </span>
    <span class="ltx_bibblock">
     S S Raman, V Cohen, E Rosen, and I Idrees.
    </span>
    <span class="ltx_bibblock">
     Planning with large language models via corrective re-prompting.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib387.1.1">
      Foundation Models
     </em>
     , November 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=cMDMRBe1TKs" target="_blank" title="">
      https://openreview.net/pdf?id=cMDMRBe1TKs
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib388">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dhuliawala et al. 2023
    </span>
    <span class="ltx_bibblock">
     Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston.
    </span>
    <span class="ltx_bibblock">
     Chain-of-verification reduces hallucination in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib388.1.1">
      arXiv [cs.CL]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.11495" target="_blank" title="">
      http://arxiv.org/abs/2309.11495
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib389">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. 2022
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.
    </span>
    <span class="ltx_bibblock">
     Inner monologue: Embodied reasoning through planning with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib389.1.1">
      arXiv [cs.RO]
     </em>
     , July 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2207.05608" target="_blank" title="">
      http://arxiv.org/abs/2207.05608
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib390">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. 2022
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib390.1.1">
      arXiv [cs.CL]
     </em>
     , pages 22199–22213, May 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf" target="_blank" title="">
      https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib391">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. 2022
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, E Chi, F Xia, Quoc Le, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Chain of thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib391.1.1">
      Neural Inf Process Syst
     </em>
     , abs/2201.11903, January 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2201.11903" target="_blank" title="">
      http://arxiv.org/abs/2201.11903
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib392">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2022c
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib392.1.1">
      arXiv [cs.CL]
     </em>
     , March 2022c.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.11171" target="_blank" title="">
      http://arxiv.org/abs/2203.11171
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib393">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. 2022
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
    </span>
    <span class="ltx_bibblock">
     ReAct: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib393.1.1">
      arXiv [cs.CL]
     </em>
     , October 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.03629" target="_blank" title="">
      http://arxiv.org/abs/2210.03629
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib394">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hao et al. 2023
    </span>
    <span class="ltx_bibblock">
     Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.
    </span>
    <span class="ltx_bibblock">
     Reasoning with language model is planning with world model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib394.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14992" target="_blank" title="">
      http://arxiv.org/abs/2305.14992
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib395">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023d
    </span>
    <span class="ltx_bibblock">
     Hao Liu, Carmelo Sferrazza, and Pieter Abbeel.
    </span>
    <span class="ltx_bibblock">
     Chain of hindsight aligns language models with feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib395.1.1">
      arXiv [cs.LG]
     </em>
     , February 2023d.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.02676" target="_blank" title="">
      http://arxiv.org/abs/2302.02676
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib396">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. 2023
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib396.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.10601" target="_blank" title="">
      http://arxiv.org/abs/2305.10601
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib397">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kang et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, and Jie Fu.
    </span>
    <span class="ltx_bibblock">
     Think before you act: Decision transformers with working memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib397.1.1">
      arXiv [cs.LG]
     </em>
     , May 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.16338" target="_blank" title="">
      http://arxiv.org/abs/2305.16338
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib398">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib398.1.1">
      arXiv [cs.CL]
     </em>
     , January 2024b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.13996" target="_blank" title="">
      http://arxiv.org/abs/2401.13996
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib399">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Anil et al. 2023
    </span>
    <span class="ltx_bibblock">
     Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta,
Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, Yaguang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
    </span>
    <span class="ltx_bibblock">
     PaLM 2 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib399.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.10403" target="_blank" title="">
      http://arxiv.org/abs/2305.10403
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib400">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cobbe et al. 2021
    </span>
    <span class="ltx_bibblock">
     Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
    </span>
    <span class="ltx_bibblock">
     Training verifiers to solve math word problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib400.1.1">
      arXiv [cs.LG]
     </em>
     , October 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2110.14168" target="_blank" title="">
      http://arxiv.org/abs/2110.14168
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib401">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jie et al. 2022
    </span>
    <span class="ltx_bibblock">
     Zhanming Jie, Jierui Li, and Wei Lu.
    </span>
    <span class="ltx_bibblock">
     Learning to reason deductively: Math word problem solving as complex relation extraction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib401.1.1">
      arXiv [cs.CL]
     </em>
     , March 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.10316" target="_blank" title="">
      http://arxiv.org/abs/2203.10316
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib402">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lan et al. 2021
    </span>
    <span class="ltx_bibblock">
     Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim.
    </span>
    <span class="ltx_bibblock">
     MWPToolkit: An open-source framework for deep learning-based math word problem solvers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib402.1.1">
      arXiv [cs.CL]
     </em>
     , September 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2109.00799" target="_blank" title="">
      http://arxiv.org/abs/2109.00799
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib403">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Besta et al. 2023
    </span>
    <span class="ltx_bibblock">
     Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler.
    </span>
    <span class="ltx_bibblock">
     Graph of thoughts: Solving elaborate problems with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib403.1.1">
      arXiv [cs.CL]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.09687" target="_blank" title="">
      http://arxiv.org/abs/2308.09687
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib404">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sel et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin.
    </span>
    <span class="ltx_bibblock">
     Algorithm of thoughts: Enhancing exploration of ideas in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib404.1.1">
      arXiv [cs.CL]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.10379" target="_blank" title="">
      http://arxiv.org/abs/2308.10379
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib405">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. 2023
    </span>
    <span class="ltx_bibblock">
     Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi.
    </span>
    <span class="ltx_bibblock">
     Encouraging divergent thinking in large language models through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib405.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.19118" target="_blank" title="">
      http://arxiv.org/abs/2305.19118
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib406">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib406.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14325" target="_blank" title="">
      http://arxiv.org/abs/2305.14325
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib407">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shan Zhang, Jie Fu, and Zhiyuan Liu.
    </span>
    <span class="ltx_bibblock">
     ChatEval: Towards better LLM-based evaluators through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib407.1.1">
      ArXiv
     </em>
     , abs/2308.07201, August 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2331-8422.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.07201" target="_blank" title="">
      10.48550/arXiv.2308.07201
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.48550/arXiv.2308.07201" target="_blank" title="">
      http://dx.doi.org/10.48550/arXiv.2308.07201
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib408">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al. 2022
    </span>
    <span class="ltx_bibblock">
     Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su.
    </span>
    <span class="ltx_bibblock">
     LLM-planner: Few-shot grounded planning for embodied agents with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib408.1.1">
      arXiv [cs.AI]
     </em>
     , December 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2212.04088" target="_blank" title="">
      http://arxiv.org/abs/2212.04088
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib409">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023e
    </span>
    <span class="ltx_bibblock">
     Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone.
    </span>
    <span class="ltx_bibblock">
     LLM+P: Empowering large language models with optimal planning proficiency.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib409.1.1">
      arXiv [cs.AI]
     </em>
     , April 2023e.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.11477" target="_blank" title="">
      http://arxiv.org/abs/2304.11477
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib410">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib410.1.1">
      arXiv [cs.CL]
     </em>
     , March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.17651" target="_blank" title="">
      http://arxiv.org/abs/2303.17651
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib411">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xi et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang.
    </span>
    <span class="ltx_bibblock">
     Self-polish: Enhance reasoning in large language models via problem refinement.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib411.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14497" target="_blank" title="">
      http://arxiv.org/abs/2305.14497
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib412">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2024c
    </span>
    <span class="ltx_bibblock">
     Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö Arik.
    </span>
    <span class="ltx_bibblock">
     Chain of agents: Large language models collaborating on long-context tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib412.1.1">
      arXiv [cs.CL]
     </em>
     , June 2024c.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2406.02818" target="_blank" title="">
      http://arxiv.org/abs/2406.02818
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib413">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023e
    </span>
    <span class="ltx_bibblock">
     Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang.
    </span>
    <span class="ltx_bibblock">
     Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib413.1.1">
      arXiv [cs.AI]
     </em>
     , February 2023e.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.01560" target="_blank" title="">
      http://arxiv.org/abs/2302.01560
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib414">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deshpande et al. 2023
    </span>
    <span class="ltx_bibblock">
     Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan.
    </span>
    <span class="ltx_bibblock">
     Toxicity in ChatGPT: Analyzing persona-assigned language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib414.1.1">
      arXiv [cs.CL]
     </em>
     , April 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.05335" target="_blank" title="">
      http://arxiv.org/abs/2304.05335
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib415">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. 2023
    </span>
    <span class="ltx_bibblock">
     Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber.
    </span>
    <span class="ltx_bibblock">
     MetaGPT: Meta programming for a multi-agent collaborative framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib415.1.1">
      arXiv [cs.AI]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.00352" target="_blank" title="">
      http://arxiv.org/abs/2308.00352
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib416">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. 2023
    </span>
    <span class="ltx_bibblock">
     Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
    </span>
    <span class="ltx_bibblock">
     CAMEL: Communicative agents for “mind” exploration of large language model society.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib416.1.1">
      arXiv [cs.AI]
     </em>
     , March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.17760" target="_blank" title="">
      http://arxiv.org/abs/2303.17760
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib417">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jinxin et al. 2023
    </span>
    <span class="ltx_bibblock">
     Shi Jinxin, Zhao Jiabao, Wang Yilei, Wu Xingjiao, Li Jiawen, and He Liang.
    </span>
    <span class="ltx_bibblock">
     CGMI: Configurable general multi-agent interaction framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib417.1.1">
      arXiv [cs.AI]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.12503" target="_blank" title="">
      http://arxiv.org/abs/2308.12503
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib418">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib418.1.1">
      arXiv [cs.SE]
     </em>
     , July 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.07924" target="_blank" title="">
      http://arxiv.org/abs/2307.07924
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib419">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shao et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu.
    </span>
    <span class="ltx_bibblock">
     Character-LLM: A trainable agent for role-playing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib419.1.1">
      arXiv [cs.CL]
     </em>
     , October 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.10158" target="_blank" title="">
      http://arxiv.org/abs/2310.10158
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib420">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ke et al. 2023
    </span>
    <span class="ltx_bibblock">
     Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang.
    </span>
    <span class="ltx_bibblock">
     CritiqueLLM: Scaling LLM-as-critic for effective and explainable evaluation of large language model generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib420.1.1">
      arXiv [cs.CL]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.18702" target="_blank" title="">
      http://arxiv.org/abs/2311.18702
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib421">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023f
    </span>
    <span class="ltx_bibblock">
     Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen.
    </span>
    <span class="ltx_bibblock">
     User behavior simulation with large language model based agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib421.1.1">
      arXiv [cs.IR]
     </em>
     , June 2023f.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.02552" target="_blank" title="">
      http://arxiv.org/abs/2306.02552
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib422">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Argyle et al. 2023
    </span>
    <span class="ltx_bibblock">
     Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate.
    </span>
    <span class="ltx_bibblock">
     Out of one, many: Using language models to simulate human samples.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib422.1.1">
      Polit. Anal.
     </em>
     , 31(3):337–351, July 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1047-1987, 1476-4989.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1017/pan.2023.2" target="_blank" title="">
      10.1017/pan.2023.2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49" target="_blank" title="">
      https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib423">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kirillov et al. 2023
    </span>
    <span class="ltx_bibblock">
     Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick.
    </span>
    <span class="ltx_bibblock">
     Segment anything.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib423.1.1">
      arXiv [cs.CV]
     </em>
     , April 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.02643" target="_blank" title="">
      http://arxiv.org/abs/2304.02643
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib424">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Open 2023
    </span>
    <span class="ltx_bibblock">
     AI Open.
    </span>
    <span class="ltx_bibblock">
     GPT-4V(ision) system card, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf" target="_blank" title="">
      https://cdn.openai.com/papers/GPTV_System_Card.pdf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib425">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023f
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib425.1.1">
      arXiv [cs.CV]
     </em>
     , April 2023f.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.08485" target="_blank" title="">
      http://arxiv.org/abs/2304.08485
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib426">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang.
    </span>
    <span class="ltx_bibblock">
     BuboGPT: Enabling visual grounding in multi-modal LLMs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib426.1.1">
      arXiv [cs.CV]
     </em>
     , July 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.08581" target="_blank" title="">
      http://arxiv.org/abs/2307.08581
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib427">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lyu et al. 2023
    </span>
    <span class="ltx_bibblock">
     Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu.
    </span>
    <span class="ltx_bibblock">
     Macaw-LLM: Multi-modal language modeling with image, audio, video, and text integration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib427.1.1">
      arXiv [cs.CL]
     </em>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.09093" target="_blank" title="">
      http://arxiv.org/abs/2306.09093
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib428">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Xiaohua, Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao.
    </span>
    <span class="ltx_bibblock">
     Tool-LMM: A large multi-modal model for tool agent learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib428.1.1">
      arXiv [cs.CV]
     </em>
     , January 2024b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.10727" target="_blank" title="">
      http://arxiv.org/abs/2401.10727
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib429">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. 2023
    </span>
    <span class="ltx_bibblock">
     Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou.
    </span>
    <span class="ltx_bibblock">
     AssistGPT: A general multi-modal assistant that can plan, execute, inspect, and learn.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib429.1.1">
      arXiv [cs.CV]
     </em>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.08640" target="_blank" title="">
      http://arxiv.org/abs/2306.08640
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib430">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023g
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
    </span>
    <span class="ltx_bibblock">
     Voyager: An open-ended embodied agent with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib430.1.1">
      arXiv [cs.AI]
     </em>
     , May 2023g.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.16291" target="_blank" title="">
      http://arxiv.org/abs/2305.16291
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib431">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahn et al. 2022
    </span>
    <span class="ltx_bibblock">
     Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng.
    </span>
    <span class="ltx_bibblock">
     Do as I can, not as I say: Grounding language in robotic affordances.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib431.1.1">
      arXiv [cs.RO]
     </em>
     , April 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2204.01691" target="_blank" title="">
      http://arxiv.org/abs/2204.01691
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib432">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. 2021
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib432.1.1">
      arXiv [cs.LG]
     </em>
     , July 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2107.03374" target="_blank" title="">
      http://arxiv.org/abs/2107.03374
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib433">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun.
    </span>
    <span class="ltx_bibblock">
     Tool learning with foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib433.1.1">
      arXiv [cs.CL]
     </em>
     , April 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/BMTools" target="_blank" title="">
      https://github.com/OpenBMB/BMTools
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib434">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. 2021
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.
    </span>
    <span class="ltx_bibblock">
     WebGPT: Browser-assisted question-answering with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib434.1.1">
      arXiv [cs.CL]
     </em>
     , December 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2112.09332" target="_blank" title="">
      http://arxiv.org/abs/2112.09332
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib435">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. 2023
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib435.1.1">
      arXiv [cs.CL]
     </em>
     , February 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2302.04761" target="_blank" title="">
      http://arxiv.org/abs/2302.04761
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib436">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Parisi et al. 2022
    </span>
    <span class="ltx_bibblock">
     Aaron Parisi, Yao Zhao, and Noah Fiedel.
    </span>
    <span class="ltx_bibblock">
     TALM: Tool augmented language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib436.1.1">
      arXiv [cs.CL]
     </em>
     , May 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.12255" target="_blank" title="">
      http://arxiv.org/abs/2205.12255
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib437">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu.
    </span>
    <span class="ltx_bibblock">
     Toolink: Linking toolkit creation and using through chain-of-solving on open-source model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib437.1.1">
      arXiv [cs.CL]
     </em>
     , October 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.05155" target="_blank" title="">
      http://arxiv.org/abs/2310.05155
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib438">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Karpas et al. 2022
    </span>
    <span class="ltx_bibblock">
     Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tenenholtz.
    </span>
    <span class="ltx_bibblock">
     MRKL systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib438.1.1">
      arXiv [cs.CL]
     </em>
     , May 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.00445" target="_blank" title="">
      http://arxiv.org/abs/2205.00445
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib439">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen.
    </span>
    <span class="ltx_bibblock">
     ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib439.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14323" target="_blank" title="">
      http://arxiv.org/abs/2305.14323
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib440">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cai et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Large language models as tool makers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib440.1.1">
      arXiv [cs.LG]
     </em>
     , May 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.17126" target="_blank" title="">
      http://arxiv.org/abs/2305.17126
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib441">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. 2023c
    </span>
    <span class="ltx_bibblock">
     Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     CREATOR: Disentangling abstract and concrete reasonings of large language models through tool creation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib441.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023c.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14318" target="_blank" title="">
      http://arxiv.org/abs/2305.14318
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib442">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yuan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji.
    </span>
    <span class="ltx_bibblock">
     CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib442.1.1">
      arXiv [cs.CL]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.17428" target="_blank" title="">
      http://arxiv.org/abs/2309.17428
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib443">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     443
    </span>
    <span class="ltx_bibblock">
     Hilmy Abiyyu A.
    </span>
    <span class="ltx_bibblock">
     Flaticon.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.flaticon.com/authors/hilmy-abiyyu-a" target="_blank" title="">
      https://www.flaticon.com/authors/hilmy-abiyyu-a
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-5-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib444">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     444
    </span>
    <span class="ltx_bibblock">
     Laisa Islam Ani.
    </span>
    <span class="ltx_bibblock">
     Flaticon.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.flaticon.com/authors/laisa-islam-ani" target="_blank" title="">
      https://www.flaticon.com/authors/laisa-islam-ani
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-5-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib445">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     445
    </span>
    <span class="ltx_bibblock">
     Freepik.
    </span>
    <span class="ltx_bibblock">
     Flaticon.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.flaticon.com/authors/freepik" target="_blank" title="">
      https://www.flaticon.com/authors/freepik
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-5-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib446">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     446
    </span>
    <span class="ltx_bibblock">
     Kiranshastry.
    </span>
    <span class="ltx_bibblock">
     Flaticon.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.flaticon.com/authors/kiranshastry" target="_blank" title="">
      https://www.flaticon.com/authors/kiranshastry
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-5-1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib447">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chiang et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, and Janosh Riebesell.
    </span>
    <span class="ltx_bibblock">
     LLaMP: Large language model made powerful for high-fidelity materials knowledge retrieval and distillation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib447.1.1">
      arXiv [cs.CL]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.17244" target="_blank" title="">
      http://arxiv.org/abs/2401.17244
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib448">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ma et al. 2024a
    </span>
    <span class="ltx_bibblock">
     Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, and Wojciech Matusik.
    </span>
    <span class="ltx_bibblock">
     LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib448.1.1">
      arXiv [cs.LG]
     </em>
     , May 2024a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.09783" target="_blank" title="">
      http://arxiv.org/abs/2405.09783
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib449">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qu et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yuanhao Qu, Kaixuan Huang, Henry Cousins, William A Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong.
    </span>
    <span class="ltx_bibblock">
     CRISPR-GPT: An LLM agent for automated design of gene-editing experiments.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib449.1.1">
      bioRxiv
     </em>
     , April 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1101/2024.04.25.591003" target="_blank" title="">
      10.1101/2024.04.25.591003
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.48550/arXiv.2404.18021" target="_blank" title="">
      http://dx.doi.org/10.48550/arXiv.2404.18021
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib450">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, and Haohan Wang.
    </span>
    <span class="ltx_bibblock">
     Toward a team of AI-made scientists for scientific discovery from gene expression data.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib450.1.1">
      arXiv [q-bio.GN]
     </em>
     , February 2024b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.12391" target="_blank" title="">
      http://arxiv.org/abs/2402.12391
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib451">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sprueill et al. 2024
    </span>
    <span class="ltx_bibblock">
     Henry W Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, and Sutanay Choudhury.
    </span>
    <span class="ltx_bibblock">
     ChemReasoner: Heuristic search over a large language model’s knowledge space using quantum-chemical feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib451.1.1">
      arXiv [physics.chem-ph]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.10980" target="_blank" title="">
      http://arxiv.org/abs/2402.10980
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib452">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ma et al. 2024b
    </span>
    <span class="ltx_bibblock">
     Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, and Aixin Sun.
    </span>
    <span class="ltx_bibblock">
     SciAgent: Tool-augmented language models for scientific reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib452.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.11451" target="_blank" title="">
      http://arxiv.org/abs/2402.11451
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib453">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shao et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, and Monica S Lam.
    </span>
    <span class="ltx_bibblock">
     Assisting in writing wikipedia-like articles from scratch with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib453.1.1">
      arXiv [cs.CL]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.14207" target="_blank" title="">
      http://arxiv.org/abs/2402.14207
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib454">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Völker et al. 2024
    </span>
    <span class="ltx_bibblock">
     Christoph Völker, Tehseen Rug, Kevin Maik Jablonka, and Sabine Kruschwitz.
    </span>
    <span class="ltx_bibblock">
     LLMs can design sustainable concrete – a systematic benchmark.
    </span>
    <span class="ltx_bibblock">
     February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.researchsquare.com/article/rs-3913272/v1" target="_blank" title="">
      https://www.researchsquare.com/article/rs-3913272/v1
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib455">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ghafarollahi and Buehler 2024
    </span>
    <span class="ltx_bibblock">
     A Ghafarollahi and M J Buehler.
    </span>
    <span class="ltx_bibblock">
     ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib455.1.1">
      arXiv [cond-mat.soft]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.04268" target="_blank" title="">
      http://arxiv.org/abs/2402.04268
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib456">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lála et al. 2023
    </span>
    <span class="ltx_bibblock">
     Jakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and Andrew D White.
    </span>
    <span class="ltx_bibblock">
     PaperQA: Retrieval-augmented generative agent for scientific research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib456.1.1">
      arXiv [cs.CL]
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.07559" target="_blank" title="">
      http://arxiv.org/abs/2312.07559
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib457">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sam Cox, Michael Hammerling, Jakub Lála, Jon Laurent, Sam Rodriques, Matt Rubashkin, Andrew White 2023
    </span>
    <span class="ltx_bibblock">
     Sam Cox, Michael Hammerling, Jakub Lála, Jon Laurent, Sam Rodriques, Matt Rubashkin, Andrew White.
    </span>
    <span class="ltx_bibblock">
     WikiCrow: Automating synthesis of human scientific knowledge.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.futurehouse.org/wikicrow" target="_blank" title="">
      https://www.futurehouse.org/wikicrow
     </a>
     , 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.futurehouse.org/wikicrow" target="_blank" title="">
      https://www.futurehouse.org/wikicrow
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-2-15.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib458">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ansari and Moosavi 2023
    </span>
    <span class="ltx_bibblock">
     Mehrad Ansari and Seyed Mohamad Moosavi.
    </span>
    <span class="ltx_bibblock">
     Agent-based learning of materials datasets from scientific literature.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib458.1.1">
      arXiv [cs.AI]
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.11690" target="_blank" title="">
      http://arxiv.org/abs/2312.11690
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib459">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Prince et al. 2023
    </span>
    <span class="ltx_bibblock">
     Michael H Prince, Henry Chan, Aikaterini Vriza, Tao Zhou, Varuni K Sastry, Matthew T Dearing, Ross J Harder, Rama K Vasudevan, and Mathew J Cherukara.
    </span>
    <span class="ltx_bibblock">
     Opportunities for retrieval and tool augmented large language models in scientific facilities.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib459.1.1">
      arXiv [cs.CE]
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.01291" target="_blank" title="">
      http://arxiv.org/abs/2312.01291
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib460">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2023g
    </span>
    <span class="ltx_bibblock">
     Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang.
    </span>
    <span class="ltx_bibblock">
     CoQuest: Exploring research question co-creation with an LLM-based agent.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib460.1.1">
      arXiv [cs.HC]
     </em>
     , October 2023g.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.06155" target="_blank" title="">
      http://arxiv.org/abs/2310.06155
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib461">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     O’Donoghue et al. 2023
    </span>
    <span class="ltx_bibblock">
     Odhran O’Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, and Samuel G Rodriques.
    </span>
    <span class="ltx_bibblock">
     BioPlanner: Automatic evaluation of LLMs on protocol planning in biology.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib461.1.1">
      arXiv [cs.CL]
     </em>
     , October 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.10632" target="_blank" title="">
      http://arxiv.org/abs/2310.10632
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib462">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Janakarajan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nikita Janakarajan, Tim Erdmann, Sarath Swaminathan, Teodoro Laino, and Jannis Born.
    </span>
    <span class="ltx_bibblock">
     Language models in molecular discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib462.1.1">
      arXiv [physics.chem-ph]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.16235" target="_blank" title="">
      http://arxiv.org/abs/2309.16235
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib463">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kang and Kim 2023
    </span>
    <span class="ltx_bibblock">
     Yeonghun Kang and Jihan Kim.
    </span>
    <span class="ltx_bibblock">
     ChatMOF: An autonomous AI system for predicting and generating metal-organic frameworks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib463.1.1">
      arXiv [cs.CL]
     </em>
     , July 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.01423" target="_blank" title="">
      http://arxiv.org/abs/2308.01423
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib464">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mouriño et al. 2023
    </span>
    <span class="ltx_bibblock">
     Beatriz Mouriño, Elias Moubarak, Joren Van Herck, Sauradeep Majumdar, and Xiaoqi Zhang.
    </span>
    <span class="ltx_bibblock">
     i-digest: v1.0, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8080962" target="_blank" title="">
      https://zenodo.org/record/8080962
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib465">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rankovic et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bojana Rankovic, Andres M Bran, and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     BOLLaMa: BOLLaMA interface working with CHAOS, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8096827" target="_blank" title="">
      https://zenodo.org/record/8096827
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib466">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kruschwitz et al. 2023
    </span>
    <span class="ltx_bibblock">
     Sabine Kruschwitz, Christoph Völker, and Ghezal Ahmad Zia.
    </span>
    <span class="ltx_bibblock">
     Text2Concrete, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8091195" target="_blank" title="">
      https://zenodo.org/record/8091195
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib467">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ramos et al. 2023a
    </span>
    <span class="ltx_bibblock">
     Mayk Caldas Ramos, Sam Cox, and Andrew White.
    </span>
    <span class="ltx_bibblock">
     MAPI_LLM: MAPI_LLM first release, 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8097336" target="_blank" title="">
      https://zenodo.org/record/8097336
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib468">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ramos et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White.
    </span>
    <span class="ltx_bibblock">
     Bayesian optimization of catalysts with in-context learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib468.1.1">
      arXiv [physics.chem-ph]
     </em>
     , April 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2304.05341" target="_blank" title="">
      http://arxiv.org/abs/2304.05341
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib469">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hocky and White 2022
    </span>
    <span class="ltx_bibblock">
     Glen M Hocky and Andrew D White.
    </span>
    <span class="ltx_bibblock">
     Natural language processing models that automate programming will transform chemistry research and teaching.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib469.1.1">
      Digit. Discov.
     </em>
     , 1(2):79–83, April 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/d1dd00009h" target="_blank" title="">
      10.1039/d1dd00009h
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2022/dd/d1dd00009h" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2022/dd/d1dd00009h
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib470">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     White et al. 2023
    </span>
    <span class="ltx_bibblock">
     Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J Peña Ccoa.
    </span>
    <span class="ltx_bibblock">
     Assessment of chemistry knowledge in large language models that generate code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib470.1.1">
      Digital Discovery
     </em>
     , 2(2):368–376, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D2DD00087C" target="_blank" title="">
      10.1039/D2DD00087C
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00087c" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00087c
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib471">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kristiadi et al. 2024
    </span>
    <span class="ltx_bibblock">
     Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, and Geoff Pleiss.
    </span>
    <span class="ltx_bibblock">
     A sober look at LLMs for material discovery: Are they actually good for bayesian optimization over molecules?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib471.1.1">
      arXiv [cs.LG]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.05015" target="_blank" title="">
      http://arxiv.org/abs/2402.05015
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib472">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ranković and Schwaller 2023
    </span>
    <span class="ltx_bibblock">
     Bojana Ranković and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     BoChemian: Large language model embeddings for bayesian optimization of chemical reactions.
    </span>
    <span class="ltx_bibblock">
     December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=A1RVn1m3J3" target="_blank" title="">
      https://openreview.net/pdf?id=A1RVn1m3J3
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib473">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jablonka et al. 2023
    </span>
    <span class="ltx_bibblock">
     K Jablonka, Qianxiang Ai, Alexander H Al-Feghali, S Badhwar, Joshua D Bocarsly Andres Bran, S Bringuier, L Brinson, K Choudhary, Defne Circi, Sam Cox, W D Jong, Matthew L Evans, Nicolas Gastellu, Jerome Genzling, M Gil, Ankur Gupta, Zhi Hong, A Imran, S Kruschwitz, A Labarre, Jakub L’ala, Tao Liu, Steven Ma, Sauradeep Majumdar, G Merz, N Moitessier, E Moubarak, B Mouriño, Brenden G Pelkie, M Pieler, M C Ramos, Bojana Rankovi’c, Samuel G Rodriques, J N Sanders, P Schwaller, Marcus Schwarting, Jia-Xin Shi, B Smit, Benn Smith, J V Heck, C Volker, Logan T Ward, S Warren, B Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Jan Zia, A Scourtas, K Schmidt, Ian T Foster, Andrew D White, and B Blaiszik.
    </span>
    <span class="ltx_bibblock">
     14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib473.1.1">
      Digit. Discov.
     </em>
     , 2(5):1233–1250, June 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/D3DD00113J" target="_blank" title="">
      10.1039/D3DD00113J
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2023/dd/d3dd00113j" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2023/dd/d3dd00113j
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib474">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ranković et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bojana Ranković, Ryan-Rhys Griffiths, Henry B Moss, and Philippe Schwaller.
    </span>
    <span class="ltx_bibblock">
     Bayesian optimisation for additive screening and yield improvements in chemical reactions – beyond one-hot encoding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib474.1.1">
      ChemRxiv
     </em>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2022-nll2j-v3" target="_blank" title="">
      10.26434/chemrxiv-2022-nll2j-v3
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/6489f95c4f8b1884b74b69c8" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/6489f95c4f8b1884b74b69c8
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib475">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weiser et al. 2023
    </span>
    <span class="ltx_bibblock">
     Benjamin Weiser, Jerome Genzling, Nicolas Gastellu, Sylvester Zhang, Tao Liu, Alexander Al-Feghali, Nicolas Moitessier, Anne Labarre, and Steven Ma.
    </span>
    <span class="ltx_bibblock">
     LLM-Guided-GA: LLM-Guided-GA digital discovery release, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8125541" target="_blank" title="">
      https://zenodo.org/record/8125541
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib476">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Circi and Badhwar 2023
    </span>
    <span class="ltx_bibblock">
     Defne Circi and Shruti Badhwar.
    </span>
    <span class="ltx_bibblock">
     InsightGraph: InsightGraph, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/8092575" target="_blank" title="">
      https://zenodo.org/record/8092575
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib477">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zaabi et al. 2023
    </span>
    <span class="ltx_bibblock">
     Marwa Zaabi, Walid Hariri, and Nadia Smaoui.
    </span>
    <span class="ltx_bibblock">
     A review study of ChatGPT applications in education.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib477.1.1">
      2023 International Conference on Innovations in Intelligent Systems and Applications (INISTA)
     </em>
     , pages 1–5. IEEE, September 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/inista59065.2023.10310439" target="_blank" title="">
      10.1109/inista59065.2023.10310439
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/INISTA59065.2023.10310439" target="_blank" title="">
      http://dx.doi.org/10.1109/INISTA59065.2023.10310439
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib478">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kasneci et al. 2023
    </span>
    <span class="ltx_bibblock">
     Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci.
    </span>
    <span class="ltx_bibblock">
     ChatGPT for good? on opportunities and challenges of large language models for education.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib478.1.1">
      Learn. Individ. Differ.
     </em>
     , 103(102274):102274, April 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1041-6080, 1873-3425.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.lindif.2023.102274" target="_blank" title="">
      10.1016/j.lindif.2023.102274
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1041608023000195" target="_blank" title="">
      https://www.sciencedirect.com/science/article/pii/S1041608023000195
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib479">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hellas et al. 2023
    </span>
    <span class="ltx_bibblock">
     Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanpaa, and Juha Sorva.
    </span>
    <span class="ltx_bibblock">
     Exploring the responses of large language models to beginner programmers’ help requests.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib479.1.1">
      arXiv [cs.CY]
     </em>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.05715" target="_blank" title="">
      http://arxiv.org/abs/2306.05715
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib480">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, Aimin Zhou, Ze Zhou, Qin Chen, Jie Zhou, Liang He, and Xipeng Qiu.
    </span>
    <span class="ltx_bibblock">
     EduChat: A large-scale language model-based chatbot system for intelligent education.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib480.1.1">
      arXiv [cs.CL]
     </em>
     , August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.02773" target="_blank" title="">
      http://arxiv.org/abs/2308.02773
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib481">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tian et al. 2024
    </span>
    <span class="ltx_bibblock">
     Jie Tian, Jixin Hou, Zihao Wu, Peng Shu, Zhengliang Liu, Yujie Xiang, Beikang Gu, Nicholas Filla, Yiwei Li, Ning Liu, Xianyan Chen, Keke Tang, Tianming Liu, and Xianqiao Wang.
    </span>
    <span class="ltx_bibblock">
     Assessing large language models in mechanical engineering education: A study on mechanics-focused conceptual understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib481.1.1">
      arXiv [cs.CL]
     </em>
     , January 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2401.12983" target="_blank" title="">
      http://arxiv.org/abs/2401.12983
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib482">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. 2022
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
    </span>
    <span class="ltx_bibblock">
     Robust speech recognition via large-scale weak supervision.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib482.1.1">
      arXiv [eess.AS]
     </em>
     , December 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2212.04356" target="_blank" title="">
      http://arxiv.org/abs/2212.04356
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib483">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewis et al. 2020
    </span>
    <span class="ltx_bibblock">
     Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-Tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela.
    </span>
    <span class="ltx_bibblock">
     Retrieval-augmented generation for knowledge-intensive NLP tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib483.1.1">
      arXiv [cs.CL]
     </em>
     , May 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2005.11401" target="_blank" title="">
      http://arxiv.org/abs/2005.11401
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib484">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, Qun Fang, Pheng Ann Heng, and Guangyong Chen.
    </span>
    <span class="ltx_bibblock">
     Chemist-X: Large language model-empowered agent for reaction condition recommendation in chemical synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib484.1.1">
      arXiv [cs.IR]
     </em>
     , November 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.10776" target="_blank" title="">
      http://arxiv.org/abs/2311.10776
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib485">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ingraham et al. 2023
    </span>
    <span class="ltx_bibblock">
     John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, Shan Tie, Vincent Xue, Sarah C Cowles, Alan Leung, João V Rodrigues, Claudio L Morales-Perez, Alex M Ayoub, Robin Green, Katherine Puentes, Frank Oplinger, Nishant V Panwar, Fritz Obermeyer, Adam R Root, Andrew L Beam, Frank J Poelwijk, and Gevorg Grigoryan.
    </span>
    <span class="ltx_bibblock">
     Illuminating protein space with a programmable generative model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib485.1.1">
      Nature
     </em>
     , 623(7989):1070–1078, November 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0028-0836,1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-023-06728-8" target="_blank" title="">
      10.1038/s41586-023-06728-8
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41586-023-06728-8" target="_blank" title="">
      http://dx.doi.org/10.1038/s41586-023-06728-8
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib486">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. 2024
    </span>
    <span class="ltx_bibblock">
     Kevin E Wu, Kevin K Yang, Rianne van den Berg, Sarah Alamdari, James Y Zou, Alex X Lu, and Ava P Amini.
    </span>
    <span class="ltx_bibblock">
     Protein structure generation via folding diffusion.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib486.1.1">
      Nat. Commun.
     </em>
     , 15(1):1059, February 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2041-1723.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41467-024-45051-2" target="_blank" title="">
      10.1038/s41467-024-45051-2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41467-024-45051-2" target="_blank" title="">
      http://dx.doi.org/10.1038/s41467-024-45051-2
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib487">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tang et al. 2024
    </span>
    <span class="ltx_bibblock">
     Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, and Mark Gerstein.
    </span>
    <span class="ltx_bibblock">
     Prioritizing safeguarding over autonomy: Risks of LLM agents for science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib487.1.1">
      arXiv [cs.CY]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.04247" target="_blank" title="">
      http://arxiv.org/abs/2402.04247
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib488">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ruan et al. 2024
    </span>
    <span class="ltx_bibblock">
     Yixiang Ruan, Chenyin Lu, Ning Xu, Jian Zhang, Jun Xuan, Jianzhang Pan, Qun Fang, Hanyu Gao, Xiaodong Shen, Ning Ye, Qiang Zhang, and Yiming Mo.
    </span>
    <span class="ltx_bibblock">
     Accelerated end-to-end chemical synthesis development with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib488.1.1">
      ChemRxiv
     </em>
     , May 2024.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2024-6wmg4" target="_blank" title="">
      10.26434/chemrxiv-2024-6wmg4
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/6634f02021291e5d1d58702c" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/6634f02021291e5d1d58702c
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib489">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023d
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T Chayes, and Omar M Yaghi.
    </span>
    <span class="ltx_bibblock">
     ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib489.1.1">
      J. Am. Chem. Soc.
     </em>
     , 145(Copyright © 2024 American Chemical Society (ACS). All Rights Reserved.; Copyright © 2024 U.S. National Library of Medicine.):18048–18062, 2023d.
    </span>
    <span class="ltx_bibblock">
     ISSN 0002-7863,1520-5126.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/jacs.3c05819" target="_blank" title="">
      10.1021/jacs.3c05819
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1021/jacs.3c05819" target="_blank" title="">
      http://dx.doi.org/10.1021/jacs.3c05819
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib490">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. 2024d
    </span>
    <span class="ltx_bibblock">
     Wei Zhang, Qinggong Wang, Xiangtai Kong, Jiacheng Xiong, Shengkun Ni, Duanhua Cao, Buying Niu, Mingan Chen, Runze Zhang, Yitian Wang, Lehan Zhang, Xutong Li, Zhaoping Xiong, Qian Shi, Ziming Huang, Zunyun Fu, and Mingyue Zheng.
    </span>
    <span class="ltx_bibblock">
     Fine-tuning large language models for chemical text mining.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib490.1.1">
      ChemRxiv
     </em>
     , February 2024d.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2023-k7ct5-v2" target="_blank" title="">
      10.26434/chemrxiv-2023-k7ct5-v2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/65baa07b9138d2316124f224" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/65baa07b9138d2316124f224
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib491">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2024
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Zhiguo He, Omar Khattab, Nakul Rampal, Matei A Zaharia, Christian Borgs, Jennifer T Chayes, and Omar M Yaghi.
    </span>
    <span class="ltx_bibblock">
     Image and data mining in reticular chemistry powered by GPT-4V.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib491.1.1">
      Digit. Discov.
     </em>
     , 3(3):491–501, 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 2635-098X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1039/d3dd00239j" target="_blank" title="">
      10.1039/d3dd00239j
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00239j" target="_blank" title="">
      https://pubs.rsc.org/en/content/articlelanding/2024/dd/d3dd00239j
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib492">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023e
    </span>
    <span class="ltx_bibblock">
     Zhiling Zheng, Oufan Zhang, Ha L Nguyen, Nakul Rampal, Ali H Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer T Chayes, and Omar M Yaghi.
    </span>
    <span class="ltx_bibblock">
     ChatGPT research group for optimizing the crystallinity of MOFs and COFs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib492.1.1">
      ACS Cent. Sci.
     </em>
     , 9(11):2161–2170, November 2023e.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-7943,2374-7951.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acscentsci.3c01087" target="_blank" title="">
      10.1021/acscentsci.3c01087
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/full/10.1021/acscentsci.3c01087" target="_blank" title="">
      https://pubs.acs.org/doi/full/10.1021/acscentsci.3c01087
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib493">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nascimento et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nathalia Nascimento, Paulo Alencar, and Donald Cowan.
    </span>
    <span class="ltx_bibblock">
     Self-adaptive large language model (LLM)-based multiagent systems.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib493.1.1">
      2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)
     </em>
     , pages 104–109. IEEE, September 2023.
    </span>
    <span class="ltx_bibblock">
     ISBN 9798350337464.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ACSOS-C58168.2023.00048" target="_blank" title="">
      10.1109/ACSOS-C58168.2023.00048
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10336211" target="_blank" title="">
      https://ieeexplore.ieee.org/document/10336211
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib494">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Niazi and Mariam 2023
    </span>
    <span class="ltx_bibblock">
     Sarfaraz K Niazi and Zamara Mariam.
    </span>
    <span class="ltx_bibblock">
     Recent advances in Machine-Learning-Based chemoinformatics: A comprehensive review.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib494.1.1">
      Int. J. Mol. Sci.
     </em>
     , 24(14), July 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1422-0067.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.3390/ijms241411488" target="_blank" title="">
      10.3390/ijms241411488
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.3390/ijms241411488" target="_blank" title="">
      http://dx.doi.org/10.3390/ijms241411488
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib495">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     McNaughton et al. 2024
    </span>
    <span class="ltx_bibblock">
     Andrew D McNaughton, Gautham Ramalaxmi, Agustin Kruel, Carter R Knutson, Rohith A Varikoti, and Neeraj Kumar.
    </span>
    <span class="ltx_bibblock">
     CACTUS: Chemistry agent connecting tool-usage to science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib495.1.1">
      arXiv [cs.CL]
     </em>
     , May 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.00972" target="_blank" title="">
      http://arxiv.org/abs/2405.00972
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib496">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kang et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Yeonghun Kang, Hyunsoo Park, Berend Smit, and Jihan Kim.
    </span>
    <span class="ltx_bibblock">
     A multi-modal pre-training transformer for universal transfer learning in metal–organic frameworks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib496.1.1">
      Nature Machine Intelligence
     </em>
     , 5(3):309–318, March 2023b.
    </span>
    <span class="ltx_bibblock">
     ISSN 2522-5839, 2522-5839.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s42256-023-00628-2" target="_blank" title="">
      10.1038/s42256-023-00628-2
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s42256-023-00628-2" target="_blank" title="">
      https://www.nature.com/articles/s42256-023-00628-2
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib497">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Manica et al. 2023
    </span>
    <span class="ltx_bibblock">
     Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan Nana Teukam, Giorgio Giannone, Samuel C Hoffman, Matthew Buchan, Vijil Chenthamarakshan, Timothy Donovan, Hsiang Han Hsu, Federico Zipoli, Oliver Schilter, Akihiro Kishimoto, Lisa Hamada, Inkit Padhi, Karl Wehden, Lauren McHugh, Alexy Khrabrov, Payel Das, Seiji Takeda, and John R Smith.
    </span>
    <span class="ltx_bibblock">
     Accelerating material design with the generative toolkit for scientific discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib497.1.1">
      npj Computational Materials
     </em>
     , 9(1):1–6, May 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 2057-3960, 2057-3960.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41524-023-01028-1" target="_blank" title="">
      10.1038/s41524-023-01028-1
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41524-023-01028-1" target="_blank" title="">
      https://www.nature.com/articles/s41524-023-01028-1
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib498">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     498
    </span>
    <span class="ltx_bibblock">
     rxn4chemistry: Python wrapper for the IBM RXN for chemistry API.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/rxn4chemistry/rxn4chemistry" target="_blank" title="">
      https://github.com/rxn4chemistry/rxn4chemistry
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib499">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gaiński et al. 2022
    </span>
    <span class="ltx_bibblock">
     Piotr Gaiński, Lukasz Maziarka, Tomasz Danel, and Stanislaw Jastrzebski.
    </span>
    <span class="ltx_bibblock">
     HuggingMolecules: An Open-Source library for Transformer-Based molecular property prediction (student abstract).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib499.1.1">
      AAAI
     </em>
     , 36(11):12949–12950, June 2022.
    </span>
    <span class="ltx_bibblock">
     ISSN 2374-3468, 2374-3468.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v36i11.21611" target="_blank" title="">
      10.1609/aaai.v36i11.21611
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/21611" target="_blank" title="">
      https://ojs.aaai.org/index.php/AAAI/article/view/21611
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib500">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Landrum 2013
    </span>
    <span class="ltx_bibblock">
     Greg Landrum.
    </span>
    <span class="ltx_bibblock">
     Rdkit documentation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib500.1.1">
      Release
     </em>
     , 1(1-79):4, 2013.
    </span>
    <span class="ltx_bibblock">
     ISSN 1047-935X, 1533-3752.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://media.readthedocs.org/pdf/rdkit/latest/rdkit.pdf" target="_blank" title="">
      https://media.readthedocs.org/pdf/rdkit/latest/rdkit.pdf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib501">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. 2023f
    </span>
    <span class="ltx_bibblock">
     Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T N Nguyen, Lauren T May, Geoffrey I Webb, and Shirui Pan.
    </span>
    <span class="ltx_bibblock">
     Large language models for scientific synthesis, inference and explanation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib501.1.1">
      arXiv [cs.AI]
     </em>
     , October 2023f.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.07984" target="_blank" title="">
      http://arxiv.org/abs/2310.07984
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib502">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. 2023h
    </span>
    <span class="ltx_bibblock">
     Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope.
    </span>
    <span class="ltx_bibblock">
     SciMON: Scientific inspiration machines optimized for novelty.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib502.1.1">
      arXiv [cs.CL]
     </em>
     , May 2023h.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.14259" target="_blank" title="">
      http://arxiv.org/abs/2305.14259
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib503">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gu and Krenn 2024
    </span>
    <span class="ltx_bibblock">
     Xuemei Gu and Mario Krenn.
    </span>
    <span class="ltx_bibblock">
     Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib503.1.1">
      arXiv [cs.AI]
     </em>
     , May 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.17044" target="_blank" title="">
      http://arxiv.org/abs/2405.17044
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib504">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sprueill et al. 2023
    </span>
    <span class="ltx_bibblock">
     Henry W Sprueill, Carl Edwards, Mariefel V Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury.
    </span>
    <span class="ltx_bibblock">
     Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib504.1.1">
      arXiv [cs.AI]
     </em>
     , October 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.14420" target="_blank" title="">
      http://arxiv.org/abs/2310.14420
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib505">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. 2023
    </span>
    <span class="ltx_bibblock">
     Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke.
    </span>
    <span class="ltx_bibblock">
     Uni-Mol: A universal 3D molecular representation learning framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib505.1.1">
      ChemRxiv
     </em>
     , March 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.26434/chemrxiv-2022-jjm0j-v4" target="_blank" title="">
      10.26434/chemrxiv-2022-jjm0j-v4
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581" target="_blank" title="">
      https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib506">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI et al. 2023
    </span>
    <span class="ltx_bibblock">
     OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford,
Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez,
Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri
Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C J Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,
Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.
    </span>
    <span class="ltx_bibblock">
     GPT-4 technical report.
    </span>
    <span class="ltx_bibblock">
     March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" target="_blank" title="">
      http://arxiv.org/abs/2303.08774
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib507">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao and Coley 2020
    </span>
    <span class="ltx_bibblock">
     Wenhao Gao and Connor W Coley.
    </span>
    <span class="ltx_bibblock">
     The synthesizability of molecules proposed by generative models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib507.1.1">
      J. Chem. Inf. Model.
     </em>
     , (12):5714–5723, February 2020.
    </span>
    <span class="ltx_bibblock">
     ISSN 1549-9596,1549-960X.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1021/acs.jcim.0c00174" target="_blank" title="">
      10.1021/acs.jcim.0c00174
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubs.acs.org/doi/10.1021/acs.jcim.0c00174" target="_blank" title="">
      https://pubs.acs.org/doi/10.1021/acs.jcim.0c00174
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib508">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. 2024c
    </span>
    <span class="ltx_bibblock">
     Pengfei Liu, Jun Tao, and Zhixiang Ren.
    </span>
    <span class="ltx_bibblock">
     Scientific language modeling: A quantitative review of large language models in molecular science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib508.1.1">
      arXiv [cs.LG]
     </em>
     , February 2024c.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.04119" target="_blank" title="">
      http://arxiv.org/abs/2402.04119
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib509">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schwaller et al. 2021c
    </span>
    <span class="ltx_bibblock">
     Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Extraction of organic chemistry grammar from unsupervised learning of chemical reactions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib509.1.1">
      Sci. Adv.
     </em>
     , 7(15):eabe4166, April 2021c.
    </span>
    <span class="ltx_bibblock">
     ISSN 2375-2548.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1126/sciadv.abe4166" target="_blank" title="">
      10.1126/sciadv.abe4166
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1126/sciadv.abe4166" target="_blank" title="">
      http://dx.doi.org/10.1126/sciadv.abe4166
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib510">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schilter et al. 2023
    </span>
    <span class="ltx_bibblock">
     Oliver Schilter, Marvin Alberts, Federico Zipoli, Alain C Vaucher, Philippe Schwaller, and Teodoro Laino.
    </span>
    <span class="ltx_bibblock">
     Unveiling the secrets of
     <sup class="ltx_sup" id="bib.bib510.2.1">
      1
     </sup>
     H-NMR spectroscopy: A novel approach utilizing attention mechanisms.
    </span>
    <span class="ltx_bibblock">
     November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/pdf?id=4ilKwquW51" target="_blank" title="">
      https://openreview.net/pdf?id=4ilKwquW51
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib511">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Aryal et al. 2024
    </span>
    <span class="ltx_bibblock">
     Shiva Aryal, Tuyen Do, Bisesh Heyojoo, Sandeep Chataut, Bichar Dip Shrestha Gurung, Venkataramana Gadhamshetty, and Etienne Gnimpieba.
    </span>
    <span class="ltx_bibblock">
     Leveraging multi-AI agents for cross-domain knowledge discovery.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib511.1.1">
      arXiv [cs.AI]
     </em>
     , April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.08511" target="_blank" title="">
      http://arxiv.org/abs/2404.08511
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib512">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bohm et al. 2019
    </span>
    <span class="ltx_bibblock">
     Florian Bohm, Yang Gao, Christian M Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych.
    </span>
    <span class="ltx_bibblock">
     Better rewards yield better summaries: Learning to summarise without references.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib512.1.1">
      arXiv [cs.CL]
     </em>
     , September 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.01214" target="_blank" title="">
      http://arxiv.org/abs/1909.01214
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib513">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Sarah Pan, Vladislav Lialin, Sherin Muckatira, and Anna Rumshisky.
    </span>
    <span class="ltx_bibblock">
     Let’s reinforce step by step.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib513.1.1">
      arXiv [cs.CL]
     </em>
     , November 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.05821" target="_blank" title="">
      http://arxiv.org/abs/2311.05821
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib514">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. 2023
    </span>
    <span class="ltx_bibblock">
     Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu.
    </span>
    <span class="ltx_bibblock">
     Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib514.1.1">
      arXiv [cs.AI]
     </em>
     , June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.03604" target="_blank" title="">
      http://arxiv.org/abs/2306.03604
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib515">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu.
    </span>
    <span class="ltx_bibblock">
     Language agents with reinforcement learning for strategic play in the werewolf game.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib515.1.1">
      arXiv [cs.AI]
     </em>
     , October 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.18940" target="_blank" title="">
      http://arxiv.org/abs/2310.18940
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib516">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Langley et al. 2009
    </span>
    <span class="ltx_bibblock">
     Pat Langley, John E Laird, and Seth Rogers.
    </span>
    <span class="ltx_bibblock">
     Cognitive architectures: Research issues and challenges.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib516.1.1">
      Cogn. Syst. Res.
     </em>
     , 10(2):141–160, June 2009.
    </span>
    <span class="ltx_bibblock">
     ISSN 1389-0417,2214-4366.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.cogsys.2006.07.004" target="_blank" title="">
      10.1016/j.cogsys.2006.07.004
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.cogsys.2006.07.004" target="_blank" title="">
      http://dx.doi.org/10.1016/j.cogsys.2006.07.004
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib517">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goertzel 2014
    </span>
    <span class="ltx_bibblock">
     Ben Goertzel.
    </span>
    <span class="ltx_bibblock">
     Artificial general intelligence: Concept, state of the art, and future prospects.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib517.1.1">
      J. Artif. Gen. Intell.
     </em>
     , 5(1):1–48, December 2014.
    </span>
    <span class="ltx_bibblock">
     ISSN 1946-0163.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.2478/jagi-2014-0001" target="_blank" title="">
      10.2478/jagi-2014-0001
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sciendo.com/pdf/10.2478/jagi-2014-0001" target="_blank" title="">
      https://sciendo.com/pdf/10.2478/jagi-2014-0001
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib518">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ruan et al. 2023
    </span>
    <span class="ltx_bibblock">
     Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, and Tatsunori Hashimoto.
    </span>
    <span class="ltx_bibblock">
     Identifying the risks of LM agents with an LM-emulated sandbox.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib518.1.1">
      arXiv [cs.AI]
     </em>
     , September 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.15817" target="_blank" title="">
      http://arxiv.org/abs/2309.15817
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib519">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ji et al. 2023
    </span>
    <span class="ltx_bibblock">
     Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.
    </span>
    <span class="ltx_bibblock">
     Survey of hallucination in natural language generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib519.1.1">
      ACM Comput. Surv.
     </em>
     , 55(12):1–38, March 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 0360-0300.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3571730" target="_blank" title="">
      10.1145/3571730
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3571730" target="_blank" title="">
      https://doi.org/10.1145/3571730
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib520">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cai et al. 2023b
    </span>
    <span class="ltx_bibblock">
     Zefan Cai, Baobao Chang, and Wenjuan Han.
    </span>
    <span class="ltx_bibblock">
     Human-in-the-loop through chain-of-thought.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib520.1.1">
      arXiv [cs.CL]
     </em>
     , June 2023b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.07932" target="_blank" title="">
      http://arxiv.org/abs/2306.07932
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib521">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiao and Wang 2023
    </span>
    <span class="ltx_bibblock">
     Hengjia Xiao and Peng Wang.
    </span>
    <span class="ltx_bibblock">
     LLM a*: Human in the loop large language models enabled a* search for robotics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib521.1.1">
      arXiv [cs.RO]
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2312.01797" target="_blank" title="">
      http://arxiv.org/abs/2312.01797
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib522">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Drori and Te’eni 2024
    </span>
    <span class="ltx_bibblock">
     Iddo Drori and Dov Te’eni.
    </span>
    <span class="ltx_bibblock">
     Human-in-the-Loop AI reviewing: Feasibility, opportunities, and risks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib522.1.1">
      Journal of the Association for Information Systems
     </em>
     , 25(1):98–109, 2024.
    </span>
    <span class="ltx_bibblock">
     ISSN 1536-9323.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.17705/1jais.00867" target="_blank" title="">
      10.17705/1jais.00867
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aisel.aisnet.org/jais/vol25/iss1/7/" target="_blank" title="">
      https://aisel.aisnet.org/jais/vol25/iss1/7/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib523">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Szymanski et al. 2023
    </span>
    <span class="ltx_bibblock">
     Nathan J. Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E. Kumar, Tanjin He, David Milsted, Matthew J. McDermott, Max Gallant, Ekin Dogus Cubuk, Amil Merchant, Haegyeom Kim, Anubhav Jain, Christopher J. Bartel, Kristin Persson, Yan Zeng, and Gerbrand Ceder.
    </span>
    <span class="ltx_bibblock">
     An autonomous laboratory for the accelerated synthesis of novel materials.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib523.1.1">
      Nature
     </em>
     , 624(7990):86–91, December 2023.
    </span>
    <span class="ltx_bibblock">
     ISSN 1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-023-06734-w" target="_blank" title="">
      10.1038/s41586-023-06734-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41586-023-06734-w" target="_blank" title="">
      https://www.nature.com/articles/s41586-023-06734-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: Nature Publishing Group.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib524">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peplow 2023
    </span>
    <span class="ltx_bibblock">
     Mark Peplow.
    </span>
    <span class="ltx_bibblock">
     Robot chemist sparks row with claim it created new materials.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib524.1.1">
      Nature
     </em>
     , December 2023.
    </span>
    <span class="ltx_bibblock">
     doi:
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/d41586-023-03956-w" target="_blank" title="">
      10.1038/d41586-023-03956-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/d41586-023-03956-w" target="_blank" title="">
      https://www.nature.com/articles/d41586-023-03956-w
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Bandiera_abtest: a Cg_type: News Publisher: Nature Publishing Group Subject_term: Scientific community, Materials science, Machine learning, Chemistry.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib525">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. 2024
    </span>
    <span class="ltx_bibblock">
     Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu.
    </span>
    <span class="ltx_bibblock">
     Data interpreter: An LLM agent for data science.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib525.1.1">
      arXiv [cs.AI]
     </em>
     , February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.18679" target="_blank" title="">
      http://arxiv.org/abs/2402.18679
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib526">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qi and Wang 2024
    </span>
    <span class="ltx_bibblock">
     Danrui Qi and Jiannan Wang.
    </span>
    <span class="ltx_bibblock">
     CleanAgent: Automating data standardization with LLM-based agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib526.1.1">
      arXiv [cs.LG]
     </em>
     , March 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.08291" target="_blank" title="">
      http://arxiv.org/abs/2403.08291
     </a>
     .
    </span>
   </li>
  </ul>
 </section>
 <div about="" class="ltx_rdf" content="Caldas Ramos, Mayk and Collison, Christopher J and White, Andrew D" property="dcterms:creator">
 </div>
 <div about="" class="ltx_rdf" content="Large Language Model, LLM, LLM agent, agent, science, chemistry" property="dcterms:subject">
 </div>
 <div about="" class="ltx_rdf" property="dcterms:subject">
 </div>
 <div about="" class="ltx_rdf" content="A Review of Large Language Models and Autonomous Agents in Chemistry" property="dcterms:title">
 </div>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
</article>
