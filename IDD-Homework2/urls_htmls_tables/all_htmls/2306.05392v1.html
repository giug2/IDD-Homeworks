<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.05392] Modular Visual Question Answering via Code Generation</title><meta property="og:description" content="We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-traine…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Modular Visual Question Answering via Code Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Modular Visual Question Answering via Code Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.05392">

<!--Generated on Thu Feb 29 01:26:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Modular Visual Question Answering via Code Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id12.12.id1" class="ltx_text"></span> <span id="id9.9.9" class="ltx_text">
<span id="id9.9.9.9" class="ltx_tabular ltx_align_middle">
<span id="id3.3.3.3.3" class="ltx_tr">
<span id="id3.3.3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center">Sanjay Subramanian<sup id="id3.3.3.3.3.3.1" class="ltx_sup"><span id="id3.3.3.3.3.3.1.1" class="ltx_text ltx_font_italic">1</span></sup>         Medhini Narasimhan<sup id="id3.3.3.3.3.3.2" class="ltx_sup"><span id="id3.3.3.3.3.3.2.1" class="ltx_text ltx_font_italic">1</span></sup>         Kushal Khangaonkar<sup id="id3.3.3.3.3.3.3" class="ltx_sup"><span id="id3.3.3.3.3.3.3.1" class="ltx_text ltx_font_italic">1</span></sup></span></span>
<span id="id7.7.7.7.7" class="ltx_tr">
<span id="id7.7.7.7.7.4" class="ltx_td ltx_nopad_r ltx_align_center">Kevin Yang<sup id="id7.7.7.7.7.4.1" class="ltx_sup"><span id="id7.7.7.7.7.4.1.1" class="ltx_text ltx_font_italic">1</span></sup>       Arsha Nagrani<sup id="id7.7.7.7.7.4.2" class="ltx_sup"><span id="id7.7.7.7.7.4.2.1" class="ltx_text ltx_font_italic">2</span></sup>       Cordelia Schmid<sup id="id7.7.7.7.7.4.3" class="ltx_sup"><span id="id7.7.7.7.7.4.3.1" class="ltx_text ltx_font_italic">2</span></sup>       Andy Zeng<sup id="id7.7.7.7.7.4.4" class="ltx_sup"><span id="id7.7.7.7.7.4.4.1" class="ltx_text ltx_font_italic">2</span></sup></span></span>
<span id="id9.9.9.9.9" class="ltx_tr">
<span id="id9.9.9.9.9.2" class="ltx_td ltx_nopad_r ltx_align_center">Trevor Darrell<sup id="id9.9.9.9.9.2.1" class="ltx_sup"><span id="id9.9.9.9.9.2.1.1" class="ltx_text ltx_font_italic">1</span></sup>       Dan Klein<sup id="id9.9.9.9.9.2.2" class="ltx_sup"><span id="id9.9.9.9.9.2.2.1" class="ltx_text ltx_font_italic">1</span></sup></span></span>
</span></span> <span id="id13.13.id2" class="ltx_text"></span> 
<br class="ltx_break"><sup id="id14.14.id3" class="ltx_sup"><span id="id14.14.id3.1" class="ltx_text ltx_font_italic">1</span></sup>UC Berkeley     
<sup id="id15.15.id4" class="ltx_sup"><span id="id15.15.id4.1" class="ltx_text ltx_font_italic">2</span></sup>Google Research      
<br class="ltx_break"><span id="id16.16.id5" class="ltx_text"></span><span id="id17.17.id6" class="ltx_text ltx_font_typewriter"> <span id="id17.17.id6.1" class="ltx_text">
<span id="id17.17.id6.1.1" class="ltx_tabular ltx_align_middle">
<span id="id17.17.id6.1.1.1" class="ltx_tr">
<span id="id17.17.id6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">{sanjayss,medhini,kushaltk,yangk,trevordarrell,klein}@berkeley.edu,</span></span>
<span id="id17.17.id6.1.1.2" class="ltx_tr">
<span id="id17.17.id6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">{anagrani,cordelias,andyzeng}@google.com</span></span>
</span></span> <span id="id17.17.id6.2" class="ltx_text"></span></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The scope of reasoning needed for visual question answering (VQA) is vast, and demands the synthesis of many skills – from grounding language to pixels <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Radford et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Zhai et al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> and spatial reasoning <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> to commonsense and knowledge-based reasoning <cite class="ltx_cite ltx_citemacro_citep">(Marino et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. Consider the question <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">“Is the carriage to the right of a horse?”</em>. To consistently answer such questions correctly, a system must recognize that the question is the conjunction of two subquestions: <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">“Is there a horse?”</em> and <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">“Is the carriage to the right of the horse?”</em> Scaling the typical finetuning paradigm to all possible combinations of reasoning skills is prohibitively expensive in annotation cost and makes it difficult to add skills to an already-trained system.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Modular approaches, on the other hand – from classic methods <cite class="ltx_cite ltx_citemacro_citep">(Krishnamurthy and Kollar, <a href="#bib.bib13" title="" class="ltx_ref">2013</a>)</cite>, to differentiable neural module networks (NMNs) <cite class="ltx_cite ltx_citemacro_citep">(Andreas et al., <a href="#bib.bib3" title="" class="ltx_ref">2016</a>; Hu et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Saqur and Narasimhan, <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>) – offer a potential route to leverage and scale to the compositional nature of visual reasoning as a means to generalize: i.e., <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">infinite use of finite means</span>. However, the modules of an NMN must still be trained jointly on a large dataset, and are also restricted in that they (i) require a parser, which must be modified if modules are added or removed from the system, and (ii) require retraining if a module is replaced.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.05392/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">CodeVQA Overview.</span> <span id="S1.F1.6.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> first prompts Codex with in-context examples that break down a given question into Python code. Using just the question, Codex generates an executable program that composes pre-defined visual modules using conditional logic, arithmetic, etc. The visual module, <span id="S1.F1.7.3" class="ltx_text ltx_font_typewriter">query</span> answers a question by captioning the image and using an LM to answer based on the captions. <span id="S1.F1.8.4" class="ltx_text ltx_font_typewriter">get_pos</span> retrieves the location of the object. Here, CodeVQA correctly identifies the question as a conjunction of a query and a spatial comparison and arrives at the right answer.
</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we investigate an alternative class of modular VQA approaches, whereby building on the recent advent of highly capable out-of-the-box language models (LMs) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>); Ouyang et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> and visual language models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, we develop systems that formulate VQA as a program synthesis problem. Specifically, our method <span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span>, illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, uses code-writing LMs to take questions as input, and outputs code to (i) orchestrate a series of visual primitive APIs that wrap around VLMs to probe the image for specific pieces of visual information (e.g., captions, pixel locations of entities, or image-text similarity scores), and (ii) reason about that information with the full expression of Python code (e.g. arithmetic, logic structures, feedback loops, etc.) to arrive at an answer. From a practical perspective, the modularity of <span id="S1.p3.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> combined with the few-shot prompting capabilities of LMs enable it to adapt to a broad range of desired VQA label distributions without additional model training, and benefits from replacing individual modules with improved versions as they become available.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We evaluate <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> in the few-shot VQA setting, which has seen a great deal of recent work <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a href="#bib.bib2" title="" class="ltx_ref">2022</a>; Jin et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Yang et al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>; Tiong et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>. Our method outperforms previous approaches by at least 3% on the COVR dataset <cite class="ltx_cite ltx_citemacro_citep">(Bogin et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, which requires reasoning over multiple images, and by roughly 2% on the GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. Our results suggest that the benefits of modularity with recent off-the-shelf models can be realized in VQA without additional model training.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our code and annotated programs are available at <a target="_blank" href="https://github.com/sanjayss34/codevqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/sanjayss34/codevqa</a>.</span></span></span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Several recent approaches for reasoning tasks consist of an LM that writes programs and an interpreter for these programs. <cite class="ltx_cite ltx_citemacro_citet">Liang et al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> applies this approach to robotics. <cite class="ltx_cite ltx_citemacro_citet">Cheng et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> introduces a framework for reasoning jointly over tables, text, and images, where the images are represented by image captions. <cite class="ltx_cite ltx_citemacro_citet">Subramanian et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> used a syntactic parser and hard-coded rules rather than an LM to aggregate outputs from CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> for zero-shot referring expression comprehension; their finding that CLIP is not useful for spatial keywords motivates our code generation approach to spatial reasoning.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Concurrent with our work, other papers have introduced similar frameworks for multi-hop VQA <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Kembhavi, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>; Surís et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. These papers conflate the benefit of program synthesis with the benefits of the LM, in-context examples, and vision models used as primitives. By contrast, we analyze the effect of program synthesis by comparing <span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> against a strong LM-based few-shot baseline using the same in-context example selection method. Moreover, while these frameworks rely on supervised VQA or object detection models, we show that we can obtain comparable performance (on the GQA dataset) using only the LM and models pre-trained on image-text pairs.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Few-shot VQA via Code Generation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In visual question answering (VQA), the inputs to the system are an image and a question and the output is a textual answer. We consider the few-shot VQA setting in which the system has access to only a small number (50) of human-annotated VQA instances.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Overview.</span> Fig <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates our approach. Given an image and a corresponding question, <span id="S3.p2.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> first generates a Python program using just the question. It then executes this program, using the image when necessary, to predict the answer. We first define the set of code primitives that our system uses (§ <a href="#S3.SS1" title="3.1 Code Primitives ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Then we describe how we generate a program that composes these primitives based on the question (§ <a href="#S3.SS2" title="3.2 Code generation ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Finally, we enumerate the pre-trained models that we employ (§ <a href="#S3.SS3" title="3.3 Component models ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Code Primitives</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Primitives define basic operations over the image or over text that are often useful for VQA. In <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span>, we use three primitives, which are defined below. Each of these primitives is implemented using image-text matching (ITM), image-text contrastive (ITC), and image-captioning models, each of which can be trained with only image-caption pairs.
The difference between ITM and ITC is that ITC computes separate image and text embeddings and takes a dot product, while ITM performs early fusion on the image and text features and is thus more computationally expensive. We note that our framework is not tied to this choice of primitives and can support other, more complex primitives that could draw on other aspects of the programming language and third-party libraries.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_typewriter ltx_title_paragraph">query(image, question)</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.2" class="ltx_p">This function answers a question about the given image. Our implementation of this function is based on PnP-VQA <cite class="ltx_cite ltx_citemacro_citep">(Tiong et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> and PICa <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and is implemented with the following steps: (1) using the ITM model, compute the GradCAM <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al., <a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> between the question and the image (averaged over question tokens), (2) sample <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="K=20" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐾</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">K=20</annotation></semantics></math> image patches based on their GradCAM score, (3) generate a captions from the sampled patches using the captioning model, (4) Repeat steps (2) and (3) until <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">C</annotation></semantics></math> unique captions have been generated, and (5) predict the answer by prompting an LM with the question, captions, and in-context examples. The in-context examples in step (5) are selected as described in § <a href="#S3.SS2" title="3.2 Code generation ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. When the dataset involves reasoning over multiple images, each in-context example has the captions for all images.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_typewriter ltx_title_paragraph">get_pos(image, text)</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">This function computes the GradCAM between the given text tokens and the image using the ITM model and returns the (x, y) pair that maximizes the GradCAM value. Note that this application of GradCAM is different from the one in <span id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">query</span> since we do not average over all question tokens. See Appendix <a href="#A2" title="Appendix B GradCAM ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for more information on how we compute GradCAM maps.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_typewriter ltx_title_paragraph">find_matching_image(images, text)</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">In the setting where multiple images are associated with each question, there are questions that refer specifically to one image (e.g. “What is the woman holding?”). This function can be used to select the most relevant image from the set. It is implemented by scoring each image with the text using the ITC model and picking the image with the highest score.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Code generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the first stage of <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span>, we generate a Python program based on the question. Using Python over a domain-specific language is advantageous because (1) it supports arithmetic as well as control flow including loops and if statements <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>–all of which we use in our programs–and (2) large LMs for code generation (e.g. Codex <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>) have been trained on a large amount of Python code.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We construct a prompt that consists of an instruction, constants that define the dimensions of the image, and import statements and API documentation (as a code comment) that specify the available functions. In addition to the prompt, the input to the LM also includes expert-annotated programs for several in-context examples. An in-context example for few-shot prompting on the COVR dataset is shown below (question in <span id="S3.SS2.p2.1.1" class="ltx_text" style="color:#A7A7A7;">gray</span>, the program is <span id="S3.SS2.p2.1.2" class="ltx_text" style="background-color:#E3EEFF;">highlighted</span>).

<span id="S3.SS2.p2.1.3" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #CCCCCC;">
<span id="S3.SS2.p2.1.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:420.6pt;background-color:#FFFFFF;">
<span id="S3.SS2.p2.1.3.1.1" class="ltx_p"><span id="S3.SS2.p2.1.3.1.1.1" class="ltx_text ltx_font_typewriter" style="color:#A7A7A7;"># Image Set 1: How many images contain exactly 2 pink shoes??
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">images = <span id="S3.SS2.p2.1.3.1.1.1.1.1" class="ltx_text" style="color:#693DA8;">open_images</span>("ImageSet1.jpg")</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.2" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">count = 0</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.3" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">for image in images:</span><span id="S3.SS2.p2.1.3.1.1.1.4" class="ltx_text" style="color:#000000;">
<span id="S3.SS2.p2.1.3.1.1.1.4.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       two_pink_shoes = <span id="S3.SS2.p2.1.3.1.1.1.4.1.1" class="ltx_text" style="color:#693DA8;">query</span>(image, "Are</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.4.2" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              there exactly 2 pink shoes?")</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.4.3" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       if two_pink_shoes == "yes":</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.4.4" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              count += 1</span>
<br class="ltx_break"><span id="S3.SS2.p2.1.3.1.1.1.4.5" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">answer = count</span>
<br class="ltx_break"></span></span></span>
</span></span>
<br class="ltx_break">For an example of the rest of the prompt for the LM, see Appendix <a href="#A1" title="Appendix A Code generation prompts ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. When executing the generated program results in a runtime error, we return call <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">query</span> on the image and the original question (including captions for all images if the instance involves multiple images).</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Since all annotated programs cannot fit into a single input to the model, we must select which programs to use as in-context examples for each test question. Following <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, we use sentence embeddings<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" title="" class="ltx_ref ltx_href">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a></span></span></span> to retrieve the most similar questions for each test question.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Component models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Our approach relies on four pre-trained models: a code generation model, an ITM model, an ITC model, an IC model, and a question-answering LM for answering questions based on captions. We use the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">code-davinci-002</span> model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> via the OpenAI API for both generating programs and for question-answering.
We use the BLIP models <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> finetuned for ITM, ITC, and captioning.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:222pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.1pt,24.0pt) scale(0.822310600029683,0.822310600029683) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">GQA</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">COVR</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">NLVR2</td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_center">Acc.</td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_center">Acc.</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.3.1.1" class="ltx_text ltx_font_bold">Finetuned</span></td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.3.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">VisualBERT</td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">57.9</td>
<td id="S3.T1.1.1.4.4" class="ltx_td ltx_align_center">67.0</td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">VinVL-Base</td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">65.1</td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.5.4" class="ltx_td ltx_align_center">83.1</td>
</tr>
<tr id="S3.T1.1.1.6" class="ltx_tr">
<td id="S3.T1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.6.1.1" class="ltx_text ltx_font_bold">Zero-shot</span></td>
<td id="S3.T1.1.1.6.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.6.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.6.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.1.7" class="ltx_tr">
<td id="S3.T1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r">FewVLM</td>
<td id="S3.T1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">29.3</td>
<td id="S3.T1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.7.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.1.1.8" class="ltx_tr">
<td id="S3.T1.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r">PnP-VQA</td>
<td id="S3.T1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r">42.3</td>
<td id="S3.T1.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.8.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.1.1.9" class="ltx_tr">
<td id="S3.T1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">BLIP-v2*</td>
<td id="S3.T1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r">44.7</td>
<td id="S3.T1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.9.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.1.1.10" class="ltx_tr">
<td id="S3.T1.1.1.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.1.1.10.1.1" class="ltx_text ltx_font_bold">Few-shot</span></td>
<td id="S3.T1.1.1.10.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.1.10.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.1.11" class="ltx_tr">
<td id="S3.T1.1.1.11.1" class="ltx_td ltx_align_left ltx_border_r">FewVLM</td>
<td id="S3.T1.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r">35.7</td>
<td id="S3.T1.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.11.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.1.1.12" class="ltx_tr">
<td id="S3.T1.1.1.12.1" class="ltx_td ltx_align_left ltx_border_r">VisProg*</td>
<td id="S3.T1.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r">50.5†</td>
<td id="S3.T1.1.1.12.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.12.4" class="ltx_td ltx_align_center">62.4</td>
</tr>
<tr id="S3.T1.1.1.13" class="ltx_tr">
<td id="S3.T1.1.1.13.1" class="ltx_td ltx_align_left ltx_border_r">ViperGPT*</td>
<td id="S3.T1.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r">48.2</td>
<td id="S3.T1.1.1.13.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S3.T1.1.1.13.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.1.1.14" class="ltx_tr">
<td id="S3.T1.1.1.14.1" class="ltx_td ltx_align_left ltx_border_r">Few-shot PnP-VQA</td>
<td id="S3.T1.1.1.14.2" class="ltx_td ltx_align_center ltx_border_r">46.6</td>
<td id="S3.T1.1.1.14.3" class="ltx_td ltx_align_center ltx_border_r">45.8</td>
<td id="S3.T1.1.1.14.4" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="S3.T1.1.1.15" class="ltx_tr">
<td id="S3.T1.1.1.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T1.1.1.15.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> (ours)</td>
<td id="S3.T1.1.1.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.1.1.15.2.1" class="ltx_text ltx_font_bold">49.0</span></td>
<td id="S3.T1.1.1.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T1.1.1.15.3.1" class="ltx_text ltx_font_bold">50.7</span></td>
<td id="S3.T1.1.1.15.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.1.15.4.1" class="ltx_text ltx_font_bold">64.0</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.5.1" class="ltx_text ltx_font_bold">Results on GQA (testdev), COVR (test), and NLVR2 (test-public)</span> datasets from <span id="S3.T1.6.2" class="ltx_text ltx_font_typewriter">CodeVQA</span>, Few-shot PnP-VQA, prior work VisualBERT <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, VinVL-Base <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, FewVLM <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>, PnP-VQA <cite class="ltx_cite ltx_citemacro_citep">(Tiong et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>, and concurrent work (marked with *) BLIP-v2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, VisProg <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Kembhavi, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, and ViperGPT <cite class="ltx_cite ltx_citemacro_citep">(Surís et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. Our method outperforms all few-shot methods from prior work. Highest few-shot scores for each full dataset are in <span id="S3.T1.7.3" class="ltx_text ltx_font_bold">bold</span>. †indicates an evaluation on a stratified sample of the test set, which may not be directly comparable to results on the full test set.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">See Appendix <a href="#A3" title="Appendix C Implementation Details ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for implementation details.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> contains multi-hop questions generated from human-annotated scene graphs of individual images in Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>. The COVR dataset <cite class="ltx_cite ltx_citemacro_citep">(Bogin et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> contains multi-hop questions about <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">sets of images</em> in the Visual Genome and imSitu <cite class="ltx_cite ltx_citemacro_citep">(Yatskar et al., <a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite> datasets. These questions are synthetically generated from templates and are then paraphrased by humans. Unless otherwise specified, we present results on the paraphrased questions. The NLVR2 dataset <cite class="ltx_cite ltx_citemacro_citep">(Suhr et al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> contains statements about <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">pairs of images</em>, and the task is to determine whether each statement is true or false (we rephrase the statements as questions before feeding it to the methods that we evaluate). Appendix <a href="#A8" title="Appendix H Licenses and Other Dataset Details ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a> has further details about the datasets.
For each of the three datasets, we wrote programs for 50 questions randomly sampled from the corresponding training set. Unless stated otherwise, we put 12 in-context examples in a prompt for a single-image dataset and 6 in-context examples in a prompt for a multi-image dataset (since including captions for multiple images increases the necessary context size for each example). We report the exact-match accuracies of the lower-cased answers.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baseline</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Our primary baseline is an adaptation of PnP-VQA <cite class="ltx_cite ltx_citemacro_citep">(Tiong et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> to the few-shot setting. We refer to it as “Few-shot PnP-VQA.” This baseline is equivalent to running the five-step <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">query</span> procedure described in § <a href="#S3.SS1" title="3.1 Code Primitives ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for every question. We also compare to zero-shot and few-shot methods from prior and concurrent work. Appendix <a href="#A4" title="Appendix D Details on Baselines ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> contains further details about those methods.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Component models ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results on the three datasets. <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> has the highest accuracy among the few-shot techniques. Compared to Few-shot PnP-VQA, it has markedly better performance on COVR, which makes sense because in this dataset, the baseline approach must combine information across image captions for multiple images when given a single prompt. On the other hand, our method loops over the images and queries a single image at a time or selects the image most relevant to the question. Indeed, Table <a href="#S4.T3" title="Table 3 ‣ 4.6 Analysis ‣ 4 Experiments ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> has the greatest advantage on instances involving 4 or 5 images.
Compared to concurrent work that also uses program synthesis, <span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">CodeVQA</span> generally performs better, which could be due to methodological differences like our in-context example retrieval or due to implementation details.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.4 Results ‣ 4 Experiments ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a qualitative comparison of <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> and the baseline Few-shot PnP-VQA on the COVR dataset. <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> answers the question correctly by answering a simpler question for each image and comparing the answers, while Few-shot PnP-VQA answers incorrectly despite producing captions with the necessary information.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2306.05392/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S4.F2.4.1" class="ltx_text ltx_font_bold">Qualitative Comparison</span>. <span id="S4.F2.5.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> correctly answers this question from COVR by breaking it into simpler questions, answering each separately, and comparing the answers. Few-shot PnP-VQA answers incorrectly, even though the captions contain the necessary information. (Note that <span id="S4.F2.6.3" class="ltx_text ltx_font_typewriter">CodeVQA</span> also generates captions, which are not shown here.)
</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablations</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.5 Ablations ‣ 4 Experiments ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares embedding-based retrieval of in-context examples with random retrieval. <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span>’s improvement over Few-shot PnP-VQA is greater when in-context examples are retrieved by embedding. Embedding-based retrieval offers a systematic way to collect relevant in-context examples rather than curating a single set of examples as in <cite class="ltx_cite ltx_citemacro_citet">Gupta and Kembhavi (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In Appendix <a href="#A6" title="Appendix F Additional Quantitative Results ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>, we include ablations for the question-answering LM and for the number of shots in the prompt as well as results on validation sets. Table <a href="#A4.T4" title="Table 4 ‣ Appendix D Details on Baselines ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> improves over Few-shot PnP-VQA when either <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_typewriter">code-davinci-002</span> or <span id="S4.SS5.p2.1.3" class="ltx_text ltx_font_typewriter">text-davinci-003</span> is used as the question-answering LM. Table <a href="#A6.T5" title="Table 5 ‣ Appendix F Additional Quantitative Results ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows roughly constant accuracy as the number of in-context examples is varied.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:108.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.4pt,8.7pt) scale(0.861590482960514,0.861590482960514) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Retrieval Method</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Few-shot PnP-VQA</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.1.2.1.1" class="ltx_text ltx_font_italic">text-davinci-003</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_left">Random</td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center">48.15</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center">49.9</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_left">Embedding</td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center">49.4</td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center">52.5</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.1.5.1.1" class="ltx_text ltx_font_italic">code-davinci-002</span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_left">Random</td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center">49.5</td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center">50.7</td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">Embedding</td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_bb">52.1</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">55.3</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.3.1" class="ltx_text ltx_font_bold">Comparing Example Retrieval Techniques</span> on 2000 GQA validation examples. Italicized GPT model name denotes the model used as the question-answering LM.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Analysis</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.6 Analysis ‣ 4 Experiments ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> breaks down accuracy
by question type. <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span>’s greatest improvement (roughly 30%) is in the subset consisting of questions about left/right or top/bottom object positions. There is also an improvement in “and” and “or” questions. This improvement could be related to the recent finding that LMs benefit from converting multi-hop into single-hop questions <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Accuracy on this kind of question can also be improved by improving the LM. For instance, using <span id="footnote3.1" class="ltx_text ltx_font_typewriter">text-davinci-003</span> as the LM for QA closes the gap between Few-shot PnP-VQA and <span id="footnote3.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> on “and” questions in GQA.</span></span></span></p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:75.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.2pt,7.1pt) scale(0.842979529201172,0.842979529201172) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">Number of images</td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td"></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center">1</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center">2</td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center">3</td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center">4</td>
<td id="S4.T3.1.1.2.6" class="ltx_td ltx_align_center">5</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"># of Instances</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">915</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">828</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">696</td>
<td id="S4.T3.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">4440</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_left">Few-shot PnP-VQA</td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center">91.7</td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center">51.5</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center">48.3</td>
<td id="S4.T3.1.1.4.5" class="ltx_td ltx_align_center">47.0</td>
<td id="S4.T3.1.1.4.6" class="ltx_td ltx_align_center">46.9</td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.1.5.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">75.0</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb">53.3</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">48.7</td>
<td id="S4.T3.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb">53.2</td>
<td id="S4.T3.1.1.5.6" class="ltx_td ltx_align_center ltx_border_bb">53.4</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.3.1" class="ltx_text ltx_font_bold">Accuracy by number of images per instance</span> on COVR validation set.</figcaption>
</figure>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">We analyzed sources of error in <span id="S4.SS6.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> on 100 examples in the COVR validation set for which <span id="S4.SS6.p2.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> answered incorrectly: irrelevant captions (31%), mistake in <span id="S4.SS6.p2.1.3" class="ltx_text ltx_font_typewriter">find_matching_image</span> (12%), program generation error (14%), question-answering error (25%), predicted answer could be considered correct (14%), ground-truth is unclear/incorrect (16%), and numerical error (1%). Note that these categories are not mutually exclusive, and 13 of the 100 examples were marked with multiple categories. Thus, more errors are due to execution of the modules than program generation.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2306.05392/assets/figures/gqa_fulltest_codedavinci_questiontypes.png" id="S4.F3.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.4.1" class="ltx_text ltx_font_bold">Accuracy by question type in GQA test set.</span> <span id="S4.F3.5.2" class="ltx_text ltx_font_typewriter" style="color:#1F75B3;">CodeVQA<span id="S4.F3.5.2.1" class="ltx_text ltx_font_serif"> (blue)</span></span> outperforms <span id="S4.F3.6.3" class="ltx_text" style="color:#FF800E;">Few-shot PnP-VQA (orange)</span> on the spatial, and, or questions. “Spatial” refers to questions focusing on left/right or top/bottom relations or object positions.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have introduced a framework for modular few-shot VQA. Our approach prompts an LM to generate a Python program that invokes pre-trained visual modules and composes the outputs of these modules to predict the answer. Unlike previous modular VQA techniques, this framework does not require (re-)training modules or a parser. Also, obtaining interpretable module outputs from previous modular approaches is nontrivial <cite class="ltx_cite ltx_citemacro_citep">(Subramanian et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, whereas in our approach the modules are frozen and thus interpretable. <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> can also be viewed as expanding pipelined systems <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite> to the full expression of code. Our approach exhibits empirical gains, motivating future work on modular few-shot VQA.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While the initial results are promising, the accuracy of our method remains lower than human VQA accuracy and models finetuned on the VQA datasets, which suggests that there may still be substantial progress that must be made before few-shot VQA methods with code synthesis are useful for practical real world applications. Also, further work is needed on extending the framework to additional primitives, as the results in Appendix <a href="#A7" title="Appendix G Experiments with Additional Primitives ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a> show that doing so does not always lead to improvements over the baseline method.
Another limitation of our approach is that it relies on large capable LMs, which may be restricted in use due to compute requirements or cost (e.g. via available APIs). We also focus in this work on benchmarking VQA capabilities with English as the primary language – future work may extend this to other languages via multilingual LMs.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We thank the members of the Berkeley NLP group (especially Eric Wallace), Grace Luo, and the anonymous reviewers for feedback on earlier drafts of this paper. We are grateful to Ben Bogin and Shivanshu Gupta for their assistance in evaluating <span id="S7.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> and Few-shot PnP-VQA on the private COVR test set. SS, MN, and TD were supported in part by the DoD, including an NDSEG fellowship (for SS) and DARPA’s LwLL, PTG, and/or SemaFor programs, by the NSF, and/or by the Berkeley Artificial Intelligence Research (BAIR) industrial alliance program.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. (2021)</span>
<span class="ltx_bibblock">
Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim,
and Miles Brundage. 2021.

</span>
<span class="ltx_bibblock">Evaluating clip: Towards characterization of broader capabilities and
downstream implications.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2108.02818.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock,
Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2204.14198.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N16-1181" title="" class="ltx_ref ltx_href">Learning to compose
neural networks for question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1545–1554, San Diego, California. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogin et al. (2021)</span>
<span class="ltx_bibblock">
Ben Bogin, Shivanshu Gupta, Matt Gardner, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">Covr: A test-bed for visually grounded compositional generalization
with real images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan,
Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul
Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor
Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2107.03374.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu,
Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A.
Smith, and Tao Yu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=lH1PV42cbF" title="" class="ltx_ref ltx_href">Binding language
models in symbolic languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6904–6913.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Kembhavi (2023)</span>
<span class="ltx_bibblock">
Tanmay Gupta and Aniruddha Kembhavi. 2023.

</span>
<span class="ltx_bibblock">Visual programming: Compositional visual reasoning without training.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2017)</span>
<span class="ltx_bibblock">
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
2017.

</span>
<span class="ltx_bibblock">Learning to reason: End-to-end module networks for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 804–813.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 6693–6702.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2021)</span>
<span class="ltx_bibblock">
Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. 2021.

</span>
<span class="ltx_bibblock">A good prompt is worth millions of parameters: Low-resource
prompt-based learning for vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2110.08484.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2016)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123:32–73.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnamurthy and Kollar (2013)</span>
<span class="ltx_bibblock">
Jayant Krishnamurthy and Thomas Kollar. 2013.

</span>
<span class="ltx_bibblock">Jointly learning to parse and perceive: Connecting natural language
to the physical world.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
1:193–206.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">BLIP-2: bootstrapping language-image pre-training with frozen image
encoders and large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty,
Caiming Xiong, and Steven C. H. Hoi. 2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with
momentum distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
2019.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1908.03557.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022)</span>
<span class="ltx_bibblock">
J. Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R.
Florence, and Andy Zeng. 2022.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2209.07753.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023.

</span>
<span class="ltx_bibblock">Grounding dino: Marrying dino with grounded pre-training for open-set
object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.05499</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 3190–3199.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe.
2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2203.02155.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike
Lewis. 2022.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2210.03350.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
8748–8763. PMLR.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ross et al. (2020)</span>
<span class="ltx_bibblock">
Candace Ross, Boris Katz, and Andrei Barbu. 2020.

</span>
<span class="ltx_bibblock">Measuring social biases in grounded vision and language embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">North American Chapter of the Association for Computational
Linguistics</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saqur and Narasimhan (2020)</span>
<span class="ltx_bibblock">
Raeid Saqur and Karthik Narasimhan. 2020.

</span>
<span class="ltx_bibblock">Multimodal graph networks for compositional generalization in visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. (2016)</span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell,
Devi Parikh, and Dhruv Batra. 2016.

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks via gradient-based
localization.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 128:336–359.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramanian et al. (2020)</span>
<span class="ltx_bibblock">
Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh,
Jonathan Berant, and Matt Gardner. 2020.

</span>
<span class="ltx_bibblock">Obtaining faithful interpretations from compositional neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational
Linguistics</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramanian et al. (2022)</span>
<span class="ltx_bibblock">
Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer
Singh, and Anna Rohrbach. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.357" title="" class="ltx_ref ltx_href">ReCLIP: A
strong zero-shot baseline for referring expression comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 5198–5215,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. (2019)</span>
<span class="ltx_bibblock">
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1644" title="" class="ltx_ref ltx_href">A corpus for reasoning
about natural language grounded in photographs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6418–6428, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Surís et al. (2023)</span>
<span class="ltx_bibblock">
Dídac Surís, Sachit Menon, and Carl Vondrick. 2023.

</span>
<span class="ltx_bibblock">Vipergpt: Visual inference via python execution for reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08128</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiong et al. (2022)</span>
<span class="ltx_bibblock">
Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, and Steven CH
Hoi. 2022.

</span>
<span class="ltx_bibblock">Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained
models with zero training.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Findings of ACL: EMNLP</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin,
Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit
Bansal, and Heng Ji. 2022.

</span>
<span class="ltx_bibblock">Language models with image descriptors are strong few-shot
video-language learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.10747.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and
Lijuan Wang. 2021.

</span>
<span class="ltx_bibblock">An empirical study of gpt-3 for few-shot knowledge-based vqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yatskar et al. (2016)</span>
<span class="ltx_bibblock">
Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. 2016.

</span>
<span class="ltx_bibblock">Situation recognition: Visual semantic role labeling for image
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, pages 5534–5542.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2022)</span>
<span class="ltx_bibblock">
Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,
et al. 2022.

</span>
<span class="ltx_bibblock">Socratic models: Composing zero-shot multimodal reasoning with
language.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.00598</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2022)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers,
Alexander Kolesnikov, and Lucas Beyer. 2022.

</span>
<span class="ltx_bibblock">Lit: Zero-shot transfer with locked-image text tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 18123–18133.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao. 2021.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 5575–5584.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Code generation prompts</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>GQA</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">The preamble of the prompt (<span id="A1.SS1.p1.1.1" class="ltx_text" style="color:#A7A7A7;">gray</span>)–containing the instruction, constants, import statements, and API documentation–and a single in-context example are shown below (question in <span id="A1.SS1.p1.1.2" class="ltx_text" style="color:#6F931B;">green</span>, program <span id="A1.SS1.p1.1.3" class="ltx_text" style="background-color:#E3EEFF;">highlighted</span>). In our main GQA experiments, 12 in-context examples are used for each evaluation example.

<span id="A1.SS1.p1.1.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #CCCCCC;">
<span id="A1.SS1.p1.1.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:420.6pt;background-color:#FFFFFF;">
<span id="A1.SS1.p1.1.4.1.1" class="ltx_p"><span id="A1.SS1.p1.1.4.1.1.1" class="ltx_text ltx_font_typewriter" style="color:#A7A7A7;">"""Write Python code to answer the questions about each image."""
<br class="ltx_break"># Global constants
<br class="ltx_break"># min x coordinate<span id="A1.SS1.p1.1.4.1.1.1.1" class="ltx_text" style="color:#000000;">
</span>LEFT = 0
<br class="ltx_break"># min y coordinate<span id="A1.SS1.p1.1.4.1.1.1.2" class="ltx_text" style="color:#000000;">
</span>BOTTOM = 0<span id="A1.SS1.p1.1.4.1.1.1.3" class="ltx_text" style="color:#000000;">
</span># max x coordinate<span id="A1.SS1.p1.1.4.1.1.1.4" class="ltx_text" style="color:#000000;">
</span>RIGHT = 24<span id="A1.SS1.p1.1.4.1.1.1.5" class="ltx_text" style="color:#000000;">
</span># max y coordinate<span id="A1.SS1.p1.1.4.1.1.1.6" class="ltx_text" style="color:#000000;">
</span>TOP = 24<span id="A1.SS1.p1.1.4.1.1.1.7" class="ltx_text" style="color:#000000;">
</span>from PIL import Image<span id="A1.SS1.p1.1.4.1.1.1.8" class="ltx_text" style="color:#000000;">
</span>from utils import open_images, query, find_matching_image, get_pos
<br class="ltx_break">"""
<br class="ltx_break">API Reference:
<br class="ltx_break">open_image(path: str) -&gt; Image - opens the image at the path and returns it as an Image object
<br class="ltx_break">query(img: Image, question: str) -&gt; str - queries the image returns an answer to the question
<br class="ltx_break">get_pos(img: Image, object: str) -&gt; (float, float) - returns the position of the object in the image<span id="A1.SS1.p1.1.4.1.1.1.9" class="ltx_text" style="color:#000000;">
</span>"""
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10" class="ltx_text" style="color:#6F931B;"># Image 1: Does the bench look silver and metallic?
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">img = <span id="A1.SS1.p1.1.4.1.1.1.10.1.1" class="ltx_text" style="color:#693DA8;">open_image</span>("Image1.jpg")</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.2" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">is_silver = <span id="A1.SS1.p1.1.4.1.1.1.10.2.1" class="ltx_text" style="color:#693DA8;">query</span>(img, "Does the bench look</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.3" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">    silver and metallic?")</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.4" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">is_metallic = <span id="A1.SS1.p1.1.4.1.1.1.10.4.1" class="ltx_text" style="color:#693DA8;">query</span>(img, "Does the bench look</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.5" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">    metallic?")</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.6" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">if is_silver == "yes" and is_metallic == "yes":</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.7" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">       answer = "yes"</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.8" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">else:</span>
<br class="ltx_break"><span id="A1.SS1.p1.1.4.1.1.1.10.9" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">       answer = "no"</span>
<br class="ltx_break"></span></span></span>
</span></span></p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>COVR</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">The preamble of the prompt (<span id="A1.SS2.p1.1.1" class="ltx_text" style="color:#A7A7A7;">gray</span>)–containing the instruction, constants, import statements, and API documentation–and a single in-context example (question in <span id="A1.SS2.p1.1.2" class="ltx_text" style="color:#6F931B;">green</span>, program <span id="A1.SS2.p1.1.3" class="ltx_text" style="background-color:#E3EEFF;">highlighted</span>) are shown below. In our COVR experiments, 6 in-context examples are used for each evaluation example.

<span id="A1.SS2.p1.1.4" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #CCCCCC;">
<span id="A1.SS2.p1.1.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:420.6pt;background-color:#FFFFFF;">
<span id="A1.SS2.p1.1.4.1.1" class="ltx_p"><span id="A1.SS2.p1.1.4.1.1.1" class="ltx_text ltx_font_typewriter" style="color:#A7A7A7;">"""Write Python code to answer the questions about each image."""
<br class="ltx_break"># Global constants
<br class="ltx_break"># min x coordinate<span id="A1.SS2.p1.1.4.1.1.1.1" class="ltx_text" style="color:#000000;">
</span>LEFT = 0
<br class="ltx_break"># min y coordinate<span id="A1.SS2.p1.1.4.1.1.1.2" class="ltx_text" style="color:#000000;">
</span>BOTTOM = 0<span id="A1.SS2.p1.1.4.1.1.1.3" class="ltx_text" style="color:#000000;">
</span># max x coordinate<span id="A1.SS2.p1.1.4.1.1.1.4" class="ltx_text" style="color:#000000;">
</span>RIGHT = 24<span id="A1.SS2.p1.1.4.1.1.1.5" class="ltx_text" style="color:#000000;">
</span># max y coordinate<span id="A1.SS2.p1.1.4.1.1.1.6" class="ltx_text" style="color:#000000;">
</span>TOP = 24<span id="A1.SS2.p1.1.4.1.1.1.7" class="ltx_text" style="color:#000000;">
</span>from PIL import Image<span id="A1.SS2.p1.1.4.1.1.1.8" class="ltx_text" style="color:#000000;">
</span>from utils import open_images, query, find_matching_image, get_pos
<br class="ltx_break">"""
<br class="ltx_break">API Reference:
<br class="ltx_break">open_image(path: str) -&gt; List[Image] - opens the images in the given directory and returns them in a list of Image objects
<br class="ltx_break">query(img: Image, question: str) -&gt; str - queries the region of the image in the given coordinates and returns an answer
<br class="ltx_break">find_matching_image(images: List[Image], text: str) -&gt; Image - returns the image that best matches the text
<br class="ltx_break">get_pos(img: Image, object: str) -&gt; (float, float) - returns the position of the object in the image<span id="A1.SS2.p1.1.4.1.1.1.9" class="ltx_text" style="color:#000000;">
</span>"""
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10" class="ltx_text" style="color:#6F931B;"># Image Set 1: Is it true that there are more ladies that are wearing black shirt than men that are wearing black shirt?
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">images = <span id="A1.SS2.p1.1.4.1.1.1.10.1.1" class="ltx_text" style="color:#693DA8;">open_images</span>("ImageSet1.jpg")</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.2" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">ladies_total = 0</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.3" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">men_total = 0</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.4" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">for image in images:</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.5" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;color:#000000;background-color:#E3EEFF;">       ladies_exist = <span id="A1.SS2.p1.1.4.1.1.1.10.5.1" class="ltx_text" style="color:#693DA8;">query</span>(image, "Is there a</span><span id="A1.SS2.p1.1.4.1.1.1.10.6" class="ltx_text" style="color:#000000;">
<span id="A1.SS2.p1.1.4.1.1.1.10.6.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">           lady?")</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.2" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       if ladies_exist == "yes":</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.3" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              ladies_count = <span id="A1.SS2.p1.1.4.1.1.1.10.6.3.1" class="ltx_text" style="color:#693DA8;">int</span>(<span id="A1.SS2.p1.1.4.1.1.1.10.6.3.2" class="ltx_text" style="color:#693DA8;">query</span>(image, "How</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.4" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">                 many ladies are wearing black</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.5" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">                 shirt?"))</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.6" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              ladies_total += ladies_count</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.7" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       man_exist = <span id="A1.SS2.p1.1.4.1.1.1.10.6.7.1" class="ltx_text" style="color:#693DA8;">query</span>(image, "Is there a</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.8" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">           man?")</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.9" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       if men_exist == "yes":</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.10" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              men_count = <span id="A1.SS2.p1.1.4.1.1.1.10.6.10.1" class="ltx_text" style="color:#693DA8;">int</span>(<span id="A1.SS2.p1.1.4.1.1.1.10.6.10.2" class="ltx_text" style="color:#693DA8;">query</span>(image, "How</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.11" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">                 many men are wearing black</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.12" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">                 shirt?"))</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.13" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">              men_total += men_count</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.14" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">if ladies_total &gt; men_total:</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.15" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       answer = "yes"</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.16" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">else:</span>
<br class="ltx_break"><span id="A1.SS2.p1.1.4.1.1.1.10.6.17" class="ltx_text ltx_align_left ltx_inline-block" style="width:429.3pt;background-color:#E3EEFF;">       answer = "no"</span>
<br class="ltx_break"></span></span></span></span>
</span></span>
<br class="ltx_break"></p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>GradCAM</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.11" class="ltx_p">Our computation of GradCAM follows prior work that uses vision transformers <cite class="ltx_cite ltx_citemacro_citep">(Tiong et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Li et al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. We are given a question with tokens <math id="A2.p1.1.m1.3" class="ltx_Math" alttext="q_{1},...,q_{T}" display="inline"><semantics id="A2.p1.1.m1.3a"><mrow id="A2.p1.1.m1.3.3.2" xref="A2.p1.1.m1.3.3.3.cmml"><msub id="A2.p1.1.m1.2.2.1.1" xref="A2.p1.1.m1.2.2.1.1.cmml"><mi id="A2.p1.1.m1.2.2.1.1.2" xref="A2.p1.1.m1.2.2.1.1.2.cmml">q</mi><mn id="A2.p1.1.m1.2.2.1.1.3" xref="A2.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="A2.p1.1.m1.3.3.2.3" xref="A2.p1.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">…</mi><mo id="A2.p1.1.m1.3.3.2.4" xref="A2.p1.1.m1.3.3.3.cmml">,</mo><msub id="A2.p1.1.m1.3.3.2.2" xref="A2.p1.1.m1.3.3.2.2.cmml"><mi id="A2.p1.1.m1.3.3.2.2.2" xref="A2.p1.1.m1.3.3.2.2.2.cmml">q</mi><mi id="A2.p1.1.m1.3.3.2.2.3" xref="A2.p1.1.m1.3.3.2.2.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.3b"><list id="A2.p1.1.m1.3.3.3.cmml" xref="A2.p1.1.m1.3.3.2"><apply id="A2.p1.1.m1.2.2.1.1.cmml" xref="A2.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="A2.p1.1.m1.2.2.1.1.1.cmml" xref="A2.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="A2.p1.1.m1.2.2.1.1.2.cmml" xref="A2.p1.1.m1.2.2.1.1.2">𝑞</ci><cn type="integer" id="A2.p1.1.m1.2.2.1.1.3.cmml" xref="A2.p1.1.m1.2.2.1.1.3">1</cn></apply><ci id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">…</ci><apply id="A2.p1.1.m1.3.3.2.2.cmml" xref="A2.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="A2.p1.1.m1.3.3.2.2.1.cmml" xref="A2.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="A2.p1.1.m1.3.3.2.2.2.cmml" xref="A2.p1.1.m1.3.3.2.2.2">𝑞</ci><ci id="A2.p1.1.m1.3.3.2.2.3.cmml" xref="A2.p1.1.m1.3.3.2.2.3">𝑇</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.3c">q_{1},...,q_{T}</annotation></semantics></math> and an image that is tokenized into <math id="A2.p1.2.m2.1" class="ltx_Math" alttext="K\times K" display="inline"><semantics id="A2.p1.2.m2.1a"><mrow id="A2.p1.2.m2.1.1" xref="A2.p1.2.m2.1.1.cmml"><mi id="A2.p1.2.m2.1.1.2" xref="A2.p1.2.m2.1.1.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="A2.p1.2.m2.1.1.1" xref="A2.p1.2.m2.1.1.1.cmml">×</mo><mi id="A2.p1.2.m2.1.1.3" xref="A2.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.2.m2.1b"><apply id="A2.p1.2.m2.1.1.cmml" xref="A2.p1.2.m2.1.1"><times id="A2.p1.2.m2.1.1.1.cmml" xref="A2.p1.2.m2.1.1.1"></times><ci id="A2.p1.2.m2.1.1.2.cmml" xref="A2.p1.2.m2.1.1.2">𝐾</ci><ci id="A2.p1.2.m2.1.1.3.cmml" xref="A2.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.2.m2.1c">K\times K</annotation></semantics></math> patches. We use layer <math id="A2.p1.3.m3.1" class="ltx_Math" alttext="L=6" display="inline"><semantics id="A2.p1.3.m3.1a"><mrow id="A2.p1.3.m3.1.1" xref="A2.p1.3.m3.1.1.cmml"><mi id="A2.p1.3.m3.1.1.2" xref="A2.p1.3.m3.1.1.2.cmml">L</mi><mo id="A2.p1.3.m3.1.1.1" xref="A2.p1.3.m3.1.1.1.cmml">=</mo><mn id="A2.p1.3.m3.1.1.3" xref="A2.p1.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.3.m3.1b"><apply id="A2.p1.3.m3.1.1.cmml" xref="A2.p1.3.m3.1.1"><eq id="A2.p1.3.m3.1.1.1.cmml" xref="A2.p1.3.m3.1.1.1"></eq><ci id="A2.p1.3.m3.1.1.2.cmml" xref="A2.p1.3.m3.1.1.2">𝐿</ci><cn type="integer" id="A2.p1.3.m3.1.1.3.cmml" xref="A2.p1.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.3.m3.1c">L=6</annotation></semantics></math> to compute GradCAM, following <cite class="ltx_cite ltx_citemacro_citet">Tiong et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>. We compute a GradCAM map for each token as follows. Let <math id="A2.p1.4.m4.1" class="ltx_Math" alttext="C\in\mathbb{R}^{T\times K^{2}}" display="inline"><semantics id="A2.p1.4.m4.1a"><mrow id="A2.p1.4.m4.1.1" xref="A2.p1.4.m4.1.1.cmml"><mi id="A2.p1.4.m4.1.1.2" xref="A2.p1.4.m4.1.1.2.cmml">C</mi><mo id="A2.p1.4.m4.1.1.1" xref="A2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="A2.p1.4.m4.1.1.3" xref="A2.p1.4.m4.1.1.3.cmml"><mi id="A2.p1.4.m4.1.1.3.2" xref="A2.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="A2.p1.4.m4.1.1.3.3" xref="A2.p1.4.m4.1.1.3.3.cmml"><mi id="A2.p1.4.m4.1.1.3.3.2" xref="A2.p1.4.m4.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="A2.p1.4.m4.1.1.3.3.1" xref="A2.p1.4.m4.1.1.3.3.1.cmml">×</mo><msup id="A2.p1.4.m4.1.1.3.3.3" xref="A2.p1.4.m4.1.1.3.3.3.cmml"><mi id="A2.p1.4.m4.1.1.3.3.3.2" xref="A2.p1.4.m4.1.1.3.3.3.2.cmml">K</mi><mn id="A2.p1.4.m4.1.1.3.3.3.3" xref="A2.p1.4.m4.1.1.3.3.3.3.cmml">2</mn></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.4.m4.1b"><apply id="A2.p1.4.m4.1.1.cmml" xref="A2.p1.4.m4.1.1"><in id="A2.p1.4.m4.1.1.1.cmml" xref="A2.p1.4.m4.1.1.1"></in><ci id="A2.p1.4.m4.1.1.2.cmml" xref="A2.p1.4.m4.1.1.2">𝐶</ci><apply id="A2.p1.4.m4.1.1.3.cmml" xref="A2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A2.p1.4.m4.1.1.3.1.cmml" xref="A2.p1.4.m4.1.1.3">superscript</csymbol><ci id="A2.p1.4.m4.1.1.3.2.cmml" xref="A2.p1.4.m4.1.1.3.2">ℝ</ci><apply id="A2.p1.4.m4.1.1.3.3.cmml" xref="A2.p1.4.m4.1.1.3.3"><times id="A2.p1.4.m4.1.1.3.3.1.cmml" xref="A2.p1.4.m4.1.1.3.3.1"></times><ci id="A2.p1.4.m4.1.1.3.3.2.cmml" xref="A2.p1.4.m4.1.1.3.3.2">𝑇</ci><apply id="A2.p1.4.m4.1.1.3.3.3.cmml" xref="A2.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="A2.p1.4.m4.1.1.3.3.3.1.cmml" xref="A2.p1.4.m4.1.1.3.3.3">superscript</csymbol><ci id="A2.p1.4.m4.1.1.3.3.3.2.cmml" xref="A2.p1.4.m4.1.1.3.3.3.2">𝐾</ci><cn type="integer" id="A2.p1.4.m4.1.1.3.3.3.3.cmml" xref="A2.p1.4.m4.1.1.3.3.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.4.m4.1c">C\in\mathbb{R}^{T\times K^{2}}</annotation></semantics></math> be the cross-attention map from layer <math id="A2.p1.5.m5.1" class="ltx_Math" alttext="L" display="inline"><semantics id="A2.p1.5.m5.1a"><mi id="A2.p1.5.m5.1.1" xref="A2.p1.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A2.p1.5.m5.1b"><ci id="A2.p1.5.m5.1.1.cmml" xref="A2.p1.5.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.5.m5.1c">L</annotation></semantics></math>. Let <math id="A2.p1.6.m6.1" class="ltx_Math" alttext="G\in\mathbb{R}^{T\times K^{2}}" display="inline"><semantics id="A2.p1.6.m6.1a"><mrow id="A2.p1.6.m6.1.1" xref="A2.p1.6.m6.1.1.cmml"><mi id="A2.p1.6.m6.1.1.2" xref="A2.p1.6.m6.1.1.2.cmml">G</mi><mo id="A2.p1.6.m6.1.1.1" xref="A2.p1.6.m6.1.1.1.cmml">∈</mo><msup id="A2.p1.6.m6.1.1.3" xref="A2.p1.6.m6.1.1.3.cmml"><mi id="A2.p1.6.m6.1.1.3.2" xref="A2.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="A2.p1.6.m6.1.1.3.3" xref="A2.p1.6.m6.1.1.3.3.cmml"><mi id="A2.p1.6.m6.1.1.3.3.2" xref="A2.p1.6.m6.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="A2.p1.6.m6.1.1.3.3.1" xref="A2.p1.6.m6.1.1.3.3.1.cmml">×</mo><msup id="A2.p1.6.m6.1.1.3.3.3" xref="A2.p1.6.m6.1.1.3.3.3.cmml"><mi id="A2.p1.6.m6.1.1.3.3.3.2" xref="A2.p1.6.m6.1.1.3.3.3.2.cmml">K</mi><mn id="A2.p1.6.m6.1.1.3.3.3.3" xref="A2.p1.6.m6.1.1.3.3.3.3.cmml">2</mn></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.6.m6.1b"><apply id="A2.p1.6.m6.1.1.cmml" xref="A2.p1.6.m6.1.1"><in id="A2.p1.6.m6.1.1.1.cmml" xref="A2.p1.6.m6.1.1.1"></in><ci id="A2.p1.6.m6.1.1.2.cmml" xref="A2.p1.6.m6.1.1.2">𝐺</ci><apply id="A2.p1.6.m6.1.1.3.cmml" xref="A2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.p1.6.m6.1.1.3.1.cmml" xref="A2.p1.6.m6.1.1.3">superscript</csymbol><ci id="A2.p1.6.m6.1.1.3.2.cmml" xref="A2.p1.6.m6.1.1.3.2">ℝ</ci><apply id="A2.p1.6.m6.1.1.3.3.cmml" xref="A2.p1.6.m6.1.1.3.3"><times id="A2.p1.6.m6.1.1.3.3.1.cmml" xref="A2.p1.6.m6.1.1.3.3.1"></times><ci id="A2.p1.6.m6.1.1.3.3.2.cmml" xref="A2.p1.6.m6.1.1.3.3.2">𝑇</ci><apply id="A2.p1.6.m6.1.1.3.3.3.cmml" xref="A2.p1.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="A2.p1.6.m6.1.1.3.3.3.1.cmml" xref="A2.p1.6.m6.1.1.3.3.3">superscript</csymbol><ci id="A2.p1.6.m6.1.1.3.3.3.2.cmml" xref="A2.p1.6.m6.1.1.3.3.3.2">𝐾</ci><cn type="integer" id="A2.p1.6.m6.1.1.3.3.3.3.cmml" xref="A2.p1.6.m6.1.1.3.3.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.6.m6.1c">G\in\mathbb{R}^{T\times K^{2}}</annotation></semantics></math> be the gradient of the image-text matching score with respect to <math id="A2.p1.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="A2.p1.7.m7.1a"><mi id="A2.p1.7.m7.1.1" xref="A2.p1.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A2.p1.7.m7.1b"><ci id="A2.p1.7.m7.1.1.cmml" xref="A2.p1.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.7.m7.1c">C</annotation></semantics></math>. Then the GradCAM map for token <math id="A2.p1.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A2.p1.8.m8.1a"><mi id="A2.p1.8.m8.1.1" xref="A2.p1.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A2.p1.8.m8.1b"><ci id="A2.p1.8.m8.1.1.cmml" xref="A2.p1.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.8.m8.1c">i</annotation></semantics></math> is given by the <math id="A2.p1.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A2.p1.9.m9.1a"><mi id="A2.p1.9.m9.1.1" xref="A2.p1.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A2.p1.9.m9.1b"><ci id="A2.p1.9.m9.1.1.cmml" xref="A2.p1.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.9.m9.1c">i</annotation></semantics></math>th row of <math id="A2.p1.10.m10.1" class="ltx_math_unparsed" alttext="C\bigodot ReLU(G))" display="inline"><semantics id="A2.p1.10.m10.1a"><mrow id="A2.p1.10.m10.1b"><mi id="A2.p1.10.m10.1.2">C</mi><mo id="A2.p1.10.m10.1.3">⨀</mo><mi id="A2.p1.10.m10.1.4">R</mi><mi id="A2.p1.10.m10.1.5">e</mi><mi id="A2.p1.10.m10.1.6">L</mi><mi id="A2.p1.10.m10.1.7">U</mi><mrow id="A2.p1.10.m10.1.8"><mo stretchy="false" id="A2.p1.10.m10.1.8.1">(</mo><mi id="A2.p1.10.m10.1.1">G</mi><mo stretchy="false" id="A2.p1.10.m10.1.8.2">)</mo></mrow><mo stretchy="false" id="A2.p1.10.m10.1.9">)</mo></mrow><annotation encoding="application/x-tex" id="A2.p1.10.m10.1c">C\bigodot ReLU(G))</annotation></semantics></math>, where <math id="A2.p1.11.m11.1" class="ltx_Math" alttext="\bigodot" display="inline"><semantics id="A2.p1.11.m11.1a"><mo id="A2.p1.11.m11.1.1" xref="A2.p1.11.m11.1.1.cmml">⨀</mo><annotation-xml encoding="MathML-Content" id="A2.p1.11.m11.1b"><ci id="A2.p1.11.m11.1.1.cmml" xref="A2.p1.11.m11.1.1">⨀</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.11.m11.1c">\bigodot</annotation></semantics></math> denotes elementwise multiplication. As stated in Section <a href="#S3.SS1" title="3.1 Code Primitives ‣ 3 Few-shot VQA via Code Generation ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, for the <span id="A2.p1.11.1" class="ltx_text ltx_font_typewriter">query</span> primitive, we take the average GradCAM map across all question tokens, whereas for the <span id="A2.p1.11.2" class="ltx_text ltx_font_typewriter">get_pos</span> primitive, we take the average GradCAM map across the input text tokens (which are part of the question tokens).</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Implementation Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.4" class="ltx_p">To generate captions for in-context examples in each dataset, we run steps <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="1-4" display="inline"><semantics id="A3.p1.1.m1.1a"><mrow id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml"><mn id="A3.p1.1.m1.1.1.2" xref="A3.p1.1.m1.1.1.2.cmml">1</mn><mo id="A3.p1.1.m1.1.1.1" xref="A3.p1.1.m1.1.1.1.cmml">−</mo><mn id="A3.p1.1.m1.1.1.3" xref="A3.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><apply id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"><minus id="A3.p1.1.m1.1.1.1.cmml" xref="A3.p1.1.m1.1.1.1"></minus><cn type="integer" id="A3.p1.1.m1.1.1.2.cmml" xref="A3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="A3.p1.1.m1.1.1.3.cmml" xref="A3.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">1-4</annotation></semantics></math> for each of the 50 questions in the database of in-context examples. For GQA experiments, we use <math id="A3.p1.2.m2.1" class="ltx_Math" alttext="C=7" display="inline"><semantics id="A3.p1.2.m2.1a"><mrow id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml"><mi id="A3.p1.2.m2.1.1.2" xref="A3.p1.2.m2.1.1.2.cmml">C</mi><mo id="A3.p1.2.m2.1.1.1" xref="A3.p1.2.m2.1.1.1.cmml">=</mo><mn id="A3.p1.2.m2.1.1.3" xref="A3.p1.2.m2.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><apply id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"><eq id="A3.p1.2.m2.1.1.1.cmml" xref="A3.p1.2.m2.1.1.1"></eq><ci id="A3.p1.2.m2.1.1.2.cmml" xref="A3.p1.2.m2.1.1.2">𝐶</ci><cn type="integer" id="A3.p1.2.m2.1.1.3.cmml" xref="A3.p1.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">C=7</annotation></semantics></math> captions per image, and for COVR experiments, where each question is associated with multiple images, we use <math id="A3.p1.3.m3.1" class="ltx_Math" alttext="C=3" display="inline"><semantics id="A3.p1.3.m3.1a"><mrow id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml"><mi id="A3.p1.3.m3.1.1.2" xref="A3.p1.3.m3.1.1.2.cmml">C</mi><mo id="A3.p1.3.m3.1.1.1" xref="A3.p1.3.m3.1.1.1.cmml">=</mo><mn id="A3.p1.3.m3.1.1.3" xref="A3.p1.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><apply id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1"><eq id="A3.p1.3.m3.1.1.1.cmml" xref="A3.p1.3.m3.1.1.1"></eq><ci id="A3.p1.3.m3.1.1.2.cmml" xref="A3.p1.3.m3.1.1.2">𝐶</ci><cn type="integer" id="A3.p1.3.m3.1.1.3.cmml" xref="A3.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">C=3</annotation></semantics></math> captions per image.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We chose this number of captions to be the maximum possible subject to the number of shots and the context size of the <span id="footnote4.1" class="ltx_text ltx_font_typewriter">davinci</span> model, which we used as our question-answering LM in preliminary experiments.</span></span></span> We use <math id="A3.p1.4.m4.1" class="ltx_Math" alttext="C=7" display="inline"><semantics id="A3.p1.4.m4.1a"><mrow id="A3.p1.4.m4.1.1" xref="A3.p1.4.m4.1.1.cmml"><mi id="A3.p1.4.m4.1.1.2" xref="A3.p1.4.m4.1.1.2.cmml">C</mi><mo id="A3.p1.4.m4.1.1.1" xref="A3.p1.4.m4.1.1.1.cmml">=</mo><mn id="A3.p1.4.m4.1.1.3" xref="A3.p1.4.m4.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.4.m4.1b"><apply id="A3.p1.4.m4.1.1.cmml" xref="A3.p1.4.m4.1.1"><eq id="A3.p1.4.m4.1.1.1.cmml" xref="A3.p1.4.m4.1.1.1"></eq><ci id="A3.p1.4.m4.1.1.2.cmml" xref="A3.p1.4.m4.1.1.2">𝐶</ci><cn type="integer" id="A3.p1.4.m4.1.1.3.cmml" xref="A3.p1.4.m4.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.4.m4.1c">C=7</annotation></semantics></math> captions for the NLVR2 dataset. Each reported accuracy result represents a single evaluation run over the corresponding evaluation set. For NLVR2 and some instances of COVR, the text input is a statement (to be classified as True/False). We convert each such statement to a question by adding the prefix “Is it true that” to it and converting the answer to “yes”/“no.” We use question embeddings to select 12 examples for GQA and 6 examples for COVR and NLVR2.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details on Baselines</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">FewVLM randomly samples 16 few-shot examples for GQA. VisProg runs the program generation and execution pipeline five times, and for each iteration randomly samples 24 few-shot examples for GQA and 16 few-shot examples for NLVR2. ViperGPT uses 8 few-shot examples for GQA. VisProg uses <span id="A4.p1.1.1" class="ltx_text ltx_font_typewriter">text-davinci-002</span> or <span id="A4.p1.1.2" class="ltx_text ltx_font_typewriter">text-davinci-003</span> for code generation (according to the code release), while ViperGPT uses <span id="A4.p1.1.3" class="ltx_text ltx_font_typewriter">code-davinci-002</span>.</p>
</div>
<figure id="A4.F4" class="ltx_figure"><img src="/html/2306.05392/assets/figures/gqa_val2000_codedavinci_questiontypes.png" id="A4.F4.g1" class="ltx_graphics ltx_img_landscape" width="287" height="216" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="A4.F4.4.1" class="ltx_text ltx_font_bold">Accuracy by question type in 2000 GQA validation examples.</span> <span id="A4.F4.5.2" class="ltx_text ltx_font_typewriter" style="color:#1F75B3;">CodeVQA<span id="A4.F4.5.2.1" class="ltx_text ltx_font_serif"> (blue)</span></span> outperforms <span id="A4.F4.6.3" class="ltx_text" style="color:#FF800E;">Few-shot PnP-VQA (orange)</span> on the spatial and or questions. “Spatial” refers to questions focusing on left/right or top/bottom relations or object positions.</figcaption>
</figure>
<figure id="A4.T4" class="ltx_table">
<div id="A4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:179.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.4pt,0.6pt) scale(0.993572170139216,0.993572170139216) ;">
<table id="A4.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A4.T4.1.1.1" class="ltx_tr">
<td id="A4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="A4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="A4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">GQA</td>
<td id="A4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">COVR</td>
</tr>
<tr id="A4.T4.1.1.2" class="ltx_tr">
<td id="A4.T4.1.1.2.1" class="ltx_td ltx_align_center">Shots</td>
<td id="A4.T4.1.1.2.2" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.2.2.1" class="ltx_text ltx_font_bold">Val Sample</span></td>
<td id="A4.T4.1.1.2.3" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.2.3.1" class="ltx_text ltx_font_bold">Testdev</span></td>
<td id="A4.T4.1.1.2.4" class="ltx_td ltx_align_center">Shots</td>
<td id="A4.T4.1.1.2.5" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.2.5.1" class="ltx_text ltx_font_bold">Val Sample</span></td>
<td id="A4.T4.1.1.2.6" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.2.6.1" class="ltx_text ltx_font_bold">Val</span></td>
<td id="A4.T4.1.1.2.7" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.2.7.1" class="ltx_text ltx_font_bold">Test</span></td>
</tr>
<tr id="A4.T4.1.1.3" class="ltx_tr">
<td id="A4.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Few-shot PnP-VQA</td>
<td id="A4.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="A4.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">49.4</td>
<td id="A4.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.9</td>
<td id="A4.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="A4.T4.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="A4.T4.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="A4.T4.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="A4.T4.1.1.4" class="ltx_tr">
<td id="A4.T4.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">w/ <span id="A4.T4.1.1.4.1.1" class="ltx_text ltx_font_typewriter">text-davinci-003</span>
</td>
<td id="A4.T4.1.1.4.2" class="ltx_td"></td>
<td id="A4.T4.1.1.4.3" class="ltx_td"></td>
<td id="A4.T4.1.1.4.4" class="ltx_td ltx_border_r"></td>
<td id="A4.T4.1.1.4.5" class="ltx_td"></td>
<td id="A4.T4.1.1.4.6" class="ltx_td"></td>
<td id="A4.T4.1.1.4.7" class="ltx_td"></td>
<td id="A4.T4.1.1.4.8" class="ltx_td"></td>
</tr>
<tr id="A4.T4.1.1.5" class="ltx_tr">
<td id="A4.T4.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="A4.T4.1.1.5.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> (ours)</td>
<td id="A4.T4.1.1.5.2" class="ltx_td ltx_align_center">12</td>
<td id="A4.T4.1.1.5.3" class="ltx_td ltx_align_center">52.5</td>
<td id="A4.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">46.8</td>
<td id="A4.T4.1.1.5.5" class="ltx_td ltx_align_center">6</td>
<td id="A4.T4.1.1.5.6" class="ltx_td ltx_align_center">54.4</td>
<td id="A4.T4.1.1.5.7" class="ltx_td ltx_align_center">–</td>
<td id="A4.T4.1.1.5.8" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="A4.T4.1.1.6" class="ltx_tr">
<td id="A4.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">w/ <span id="A4.T4.1.1.6.1.1" class="ltx_text ltx_font_typewriter">text-davinci-003</span>
</td>
<td id="A4.T4.1.1.6.2" class="ltx_td"></td>
<td id="A4.T4.1.1.6.3" class="ltx_td"></td>
<td id="A4.T4.1.1.6.4" class="ltx_td ltx_border_r"></td>
<td id="A4.T4.1.1.6.5" class="ltx_td"></td>
<td id="A4.T4.1.1.6.6" class="ltx_td"></td>
<td id="A4.T4.1.1.6.7" class="ltx_td"></td>
<td id="A4.T4.1.1.6.8" class="ltx_td"></td>
</tr>
<tr id="A4.T4.1.1.7" class="ltx_tr">
<td id="A4.T4.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r">Few-shot PnP-VQA</td>
<td id="A4.T4.1.1.7.2" class="ltx_td ltx_align_center">12</td>
<td id="A4.T4.1.1.7.3" class="ltx_td ltx_align_center">52.1</td>
<td id="A4.T4.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r">46.6</td>
<td id="A4.T4.1.1.7.5" class="ltx_td ltx_align_center">6</td>
<td id="A4.T4.1.1.7.6" class="ltx_td ltx_align_center">49.0</td>
<td id="A4.T4.1.1.7.7" class="ltx_td ltx_align_center">47.8</td>
<td id="A4.T4.1.1.7.8" class="ltx_td ltx_align_center">45.8</td>
</tr>
<tr id="A4.T4.1.1.8" class="ltx_tr">
<td id="A4.T4.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r">w/ <span id="A4.T4.1.1.8.1.1" class="ltx_text ltx_font_typewriter">code-davinci-002</span>
</td>
<td id="A4.T4.1.1.8.2" class="ltx_td"></td>
<td id="A4.T4.1.1.8.3" class="ltx_td"></td>
<td id="A4.T4.1.1.8.4" class="ltx_td ltx_border_r"></td>
<td id="A4.T4.1.1.8.5" class="ltx_td"></td>
<td id="A4.T4.1.1.8.6" class="ltx_td"></td>
<td id="A4.T4.1.1.8.7" class="ltx_td"></td>
<td id="A4.T4.1.1.8.8" class="ltx_td"></td>
</tr>
<tr id="A4.T4.1.1.9" class="ltx_tr">
<td id="A4.T4.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="A4.T4.1.1.9.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> (ours)</td>
<td id="A4.T4.1.1.9.2" class="ltx_td ltx_align_center">12</td>
<td id="A4.T4.1.1.9.3" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.9.3.1" class="ltx_text ltx_font_bold">55.3</span></td>
<td id="A4.T4.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r"><span id="A4.T4.1.1.9.4.1" class="ltx_text ltx_font_bold">49.0</span></td>
<td id="A4.T4.1.1.9.5" class="ltx_td ltx_align_center">6</td>
<td id="A4.T4.1.1.9.6" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.9.6.1" class="ltx_text ltx_font_bold">54.5</span></td>
<td id="A4.T4.1.1.9.7" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.9.7.1" class="ltx_text ltx_font_bold">52.9</span></td>
<td id="A4.T4.1.1.9.8" class="ltx_td ltx_align_center"><span id="A4.T4.1.1.9.8.1" class="ltx_text ltx_font_bold">50.7</span></td>
</tr>
<tr id="A4.T4.1.1.10" class="ltx_tr">
<td id="A4.T4.1.1.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">w/ <span id="A4.T4.1.1.10.1.1" class="ltx_text ltx_font_typewriter">code-davinci-002</span>
</td>
<td id="A4.T4.1.1.10.2" class="ltx_td ltx_border_bb"></td>
<td id="A4.T4.1.1.10.3" class="ltx_td ltx_border_bb"></td>
<td id="A4.T4.1.1.10.4" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="A4.T4.1.1.10.5" class="ltx_td ltx_border_bb"></td>
<td id="A4.T4.1.1.10.6" class="ltx_td ltx_border_bb"></td>
<td id="A4.T4.1.1.10.7" class="ltx_td ltx_border_bb"></td>
<td id="A4.T4.1.1.10.8" class="ltx_td ltx_border_bb"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="A4.T4.6.1" class="ltx_text ltx_font_bold">Validation and test results on GQA and COVR.</span> OpenAI model name (<span id="A4.T4.7.2" class="ltx_text ltx_font_typewriter">text-davinci-003</span> or <span id="A4.T4.8.3" class="ltx_text ltx_font_typewriter">code-davinci-002</span>) denotes which model was used as the question-answering model. GQA validation sample contains 2000 examples from the GQA validation set. COVR validation sample contains 1000 examples from the COVR non-paraphrased validation set. Highest scores on are in <span id="A4.T4.9.4" class="ltx_text ltx_font_bold">bold</span>.</figcaption>
</figure>
<figure id="A4.F5" class="ltx_figure"><img src="/html/2306.05392/assets/x3.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="A4.F5.3.1" class="ltx_text ltx_font_bold">GQA Results</span>. We show example results from the GQA dataset where our method <span id="A4.F5.4.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> outperforms the baseline Few-Shot PnP-VQA. 
</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Qualitative Comparisons</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">We include qualitative comparisons of our method CodeVQA to the baseline Few-shot PnP-VQA (text-davinci-003) in Fig <a href="#A4.F5" title="Figure 5 ‣ Appendix D Details on Baselines ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In all the instances, we can see that PnP-VQA produces captions that are irrelevant to the question, resulting in incorrect answers. On the other hand, <span id="A5.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> breaks down the question into a Python code block. <span id="A5.p1.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> uses if-else conditions along with the pre-defined visual modules <span id="A5.p1.1.3" class="ltx_text ltx_font_typewriter">get_pos(image, text)</span> and <span id="A5.p1.1.4" class="ltx_text ltx_font_typewriter">query(image, text)</span> to focus on the right regions of the image, arriving at the correct answer in an explainable fashion.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">Fig. <a href="#A5.F6" title="Figure 6 ‣ Appendix E Qualitative Comparisons ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows two examples from the NLVR-2 dataset where our method <span id="A5.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> answers the questions correctly. In the first example, it queries each of the images for the count of the pandas, and answers the question correctly based on that. In the second example, our method breaks the question down into three simpler queries and an <span id="A5.p2.1.2" class="ltx_text ltx_font_typewriter">if-else</span> statement to arrive at the correct answer.</p>
</div>
<figure id="A5.F6" class="ltx_figure"><img src="/html/2306.05392/assets/x4.png" id="A5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="463" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="A5.F6.3.1" class="ltx_text ltx_font_bold">NLVR2 Results</span>. We show example results from the NLVR-2 dataset of our method <span id="A5.F6.4.2" class="ltx_text ltx_font_typewriter">CodeVQA</span>.
</figcaption>
</figure>
<div id="A5.p3" class="ltx_para">
<p id="A5.p3.1" class="ltx_p">Fig. <a href="#A5.F7" title="Figure 7 ‣ Appendix E Qualitative Comparisons ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the correct results of our method on complex multireference questions in the COVR dataset. <span id="A5.p3.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> is able to break down the logic to obtain the counts of images with a cake on a white plate and images with a lemon on a white plate and then evaluates if the two counts are the same.</p>
</div>
<figure id="A5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2306.05392/assets/x5.png" id="A5.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="553" height="311" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2306.05392/assets/x6.png" id="A5.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="553" height="311" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A5.F7.2.1" class="ltx_text ltx_font_bold">COVR Results</span>. We show results on the COVR dataset where our method correctly answers the question by referencing all the images.
</figcaption>
</figure>
<div id="A5.p4" class="ltx_para">
<p id="A5.p4.1" class="ltx_p">In the second more complex example, our method uses <span id="A5.p4.1.1" class="ltx_text ltx_font_typewriter">for</span> loops and complex <span id="A5.p4.1.2" class="ltx_text ltx_font_typewriter">if-else</span> logic to first locate the images that satisfy the criterion, <em id="A5.p4.1.3" class="ltx_emph ltx_font_italic">“pillows on a couch near a table”</em> and <em id="A5.p4.1.4" class="ltx_emph ltx_font_italic">“pillows on a couch near a bed”</em> to count the individual occurrences.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Additional Quantitative Results</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">Table <a href="#A4.T4" title="Table 4 ‣ Appendix D Details on Baselines ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows results on validation sets and compares the accuracies of <span id="A6.p1.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> and Few-shot PnP-VQA when using <span id="A6.p1.1.2" class="ltx_text ltx_font_typewriter">code-davinci-002</span> and <span id="A6.p1.1.3" class="ltx_text ltx_font_typewriter">text-davinci-003</span> as the question-answering LM.</p>
</div>
<div id="A6.p2" class="ltx_para">
<p id="A6.p2.1" class="ltx_p">Table <a href="#A6.T5" title="Table 5 ‣ Appendix F Additional Quantitative Results ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows how the accuracies of <span id="A6.p2.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span> and Few-shot PnP-VQA vary with the number of shots in the prompt.</p>
</div>
<figure id="A6.T5" class="ltx_table">
<div id="A6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:159.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.7pt,-7.9pt) scale(1.10947379237636,1.10947379237636) ;">
<table id="A6.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A6.T5.1.1.1" class="ltx_tr">
<td id="A6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A6.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="A6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">Number of shots</td>
</tr>
<tr id="A6.T5.1.1.2" class="ltx_tr">
<td id="A6.T5.1.1.2.1" class="ltx_td ltx_align_center">8</td>
<td id="A6.T5.1.1.2.2" class="ltx_td ltx_align_center">12</td>
<td id="A6.T5.1.1.2.3" class="ltx_td ltx_align_center">16</td>
</tr>
<tr id="A6.T5.1.1.3" class="ltx_tr">
<td id="A6.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T5.1.1.3.1.1" class="ltx_text ltx_font_italic">text-davinci-003</span></td>
<td id="A6.T5.1.1.3.2" class="ltx_td ltx_border_t"></td>
<td id="A6.T5.1.1.3.3" class="ltx_td ltx_border_t"></td>
<td id="A6.T5.1.1.3.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A6.T5.1.1.4" class="ltx_tr">
<td id="A6.T5.1.1.4.1" class="ltx_td ltx_align_left">Few-shot PnP-VQA</td>
<td id="A6.T5.1.1.4.2" class="ltx_td ltx_align_center">48.3</td>
<td id="A6.T5.1.1.4.3" class="ltx_td ltx_align_center">49.4</td>
<td id="A6.T5.1.1.4.4" class="ltx_td ltx_align_center">49.5</td>
</tr>
<tr id="A6.T5.1.1.5" class="ltx_tr">
<td id="A6.T5.1.1.5.1" class="ltx_td ltx_align_left"><span id="A6.T5.1.1.5.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="A6.T5.1.1.5.2" class="ltx_td ltx_align_center">52.8</td>
<td id="A6.T5.1.1.5.3" class="ltx_td ltx_align_center">52.5</td>
<td id="A6.T5.1.1.5.4" class="ltx_td ltx_align_center">52.7</td>
</tr>
<tr id="A6.T5.1.1.6" class="ltx_tr">
<td id="A6.T5.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A6.T5.1.1.6.1.1" class="ltx_text ltx_font_italic">code-davinci-002</span></td>
<td id="A6.T5.1.1.6.2" class="ltx_td ltx_border_t"></td>
<td id="A6.T5.1.1.6.3" class="ltx_td ltx_border_t"></td>
<td id="A6.T5.1.1.6.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A6.T5.1.1.7" class="ltx_tr">
<td id="A6.T5.1.1.7.1" class="ltx_td ltx_align_left">Few-shot PnP-VQA</td>
<td id="A6.T5.1.1.7.2" class="ltx_td ltx_align_center">50.6</td>
<td id="A6.T5.1.1.7.3" class="ltx_td ltx_align_center">52.1</td>
<td id="A6.T5.1.1.7.4" class="ltx_td ltx_align_center">51.2</td>
</tr>
<tr id="A6.T5.1.1.8" class="ltx_tr">
<td id="A6.T5.1.1.8.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A6.T5.1.1.8.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="A6.T5.1.1.8.2" class="ltx_td ltx_align_center ltx_border_bb">55.1</td>
<td id="A6.T5.1.1.8.3" class="ltx_td ltx_align_center ltx_border_bb">55.3</td>
<td id="A6.T5.1.1.8.4" class="ltx_td ltx_align_center ltx_border_bb">55.4</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy with different numbers of shots on 2000 GQA validation examples.</figcaption>
</figure>
<div id="A6.p3" class="ltx_para">
<p id="A6.p3.1" class="ltx_p">Figure <a href="#A4.F4" title="Figure 4 ‣ Appendix D Details on Baselines ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the breakdown of accuracy by question type for 2000 GQA validation examples, which we used for initial experimentation (similar to Figure <a href="#S4.F3" title="Figure 3 ‣ 4.6 Analysis ‣ 4 Experiments ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> but on validation examples). We note that on this sample, Few-shot PnP-VQA has an advantage on “and” questions.</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Experiments with Additional Primitives</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">We also experiment with two other primitives, on datasets involving counting objects or knowledge retrieval:</p>
</div>
<section id="A7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_typewriter ltx_title_paragraph">find_object(image, object_description)</h4>

<div id="A7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A7.SS0.SSS0.Px1.p1.1" class="ltx_p">This function returns a set of references to objects in the image that match the given description, and we use it for counting objects. We implement this function using Grounding DINO <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, which is an open-vocabulary object detector that is also trained on referring expression comprehension.</p>
</div>
<div id="A7.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="A7.SS0.SSS0.Px1.p2.1" class="ltx_p">We evaluate this primitive on the VQAv2 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, for which we use only this primitive and <span id="A7.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_typewriter">query</span>, as well as the COVR and NLVR2 datasets. We used 12 in-context examples for the VQAv2 dataset. Table <a href="#A7.T6" title="Table 6 ‣ knowledge_query(question) ‣ Appendix G Experiments with Additional Primitives ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results indicating that using this module for counting rather than <span id="A7.SS0.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_typewriter">query</span> yields mixed results. Qualitatively, we observe a few reasons for errors in the <span id="A7.SS0.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_typewriter">find_object</span> version. First, the object detector is not always accurate (e.g. finding “person holding a banana” when there is a person but no banana). Second, our program may omit key details from the question (e.g. for “How many boats have people in them?” the program counts the number of boats overall). Third, our program may invoke the detector when it is ill-suited to the question (e.g. “How many blades of grass surround the fire hydrant?”). On the other hand, captions often convey the number of objects when the number is small, which is very common in these datasets, so <span id="A7.SS0.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_typewriter">query</span> can be effective on counting.</p>
</div>
</section>
<section id="A7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_typewriter ltx_title_paragraph">knowledge_query(question)</h4>

<div id="A7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A7.SS0.SSS0.Px2.p1.1" class="ltx_p">This function returns the answer to a question based on world knowledge (e.g. “Which football team has won the most Super Bowls?”). We implement this function using the same LM that is used for <span id="A7.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">query</span>. In order to better match the format of the OK-VQA dataset, we add a large negative bias to the logits of the following tokens to prevent the LM from generating them: hyphens, “to”, and <sup id="A7.SS0.SSS0.Px2.p1.1.2" class="ltx_sup"><span id="A7.SS0.SSS0.Px2.p1.1.2.1" class="ltx_text ltx_font_italic">∘</span></sup>. This choice was made based on preliminary experiments on the OK-VQA dataset.</p>
</div>
<div id="A7.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A7.SS0.SSS0.Px2.p2.1" class="ltx_p">We evaluate this primitive on the OK-VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Marino et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, for which we use only this primitive and <span id="A7.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_typewriter">query</span>. For <span id="A7.SS0.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_typewriter">CodeVQA</span> and Few-shot VQA, we used 7 in-context examples to be consistent with the OK-VQA results of ViperGPT <cite class="ltx_cite ltx_citemacro_citep">(Surís et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. Table <a href="#A7.T7" title="Table 7 ‣ knowledge_query(question) ‣ Appendix G Experiments with Additional Primitives ‣ Modular Visual Question Answering via Code Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides the results, showing that for questions involving both visual information and general knowledge, breaking down the questions in this way does not lead to improved accuracy.</p>
</div>
<div id="A7.SS0.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="A7.SS0.SSS0.Px2.p3.1" class="ltx_p">For both VQAv2 and OK-VQA, we use the standard evaluation method associated with the VQAv2 dataset, which takes into account the set of ground-truth answers for each question. The Flamingo <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> results that we report on both datasets used 32 in-context examples.</p>
</div>
<figure id="A7.T6" class="ltx_table">
<div id="A7.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:143.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.3pt,9.5pt) scale(0.883206327273157,0.883206327273157) ;">
<table id="A7.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A7.T6.1.1.1" class="ltx_tr">
<td id="A7.T6.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="A7.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VQAv2</td>
<td id="A7.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">COVR</td>
<td id="A7.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">NLVR2</td>
</tr>
<tr id="A7.T6.1.1.2" class="ltx_tr">
<td id="A7.T6.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T6.1.1.2.1.1" class="ltx_text ltx_font_bold">Zero-shot</span></td>
<td id="A7.T6.1.1.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A7.T6.1.1.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A7.T6.1.1.2.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A7.T6.1.1.3" class="ltx_tr">
<td id="A7.T6.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">BLIP-v2</td>
<td id="A7.T6.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r">65.0</td>
<td id="A7.T6.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="A7.T6.1.1.3.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="A7.T6.1.1.4" class="ltx_tr">
<td id="A7.T6.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T6.1.1.4.1.1" class="ltx_text ltx_font_bold">Few-shot</span></td>
<td id="A7.T6.1.1.4.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A7.T6.1.1.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A7.T6.1.1.4.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A7.T6.1.1.5" class="ltx_tr">
<td id="A7.T6.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">Flamingo</td>
<td id="A7.T6.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">67.6‡</td>
<td id="A7.T6.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="A7.T6.1.1.5.4" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="A7.T6.1.1.6" class="ltx_tr">
<td id="A7.T6.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">Few-shot PnP-VQA</td>
<td id="A7.T6.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r">66.84</td>
<td id="A7.T6.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r">47.8</td>
<td id="A7.T6.1.1.6.4" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="A7.T6.1.1.7" class="ltx_tr">
<td id="A7.T6.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A7.T6.1.1.7.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="A7.T6.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="A7.T6.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r">52.9</td>
<td id="A7.T6.1.1.7.4" class="ltx_td ltx_align_center">64.0</td>
</tr>
<tr id="A7.T6.1.1.8" class="ltx_tr">
<td id="A7.T6.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A7.T6.1.1.8.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="A7.T6.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r">66.63</td>
<td id="A7.T6.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r">52.9</td>
<td id="A7.T6.1.1.8.4" class="ltx_td ltx_align_center">66.0</td>
</tr>
<tr id="A7.T6.1.1.9" class="ltx_tr">
<td id="A7.T6.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">w/ <span id="A7.T6.1.1.9.1.1" class="ltx_text ltx_font_typewriter">find_object</span>
</td>
<td id="A7.T6.1.1.9.2" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="A7.T6.1.1.9.3" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="A7.T6.1.1.9.4" class="ltx_td ltx_border_bb"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results with <span id="A7.T6.3.1" class="ltx_text ltx_font_typewriter">find_object</span> used for counting objects on VQAv2 (a random sample of 4000 examples from validation set), COVR (validation), and NLVR2 (test-public). ‡indicates a result on the full VQAv2 test-dev set, which may not be directly comparable with our results on a sample of the validation set.</figcaption>
</figure>
<figure id="A7.T7" class="ltx_table">
<div id="A7.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:202.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(21.8pt,-20.4pt) scale(1.25237273602906,1.25237273602906) ;">
<table id="A7.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A7.T7.1.1.1" class="ltx_tr">
<td id="A7.T7.1.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="A7.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">OK-VQA</td>
</tr>
<tr id="A7.T7.1.1.2" class="ltx_tr">
<td id="A7.T7.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T7.1.1.2.1.1" class="ltx_text ltx_font_bold">Zero-shot</span></td>
<td id="A7.T7.1.1.2.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A7.T7.1.1.3" class="ltx_tr">
<td id="A7.T7.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r">BLIP-v2</td>
<td id="A7.T7.1.1.3.2" class="ltx_td ltx_align_center">45.9</td>
</tr>
<tr id="A7.T7.1.1.4" class="ltx_tr">
<td id="A7.T7.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T7.1.1.4.1.1" class="ltx_text ltx_font_bold">Few-shot</span></td>
<td id="A7.T7.1.1.4.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A7.T7.1.1.5" class="ltx_tr">
<td id="A7.T7.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">Flamingo</td>
<td id="A7.T7.1.1.5.2" class="ltx_td ltx_align_center">57.8</td>
</tr>
<tr id="A7.T7.1.1.6" class="ltx_tr">
<td id="A7.T7.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">ViperGPT</td>
<td id="A7.T7.1.1.6.2" class="ltx_td ltx_align_center">51.9</td>
</tr>
<tr id="A7.T7.1.1.7" class="ltx_tr">
<td id="A7.T7.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r">Few-shot PnP-VQA</td>
<td id="A7.T7.1.1.7.2" class="ltx_td ltx_align_center">54.1</td>
</tr>
<tr id="A7.T7.1.1.8" class="ltx_tr">
<td id="A7.T7.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="A7.T7.1.1.8.1.1" class="ltx_text ltx_font_typewriter">CodeVQA</span></td>
<td id="A7.T7.1.1.8.2" class="ltx_td ltx_align_center">53.5</td>
</tr>
<tr id="A7.T7.1.1.9" class="ltx_tr">
<td id="A7.T7.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">w/ <span id="A7.T7.1.1.9.1.1" class="ltx_text ltx_font_typewriter">knowledge_query</span>
</td>
<td id="A7.T7.1.1.9.2" class="ltx_td ltx_border_bb"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results with <span id="A7.T7.3.1" class="ltx_text ltx_font_typewriter">knowledge_query</span> on the OK-VQA validation set.</figcaption>
</figure>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Licenses and Other Dataset Details</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">GQA is licensed under the CC-BY-4.0 license (<a target="_blank" href="https://creativecommons.org/licenses/by/4.0/" title="" class="ltx_ref ltx_href">https://creativecommons.org/licenses/by/4.0/</a>). The COVR repository (<a target="_blank" href="https://github.com/benbogin/covr-dataset" title="" class="ltx_ref ltx_href">https://github.com/benbogin/covr-dataset</a>) is licensed under an MIT license (though imSitu images may not be licensed). The text in both datasets is written in English. The annotations in NLVR2 are licensed under CC-BY-4.0, but the images in the dataset are not licensed. The annotations in VQAv2 are licensed under CC-BY-4.0.</p>
</div>
<div id="A8.p2" class="ltx_para ltx_noindent">
<p id="A8.p2.1" class="ltx_p">The testdev set of GQA contains 12578 instances. The test set of COVR contains 7024 instances. The validation set of COVR contains 6891 instances. The public test set of NLVR2 contains 6967 instances. The validation set of OK-VQA contains 5046 instances. For VQAv2, we evaluate on a random sample of 4000 examples from the validation set.</p>
</div>
<div id="A8.p3" class="ltx_para ltx_noindent">
<p id="A8.p3.1" class="ltx_p">During the development and intermediate evaluations of our method, we evaluated on a random sample of 200 training examples and a random sample of 2000 validation examples from GQA, a random sample of 200 training examples and the validation set from COVR, a random sample of 2000 training examples from NLVR2, a random sample of 1200 training examples from OK-VQA, and a random sample of 2200 training examples from VQAv2.</p>
</div>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Ethics and Impact Statement</h2>

<div id="A9.p1" class="ltx_para">
<p id="A9.p1.1" class="ltx_p">One goal of our work is to decrease the need for (re-)training VQA systems. Achieving this goal would mean a decrease in carbon emissions from training models. However, our approach also has a high inference cost, given the use of large language models. A decision to employ our approach should take into consideration this computational cost and the associated environmental impact.</p>
</div>
<div id="A9.p2" class="ltx_para">
<p id="A9.p2.1" class="ltx_p">Another potential positive impact of our approach is improved interpretability via the generated programs. These programs offer to people familiar with Python a record of which visual tasks the system uses for a given question and how the system combines the outputs of these tasks to predict the answer.</p>
</div>
<div id="A9.p3" class="ltx_para">
<p id="A9.p3.1" class="ltx_p">Our system relies on pre-trained vision-language models to predict answers to visual questions. Prior work <cite class="ltx_cite ltx_citemacro_citep">(Ross et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>; Agarwal et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> has found evidence of social biases in vision-language models trained on image-captions. Therefore, our system may exhibit these biases as well. Practitioners should be aware of this risk and ideally should take steps to mitigate this risk when they consider deploying this system in ways that can impact human lives.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.05390" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.05392" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.05392">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.05392" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.05393" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 01:26:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
