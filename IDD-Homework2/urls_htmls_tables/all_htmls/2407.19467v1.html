<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights</title>
<!--Generated on Sun Jul 28 11:28:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Multimodal Representations,  Recommendation System" lang="en" name="keywords"/>
<base href="/html/2407.19467v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S1" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S2" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pre-Training of Multimodal Representations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.SS1" title="In 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Semantic-Aware Contrastive Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.SS2" title="In 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Construction of Pre-Training Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.SS3" title="In 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Integration with Recommendation Models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.SS1" title="In 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Observations and Insights</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.SS2" title="In 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Method I: <span class="ltx_text ltx_font_smallcaps">SimTier</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.SS3" title="In 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Method II: Multimodal Knowledage Extractor (<span class="ltx_text ltx_font_smallcaps">MAKE</span>)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S5" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>INDUSTRIAL DESIGN for Online Deployment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1" title="In 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1.SSS1" title="In 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1.SSS2" title="In 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Compared Pre-training Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1.SSS3" title="In 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Compared Recommendation Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1.SSS4" title="In 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS2" title="In 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Performance on Pre-Training Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS2.SSS1" title="In 6.2. Performance on Pre-Training Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Rationale for Utilizing Accuracy to Evaluate the Effectiveness of Pre-trained Representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS2.SSS2" title="In 6.2. Performance on Pre-Training Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Importance of Semantic-Aware Contrastive Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS3" title="In 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Performance on CTR prediction Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS3.SSS1" title="In 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>
Performance on Different Integration Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS3.SSS2" title="In 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Performance on Different Training Epochs of <span class="ltx_text ltx_font_smallcaps">MAKE</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS3.SSS3" title="In 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.3 </span>Performance on Infrequent Items.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS4" title="In 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Online Performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S7" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S8" title="In Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion and Discussion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiang-Rong Sheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xiangrong.sxr@alibaba-inc.com">xiangrong.sxr@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Feifan Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yangfeifan.yff@alibaba-inc.com">yangfeifan.yff@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Litong Gong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gonglitong.glt@alibaba-inc.com">gonglitong.glt@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Biao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:eric.wb@alibaba-inc.com">eric.wb@alibaba-inc.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Alibaba Group</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhangming Chan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangming.czm@alibaba-inc.com">zhangming.czm@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yujing Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jinghan.zyj@alibaba-inc.com">jinghan.zyj@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yueyao Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yueyao.syy@alibaba-inc.com">yueyao.syy@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong-Nan Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yongnan.zy@alibaba-inc.com">yongnan.zy@alibaba-inc.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Alibaba Group</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiezheng Ge
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tiezheng.gtz@alibaba-inc.com">tiezheng.gtz@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Han Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhuhan.zh@alibaba-inc.com">zhuhan.zh@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuning Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mengzhu.jyn@alibaba-inc.com">mengzhu.jyn@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xiyu.xj@alibaba-inc.com">xiyu.xj@alibaba-inc.com</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bozheng@alibaba-inc.com">bozheng@alibaba-inc.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Alibaba Group</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Despite the recognized potential of multimodal data to improve model accuracy, many large-scale industrial recommendation systems, including Taobao display advertising system, predominantly depend on sparse ID features in their models.
In this work, we explore approaches to leverage multimodal data to enhance the recommendation accuracy. We start from identifying the key challenges in adopting multimodal data in a manner that is both effective and cost-efficient for industrial systems.
To address these challenges, we introduce a two-phase framework, including: 1) the pre-training of multimodal representations to capture semantic similarity, and 2) the integration of these representations with existing ID-based models.
Furthermore, we detail the architecture of our production system, which is designed to facilitate the deployment of multimodal representations.
Since the integration of multimodal representations in mid-2023, we have observed significant performance improvements in Taobao display advertising system. We believe that the insights we have gathered will serve as a valuable resource for practitioners seeking to leverage multimodal data in their systems.</p>
</div>
<div class="ltx_keywords">Multimodal Representations, Recommendation System
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21â€“25, 2024; Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM â€™24), October 21â€“25, 2024, Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3627673.3680068</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>979-8-4007-0436-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Information retrieval</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Traditionally, the recommendation models employed in Taobaoâ€™s display advertising system, as with many other industrial systems, have largely relied on discrete IDs as features. Despite their widespread use, ID-based models have intrinsic drawbacks, such as the inability to capture the semantic information contained within multimodal data.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address these issues, certain industrial systems have attempted to incorporate multimodal data into the ID-based modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib30" title="">2023</a>; Singh etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib27" title="">2023</a>)</cite>.
Typically, these approaches employ a two-phase framework, including 1) acquiring multimodal representations through either generic or scenario-specific pre-training, and 2) integrating these representations into the recommendation models.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="284" id="S1.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>An overview of our two-phase framework: the pre-training of multimodal representations, followed by the integration of pre-trained representations into recommendation models. In the first phase (refer to Figure (a)), we undertake pre-training through semantic-aware contrastive learning. This method equips the multimodal representations with ability to identify semantic similar items. Subsequently, in the second phase (refer to Figure (b)), we introduce our proposed <span class="ltx_text ltx_font_smallcaps" id="S1.F1.3.1">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S1.F1.4.2">make</span> methods to effectively incorporate the pre-trained multimodal representations into the recommendation models.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite these advancements, a significant number of industrial systems still depend exclusively on ID features. This is often attributed to the concern that the performance gains from multimodal data might not compensate for the costs involved in their deployment. These costs encompass pre-training multimodal encoders, incrementally generating representations for new items, and other necessary upgrades to both online servers and near-line training systems.
Therefore, the successful integration of multimodal representations hinges on the ability to boost their performance benefits while concurrently minimizing deployment costs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To accomplish these two key objectives, three practical challenges should be addressed:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Design of the pre-training task.</span> The effectiveness of multimodal representations in enhancing performance hinges on their ability to provide meaningful semantic information, which is difficult for ID features to capture. It is essential to design pre-training tasks that enable multimodal representations to encapsulate such semantic information.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Integration of multimodal representation.</span>
The inherent discrepancies between ID features and multimodal representations, such as difference in training epochs, calls for approaches that can effectively incorporating multimodal representations into the ID-based model. These approaches should leverage the strengths of each feature type to enhance the modelâ€™s overall performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Design of the production system.</span> The entire production workflow, including the generation of multimodal representations for new items and their up-to-date application in downstream tasks, should be designed with efficiency.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we adopt a two-phase framework as depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">1</span></a>. Specifically, during the pre-training phase, we propose the <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">S</span>emantic-aware <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">C</span>ontrastive <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">L</span>earning (SCL) method. In this phase, we utilize the userâ€™s search query and subsequent purchase action to construct semantically similar sample pairs, capturing the dimensions of semantic similarity that are most relevant to users in e-commerce scenarios. For negative samples, we draw from a large memory bank. The SCL method enables multimodal representations to effectively measure semantic similarities among items.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Upon obtaining high-quality multimodal representations, we propose two approaches to incorporate these representations into the existing ID-based model. Firstly, we develop an approach named <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">SimTier</span> to measure the degree of similarity between the target item and items the user has previously interacted with. The resulting <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">SimTier</span> vector is then concatenated with other embeddings and fed into the subsequent layers. Furthermore, to address the discrepancy in training epochs between multimodal representations and ID embeddings, we introduce the <span class="ltx_text ltx_font_bold" id="S1.p6.1.3">M</span>ultimod<span class="ltx_text ltx_font_bold" id="S1.p6.1.4">A</span>l <span class="ltx_text ltx_font_bold" id="S1.p6.1.5">K</span>nowledge <span class="ltx_text ltx_font_bold" id="S1.p6.1.6">E</span>xtractor (<span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.7">MAKE</span>) module. The <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.8">MAKE</span> module separates the optimization of parameters associated with multimodal representations from those of the ID-based model, enabling more effective learning for the parameters related to multimodal representations.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We also present the design of our production system that facilitates the deployment of multimodal representations. Specifically, the system generates multimodal representations for newly introduced items in real-time and ensures these representations are immediately available for the training infrastructure and the online prediction server. This design achieves minimal latencyâ€”merely a few seconds between the introduction of an item and the modelâ€™s use of its multimodal representation. Since mid-2023, multimodal representations have been deployed in Taobao display advertising system, leading to significant performance improvements.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Preliminaries</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Before delving into the specifics, we first introduce the typical ID-based model structure utilized in different stages (including retrievalÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhu
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib39" title="">2018</a>; Huang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib16" title="">2020</a>)</cite>, pre-rankingÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib28" title="">2020</a>; Zhao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib35" title="">2023</a>)</cite>, and ranking stagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Covington
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib8" title="">2016</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib37" title="">2018</a>; Bian etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib2" title="">2022</a>; Gu
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib11" title="">2021</a>; Chan etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib4" title="">2023</a>)</cite>) of industrial system.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">ID Features in Recommendation Models:</span> Common recommendation models are trained on large-scale datasets comprising billions of ID featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib17" title="">2019</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib33" title="">2022b</a>)</cite>. These ID features serve to represent users profiles, user historical interacted items, the target item (to be predicted), and the contextual information. For example, we can represent the target item with its respective item ID and category ID and represent user historical interacted items through a sequence of corresponding item IDs and category IDs.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Structure of ID-based Model:</span> The ID-based recommendation model follows an embedding and MLP (Multi-Layer Perceptron) architecture, which typically incorporates <span class="ltx_text ltx_font_italic" id="S2.p3.1.2">historical behavior modeling</span> modulesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib37" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib36" title="">2019</a>)</cite>. Initially, all ID features are converted into embeddings. The historical behavior modeling modules then measures a userâ€™s interest towards the target item by analyzing the relevance between the embedding of target item and embeddings of the userâ€™s historical interacted items. Specifically, the modules produce fixed-length vectors by aggregating the embeddings of the target item and those of the historical interacted items. These vectors are then concatenated with other ID embeddings to form the input for subsequent MLP, which produce the final prediction.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Pre-Training of Multimodal Representations</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As for the multimodal data, its utilization can improve historical behavior modeling. Specifically, multimodal data can be used to measure the semantic similarity between the target item and usersâ€™ historical interacted items. Take item images as an example, they can be used to measure the visual similarity between the image of the target item and those of historical interacted items. Intuitively, a higher semantic similarity indicates a stronger resemblance between the target item and the usersâ€™ historical behavior, suggesting a higher likelihood of the userâ€™s interest. This insight emphasizes the importance of designing a pre-training task tailored to enable multimodal representations to effectively discern the semantic similarity across item pairs.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Semantic-Aware Contrastive Learning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To derive representations with the capability to discern semantic similarity, we propose the semantic-aware contrastive learning (SCL) method that attracts the semantically similar sample pairs and repulses the dissimilar sample pairs. To accomplish this, it is essential to define semantically similar and dissimilar pairs for supervision.
To understand the importance of this point, consider the example shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.F5" title="Figure 5 â€£ 6.1.2. Compared Pre-training Methods â€£ 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">5</span></a>, where the three pillows are almost identical yet display slight variances, like differences in patterns or minor appearance discrepancies. If the definition of semantically similar pairs is inadequate, the representation might fail to capture such subtle differences. Indeed, these slight distinctions are frequently missed by representations focused on capturing general concepts.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the following, we will elaborate our construction of pre-training dataset tailored for e-commerce scenario and the optimization strategies of contrastive learning process.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Construction of Pre-Training Dataset</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In the context of e-commerce, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">a userâ€™s search query and subsequent purchase action</span> often signifies a strong semantic similarity between the query and the purchased item. For example, if a user searches for an image of a pillow and subsequently purchases a pillow, this sequence of actions indicates that the two images (the queried image and the image of the purchased item) are semantically similar enough to satisfy the userâ€™s purchase intentions. Thus, as shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.T1" title="Table 1 â€£ 3.2. Construction of Pre-Training Dataset â€£ 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">1</span></a>, in training the text encoder, we pair the text of the userâ€™s search query with the title of the item they ultimately purchased as the semantically similar pair. Similarly, for the image modality, userâ€™s image query (obtained from the image search scenario of Taobao) is paired with the image of the subsequent purchased item. This pairing strategy naturally captures the dimensions of semantic similarity that are most relevant to users in e-commerce scenarios, reflecting the elements that influence their purchasing decisions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For each semantically similar pair, we consider all other samples as potential dissimilar samples, which can be achieved by using the samples in the current mini-batch as negatives. To further improve the model performance, we aim to increase the number of negative samples available during training. Specifically, we draw inspiration from MoCoÂ <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib13" title="">2020</a>)</cite> and adopt a technique that updates the model with momentum, facilitating sampling more negatives from a larger memory bank.
More sophisticated strategies for constructing negative pairs, such as identifying hard negativesÂ <cite class="ltx_cite ltx_citemacro_citep">(Schroff
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib24" title="">2015</a>)</cite>, will be elaborated in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS1.SSS2" title="6.1.2. Compared Pre-training Methods â€£ 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">6.1.2</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>The construction of semantically similar pair for pre-training.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1">modality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2">semantically similar pair</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.1.1">Image</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2">Â¡userâ€™s image query, image of the purchased itemÂ¿</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.3.2.1">Text</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.3.2.2">Â¡userâ€™s text query, title of the purchased itemÂ¿</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S3.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>An illustration of the proposed <span class="ltx_text ltx_font_smallcaps" id="S3.F2.3.1">SimTier</span> and Multimodal Knowledge Extractor (<span class="ltx_text ltx_font_smallcaps" id="S3.F2.4.2">MAKE</span>) approaches, with details provided in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.SS2" title="4.2. Method I: SimTier â€£ 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">4.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.SS3" title="4.3. Method II: Multimodal Knowledage Extractor (MAKE) â€£ 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">4.3</span></a>, respectively.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Optimization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.6">We utilize the well-regarded InfoNCE lossÂ <cite class="ltx_cite ltx_citemacro_citep">(Oord
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib20" title="">2018</a>)</cite> as the loss function. Given an encoded query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_q</annotation></semantics></math> and its corresponding encoded positive sample <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_k</annotation></semantics></math>, along with <math alttext="{k_{0},k_{1},\dots,k_{K}}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.4"><semantics id="S3.SS3.p1.3.m3.4a"><mrow id="S3.SS3.p1.3.m3.4.4.3" xref="S3.SS3.p1.3.m3.4.4.4.cmml"><msub id="S3.SS3.p1.3.m3.2.2.1.1" xref="S3.SS3.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS3.p1.3.m3.2.2.1.1.2" xref="S3.SS3.p1.3.m3.2.2.1.1.2.cmml">k</mi><mn id="S3.SS3.p1.3.m3.2.2.1.1.3" xref="S3.SS3.p1.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS3.p1.3.m3.4.4.3.4" xref="S3.SS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.3.m3.3.3.2.2" xref="S3.SS3.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS3.p1.3.m3.3.3.2.2.2" xref="S3.SS3.p1.3.m3.3.3.2.2.2.cmml">k</mi><mn id="S3.SS3.p1.3.m3.3.3.2.2.3" xref="S3.SS3.p1.3.m3.3.3.2.2.3.cmml">1</mn></msub><mo id="S3.SS3.p1.3.m3.4.4.3.5" xref="S3.SS3.p1.3.m3.4.4.4.cmml">,</mo><mi id="S3.SS3.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS3.p1.3.m3.1.1.cmml">â€¦</mi><mo id="S3.SS3.p1.3.m3.4.4.3.6" xref="S3.SS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS3.p1.3.m3.4.4.3.3" xref="S3.SS3.p1.3.m3.4.4.3.3.cmml"><mi id="S3.SS3.p1.3.m3.4.4.3.3.2" xref="S3.SS3.p1.3.m3.4.4.3.3.2.cmml">k</mi><mi id="S3.SS3.p1.3.m3.4.4.3.3.3" xref="S3.SS3.p1.3.m3.4.4.3.3.3.cmml">K</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.4b"><list id="S3.SS3.p1.3.m3.4.4.4.cmml" xref="S3.SS3.p1.3.m3.4.4.3"><apply id="S3.SS3.p1.3.m3.2.2.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS3.p1.3.m3.2.2.1.1.2">ğ‘˜</ci><cn id="S3.SS3.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.2.2.1.1.3">0</cn></apply><apply id="S3.SS3.p1.3.m3.3.3.2.2.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS3.p1.3.m3.3.3.2.2.2">ğ‘˜</ci><cn id="S3.SS3.p1.3.m3.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.3.3.2.2.3">1</cn></apply><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">â€¦</ci><apply id="S3.SS3.p1.3.m3.4.4.3.3.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.4.4.3.3.1.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S3.SS3.p1.3.m3.4.4.3.3.2.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.2">ğ‘˜</ci><ci id="S3.SS3.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS3.p1.3.m3.4.4.3.3.3">ğ¾</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.4c">{k_{0},k_{1},\dots,k_{K}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.4d">italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_k start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT</annotation></semantics></math> representing the set of encoded sample representations in the memory bank, where <math alttext="K" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_K</annotation></semantics></math> denotes the memory bank sizeÂ <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib13" title="">2020</a>)</cite>, the InfoNCE loss employs the dot product to measure similarity (with all representations being L2 normalized). As shown in EquationÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.E1" title="In 3.3. Optimization â€£ 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">1</span></a>, the loss value decreases when query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_q</annotation></semantics></math> closely matches its designated positive sample <math alttext="k" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_k</annotation></semantics></math> and diverges from all other samples within the memory bank.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\text{InfoNCE}}=-\log\frac{\exp(q\cdot k_{+})/\tau}{\sum_{i=0}^{K}\exp(q%
\cdot k_{i})/\tau}." class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msub id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml">L</mi><mtext id="S3.E1.m1.5.5.1.1.2.3" xref="S3.E1.m1.5.5.1.1.2.3a.cmml">InfoNCE</mtext></msub><mo id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mo id="S3.E1.m1.5.5.1.1.3a" rspace="0.167em" xref="S3.E1.m1.5.5.1.1.3.cmml">âˆ’</mo><mrow id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2.1" xref="S3.E1.m1.5.5.1.1.3.2.1.cmml">log</mi><mo id="S3.E1.m1.5.5.1.1.3.2a" lspace="0.167em" xref="S3.E1.m1.5.5.1.1.3.2.cmml">â¡</mo><mfrac id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">exp</mi><mo id="S3.E1.m1.2.2.2.2.1a" xref="S3.E1.m1.2.2.2.2.2.cmml">â¡</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">q</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml">â‹…</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.1.3.2.cmml">k</mi><mo id="S3.E1.m1.2.2.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.3.cmml">+</mo></msub></mrow><mo id="S3.E1.m1.2.2.2.2.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">/</mo><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">Ï„</mi></mrow><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><msubsup id="S3.E1.m1.4.4.4.3" xref="S3.E1.m1.4.4.4.3.cmml"><mo id="S3.E1.m1.4.4.4.3.2.2" xref="S3.E1.m1.4.4.4.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.4.4.4.3.2.3" xref="S3.E1.m1.4.4.4.3.2.3.cmml"><mi id="S3.E1.m1.4.4.4.3.2.3.2" xref="S3.E1.m1.4.4.4.3.2.3.2.cmml">i</mi><mo id="S3.E1.m1.4.4.4.3.2.3.1" xref="S3.E1.m1.4.4.4.3.2.3.1.cmml">=</mo><mn id="S3.E1.m1.4.4.4.3.2.3.3" xref="S3.E1.m1.4.4.4.3.2.3.3.cmml">0</mn></mrow><mi id="S3.E1.m1.4.4.4.3.3" xref="S3.E1.m1.4.4.4.3.3.cmml">K</mi></msubsup><mrow id="S3.E1.m1.4.4.4.2" xref="S3.E1.m1.4.4.4.2.cmml"><mrow id="S3.E1.m1.4.4.4.2.1.1" xref="S3.E1.m1.4.4.4.2.1.2.cmml"><mi id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">exp</mi><mo id="S3.E1.m1.4.4.4.2.1.1a" xref="S3.E1.m1.4.4.4.2.1.2.cmml">â¡</mo><mrow id="S3.E1.m1.4.4.4.2.1.1.1" xref="S3.E1.m1.4.4.4.2.1.2.cmml"><mo id="S3.E1.m1.4.4.4.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.2.cmml">(</mo><mrow id="S3.E1.m1.4.4.4.2.1.1.1.1" xref="S3.E1.m1.4.4.4.2.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.2.1.1.1.1.2" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml">q</mi><mo id="S3.E1.m1.4.4.4.2.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1.cmml">â‹…</mo><msub id="S3.E1.m1.4.4.4.2.1.1.1.1.3" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.4.4.4.2.1.1.1.1.3.2" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3.2.cmml">k</mi><mi id="S3.E1.m1.4.4.4.2.1.1.1.1.3.3" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.4.4.4.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.4.2.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.4.2.2" xref="S3.E1.m1.4.4.4.2.2.cmml">/</mo><mi id="S3.E1.m1.4.4.4.2.3" xref="S3.E1.m1.4.4.4.2.3.cmml">Ï„</mi></mrow></mrow></mfrac></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" lspace="0em" xref="S3.E1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"></eq><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2">ğ¿</ci><ci id="S3.E1.m1.5.5.1.1.2.3a.cmml" xref="S3.E1.m1.5.5.1.1.2.3"><mtext id="S3.E1.m1.5.5.1.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.5.5.1.1.2.3">InfoNCE</mtext></ci></apply><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><minus id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3"></minus><apply id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2"><log id="S3.E1.m1.5.5.1.1.3.2.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2.1"></log><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><divide id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><divide id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></divide><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><exp id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"></exp><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1">â‹…</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">ğ‘</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3.2">ğ‘˜</ci><plus id="S3.E1.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3.3"></plus></apply></apply></apply><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">ğœ</ci></apply><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><apply id="S3.E1.m1.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.3.1.cmml" xref="S3.E1.m1.4.4.4.3">superscript</csymbol><apply id="S3.E1.m1.4.4.4.3.2.cmml" xref="S3.E1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.3.2.1.cmml" xref="S3.E1.m1.4.4.4.3">subscript</csymbol><sum id="S3.E1.m1.4.4.4.3.2.2.cmml" xref="S3.E1.m1.4.4.4.3.2.2"></sum><apply id="S3.E1.m1.4.4.4.3.2.3.cmml" xref="S3.E1.m1.4.4.4.3.2.3"><eq id="S3.E1.m1.4.4.4.3.2.3.1.cmml" xref="S3.E1.m1.4.4.4.3.2.3.1"></eq><ci id="S3.E1.m1.4.4.4.3.2.3.2.cmml" xref="S3.E1.m1.4.4.4.3.2.3.2">ğ‘–</ci><cn id="S3.E1.m1.4.4.4.3.2.3.3.cmml" type="integer" xref="S3.E1.m1.4.4.4.3.2.3.3">0</cn></apply></apply><ci id="S3.E1.m1.4.4.4.3.3.cmml" xref="S3.E1.m1.4.4.4.3.3">ğ¾</ci></apply><apply id="S3.E1.m1.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.2"><divide id="S3.E1.m1.4.4.4.2.2.cmml" xref="S3.E1.m1.4.4.4.2.2"></divide><apply id="S3.E1.m1.4.4.4.2.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1"><exp id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1"></exp><apply id="S3.E1.m1.4.4.4.2.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1"><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.1">â‹…</ci><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.2">ğ‘</ci><apply id="S3.E1.m1.4.4.4.2.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3.2">ğ‘˜</ci><ci id="S3.E1.m1.4.4.4.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.4.2.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply><ci id="S3.E1.m1.4.4.4.2.3.cmml" xref="S3.E1.m1.4.4.4.2.3">ğœ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">L_{\text{InfoNCE}}=-\log\frac{\exp(q\cdot k_{+})/\tau}{\sum_{i=0}^{K}\exp(q%
\cdot k_{i})/\tau}.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_L start_POSTSUBSCRIPT InfoNCE end_POSTSUBSCRIPT = - roman_log divide start_ARG roman_exp ( italic_q â‹… italic_k start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ) / italic_Ï„ end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( italic_q â‹… italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_Ï„ end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.8">Here, <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m1.1"><semantics id="S3.SS3.p1.7.m1.1a"><mi id="S3.SS3.p1.7.m1.1.1" xref="S3.SS3.p1.7.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m1.1b"><ci id="S3.SS3.p1.7.m1.1.1.cmml" xref="S3.SS3.p1.7.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m1.1d">italic_Ï„</annotation></semantics></math> represents a learnable temperature parameter. For our experiments, we set the value of <math alttext="K" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m2.1"><semantics id="S3.SS3.p1.8.m2.1a"><mi id="S3.SS3.p1.8.m2.1.1" xref="S3.SS3.p1.8.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m2.1b"><ci id="S3.SS3.p1.8.m2.1.1.cmml" xref="S3.SS3.p1.8.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m2.1d">italic_K</annotation></semantics></math> to 196,800.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">By this means, the SCL method enables the representations to have the ability to discern fine differences between comparable items, which are crucial for recommendation models.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Integration with Recommendation Models</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">A direct method to integrate multimodal representations into an ID-based recommendation model involves concatenating these multimodal representations with the ID embeddings for both the target item and usersâ€™ past interacted itemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Ge etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib10" title="">2018</a>; Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib30" title="">2023</a>)</cite>. This concatenation is followed by the utilization of user behavior modeling modules, which are subsequently input into the MLP for the final prediction.
Although this method is straightforward, we find that the performance improvements are modest. To investigate this matter, we share our observations and insights.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Observations and Insights</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We begin by sharing our observations and insights on incorporating multimodal representations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Observation 1: simplifying the usage of multimodal representations improves performances.</span>
Our research reveals that the direct integration of multimodal representations into the ID-based model does not yield optimal performance, as explained in more detail in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.SS3" title="6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">6.3</span></a>. This issue arises because the parameters associated with multimodal representations, e.g., the parameters of the MLP that are connected with multimodal representations, are not adequately learned during the joint training process with the ID embeddingsÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib30" title="">2023</a>)</cite>. In contrast, strategies that simplify the usage of multimodal representations, for instance, transforming them into semantic IDs (thereby representing the embedding vectors with IDs)Â <cite class="ltx_cite ltx_citemacro_citep">(Singh etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib27" title="">2023</a>; Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib30" title="">2023</a>)</cite>, appear to offer improved performance.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Observation 2: ID-based and multimodal-based models have training epoch discrepancy.</span>
In industrial scenario, ID-based models are typically trained for only one epoch to avoid overfittingÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib34" title="">2022c</a>)</cite>. In contrast, models in CV and NLP field often undergo training over multiple epochs.
A natural question arises: how many training epochs are ideal for a multimodal-based recommendation model? To answer this question, we developed a recommendation model that exclusively utilizes multimodal representations as input features, without any ID features, and analyzed how its performance varied with the number of training epochs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">The detailed convergence curve is illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.F3" title="Figure 3 â€£ 4.1. Observations and Insights â€£ 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">3</span></a>. We find that model leveraging multimodal data benefits from training across multiple epochs on the same dataset, with its performance showing notable enhancements as the number of epochs increases. In contrast, ID-based models suffer from the one-epoch overfitting phenomenon, where model performance dramatically degrades at the beginning of the second epochÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib34" title="">2022c</a>)</cite>.
The result suggests that the parameters associated with multimodal representations require more epochs to converge properly, which contrasts with the behavior of the ID-based model.
<span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Consequently, when incorporating multimodal representations into an ID-based model, and trained over only one epoch, there is a risk that the multimodal-related parameters may not be sufficiently trained.</span></p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="439" id="S4.F3.g1" src="x3.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The multimodal-based CTR prediction model (MM) demonstrates a continuous increase in GAUC after several training epochs. In contrast, the ID-based model (ID) shows a sharp decline in GAUC during testing after the second epoch of training.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Method I: <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.1.1">SimTier</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">The observation 1 calls for simplifying the usage of multimodal representations. To this end, we propose a straightforward yet effective method <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.2.1">SimTier</span>. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.F2" title="Figure 2 â€£ 3.2. Construction of Pre-Training Dataset â€£ 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">2</span></a> (a), <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p1.2.2">SimTier</span> begins by computing the dot product similarity between the multimodal representation of the target candidate item, denoted as <math alttext="v_{c}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">v</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">ğ‘£</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">v_{c}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, and the multimodal representations of the userâ€™s historically interacted items <math alttext="\{v_{i}\}_{i=1}^{L}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><msubsup id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mrow id="S4.SS2.p1.2.m2.1.1.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.1.2.cmml"><mo id="S4.SS2.p1.2.m2.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p1.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S4.SS2.p1.2.m2.1.1.1.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1.1.1.1.2" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1.2.cmml">v</mi><mi id="S4.SS2.p1.2.m2.1.1.1.1.1.1.3" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS2.p1.2.m2.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p1.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS2.p1.2.m2.1.1.1.3" xref="S4.SS2.p1.2.m2.1.1.1.3.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p1.2.m2.1.1.1.3.1" xref="S4.SS2.p1.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1">superscript</csymbol><apply id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><set id="S4.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.1"><apply id="S4.SS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S4.SS2.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S4.SS2.p1.2.m2.1.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.1.3"><eq id="S4.SS2.p1.2.m2.1.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.3.1"></eq><ci id="S4.SS2.p1.2.m2.1.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p1.2.m2.1.1.1.3.3.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\{v_{i}\}_{i=1}^{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">{ italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>,</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E2X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle s_{i}=v_{i}\cdot v_{c},\forall i\in\{1,\dots,L\}." class="ltx_Math" display="inline" id="S4.E2X.2.1.1.m1.4"><semantics id="S4.E2X.2.1.1.m1.4a"><mrow id="S4.E2X.2.1.1.m1.4.4.1"><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.3.cmml"><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.1.1" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.cmml"><msub id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml"><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.2.cmml">s</mi><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.1" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.cmml"><msub id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.cmml"><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.2.cmml">v</mi><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.1.cmml">â‹…</mo><msub id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.cmml"><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.2.cmml">v</mi><mi id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.3.cmml">c</mi></msub></mrow></mrow><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.3a.cmml">,</mo><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.cmml"><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.cmml"><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.1" rspace="0.167em" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.1.cmml">âˆ€</mo><mi id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.2.cmml">i</mi></mrow><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.1" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.1.cmml">âˆˆ</mo><mrow id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml"><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2.1" stretchy="false" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml">{</mo><mn id="S4.E2X.2.1.1.m1.1.1" xref="S4.E2X.2.1.1.m1.1.1.cmml">1</mn><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2.2" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml">,</mo><mi id="S4.E2X.2.1.1.m1.2.2" mathvariant="normal" xref="S4.E2X.2.1.1.m1.2.2.cmml">â€¦</mi><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2.3" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml">,</mo><mi id="S4.E2X.2.1.1.m1.3.3" xref="S4.E2X.2.1.1.m1.3.3.cmml">L</mi><mo id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2.4" stretchy="false" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E2X.2.1.1.m1.4.4.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2X.2.1.1.m1.4b"><apply id="S4.E2X.2.1.1.m1.4.4.1.1.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S4.E2X.2.1.1.m1.4.4.1.1.3a.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1"><eq id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.1"></eq><apply id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2">subscript</csymbol><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.2">ğ‘ </ci><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3"><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.1">â‹…</ci><apply id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.2">ğ‘£</ci><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.2">ğ‘£</ci><ci id="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.3.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.1.1.3.3.3">ğ‘</ci></apply></apply></apply><apply id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2"><in id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.1"></in><apply id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2"><csymbol cd="latexml" id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.1">for-all</csymbol><ci id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.2.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.2.2">ğ‘–</ci></apply><set id="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.1.cmml" xref="S4.E2X.2.1.1.m1.4.4.1.1.2.2.3.2"><cn id="S4.E2X.2.1.1.m1.1.1.cmml" type="integer" xref="S4.E2X.2.1.1.m1.1.1">1</cn><ci id="S4.E2X.2.1.1.m1.2.2.cmml" xref="S4.E2X.2.1.1.m1.2.2">â€¦</ci><ci id="S4.E2X.2.1.1.m1.3.3.cmml" xref="S4.E2X.2.1.1.m1.3.3">ğ¿</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2X.2.1.1.m1.4c">\displaystyle s_{i}=v_{i}\cdot v_{c},\forall i\in\{1,\dots,L\}.</annotation><annotation encoding="application/x-llamapun" id="S4.E2X.2.1.1.m1.4d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‹… italic_v start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , âˆ€ italic_i âˆˆ { 1 , â€¦ , italic_L } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.4">Following the calculation of the similarity scores, we partition the score range of [-1.0, 1.0] into <math alttext="\bf{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\bf{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">bold_N</annotation></semantics></math> predefined tiers. Within each tier, we count the number of similarity scores that fall into that corresponding range. Hence, we obtain an <math alttext="\bf{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\bf{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">bold_N</annotation></semantics></math>-dimensional vector, with each dimension representing the number of similarity scores in the corresponding tier. Thus, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.4.1">SimTier</span> effectively converts a set of high-dimensional multimodal representations into a <math alttext="\bf{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\bf{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">bold_N</annotation></semantics></math>-dimensional vector that encapsulate the degree of similarity between the target item and the userâ€™s historical interactions.
The obtained <math alttext="\bf{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\bf{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">bold_N</annotation></semantics></math>-dimensional vector is then concatenated with other embeddings and fed to the following MLP.
We provide the pseudo code of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.p2.4.2">SimTier</span> in AlgorithmÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#alg1" title="Algorithm 1 â€£ 4.3. Method II: Multimodal Knowledage Extractor (MAKE) â€£ 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Method II: Multimodal Knowledage Extractor (<span class="ltx_text ltx_font_smallcaps" id="S4.SS3.1.1">MAKE</span>)</h3>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> A Tensorflow-style Pseudocode of the SimTier.</figcaption>
<div class="ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing" id="alg1.3" style="background-color:#FFFFFF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,IyBCOiBiYXRjaCBzaXplLCBTOiBzZXF1ZW5jZSBsZW5ndGgsIE46IHRpZXIgbnVtYmVyLCBEOiBkaW0KIyBJbnB1dDogdGFyZ2V0IFtCLCBEXSwgc2VxIFtCLCBTLCBEXQojIE91dHB1dDogc2ltX3RpZXIgW0IsIE5dCgojIENvbXB1dGUgdGhlIHNpbWlsYXJpdHkgc2NvcmVzIFtCLCBTXQpzaW1fc2NvcmUgPSByZWR1Y2Vfc3VtKGV4cGFuZF9kaW1zKHRhcmdldCwgMSkqc2VxLCBheGlzPTIpCiMgQXNzaWduIHRpZXIgdG8gZWFjaCBzY29yZSBbQiwgU10KaW5kaWNlcyA9IHJlc2hhcGUoY2VpbCgoc2ltX3Njb3JlICsgMSkgLyAyICogTiksIFstMSwgU10pCiMgQWNjdW11bGF0ZSBjb3VudHMgZm9yIGVhY2ggdGllciBbQiwgTl0Kd2VpZ2h0ID0gZXF1YWwoCiAgICAgICAgICAgIHJlc2hhcGUocmFuZ2UoMCwgTiwgMSksIFsxLCBOLCAxXSksCiAgICAgICAgICAgIGV4cGFuZF9kaW1zKGluZGljZXMsIGF4aXM9MSkKICAgICAgICApCnNpbV90aWVyID0gcmVkdWNlX3N1bSh3ZWlnaHQsIGF4aXM9Miwga2VlcF9kaW1zPVRydWUp">â¬‡</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx1.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.1"> </span>B:<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.2"> </span>batch<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.3"> </span>size,<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.4"> </span>S:<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.5"> </span>sequence<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.6"> </span>length,<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.7"> </span>N:<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.8"> </span>tier<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.9"> </span>number,<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.10"> </span>D:<span class="ltx_text ltx_lst_space" id="lstnumberx1.1.11"> </span>dim</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx2.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.1"> </span>Input:<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.2"> </span>target<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.3"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.4"> </span>D],<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.5"> </span>seq<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.6"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.7"> </span>S,<span class="ltx_text ltx_lst_space" id="lstnumberx2.1.8"> </span>D]</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx3.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.1"> </span>Output:<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.2"> </span>sim_tier<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.3"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx3.1.4"> </span>N]</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx5.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.1"> </span>Compute<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.2"> </span>the<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.3"> </span>similarity<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.4"> </span>scores<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.5"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx5.1.6"> </span>S]</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.1">sim_score</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.5">reduce_sum</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.7">expand_dims</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.8">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.9">target</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.10">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.11"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.12">1)*</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.13">seq</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.14">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.16">axis</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.17">=2)</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx7.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.1"> </span>Assign<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.2"> </span>tier<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.3"> </span>to<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.4"> </span>each<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.5"> </span>score<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.6"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx7.1.7"> </span>S]</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">indices</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.3">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5">reshape</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.7">ceil</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.8">((</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.9">sim_score</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.11">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.12"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.13">1)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.14"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.15">/</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.17">2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.18"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.19">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.21">N</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.22">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.23"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.24">[-1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.26">S</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.27">])</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx9.1" style="color:#408080;">#<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.1"> </span>Accumulate<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.2"> </span>counts<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.3"> </span>for<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.4"> </span>each<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.5"> </span>tier<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.6"> </span>[B,<span class="ltx_text ltx_lst_space" id="lstnumberx9.1.7"> </span>N]</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1">weight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.3">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.5">equal</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.6">(</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.2">reshape</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.3">(</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx11.4" style="color:#D92E80;">range</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.5">(0,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.7">N</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.9"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10">1),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.11"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.12">[1,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.14">N</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.15">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.17">1]),</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.2">expand_dims</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.3">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.4">indices</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.5">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.7">axis</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.8">=1)</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.1"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">sim_tier</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.3">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">reduce_sum</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.7">weight</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.10">axis</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.11">=2,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.13">keep_dims</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.14">=</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.15">True</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.16">)</span>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To address the difference in training epochs required for ID features versus multimodal representations, we introduce the <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">M</span>ultimod<span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">A</span>l <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">K</span>nowledge <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">E</span>xtractor (<span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.5">MAKE</span>) module, decoupling the optimization of multimodal related parameters from that of ID features. The <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.6">MAKE</span> module consist of two steps: 1) multi-epoch training to extract useful multimodal knowledge and 2) knowledge utilization by the downstream task.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Multi-epoch training of multimodal related parameters.</span>
The goal of the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p2.1.2">MAKE</span> module is pre-training the parameters related to multimodal representations over multiple epochs to ensure their convergence. In practice, we utilize the CTR prediction task as the recommendation pre-training task.
As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S3.F2" title="Figure 2 â€£ 3.2. Construction of Pre-Training Dataset â€£ 3. Pre-Training of Multimodal Representations â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">2</span></a> (b), we first develop a DIN-based user behavior modeling moduleÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib37" title="">2018</a>)</cite>. This module processes the pre-trained multimodal representations of the target item and historical interacted items, resulting in the output <math alttext="\bf{v}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">ğ¯</mi><mtext id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">ğ¯</ci><ci id="S4.SS3.p2.1.m1.1.1.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><mtext id="S4.SS3.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p2.1.m1.1.1.3">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\bf{v}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bf{v}_{\text{MAKE}}=\text{DIN}(\{\bf{v}_{i}\}_{i=0}^{L},\bf{v}_{%
c})." class="ltx_Math" display="inline" id="S4.E3X.2.1.1.m1.1"><semantics id="S4.E3X.2.1.1.m1.1a"><mrow id="S4.E3X.2.1.1.m1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E3X.2.1.1.m1.1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.cmml"><msub id="S4.E3X.2.1.1.m1.1.1.1.1.4" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.1.1.4.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.2.cmml">ğ¯</mi><mtext id="S4.E3X.2.1.1.m1.1.1.1.1.4.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.3a.cmml">MAKE</mtext></msub><mo id="S4.E3X.2.1.1.m1.1.1.1.1.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E3X.2.1.1.m1.1.1.1.1.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.cmml"><mtext id="S4.E3X.2.1.1.m1.1.1.1.1.2.4" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.4a.cmml">DIN</mtext><mo id="S4.E3X.2.1.1.m1.1.1.1.1.2.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.3.cmml">â¢</mo><mrow id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.3.cmml"><mo id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.3.cmml">(</mo><msubsup id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ¯</mi><mi id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ¢</mi></msub><mo id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ¢</mi><mo id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğŸ</mn></mrow><mi id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml">ğ‹</mi></msubsup><mo id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.4" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.2" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.2.cmml">ğ¯</mi><mi id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.3" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.3.cmml">ğœ</mi></msub><mo id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3X.2.1.1.m1.1.1.1.2" lspace="0em" xref="S4.E3X.2.1.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3X.2.1.1.m1.1b"><apply id="S4.E3X.2.1.1.m1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1"><eq id="S4.E3X.2.1.1.m1.1.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.3"></eq><apply id="S4.E3X.2.1.1.m1.1.1.1.1.4.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.1.1.4.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.4">subscript</csymbol><ci id="S4.E3X.2.1.1.m1.1.1.1.1.4.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.2">ğ¯</ci><ci id="S4.E3X.2.1.1.m1.1.1.1.1.4.3a.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.3"><mtext id="S4.E3X.2.1.1.m1.1.1.1.1.4.3.cmml" mathsize="70%" xref="S4.E3X.2.1.1.m1.1.1.1.1.4.3">MAKE</mtext></ci></apply><apply id="S4.E3X.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2"><times id="S4.E3X.2.1.1.m1.1.1.1.1.2.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.3"></times><ci id="S4.E3X.2.1.1.m1.1.1.1.1.2.4a.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.4"><mtext id="S4.E3X.2.1.1.m1.1.1.1.1.2.4.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.4">DIN</mtext></ci><interval closure="open" id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2"><apply id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><set id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ¯</ci><ci id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ¢</ci></apply></set><apply id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3"><eq id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.1"></eq><ci id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.2">ğ¢</ci><cn id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.1.3.3">0</cn></apply></apply><ci id="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.1.1.1.1.3">ğ‹</ci></apply><apply id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.2">ğ¯</ci><ci id="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.1.1.2.2.2.2.3">ğœ</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3X.2.1.1.m1.1c">\displaystyle\bf{v}_{\text{MAKE}}=\text{DIN}(\{\bf{v}_{i}\}_{i=0}^{L},\bf{v}_{%
c}).</annotation><annotation encoding="application/x-llamapun" id="S4.E3X.2.1.1.m1.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT = DIN ( { bold_v start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT bold_i = bold_0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_L end_POSTSUPERSCRIPT , bold_v start_POSTSUBSCRIPT bold_c end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.3">After that, <math alttext="\bf{v}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">ğ¯</mi><mtext id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">ğ¯</ci><ci id="S4.SS3.p3.1.m1.1.1.3a.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><mtext id="S4.SS3.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p3.1.m1.1.1.3">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\bf{v}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT</annotation></semantics></math> is fed into a four-layer Multi-layer Perceptron (MLP<math alttext="{}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><msub id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1a" xref="S4.SS3.p3.2.m2.1.1.cmml"></mi><mtext id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><ci id="S4.SS3.p3.2.m2.1.1.1a.cmml" xref="S4.SS3.p3.2.m2.1.1.1"><mtext id="S4.SS3.p3.2.m2.1.1.1.cmml" mathsize="70%" xref="S4.SS3.p3.2.m2.1.1.1">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">{}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">start_FLOATSUBSCRIPT MAKE end_FLOATSUBSCRIPT</annotation></semantics></math>) and produced the logit <math alttext="\hat{v}" class="ltx_Math" display="inline" id="S4.SS3.p3.3.m3.1"><semantics id="S4.SS3.p3.3.m3.1a"><mover accent="true" id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">v</mi><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><ci id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1">^</ci><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">\hat{v}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.3.m3.1d">over^ start_ARG italic_v end_ARG</annotation></semantics></math></p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{v}=\text{MLP}_{\text{MAKE}}(\bf{v}_{\text{MAKE}})." class="ltx_Math" display="inline" id="S4.E4X.2.1.1.m1.1"><semantics id="S4.E4X.2.1.1.m1.1a"><mrow id="S4.E4X.2.1.1.m1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E4X.2.1.1.m1.1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.cmml"><mover accent="true" id="S4.E4X.2.1.1.m1.1.1.1.1.3" xref="S4.E4X.2.1.1.m1.1.1.1.1.3.cmml"><mi id="S4.E4X.2.1.1.m1.1.1.1.1.3.2" xref="S4.E4X.2.1.1.m1.1.1.1.1.3.2.cmml">v</mi><mo id="S4.E4X.2.1.1.m1.1.1.1.1.3.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S4.E4X.2.1.1.m1.1.1.1.1.2" xref="S4.E4X.2.1.1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E4X.2.1.1.m1.1.1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.E4X.2.1.1.m1.1.1.1.1.1.3" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.cmml"><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2a.cmml">MLP</mtext><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3a.cmml">MAKE</mtext></msub><mo id="S4.E4X.2.1.1.m1.1.1.1.1.1.2" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">ğ¯</mi><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3a.cmml">MAKE</mtext></msub><mo id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E4X.2.1.1.m1.1.1.1.2" lspace="0em" xref="S4.E4X.2.1.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4X.2.1.1.m1.1b"><apply id="S4.E4X.2.1.1.m1.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1"><eq id="S4.E4X.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.2"></eq><apply id="S4.E4X.2.1.1.m1.1.1.1.1.3.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.3"><ci id="S4.E4X.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.3.1">^</ci><ci id="S4.E4X.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.3.2">ğ‘£</ci></apply><apply id="S4.E4X.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1"><times id="S4.E4X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.2"></times><apply id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2a.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2"><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.2">MLP</mtext></ci><ci id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3a.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3"><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.3.3">MAKE</mtext></ci></apply><apply id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.2">ğ¯</ci><ci id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3"><mtext id="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="S4.E4X.2.1.1.m1.1.1.1.1.1.1.1.1.3">MAKE</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4X.2.1.1.m1.1c">\displaystyle\hat{v}=\text{MLP}_{\text{MAKE}}(\bf{v}_{\text{MAKE}}).</annotation><annotation encoding="application/x-llamapun" id="S4.E4X.2.1.1.m1.1d">over^ start_ARG italic_v end_ARG = MLP start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT ( bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS3.p3.4">Then we optimize the cross-entropy loss between the predicted click probability and the binary click label <math alttext="y" class="ltx_Math" display="inline" id="S4.SS3.p3.4.m1.1"><semantics id="S4.SS3.p3.4.m1.1a"><mi id="S4.SS3.p3.4.m1.1.1" xref="S4.SS3.p3.4.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m1.1b"><ci id="S4.SS3.p3.4.m1.1.1.cmml" xref="S4.SS3.p3.4.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.4.m1.1d">italic_y</annotation></semantics></math>, as shown in EquationÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S4.E5" title="In 4.3. Method II: Multimodal Knowledage Extractor (MAKE) â€£ 4. Integration with Recommendation Models â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{MAKE}}=" class="ltx_Math" display="inline" id="S4.E5X.2.1.1.m1.1"><semantics id="S4.E5X.2.1.1.m1.1a"><mrow id="S4.E5X.2.1.1.m1.1.1" xref="S4.E5X.2.1.1.m1.1.1.cmml"><msub id="S4.E5X.2.1.1.m1.1.1.2" xref="S4.E5X.2.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5X.2.1.1.m1.1.1.2.2" xref="S4.E5X.2.1.1.m1.1.1.2.2.cmml">â„’</mi><mtext id="S4.E5X.2.1.1.m1.1.1.2.3" xref="S4.E5X.2.1.1.m1.1.1.2.3a.cmml">MAKE</mtext></msub><mo id="S4.E5X.2.1.1.m1.1.1.1" xref="S4.E5X.2.1.1.m1.1.1.1.cmml">=</mo><mi id="S4.E5X.2.1.1.m1.1.1.3" xref="S4.E5X.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E5X.2.1.1.m1.1b"><apply id="S4.E5X.2.1.1.m1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1"><eq id="S4.E5X.2.1.1.m1.1.1.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.1"></eq><apply id="S4.E5X.2.1.1.m1.1.1.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E5X.2.1.1.m1.1.1.2.1.cmml" xref="S4.E5X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.E5X.2.1.1.m1.1.1.2.2.cmml" xref="S4.E5X.2.1.1.m1.1.1.2.2">â„’</ci><ci id="S4.E5X.2.1.1.m1.1.1.2.3a.cmml" xref="S4.E5X.2.1.1.m1.1.1.2.3"><mtext id="S4.E5X.2.1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S4.E5X.2.1.1.m1.1.1.2.3">MAKE</mtext></ci></apply><csymbol cd="latexml" id="S4.E5X.2.1.1.m1.1.1.3.cmml" xref="S4.E5X.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.2.1.1.m1.1c">\displaystyle\mathcal{L}_{\text{MAKE}}=</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.2.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT =</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sum-y\log\sigma(\hat{v})-(1-y)\log(1-\sigma(\hat{v}))" class="ltx_Math" display="inline" id="S4.E5X.3.2.2.m1.5"><semantics id="S4.E5X.3.2.2.m1.5a"><mrow id="S4.E5X.3.2.2.m1.5.5" xref="S4.E5X.3.2.2.m1.5.5.cmml"><mstyle displaystyle="true" id="S4.E5X.3.2.2.m1.5.5.4" xref="S4.E5X.3.2.2.m1.5.5.4.cmml"><mo id="S4.E5X.3.2.2.m1.5.5.4a" movablelimits="false" xref="S4.E5X.3.2.2.m1.5.5.4.cmml">âˆ‘</mo></mstyle><mo id="S4.E5X.3.2.2.m1.5.5.3" lspace="0em" xref="S4.E5X.3.2.2.m1.5.5.3.cmml">âˆ’</mo><mrow id="S4.E5X.3.2.2.m1.5.5.5" xref="S4.E5X.3.2.2.m1.5.5.5.cmml"><mi id="S4.E5X.3.2.2.m1.5.5.5.2" xref="S4.E5X.3.2.2.m1.5.5.5.2.cmml">y</mi><mo id="S4.E5X.3.2.2.m1.5.5.5.1" lspace="0.167em" xref="S4.E5X.3.2.2.m1.5.5.5.1.cmml">â¢</mo><mrow id="S4.E5X.3.2.2.m1.5.5.5.3" xref="S4.E5X.3.2.2.m1.5.5.5.3.cmml"><mi id="S4.E5X.3.2.2.m1.5.5.5.3.1" xref="S4.E5X.3.2.2.m1.5.5.5.3.1.cmml">log</mi><mo id="S4.E5X.3.2.2.m1.5.5.5.3a" lspace="0.167em" xref="S4.E5X.3.2.2.m1.5.5.5.3.cmml">â¡</mo><mi id="S4.E5X.3.2.2.m1.5.5.5.3.2" xref="S4.E5X.3.2.2.m1.5.5.5.3.2.cmml">Ïƒ</mi></mrow><mo id="S4.E5X.3.2.2.m1.5.5.5.1a" xref="S4.E5X.3.2.2.m1.5.5.5.1.cmml">â¢</mo><mrow id="S4.E5X.3.2.2.m1.5.5.5.4.2" xref="S4.E5X.3.2.2.m1.1.1.cmml"><mo id="S4.E5X.3.2.2.m1.5.5.5.4.2.1" stretchy="false" xref="S4.E5X.3.2.2.m1.1.1.cmml">(</mo><mover accent="true" id="S4.E5X.3.2.2.m1.1.1" xref="S4.E5X.3.2.2.m1.1.1.cmml"><mi id="S4.E5X.3.2.2.m1.1.1.2" xref="S4.E5X.3.2.2.m1.1.1.2.cmml">v</mi><mo id="S4.E5X.3.2.2.m1.1.1.1" xref="S4.E5X.3.2.2.m1.1.1.1.cmml">^</mo></mover><mo id="S4.E5X.3.2.2.m1.5.5.5.4.2.2" stretchy="false" xref="S4.E5X.3.2.2.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E5X.3.2.2.m1.5.5.3a" xref="S4.E5X.3.2.2.m1.5.5.3.cmml">âˆ’</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2" xref="S4.E5X.3.2.2.m1.5.5.2.cmml"><mrow id="S4.E5X.3.2.2.m1.4.4.1.1.1" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.cmml"><mo id="S4.E5X.3.2.2.m1.4.4.1.1.1.2" stretchy="false" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S4.E5X.3.2.2.m1.4.4.1.1.1.1" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.cmml"><mn id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.2" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.2.cmml">1</mn><mo id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.1" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.1.cmml">âˆ’</mo><mi id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.3" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.3.cmml">y</mi></mrow><mo id="S4.E5X.3.2.2.m1.4.4.1.1.1.3" stretchy="false" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E5X.3.2.2.m1.5.5.2.3" lspace="0.167em" xref="S4.E5X.3.2.2.m1.5.5.2.3.cmml">â¢</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2.2.1" xref="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml"><mi id="S4.E5X.3.2.2.m1.3.3" xref="S4.E5X.3.2.2.m1.3.3.cmml">log</mi><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1a" xref="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml">â¡</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2.2.1.1" xref="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml"><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.2" stretchy="false" xref="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml">(</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.cmml"><mn id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.2" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.2.cmml">1</mn><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.1" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.1.cmml">âˆ’</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.cmml"><mi id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.2" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.2.cmml">Ïƒ</mi><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.1" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.1.cmml">â¢</mo><mrow id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.3.2" xref="S4.E5X.3.2.2.m1.2.2.cmml"><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.3.2.1" stretchy="false" xref="S4.E5X.3.2.2.m1.2.2.cmml">(</mo><mover accent="true" id="S4.E5X.3.2.2.m1.2.2" xref="S4.E5X.3.2.2.m1.2.2.cmml"><mi id="S4.E5X.3.2.2.m1.2.2.2" xref="S4.E5X.3.2.2.m1.2.2.2.cmml">v</mi><mo id="S4.E5X.3.2.2.m1.2.2.1" xref="S4.E5X.3.2.2.m1.2.2.1.cmml">^</mo></mover><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.3.2.2" stretchy="false" xref="S4.E5X.3.2.2.m1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.3" stretchy="false" xref="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5X.3.2.2.m1.5b"><apply id="S4.E5X.3.2.2.m1.5.5.cmml" xref="S4.E5X.3.2.2.m1.5.5"><minus id="S4.E5X.3.2.2.m1.5.5.3.cmml" xref="S4.E5X.3.2.2.m1.5.5.3"></minus><sum id="S4.E5X.3.2.2.m1.5.5.4.cmml" xref="S4.E5X.3.2.2.m1.5.5.4"></sum><apply id="S4.E5X.3.2.2.m1.5.5.5.cmml" xref="S4.E5X.3.2.2.m1.5.5.5"><times id="S4.E5X.3.2.2.m1.5.5.5.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.1"></times><ci id="S4.E5X.3.2.2.m1.5.5.5.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.2">ğ‘¦</ci><apply id="S4.E5X.3.2.2.m1.5.5.5.3.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.3"><log id="S4.E5X.3.2.2.m1.5.5.5.3.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.3.1"></log><ci id="S4.E5X.3.2.2.m1.5.5.5.3.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.3.2">ğœ</ci></apply><apply id="S4.E5X.3.2.2.m1.1.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.5.4.2"><ci id="S4.E5X.3.2.2.m1.1.1.1.cmml" xref="S4.E5X.3.2.2.m1.1.1.1">^</ci><ci id="S4.E5X.3.2.2.m1.1.1.2.cmml" xref="S4.E5X.3.2.2.m1.1.1.2">ğ‘£</ci></apply></apply><apply id="S4.E5X.3.2.2.m1.5.5.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.2"><times id="S4.E5X.3.2.2.m1.5.5.2.3.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.3"></times><apply id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.cmml" xref="S4.E5X.3.2.2.m1.4.4.1.1.1"><minus id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.1.cmml" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.1"></minus><cn id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.2.cmml" type="integer" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.2">1</cn><ci id="S4.E5X.3.2.2.m1.4.4.1.1.1.1.3.cmml" xref="S4.E5X.3.2.2.m1.4.4.1.1.1.1.3">ğ‘¦</ci></apply><apply id="S4.E5X.3.2.2.m1.5.5.2.2.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1"><log id="S4.E5X.3.2.2.m1.3.3.cmml" xref="S4.E5X.3.2.2.m1.3.3"></log><apply id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1"><minus id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.1"></minus><cn id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.2.cmml" type="integer" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.2">1</cn><apply id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3"><times id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.1.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.1"></times><ci id="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.2">ğœ</ci><apply id="S4.E5X.3.2.2.m1.2.2.cmml" xref="S4.E5X.3.2.2.m1.5.5.2.2.1.1.1.3.3.2"><ci id="S4.E5X.3.2.2.m1.2.2.1.cmml" xref="S4.E5X.3.2.2.m1.2.2.1">^</ci><ci id="S4.E5X.3.2.2.m1.2.2.2.cmml" xref="S4.E5X.3.2.2.m1.2.2.2">ğ‘£</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5X.3.2.2.m1.5c">\displaystyle\sum-y\log\sigma(\hat{v})-(1-y)\log(1-\sigma(\hat{v}))</annotation><annotation encoding="application/x-llamapun" id="S4.E5X.3.2.2.m1.5d">âˆ‘ - italic_y roman_log italic_Ïƒ ( over^ start_ARG italic_v end_ARG ) - ( 1 - italic_y ) roman_log ( 1 - italic_Ïƒ ( over^ start_ARG italic_v end_ARG ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">The recommendation pre-training task allows the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p4.1.1">MAKE</span> module to refine its parameters via training over multiple epochs and extract knowledge <math alttext="\bf{v}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><msub id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mi id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">ğ¯</mi><mtext id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">ğ¯</ci><ci id="S4.SS3.p4.1.m1.1.1.3a.cmml" xref="S4.SS3.p4.1.m1.1.1.3"><mtext id="S4.SS3.p4.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p4.1.m1.1.1.3">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">\bf{v}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT</annotation></semantics></math> from multimodal representations, thereby enhancing its effectiveness for recommendation tasks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.3"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.3.1">Knowledge utilization.</span>
After acquiring the vector <math alttext="\bf{v}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p5.1.m1.1"><semantics id="S4.SS3.p5.1.m1.1a"><msub id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml"><mi id="S4.SS3.p5.1.m1.1.1.2" xref="S4.SS3.p5.1.m1.1.1.2.cmml">ğ¯</mi><mtext id="S4.SS3.p5.1.m1.1.1.3" xref="S4.SS3.p5.1.m1.1.1.3a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><apply id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p5.1.m1.1.1.2.cmml" xref="S4.SS3.p5.1.m1.1.1.2">ğ¯</ci><ci id="S4.SS3.p5.1.m1.1.1.3a.cmml" xref="S4.SS3.p5.1.m1.1.1.3"><mtext id="S4.SS3.p5.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p5.1.m1.1.1.3">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">\bf{v}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.1.m1.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT</annotation></semantics></math>, the subsequent step is to integrate it into the downstream recommendation task. Practically, we concatenate <math alttext="\bf{v}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p5.2.m2.1"><semantics id="S4.SS3.p5.2.m2.1a"><msub id="S4.SS3.p5.2.m2.1.1" xref="S4.SS3.p5.2.m2.1.1.cmml"><mi id="S4.SS3.p5.2.m2.1.1.2" xref="S4.SS3.p5.2.m2.1.1.2.cmml">ğ¯</mi><mtext id="S4.SS3.p5.2.m2.1.1.3" xref="S4.SS3.p5.2.m2.1.1.3a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.m2.1b"><apply id="S4.SS3.p5.2.m2.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p5.2.m2.1.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p5.2.m2.1.1.2.cmml" xref="S4.SS3.p5.2.m2.1.1.2">ğ¯</ci><ci id="S4.SS3.p5.2.m2.1.1.3a.cmml" xref="S4.SS3.p5.2.m2.1.1.3"><mtext id="S4.SS3.p5.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS3.p5.2.m2.1.1.3">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.m2.1c">\bf{v}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.2.m2.1d">bold_v start_POSTSUBSCRIPT MAKE end_POSTSUBSCRIPT</annotation></semantics></math> and intermediate outputs from MLP<math alttext="{}_{\text{MAKE}}" class="ltx_Math" display="inline" id="S4.SS3.p5.3.m3.1"><semantics id="S4.SS3.p5.3.m3.1a"><msub id="S4.SS3.p5.3.m3.1.1" xref="S4.SS3.p5.3.m3.1.1.cmml"><mi id="S4.SS3.p5.3.m3.1.1a" xref="S4.SS3.p5.3.m3.1.1.cmml"></mi><mtext id="S4.SS3.p5.3.m3.1.1.1" xref="S4.SS3.p5.3.m3.1.1.1a.cmml">MAKE</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.3.m3.1b"><apply id="S4.SS3.p5.3.m3.1.1.cmml" xref="S4.SS3.p5.3.m3.1.1"><ci id="S4.SS3.p5.3.m3.1.1.1a.cmml" xref="S4.SS3.p5.3.m3.1.1.1"><mtext id="S4.SS3.p5.3.m3.1.1.1.cmml" mathsize="70%" xref="S4.SS3.p5.3.m3.1.1.1">MAKE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.3.m3.1c">{}_{\text{MAKE}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p5.3.m3.1d">start_FLOATSUBSCRIPT MAKE end_FLOATSUBSCRIPT</annotation></semantics></math> with other embeddings and input this combined data into the subsequent layers.
The implementation of the multi-epoch training of <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p5.3.2">MAKE</span> module reconciles the training epochs difference required for ID embeddings and multimodal representations, resulting in better performances.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>INDUSTRIAL DESIGN for Online Deployment</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In industrial settings, new items (including advertisements) are constantly being created. To maintain prediction accuracy for these new items, itâ€™s crucial for the recommendation model to acquire the multimodal representation of new items in real-time. This calls for the ability of continuous generating multimodal representations for new items and a real-time utilization by near-line trainer and online server.
</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S5.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>An overview of the online system.</figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The illustrative overview of our online system is provided in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S5.F4" title="Figure 4 â€£ 5. INDUSTRIAL DESIGN for Online Deployment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">4</span></a>. To achieve the real-time generation of multimodal representations, upon the introduction of new items, the system automatically initiates a request to the pre-trained multimodal encoders to compute the multimodal representations for these new items. Once inferred, these representations are sent to the multimodal index table. Following this step, the downstream training systems and inference servers are able to retrieve the multimodal representations from the index table, facilitating near-line training and real-time online prediction capabilities. This process ensures minimal latencyâ€”reduced to just a few secondsâ€”between an itemâ€™s introduction and the utilization of its multimodal representation by the model.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Experiment</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we delve into a case study that examines the integration of image representations into the CTR prediction model, aiming to provide a comprehensive analysis.
It is notable that our approach is general, capable of accommodating several modal types (such as text and video) and applicable across different stages within the recommendation systems.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Experiment Setup</h3>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Datasets.</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">We start by detailing the datasets used for the pre-training phase and the downstream integration phase.</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Pre-training Dataset.</span>
In the pre-training dataset,
each sample consists of a Query (userâ€™s image query) and a Positive (the image of the purchased item). To further enhance the performance, we also add a hard Negative (the image of the clicked item triggered by the positive item).
A case of the pre-training dataset is depicted in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.F5" title="Figure 5 â€£ 6.1.2. Compared Pre-training Methods â€£ 6.1. Experiment Setup â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">5</span></a>.
</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">CTR Prediction Dataset.</span>
The CTR prediction dataset is obtained from the Taobao display advertising system, using impression logs of one week.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Compared Pre-training Methods</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">We employ a range of widely-used pre-training method for comparison.</p>
<ul class="ltx_itemize" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p" id="S6.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I2.i1.p1.1.1">CLIP-O.</span> CLIP-O refers to the CLIP visual encoder (CLIP-ViT-B/16) that has been pre-trained using a universal datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib22" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p" id="S6.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I2.i2.p1.1.1">CLIP-E.</span> CLIP-E is the fine-tuned version based on the CLIP-O model in the e-commerce scenario using aligned item descriptions and item images.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I2.i3.p1">
<p class="ltx_p" id="S6.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I2.i3.p1.1.1">SCL.</span> The proposed semantic-aware pre-training method.
The SCL approach employs Momentum Contrast (MoCo) to expand the set of negative samples. Furthermore, SCL applies triplet lossÂ <cite class="ltx_cite ltx_citemacro_citep">(Schroff
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib24" title="">2015</a>)</cite> to each Â¡Query, Positive, NegativeÂ¿ triplet to effectively discriminate hard negatives. Our experimental analysis investigate the impact of excluding triplet loss and MoCo to assess their contributions to the overall performance.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="270" id="S6.F5.g1" src="x5.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>A case of the pre-training dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Compared Recommendation Methods</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">We employ a range of widely-used integration approach for comparison.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<ul class="ltx_itemize" id="S6.I3">
<li class="ltx_item" id="S6.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I3.i1.p1">
<p class="ltx_p" id="S6.I3.i1.p1.1">ID-based Model (production baseline). The baseline is a ID-based model that underpins our online system.
</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I3.i2.p1">
<p class="ltx_p" id="S6.I3.i2.p1.1">Vector. The Vector method utilize the pre-trained multimodal representations as the side-information of each item, and concatenated them with other ID embeddings within the model.</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I3.i3.p1">
<p class="ltx_p" id="S6.I3.i3.p1.1">SimScore. Similarity Score (SimScore) can be seen as a simplified version of the Vector method. The semantic similarity score for each historical interacted item with respect to the target item is used as side information.
</p>
</div>
</li>
<li class="ltx_item" id="S6.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I3.i4.p1">
<p class="ltx_p" id="S6.I3.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.I3.i4.p1.1.1">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.I3.i4.p1.1.2">MAKE</span>. The proposed <span class="ltx_text ltx_font_smallcaps" id="S6.I3.i4.p1.1.3">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.I3.i4.p1.1.4">MAKE</span> approaches.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4. </span>Evaluation Metrics</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">We evaluate both the pre-training performance and CTR prediction performance of the proposed methods.</p>
<ul class="ltx_itemize" id="S6.I4">
<li class="ltx_item" id="S6.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I4.i1.p1">
<p class="ltx_p" id="S6.I4.i1.p1.12"><span class="ltx_text ltx_font_bold" id="S6.I4.i1.p1.12.1">Evaluating Pre-training Methods.</span>
Throughout our extensive experimentation, we discovered that the
Top-N accuracy (Acc@N) is well correlated with the performance of downstream recommendation models.
In detail, the Acc@N metric quantifies the ability of the representation to identify semantic similar items:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Acc@N}=\frac{1}{D}\sum_{i=1}^{D}\mathbb{I}(p_{i}\in\text{Top}_{N}(q_{i},%
S))," class="ltx_Math" display="block" id="S6.E6.m1.2"><semantics id="S6.E6.m1.2a"><mrow id="S6.E6.m1.2.2.1" xref="S6.E6.m1.2.2.1.1.cmml"><mrow id="S6.E6.m1.2.2.1.1" xref="S6.E6.m1.2.2.1.1.cmml"><mtext id="S6.E6.m1.2.2.1.1.3" xref="S6.E6.m1.2.2.1.1.3a.cmml">Acc@N</mtext><mo id="S6.E6.m1.2.2.1.1.2" xref="S6.E6.m1.2.2.1.1.2.cmml">=</mo><mrow id="S6.E6.m1.2.2.1.1.1" xref="S6.E6.m1.2.2.1.1.1.cmml"><mfrac id="S6.E6.m1.2.2.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.3.cmml"><mn id="S6.E6.m1.2.2.1.1.1.3.2" xref="S6.E6.m1.2.2.1.1.1.3.2.cmml">1</mn><mi id="S6.E6.m1.2.2.1.1.1.3.3" xref="S6.E6.m1.2.2.1.1.1.3.3.cmml">D</mi></mfrac><mo id="S6.E6.m1.2.2.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S6.E6.m1.2.2.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.cmml"><munderover id="S6.E6.m1.2.2.1.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.1.2.cmml"><mo id="S6.E6.m1.2.2.1.1.1.1.2.2.2" movablelimits="false" xref="S6.E6.m1.2.2.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S6.E6.m1.2.2.1.1.1.1.2.2.3" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S6.E6.m1.2.2.1.1.1.1.2.2.3.2" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S6.E6.m1.2.2.1.1.1.1.2.2.3.1" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S6.E6.m1.2.2.1.1.1.1.2.2.3.3" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S6.E6.m1.2.2.1.1.1.1.2.3" xref="S6.E6.m1.2.2.1.1.1.1.2.3.cmml">D</mi></munderover><mrow id="S6.E6.m1.2.2.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.cmml"><mi id="S6.E6.m1.2.2.1.1.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.1.1.3.cmml">ğ•€</mi><mo id="S6.E6.m1.2.2.1.1.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S6.E6.m1.2.2.1.1.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.2" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">âˆˆ</mo><mrow id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2a.cmml">Top</mtext><mi id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">N</mi></msub><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S6.E6.m1.1.1" xref="S6.E6.m1.1.1.cmml">S</mi><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S6.E6.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S6.E6.m1.2.2.1.2" xref="S6.E6.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E6.m1.2b"><apply id="S6.E6.m1.2.2.1.1.cmml" xref="S6.E6.m1.2.2.1"><eq id="S6.E6.m1.2.2.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.2"></eq><ci id="S6.E6.m1.2.2.1.1.3a.cmml" xref="S6.E6.m1.2.2.1.1.3"><mtext id="S6.E6.m1.2.2.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.3">Acc@N</mtext></ci><apply id="S6.E6.m1.2.2.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1"><times id="S6.E6.m1.2.2.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.2"></times><apply id="S6.E6.m1.2.2.1.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.1.3"><divide id="S6.E6.m1.2.2.1.1.1.3.1.cmml" xref="S6.E6.m1.2.2.1.1.1.3"></divide><cn id="S6.E6.m1.2.2.1.1.1.3.2.cmml" type="integer" xref="S6.E6.m1.2.2.1.1.1.3.2">1</cn><ci id="S6.E6.m1.2.2.1.1.1.3.3.cmml" xref="S6.E6.m1.2.2.1.1.1.3.3">ğ·</ci></apply><apply id="S6.E6.m1.2.2.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1"><apply id="S6.E6.m1.2.2.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S6.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E6.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S6.E6.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S6.E6.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3"><eq id="S6.E6.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S6.E6.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.2">ğ‘–</ci><cn id="S6.E6.m1.2.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S6.E6.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S6.E6.m1.2.2.1.1.1.1.2.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.2.3">ğ·</ci></apply><apply id="S6.E6.m1.2.2.1.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1"><times id="S6.E6.m1.2.2.1.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.2"></times><ci id="S6.E6.m1.2.2.1.1.1.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.3">ğ•€</ci><apply id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1"><in id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.2"></in><apply id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1"><times id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.2"></times><apply id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">Top</mtext></ci><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘</ci></apply><interval closure="open" id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><apply id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.E6.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S6.E6.m1.1.1.cmml" xref="S6.E6.m1.1.1">ğ‘†</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E6.m1.2c">\text{Acc@N}=\frac{1}{D}\sum_{i=1}^{D}\mathbb{I}(p_{i}\in\text{Top}_{N}(q_{i},%
S)),</annotation><annotation encoding="application/x-llamapun" id="S6.E6.m1.2d">Acc@N = divide start_ARG 1 end_ARG start_ARG italic_D end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT blackboard_I ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ Top start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_S ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S6.I4.i1.p1.11">where <math alttext="D" class="ltx_Math" display="inline" id="S6.I4.i1.p1.1.m1.1"><semantics id="S6.I4.i1.p1.1.m1.1a"><mi id="S6.I4.i1.p1.1.m1.1.1" xref="S6.I4.i1.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.1.m1.1b"><ci id="S6.I4.i1.p1.1.m1.1.1.cmml" xref="S6.I4.i1.p1.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.1.m1.1d">italic_D</annotation></semantics></math> denotes the size of test set. The terms <math alttext="q_{i}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.2.m2.1"><semantics id="S6.I4.i1.p1.2.m2.1a"><msub id="S6.I4.i1.p1.2.m2.1.1" xref="S6.I4.i1.p1.2.m2.1.1.cmml"><mi id="S6.I4.i1.p1.2.m2.1.1.2" xref="S6.I4.i1.p1.2.m2.1.1.2.cmml">q</mi><mi id="S6.I4.i1.p1.2.m2.1.1.3" xref="S6.I4.i1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.2.m2.1b"><apply id="S6.I4.i1.p1.2.m2.1.1.cmml" xref="S6.I4.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.2.m2.1.1.1.cmml" xref="S6.I4.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S6.I4.i1.p1.2.m2.1.1.2.cmml" xref="S6.I4.i1.p1.2.m2.1.1.2">ğ‘</ci><ci id="S6.I4.i1.p1.2.m2.1.1.3.cmml" xref="S6.I4.i1.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.2.m2.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p_{i}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.3.m3.1"><semantics id="S6.I4.i1.p1.3.m3.1a"><msub id="S6.I4.i1.p1.3.m3.1.1" xref="S6.I4.i1.p1.3.m3.1.1.cmml"><mi id="S6.I4.i1.p1.3.m3.1.1.2" xref="S6.I4.i1.p1.3.m3.1.1.2.cmml">p</mi><mi id="S6.I4.i1.p1.3.m3.1.1.3" xref="S6.I4.i1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.3.m3.1b"><apply id="S6.I4.i1.p1.3.m3.1.1.cmml" xref="S6.I4.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.3.m3.1.1.1.cmml" xref="S6.I4.i1.p1.3.m3.1.1">subscript</csymbol><ci id="S6.I4.i1.p1.3.m3.1.1.2.cmml" xref="S6.I4.i1.p1.3.m3.1.1.2">ğ‘</ci><ci id="S6.I4.i1.p1.3.m3.1.1.3.cmml" xref="S6.I4.i1.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.3.m3.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> correspond to the query and the positive of the <math alttext="i" class="ltx_Math" display="inline" id="S6.I4.i1.p1.4.m4.1"><semantics id="S6.I4.i1.p1.4.m4.1a"><mi id="S6.I4.i1.p1.4.m4.1.1" xref="S6.I4.i1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.4.m4.1b"><ci id="S6.I4.i1.p1.4.m4.1.1.cmml" xref="S6.I4.i1.p1.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.4.m4.1d">italic_i</annotation></semantics></math>-th sample, respectively. <math alttext="S=\{pos_{i}\}_{i=1}^{D}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.5.m5.1"><semantics id="S6.I4.i1.p1.5.m5.1a"><mrow id="S6.I4.i1.p1.5.m5.1.1" xref="S6.I4.i1.p1.5.m5.1.1.cmml"><mi id="S6.I4.i1.p1.5.m5.1.1.3" xref="S6.I4.i1.p1.5.m5.1.1.3.cmml">S</mi><mo id="S6.I4.i1.p1.5.m5.1.1.2" xref="S6.I4.i1.p1.5.m5.1.1.2.cmml">=</mo><msubsup id="S6.I4.i1.p1.5.m5.1.1.1" xref="S6.I4.i1.p1.5.m5.1.1.1.cmml"><mrow id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.2.cmml"><mo id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.2" stretchy="false" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.2" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.2.cmml">p</mi><mo id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.3" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.3.cmml">o</mi><mo id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1a" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1.cmml">â¢</mo><msub id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.cmml"><mi id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.2" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.2.cmml">s</mi><mi id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.3" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub></mrow><mo id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.3" stretchy="false" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.I4.i1.p1.5.m5.1.1.1.1.3" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.cmml"><mi id="S6.I4.i1.p1.5.m5.1.1.1.1.3.2" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S6.I4.i1.p1.5.m5.1.1.1.1.3.1" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S6.I4.i1.p1.5.m5.1.1.1.1.3.3" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S6.I4.i1.p1.5.m5.1.1.1.3" xref="S6.I4.i1.p1.5.m5.1.1.1.3.cmml">D</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.5.m5.1b"><apply id="S6.I4.i1.p1.5.m5.1.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1"><eq id="S6.I4.i1.p1.5.m5.1.1.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.2"></eq><ci id="S6.I4.i1.p1.5.m5.1.1.3.cmml" xref="S6.I4.i1.p1.5.m5.1.1.3">ğ‘†</ci><apply id="S6.I4.i1.p1.5.m5.1.1.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.5.m5.1.1.1.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1">superscript</csymbol><apply id="S6.I4.i1.p1.5.m5.1.1.1.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.5.m5.1.1.1.1.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1">subscript</csymbol><set id="S6.I4.i1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1"><apply id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1"><times id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.1"></times><ci id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.3">ğ‘œ</ci><apply id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.2">ğ‘ </ci><ci id="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.3.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.1.1.1.4.3">ğ‘–</ci></apply></apply></set><apply id="S6.I4.i1.p1.5.m5.1.1.1.1.3.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3"><eq id="S6.I4.i1.p1.5.m5.1.1.1.1.3.1.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.1"></eq><ci id="S6.I4.i1.p1.5.m5.1.1.1.1.3.2.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.2">ğ‘–</ci><cn id="S6.I4.i1.p1.5.m5.1.1.1.1.3.3.cmml" type="integer" xref="S6.I4.i1.p1.5.m5.1.1.1.1.3.3">1</cn></apply></apply><ci id="S6.I4.i1.p1.5.m5.1.1.1.3.cmml" xref="S6.I4.i1.p1.5.m5.1.1.1.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.5.m5.1c">S=\{pos_{i}\}_{i=1}^{D}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.5.m5.1d">italic_S = { italic_p italic_o italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> represents the set comprising all positives, and <math alttext="\text{Top}_{N}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.6.m6.1"><semantics id="S6.I4.i1.p1.6.m6.1a"><msub id="S6.I4.i1.p1.6.m6.1.1" xref="S6.I4.i1.p1.6.m6.1.1.cmml"><mtext id="S6.I4.i1.p1.6.m6.1.1.2" xref="S6.I4.i1.p1.6.m6.1.1.2a.cmml">Top</mtext><mi id="S6.I4.i1.p1.6.m6.1.1.3" xref="S6.I4.i1.p1.6.m6.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.6.m6.1b"><apply id="S6.I4.i1.p1.6.m6.1.1.cmml" xref="S6.I4.i1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.6.m6.1.1.1.cmml" xref="S6.I4.i1.p1.6.m6.1.1">subscript</csymbol><ci id="S6.I4.i1.p1.6.m6.1.1.2a.cmml" xref="S6.I4.i1.p1.6.m6.1.1.2"><mtext id="S6.I4.i1.p1.6.m6.1.1.2.cmml" xref="S6.I4.i1.p1.6.m6.1.1.2">Top</mtext></ci><ci id="S6.I4.i1.p1.6.m6.1.1.3.cmml" xref="S6.I4.i1.p1.6.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.6.m6.1c">\text{Top}_{N}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.6.m6.1d">Top start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math> is a function that retrieves the top-N results for each query by leveraging a multimodal representation from the set <math alttext="S" class="ltx_Math" display="inline" id="S6.I4.i1.p1.7.m7.1"><semantics id="S6.I4.i1.p1.7.m7.1a"><mi id="S6.I4.i1.p1.7.m7.1.1" xref="S6.I4.i1.p1.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.7.m7.1b"><ci id="S6.I4.i1.p1.7.m7.1.1.cmml" xref="S6.I4.i1.p1.7.m7.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.7.m7.1c">S</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.7.m7.1d">italic_S</annotation></semantics></math>. The symbol <math alttext="\mathbb{I}(\cdot)" class="ltx_Math" display="inline" id="S6.I4.i1.p1.8.m8.1"><semantics id="S6.I4.i1.p1.8.m8.1a"><mrow id="S6.I4.i1.p1.8.m8.1.2" xref="S6.I4.i1.p1.8.m8.1.2.cmml"><mi id="S6.I4.i1.p1.8.m8.1.2.2" xref="S6.I4.i1.p1.8.m8.1.2.2.cmml">ğ•€</mi><mo id="S6.I4.i1.p1.8.m8.1.2.1" xref="S6.I4.i1.p1.8.m8.1.2.1.cmml">â¢</mo><mrow id="S6.I4.i1.p1.8.m8.1.2.3.2" xref="S6.I4.i1.p1.8.m8.1.2.cmml"><mo id="S6.I4.i1.p1.8.m8.1.2.3.2.1" stretchy="false" xref="S6.I4.i1.p1.8.m8.1.2.cmml">(</mo><mo id="S6.I4.i1.p1.8.m8.1.1" lspace="0em" rspace="0em" xref="S6.I4.i1.p1.8.m8.1.1.cmml">â‹…</mo><mo id="S6.I4.i1.p1.8.m8.1.2.3.2.2" stretchy="false" xref="S6.I4.i1.p1.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.8.m8.1b"><apply id="S6.I4.i1.p1.8.m8.1.2.cmml" xref="S6.I4.i1.p1.8.m8.1.2"><times id="S6.I4.i1.p1.8.m8.1.2.1.cmml" xref="S6.I4.i1.p1.8.m8.1.2.1"></times><ci id="S6.I4.i1.p1.8.m8.1.2.2.cmml" xref="S6.I4.i1.p1.8.m8.1.2.2">ğ•€</ci><ci id="S6.I4.i1.p1.8.m8.1.1.cmml" xref="S6.I4.i1.p1.8.m8.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.8.m8.1c">\mathbb{I}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.8.m8.1d">blackboard_I ( â‹… )</annotation></semantics></math> signifies an indicator function that yields a value of 1 when the <math alttext="i" class="ltx_Math" display="inline" id="S6.I4.i1.p1.9.m9.1"><semantics id="S6.I4.i1.p1.9.m9.1a"><mi id="S6.I4.i1.p1.9.m9.1.1" xref="S6.I4.i1.p1.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.9.m9.1b"><ci id="S6.I4.i1.p1.9.m9.1.1.cmml" xref="S6.I4.i1.p1.9.m9.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.9.m9.1c">i</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.9.m9.1d">italic_i</annotation></semantics></math>-th <math alttext="p_{i}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.10.m10.1"><semantics id="S6.I4.i1.p1.10.m10.1a"><msub id="S6.I4.i1.p1.10.m10.1.1" xref="S6.I4.i1.p1.10.m10.1.1.cmml"><mi id="S6.I4.i1.p1.10.m10.1.1.2" xref="S6.I4.i1.p1.10.m10.1.1.2.cmml">p</mi><mi id="S6.I4.i1.p1.10.m10.1.1.3" xref="S6.I4.i1.p1.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.10.m10.1b"><apply id="S6.I4.i1.p1.10.m10.1.1.cmml" xref="S6.I4.i1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.10.m10.1.1.1.cmml" xref="S6.I4.i1.p1.10.m10.1.1">subscript</csymbol><ci id="S6.I4.i1.p1.10.m10.1.1.2.cmml" xref="S6.I4.i1.p1.10.m10.1.1.2">ğ‘</ci><ci id="S6.I4.i1.p1.10.m10.1.1.3.cmml" xref="S6.I4.i1.p1.10.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.10.m10.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.10.m10.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is among the retrieval results for <math alttext="q_{i}" class="ltx_Math" display="inline" id="S6.I4.i1.p1.11.m11.1"><semantics id="S6.I4.i1.p1.11.m11.1a"><msub id="S6.I4.i1.p1.11.m11.1.1" xref="S6.I4.i1.p1.11.m11.1.1.cmml"><mi id="S6.I4.i1.p1.11.m11.1.1.2" xref="S6.I4.i1.p1.11.m11.1.1.2.cmml">q</mi><mi id="S6.I4.i1.p1.11.m11.1.1.3" xref="S6.I4.i1.p1.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.I4.i1.p1.11.m11.1b"><apply id="S6.I4.i1.p1.11.m11.1.1.cmml" xref="S6.I4.i1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S6.I4.i1.p1.11.m11.1.1.1.cmml" xref="S6.I4.i1.p1.11.m11.1.1">subscript</csymbol><ci id="S6.I4.i1.p1.11.m11.1.1.2.cmml" xref="S6.I4.i1.p1.11.m11.1.1.2">ğ‘</ci><ci id="S6.I4.i1.p1.11.m11.1.1.3.cmml" xref="S6.I4.i1.p1.11.m11.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I4.i1.p1.11.m11.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.I4.i1.p1.11.m11.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and 0 in all other cases.</p>
</div>
</li>
<li class="ltx_item" id="S6.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I4.i2.p1">
<p class="ltx_p" id="S6.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I4.i2.p1.1.1">Evaluating Recommendation Methods.</span>
We assess the effectiveness of the CTR prediction model using the AUC
and Group AUC (GAUC) metrics, where a higher AUC/GAUC value signifies superior ranking abilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhu
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib38" title="">2017</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib37" title="">2018</a>; Sheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib26" title="">2021</a>)</cite>.
</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Performance on Pre-Training Dataset</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Rationale for Utilizing Accuracy to Evaluate the Effectiveness of Pre-trained Representations</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">The most precise way to gauge the effectiveness of pre-trained representations is by measuring the improvement in recommendation accuracy with the integration of multimodal representations. However, this evaluation process can be lengthy for iteration of pre-training methods, and an intermediary metric for a quicker assessment of pre-trained multimodal representations is desirable.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">In our research, we observed <span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.p2.1.1">a strong correlation between the enhancement in pre-training accuracy and the boost in recommendation performance</span>. We illustrate this relationship in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.F6" title="Figure 6 â€£ 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">6</span></a>, where we can see that the improvement of Acc@1 is consistent with the improvement of GAUC. Hence, we predominantly rely on pre-training accuracy to determine the quality of multimodal representations. The exploration of other potential intermediary metrics for evaluating pre-trained multimodal representations remains a interesting topic for future work.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Importance of Semantic-Aware Contrastive Learning</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">To investigate how different pre-training tasks affect the quality of multimodal representations, we conducted a series of experiments.
The results, presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.T2" title="Table 2 â€£ 6.2.2. Importance of Semantic-Aware Contrastive Learning â€£ 6.2. Performance on Pre-Training Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">2</span></a>, offer two important observations. First, the proposed SCL pre-training method surpasses other semantic-similarity-agnostic methods, emphasizing <span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p1.1.1">the necessity of the semantic-aware learning.</span>. Second, incorporating techniques like Momentum Contrast (MoCo)Â <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib13" title="">2020</a>)</cite> and Triplet LossÂ <cite class="ltx_cite ltx_citemacro_citep">(Schroff
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib24" title="">2015</a>)</cite> further enhances the quality of the multimodal representations, demonstrating the choice of negative sample greatly impacts the performance.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Pre-training performance of different methods</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T2.1.1.1.1">Method</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.2">Acc@1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.1.1.1.3">Acc@5</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.2.2.1">CLIP-O</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2.2">0.2559</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2.3">0.3575</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T2.1.3.3.1">CLIP-E</th>
<td class="ltx_td ltx_align_center" id="S6.T2.1.3.3.2">0.2952</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.3.3.3">0.3917</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.4.4.1">SCL</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.4.4.2.1">0.7474</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.4.4.3.1">0.8850</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T2.1.5.5.1">Â Â â€ƒw/o Triplet Loss</th>
<td class="ltx_td ltx_align_center" id="S6.T2.1.5.5.2">0.6957</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.5.5.3">0.8604</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T2.1.6.6.1">Â Â â€ƒw/o Triplet Loss &amp; MoCo</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.6.6.2">0.5760</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.6.6.3">0.7590</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Performance on CTR prediction Dataset</h3>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Overall performance on CTR prediction dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.1">Method</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.2">GAUC</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.3">AUC</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.2.2.1">ID-based Model</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.2.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.2.2.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.3.3.1">Vector</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.3.3.2">+0.29%</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.3.3.3">+0.18%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.4.4.1">SimScore</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.4.4.2">+0.77%</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.4.4.3">+0.40%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.5.5.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T3.1.5.5.1.1">SimTier</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.5.5.2">+0.96%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.5.5.3">+0.59%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.6.6.1"><span class="ltx_text ltx_font_smallcaps" id="S6.T3.1.6.6.1.1">MAKE</span></th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.6.6.2">+0.93%</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.6.6.3">+0.51%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.1.7.7.1">
<span class="ltx_text ltx_font_smallcaps" id="S6.T3.1.7.7.1.1">SimTier</span>+<span class="ltx_text ltx_font_smallcaps" id="S6.T3.1.7.7.1.2">MAKE</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T3.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.7.7.2.1">+1.25%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T3.1.7.7.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.7.7.3.1">+0.75%</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S6.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="142" id="S6.F6.1.g1" src="x6.png" width="261"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="144" id="S6.F6.2.g1" src="x7.png" width="265"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="142" id="S6.F6.3.g1" src="x8.png" width="261"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>(a) The correlation between pre-training metric and CTR prediction performance. (b) The relative improvement on item groups with different frequency.
(c) The relative performance of the recommendation model with different pre-training epochs on <span class="ltx_text ltx_font_smallcaps" id="S6.F6.6.1">MAKE</span>. Note that 0 epoch implies that <span class="ltx_text ltx_font_smallcaps" id="S6.F6.7.2">MAKE</span> does not undergo pre-training, but is instead jointly optimized with the downstream task.
</figcaption>
</figure>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>
Performance on Different Integration Strategies</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">In the CTR prediction dataset, we evaluate the proposed <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.1">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.2">MAKE</span> against other methods. The overall results are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.T3" title="Table 3 â€£ 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">3</span></a>, from which two observations can be noted. Firstly, <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.3">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.4">MAKE</span> outperform other methods significantly.
Secondly, the combination of <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.5">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS1.p1.1.6">MAKE</span> can further improve the performance, with a 1.25% increase in GAUC, 0.75% increase in AUC compared with the ID-based model. The above results demonstrate the effectiveness of the proposed methods on integrating multimodal representations into the ID-based model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Performance on Different Training Epochs of <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.1.1">MAKE</span>
</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">To explore the impact of multi-epoch pre-training of <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.p1.1.1">MAKE</span> on the final recommendation performance, we conduct experiments where <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.p1.1.2">MAKE</span> is pre-trained for varying numbers of epochs before integration into the recommendation model. The results are shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.F6" title="Figure 6 â€£ 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">6</span></a>. Note that 0 epochs implies that <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.p1.1.3">MAKE</span> does not undergo pre-training and is instead jointly optimized with the downstream task. The results indicate that as the number of pre-training epochs for <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.p1.1.4">MAKE</span> increases, the performance of the final recommendation model also improves. This demonstrates that multi-epoch training of <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS2.p1.1.5">MAKE</span> effectively enhances model performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.3. </span>Performance on Infrequent Items.</h4>
<div class="ltx_para" id="S6.SS3.SSS3.p1">
<p class="ltx_p" id="S6.SS3.SSS3.p1.8">To examine the generalization ability of multimodal representation on long-tail items, we assess the relative improvement across item groups categorized by their frequency of occurrence. We divide all items into eight groups, with Group 1 containing items of the lowest frequency and Group 8 encompassing those with the highest frequency in the training dataset. Afterward, we compute the relative improvement of AUC as â€”AUC<math alttext="{}_{\text{MM}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.1.m1.1"><semantics id="S6.SS3.SSS3.p1.1.m1.1a"><msub id="S6.SS3.SSS3.p1.1.m1.1.1" xref="S6.SS3.SSS3.p1.1.m1.1.1.cmml"><mi id="S6.SS3.SSS3.p1.1.m1.1.1a" xref="S6.SS3.SSS3.p1.1.m1.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.1.m1.1.1.1" xref="S6.SS3.SSS3.p1.1.m1.1.1.1a.cmml">MM</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.1.m1.1b"><apply id="S6.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S6.SS3.SSS3.p1.1.m1.1.1"><ci id="S6.SS3.SSS3.p1.1.m1.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.1.m1.1.1.1"><mtext id="S6.SS3.SSS3.p1.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.1.m1.1.1.1">MM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.1.m1.1c">{}_{\text{MM}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.1.m1.1d">start_FLOATSUBSCRIPT MM end_FLOATSUBSCRIPT</annotation></semantics></math>-AUC<math alttext="{}_{\text{ID}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.2.m2.1"><semantics id="S6.SS3.SSS3.p1.2.m2.1a"><msub id="S6.SS3.SSS3.p1.2.m2.1.1" xref="S6.SS3.SSS3.p1.2.m2.1.1.cmml"><mi id="S6.SS3.SSS3.p1.2.m2.1.1a" xref="S6.SS3.SSS3.p1.2.m2.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.2.m2.1.1.1" xref="S6.SS3.SSS3.p1.2.m2.1.1.1a.cmml">ID</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.2.m2.1b"><apply id="S6.SS3.SSS3.p1.2.m2.1.1.cmml" xref="S6.SS3.SSS3.p1.2.m2.1.1"><ci id="S6.SS3.SSS3.p1.2.m2.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.2.m2.1.1.1"><mtext id="S6.SS3.SSS3.p1.2.m2.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.2.m2.1.1.1">ID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.2.m2.1c">{}_{\text{ID}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.2.m2.1d">start_FLOATSUBSCRIPT ID end_FLOATSUBSCRIPT</annotation></semantics></math>â€”/AUC<math alttext="{}_{\text{ID}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.3.m3.1"><semantics id="S6.SS3.SSS3.p1.3.m3.1a"><msub id="S6.SS3.SSS3.p1.3.m3.1.1" xref="S6.SS3.SSS3.p1.3.m3.1.1.cmml"><mi id="S6.SS3.SSS3.p1.3.m3.1.1a" xref="S6.SS3.SSS3.p1.3.m3.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.3.m3.1.1.1" xref="S6.SS3.SSS3.p1.3.m3.1.1.1a.cmml">ID</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.3.m3.1b"><apply id="S6.SS3.SSS3.p1.3.m3.1.1.cmml" xref="S6.SS3.SSS3.p1.3.m3.1.1"><ci id="S6.SS3.SSS3.p1.3.m3.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.3.m3.1.1.1"><mtext id="S6.SS3.SSS3.p1.3.m3.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.3.m3.1.1.1">ID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.3.m3.1c">{}_{\text{ID}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.3.m3.1d">start_FLOATSUBSCRIPT ID end_FLOATSUBSCRIPT</annotation></semantics></math> for each group, where <math alttext="MM" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.4.m4.1"><semantics id="S6.SS3.SSS3.p1.4.m4.1a"><mrow id="S6.SS3.SSS3.p1.4.m4.1.1" xref="S6.SS3.SSS3.p1.4.m4.1.1.cmml"><mi id="S6.SS3.SSS3.p1.4.m4.1.1.2" xref="S6.SS3.SSS3.p1.4.m4.1.1.2.cmml">M</mi><mo id="S6.SS3.SSS3.p1.4.m4.1.1.1" xref="S6.SS3.SSS3.p1.4.m4.1.1.1.cmml">â¢</mo><mi id="S6.SS3.SSS3.p1.4.m4.1.1.3" xref="S6.SS3.SSS3.p1.4.m4.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.4.m4.1b"><apply id="S6.SS3.SSS3.p1.4.m4.1.1.cmml" xref="S6.SS3.SSS3.p1.4.m4.1.1"><times id="S6.SS3.SSS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.SSS3.p1.4.m4.1.1.1"></times><ci id="S6.SS3.SSS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.SSS3.p1.4.m4.1.1.2">ğ‘€</ci><ci id="S6.SS3.SSS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.SSS3.p1.4.m4.1.1.3">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.4.m4.1c">MM</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.4.m4.1d">italic_M italic_M</annotation></semantics></math> denotes the combination of <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS3.p1.8.1">SimTier</span> and <span class="ltx_text ltx_font_smallcaps" id="S6.SS3.SSS3.p1.8.2">MAKE</span> method and <math alttext="ID" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.5.m5.1"><semantics id="S6.SS3.SSS3.p1.5.m5.1a"><mrow id="S6.SS3.SSS3.p1.5.m5.1.1" xref="S6.SS3.SSS3.p1.5.m5.1.1.cmml"><mi id="S6.SS3.SSS3.p1.5.m5.1.1.2" xref="S6.SS3.SSS3.p1.5.m5.1.1.2.cmml">I</mi><mo id="S6.SS3.SSS3.p1.5.m5.1.1.1" xref="S6.SS3.SSS3.p1.5.m5.1.1.1.cmml">â¢</mo><mi id="S6.SS3.SSS3.p1.5.m5.1.1.3" xref="S6.SS3.SSS3.p1.5.m5.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.5.m5.1b"><apply id="S6.SS3.SSS3.p1.5.m5.1.1.cmml" xref="S6.SS3.SSS3.p1.5.m5.1.1"><times id="S6.SS3.SSS3.p1.5.m5.1.1.1.cmml" xref="S6.SS3.SSS3.p1.5.m5.1.1.1"></times><ci id="S6.SS3.SSS3.p1.5.m5.1.1.2.cmml" xref="S6.SS3.SSS3.p1.5.m5.1.1.2">ğ¼</ci><ci id="S6.SS3.SSS3.p1.5.m5.1.1.3.cmml" xref="S6.SS3.SSS3.p1.5.m5.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.5.m5.1c">ID</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.5.m5.1d">italic_I italic_D</annotation></semantics></math> represents the ID-based production baseline model. We also compute the relative improvement of LogLoss as â€”LogLoss<math alttext="{}_{\text{MM}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.6.m6.1"><semantics id="S6.SS3.SSS3.p1.6.m6.1a"><msub id="S6.SS3.SSS3.p1.6.m6.1.1" xref="S6.SS3.SSS3.p1.6.m6.1.1.cmml"><mi id="S6.SS3.SSS3.p1.6.m6.1.1a" xref="S6.SS3.SSS3.p1.6.m6.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.6.m6.1.1.1" xref="S6.SS3.SSS3.p1.6.m6.1.1.1a.cmml">MM</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.6.m6.1b"><apply id="S6.SS3.SSS3.p1.6.m6.1.1.cmml" xref="S6.SS3.SSS3.p1.6.m6.1.1"><ci id="S6.SS3.SSS3.p1.6.m6.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.6.m6.1.1.1"><mtext id="S6.SS3.SSS3.p1.6.m6.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.6.m6.1.1.1">MM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.6.m6.1c">{}_{\text{MM}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.6.m6.1d">start_FLOATSUBSCRIPT MM end_FLOATSUBSCRIPT</annotation></semantics></math>-LogLoss<math alttext="{}_{\text{ID}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.7.m7.1"><semantics id="S6.SS3.SSS3.p1.7.m7.1a"><msub id="S6.SS3.SSS3.p1.7.m7.1.1" xref="S6.SS3.SSS3.p1.7.m7.1.1.cmml"><mi id="S6.SS3.SSS3.p1.7.m7.1.1a" xref="S6.SS3.SSS3.p1.7.m7.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.7.m7.1.1.1" xref="S6.SS3.SSS3.p1.7.m7.1.1.1a.cmml">ID</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.7.m7.1b"><apply id="S6.SS3.SSS3.p1.7.m7.1.1.cmml" xref="S6.SS3.SSS3.p1.7.m7.1.1"><ci id="S6.SS3.SSS3.p1.7.m7.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.7.m7.1.1.1"><mtext id="S6.SS3.SSS3.p1.7.m7.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.7.m7.1.1.1">ID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.7.m7.1c">{}_{\text{ID}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.7.m7.1d">start_FLOATSUBSCRIPT ID end_FLOATSUBSCRIPT</annotation></semantics></math>â€”/LogLoss<math alttext="{}_{\text{ID}}" class="ltx_Math" display="inline" id="S6.SS3.SSS3.p1.8.m8.1"><semantics id="S6.SS3.SSS3.p1.8.m8.1a"><msub id="S6.SS3.SSS3.p1.8.m8.1.1" xref="S6.SS3.SSS3.p1.8.m8.1.1.cmml"><mi id="S6.SS3.SSS3.p1.8.m8.1.1a" xref="S6.SS3.SSS3.p1.8.m8.1.1.cmml"></mi><mtext id="S6.SS3.SSS3.p1.8.m8.1.1.1" xref="S6.SS3.SSS3.p1.8.m8.1.1.1a.cmml">ID</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p1.8.m8.1b"><apply id="S6.SS3.SSS3.p1.8.m8.1.1.cmml" xref="S6.SS3.SSS3.p1.8.m8.1.1"><ci id="S6.SS3.SSS3.p1.8.m8.1.1.1a.cmml" xref="S6.SS3.SSS3.p1.8.m8.1.1.1"><mtext id="S6.SS3.SSS3.p1.8.m8.1.1.1.cmml" mathsize="70%" xref="S6.SS3.SSS3.p1.8.m8.1.1.1">ID</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p1.8.m8.1c">{}_{\text{ID}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS3.p1.8.m8.1d">start_FLOATSUBSCRIPT ID end_FLOATSUBSCRIPT</annotation></semantics></math> to measure the calibration abilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib12" title="">2017</a>; Sheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib25" title="">2023</a>)</cite>. The result presented in the FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#S6.F6" title="Figure 6 â€£ 6.3. Performance on CTR prediction Dataset â€£ 6. Experiment â€£ Enhancing Taobao Display Advertising with Multimodal Representations: Challenges, Approaches and Insights"><span class="ltx_text ltx_ref_tag">6</span></a> indicates that the multimodal representation exhibits a significant improvement in all groups, demonstrating the effectiveness of our model over different types of items. Meanwhile, we see <span class="ltx_text ltx_font_bold" id="S6.SS3.SSS3.p1.8.3">a more significant improvement for low-frequency items</span>. The result demonstrate that multimodal representations can address the shortcomings of ID-based model on long-tail items and enhance the prediction accuracy.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Online Performance</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Since mid-2023, multimodal representations have been integrated into pre-ranking, ranking, and re-ranking models within the Taobao display advertising system, resulting in substantial performance improvements. For instance, incorporating image representations in the CTR prediction model yielded a overall 3.5% increase in CTR, a 1.5% boost in RPM, and a 2.9% rise in ROI. Notably, the impact was even more pronounced for new ads (created within the last 24 hours), with improvements of 6.9% in CTR, 3.7% in RPM, and 7.7% in ROI. The significant gains on new ads also validate the effectiveness of multimodal data in mitigating the cold-start issue.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Related Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Currently, ID features constitute the core of industrial recommendation modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Cheng
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib6" title="">2016</a>; Covington
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib8" title="">2016</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib37" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib36" title="">2019</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib34" title="">2022c</a>; Chan etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib3" title="">2020</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib32" title="">2022a</a>; Hu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib15" title="">2023</a>)</cite>. Despite their widespread adoption, ID features have notable limitations, including the challenge of capturing semantic information and the persistent issue of the cold-start problemÂ <cite class="ltx_cite ltx_citemacro_citep">(Schein
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib23" title="">2002</a>; Ge etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib10" title="">2018</a>; Mo
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib19" title="">2015</a>; Wu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib29" title="">2022</a>)</cite>. In contrast, multimodal data offer rich semantic information, prompting numerous studies to explore their incorporation into recommendation models. Some research has investigated the potential of learning multimodal representations in an end-to-end manner alongside recommendation model trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib5" title="">2016</a>; Yuan etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib31" title="">2023</a>; Elsayed etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib9" title="">2022</a>)</cite>. However, the substantial computational resources required for such processes often preclude their adoption in industrial systems. Therefore, our focus is on the two-phase paradigmÂ <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib7" title="">2012</a>; Lynch
etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib18" title="">2016</a>; Ge etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib10" title="">2018</a>; He and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib14" title="">2016</a>; Singh etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib27" title="">2023</a>; Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib30" title="">2023</a>; Pal etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.19467v1#bib.bib21" title="">2020</a>)</cite>, and we aim to share our methods and the valuable insights.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion and Discussion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Multimodal-based recommendation has attracted attention over decades.
However, integrating multimodal representations into industrial systems presents many hard challenges, particularly in the realms of representation quality, integration methods, and system implementationâ€”challenges that are amplified within the context of large-scale industrial systems.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">In this study, we delve into these challenges and share the approaches we employed for pre-training and incorporation of multimodal representations. Additionally, we provide insights gleaned from our experiences during the online deployment stage. We believe the strategies and insights we have amassed through our journey will serve as a valuable resource for those aiming to expedite the adoption of multimodal-based recommendations in industrial systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bian etÂ al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Weijie Bian, Kailun Wu,
Lejian Ren, Qi Pi,
Yujing Zhang, Can Xiao,
Xiang-Rong Sheng, Yong-Nan Zhu,
Zhangming Chan, Na Mou,
Xinchen Luo, Shiming Xiang,
Guorui Zhou, Xiaoqiang Zhu, and
Hongbo Deng. 2022.

</span>
<span class="ltx_bibblock">CAN: Feature Co-Action Network for Click-Through
Rate Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 15th ACM
International Conference on Web Search and Data Mining</em>.
57â€“65.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhangming Chan, Yuchi
Zhang, Xiuying Chen, Shen Gao,
Zhiqiang Zhang, Dongyan Zhao, and
Rui Yan. 2020.

</span>
<span class="ltx_bibblock">Selection and generation: Learning towards
multi-product advertisement post generation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>.
3818â€“3829.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhangming Chan, Yu Zhang,
Shuguang Han, Yong Bai,
Xiang-Rong Sheng, Siyuan Lou,
Jiacen Hu, Baolin Liu,
Yuning Jiang, Jian Xu, and
Bo Zheng. 2023.

</span>
<span class="ltx_bibblock">Capturing Conversion Rate Fluctuation during Sales
Promotions: A Novel Historical Data Reuse Approach. In
<em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining</em>. 3774â€“3784.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Junxuan Chen, Baigui Sun,
Hao Li, Hongtao Lu, and
Xian-Sheng Hua. 2016.

</span>
<span class="ltx_bibblock">Deep CTR Prediction in Display Advertising. In
<em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 2016 ACM Conference on
Multimedia Conference</em>. 811â€“820.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
etÂ al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Heng-Tze Cheng, Levent
Koc, Jeremiah Harmsen, Tal Shaked,
Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado,
Wei Chai, Mustafa Ispir,
Rohan Anil, Zakaria Haque,
Lichan Hong, Vihan Jain,
Xiaobing Liu, and Hemal Shah.
2016.

</span>
<span class="ltx_bibblock">Wide &amp; deep learning for recommender systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 1st Workshop on Deep Learning
for Recommender Systems</em>. ACM, 7â€“10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Haibin Cheng, Roelof van
Zwol, Javad Azimi, Eren Manavoglu,
Ruofei Zhang, Yang Zhou, and
Vidhya Navalpakkam. 2012.

</span>
<span class="ltx_bibblock">Multimedia features for click prediction of new ads
in display advertising. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 18th
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining</em>. 777â€“785.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington
etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay
Adams, and Emre Sargin.
2016.

</span>
<span class="ltx_bibblock">Deep Neural Networks for YouTube Recommendations.
In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 10th ACM Conference on
Recommender Systems</em>. 191â€“198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elsayed etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shereen Elsayed, Lukas
Brinkmeyer, and Lars Schmidt-Thieme.
2022.

</span>
<span class="ltx_bibblock">End-to-End Image-Based Fashion Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">CoRR</em> abs/2205.02923
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge etÂ al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tiezheng Ge, Liqin Zhao,
Guorui Zhou, Keyu Chen,
Shuying Liu, Huiming Yi,
Zelin Hu, Bochao Liu,
Peng Sun, Haoyu Liu,
Pengtao Yi, Sui Huang,
Zhiqiang Zhang, Xiaoqiang Zhu,
Yu Zhang, and Kun Gai.
2018.

</span>
<span class="ltx_bibblock">Image Matters: Visually Modeling User Behaviors
Using Advanced Model Server. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the
27th ACM International Conference on Information and Knowledge
Management</em>. 2087â€“2095.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu
etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Siyu Gu, Xiang-Rong
Sheng, Ying Fan, Guorui Zhou, and
Xiaoqiang Zhu. 2021.

</span>
<span class="ltx_bibblock">Real Negatives Matter: Continuous Training with
Real Negatives for Delayed Feedback Modeling. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of The 27th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining</em>. 2890â€“2898.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo
etÂ al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Chuan Guo, Geoff Pleiss,
Yu Sun, and KilianÂ Q. Weinberger.
2017.

</span>
<span class="ltx_bibblock">On Calibration of Modern Neural Networks. In
<em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 34th International Conference on
Machine Learning</em>, Vol.Â 70. 1321â€“1330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
etÂ al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kaiming He, Haoqi Fan,
Yuxin Wu, Saining Xie, and
RossÂ B. Girshick. 2020.

</span>
<span class="ltx_bibblock">Momentum Contrast for Unsupervised Visual
Representation Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.
9726â€“9735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and McAuley (2016)</span>
<span class="ltx_bibblock">
Ruining He and JulianÂ J.
McAuley. 2016.

</span>
<span class="ltx_bibblock">Ups and Downs: Modeling the Visual Evolution of
Fashion Trends with One-Class Collaborative Filtering. In
<em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 25th International Conference on
World Wide Web</em>. 507â€“517.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiacen Hu, Zhangming
Chan, Yu Zhang, Shuguang Han,
Siyuan Lou, Baolin Liu,
Han Zhu, Yuning Jiang,
Jian Xu, and Bo Zheng.
2023.

</span>
<span class="ltx_bibblock">PS-SA: An Efficient Self-Attention via Progressive
Sampling for User Behavior Sequence Modeling. In
<em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management</em>.
4639â€“4645.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jui-Ting Huang, Ashish
Sharma, Shuying Sun, Li Xia,
David Zhang, Philip Pronin,
Janani Padmanabhan, Giuseppe Ottaviano,
and Linjun Yang. 2020.

</span>
<span class="ltx_bibblock">Embedding-based Retrieval in Facebook Search. In
<em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of The 26th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining</em>. 2553â€“2561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al<span class="ltx_text" id="bib.bib17.3.3.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Biye Jiang, Chao Deng,
Huimin Yi, Zelin Hu,
Guorui Zhou, Yang Zheng,
Sui Huang, Xinyang Guo,
Dongyue Wang, Yue Song, etÂ al<span class="ltx_text" id="bib.bib17.4.1">.</span>
2019.

</span>
<span class="ltx_bibblock">XDL: An Industrial Deep Learning Framework for
High-Dimensional Sparse Data. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.5.1">Proceedings of
the 1st International Workshop on Deep Learning Practice for High-Dimensional
Sparse Data</em>. 1â€“9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lynch
etÂ al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Corey Lynch, Kamelia
Aryafar, and Josh Attenberg.
2016.

</span>
<span class="ltx_bibblock">Images Donâ€™t Lie: Transferring Deep Visual Semantic
Features to Large-Scale Multimodal Learning to Rank. In
<em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining</em>.
541â€“548.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo
etÂ al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Kaixiang Mo, Bo Liu,
Lei Xiao, Yong Li, and
Jie Jiang. 2015.

</span>
<span class="ltx_bibblock">Image Feature Learning for Cold Start Problem in
Display Advertising. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the
Twenty-Fourth International Joint Conference on Artificial Intelligence</em>.
3728â€“3734.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord
etÂ al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Aaron vanÂ den Oord, Yazhe
Li, and Oriol Vinyals. 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive
coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:1807.03748</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal etÂ al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Aditya Pal, Chantat
Eksombatchai, Yitong Zhou, Bo Zhao,
Charles Rosenberg, and Jure Leskovec.
2020.

</span>
<span class="ltx_bibblock">PinnerSage: Multi-Modal User Embedding Framework
for Recommendations at Pinterest. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings
of The 26th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining</em>. 2311â€“2320.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford
etÂ al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook
Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.
2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural
Language Supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 38th
International Conference on Machine Learning</em>, Vol.Â 139.
8748â€“8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schein
etÂ al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
AndrewÂ I. Schein,
Alexandrin Popescul, LyleÂ H. Ungar, and
DavidÂ M. Pennock. 2002.

</span>
<span class="ltx_bibblock">Methods and metrics for cold-start
recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 25th Annual
International ACM SIGIR Conference on Research and Development in
Information Retrieval</em>. 253â€“260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schroff
etÂ al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Florian Schroff, Dmitry
Kalenichenko, and James Philbin.
2015.

</span>
<span class="ltx_bibblock">FaceNet: A unified embedding for face recognition
and clustering. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>.
815â€“823.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng etÂ al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiang-Rong Sheng,
Jingyue Gao, Yueyao Cheng,
Siran Yang, Shuguang Han,
Hongbo Deng, Yuning Jiang,
Jian Xu, and Bo Zheng.
2023.

</span>
<span class="ltx_bibblock">Joint Optimization of Ranking and Calibration with
Contextualized Hybrid Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>.
4813â€“4822.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng etÂ al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiang-Rong Sheng, Liqin
Zhao, Guorui Zhou, Xinyao Ding,
Binding Dai, Qiang Luo,
Siran Yang, Jingshan Lv,
Chi Zhang, Hongbo Deng, and
Xiaoqiang Zhu. 2021.

</span>
<span class="ltx_bibblock">One Model to Serve All: Star Topology Adaptive
Recommender for Multi-Domain CTR Prediction. In
<em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of The 30th ACM International
Conference on Information and Knowledge Management</em>.
4104â€“4113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Anima Singh, Trung Vu,
RaghunandanÂ H. Keshavan, Nikhil Mehta,
Xinyang Yi, Lichan Hong,
Lukasz Heldt, Li Wei,
EdÂ H. Chi, and Maheswaran
Sathiamoorthy. 2023.

</span>
<span class="ltx_bibblock">Better Generalization with Semantic IDs: A case
study in Ranking for Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">CoRR</em> abs/2306.08121
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhe Wang, Liqin Zhao,
Biye Jiang, Guorui Zhou,
Xiaoqiang Zhu, and Kun Gai.
2020.

</span>
<span class="ltx_bibblock">COLD: Towards the Next Generation of Pre-Ranking
System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">CoRR</em> abs/2007.16122
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kailun Wu, Weijie Bian,
Zhangming Chan, Lejian Ren,
Shiming Xiang, Shu-Guang Han,
Hongbo Deng, and Bo Zheng.
2022.

</span>
<span class="ltx_bibblock">Adversarial gradient driven exploration for deep
click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>.
2050â€“2058.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jia-Qi Yang, Chenglei
Dai, Dan Ou, Ju Huang,
De-Chuan Zhan, Qingwen Liu,
Xiaoyi Zeng, and Yang Yang.
2023.

</span>
<span class="ltx_bibblock">COURIER: Contrastive User Intention
Reconstruction for Large-Scale Pre-Train of Image Features.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">CoRR</em> abs/2306.05001
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Fajie Yuan,
Yu Song, Youhua Li,
Junchen Fu, Fei Yang,
Yunzhu Pan, and Yongxin Ni.
2023.

</span>
<span class="ltx_bibblock">Where to Go Next for Recommender Systems? ID- vs.
Modality-based Recommender Models Revisited. In
<em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
2639â€“2649.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yujing Zhang, Zhangming
Chan, Shuhao Xu, Weijie Bian,
Shuguang Han, Hongbo Deng, and
Bo Zheng. 2022a.

</span>
<span class="ltx_bibblock">KEEP: An Industrial Pre-Training Framework for
Online Recommendation via Knowledge Extraction and Plugging. In
<em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 31st ACM International
Conference on Information &amp; Knowledge Management</em>.
3684â€“3693.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span class="ltx_text" id="bib.bib33.3.3.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuanxing Zhang, Langshi
Chen, Siran Yang, Man Yuan,
Huimin Yi, etÂ al<span class="ltx_text" id="bib.bib33.4.1">.</span>
2022b.

</span>
<span class="ltx_bibblock">PICASSO: Unleashing the Potential of GPU-centric
Training for Wide-and-deep Recommender Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib33.5.1">2022 IEEE 38th International Conference on Data
Engineering (ICDE)</em>. 3453â€“3466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022c)</span>
<span class="ltx_bibblock">
Zhao-Yu Zhang,
Xiang-Rong Sheng, Yujing Zhang,
Biye Jiang, Shuguang Han,
Hongbo Deng, and Bo Zheng.
2022c.

</span>
<span class="ltx_bibblock">Towards Understanding the Overfitting Phenomenon of
Deep Click-Through Rate Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of
the 31st ACM International Conference on Information &amp; Knowledge
Management</em>. 2671â€“2680.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhishan Zhao, Jingyue
Gao, Yu Zhang, Shuguang Han,
Siyuan Lou, Xiang-Rong Sheng,
Zhe Wang, Han Zhu,
Yuning Jiang, Jian Xu, and
Bo Zheng. 2023.

</span>
<span class="ltx_bibblock">COPR: Consistency-Oriented Pre-Ranking for Online
Advertising. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management</em>.
4974â€“4980.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Guorui Zhou, Na Mou,
Ying Fan, Qi Pi, Weijie
Bian, Chang Zhou, Xiaoqiang Zhu, and
Kun Gai. 2019.

</span>
<span class="ltx_bibblock">Deep Interest Evolution Network for Click-Through
Rate Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 33rd AAAI
Conference on Artificial Intelligence</em>. Honolulu, Hawaii,
USA, 5941â€“5948.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Guorui Zhou, Xiaoqiang
Zhu, Chenru Song, Ying Fan,
Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin,
Han Li, and Kun Gai.
2018.

</span>
<span class="ltx_bibblock">Deep interest network for click-through rate
prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &amp; Data Mining</em>. ACM,
1059â€“1068.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
etÂ al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Han Zhu, Junqi Jin,
Chang Tan, Fei Pan,
Yifan Zeng, Han Li, and
Kun Gai. 2017.

</span>
<span class="ltx_bibblock">Optimized Cost per Click in Taobao Display
Advertising. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining</em>.
2191â€“2200.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
etÂ al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Han Zhu, Xiang Li,
Pengye Zhang, Guozheng Li,
Jie He, Han Li, and
Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Learning Tree-based Deep Model for Recommender
Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &amp; Data Mining</em>.
London, UK, 1079â€“1088.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jul 28 11:28:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
