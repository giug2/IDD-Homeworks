<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.17874] VR-based generation of photorealistic synthetic data for training hand-object tracking models</title><meta property="og:description" content="Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g.…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VR-based generation of photorealistic synthetic data for training hand-object tracking models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VR-based generation of photorealistic synthetic data for training hand-object tracking models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.17874">

<!--Generated on Tue Feb 27 06:25:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VR-based generation of photorealistic synthetic data for training hand-object tracking models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present ”blender-hoisynth”, an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text" style="font-size:80%;">”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.”</span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms —</span> 3D, Synthetic data, Photorealistic, Hand-Object Interaction</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The semantic understanding of hand-object interactions is dependent on low-level Computer vision tasks such as detection, localization, 6DoF pose estimation and tracking of both hands and objects. Solutions to these tasks, especially the ones based on Deep Learning, rely on good quality raw visual data and ground truth annotations. Several datasets have been introduced in the literature to address this need. Prominent examples include LineMOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, YCB-Video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, T-LESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, HOPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and HomebrewedDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for object-only data and DexYCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and H2O-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for hand-object interaction data. A large amount of resources and effort are usually spent on building such datasets – both for recording raw sensor data as well as for annotating the acquired data. Annotations such as marking 3D bounding box vertices on 2D images cannot be easily done by non-experts. Also, the dataset cannot be easily extended without investing a similar amount of effort as before.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address the above issues, researchers have resorted to synthetically generated data. The popularity and efficacy of such approaches is driven by a steadily shrinking “sim2real gap” as more and more high quality 3D scans/models and renderers become available. Synthetic data generation based on virtual models is fast, inexpensive, and highly scalable. Furthermore, 2D and 3D data annotations such as object identities and bounding boxes, segmentation masks, 6D pose, etc. practically come for free with the raw data.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In this work we introduce a novel interactive synthetic data generator called “blender-hoisynth”, a short form for “blender-based hand-object interaction synthetic data generator”. Here, blender<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>www.blender.org</span></span></span> refers to the free and open-source 3D software popularly used for creating object and scene models, animated films, visual effects, and interactive 3D applications. Blender-hoisynth allows intuitive interaction between virtual hands and objects in a scene using standard Virtual Reality (VR) hardware. The interaction can be saved as animation data, and subsequently re-rendered according to user requirements to produce synthetic raw sensor data and annotations useful for Machine Learning. An example of a multi-view render from hoisynth is shown in the picture on Page 1.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Secondly, we provide a synthetically generated hand-object interaction dataset with objects, grasping style and scenarios similar to the ones in the DexYCB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Note that the objects mentioned here are from the Yale-CMU-Berkeley (YCB) object set<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://registry.opendata.aws/ycb-benchmarks</span></span></span>. We name this dataset “SynthDexYCB”. This allows the study of performance on typical Computer Visions tasks while combining the synthetic data from blender-hoisynth with the original real-world DexYCB dataset in different ways.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Thirdly, to show the efficacy of SynthDexYCB, we train a representative state-of-the-art hand-object mesh reconstruction model – geometry-aligned Signed Distance Function (gSDF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> – on a mixture of synthetic and real data from DexYCB. We show that such a mixture does not significantly degrade in the model performance, indicating that the synthetic data quality is comparable to that of the real data.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Game engine technology</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Most of the state-of-the-art synthetic data generators are based on commercial game engines such as Unity<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>www.unity.com</span></span></span> or Unreal Engine<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>www.unrealengine.com</span></span></span> (UE). To the best of our knowledge, blender-hoisynth is the first work relying on the free and open-source Blender game engine. Unlike Unity or UE, which only provide free versions for personal use, Blender can be used by teams of any size free of cost. Unlike Unity or UE, with blender-hoisynth creation of assets and HOI recordings can both happen in one place. We use a special fork of Blender called “UPBGE”<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>www.upbge.org</span></span></span> after Uchronia Project Blender Game Engine. In the following, we use the names “Blender” and “UPBGE” interchangeably.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Works based on commercial game engines often only release the simulator containing the virtual world in binary form. Sometimes this may be due to licensing restrictions. But mostly the intention is to preclude the need for researchers to learn a new and complex game engine framework and programming language (e.g. C# for Unity, C++ for UE). A Python API is additionally offered to interact with the simulator binary. However, this way the researcher only has an indirect and limited control over the 3D environment, making customization difficult. In the cases where the simulator source code is released, the code repository can easily run into tens of gigabytes in size.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">In contrast to the above approaches, blender-hoisynth is a single application purely written in Python, the most popular programming language in Computer Vision and AI at present. It is highly customizable, as any python library (e.g. a differentiable renderer) can be easily installed and directly used in the simulator. Python wrappers can be written for hardware drivers using SWIG<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>www.swig.org</span></span></span> enabling access to any VR hardware in blender-hoisynth. For robotics applications, it is possible to communicate with external hardware by using, e.g. the PySerial Python module or Python wrappers for the Robot Operating System (ROS<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>www.ros.org</span></span></span>) nodes. Due to all these features, we believe blender-hoisynth has the potential for widespread adoption among researchers.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Applications</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Blender-hoisynth can export multi-view visual data and 2D/3D ground truth annotations of hand-object interactions. These data can be used by researchers for Computer Vision (CV) tasks such as 3D reconstruction, monocular or stereo depth estimation, semantic segmentation, normal estimation, etc. Also high-level CV tasks such as hand-object detection, pose estimation and tracking, visual object grasping and manipulation, etc. can benefit from training on the data exported from blender-hoisynth.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">Just as GPS on mobile phones enables several higher-level applications (navigation, neighborhood search), tracking of hands and objects can enable higher-level applications where humans manually interact with objects. Possible examples include: assistance/training in manual assembly of complex structures from parts, in medical surgery (visualization of target organs, prediction of complications), assistance for patients of dementia in conducting activities of daily living, etc.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetically generated data has been used increasingly in the past decade for training data-hungry Machine Learning models in Robotics and Computer Vision. Works in this direction include the Cornell House Agent Learning Environment (CHALET) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Household Multimodal Environment (HoME) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, AI2-THOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, Multimodal Indoor Simulator (MINOS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, Gibson <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, etc. Since these papers are not directly relevant for our work, we only mention their drawbacks here briefly: the lack of (photo-)realism, missing embodiment of the agent or its parts, inflexible camera configurations (number or poses of cameras), and only discrete interactions with the scene. They are generally characterized by a focus on larger building-scale 3D environments without necessarily a real-world correspondence, and on navigation rather than interaction. Another synthetic data generator called BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has been one of the main inspirations for our proposal. It is built on Blender and is characterized by photorealistic data and flexible camera configurations. However, it does not have an embodied agent and lacks real-time user interaction with the scene.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">We are interested in synthetic data generation from 3D scenes containing embodied hands interacting with objects through their hands. Works in this direction include: HOISIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, VirtualHome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Sims4Action <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and ElderSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. However, none of these works offer the user control over the virtual agent in real-time. Animations are either pre-recorded or generated automatically using standard game engine functionality such as navigation meshes, inverse kinematics for skeleton deformations, and collision detection between the agent and objects.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Finally, here we review some works allowing interactive control of the agent and 3D object grasping using VR hardware. The Modular Open Robots Simulation Engine (MORSE) presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> from 2011 was an early work in this direction. Similar to BlenderProc, MORSE is based on Blender and is one of the main inspirations for our work. Unfortunately, the project hasn’t been actively developed for several years now. A very recent related approach, presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, combines virtual environments from the Habitat-Matterport 3D dataset and object-grasp pairs selected from DexGraspNet to generate fully-annotated HOI synthetic data. However, it does not have an embodied agent and lacks real-time user interaction with the scene.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Unreal Engine-based VRKitchen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and Unity-based ThreeDWorld <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> both support interaction with 3D objects using VR. Both these simulators follow the server-client architecture, with the server consisting of a binary of the game engine program and the client consisting of a Python program interacting with the server. As mentioned before in Section <a href="#S1.SS2" title="1.2 Game engine technology ‣ 1 Introduction ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.2</span></a>, this way the user only has an indirect and limited control on the 3D environment, making customization difficult.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Unreal Engine (UE)-based UnrealROX, presented by Martinez-Gonzalez et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, comes closest to our work in terms of the goals as well as the approach. Among other things, like blender-hoisynth it can produce photorealistic data and ground truth annotations for user-recorded hand-object interactions. UnrealROX is a single application (no server-client) and the authors have made the entire code available. Since all this makes UnrealROX very promising, we experimented extensively with it before deciding to develop our own simulator. We found that UE has a complex architecture and workflows. UE applications must be programmed in C++ or the Blueprint system, both of which have a steep learning curve. The engine and the application together occupy 40-50GB of disk space and have high runtime resource requirements. UnrealROX was originally implemented using UE v4.18, which was released in 2017. The project has not been actively developed for more than two years at the time of this writing. We found it very difficult to upgrade UnrealROX to more recent versions of UE while maintaining original functionality. As an example, human skeleton tracking failed in going from UE v4.18 to v4.26. We believe that blender-hoisynth, with its reliance on the compact Blender Game Engine (couple of gigabytes in disk space and with moderate runtime requirements) and the Python programming language, can be more accessible to a wider audience both for use as well as for further customization.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>blender-hoisynth</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2401.17874/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Overview of blender-hoisynth and its usage.Fast iteration of HOI tracking model development can be achieved by customizing the data generator as required (dashed errors) in a feedback loop.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The 3D environment in blender-hoisynth consists of textured 3D meshes of hands, objects, and the surrounding environment obtained from 3D scanners. Given the popularity of synthetic data generation methods in recent years, large repositories of high-quality object scans published by researchers can also be found and imported.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To interact with virtual objects, different VR hardware can be used for controlling the 6D pose of the virtual hand and the joint angles of the individual fingers. So far we have experimented with the Leap Motion Controller from Ultraleap Inc.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>www.ultraleap.com</span></span></span>, the Meta Quest 2 VR controllers<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>www.meta.com/de/quest/products/quest-2/</span></span></span>, haptic I/O devices such as the Geomagic Touch<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>www.3dsystems.com/haptics-devices/touch</span></span></span>, and the OptiTrack<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>www.optitrack.com/</span></span></span> motion capture system.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Interactions typically consist of the hands approaching an object, grasping it, moving it around, and placing it somewhere. The hand-object interaction session can be recorded and played back later for review. If the user is satisfied with the created animation, the desired data can be rendered out. Figure <a href="#S3.F1" title="Figure 1 ‣ 3 blender-hoisynth ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of the main components of blender-hoisynth.</p>
</div>
<section id="S3.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Hand-object interaction</h4>

<div id="S3.SS0.SSSx1.p1" class="ltx_para">
<p id="S3.SS0.SSSx1.p1.1" class="ltx_p">Blender-hoisynth firstly loads the 3D environment and hand-object meshes. Then it places the interactable objects on a surface chosen by the user with random poses. If desired, the user can also place the objects manually in the scene. Users immerse themselves into the 3D scene using a VR-headset and hand controllers. Presently we use the Meta Quest 2 headset for this purpose, since it is relatively inexpensive and is able to track the controllers in 3D space very precisely without the need for additional base stations.</p>
</div>
<div id="S3.SS0.SSSx1.p2" class="ltx_para">
<p id="S3.SS0.SSSx1.p2.1" class="ltx_p">Hands are represented in the scene in the form of a mesh parented to a standard hierarchical bone rig as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ Hand-object interaction ‣ 3 blender-hoisynth ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Each bone in the rig is associated with vertices on the hand mesh with pre-defined weights that display realistic hand deformations. Controlling the relative rotations of the bones deforms the hand mesh accordingly. We drive the wrists with the 6D pose of the VR controllers. We use the analog trigger signal from the controller for driving the curling motion of the fingers when grasping an object.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2401.17874/assets/figures/rigged_sensorized_hand.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Virtual hand mesh showing the hierarchical bone rig and the half-cylinders collision sensors.</figcaption>
</figure>
<div id="S3.SS0.SSSx1.p3" class="ltx_para">
<p id="S3.SS0.SSSx1.p3.1" class="ltx_p">The fingers are clad from the inside with “sensors” (visualized in Figure <a href="#S3.F2" title="Figure 2 ‣ Hand-object interaction ‣ 3 blender-hoisynth ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). These sensors are invisible to the user. When a collision is detected between the sensor and an object, the corresponding finger stops curling. When the thumb and at least one of the other four fingers are in contact with an object, the object is deemed to have been grasped. While being in the grasped state, the object is parented to the hand and thus follows the hand motion in 3D space.</p>
</div>
<div id="S3.SS0.SSSx1.p4" class="ltx_para">
<p id="S3.SS0.SSSx1.p4.1" class="ltx_p">High-polygon object meshes in the scene are decimated to a couple of hundred polyons at most. At this level of decimation, the objects retain most of their characteristic shape making visually plausible grasping possible. These reductions are only necessary when the user is interacting with the scene in real-time. When the scene is rendered subsequently for data export, the original high-resolution meshes are used. During the interaction, poses of hands and objects are continuously being recorded and get saved as “action” data in the Blender standard “blend” file format.</p>
</div>
</section>
<section id="S3.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Implementation in Blender/UPBGE</h4>

<div id="S3.SS0.SSSx2.p1" class="ltx_para">
<p id="S3.SS0.SSSx2.p1.1" class="ltx_p">The blender game engine in the form of UPBGE (see Section <a href="#S1.SS2" title="1.2 Game engine technology ‣ 1 Introduction ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.2</span></a>) enforces an object-oriented and modular architecture by default. Python components for hand/finger control, collision detection and response, object pose update, etc. are directly attached to the relevant simulation objects in the scene. They typically contain a function for initializing the object once at the beginning and other functions that are called at each simulation step. For example, when an object is deemed to be grasped by the hand, the object pose is updated at every frame to follow the hand. A single blend-file contains all the 3D scene data, the code, as well as the recorded interactions.</p>
</div>
<div id="S3.SS0.SSSx2.p2" class="ltx_para">
<p id="S3.SS0.SSSx2.p2.1" class="ltx_p">The desired data can be exported from the session recorded in blender-hoisynth in an offline rendering step. Depending on the camera viewpoints desired, the user can specify camera poses manually in the scene. It is also possible to only specify the number of cameras and let blender-hoisynth sample camera poses automatically, e.g. over a spherical surface. There is no restriction on the number of cameras that can be rendered. We set the virtual camera parameters to be the same as those of the real cameras in DexYCB.</p>
</div>
<figure id="S3.SS0.SSSx2.1" class="ltx_figure"><img src="/html/2401.17874/assets/figures/gt_distrib_dexycb_s0/s0_train_stitched.png" id="S3.SS0.SSSx2.1.g1" class="ltx_graphics ltx_img_landscape" width="671" height="135" alt="[Uncaptioned image]">
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2401.17874/assets/figures/gt_distrib_synthdexycb/4_stitched.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="671" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>This figure shows that the ground truth parameter distributions of the synthetic data (bottom row) are qualitatively similar to those of the real data. Parameters shown are distance of the object from the camera optical center, object azimuths and elevations in the camera coordinate frame, and visibility fraction of the objects.</figcaption>
</figure>
<div id="S3.SS0.SSSx2.p3" class="ltx_para">
<p id="S3.SS0.SSSx2.p3.1" class="ltx_p">Once the session to be rendered and the cameras are configured, one of the in-built blender renderers (e.g. Cycles) can selected and the session can be rendered out in the form of multi-view RGB and depth image sequences. Furthermore, ground truth annotations such as object identities, bounding boxes and segmentation masks can be exported in the standard COCO format<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://cocodataset.org</span></span></span>, and 3D object poses can be exported in the BOP format<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://bop.felk.cvut.cz</span></span></span>. 21-DoF hand poses are exported in the MANO format<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://mano.is.tue.mpg.de/</span></span></span>. We rely on BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to provide the above export functionality, except for the hand poses for which we write a custom exporter.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Blender-hoisynth allows us to generate large-scale photorealistic visual data and annotations necessary for Machine Learning with very little effort. To investigate the quality of the data produced by blender-hoisynth, we generate a dataset analogous to the well-known DexYCB dataset from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and call it “SynthDexYCB”.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The sequences in DexYCB consist of a subject reaching out to an object on the table, grasping it, lifting it up, before putting it down in roughly the same position again. SynthDexYCB captures interactions with 20 YCB objects from 8 views (same as DexYCB). It is very hard to extend the DexYCB dataset with the original interaction data for new camera views. In hoisynth, however, we can easily extend the number of views indefinitely by specifying new camera poses while retaining exactly the same interactions recorded by users before.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">SynthDexYCB currently contains a total of 115,200 frames (right hands only) (as compared to 300,000 in DexYCB). This comes from ca. 200 hand-object interaction sequences, with 8 camera views rendered per sequence, and 72 frames per camera view. Human involvement was necessary only while recording these sequences in VR, which took ca. 7 hours. The Meta Quest 2 headset was used together with the ”VR Scene Inspection” plugin for Blender. The authors closely replicated the grasps and motions in the original DexYCB videos in blender-hoisynth. While DexYCB acquired its ground truth annotations through a time-consuming and expensive crowd-sourcing process, the annotations in SynthDexYCB were generated automatically. Figure <a href="#S3.F3" title="Figure 3 ‣ Implementation in Blender/UPBGE ‣ 3 blender-hoisynth ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a comparison of the distributions of ground truth parameters in the real data with those in the synthetic data.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present SynthDexYCB-based training experiments for a representative 6D HOI tracking and reconstruction model – gSDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. gSDF reconstructs the hand-object meshes in a HOI with a monocular RGB image as input. gSDF has been trained and evaluated on the DexYCB dataset in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We follow the same scheme and export data from hoisynth in the DexYCB format for training. We devise the following scenarios to evaluate the quality of the data in SynthDexYCB:</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Scenario s0 (synthetic data only)</span> investigates if the training converges at all with synthetic data only. Only 4 subjects are used. Training, validation, and test are all carried out on synthetic data only.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Scenario s1 (real data only)</span> uses real data of 5 DexYCB subjects for training, validation, and test.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Scenario s2 (real data only)</span> is the original s0 scenario with 10 subjects as defined in the DexYCB dataset and only uses real data for training, validation, and test.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Scenario s3 (real + synthetic with replacement)</span> evaluates using synthetic and real data for training. We replace the data for subjects 1,2,3, and 10 in the original DexYCB dataset with synthetic data. While training is carried out on synthetic and real data, validation and test are carried out on real data.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.33.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Hand-object reconstruction on gSDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. R = Real, S = Synth. Metrics shown (<sub id="S5.T1.34.2" class="ltx_sub"><span id="S5.T1.34.2.1" class="ltx_text ltx_font_italic">h</span></sub> = hand, <sub id="S5.T1.35.3" class="ltx_sub"><span id="S5.T1.35.3.1" class="ltx_text ltx_font_italic">o</span></sub> = object): <math id="S5.T1.13.m3.1" class="ltx_Math" alttext="CD" display="inline"><semantics id="S5.T1.13.m3.1b"><mrow id="S5.T1.13.m3.1.1" xref="S5.T1.13.m3.1.1.cmml"><mi id="S5.T1.13.m3.1.1.2" xref="S5.T1.13.m3.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.T1.13.m3.1.1.1" xref="S5.T1.13.m3.1.1.1.cmml">​</mo><mi id="S5.T1.13.m3.1.1.3" xref="S5.T1.13.m3.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.13.m3.1c"><apply id="S5.T1.13.m3.1.1.cmml" xref="S5.T1.13.m3.1.1"><times id="S5.T1.13.m3.1.1.1.cmml" xref="S5.T1.13.m3.1.1.1"></times><ci id="S5.T1.13.m3.1.1.2.cmml" xref="S5.T1.13.m3.1.1.2">𝐶</ci><ci id="S5.T1.13.m3.1.1.3.cmml" xref="S5.T1.13.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.m3.1d">CD</annotation></semantics></math> = chamfer distance in <math id="S5.T1.14.m4.1" class="ltx_Math" alttext="cm^{2}" display="inline"><semantics id="S5.T1.14.m4.1b"><mrow id="S5.T1.14.m4.1.1" xref="S5.T1.14.m4.1.1.cmml"><mi id="S5.T1.14.m4.1.1.2" xref="S5.T1.14.m4.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T1.14.m4.1.1.1" xref="S5.T1.14.m4.1.1.1.cmml">​</mo><msup id="S5.T1.14.m4.1.1.3" xref="S5.T1.14.m4.1.1.3.cmml"><mi id="S5.T1.14.m4.1.1.3.2" xref="S5.T1.14.m4.1.1.3.2.cmml">m</mi><mn id="S5.T1.14.m4.1.1.3.3" xref="S5.T1.14.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.14.m4.1c"><apply id="S5.T1.14.m4.1.1.cmml" xref="S5.T1.14.m4.1.1"><times id="S5.T1.14.m4.1.1.1.cmml" xref="S5.T1.14.m4.1.1.1"></times><ci id="S5.T1.14.m4.1.1.2.cmml" xref="S5.T1.14.m4.1.1.2">𝑐</ci><apply id="S5.T1.14.m4.1.1.3.cmml" xref="S5.T1.14.m4.1.1.3"><csymbol cd="ambiguous" id="S5.T1.14.m4.1.1.3.1.cmml" xref="S5.T1.14.m4.1.1.3">superscript</csymbol><ci id="S5.T1.14.m4.1.1.3.2.cmml" xref="S5.T1.14.m4.1.1.3.2">𝑚</ci><cn type="integer" id="S5.T1.14.m4.1.1.3.3.cmml" xref="S5.T1.14.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.m4.1d">cm^{2}</annotation></semantics></math>, <math id="S5.T1.15.m5.1" class="ltx_Math" alttext="FS@X" display="inline"><semantics id="S5.T1.15.m5.1b"><mrow id="S5.T1.15.m5.1.1" xref="S5.T1.15.m5.1.1.cmml"><mi id="S5.T1.15.m5.1.1.2" xref="S5.T1.15.m5.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.T1.15.m5.1.1.1" xref="S5.T1.15.m5.1.1.1.cmml">​</mo><mi id="S5.T1.15.m5.1.1.3" xref="S5.T1.15.m5.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.T1.15.m5.1.1.1b" xref="S5.T1.15.m5.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S5.T1.15.m5.1.1.4" xref="S5.T1.15.m5.1.1.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.T1.15.m5.1.1.1c" xref="S5.T1.15.m5.1.1.1.cmml">​</mo><mi id="S5.T1.15.m5.1.1.5" xref="S5.T1.15.m5.1.1.5.cmml">X</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.15.m5.1c"><apply id="S5.T1.15.m5.1.1.cmml" xref="S5.T1.15.m5.1.1"><times id="S5.T1.15.m5.1.1.1.cmml" xref="S5.T1.15.m5.1.1.1"></times><ci id="S5.T1.15.m5.1.1.2.cmml" xref="S5.T1.15.m5.1.1.2">𝐹</ci><ci id="S5.T1.15.m5.1.1.3.cmml" xref="S5.T1.15.m5.1.1.3">𝑆</ci><ci id="S5.T1.15.m5.1.1.4.cmml" xref="S5.T1.15.m5.1.1.4">@</ci><ci id="S5.T1.15.m5.1.1.5.cmml" xref="S5.T1.15.m5.1.1.5">𝑋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.m5.1d">FS@X</annotation></semantics></math> = F-score at threshold <math id="S5.T1.16.m6.1" class="ltx_Math" alttext="Xmm" display="inline"><semantics id="S5.T1.16.m6.1b"><mrow id="S5.T1.16.m6.1.1" xref="S5.T1.16.m6.1.1.cmml"><mi id="S5.T1.16.m6.1.1.2" xref="S5.T1.16.m6.1.1.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S5.T1.16.m6.1.1.1" xref="S5.T1.16.m6.1.1.1.cmml">​</mo><mi id="S5.T1.16.m6.1.1.3" xref="S5.T1.16.m6.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T1.16.m6.1.1.1b" xref="S5.T1.16.m6.1.1.1.cmml">​</mo><mi id="S5.T1.16.m6.1.1.4" xref="S5.T1.16.m6.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.16.m6.1c"><apply id="S5.T1.16.m6.1.1.cmml" xref="S5.T1.16.m6.1.1"><times id="S5.T1.16.m6.1.1.1.cmml" xref="S5.T1.16.m6.1.1.1"></times><ci id="S5.T1.16.m6.1.1.2.cmml" xref="S5.T1.16.m6.1.1.2">𝑋</ci><ci id="S5.T1.16.m6.1.1.3.cmml" xref="S5.T1.16.m6.1.1.3">𝑚</ci><ci id="S5.T1.16.m6.1.1.4.cmml" xref="S5.T1.16.m6.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.16.m6.1d">Xmm</annotation></semantics></math>, <math id="S5.T1.17.m7.1" class="ltx_Math" alttext="E_{h}" display="inline"><semantics id="S5.T1.17.m7.1b"><msub id="S5.T1.17.m7.1.1" xref="S5.T1.17.m7.1.1.cmml"><mi id="S5.T1.17.m7.1.1.2" xref="S5.T1.17.m7.1.1.2.cmml">E</mi><mi id="S5.T1.17.m7.1.1.3" xref="S5.T1.17.m7.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.17.m7.1c"><apply id="S5.T1.17.m7.1.1.cmml" xref="S5.T1.17.m7.1.1"><csymbol cd="ambiguous" id="S5.T1.17.m7.1.1.1.cmml" xref="S5.T1.17.m7.1.1">subscript</csymbol><ci id="S5.T1.17.m7.1.1.2.cmml" xref="S5.T1.17.m7.1.1.2">𝐸</ci><ci id="S5.T1.17.m7.1.1.3.cmml" xref="S5.T1.17.m7.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.17.m7.1d">E_{h}</annotation></semantics></math> = mean hand joint error in <math id="S5.T1.18.m8.1" class="ltx_Math" alttext="cm" display="inline"><semantics id="S5.T1.18.m8.1b"><mrow id="S5.T1.18.m8.1.1" xref="S5.T1.18.m8.1.1.cmml"><mi id="S5.T1.18.m8.1.1.2" xref="S5.T1.18.m8.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T1.18.m8.1.1.1" xref="S5.T1.18.m8.1.1.1.cmml">​</mo><mi id="S5.T1.18.m8.1.1.3" xref="S5.T1.18.m8.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.18.m8.1c"><apply id="S5.T1.18.m8.1.1.cmml" xref="S5.T1.18.m8.1.1"><times id="S5.T1.18.m8.1.1.1.cmml" xref="S5.T1.18.m8.1.1.1"></times><ci id="S5.T1.18.m8.1.1.2.cmml" xref="S5.T1.18.m8.1.1.2">𝑐</ci><ci id="S5.T1.18.m8.1.1.3.cmml" xref="S5.T1.18.m8.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.18.m8.1d">cm</annotation></semantics></math>, <math id="S5.T1.19.m9.1" class="ltx_Math" alttext="E_{h}" display="inline"><semantics id="S5.T1.19.m9.1b"><msub id="S5.T1.19.m9.1.1" xref="S5.T1.19.m9.1.1.cmml"><mi id="S5.T1.19.m9.1.1.2" xref="S5.T1.19.m9.1.1.2.cmml">E</mi><mi id="S5.T1.19.m9.1.1.3" xref="S5.T1.19.m9.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.19.m9.1c"><apply id="S5.T1.19.m9.1.1.cmml" xref="S5.T1.19.m9.1.1"><csymbol cd="ambiguous" id="S5.T1.19.m9.1.1.1.cmml" xref="S5.T1.19.m9.1.1">subscript</csymbol><ci id="S5.T1.19.m9.1.1.2.cmml" xref="S5.T1.19.m9.1.1.2">𝐸</ci><ci id="S5.T1.19.m9.1.1.3.cmml" xref="S5.T1.19.m9.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.19.m9.1d">E_{h}</annotation></semantics></math> = mean object error in <math id="S5.T1.20.m10.1" class="ltx_Math" alttext="cm" display="inline"><semantics id="S5.T1.20.m10.1b"><mrow id="S5.T1.20.m10.1.1" xref="S5.T1.20.m10.1.1.cmml"><mi id="S5.T1.20.m10.1.1.2" xref="S5.T1.20.m10.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T1.20.m10.1.1.1" xref="S5.T1.20.m10.1.1.1.cmml">​</mo><mi id="S5.T1.20.m10.1.1.3" xref="S5.T1.20.m10.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.20.m10.1c"><apply id="S5.T1.20.m10.1.1.cmml" xref="S5.T1.20.m10.1.1"><times id="S5.T1.20.m10.1.1.1.cmml" xref="S5.T1.20.m10.1.1.1"></times><ci id="S5.T1.20.m10.1.1.2.cmml" xref="S5.T1.20.m10.1.1.2">𝑐</ci><ci id="S5.T1.20.m10.1.1.3.cmml" xref="S5.T1.20.m10.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.20.m10.1d">cm</annotation></semantics></math>.</figcaption>
<table id="S5.T1.29" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.29.9" class="ltx_tr">
<td id="S5.T1.29.9.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.29.9.10.1" class="ltx_text" style="font-size:90%;">Scenario</span></td>
<td id="S5.T1.29.9.11" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S5.T1.29.9.11.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.29.9.11.1.1" class="ltx_tr">
<td id="S5.T1.29.9.11.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.9.11.1.1.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></td>
</tr>
<tr id="S5.T1.29.9.11.1.2" class="ltx_tr">
<td id="S5.T1.29.9.11.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.9.11.1.2.1.1" class="ltx_text" style="font-size:90%;">Subjects</span></td>
</tr>
</table>
</td>
<td id="S5.T1.21.1.1" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.21.1.1.m1.1" class="ltx_Math" alttext="CD_{h}\downarrow" display="inline"><semantics id="S5.T1.21.1.1.m1.1a"><mrow id="S5.T1.21.1.1.m1.1.1" xref="S5.T1.21.1.1.m1.1.1.cmml"><mrow id="S5.T1.21.1.1.m1.1.1.2" xref="S5.T1.21.1.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.21.1.1.m1.1.1.2.2" xref="S5.T1.21.1.1.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.T1.21.1.1.m1.1.1.2.1" xref="S5.T1.21.1.1.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.21.1.1.m1.1.1.2.3" xref="S5.T1.21.1.1.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.21.1.1.m1.1.1.2.3.2" xref="S5.T1.21.1.1.m1.1.1.2.3.2.cmml">D</mi><mi mathsize="90%" id="S5.T1.21.1.1.m1.1.1.2.3.3" xref="S5.T1.21.1.1.m1.1.1.2.3.3.cmml">h</mi></msub></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.21.1.1.m1.1.1.1" xref="S5.T1.21.1.1.m1.1.1.1.cmml">↓</mo><mi id="S5.T1.21.1.1.m1.1.1.3" xref="S5.T1.21.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.21.1.1.m1.1b"><apply id="S5.T1.21.1.1.m1.1.1.cmml" xref="S5.T1.21.1.1.m1.1.1"><ci id="S5.T1.21.1.1.m1.1.1.1.cmml" xref="S5.T1.21.1.1.m1.1.1.1">↓</ci><apply id="S5.T1.21.1.1.m1.1.1.2.cmml" xref="S5.T1.21.1.1.m1.1.1.2"><times id="S5.T1.21.1.1.m1.1.1.2.1.cmml" xref="S5.T1.21.1.1.m1.1.1.2.1"></times><ci id="S5.T1.21.1.1.m1.1.1.2.2.cmml" xref="S5.T1.21.1.1.m1.1.1.2.2">𝐶</ci><apply id="S5.T1.21.1.1.m1.1.1.2.3.cmml" xref="S5.T1.21.1.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.21.1.1.m1.1.1.2.3.1.cmml" xref="S5.T1.21.1.1.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.21.1.1.m1.1.1.2.3.2.cmml" xref="S5.T1.21.1.1.m1.1.1.2.3.2">𝐷</ci><ci id="S5.T1.21.1.1.m1.1.1.2.3.3.cmml" xref="S5.T1.21.1.1.m1.1.1.2.3.3">ℎ</ci></apply></apply><csymbol cd="latexml" id="S5.T1.21.1.1.m1.1.1.3.cmml" xref="S5.T1.21.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.21.1.1.m1.1c">CD_{h}\downarrow</annotation></semantics></math></td>
<td id="S5.T1.22.2.2" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.22.2.2.m1.1" class="ltx_Math" alttext="FS_{h}@1\uparrow" display="inline"><semantics id="S5.T1.22.2.2.m1.1a"><mrow id="S5.T1.22.2.2.m1.1.1" xref="S5.T1.22.2.2.m1.1.1.cmml"><mrow id="S5.T1.22.2.2.m1.1.1.2" xref="S5.T1.22.2.2.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.22.2.2.m1.1.1.2.2" xref="S5.T1.22.2.2.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.T1.22.2.2.m1.1.1.2.1" xref="S5.T1.22.2.2.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.22.2.2.m1.1.1.2.3" xref="S5.T1.22.2.2.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.22.2.2.m1.1.1.2.3.2" xref="S5.T1.22.2.2.m1.1.1.2.3.2.cmml">S</mi><mi mathsize="90%" id="S5.T1.22.2.2.m1.1.1.2.3.3" xref="S5.T1.22.2.2.m1.1.1.2.3.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.22.2.2.m1.1.1.2.1a" xref="S5.T1.22.2.2.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S5.T1.22.2.2.m1.1.1.2.4" xref="S5.T1.22.2.2.m1.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.T1.22.2.2.m1.1.1.2.1b" xref="S5.T1.22.2.2.m1.1.1.2.1.cmml">​</mo><mn mathsize="90%" id="S5.T1.22.2.2.m1.1.1.2.5" xref="S5.T1.22.2.2.m1.1.1.2.5.cmml">1</mn></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.22.2.2.m1.1.1.1" xref="S5.T1.22.2.2.m1.1.1.1.cmml">↑</mo><mi id="S5.T1.22.2.2.m1.1.1.3" xref="S5.T1.22.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.22.2.2.m1.1b"><apply id="S5.T1.22.2.2.m1.1.1.cmml" xref="S5.T1.22.2.2.m1.1.1"><ci id="S5.T1.22.2.2.m1.1.1.1.cmml" xref="S5.T1.22.2.2.m1.1.1.1">↑</ci><apply id="S5.T1.22.2.2.m1.1.1.2.cmml" xref="S5.T1.22.2.2.m1.1.1.2"><times id="S5.T1.22.2.2.m1.1.1.2.1.cmml" xref="S5.T1.22.2.2.m1.1.1.2.1"></times><ci id="S5.T1.22.2.2.m1.1.1.2.2.cmml" xref="S5.T1.22.2.2.m1.1.1.2.2">𝐹</ci><apply id="S5.T1.22.2.2.m1.1.1.2.3.cmml" xref="S5.T1.22.2.2.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.22.2.2.m1.1.1.2.3.1.cmml" xref="S5.T1.22.2.2.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.22.2.2.m1.1.1.2.3.2.cmml" xref="S5.T1.22.2.2.m1.1.1.2.3.2">𝑆</ci><ci id="S5.T1.22.2.2.m1.1.1.2.3.3.cmml" xref="S5.T1.22.2.2.m1.1.1.2.3.3">ℎ</ci></apply><ci id="S5.T1.22.2.2.m1.1.1.2.4.cmml" xref="S5.T1.22.2.2.m1.1.1.2.4">@</ci><cn type="integer" id="S5.T1.22.2.2.m1.1.1.2.5.cmml" xref="S5.T1.22.2.2.m1.1.1.2.5">1</cn></apply><csymbol cd="latexml" id="S5.T1.22.2.2.m1.1.1.3.cmml" xref="S5.T1.22.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.22.2.2.m1.1c">FS_{h}@1\uparrow</annotation></semantics></math></td>
<td id="S5.T1.23.3.3" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.23.3.3.m1.1" class="ltx_Math" alttext="FS_{h}@5\uparrow" display="inline"><semantics id="S5.T1.23.3.3.m1.1a"><mrow id="S5.T1.23.3.3.m1.1.1" xref="S5.T1.23.3.3.m1.1.1.cmml"><mrow id="S5.T1.23.3.3.m1.1.1.2" xref="S5.T1.23.3.3.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.23.3.3.m1.1.1.2.2" xref="S5.T1.23.3.3.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.T1.23.3.3.m1.1.1.2.1" xref="S5.T1.23.3.3.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.23.3.3.m1.1.1.2.3" xref="S5.T1.23.3.3.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.23.3.3.m1.1.1.2.3.2" xref="S5.T1.23.3.3.m1.1.1.2.3.2.cmml">S</mi><mi mathsize="90%" id="S5.T1.23.3.3.m1.1.1.2.3.3" xref="S5.T1.23.3.3.m1.1.1.2.3.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.23.3.3.m1.1.1.2.1a" xref="S5.T1.23.3.3.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S5.T1.23.3.3.m1.1.1.2.4" xref="S5.T1.23.3.3.m1.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.T1.23.3.3.m1.1.1.2.1b" xref="S5.T1.23.3.3.m1.1.1.2.1.cmml">​</mo><mn mathsize="90%" id="S5.T1.23.3.3.m1.1.1.2.5" xref="S5.T1.23.3.3.m1.1.1.2.5.cmml">5</mn></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.23.3.3.m1.1.1.1" xref="S5.T1.23.3.3.m1.1.1.1.cmml">↑</mo><mi id="S5.T1.23.3.3.m1.1.1.3" xref="S5.T1.23.3.3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.23.3.3.m1.1b"><apply id="S5.T1.23.3.3.m1.1.1.cmml" xref="S5.T1.23.3.3.m1.1.1"><ci id="S5.T1.23.3.3.m1.1.1.1.cmml" xref="S5.T1.23.3.3.m1.1.1.1">↑</ci><apply id="S5.T1.23.3.3.m1.1.1.2.cmml" xref="S5.T1.23.3.3.m1.1.1.2"><times id="S5.T1.23.3.3.m1.1.1.2.1.cmml" xref="S5.T1.23.3.3.m1.1.1.2.1"></times><ci id="S5.T1.23.3.3.m1.1.1.2.2.cmml" xref="S5.T1.23.3.3.m1.1.1.2.2">𝐹</ci><apply id="S5.T1.23.3.3.m1.1.1.2.3.cmml" xref="S5.T1.23.3.3.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.23.3.3.m1.1.1.2.3.1.cmml" xref="S5.T1.23.3.3.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.23.3.3.m1.1.1.2.3.2.cmml" xref="S5.T1.23.3.3.m1.1.1.2.3.2">𝑆</ci><ci id="S5.T1.23.3.3.m1.1.1.2.3.3.cmml" xref="S5.T1.23.3.3.m1.1.1.2.3.3">ℎ</ci></apply><ci id="S5.T1.23.3.3.m1.1.1.2.4.cmml" xref="S5.T1.23.3.3.m1.1.1.2.4">@</ci><cn type="integer" id="S5.T1.23.3.3.m1.1.1.2.5.cmml" xref="S5.T1.23.3.3.m1.1.1.2.5">5</cn></apply><csymbol cd="latexml" id="S5.T1.23.3.3.m1.1.1.3.cmml" xref="S5.T1.23.3.3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.23.3.3.m1.1c">FS_{h}@5\uparrow</annotation></semantics></math></td>
<td id="S5.T1.24.4.4" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.24.4.4.m1.1" class="ltx_Math" alttext="E_{h}\downarrow" display="inline"><semantics id="S5.T1.24.4.4.m1.1a"><mrow id="S5.T1.24.4.4.m1.1.1" xref="S5.T1.24.4.4.m1.1.1.cmml"><msub id="S5.T1.24.4.4.m1.1.1.2" xref="S5.T1.24.4.4.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.24.4.4.m1.1.1.2.2" xref="S5.T1.24.4.4.m1.1.1.2.2.cmml">E</mi><mi mathsize="90%" id="S5.T1.24.4.4.m1.1.1.2.3" xref="S5.T1.24.4.4.m1.1.1.2.3.cmml">h</mi></msub><mo mathsize="90%" stretchy="false" id="S5.T1.24.4.4.m1.1.1.1" xref="S5.T1.24.4.4.m1.1.1.1.cmml">↓</mo><mi id="S5.T1.24.4.4.m1.1.1.3" xref="S5.T1.24.4.4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.24.4.4.m1.1b"><apply id="S5.T1.24.4.4.m1.1.1.cmml" xref="S5.T1.24.4.4.m1.1.1"><ci id="S5.T1.24.4.4.m1.1.1.1.cmml" xref="S5.T1.24.4.4.m1.1.1.1">↓</ci><apply id="S5.T1.24.4.4.m1.1.1.2.cmml" xref="S5.T1.24.4.4.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.24.4.4.m1.1.1.2.1.cmml" xref="S5.T1.24.4.4.m1.1.1.2">subscript</csymbol><ci id="S5.T1.24.4.4.m1.1.1.2.2.cmml" xref="S5.T1.24.4.4.m1.1.1.2.2">𝐸</ci><ci id="S5.T1.24.4.4.m1.1.1.2.3.cmml" xref="S5.T1.24.4.4.m1.1.1.2.3">ℎ</ci></apply><csymbol cd="latexml" id="S5.T1.24.4.4.m1.1.1.3.cmml" xref="S5.T1.24.4.4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.24.4.4.m1.1c">E_{h}\downarrow</annotation></semantics></math></td>
<td id="S5.T1.25.5.5" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.25.5.5.m1.1" class="ltx_Math" alttext="CD_{o}\downarrow" display="inline"><semantics id="S5.T1.25.5.5.m1.1a"><mrow id="S5.T1.25.5.5.m1.1.1" xref="S5.T1.25.5.5.m1.1.1.cmml"><mrow id="S5.T1.25.5.5.m1.1.1.2" xref="S5.T1.25.5.5.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.25.5.5.m1.1.1.2.2" xref="S5.T1.25.5.5.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.T1.25.5.5.m1.1.1.2.1" xref="S5.T1.25.5.5.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.25.5.5.m1.1.1.2.3" xref="S5.T1.25.5.5.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.25.5.5.m1.1.1.2.3.2" xref="S5.T1.25.5.5.m1.1.1.2.3.2.cmml">D</mi><mi mathsize="90%" id="S5.T1.25.5.5.m1.1.1.2.3.3" xref="S5.T1.25.5.5.m1.1.1.2.3.3.cmml">o</mi></msub></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.25.5.5.m1.1.1.1" xref="S5.T1.25.5.5.m1.1.1.1.cmml">↓</mo><mi id="S5.T1.25.5.5.m1.1.1.3" xref="S5.T1.25.5.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.25.5.5.m1.1b"><apply id="S5.T1.25.5.5.m1.1.1.cmml" xref="S5.T1.25.5.5.m1.1.1"><ci id="S5.T1.25.5.5.m1.1.1.1.cmml" xref="S5.T1.25.5.5.m1.1.1.1">↓</ci><apply id="S5.T1.25.5.5.m1.1.1.2.cmml" xref="S5.T1.25.5.5.m1.1.1.2"><times id="S5.T1.25.5.5.m1.1.1.2.1.cmml" xref="S5.T1.25.5.5.m1.1.1.2.1"></times><ci id="S5.T1.25.5.5.m1.1.1.2.2.cmml" xref="S5.T1.25.5.5.m1.1.1.2.2">𝐶</ci><apply id="S5.T1.25.5.5.m1.1.1.2.3.cmml" xref="S5.T1.25.5.5.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.25.5.5.m1.1.1.2.3.1.cmml" xref="S5.T1.25.5.5.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.25.5.5.m1.1.1.2.3.2.cmml" xref="S5.T1.25.5.5.m1.1.1.2.3.2">𝐷</ci><ci id="S5.T1.25.5.5.m1.1.1.2.3.3.cmml" xref="S5.T1.25.5.5.m1.1.1.2.3.3">𝑜</ci></apply></apply><csymbol cd="latexml" id="S5.T1.25.5.5.m1.1.1.3.cmml" xref="S5.T1.25.5.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.25.5.5.m1.1c">CD_{o}\downarrow</annotation></semantics></math></td>
<td id="S5.T1.26.6.6" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.26.6.6.m1.1" class="ltx_Math" alttext="FS_{o}@5\uparrow" display="inline"><semantics id="S5.T1.26.6.6.m1.1a"><mrow id="S5.T1.26.6.6.m1.1.1" xref="S5.T1.26.6.6.m1.1.1.cmml"><mrow id="S5.T1.26.6.6.m1.1.1.2" xref="S5.T1.26.6.6.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.26.6.6.m1.1.1.2.2" xref="S5.T1.26.6.6.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.T1.26.6.6.m1.1.1.2.1" xref="S5.T1.26.6.6.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.26.6.6.m1.1.1.2.3" xref="S5.T1.26.6.6.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.26.6.6.m1.1.1.2.3.2" xref="S5.T1.26.6.6.m1.1.1.2.3.2.cmml">S</mi><mi mathsize="90%" id="S5.T1.26.6.6.m1.1.1.2.3.3" xref="S5.T1.26.6.6.m1.1.1.2.3.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.26.6.6.m1.1.1.2.1a" xref="S5.T1.26.6.6.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S5.T1.26.6.6.m1.1.1.2.4" xref="S5.T1.26.6.6.m1.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.T1.26.6.6.m1.1.1.2.1b" xref="S5.T1.26.6.6.m1.1.1.2.1.cmml">​</mo><mn mathsize="90%" id="S5.T1.26.6.6.m1.1.1.2.5" xref="S5.T1.26.6.6.m1.1.1.2.5.cmml">5</mn></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.26.6.6.m1.1.1.1" xref="S5.T1.26.6.6.m1.1.1.1.cmml">↑</mo><mi id="S5.T1.26.6.6.m1.1.1.3" xref="S5.T1.26.6.6.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.26.6.6.m1.1b"><apply id="S5.T1.26.6.6.m1.1.1.cmml" xref="S5.T1.26.6.6.m1.1.1"><ci id="S5.T1.26.6.6.m1.1.1.1.cmml" xref="S5.T1.26.6.6.m1.1.1.1">↑</ci><apply id="S5.T1.26.6.6.m1.1.1.2.cmml" xref="S5.T1.26.6.6.m1.1.1.2"><times id="S5.T1.26.6.6.m1.1.1.2.1.cmml" xref="S5.T1.26.6.6.m1.1.1.2.1"></times><ci id="S5.T1.26.6.6.m1.1.1.2.2.cmml" xref="S5.T1.26.6.6.m1.1.1.2.2">𝐹</ci><apply id="S5.T1.26.6.6.m1.1.1.2.3.cmml" xref="S5.T1.26.6.6.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.26.6.6.m1.1.1.2.3.1.cmml" xref="S5.T1.26.6.6.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.26.6.6.m1.1.1.2.3.2.cmml" xref="S5.T1.26.6.6.m1.1.1.2.3.2">𝑆</ci><ci id="S5.T1.26.6.6.m1.1.1.2.3.3.cmml" xref="S5.T1.26.6.6.m1.1.1.2.3.3">𝑜</ci></apply><ci id="S5.T1.26.6.6.m1.1.1.2.4.cmml" xref="S5.T1.26.6.6.m1.1.1.2.4">@</ci><cn type="integer" id="S5.T1.26.6.6.m1.1.1.2.5.cmml" xref="S5.T1.26.6.6.m1.1.1.2.5">5</cn></apply><csymbol cd="latexml" id="S5.T1.26.6.6.m1.1.1.3.cmml" xref="S5.T1.26.6.6.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.26.6.6.m1.1c">FS_{o}@5\uparrow</annotation></semantics></math></td>
<td id="S5.T1.27.7.7" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.27.7.7.m1.1" class="ltx_Math" alttext="FS_{o}@10\uparrow" display="inline"><semantics id="S5.T1.27.7.7.m1.1a"><mrow id="S5.T1.27.7.7.m1.1.1" xref="S5.T1.27.7.7.m1.1.1.cmml"><mrow id="S5.T1.27.7.7.m1.1.1.2" xref="S5.T1.27.7.7.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.T1.27.7.7.m1.1.1.2.2" xref="S5.T1.27.7.7.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.T1.27.7.7.m1.1.1.2.1" xref="S5.T1.27.7.7.m1.1.1.2.1.cmml">​</mo><msub id="S5.T1.27.7.7.m1.1.1.2.3" xref="S5.T1.27.7.7.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S5.T1.27.7.7.m1.1.1.2.3.2" xref="S5.T1.27.7.7.m1.1.1.2.3.2.cmml">S</mi><mi mathsize="90%" id="S5.T1.27.7.7.m1.1.1.2.3.3" xref="S5.T1.27.7.7.m1.1.1.2.3.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.27.7.7.m1.1.1.2.1a" xref="S5.T1.27.7.7.m1.1.1.2.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S5.T1.27.7.7.m1.1.1.2.4" xref="S5.T1.27.7.7.m1.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S5.T1.27.7.7.m1.1.1.2.1b" xref="S5.T1.27.7.7.m1.1.1.2.1.cmml">​</mo><mn mathsize="90%" id="S5.T1.27.7.7.m1.1.1.2.5" xref="S5.T1.27.7.7.m1.1.1.2.5.cmml">10</mn></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.27.7.7.m1.1.1.1" xref="S5.T1.27.7.7.m1.1.1.1.cmml">↑</mo><mi id="S5.T1.27.7.7.m1.1.1.3" xref="S5.T1.27.7.7.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.27.7.7.m1.1b"><apply id="S5.T1.27.7.7.m1.1.1.cmml" xref="S5.T1.27.7.7.m1.1.1"><ci id="S5.T1.27.7.7.m1.1.1.1.cmml" xref="S5.T1.27.7.7.m1.1.1.1">↑</ci><apply id="S5.T1.27.7.7.m1.1.1.2.cmml" xref="S5.T1.27.7.7.m1.1.1.2"><times id="S5.T1.27.7.7.m1.1.1.2.1.cmml" xref="S5.T1.27.7.7.m1.1.1.2.1"></times><ci id="S5.T1.27.7.7.m1.1.1.2.2.cmml" xref="S5.T1.27.7.7.m1.1.1.2.2">𝐹</ci><apply id="S5.T1.27.7.7.m1.1.1.2.3.cmml" xref="S5.T1.27.7.7.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.T1.27.7.7.m1.1.1.2.3.1.cmml" xref="S5.T1.27.7.7.m1.1.1.2.3">subscript</csymbol><ci id="S5.T1.27.7.7.m1.1.1.2.3.2.cmml" xref="S5.T1.27.7.7.m1.1.1.2.3.2">𝑆</ci><ci id="S5.T1.27.7.7.m1.1.1.2.3.3.cmml" xref="S5.T1.27.7.7.m1.1.1.2.3.3">𝑜</ci></apply><ci id="S5.T1.27.7.7.m1.1.1.2.4.cmml" xref="S5.T1.27.7.7.m1.1.1.2.4">@</ci><cn type="integer" id="S5.T1.27.7.7.m1.1.1.2.5.cmml" xref="S5.T1.27.7.7.m1.1.1.2.5">10</cn></apply><csymbol cd="latexml" id="S5.T1.27.7.7.m1.1.1.3.cmml" xref="S5.T1.27.7.7.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.27.7.7.m1.1c">FS_{o}@10\uparrow</annotation></semantics></math></td>
<td id="S5.T1.28.8.8" class="ltx_td ltx_align_center ltx_border_tt"><math id="S5.T1.28.8.8.m1.1" class="ltx_Math" alttext="E_{o}\text{(center)}\downarrow" display="inline"><semantics id="S5.T1.28.8.8.m1.1a"><mrow id="S5.T1.28.8.8.m1.1.1" xref="S5.T1.28.8.8.m1.1.1.cmml"><mrow id="S5.T1.28.8.8.m1.1.1.2" xref="S5.T1.28.8.8.m1.1.1.2.cmml"><msub id="S5.T1.28.8.8.m1.1.1.2.2" xref="S5.T1.28.8.8.m1.1.1.2.2.cmml"><mi mathsize="90%" id="S5.T1.28.8.8.m1.1.1.2.2.2" xref="S5.T1.28.8.8.m1.1.1.2.2.2.cmml">E</mi><mi mathsize="90%" id="S5.T1.28.8.8.m1.1.1.2.2.3" xref="S5.T1.28.8.8.m1.1.1.2.2.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.28.8.8.m1.1.1.2.1" xref="S5.T1.28.8.8.m1.1.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S5.T1.28.8.8.m1.1.1.2.3" xref="S5.T1.28.8.8.m1.1.1.2.3a.cmml">(center)</mtext></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.28.8.8.m1.1.1.1" xref="S5.T1.28.8.8.m1.1.1.1.cmml">↓</mo><mi id="S5.T1.28.8.8.m1.1.1.3" xref="S5.T1.28.8.8.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.28.8.8.m1.1b"><apply id="S5.T1.28.8.8.m1.1.1.cmml" xref="S5.T1.28.8.8.m1.1.1"><ci id="S5.T1.28.8.8.m1.1.1.1.cmml" xref="S5.T1.28.8.8.m1.1.1.1">↓</ci><apply id="S5.T1.28.8.8.m1.1.1.2.cmml" xref="S5.T1.28.8.8.m1.1.1.2"><times id="S5.T1.28.8.8.m1.1.1.2.1.cmml" xref="S5.T1.28.8.8.m1.1.1.2.1"></times><apply id="S5.T1.28.8.8.m1.1.1.2.2.cmml" xref="S5.T1.28.8.8.m1.1.1.2.2"><csymbol cd="ambiguous" id="S5.T1.28.8.8.m1.1.1.2.2.1.cmml" xref="S5.T1.28.8.8.m1.1.1.2.2">subscript</csymbol><ci id="S5.T1.28.8.8.m1.1.1.2.2.2.cmml" xref="S5.T1.28.8.8.m1.1.1.2.2.2">𝐸</ci><ci id="S5.T1.28.8.8.m1.1.1.2.2.3.cmml" xref="S5.T1.28.8.8.m1.1.1.2.2.3">𝑜</ci></apply><ci id="S5.T1.28.8.8.m1.1.1.2.3a.cmml" xref="S5.T1.28.8.8.m1.1.1.2.3"><mtext mathsize="90%" id="S5.T1.28.8.8.m1.1.1.2.3.cmml" xref="S5.T1.28.8.8.m1.1.1.2.3">(center)</mtext></ci></apply><csymbol cd="latexml" id="S5.T1.28.8.8.m1.1.1.3.cmml" xref="S5.T1.28.8.8.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.28.8.8.m1.1c">E_{o}\text{(center)}\downarrow</annotation></semantics></math></td>
<td id="S5.T1.29.9.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><math id="S5.T1.29.9.9.m1.1" class="ltx_Math" alttext="E_{o}\text{(corner)}\downarrow" display="inline"><semantics id="S5.T1.29.9.9.m1.1a"><mrow id="S5.T1.29.9.9.m1.1.1" xref="S5.T1.29.9.9.m1.1.1.cmml"><mrow id="S5.T1.29.9.9.m1.1.1.2" xref="S5.T1.29.9.9.m1.1.1.2.cmml"><msub id="S5.T1.29.9.9.m1.1.1.2.2" xref="S5.T1.29.9.9.m1.1.1.2.2.cmml"><mi mathsize="90%" id="S5.T1.29.9.9.m1.1.1.2.2.2" xref="S5.T1.29.9.9.m1.1.1.2.2.2.cmml">E</mi><mi mathsize="90%" id="S5.T1.29.9.9.m1.1.1.2.2.3" xref="S5.T1.29.9.9.m1.1.1.2.2.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S5.T1.29.9.9.m1.1.1.2.1" xref="S5.T1.29.9.9.m1.1.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S5.T1.29.9.9.m1.1.1.2.3" xref="S5.T1.29.9.9.m1.1.1.2.3a.cmml">(corner)</mtext></mrow><mo mathsize="90%" stretchy="false" id="S5.T1.29.9.9.m1.1.1.1" xref="S5.T1.29.9.9.m1.1.1.1.cmml">↓</mo><mi id="S5.T1.29.9.9.m1.1.1.3" xref="S5.T1.29.9.9.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.29.9.9.m1.1b"><apply id="S5.T1.29.9.9.m1.1.1.cmml" xref="S5.T1.29.9.9.m1.1.1"><ci id="S5.T1.29.9.9.m1.1.1.1.cmml" xref="S5.T1.29.9.9.m1.1.1.1">↓</ci><apply id="S5.T1.29.9.9.m1.1.1.2.cmml" xref="S5.T1.29.9.9.m1.1.1.2"><times id="S5.T1.29.9.9.m1.1.1.2.1.cmml" xref="S5.T1.29.9.9.m1.1.1.2.1"></times><apply id="S5.T1.29.9.9.m1.1.1.2.2.cmml" xref="S5.T1.29.9.9.m1.1.1.2.2"><csymbol cd="ambiguous" id="S5.T1.29.9.9.m1.1.1.2.2.1.cmml" xref="S5.T1.29.9.9.m1.1.1.2.2">subscript</csymbol><ci id="S5.T1.29.9.9.m1.1.1.2.2.2.cmml" xref="S5.T1.29.9.9.m1.1.1.2.2.2">𝐸</ci><ci id="S5.T1.29.9.9.m1.1.1.2.2.3.cmml" xref="S5.T1.29.9.9.m1.1.1.2.2.3">𝑜</ci></apply><ci id="S5.T1.29.9.9.m1.1.1.2.3a.cmml" xref="S5.T1.29.9.9.m1.1.1.2.3"><mtext mathsize="90%" id="S5.T1.29.9.9.m1.1.1.2.3.cmml" xref="S5.T1.29.9.9.m1.1.1.2.3">(corner)</mtext></ci></apply><csymbol cd="latexml" id="S5.T1.29.9.9.m1.1.1.3.cmml" xref="S5.T1.29.9.9.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.29.9.9.m1.1c">E_{o}\text{(corner)}\downarrow</annotation></semantics></math></td>
</tr>
<tr id="S5.T1.29.10.1" class="ltx_tr">
<td id="S5.T1.29.10.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.1.1" class="ltx_text" style="font-size:90%;">s0 (S)</span></td>
<td id="S5.T1.29.10.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.2.1" class="ltx_text" style="font-size:90%;">1,2,3,10</span></td>
<td id="S5.T1.29.10.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.260</span></td>
<td id="S5.T1.29.10.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.173</span></td>
<td id="S5.T1.29.10.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.814</span></td>
<td id="S5.T1.29.10.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.6.1" class="ltx_text" style="font-size:90%;">24.188</span></td>
<td id="S5.T1.29.10.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.7.1" class="ltx_text" style="font-size:90%;">2.110</span></td>
<td id="S5.T1.29.10.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.8.1" class="ltx_text" style="font-size:90%;">0.407</span></td>
<td id="S5.T1.29.10.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.9.1" class="ltx_text" style="font-size:90%;">0.661</span></td>
<td id="S5.T1.29.10.1.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.10.1" class="ltx_text" style="font-size:90%;">20.994</span></td>
<td id="S5.T1.29.10.1.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.29.10.1.11.1" class="ltx_text" style="font-size:90%;">100.596</span></td>
</tr>
<tr id="S5.T1.29.11.2" class="ltx_tr">
<td id="S5.T1.29.11.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.1.1" class="ltx_text" style="font-size:90%;">s1 (R)</span></td>
<td id="S5.T1.29.11.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.2.1" class="ltx_text" style="font-size:90%;">None</span></td>
<td id="S5.T1.29.11.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.3.1" class="ltx_text" style="font-size:90%;">0.301</span></td>
<td id="S5.T1.29.11.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.4.1" class="ltx_text" style="font-size:90%;">0.172</span></td>
<td id="S5.T1.29.11.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.5.1" class="ltx_text" style="font-size:90%;">0.800</span></td>
<td id="S5.T1.29.11.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.6.1" class="ltx_text" style="font-size:90%;">19.864</span></td>
<td id="S5.T1.29.11.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.7.1" class="ltx_text" style="font-size:90%;">2.338</span></td>
<td id="S5.T1.29.11.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.8.1" class="ltx_text" style="font-size:90%;">0.393</span></td>
<td id="S5.T1.29.11.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.9.1" class="ltx_text" style="font-size:90%;">0.655</span></td>
<td id="S5.T1.29.11.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.10.1" class="ltx_text" style="font-size:90%;">19.336</span></td>
<td id="S5.T1.29.11.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.29.11.2.11.1" class="ltx_text" style="font-size:90%;">96.258</span></td>
</tr>
<tr id="S5.T1.29.12.3" class="ltx_tr">
<td id="S5.T1.29.12.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.1.1" class="ltx_text" style="font-size:90%;">s2 (R)</span></td>
<td id="S5.T1.29.12.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.2.1" class="ltx_text" style="font-size:90%;">None</span></td>
<td id="S5.T1.29.12.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.3.1" class="ltx_text" style="font-size:90%;">0.302</span></td>
<td id="S5.T1.29.12.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.4.1" class="ltx_text" style="font-size:90%;">0.172</span></td>
<td id="S5.T1.29.12.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.5.1" class="ltx_text" style="font-size:90%;">0.800</span></td>
<td id="S5.T1.29.12.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.325</span></td>
<td id="S5.T1.29.12.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.7.1" class="ltx_text" style="font-size:90%;">2.024</span></td>
<td id="S5.T1.29.12.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.8.1" class="ltx_text" style="font-size:90%;">0.407</span></td>
<td id="S5.T1.29.12.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.9.1" class="ltx_text" style="font-size:90%;">0.670</span></td>
<td id="S5.T1.29.12.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.319</span></td>
<td id="S5.T1.29.12.3.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.29.12.3.11.1" class="ltx_text" style="font-size:90%;">77.783</span></td>
</tr>
<tr id="S5.T1.29.13.4" class="ltx_tr">
<td id="S5.T1.29.13.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="6"><span id="S5.T1.29.13.4.1.1" class="ltx_text" style="font-size:90%;">s3 (R+S)</span></td>
<td id="S5.T1.29.13.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.2.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S5.T1.29.13.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.3.1" class="ltx_text" style="font-size:90%;">0.310</span></td>
<td id="S5.T1.29.13.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.4.1" class="ltx_text" style="font-size:90%;">0.170</span></td>
<td id="S5.T1.29.13.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.5.1" class="ltx_text" style="font-size:90%;">0.796</span></td>
<td id="S5.T1.29.13.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.6.1" class="ltx_text" style="font-size:90%;">19.733</span></td>
<td id="S5.T1.29.13.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.7.1" class="ltx_text" style="font-size:90%;">2.056</span></td>
<td id="S5.T1.29.13.4.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.8.1" class="ltx_text" style="font-size:90%;">0.405</span></td>
<td id="S5.T1.29.13.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.9.1" class="ltx_text" style="font-size:90%;">0.667</span></td>
<td id="S5.T1.29.13.4.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.10.1" class="ltx_text" style="font-size:90%;">19.546</span></td>
<td id="S5.T1.29.13.4.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S5.T1.29.13.4.11.1" class="ltx_text" style="font-size:90%;">77.486</span></td>
</tr>
<tr id="S5.T1.29.14.5" class="ltx_tr">
<td id="S5.T1.29.14.5.1" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.1.1" class="ltx_text" style="font-size:90%;">1,10</span></td>
<td id="S5.T1.29.14.5.2" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.2.1" class="ltx_text" style="font-size:90%;">0.316</span></td>
<td id="S5.T1.29.14.5.3" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.3.1" class="ltx_text" style="font-size:90%;">0.170</span></td>
<td id="S5.T1.29.14.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.4.1" class="ltx_text" style="font-size:90%;">0.795</span></td>
<td id="S5.T1.29.14.5.5" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.5.1" class="ltx_text" style="font-size:90%;">19.457</span></td>
<td id="S5.T1.29.14.5.6" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.6.1" class="ltx_text" style="font-size:90%;">1.929</span></td>
<td id="S5.T1.29.14.5.7" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.7.1" class="ltx_text" style="font-size:90%;">0.409</span></td>
<td id="S5.T1.29.14.5.8" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.8.1" class="ltx_text" style="font-size:90%;">0.673</span></td>
<td id="S5.T1.29.14.5.9" class="ltx_td ltx_align_center"><span id="S5.T1.29.14.5.9.1" class="ltx_text" style="font-size:90%;">19.773</span></td>
<td id="S5.T1.29.14.5.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.14.5.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">72.646</span></td>
</tr>
<tr id="S5.T1.29.15.6" class="ltx_tr">
<td id="S5.T1.29.15.6.1" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.1.1" class="ltx_text" style="font-size:90%;">1,2,10</span></td>
<td id="S5.T1.29.15.6.2" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.2.1" class="ltx_text" style="font-size:90%;">0.331</span></td>
<td id="S5.T1.29.15.6.3" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.3.1" class="ltx_text" style="font-size:90%;">0.167</span></td>
<td id="S5.T1.29.15.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.4.1" class="ltx_text" style="font-size:90%;">0.789</span></td>
<td id="S5.T1.29.15.6.5" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.5.1" class="ltx_text" style="font-size:90%;">19.643</span></td>
<td id="S5.T1.29.15.6.6" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.6.1" class="ltx_text" style="font-size:90%;">1.960</span></td>
<td id="S5.T1.29.15.6.7" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.7.1" class="ltx_text" style="font-size:90%;">0.410</span></td>
<td id="S5.T1.29.15.6.8" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.8.1" class="ltx_text" style="font-size:90%;">0.671</span></td>
<td id="S5.T1.29.15.6.9" class="ltx_td ltx_align_center"><span id="S5.T1.29.15.6.9.1" class="ltx_text" style="font-size:90%;">19.770</span></td>
<td id="S5.T1.29.15.6.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.15.6.10.1" class="ltx_text" style="font-size:90%;">74.501</span></td>
</tr>
<tr id="S5.T1.29.16.7" class="ltx_tr">
<td id="S5.T1.29.16.7.1" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.1.1" class="ltx_text" style="font-size:90%;">1,3,10</span></td>
<td id="S5.T1.29.16.7.2" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.2.1" class="ltx_text" style="font-size:90%;">0.337</span></td>
<td id="S5.T1.29.16.7.3" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.3.1" class="ltx_text" style="font-size:90%;">0.164</span></td>
<td id="S5.T1.29.16.7.4" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.4.1" class="ltx_text" style="font-size:90%;">0.783</span></td>
<td id="S5.T1.29.16.7.5" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.5.1" class="ltx_text" style="font-size:90%;">20.260</span></td>
<td id="S5.T1.29.16.7.6" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.6.1" class="ltx_text" style="font-size:90%;">1.934</span></td>
<td id="S5.T1.29.16.7.7" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.7.1" class="ltx_text" style="font-size:90%;">0.406</span></td>
<td id="S5.T1.29.16.7.8" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.8.1" class="ltx_text" style="font-size:90%;">0.670</span></td>
<td id="S5.T1.29.16.7.9" class="ltx_td ltx_align_center"><span id="S5.T1.29.16.7.9.1" class="ltx_text" style="font-size:90%;">19.840</span></td>
<td id="S5.T1.29.16.7.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.16.7.10.1" class="ltx_text" style="font-size:90%;">74.531</span></td>
</tr>
<tr id="S5.T1.29.17.8" class="ltx_tr">
<td id="S5.T1.29.17.8.1" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.1.1" class="ltx_text" style="font-size:90%;">2,3,10</span></td>
<td id="S5.T1.29.17.8.2" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.2.1" class="ltx_text" style="font-size:90%;">0.330</span></td>
<td id="S5.T1.29.17.8.3" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.3.1" class="ltx_text" style="font-size:90%;">0.165</span></td>
<td id="S5.T1.29.17.8.4" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.4.1" class="ltx_text" style="font-size:90%;">0.784</span></td>
<td id="S5.T1.29.17.8.5" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.5.1" class="ltx_text" style="font-size:90%;">19.837</span></td>
<td id="S5.T1.29.17.8.6" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.884</span></td>
<td id="S5.T1.29.17.8.7" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.413</span></td>
<td id="S5.T1.29.17.8.8" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.674</span></td>
<td id="S5.T1.29.17.8.9" class="ltx_td ltx_align_center"><span id="S5.T1.29.17.8.9.1" class="ltx_text" style="font-size:90%;">19.935</span></td>
<td id="S5.T1.29.17.8.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.29.17.8.10.1" class="ltx_text" style="font-size:90%;">73.916</span></td>
</tr>
<tr id="S5.T1.29.18.9" class="ltx_tr">
<td id="S5.T1.29.18.9.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.1.1" class="ltx_text" style="font-size:90%;">1,2,3,10</span></td>
<td id="S5.T1.29.18.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.2.1" class="ltx_text" style="font-size:90%;">0.344</span></td>
<td id="S5.T1.29.18.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.3.1" class="ltx_text" style="font-size:90%;">0.164</span></td>
<td id="S5.T1.29.18.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.4.1" class="ltx_text" style="font-size:90%;">0.782</span></td>
<td id="S5.T1.29.18.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.5.1" class="ltx_text" style="font-size:90%;">19.915</span></td>
<td id="S5.T1.29.18.9.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.6.1" class="ltx_text" style="font-size:90%;">1.955</span></td>
<td id="S5.T1.29.18.9.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.7.1" class="ltx_text" style="font-size:90%;">0.407</span></td>
<td id="S5.T1.29.18.9.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.8.1" class="ltx_text" style="font-size:90%;">0.669</span></td>
<td id="S5.T1.29.18.9.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.9.1" class="ltx_text" style="font-size:90%;">19.847</span></td>
<td id="S5.T1.29.18.9.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S5.T1.29.18.9.10.1" class="ltx_text" style="font-size:90%;">73.905</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The hand-object mesh reconstruction performance on gSDF is shown in Table <a href="#S5.T1" title="Table 1 ‣ 5 Experiments ‣ VR-based generation of photorealistic synthetic data for training hand-object tracking models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Scenarios s0 and s1 shows outlier values for <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="E_{o}(corner)" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><msub id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml"><mi id="S6.p1.1.m1.1.1.3.2" xref="S6.p1.1.m1.1.1.3.2.cmml">E</mi><mi id="S6.p1.1.m1.1.1.3.3" xref="S6.p1.1.m1.1.1.3.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S6.p1.1.m1.1.1.1.1" xref="S6.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.p1.1.m1.1.1.1.1.2" xref="S6.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.p1.1.m1.1.1.1.1.1" xref="S6.p1.1.m1.1.1.1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.1.1.1.2" xref="S6.p1.1.m1.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.1.1.1.1" xref="S6.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.1.1.1.3" xref="S6.p1.1.m1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.1.1.1.1a" xref="S6.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.1.1.1.4" xref="S6.p1.1.m1.1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.1.1.1.1b" xref="S6.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.1.1.1.5" xref="S6.p1.1.m1.1.1.1.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.1.1.1.1c" xref="S6.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.1.1.1.6" xref="S6.p1.1.m1.1.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.p1.1.m1.1.1.1.1.1.1d" xref="S6.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S6.p1.1.m1.1.1.1.1.1.7" xref="S6.p1.1.m1.1.1.1.1.1.7.cmml">r</mi></mrow><mo stretchy="false" id="S6.p1.1.m1.1.1.1.1.3" xref="S6.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2"></times><apply id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.3.1.cmml" xref="S6.p1.1.m1.1.1.3">subscript</csymbol><ci id="S6.p1.1.m1.1.1.3.2.cmml" xref="S6.p1.1.m1.1.1.3.2">𝐸</ci><ci id="S6.p1.1.m1.1.1.3.3.cmml" xref="S6.p1.1.m1.1.1.3.3">𝑜</ci></apply><apply id="S6.p1.1.m1.1.1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1.1"><times id="S6.p1.1.m1.1.1.1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1.1.1.1"></times><ci id="S6.p1.1.m1.1.1.1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.1.1.1.2">𝑐</ci><ci id="S6.p1.1.m1.1.1.1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.1.1.1.3">𝑜</ci><ci id="S6.p1.1.m1.1.1.1.1.1.4.cmml" xref="S6.p1.1.m1.1.1.1.1.1.4">𝑟</ci><ci id="S6.p1.1.m1.1.1.1.1.1.5.cmml" xref="S6.p1.1.m1.1.1.1.1.1.5">𝑛</ci><ci id="S6.p1.1.m1.1.1.1.1.1.6.cmml" xref="S6.p1.1.m1.1.1.1.1.1.6">𝑒</ci><ci id="S6.p1.1.m1.1.1.1.1.1.7.cmml" xref="S6.p1.1.m1.1.1.1.1.1.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">E_{o}(corner)</annotation></semantics></math>, since they use fewer subjects than s2 and s3. Scenario s0 has the best performance for hand mesh reconstruction due to highly accurate ground truth poses for hands in the synthetic data.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Comparing s2 and s3, the variance along columns is quite small. From this, we conclude that replacing parts of the real data with synthetic data does not deteriorate performance significantly. In other words, the generated synthetic data is qualitatively comparable to the real data.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The performance metrics for hand mesh reconstruction are slightly better for the purely real data than for the real-synthetic combination. There are a couple of reasons for this. Currently, it is impossible to control fingers individually in hoisynth, which may lead to artifacts not present in the real data. Secondly, while 3D scans for YCB objects are available, DexYCB does not provide 3D scans of the subjects’ hand meshes. Hence, we were forced to use 3rd party hand meshes, which are different in shape and appearance from the DexYCB subjects’ hands.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">The real-synthetic combination provides slightly better results than pure real data for object reconstruction. A possible reason for this is that the accuracy of ground truth in the synthetic data is better than that of the manually labeled ground truth provided by DexYCB.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we have presented a novel interactive synthetic HOI data generator called “blender-hoisynth”, which is characterized by a high degree of realism (both visual as well as physical). Unlike other related works, it enables VR-based user interaction with virtual objects. Thus training data is generated with the human in the loop, which is especially important for such a human-centric topic as hand-object tracking and reconstruction. Furthermore, we show that there is no significant degradation in HOI tracking model performance after replacing parts of the DexYCB dataset with synthetic data.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We plan to support bimanual HOIs in the future. The curling of fingers during grasping is currently driven by a single analog button on the VR controller. Although the timing and speed of the grasp can be controlled by the user, and the bone trajectories and the bone trjactories can be reasonably randomized, individual finger control is not yet possible. In the future, we plan to support full finger control based on visual hand tracking sensors. The deformations of the hand mesh can be made much more realistic and personalized by relying on works such as DeepHandMesh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. On the user interaction side, the intuitiveness of interactions in blender-hoisynth can benefit immensely from the inclusion of haptic feedback.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Finally, the sim2real gap can be further reduced by using Image-to-Image translation approaches trained on synthetic and real data of the same scene. Replacing the in-built physics based renderers in blender by state-of-the-art differentiable renderers could also be an interesting avenue to explore.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, and
N. Navab,

</span>
<span class="ltx_bibblock">“Model based training, detection and pose estimation of texture-less
3d objects in heavily cluttered scenes,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, vol. 97, no. 2, pp.
238–252, 2012.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox,

</span>
<span class="ltx_bibblock">“Posecnn: A convolutional neural network for 6d object pose
estimation in cluttered scenes,”

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. Hodan, P. Haluza, Š. Obdržálek, and J. Matas,

</span>
<span class="ltx_bibblock">“T-less: An rgb-d dataset for 6d pose estimation of texture-less
objects,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</span>, vol. 41, no. 12, pp. 2990–3004, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Tyree, J. Tremblay, T. To, J. Cheng, T. Mosier, J. Smith, and S. Birchfield,

</span>
<span class="ltx_bibblock">“6-dof pose estimation of household objects for robotic
manipulation: An accessible dataset and benchmark,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Conference on Intelligent Robots and Systems
(IROS)</span>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. Kaskman, S. Zakharov, I. Shugurov, and S. Ilic,

</span>
<span class="ltx_bibblock">“Homebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision (ICCV) Workshops</span>,
2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y.-W. Chao, W. Yang, Y. Xiang, P. Molchanov, A. Handa, J. Tremblay, Y. S.
Narang, K. Van Wyk, U. Iqbal, S. Birchfield, J. Kautz, and D. Fox,

</span>
<span class="ltx_bibblock">“DexYCB: A benchmark for capturing hand grasping of objects,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Hampali, S. D. Sarkar, M. Rad, and V. Lepetit,

</span>
<span class="ltx_bibblock">“Keypoint transformer: Solving joint identification in challenging
hands and object interactions for accurate 3d pose estimation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z. Chen, S. Chen, C. Schmid, and I. Laptev,

</span>
<span class="ltx_bibblock">“gSDF: Geometry-Driven signed distance functions for 3D
hand-object reconstruction,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Yan, D. Misra, A. Bennett, A. Walsman, Y. Bisk, and Y. Artzi,

</span>
<span class="ltx_bibblock">“Chalet: Cornell house agent learning environment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI)</span>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat,
H. Larochelle, and A. Courville,

</span>
<span class="ltx_bibblock">“Home: a household multimodal environment,” 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi,

</span>
<span class="ltx_bibblock">“Ai2-thor: An interactive 3d environment for visual ai,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2017, pp. 6195–6204.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun,

</span>
<span class="ltx_bibblock">“Minos: Multimodal indoor simulator for navigation in complex
environments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2017, pp. 6402–6410.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
F. Xia, A. R. Zamir, Z.-Y. He, A. Sax, J. Malik, and S. Savarese,

</span>
<span class="ltx_bibblock">“Gibson env: Real-world perception for embodied agents,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Computer Vision and Pattern
Recognition (CVPR)</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Denninger, D. Winkelbauer, M. Sundermeyer, W. Boerdijk, M. Knauer, K. H.
Strobl, M. Humt, and R. Triebel,

</span>
<span class="ltx_bibblock">“Blenderproc2: A procedural pipeline for photorealistic rendering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Journal of Open Source Software</span>, vol. 8, no. 82, pp. 4901,
2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Zakour, A. Mellouli, and R. Chaudhari,

</span>
<span class="ltx_bibblock">“Hoisim: Synthesizing realistic 3d human-object interaction data for
human activity recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2021 30th IEEE International Conference on Robot &amp; Human
Interactive Communication (RO-MAN)</span>, 2021, pp. 1124–1131.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X. Puig, J. Ra, M. Boben, T. Gangwani, and A. Torralba,

</span>
<span class="ltx_bibblock">“Virtualhome: Simulating household activities via programs,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2018, pp. 8494–8503.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, and
R. Stiefelhagen,

</span>
<span class="ltx_bibblock">“Letś play for action: Recognizing activities of daily living by
learning from life simulation video games,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2021 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</span>, 2021, pp. 8563–8569.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H. Hwang, C. Jang, G. Park, J. Cho, and I.-J. Kim,

</span>
<span class="ltx_bibblock">“Eldersim: A synthetic data generation platform for human action
recognition in eldercare applications,”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 11, pp. 9279–9294, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. Echeverria, N. Lassabe, Arnaud Degroote, and Séverin Lemaignan,

</span>
<span class="ltx_bibblock">“Modular open robots simulation engine: Morse,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)</span>, 2011, pp. 1742–1748.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Leonardi, A. Furnari, F. Ragusa, and G. M. Farinella,

</span>
<span class="ltx_bibblock">“Are synthetic data useful for egocentric hand-object interaction
detection? an investigation and the hoi-synth domain adaptation benchmark,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2312.02672, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
X. Gao, R. Gong, T. Shu, X. Xie, S. Wang, and S.-C. Zhu,

</span>
<span class="ltx_bibblock">“Vrkitchen: an interactive 3d virtual environment for task-oriented
learning,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv</span>, vol. abs/1903.05757, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas,
J. Kubilius, A. Bhandwaldar, N. Haber, M. Sano, K. Kim, E. Wang,
M. Lingelbach, A. Curtis, K. Feigelis, D. M. Bear, D. Gutfreund, D. Cox,
A. Torralba, J. J. DiCarlo, J. B. Tenenbaum, J. H. McDermott, and D. L. K.
Yamins,

</span>
<span class="ltx_bibblock">“Threedworld: A platform for interactive multi-modal physical
simulation,” 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. Martinez-Gonzalez, S. Oprea, A. Garcia-Garcia, A. Jover-Alvarez,
S. Orts-Escolano, and J. Garcia-Rodriguez,

</span>
<span class="ltx_bibblock">“UnrealROX: An extremely photorealistic virtual reality
environment for robotics simulations and synthetic data generation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Virtual Reality</span>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G. Moon, T. Shiratori, and K. M. Lee,

</span>
<span class="ltx_bibblock">“Deephandmesh: A weakly-supervised deep encoder-decoder framework
for high-fidelity hand mesh modeling,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.17873" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.17874" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.17874">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.17874" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.17875" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 06:25:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
