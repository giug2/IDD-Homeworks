<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04546] ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING</title><meta property="og:description" content="This paper proposes a novel three-tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and compu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04546">

<!--Generated on Fri Apr  5 13:33:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Satwat Bashir
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tasos Dagiuklas
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Kasra Kassai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Muddesar Iqbal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:%7Bbashis11,%20tdagiuklas,%20kasra.kassai,%20m.iqbal%7D@lsbu.ac.uk">{bashis11, tdagiuklas, kasra.kassai, m.iqbal}@lsbu.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_address"><span id="id1.1.id1" class="ltx_ERROR undefined">\add</span>1Department of Computer Science, London South Bank University, London, UK

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This paper proposes a novel three-tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy-preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture’s capability to manage non-IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6> FEDERATED LEARNING, EDGE COMPUTING, CONVERGENCE, NON-IID, MULTI-GLOBAL MODELS
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the digital era, the proliferation of data from the Internet of Things (IoT) and smart devices presents unprecedented opportunities and challenges. Edge computing has become an essential paradigm, processing data proximal to its origin, thereby reducing latency and bolstering privacy. Nonetheless, the integration of Federated Learning (FL) with edge computing encounters significant obstacles, such as high communication costs and data/model heterogeneity, especially with non-Independently and Identically Distributed (non-IID) data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">Tang2024 </a>; <a href="#bib.bib2" title="" class="ltx_ref">Zhou2024 </a>; <a href="#bib.bib3" title="" class="ltx_ref">ResearchChallengesheterogeneous </a></cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL emerges as a revolutionary method, processing data locally on devices while sharing model updates instead of raw data, thus significantly diminishing bandwidth demands and fortifying privacy. Yet, FL confronts notable challenges, including substantial communication costs and the complexity of data/model heterogeneity across disparate devices <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">Hasan2024 </a></cite>. The data collected from various devices often exhibit non-IID characteristics, meaning that the data points are neither independent nor identically distributed across the network. This variation poses significant challenges for traditional learning models, using edge-cloud computing, where data are processed close to its source <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">Zhang2024 </a>; <a href="#bib.bib6" title="" class="ltx_ref">Consul2024 </a></cite>. Therefore, underscores the need for innovative solutions tailored to the demands of edge computing environments <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Sharma2024 </a></cite> .</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the nuanced challenges presented by edge computing and non-IID data, a novel three-layered architecture for FL has been introduced. This design seamlessly integrates <span id="S1.p3.1.1" class="ltx_text ltx_font_italic"> <span id="S1.p3.1.1.1" class="ltx_text ltx_font_bold">clients</span></span>, <span id="S1.p3.1.2" class="ltx_text ltx_font_bold ltx_font_italic">edge layers</span>, and <span id="S1.p3.1.3" class="ltx_text ltx_font_bold ltx_font_italic">fedge layer</span> to enhance data processing efficiency and model efficacy. By defining clear roles and interactions among the layers, the proposed architecture provides a focused strategy for addressing data and computational heterogeneity. The architecture introduces aggregation across two layers and multi-global models, allowing for the management of distinct models and streamlining aggregation to better address the concerns of data diversity <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">Yang2024 </a>; <a href="#bib.bib9" title="" class="ltx_ref">Li2024 </a></cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper is structured as follows: Section 2 provides a background on FL, discussing the multifaceted nature of heterogeneity and its challenges. Section 3 presents the theoretical considerations for improved FL frameworks, introducing our novel three-tier architecture. Section 4 delves into empirical findings on FL with non-IID data, showcasing the architecture’s performance through various scenarios. Section 5 concludes the study with a discussion on the implications of our findings and outlines future directions for research. Throughout, we aim to highlight the innovative aspects of our approach and its potential impact on the field of FL in edge computing environments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background on Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The advent of FL represents a monumental shift towards decentralized model training across a myriad of devices, pivotal in the era dominated by IoT. This approach allows for the harnessing of vast amounts of data directly at the source, thus preserving user privacy and leveraging computational capabilities without the need for data centralization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">FL-BreastCancer </a></cite>. Despite its potential, the practical deployment of FL is fraught with challenges, predominantly due to data heterogeneity, model variance, and the diverse capabilities of devices, which collectively demand innovative solutions for effective global model development <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">ResearchChallengesheterogeneous </a></cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Multifaceted Nature of Heterogeneity</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In FL, heterogeneity manifests in several critical areas: data, model, and device, each introducing unique challenges to the learning process. Data heterogeneity, particularly with non-Independently and Identically Distributed (non-IID) data across devices, leads to significant discrepancies in local model accuracy and global model performance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">FLforECResearchProblems </a>; <a href="#bib.bib12" title="" class="ltx_ref">Adapterfl </a></cite>. This variance underscores the difficulty of creating models that generalize well across diverse data distributions, a fundamental goal of FL. Model heterogeneity, stem from the diverse computational capabilities of devices, influencing the choice of model architectures and thus impacts the coherence of aggregated global models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">huang2019patient </a></cite>. Furthermore, device heterogeneity, is characterized by disparities in computational power, memory, and connectivity, directly affects FL’s efficiency and scalability <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">briggs2020federated </a>; <a href="#bib.bib15" title="" class="ltx_ref">ghosh2019robust </a></cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Adding to these complexities, are the challenges posed by communication heterogeneity, which impacts the efficiency and reliability of data transmission, crucial for collaborative learning in distributed environments. These multifaceted aspects of heterogeneity highlight the intricate challenges FL faces, necessitating comprehensive strategies that address not only the technical hurdles, but also the ethical considerations inherent in deploying FL in real-world scenarios <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">mansour2020three </a>; <a href="#bib.bib17" title="" class="ltx_ref">kim2021dynamic </a></cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tackling Non-IID Data Challenges</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Non-IID data significantly impedes FL’s model performance and learning efficiency. To counteract this, innovative approaches such as Personalized Federated Learning (PFL) have been proposed, focusing on architecture-based and similarity-based personalization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">tan2022towards </a></cite>. Techniques like parameter decoupling and knowledge distillation aim to tailor models to individual device capabilities while drawing insights from the global model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">stallmann2022towards </a>; <a href="#bib.bib20" title="" class="ltx_ref">duan2023combining </a></cite> . Despite these advancements, achieving equitable and efficient learning in federated settings remains a formidable challenge, highlighting the need for further innovation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">DisasterMTL </a></cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Critical Analysis of Aggregation Methods</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">At the heart of FL lies the model aggregation process, with FedAvg being the prototypical method <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">konevcny2016federated </a>; <a href="#bib.bib23" title="" class="ltx_ref">mcmahan2017communication </a>; <a href="#bib.bib24" title="" class="ltx_ref">banabilah2022federated </a></cite>. While FedAvg has shown effectiveness under conditions of homogeneity, its performance significantly diminishes in the face of data heterogeneity, leading to a loss of precision in local models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">tan2022towards </a>; <a href="#bib.bib25" title="" class="ltx_ref">shamsian2021personalized </a></cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In the study referenced as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">pillutla2022federated </a></cite>, the authors evaluate the impact of utilizing pre-trained models on local devices, focusing on the aggregation of shared parameters. They find that employing pre-trained models and sharing a subset of parameters from the start can enhance convergence speed, improve performance, and lower communication costs. Similarly, the work cited as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">achituve2021personalized </a></cite> introduces pFedGP, which utilizes a shared kernel function to increase the accuracy of local models. However, this approach is constrained by the size of the dataset due to the limitations of the shared kernel function. To tackle the issue of communication overhead in personalized model training, the study <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">ozkara2021quped </a></cite> applies soft quantization and transfer learning techniques, achieving higher accuracy for local devices and enhancing communication efficiency. Likewise, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">zhang2021parameterized </a></cite> employs transfer learning to categorize devices with similar data and facilitate collaborative training for varied data sets.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Other research, such as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">liang2020think </a></cite>, focuses on incorporating personalized layers through fine-tuning techniques to train the global model’s shallow layers on each device. This method involves sharing only the foundational layers during the aggregation process, but it faces challenges related to storing the personalized model on each local device. Some studies, like <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">t2020personalized </a></cite>, address the issue of data heterogeneity by framing it as a regularization problem, utilizing Moreau Envelopes to improve the global model’s convergence. However, a notable limitation of these approaches is their reliance on numerous adjustable parameters.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Despite these innovative strategies, limitations remain, particularly in handling complex real-world data distributions, ensuring communication efficiency, and accommodating diverse scenarios. The pursuit of more adaptable and robust solutions continues, reflecting the evolving landscape of FL research <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">xia2021survey </a>; <a href="#bib.bib33" title="" class="ltx_ref">kairouz2021advances </a></cite>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Towards Comprehensive Solutions</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The exploration of aggregation methods and personalization strategies within FL reveals a concerted effort to address the challenges posed by heterogeneity and non-IID data as demonstrated in studies <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">DisasterMTL </a>; <a href="#bib.bib10" title="" class="ltx_ref">FL-BreastCancer </a></cite>. However, the limitations of current approaches underscore the pressing need for adaptable, sophisticated solutions capable of navigating the intricacies of FL environments. The shift towards advanced edge-cloud models, the employment of cutting-edge algorithms, and the development of multi-model systems illustrate the ongoing quest for enhanced FL frameworks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">evolution </a></cite>. Our findings lay the groundwork for a transformative FL ecosystem, characterized by its robustness, efficiency, and unparalleled scalability across myriad applications. Looking ahead, we explore advanced machine learning algorithms that could further refine the proposed architecture’s ability to manage extreme data heterogeneity. Additionally, we aim to investigate the implications of our framework in real-world scenarios, such as IoT and mobile computing, where FL’s potential can be fully unleashed.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Theoretical Considerations for Improved FL
Frameworks</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the rapidly evolving field of FL, identifying architectures capable of handling the complexities of distributed data processing is critical. Traditional FL models have pioneered the way forward, but often grapple with limitations due to non-IID data distributions and significant computational demands on client devices. In response to these challenges, this paper proposes a novel three-tier FL architecture designed to enhance data privacy, computational efficiency, and scalability across distributed networks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Rationale for a Multi-Global Model Framework</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed architecture significantly diverges from traditional FL paradigms by implementing a multi-global model strategy within a hierarchical framework. This approach is analyzed for its potential to facilitate personalized and efficient learning across heterogeneous devices and data distributions. The operational mechanics of each layer are shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Rationale for a Multi-Global Model Framework ‣ 3 Theoretical Considerations for Improved FL Frameworks ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, illustrating how they collectively contribute to model convergence and scalability. Preliminary simulations underscore the adaptability and performance of our architecture when compared to standard FL frameworks, particularly under non-IID conditions.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2403.04546/assets/image.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="674" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proposed three-layered architecture, delineating the interactions between the client level, edge layer, and fedge layer</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">The Client Layer</span>, profiling ensures that each participant begins with a model specifically tailored to its unique data set. This enables effective local training, optimizing computational load and aligning model evolution with individual data characteristics. Empirical evidence demonstrates that this alignment results in improved initial accuracy and faster convergence rates.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">The Edge Layer</span> acts as a critical coordinator, analyzing metadata from client training sessions to assess model states and requirements. If a suitable model match for a client is not found within its repository, the edge layer queries the fedge layer. This process ensures that clients are always provided with the most appropriate model for their data, enhancing the learning process’s efficiency and personalization.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">The Fedge Layer</span> is responsible for storing multiple distinct global models, managing them to ensure they are up-to-date and accurately reflect the diverse data landscapes they represent. When requested by the edge layer, the fedge Layer searches for a suitable model match. If no match is found, it informs the edge layer, prompting the creation of a new model for the client. This new model is then saved as a global model, enriching the fedge Layer’s repository. Furthermore, the fedge Layer periodically updates its global models based on inputs from the edge layer, performing aggregation and updates of similar models to maintain a current and effective suite of global models.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">This structured approach tackles computing issues and helps create better learning environments. We have carefully followed principles of technical precision in our work, making sure our approach, experiments, and results are shared accurately and clearly.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Innovation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The core innovation of the proposed architecture is its deployment of multiple global models, a strategy unprecedented in current FL frameworks. This supports personalized learning for diverse client datasets and introduces a novel method of model sharing and aggregation that significantly reduces computational overhead. The empirical findings affirm this architecture’s potential to redefine FL paradigms, offering a scalable solution for real-time data analysis.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Empirical Findings on Federated Learning with Non-IID Data</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section delves into the performance of FL, specifically focusing on the Federated Averaging (FedAvg) method, under non-IID data conditions. This investigation is supported by a experimental setup, revealing insights into FL’s adaptability to varied data distributions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>System Architecture and Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset:</span> The analysis employs the MNIST dataset, a benchmark for evaluating machine learning models through digit recognition tasks, featuring 70,000 grayscale images across ten classes <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">hojjatk_mnist </a></cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Scenario Design</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We aim to explore the multifaceted nature of non-IID distributions in a controlled yet challenging environment. For this, we designed two distinct non-IID scenarios using the MNIST dataset to test the FL architecture’s handling of data heterogeneity: By simplifying the scenario to these two configurations, we not only highlight the limitations of traditional FL model in handling such disparities, but also establish a clear, proof-of-concept framework for demonstrating the superior adaptability and performance of our proposed three-layer architecture. This approach allows to systematically assess the architecture’s efficacy in managing non-IID data, ensuring that the findings have practical implications for enhancing FL in heterogeneous environments.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Generalizable Non-IID Scenario</span>: The MNIST dataset has been divided among three clients to simulate a non-IID but generalized scenario. An equal number of images has been allocated to each client. Such allocation provides restriction to distinct labels, ensuring diversity in data distribution while maintaining a level of generality across the dataset.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Non-Generalizable Non-IID Scenario</span>: For a more challenging setup, we have created another non-IID scenario by dividing the MNIST dataset between two clients. One client has received images from only two labels with a robust dataset size for training, while in the second client has been allocated images from the remaining eight labels, presenting a challenging test for the architecture’s efficiency.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Client-Server and Three-Tier FL Models:</span> We compare the traditional client-server FL model with our proposed three-tier FL model, highlighting the latter’s enhanced data privacy, computational efficiency, and scalability.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_bold">Methodology and Model Implementation</span> </h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">FL Algorithms and Model Details:</span> The FedAvg algorithm benchmarks the standard FL approach, while the proposed three-tier FL model introduces a specialized algorithm to better manage non-IID data through a multi-global model framework. The dynamic interactions and the structural foundation of the proposed three-tier FL model is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.3 Methodology and Model Implementation ‣ 4 Empirical Findings on Federated Learning with Non-IID Data ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This sequence diagram elucidates the multi-global model strategy implemented within a hierarchical framework, showcasing the model’s innovative approach to managing non-IID data.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2403.04546/assets/sequence_diag.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The sequence diagram illustrates the dynamic interactions within the proposed architecture, focusing on the multi-global model strategy within a hierarchical framework </figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Model Specification:</span> A Simple Convolutional Neural Network (SimpleCNN) has been chosen for consistency in evaluating both FL models against the MNIST dataset.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Parameters and Data Handling</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Parameters and Configurations:</span> The experimental setup involves multiple communication rounds, a learning rate of 0.01, momentum of 0.5, and a batch size of 64.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Data Preprocessing:</span> Standard normalization processes has been applied to ensure uniform data scaling across clients.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Model Serialization:</span> We detail the serialization process for both FL models, emphasizing the efficient transmission of model updates between clients, edge layers, and the global model repository.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Findings, Behaviors, and Insights</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">In the first scenario, which tested the models’ response to label skew with a balanced data distribution, the standard FL model showed a progressive increase in accuracy. Client 1’s accuracy began at 96.22% and rose to 99.36%. For Client 2, the accuracy started at 96.93% and reached 99.14%, and Client 3 improved from 94.82% to 98.90%. These trends are depicted in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.5 Findings, Behaviors, and Insights ‣ 4 Empirical Findings on Federated Learning with Non-IID Data ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.04546/assets/SFL-fix_title-1.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy of Standard FL for non-IID Scenario 1</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">The three-tier FL model demonstrated a consistent high accuracy across all clients from the start, with Client 1 reaching 99.62%, Client 2 ending at 99.38%, and Client 3 at 99.47% by the tenth round. These findings indicate the three-tier model’s ability to quickly achieve and maintain high accuracy, as shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.5 Findings, Behaviors, and Insights ‣ 4 Empirical Findings on Federated Learning with Non-IID Data ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.04546/assets/3FL-1.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Accuracy of Three Layered FL for non-IID Scenario 1</figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">In the second scenario, designed to present a more complex non-IID situation with both label and data skew, the standard FL model and the three-tier FL model were subjected to a challenging test. For the standard FL model, Client 1 managed to maintain an accuracy of nearly 99.86%, while Client 2, with less representative data and a broader label distribution, started at a lower 14.5% accuracy, gradually increasing to 71.38% as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.5 Findings, Behaviors, and Insights ‣ 4 Empirical Findings on Federated Learning with Non-IID Data ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This growth trajectory signifies the model’s gradual adaptation to the non-IID conditions.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2403.04546/assets/SFL-fix_title-2.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy of Standard FL for non-IID Scenario 2</figcaption>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Conversely, the three-tier FL model excelled, with Client 1 maintaining a near-perfect accuracy of approximately 99.95%, and Client 2, despite the sparse data representation and more extensive label range, exhibited a substantial improvement, escalating from 29.25% to 84.88% accuracy. This remarkable progress is graphically represented in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5 Findings, Behaviors, and Insights ‣ 4 Empirical Findings on Federated Learning with Non-IID Data ‣ ARCHITECTURAL BLUEPRINT FOR HETEROGENEITY-RESILIENT FEDERATED LEARNING" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2403.04546/assets/3FL-2.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy of Three Layered FL for non-IID Scenario 2</figcaption>
</figure>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p">These results validate the standard FL model’s capacity to achieve generalization over time in balanced scenarios, but they also spotlight the three-tier FL model’s advanced capability to handle various degrees of non-IID data efficiently. The superior performance of the three-tier model, especially for Client 2 in the second scenario, underlines its effectiveness in real-world applications, where data distributions and label availability can significantly differ. The model’s robustness in enhancing learning under such complex conditions attests to its potential for broad deployment and underscores the value of the proposed FL technique.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study introduced a novel three-layered architecture for FL, designed to address the significant challenges of integrating FL with edge computing, especially under conditions of non-IID data. Our architecture demonstrates a promising potential to enhance FL’s applicability in edge computing environments by optimizing training and processing efficiencies. By introducing an intermediary layer, the fedge layer, and employing a multi-global model framework, we provided a focused strategy for managing client and computational heterogeneity. Empirical evaluations, utilizing the MNIST dataset under various non-IID scenarios, have shown that our proposed architecture outperforms traditional FL frameworks in terms of accuracy, scalability, and efficiency.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">However, it’s important to acknowledge the limitations of our current work. The proposed architecture, while innovative, serves as a foundational proof of concept. It is important to acknowledge that the architecture’s effectiveness, as demonstrated under controlled experimental conditions, may vary in complex real-world scenarios. The adaptation to different types of non-IID data, the management of communication overhead, and the practical deployment in edge environments with varied device capabilities and network conditions are aspects that require further exploration and validation.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In conclusion, this paper contributes a significant step forward in the pursuit of more efficient, privacy-preserving, and scalable FL solutions tailored for edge computing. The proposed three-layered architecture, with its emphasis on addressing non-IID data challenges and client heterogeneity, lays the groundwork for future advancements in the field. By continuing to refine and expand upon this framework, we can move closer to realizing the full potential of FL in the edge computing paradigm.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In future work, we plan to further refine and validate the effectiveness of our proposed three-layer architecture by testing it alongside the latest and most innovative federated learning approaches. This comparative analysis will provide valuable insights into where our model stands in the current research landscape and identify areas for improvement. Additionally, experimenting with other benchmark datasets beyond MNIST will allow us to assess the versatility and robustness of our architecture across different types of data and learning scenarios. This step is crucial for ensuring that our model can be effectively applied in various domains and with diverse data characteristics.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, we aim to enhance the functionalities and operation of the fedge layer to better suit real-world scenarios. This includes optimizing the fedge layer for more efficient model management and updating processes, which are vital for handling the dynamic and diverse nature of data in practical applications. Another critical area of development will be the refinement of the client selection method. By designing more sophisticated criteria for client selection, we can ensure that our architecture is adaptable to real and broader contexts, improving its applicability and efficiency in distributed learning environments.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">By focusing on these areas, we anticipate not only strengthening the foundation of our proposed architecture but also significantly advancing its potential for practical implementation in federated learning systems.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Tang, Y. Xiao, L. Zhang, B. Cao, M. Tang, and Q. Yang, “AFL-HCS: asynchronous federated learning based on heterogeneous edge client selection,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Cluster Computing</em>, 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10586-024-04314-9

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F. Zhou, S. Hu, X. Du, X. Wan, and J. Wu, “A Lightweight Neural Network Model for Disease Risk Prediction in Edge Intelligent Computing Architecture,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Future Internet</em>, vol. 16, no. 3, p. 75, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Ye, X. Fang, B. Du, P. C. Yuen, and D. Tao, “Heterogeneous federated learning: State-of-the-art and research challenges,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol. 56, no. 3, pp. 1–44, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. T. Hasan and A. K. Idrees, “Federated Learning for IoT/Edge/Fog Computing Systems,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, 2024. [Online]. Available: https://arxiv.org/abs/2402.13029

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z. Zhang, X. Pei, and Y. Zhang, “Cost-efficient hierarchical federated edge learning for satellite-terrestrial Internet of Things,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Research Square</em>, 2024. [Online]. Available: https://www.researchsquare.com/article/rs-3962040/latest.pdf

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P. Consul, I. Budhiraja, D. Garg, and N. Kumar, “Resource Allocation Approach For Digital Twin-Empowered UAV-Assisted MEC Network Using Federated Reinforcement Learning For Future Wireless Network,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions</em>, 2024. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/10443270/

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Sharma and P. Kaur, “Fog-based Federated Time Series Forecasting for IoT Data,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Journal of Network and Systems Management</em>, 2024. [Online]. Available: https://link.springer.com/article/10.1007/s10922-024-09802-2

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z. Yang, S. Zhang, C. Li, M. Wang, and H. Wang, “Efficient knowledge management for heterogenous federated continual learning on resource-constrained edge devices,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Li, Y. Yang, Y. Liu, and W. Pei, “Federated Dueling DQN Based Microgrid Energy Management Strategy in Edge-Cloud Computing Environment,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Sustainable Energy, Grids and Networks</em>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. N. Tan, V. P. Tinh, P. D. Lam, N. H. Nam, and T. A. Khoa, “A transfer learning approach to breast cancer classification in a federated learning framework,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 11, pp. 27 462–27 476, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q. Xia, W. Ye, Z. Tao, J. Wu, and Q. Li, “A survey of federated learning for edge computing: Research problems and solutions,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">High-Confidence Computing</em>, vol. 1, no. 1, p. 100008, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. Liu, M. Hu, Z. Xia, J. Xia, P. Zhang, Y. Huang, Y. Liu, and M. Chen, “Adapterfl: Adaptive heterogeneous federated learning for resource-constrained mobile computing systems,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.14037</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Huang, A. L. Shea, H. Qian, A. Masurkar, H. Deng, and D. Liu, “Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, vol. 99, p. 103291, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Briggs, Z. Fan, and P. Andras, “Federated learning with hierarchical clustering of local updates to improve training on non-iid data,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>.   IEEE, 2020, pp. 1–9.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Ghosh, J. Hong, D. Yin, and K. Ramchandran, “Robust federated learning in a heterogeneous environment,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06629</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three approaches for personalization with applications to federated learning,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.10619</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Kim, E. Al Hakim, J. Haraldson, H. Eriksson, J. M. B. da Silva, and C. Fischione, “Dynamic clustering in federated learning,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICC 2021-IEEE International Conference on Communications</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Z. Tan, H. Yu, L. Cui, and Q. Yang, “Towards personalized federated learning,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Stallmann and A. Wilbik, “Towards federated clustering: A federated fuzzy <math id="bib.bib19.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="bib.bib19.1.m1.1a"><mi id="bib.bib19.1.m1.1.1" xref="bib.bib19.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="bib.bib19.1.m1.1b"><ci id="bib.bib19.1.m1.1.1.cmml" xref="bib.bib19.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.1.m1.1c">c</annotation></semantics></math>-means algorithm (ffcm),” <em id="bib.bib19.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.07316</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Q. Duan, J. Huang, S. Hu, R. Deng, Z. Lu, and S. Yu, “Combining federated learning and edge computing toward ubiquitous intelligence in 6g network: Challenges, recent advances, and future directions,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. J. Wong, M.-L. Tham, B.-H. Kwan, E. M. A. Gnanamuthu, and Y. Owada, “An optimized multi-task learning model for disaster classification and victim detection in federated learning environments,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 10, pp. 115 930–115 944, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Federated learning: Strategies for improving communication efficiency,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Banabilah, M. Aloqaily, E. Alsayed, N. Malik, and Y. Jararweh, “Federated learning review: Fundamentals, enabling technologies, and future applications,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Information processing &amp; management</em>, vol. 59, no. 6, p. 103061, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Shamsian, A. Navon, E. Fetaya, and G. Chechik, “Personalized federated learning using hypernetworks,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2021, pp. 9489–9502.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. Pillutla, K. Malik, A.-R. Mohamed, M. Rabbat, M. Sanjabi, and L. Xiao, “Federated learning with partial model personalization,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 17 716–17 758.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
I. Achituve, A. Shamsian, A. Navon, G. Chechik, and E. Fetaya, “Personalized federated learning with gaussian processes,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 8392–8406, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Ozkara, N. Singh, D. Data, and S. Diggavi, “Quped: Quantized personalization via distillation with applications to federated learning,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 3622–3634, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Zhang, S. Guo, X. Ma, H. Wang, W. Xu, and F. Wu, “Parameterized knowledge transfer for personalized federated learning,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 10 092–10 104, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
P. P. Liang, T. Liu, L. Ziyin, N. B. Allen, R. P. Auerbach, D. Brent, R. Salakhutdinov, and L.-P. Morency, “Think locally, act globally: Federated learning with local and global representations,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.01523</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. T Dinh, N. Tran, and J. Nguyen, “Personalized federated learning with moreau envelopes,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 21 394–21 405, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Q. Xia, W. Ye, Z. Tao, J. Wu, and Q. Li, “A survey of federated learning for edge computing: Research problems and solutions,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">High-Confidence Computing</em>, vol. 1, no. 1, p. 100008, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances and open problems in federated learning,” <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, vol. 14, no. 1–2, pp. 1–210, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Kassai, S. Ghosh, and A. Dagiuklas, “Evolution of orchestration towards 5g,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Journal of Communication</em>, vol. 14, no. 12, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
HojjatK, “Mnist dataset,” https://www.kaggle.com/datasets/hojjatk/mnist-dataset, Year of Access, accessed: date-of-access.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.04545" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.04546" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.04546">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04546" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.04547" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 13:33:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
