<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.03977] PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</title><meta property="og:description" content="Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield gra…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.03977">

<!--Generated on Wed Feb 28 13:48:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Florian Bordes<sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>  Shashank Shekhar<sup id="id11.11.id2" class="ltx_sup"><span id="id11.11.id2.1" class="ltx_text ltx_font_italic">1</span></sup>  Mark Ibrahim<sup id="id12.12.id3" class="ltx_sup"><span id="id12.12.id3.1" class="ltx_text ltx_font_italic">1</span></sup>  Diane Bouchacourt<sup id="id13.13.id4" class="ltx_sup"><span id="id13.13.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><span id="id5.5.1" class="ltx_text ltx_font_bold">Pascal Vincent<sup id="id5.5.1.1" class="ltx_sup"><span id="id5.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2,3</span></sup></span>  <span id="id7.7.3" class="ltx_text ltx_font_bold">Ari S. Morcos<sup id="id7.7.3.1" class="ltx_sup"><span id="id7.7.3.1.1" class="ltx_text ltx_font_medium">1</span></sup>
<br class="ltx_break"><sup id="id7.7.3.2" class="ltx_sup"><span id="id7.7.3.2.1" class="ltx_text ltx_font_medium">1</span></sup></span>FAIR, Meta     <sup id="id14.14.id5" class="ltx_sup">2</sup>Mila - Quebec AI Institute     <sup id="id15.15.id6" class="ltx_sup">3</sup>Université de Montréal, DIRO
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation.
Despite such promise, the use of synthetic image data is still limited – and often played down – mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear.
In this work, we present a path to democratize the use of <em id="id16.id1.1" class="ltx_emph ltx_font_italic">photorealistic</em> synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both <em id="id16.id1.2" class="ltx_emph ltx_font_italic">controllability</em> and <em id="id16.id1.3" class="ltx_emph ltx_font_italic">realism</em>. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce <span id="id16.id1.4" class="ltx_text ltx_font_bold">PUG (Photorealistic Unreal Graphics)</span> environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models. The datasets can be downloaded at <a target="_blank" href="https://pug.metademolab.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pug.metademolab.com/</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">A grand goal of machine learning is to learn representations of data that are useful across many tasks.
Essential to measuring and making progress towards this goal is the availability of ample controllable, realistic data for evaluation and training. This is especially true when considering deep neural network models not only in terms of their raw accuracy, but also their robustness and fairness—crucial properties for models deployed in real-world applications. However, collecting such data is challenging, presenting issues with privacy, bias, and copyright. Furthermore, the majority of available image datasets lack fine-grained labels and are challenging to manipulate beyond coarse image augmentations (e.g. with a photograph, it is hard to change the viewpoint or the time of day).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Using synthetic image data where we precisely control all the factors affecting the rendered scene gives easy access to the corresponding rich set of factor labels. This enables evaluating the extent of a trained deep neural network’s abilities, most importantly its robustness. Is the network robust to change in pose? Are the predictions similar for different textures? All these questions may be answered systematically by using synthetic data, enabling highly rigorous evaluations of deep neural network models. In addition, training could also benefit from controllable factors<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We define factors here as distinctive attributes that describe the data, such as color or pose of an object.</span></span></span>, by increasing the robustness of models with respect to these factors. They may also be used to monitor training, e.g. tracking which factors a model focuses on or becomes most invariant to, and in which order, as training progresses. This potentially enables better understanding of the training and generalization dynamics in deep neural networks. However the lack of realism typical in many of the currently available synthetic image datasets, and their usually very limited scope greatly limits their usefulness for general image representation learning research.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To address this, we introduce<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>As a reminder, any use of content or technologies made available by Unreal and/or Epic Games, or any other provider, should comply with their applicable terms (such as the Content License Agreement available at <a target="_blank" href="https://www.unrealengine.com/en-US/eula/content" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/en-US/eula/content</a> or any other direct agreement you may have with Epic / Unreal)</span></span></span> a new family of synthetic <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">Photorealistic Unreal Graphics</em> (PUG) <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">datasets</em>, designed for ease of use by the representation learning research community, where image realism is significantly improved compared to current public synthetic image datasets. The environments were built using the Unreal Engine <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">EpicGames, </a>)</cite>, which is widely used in the video game and entertainment industries and praised for its realism. In addition to pre-rendered static image datasets, we also introduce the TorchMultiverse python library, which offers a simple python interface to enable easily controlled dataset creation from any given PUG environment. Using these tools, we contribute 4 new datasets and show their usefulness across several different research domains. To summarize:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.03977/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="280" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span id="S1.F1.2.1" class="ltx_text ltx_font_bold">The PUG Dataset Family</span> (left) Cartoon illustration of our dataset creation setup, which consists of two steps: environment creation and then data creation. (right) Example images from PUG: Animals, PUG: Image-Net and PUG: SPAR.
</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a new family of environments and image datasets (coined as PUG) for representation learning, based on the Unreal Engine <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">EpicGames, </a>)</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:2.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">PUG: Animals</em> for research on out-of-distribution (OOD) generalization and to study the representational space of foundation models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:2.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We introduce <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">PUG: ImageNet</em> as an additional robustness test set to ImageNet, containing a rich set of factor changes such as pose, background, size, texture, and lighting.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:2.5pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i4.p1.1" class="ltx_p">We introduce <em id="S1.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">PUG: SPAR</em> for evaluating vision-language models. We use it to demonstrate how synthetic data can be utilized to address known benchmark limitations. In addition, we introduce <em id="S1.I1.i4.p1.1.2" class="ltx_emph ltx_font_italic">PUG: AR4T</em> for fine-tuning vision-language models and use it to demonstrate the reliability of PUG: SPAR in contrast to other benchmarks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Synthetic data for representation learning</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">To address robustness shortcomings, researchers today commonly study representations using lower-fidelity controlled datasets such as CLEVR, Biased Cars, and ShapeNet <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib32" title="" class="ltx_ref">2017</a>; <a href="#bib.bib43" title="" class="ltx_ref">Madan et al., </a>; Chang et al., <a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite>. Other datasets also contain precise factor labels useful for probing how well a representation encodes each factor in a structured form <cite class="ltx_cite ltx_citemacro_citep">(Gondal et al., <a href="#bib.bib23" title="" class="ltx_ref">2019</a>; Struckmeier et al., <a href="#bib.bib54" title="" class="ltx_ref">2023</a>; Weng, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite>. While these datasets offer control in terms of the factors that change as well as the train and evaluation splits enabling controlled scientific experimentation, they lack realism. This gap between the lower-fidelity controlled data and the real world poses a challenge for the broader application of these studies. On the other hand, photorealistic datasets have been explored in various application-specific domains in machine learning (outside of representation learning.) This is especially relevant when trying to evaluate and train models on rare events in which getting real data might be really difficult, such as for autonomous driving. CARLA <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> is a popular self-driving car simulator which offer highly realistic environment with a significant amount of controllable factors such as environmental conditions, full control of all static and dynamic actors and maps rendering.
Another domain where simulated environments are commonly used is reinforcement learning (RL), as RL algorithms often requires the ability to run millions of simulations to learn to master non-trivial tasks, and this cannot be done in a real environment. Data environments based on video games like Atari have been very popular to design and evaluate RL algorithms. Alternatively, platforms like Habitat <cite class="ltx_cite ltx_citemacro_citep">(Szot et al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> offers indoor scene for training home assistant agents. While these simulators, games or datasets can offer some photo-realism and mimic real world interactions for agents, they are relegated to domain-specific applications making them challenging to use for evaluating the representations of deep neural networks more broadly. Since our focus is not RL, we do not need to embed a fast simulator capable of rendering several thousands frames per second for effective online-training. Instead we can pre-render custom high-quality datasets offline.
Photorealistic environments and datasets have also been explored for more general domains with the ThreeDWorld platform <cite class="ltx_cite ltx_citemacro_citep">(Gan et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Based on the Unity game engine, it offers an interactive environment that can be leveraged to create datasets. The environment is presented as a simulator that is generic enough to handle multiple uses cases, and users can customize the setup of a scene and the data smapling through a low level API. One such dataset that utilizes ThreeDWorld is the Synthetic Visual Concepts (SyVIC) dataset <cite class="ltx_cite ltx_citemacro_citep">(Cascante-Bonilla et al., <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>, which uses the API to create scene images and descriptive captions for training vision-language models. One of the downsides of ThreeDWorld is that the back-end, the simulator itself, is closed source which limits external contributions. In contrast with ThreeDWorld, we do not provide a platform or a generic simulator for people to use. In fact, we believe that tools like the Unreal Engine are simple enough to be used directly by researchers to create the environments they want without the need to use an intermediate platform. In addition, being free of such intermediate platform allows us to leverage most of the content created for video gaming directly into our simulator by using the existing Epic Games
marketplace.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluating model robustness</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">To study model robustness, there is an inherent trade-off between photo-realism and control. Photo-realism depicts objects as they appear in the real world, but often lacks control to precisely define the factors to describe the object such as pose or background. Prior works either collect natural images with specific factor changes <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib65" title="" class="ltx_ref">2020</a>; Barbu et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> or label distinctive factors in existing datasets <cite class="ltx_cite ltx_citemacro_citep">(Idrissi et al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>. Such datasets allow researchers to measure average accuracy on photo-realistic images, but lack granular control necessary for precisely controlled robustness experiments. On the other hand, prior studies <cite class="ltx_cite ltx_citemacro_citep">(Ibrahim et al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>; Abbas and Deny, <a href="#bib.bib1" title="" class="ltx_ref">2022</a>; Alcorn et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> examine model robustness with respect to factors such as pose and size by rendering
3D-objects such as buses. These studies precisely control how each object is depicted, but lack in realism. In this work, we advance the photo-realism of these prior works by using the Unreal engine 5.0 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">EpicGames, </a>)</cite>, a rendering engine commonly used in high-end cinematic CGI and high-resolution video games which allow us to measure robustness with respect to factors of variation such as lighting.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Benefits and limitations of using generative models as data generator</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Another way to generate realistic datasets is to use generative models<cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Goodfellow et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>. However, one limitation of such models, despite impressive improvements in the last few years <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal and Nichol, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, is the lack of quality control on what the model can produce <cite class="ltx_cite ltx_citemacro_citep">(Gandikota et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>. It’s not uncommon to find cases in which the model will ignore parts of the conditioning prompt. Despite such limitation many works have tried to leverage generative model as an additional source of data to train deep neural networks with some success <cite class="ltx_cite ltx_citemacro_citep">(Astolfi et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>; Bansal and Grover, <a href="#bib.bib6" title="" class="ltx_ref">2023</a>; Trabucco et al., <a href="#bib.bib58" title="" class="ltx_ref">2023</a>; Azizi et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib70" title="" class="ltx_ref">2021</a>; Li et al., <a href="#bib.bib37" title="" class="ltx_ref">2022a</a>; Jahanian et al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Jain et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Sariyildiz et al., <a href="#bib.bib50" title="" class="ltx_ref">2023</a>; He et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. Another limitation of using generative models are privacy concerns that arise from such models replicating data from their training datasets <cite class="ltx_cite ltx_citemacro_cite">Somepalli et al. (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>. Finally, <cite class="ltx_cite ltx_citemacro_citet">Shumailov et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite> recently demonstrated that training on data recursively generated from such models results in increasing underestimates of the tails and overestimates of the mode, amplifying bias in datasets. In contrast to generative models that might produce unreliable results, we use an entirely controllable environment for which we can have a known and accurate generation with respect to a set of factors.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Photorealistic Unreal Graphics (PUG) environments and datasets</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Leveraging Unreal Engine to create environments and datasets for representation learning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">We introduce the <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Photorealistic Unreal Graphics</em> (PUG) environments, a family of 3D graphics environments that leverage Unreal Engine for rendering image data for representation learning research. To create a PUG environment, we first obtain a number of assets <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We purchased assets from the Epic Game Store and used assets from Sketchfab <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. The complete list of assets we have used is available at <a target="_blank" href="https://github.com/facebookresearch/PUG" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/PUG</a></span></span></span> which can be 3D objects or 3D backgrounds. Then, we import them in the Unreal Engine editor and create blueprints that yield a simple generic 3D environment.
Once this generic and controllable environment is created, it is compiled into a Linux binary file, which can be run on standard GPU clusters. This environment is programmed in such a way that when running, it is listening for incoming packets through WebRTC which can specify instructions about how to change a scene.
Since most machine learning practitioners are used to python scripting, we wanted to have a very simple approach by which a user can request image data rendered from a packaged PUG environment, through very simple python code and JSON config files. To do so, we developed a python API, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">TorchMultiverse</span>, that allows a user to easily specify a scene configuration in JSON and request rendered images from the PUG by using WebRTC<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>A similar API using IPC sockets was developed by <cite class="ltx_cite ltx_citemacro_citet">Qiu et al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite> for Unreal Engine 4.</span></span></span> . Once the factors have been set as requested by the user, for a specific environment configuration, the user can send a command to freeze the current environment and receive back an image. It takes around 1 second to render an image at a resolution of 512x512 on a V100 GPU<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In our setup, we paralyze the rendering across 64 GPUs. A dataset like PUG: Animals which contains 200K images has taken around 1h to be entirely rendered.</span></span></span>. We illustrate this setup in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It shows how, starting from 3D assets, we design interactive environments that enable us to create different datasets. In the present work, we focus on pre-rendered static image datasets, however our setup also allows dynamic communication between a PUG environment and a pytorch program, meaning that new data could be requested and rendered on the fly while training a deep neural network. We leave the exploration of such active learning setups, as well as the rendering of videos, as future work.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>It might also conceivably be used as a photorealistic interactive environment for reinforcement learning (RL), but the high quality image rendering achieved in this system currently appears too compute-intensive and slow to be practically useful in the context of current RL research. Our initial targeted research community and use case is that of supervised and self/unsupervised representation learning from image data, rather than RL.</span></span></span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>PUG: Animals</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2308.03977/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="297" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>We present <em id="S3.F2.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">PUG: Animals</em>, a new photorealistic synthetic dataset with annotated factors of variations to evaluate the out-of-distribution (OOD) robustness of models.
</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As the first member of the PUG family we introduce <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">PUG: Animals</em> (Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), which contains 215 040 pre-rendered images using 70 animals assets, 64 backgrounds, 3 object sizes, 4 textures, under 4 different camera orientations. PUG: Animals is designed with the intent to create a dataset with every combination of the factors of variation available. PUG: Animals allows one to precisely control distribution shifts between training and testing which can give researchers better insight on how a deep neural network generalizes on held out factors of variations. Surprisingly, the usage of 3D realistic synthetic data is limited in OOD generalization research – with the exception of Biased-cars<cite class="ltx_cite ltx_citemacro_citep">(Madan et al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> that has been used to study generalization on new category-viewpoints. Commons OOD datasets are Colored MNIST <cite class="ltx_cite ltx_citemacro_citep">(Arjovsky et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> – to study how well a network can generalize to unseen combinations of digits and colors and MNIST-R <cite class="ltx_cite ltx_citemacro_citep">(Ghifary et al., <a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> – to study generalization on new combination of digits and rotations. However, MNIST-based dataset might be too toyish to evaluate modern architectures. A more <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">realistic</span> dataset based on real images is Nico++<cite class="ltx_cite ltx_citemacro_citep">(Xingxuan Zhang, <a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> – to study generalization with respect to different domains or environments. However, in Nico++ the objects and backgrounds are never entirely disentangle (the context background is different for each image). Thus, it is never clear if the model is failing because of the context or because of a specific object (since the contexts and the objects are never disentangle).</p>
</div>
<figure id="S3.F3" class="ltx_figure ltx_align_floatright">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2308.03977/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="110" height="70" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2308.03977/assets/x4.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="111" height="74" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2308.03977/assets/x5.png" id="S3.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="110" height="72" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Accuracy on held out factors with PUG: Animals. Each line and value C correspond to the number of animals for which all the factors are seen. The test space is built by taking all the factors minus the training factors. If we train on the Default texture, then the network is evaluate on Grass, Sky and Asphalt. If we train on 50 backgrounds, then we evaluate on 64 (total number of background) - 50 (training background) = 14 backgrounds.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">In contrast, in PUG: Animals the animal asset is always the same, in that case the environment factor and the objects are perfectly disentangle such that if the model is able to classify correctly an elephant on a road and is not able to classify the elephant in a museum, we can rigorously say that the failure is caused by the change in context background. In addition of analysis the robustness with respect to the background scene, it is also possible to analyse with PUG: Animal the robustness with respect to the camera position, asset size and texture (as we demonstrated in Appendix <a href="#S3.SS2.SSS0.Px1" title="Classification with held out sets ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Classification with held out sets</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px1.p1.3" class="ltx_p">In the first experiment, we held out some factors of variation during training (backgrounds, sizes or textures) except for a specific number of animals <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">C</annotation></semantics></math> and use the held out data as validation set. Thus, <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="C=0" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">C</mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><eq id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">C=0</annotation></semantics></math> means that the network never saw the factor during training (this is an OOD scenario with unseen factors) while <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="C=10" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">C</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><eq id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1"></eq><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">C=10</annotation></semantics></math> imply that the network saw this factor for least 10 different animals (OOD scenario with unseen combinations of factors). In Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present our results training a ResNet50 with different held out factors. Every model reached more than 99% accuracy on the training set. First, we trained on 50 backgrounds and used the remaining 14 backgrounds for validation: here, the network reached an accuracy of 80%. However, when using only 30 backgrounds for training and using the remaining 34 as validation, the accuracy drop significantly. Interestingly, showing every background for some of the animals (having unseen combination of factors instead of just unseen factors) decrease performance. In contrast, for texture, we found that having at least 10 animals for which every possible textures are seen during training improves generalization. Interestingly, the network overfits much more to the grass texture relative the default network. Lastly, when looking at the size factor, it seems that training on medium size assets leads to good generalization on small and large assets while training only on small asset leads to worse performance on medium and large assets.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Studying foundation model representational space</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.11" class="ltx_p">PUG: Animal can also be to study the equivariance of foundation models’ representations. For this, we augment each image in PUG: Animals with a caption that describes it according to its factor values (sizes are converted to three adjectives: “small”, “medium” or “big”, see Appendix <a href="#A3.SS1" title="C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a> for details), using the following template<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note that the camera and character orientations are not described, as well as the texture when it is default.</span></span></span>: <em id="S3.SS2.SSS0.Px2.p1.11.1" class="ltx_emph ltx_font_italic">“A photo of a [size] sized [character] textured with [texture] on a [background] background”</em>. Informally, equivariance of a model’s representation with respect to a factor means that when the factor changes from one value to another, the embedding of the corresponding image (or caption) changes in a predictable manner. Equivariance is a sought-after property to improve sample efficiency and robustness to transformations <cite class="ltx_cite ltx_citemacro_citep">(Klee et al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Tai et al., <a href="#bib.bib56" title="" class="ltx_ref">2019</a>; Wang et al., <a href="#bib.bib61" title="" class="ltx_ref">2023</a>)</cite>. Similar to previous works on equivariance and compositionality <cite class="ltx_cite ltx_citemacro_citet">Bouchacourt et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Xie et al. (<a href="#bib.bib66" title="" class="ltx_ref">2022</a>)</cite>, we measure equivariance as the alignment (i.e. parallelism) between embedding differences. First, we feed images and their corresponding captions to 9 pretrained vision-language models including multiple CLIP models <cite class="ltx_cite ltx_citemacro_citet">Radford et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>, NegCLIP <cite class="ltx_cite ltx_citemacro_citet">Yuksekgonul et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite> Flava <cite class="ltx_cite ltx_citemacro_citet">Singh et al. (<a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, BLIP <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib38" title="" class="ltx_ref">2022b</a>)</cite> and X-VLM <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a href="#bib.bib69" title="" class="ltx_ref">2021</a>)</cite> and collect their embeddings of PUG: Animals images and created captions. For each model, we compute difference vectors between the embeddings of two images (or captions) of an object undergoing a factor change: e.g. a big penguin textured with grass on a background “village square” modified to the same penguin but with background “restaurant”, see arrows in Figure <a href="#S3.F4" title="Figure 4 ‣ Studying foundation model representational space ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (left). Specifically, for a sample <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">i</annotation></semantics></math>, undergoing a change from background <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="b_{k}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">b_{k}</annotation></semantics></math> for background <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="b_{l}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">b_{l}</annotation></semantics></math>, we denote the difference vector between the embedding of the image of the sample with backgorund <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="b_{k}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">b_{k}</annotation></semantics></math> and the image of the same sample but with background <math id="S3.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="b_{l}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">b_{l}</annotation></semantics></math> by <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="v^{i}_{b_{k}\rightarrow b_{l}}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><msubsup id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2.cmml">v</mi><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2">𝑣</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.3.3">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">v^{i}_{b_{k}\rightarrow b_{l}}</annotation></semantics></math>. Similarly, we denote by <math id="S3.SS2.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="u^{i}_{b_{k}\rightarrow b_{l}}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><msubsup id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2.cmml">u</mi><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2">𝑢</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.3.3">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">u^{i}_{b_{k}\rightarrow b_{l}}</annotation></semantics></math> the difference vector between the embedding of each of the two captions accompanying the images.
<br class="ltx_break">
<br class="ltx_break">Then, we measure the alignment of difference vectors across pairs <em id="S3.SS2.SSS0.Px2.p1.11.2" class="ltx_emph ltx_font_italic">undergoing the same factor change</em> (here, the penguin and the cat) as their cosine similarity<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Note that foundation model representations belong to the hypersphere, yet measuring equivariance as parallelism relies on Euclidean geometry, we discuss this in Appendix <a href="#A3.SS1" title="C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>. Still, cosine similarity is a starting point to showcase how PUG: Animals can be used to study models’ representations.</span></span></span>. We estimate three types of equivariance: (i) <span id="S3.SS2.SSS0.Px2.p1.11.3" class="ltx_text" style="color:#CC0000;">Image</span> equivariance: how parallel (measured with cosine similarity) are difference vectors across image pairs? (lined and dashed <span id="S3.SS2.SSS0.Px2.p1.11.4" class="ltx_text" style="color:#CC0000;">red</span> arrows) (ii) <span id="S3.SS2.SSS0.Px2.p1.11.5" class="ltx_text" style="color:#009900;">Text</span> equivariance: same but for caption pairs (parallelism of lined and dashed <span id="S3.SS2.SSS0.Px2.p1.11.6" class="ltx_text" style="color:#009900;">green</span> arrows) (iii) Across modalities equivariance: for the same object, alignment of difference vectors between pairs of image-caption (i.e. alignment of the two arrows for the penguin). Specifically, for image equivariance between sample <math id="S3.SS2.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.1a"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.1b"><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.1c">i</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.9.m9.1a"><mi id="S3.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m9.1b"><ci id="S3.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m9.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m9.1c">j</annotation></semantics></math>, for background change <math id="S3.SS2.SSS0.Px2.p1.10.m10.1" class="ltx_Math" alttext="b_{k}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.10.m10.1a"><msub id="S3.SS2.SSS0.Px2.p1.10.m10.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m10.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m10.1c">b_{k}</annotation></semantics></math> to <math id="S3.SS2.SSS0.Px2.p1.11.m11.1" class="ltx_Math" alttext="b_{l}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.11.m11.1a"><msub id="S3.SS2.SSS0.Px2.p1.11.m11.1.1" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.11.m11.1b"><apply id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m11.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.11.m11.1c">b_{l}</annotation></semantics></math>, we compute:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="sim(v^{i}_{b_{k}\rightarrow b_{l}},v^{j}_{b_{k}\rightarrow b_{l}})=\frac{{v^{i}_{b_{k}\rightarrow b_{l}}}^{T}v^{j}_{b_{k}\rightarrow b_{l}}}{||v^{i}_{b_{k}\rightarrow b_{l}}||~{}||v^{j}_{b_{k}\rightarrow b_{l}}||}" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mi id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml">​</mo><mi id="S3.E1.m1.4.4.2.5" xref="S3.E1.m1.4.4.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.3a" xref="S3.E1.m1.4.4.2.3.cmml">​</mo><mi id="S3.E1.m1.4.4.2.6" xref="S3.E1.m1.4.4.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.3b" xref="S3.E1.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E1.m1.4.4.2.2.2" xref="S3.E1.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.2.2.2.3" xref="S3.E1.m1.4.4.2.2.3.cmml">(</mo><msubsup id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.cmml">v</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml">→</mo><msub id="S3.E1.m1.3.3.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.3.2.cmml">b</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.3.3.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.E1.m1.4.4.2.2.2.4" xref="S3.E1.m1.4.4.2.2.3.cmml">,</mo><msubsup id="S3.E1.m1.4.4.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.cmml"><mi id="S3.E1.m1.4.4.2.2.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.2.2.cmml">v</mi><mrow id="S3.E1.m1.4.4.2.2.2.2.3" xref="S3.E1.m1.4.4.2.2.2.2.3.cmml"><msub id="S3.E1.m1.4.4.2.2.2.2.3.2" xref="S3.E1.m1.4.4.2.2.2.2.3.2.cmml"><mi id="S3.E1.m1.4.4.2.2.2.2.3.2.2" xref="S3.E1.m1.4.4.2.2.2.2.3.2.2.cmml">b</mi><mi id="S3.E1.m1.4.4.2.2.2.2.3.2.3" xref="S3.E1.m1.4.4.2.2.2.2.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.4.4.2.2.2.2.3.1" xref="S3.E1.m1.4.4.2.2.2.2.3.1.cmml">→</mo><msub id="S3.E1.m1.4.4.2.2.2.2.3.3" xref="S3.E1.m1.4.4.2.2.2.2.3.3.cmml"><mi id="S3.E1.m1.4.4.2.2.2.2.3.3.2" xref="S3.E1.m1.4.4.2.2.2.2.3.3.2.cmml">b</mi><mi id="S3.E1.m1.4.4.2.2.2.2.3.3.3" xref="S3.E1.m1.4.4.2.2.2.2.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.4.4.2.2.2.2.2.3" xref="S3.E1.m1.4.4.2.2.2.2.2.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.E1.m1.4.4.2.2.2.5" xref="S3.E1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml">=</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><mmultiscripts id="S3.E1.m1.2.2.4.2" xref="S3.E1.m1.2.2.4.2.cmml"><mi id="S3.E1.m1.2.2.4.2.2.2.2" xref="S3.E1.m1.2.2.4.2.2.2.2.cmml">v</mi><mrow id="S3.E1.m1.2.2.4.2.2.3" xref="S3.E1.m1.2.2.4.2.2.3.cmml"><msub id="S3.E1.m1.2.2.4.2.2.3.2" xref="S3.E1.m1.2.2.4.2.2.3.2.cmml"><mi id="S3.E1.m1.2.2.4.2.2.3.2.2" xref="S3.E1.m1.2.2.4.2.2.3.2.2.cmml">b</mi><mi id="S3.E1.m1.2.2.4.2.2.3.2.3" xref="S3.E1.m1.2.2.4.2.2.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.4.2.2.3.1" xref="S3.E1.m1.2.2.4.2.2.3.1.cmml">→</mo><msub id="S3.E1.m1.2.2.4.2.2.3.3" xref="S3.E1.m1.2.2.4.2.2.3.3.cmml"><mi id="S3.E1.m1.2.2.4.2.2.3.3.2" xref="S3.E1.m1.2.2.4.2.2.3.3.2.cmml">b</mi><mi id="S3.E1.m1.2.2.4.2.2.3.3.3" xref="S3.E1.m1.2.2.4.2.2.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.2.2.4.2.2.2.3" xref="S3.E1.m1.2.2.4.2.2.2.3.cmml">i</mi><mrow id="S3.E1.m1.2.2.4.2a" xref="S3.E1.m1.2.2.4.2.cmml"></mrow><mi id="S3.E1.m1.2.2.4.2.3" xref="S3.E1.m1.2.2.4.2.3.cmml">T</mi></mmultiscripts><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.4.1" xref="S3.E1.m1.2.2.4.1.cmml">​</mo><msubsup id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.4.3.2.2" xref="S3.E1.m1.2.2.4.3.2.2.cmml">v</mi><mrow id="S3.E1.m1.2.2.4.3.3" xref="S3.E1.m1.2.2.4.3.3.cmml"><msub id="S3.E1.m1.2.2.4.3.3.2" xref="S3.E1.m1.2.2.4.3.3.2.cmml"><mi id="S3.E1.m1.2.2.4.3.3.2.2" xref="S3.E1.m1.2.2.4.3.3.2.2.cmml">b</mi><mi id="S3.E1.m1.2.2.4.3.3.2.3" xref="S3.E1.m1.2.2.4.3.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.4.3.3.1" xref="S3.E1.m1.2.2.4.3.3.1.cmml">→</mo><msub id="S3.E1.m1.2.2.4.3.3.3" xref="S3.E1.m1.2.2.4.3.3.3.cmml"><mi id="S3.E1.m1.2.2.4.3.3.3.2" xref="S3.E1.m1.2.2.4.3.3.3.2.cmml">b</mi><mi id="S3.E1.m1.2.2.4.3.3.3.3" xref="S3.E1.m1.2.2.4.3.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.2.2.4.3.2.3" xref="S3.E1.m1.2.2.4.3.2.3.cmml">j</mi></msubsup></mrow><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.cmml">‖</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">v</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">→</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml">b</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.1.cmml">‖</mo><msubsup id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2.2" xref="S3.E1.m1.2.2.2.2.1.1.2.2.cmml">v</mi><mrow id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml"><msub id="S3.E1.m1.2.2.2.2.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.3.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.2.2" xref="S3.E1.m1.2.2.2.2.1.1.3.2.2.cmml">b</mi><mi id="S3.E1.m1.2.2.2.2.1.1.3.2.3" xref="S3.E1.m1.2.2.2.2.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.3.1" xref="S3.E1.m1.2.2.2.2.1.1.3.1.cmml">→</mo><msub id="S3.E1.m1.2.2.2.2.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.2.2.1.1.3.3.2.cmml">b</mi><mi id="S3.E1.m1.2.2.2.2.1.1.3.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E1.m1.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"></eq><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"></times><ci id="S3.E1.m1.4.4.2.4.cmml" xref="S3.E1.m1.4.4.2.4">𝑠</ci><ci id="S3.E1.m1.4.4.2.5.cmml" xref="S3.E1.m1.4.4.2.5">𝑖</ci><ci id="S3.E1.m1.4.4.2.6.cmml" xref="S3.E1.m1.4.4.2.6">𝑚</ci><interval closure="open" id="S3.E1.m1.4.4.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2">𝑣</ci><ci id="S3.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3"><ci id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1">→</ci><apply id="S3.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3.2">𝑏</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.3.3">𝑙</ci></apply></apply></apply><apply id="S3.E1.m1.4.4.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.4.4.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2">superscript</csymbol><ci id="S3.E1.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.2.2">𝑣</ci><ci id="S3.E1.m1.4.4.2.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.2.3">𝑗</ci></apply><apply id="S3.E1.m1.4.4.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3"><ci id="S3.E1.m1.4.4.2.2.2.2.3.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.1">→</ci><apply id="S3.E1.m1.4.4.2.2.2.2.3.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.3.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E1.m1.4.4.2.2.2.2.3.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.2.2">𝑏</ci><ci id="S3.E1.m1.4.4.2.2.2.2.3.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.4.4.2.2.2.2.3.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.3.3.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.3">subscript</csymbol><ci id="S3.E1.m1.4.4.2.2.2.2.3.3.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.3.2">𝑏</ci><ci id="S3.E1.m1.4.4.2.2.2.2.3.3.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3.3.3">𝑙</ci></apply></apply></apply></interval></apply><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><times id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4.1"></times><apply id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.1.cmml" xref="S3.E1.m1.2.2.4.2">superscript</csymbol><apply id="S3.E1.m1.2.2.4.2.2.cmml" xref="S3.E1.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.2.1.cmml" xref="S3.E1.m1.2.2.4.2">subscript</csymbol><apply id="S3.E1.m1.2.2.4.2.2.2.cmml" xref="S3.E1.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.2.2.1.cmml" xref="S3.E1.m1.2.2.4.2">superscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.2.2.cmml" xref="S3.E1.m1.2.2.4.2.2.2.2">𝑣</ci><ci id="S3.E1.m1.2.2.4.2.2.2.3.cmml" xref="S3.E1.m1.2.2.4.2.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.4.2.2.3.cmml" xref="S3.E1.m1.2.2.4.2.2.3"><ci id="S3.E1.m1.2.2.4.2.2.3.1.cmml" xref="S3.E1.m1.2.2.4.2.2.3.1">→</ci><apply id="S3.E1.m1.2.2.4.2.2.3.2.cmml" xref="S3.E1.m1.2.2.4.2.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.4.2.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.4.2.2.3.2.2">𝑏</ci><ci id="S3.E1.m1.2.2.4.2.2.3.2.3.cmml" xref="S3.E1.m1.2.2.4.2.2.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.2.2.4.2.2.3.3.cmml" xref="S3.E1.m1.2.2.4.2.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.2.3.3.1.cmml" xref="S3.E1.m1.2.2.4.2.2.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.3.3.2.cmml" xref="S3.E1.m1.2.2.4.2.2.3.3.2">𝑏</ci><ci id="S3.E1.m1.2.2.4.2.2.3.3.3.cmml" xref="S3.E1.m1.2.2.4.2.2.3.3.3">𝑙</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.4.2.3.cmml" xref="S3.E1.m1.2.2.4.2.3">𝑇</ci></apply><apply id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.4.3">subscript</csymbol><apply id="S3.E1.m1.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.2.1.cmml" xref="S3.E1.m1.2.2.4.3">superscript</csymbol><ci id="S3.E1.m1.2.2.4.3.2.2.cmml" xref="S3.E1.m1.2.2.4.3.2.2">𝑣</ci><ci id="S3.E1.m1.2.2.4.3.2.3.cmml" xref="S3.E1.m1.2.2.4.3.2.3">𝑗</ci></apply><apply id="S3.E1.m1.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.4.3.3"><ci id="S3.E1.m1.2.2.4.3.3.1.cmml" xref="S3.E1.m1.2.2.4.3.3.1">→</ci><apply id="S3.E1.m1.2.2.4.3.3.2.cmml" xref="S3.E1.m1.2.2.4.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.3.2.1.cmml" xref="S3.E1.m1.2.2.4.3.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.4.3.3.2.2.cmml" xref="S3.E1.m1.2.2.4.3.3.2.2">𝑏</ci><ci id="S3.E1.m1.2.2.4.3.3.2.3.cmml" xref="S3.E1.m1.2.2.4.3.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.2.2.4.3.3.3.cmml" xref="S3.E1.m1.2.2.4.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.3.3.1.cmml" xref="S3.E1.m1.2.2.4.3.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.4.3.3.3.2.cmml" xref="S3.E1.m1.2.2.4.3.3.3.2">𝑏</ci><ci id="S3.E1.m1.2.2.4.3.3.3.3.cmml" xref="S3.E1.m1.2.2.4.3.3.3.3">𝑙</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1">→</ci><apply id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.2">𝑏</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3">𝑙</ci></apply></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.2">norm</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">subscript</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2">𝑣</ci><ci id="S3.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.3">𝑗</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><ci id="S3.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.1">→</ci><apply id="S3.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2.2">𝑏</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.2">𝑏</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.3">𝑙</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">sim(v^{i}_{b_{k}\rightarrow b_{l}},v^{j}_{b_{k}\rightarrow b_{l}})=\frac{{v^{i}_{b_{k}\rightarrow b_{l}}}^{T}v^{j}_{b_{k}\rightarrow b_{l}}}{||v^{i}_{b_{k}\rightarrow b_{l}}||~{}||v^{j}_{b_{k}\rightarrow b_{l}}||}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px2.p1.13" class="ltx_p">For text equivariance, we compute be <math id="S3.SS2.SSS0.Px2.p1.12.m1.2" class="ltx_Math" alttext="sim(u^{i}_{b_{k}\rightarrow b_{l}},u^{j}_{b_{k}\rightarrow b_{l}})" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.12.m1.2a"><mrow id="S3.SS2.SSS0.Px2.p1.12.m1.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.4" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.5" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3a" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.6" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3b" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3.cmml">​</mo><mrow id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.3.cmml">(</mo><msubsup id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.2.cmml">u</mi><mrow id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.2.cmml">u</mi><mrow id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.1" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.2" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.5" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.12.m1.2b"><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2"><times id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.3"></times><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.4.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.4">𝑠</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.5.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.5">𝑖</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.6.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.6">𝑚</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2"><apply id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.2">𝑢</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.1.1.1.1.1.3.3.3">𝑙</ci></apply></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.2">𝑢</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.2.3">𝑗</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3"><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m1.2.2.2.2.2.3.3.3">𝑙</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.12.m1.2c">sim(u^{i}_{b_{k}\rightarrow b_{l}},u^{j}_{b_{k}\rightarrow b_{l}})</annotation></semantics></math> while for across equivariance, we compute <math id="S3.SS2.SSS0.Px2.p1.13.m2.2" class="ltx_Math" alttext="sim(v^{i}_{b_{k}\rightarrow b_{l}},u^{i}_{b_{k}\rightarrow b_{l}})" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.13.m2.2a"><mrow id="S3.SS2.SSS0.Px2.p1.13.m2.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.5" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3a" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.6" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3b" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3.cmml">​</mo><mrow id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.3.cmml">(</mo><msubsup id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.2.cmml">v</mi><mrow id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.4" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.2.cmml">u</mi><mrow id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.1" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.1.cmml">→</mo><msub id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.2" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.2.cmml">b</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.5" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.13.m2.2b"><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2"><times id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.3"></times><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.4.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.4">𝑠</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.5.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.5">𝑖</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.6.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.6">𝑚</ci><interval closure="open" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2"><apply id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.2">𝑣</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.1.1.1.1.1.3.3.3">𝑙</ci></apply></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.2">𝑢</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3"><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.1">→</ci><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.2">𝑏</ci><ci id="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.13.m2.2.2.2.2.2.3.3.3">𝑙</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.13.m2.2c">sim(v^{i}_{b_{k}\rightarrow b_{l}},u^{i}_{b_{k}\rightarrow b_{l}})</annotation></semantics></math>. We report cosine similarity averaged over pairs and possible changes for each factor (higher value means higher equivariance, 1 is the maximum). Specifically, the image equivariance for background writes as</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\frac{1}{B(B-1)}\sum_{b_{k}}\sum_{b_{l}}\frac{1}{N(N-1)}\sum_{i}\sum_{j}sim(v^{i}_{b_{k}\rightarrow b_{l}},v^{j}_{b_{k}\rightarrow b_{l}})" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mfrac id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mn id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">1</mn><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">B</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><munder id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.4.4.2.3.2" xref="S3.E2.m1.4.4.2.3.2.cmml">∑</mo><msub id="S3.E2.m1.4.4.2.3.3" xref="S3.E2.m1.4.4.2.3.3.cmml"><mi id="S3.E2.m1.4.4.2.3.3.2" xref="S3.E2.m1.4.4.2.3.3.2.cmml">b</mi><mi id="S3.E2.m1.4.4.2.3.3.3" xref="S3.E2.m1.4.4.2.3.3.3.cmml">k</mi></msub></munder><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml"><munder id="S3.E2.m1.4.4.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml"><mo movablelimits="false" id="S3.E2.m1.4.4.2.2.3.2" xref="S3.E2.m1.4.4.2.2.3.2.cmml">∑</mo><msub id="S3.E2.m1.4.4.2.2.3.3" xref="S3.E2.m1.4.4.2.2.3.3.cmml"><mi id="S3.E2.m1.4.4.2.2.3.3.2" xref="S3.E2.m1.4.4.2.2.3.3.2.cmml">b</mi><mi id="S3.E2.m1.4.4.2.2.3.3.3" xref="S3.E2.m1.4.4.2.2.3.3.3.cmml">l</mi></msub></munder><mrow id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.2.2.cmml"><mfrac id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mn id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">1</mn><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">N</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml">−</mo><mn id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml"><munder id="S3.E2.m1.4.4.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.4.4.2.2.2.2.3.2" xref="S3.E2.m1.4.4.2.2.2.2.3.2.cmml">∑</mo><mi id="S3.E2.m1.4.4.2.2.2.2.3.3" xref="S3.E2.m1.4.4.2.2.2.2.3.3.cmml">i</mi></munder><mrow id="S3.E2.m1.4.4.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.cmml"><munder id="S3.E2.m1.4.4.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.3.cmml"><mo movablelimits="false" id="S3.E2.m1.4.4.2.2.2.2.2.3.2" xref="S3.E2.m1.4.4.2.2.2.2.2.3.2.cmml">∑</mo><mi id="S3.E2.m1.4.4.2.2.2.2.2.3.3" xref="S3.E2.m1.4.4.2.2.2.2.2.3.3.cmml">j</mi></munder><mrow id="S3.E2.m1.4.4.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.4" xref="S3.E2.m1.4.4.2.2.2.2.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml">​</mo><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.5" xref="S3.E2.m1.4.4.2.2.2.2.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.2.2.2.2.3a" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml">​</mo><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.6" xref="S3.E2.m1.4.4.2.2.2.2.2.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.2.2.2.2.3b" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml">→</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.cmml">b</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.4" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.2.cmml">v</mi><mrow id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.cmml"><msub id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.2.cmml">b</mi><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.1" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.1.cmml">→</mo><msub id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.2.cmml">b</mi><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.3.cmml">l</mi></msub></mrow><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.3.cmml">j</mi></msubsup><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.5" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><times id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"></times><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><divide id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1"></divide><cn type="integer" id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">1</cn><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝐵</ci><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">𝐵</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><apply id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.3.1.cmml" xref="S3.E2.m1.4.4.2.3">subscript</csymbol><sum id="S3.E2.m1.4.4.2.3.2.cmml" xref="S3.E2.m1.4.4.2.3.2"></sum><apply id="S3.E2.m1.4.4.2.3.3.cmml" xref="S3.E2.m1.4.4.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.3.3.1.cmml" xref="S3.E2.m1.4.4.2.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.2.3.3.2.cmml" xref="S3.E2.m1.4.4.2.3.3.2">𝑏</ci><ci id="S3.E2.m1.4.4.2.3.3.3.cmml" xref="S3.E2.m1.4.4.2.3.3.3">𝑘</ci></apply></apply><apply id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2"><apply id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3">subscript</csymbol><sum id="S3.E2.m1.4.4.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.3.2"></sum><apply id="S3.E2.m1.4.4.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.3.3.1.cmml" xref="S3.E2.m1.4.4.2.2.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.3.3.2.cmml" xref="S3.E2.m1.4.4.2.2.3.3.2">𝑏</ci><ci id="S3.E2.m1.4.4.2.2.3.3.3.cmml" xref="S3.E2.m1.4.4.2.2.3.3.3">𝑙</ci></apply></apply><apply id="S3.E2.m1.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2"><times id="S3.E2.m1.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.3"></times><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><divide id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2"></divide><cn type="integer" id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3">1</cn><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">𝑁</ci><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"></minus><ci id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2">𝑁</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2"><apply id="S3.E2.m1.4.4.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.3">subscript</csymbol><sum id="S3.E2.m1.4.4.2.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.3.2"></sum><ci id="S3.E2.m1.4.4.2.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.3.3">𝑖</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2"><apply id="S3.E2.m1.4.4.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.3">subscript</csymbol><sum id="S3.E2.m1.4.4.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.3.2"></sum><ci id="S3.E2.m1.4.4.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.3.3">𝑗</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2"><times id="S3.E2.m1.4.4.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.3"></times><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.4.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.4">𝑠</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.5.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.5">𝑖</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.6.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.6">𝑚</ci><interval closure="open" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.1">→</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2">𝑏</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3">𝑙</ci></apply></apply></apply><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.2">𝑣</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.2.3">𝑗</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3"><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.1">→</ci><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.2">𝑏</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.2.3">𝑘</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.2">𝑏</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.2.2.3.3.3">𝑙</ci></apply></apply></apply></interval></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\frac{1}{B(B-1)}\sum_{b_{k}}\sum_{b_{l}}\frac{1}{N(N-1)}\sum_{i}\sum_{j}sim(v^{i}_{b_{k}\rightarrow b_{l}},v^{j}_{b_{k}\rightarrow b_{l}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px2.p1.15" class="ltx_p">where <math id="S3.SS2.SSS0.Px2.p1.14.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.14.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.14.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.14.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.14.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.14.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.14.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.14.m1.1c">B</annotation></semantics></math> is the number of possible backgrounds and <math id="S3.SS2.SSS0.Px2.p1.15.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.15.m2.1a"><mi id="S3.SS2.SSS0.Px2.p1.15.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.15.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.15.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.15.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.15.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.15.m2.1c">N</annotation></semantics></math> is the number of samples.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2308.03977/assets/x6.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="62" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.4.1" class="ltx_text ltx_font_bold">Measuring foundation models equivariance thanks to PUG: Animals.</span> <span id="S3.F4.5.2" class="ltx_text ltx_font_bold">Left:</span> Illustration of how to use PUG: Animals to compute equivariance. <span id="S3.F4.6.3" class="ltx_text ltx_font_bold">Right:</span> Image and text equivariance is present with respect to background, while across modalities equivariance to background doesn’t hold as much. See main text for detailed results.</figcaption>
</figure>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p2.9" class="ltx_p">We show in Figure <a href="#S3.F4" title="Figure 4 ‣ Studying foundation model representational space ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (right) results for equivariance with respect to the background. Plots for equivariance to texture and size are in Figure <a href="#A3.F10" title="Figure 10 ‣ C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Looking at Figure <a href="#S3.F4" title="Figure 4 ‣ Studying foundation model representational space ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> results (right side, top row), we see that the foundation models’ image embeddings present high equivariance to background (<math id="S3.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="0.78\pm 0.04" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">0.78</mn><mo id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2">0.78</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">0.78\pm 0.04</annotation></semantics></math> on average over models). There is also (see Figure <a href="#A3.F10.sf1" title="In Figure 10 ‣ C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>) small image equivariance to texture (<math id="S3.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="0.15\pm 0.04" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">0.15</mn><mo id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2">0.15</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.2.m2.1c">0.15\pm 0.04</annotation></semantics></math>), but almost no equivariance to size (<math id="S3.SS2.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="0.06\pm 0.02" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.cmml">0.06</mn><mo id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2">0.06</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.3.m3.1c">0.06\pm 0.02</annotation></semantics></math>). Text equivariance is high with respect to background (average of <math id="S3.SS2.SSS0.Px2.p2.4.m4.1" class="ltx_Math" alttext="0.87\pm 0.03" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.4.m4.1a"><mrow id="S3.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2.cmml">0.87</mn><mo id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.1" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2">0.87</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.4.m4.1c">0.87\pm 0.03</annotation></semantics></math>), but is also strong for size and texture (<math id="S3.SS2.SSS0.Px2.p2.5.m5.1" class="ltx_Math" alttext="0.71\pm 0.11" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p2.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2.cmml">0.71</mn><mo id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.1" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3.cmml">0.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2">0.71</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3">0.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.5.m5.1c">0.71\pm 0.11</annotation></semantics></math> for size and <math id="S3.SS2.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="0.81\pm 0.03" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.6.m6.1a"><mrow id="S3.SS2.SSS0.Px2.p2.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2.cmml">0.81</mn><mo id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.1" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2">0.81</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.6.m6.1c">0.81\pm 0.03</annotation></semantics></math> for texture, see Figure <a href="#A3.F10.sf2" title="In Figure 10 ‣ C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>) suggesting that foundation models’ caption embeddings can be manipulated with vector arithmetic, similar to word vectors behaviours <cite class="ltx_cite ltx_citemacro_citet">Ethayarajh et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. This aligns with the recent work of <cite class="ltx_cite ltx_citemacro_citep">(Trager et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite> that show linear behavior of VLMs text embedding spaces. Across modalities, small equivariance is present with respect to background (<math id="S3.SS2.SSS0.Px2.p2.7.m7.1" class="ltx_Math" alttext="0.22\pm 0.03" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.7.m7.1a"><mrow id="S3.SS2.SSS0.Px2.p2.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.2" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.2.cmml">0.22</mn><mo id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.1" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.3" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.7.m7.1b"><apply id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.2">0.22</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.7.m7.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.7.m7.1c">0.22\pm 0.03</annotation></semantics></math> and Figure <a href="#S3.F4" title="Figure 4 ‣ Studying foundation model representational space ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> right side, bottom row). However when size or texture change for a given object, its image and caption representations seem to move in non-aligned directions (<math id="S3.SS2.SSS0.Px2.p2.8.m8.1" class="ltx_Math" alttext="0.07\pm 0.01" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.8.m8.1a"><mrow id="S3.SS2.SSS0.Px2.p2.8.m8.1.1" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.2" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.2.cmml">0.07</mn><mo id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.1" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.3" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.8.m8.1b"><apply id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.2">0.07</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.8.m8.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.8.m8.1c">0.07\pm 0.01</annotation></semantics></math> for texture and <math id="S3.SS2.SSS0.Px2.p2.9.m9.1" class="ltx_Math" alttext="0.04\pm 0.01" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.9.m9.1a"><mrow id="S3.SS2.SSS0.Px2.p2.9.m9.1.1" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.2" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.2.cmml">0.04</mn><mo id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.1" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.1.cmml">±</mo><mn id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.3" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.9.m9.1b"><apply id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.1">plus-or-minus</csymbol><cn type="float" id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.2">0.04</cn><cn type="float" id="S3.SS2.SSS0.Px2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.9.m9.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.9.m9.1c">0.04\pm 0.01</annotation></semantics></math> for size, see Figure <a href="#A3.F10.sf3" title="In Figure 10 ‣ C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(c)</span></a>). While more syntactically complex captions and other equivariance metrics could be designed, our aim here is to provide an initial study to showcase how PUG: Animals can be easily used to study state-of-the-art models representations.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>PUG: ImageNet</h3>

<figure id="S3.F5" class="ltx_figure"><img src="/html/2308.03977/assets/x7.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="90" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>We present <em id="S3.F5.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">PUG: ImageNet</em>, a new photorealistic synthetic dataset with annotated factors of variations as an additional test set for ImageNet pretrained models.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">As a second member of the PUG family, we introduce <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">PUG: ImageNet</em> (Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3 PUG: ImageNet ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), which contains 88,328 pre-rendered images using 724 assets representing 151 ImageNet classes with 64 backgrounds, 7 sizes, 9 textures, 18 different camera orientation, 18 different character orientation and 7 light intensity. In contrast to PUG: Animals, PUG: ImageNet was created by varying only a single factor at a time (which explain the lower number of images than PUG: Animals despite using more factors). The main purpose of this dataset is to provide a novel useful benchmark, paralleling ImageNet, but for <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">fine-grained evaluation of the robustness</em> of image classifiers, along several factors of variation.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">An extensive evaluation of the robustness of SOTA models</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">Our PUG: ImageNet dataset offers both photo-realism and precise control over how each object is depicted from pose and size to environment and camera-angle. We also provide a collection of objects with mappings to classes in the popular ImageNet dataset, enabling researchers to probe the robustness of SoTA vision models without retraining. We assess a variety of model architectures across several pretraining datasets including ImageNet-1/-21k, LAION (400M and 2B), and JFT300M <cite class="ltx_cite ltx_citemacro_citep">(Kolesnikov et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>; Liu et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>; Dosovitskiy et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. We observe in Table <a href="#S3.T1" title="Table 1 ‣ An extensive evaluation of the robustness of SOTA models ‣ 3.3 PUG: ImageNet ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that the models that perform the best on the ImageNet validation accuracy are not always the ones which offer the best robustness on PUG: ImageNet. For example, the pretrained ViT-B32 trained on ImageNet-21k is better on the ImageNet validation set compared to a Swin-B, but offers worse robustness across all factors. We confirm no statistically significant relationship exists between ImageNet accuracy and robustness by computing Pearson’s correlation coefficients (Appendix <a href="#A3.SS3" title="C.3 Robustness of SOTA models additional details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.3</span></a>). This result showcases how PUG: ImageNet can be added as an additional benchmark to evaluate vision models.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:134.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-162.6pt,50.1pt) scale(0.57148,0.57148) ;">
<p id="S3.T1.1.1" class="ltx_p"><span id="S3.T1.1.1.1" class="ltx_text">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:758.8pt;height:235pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T1.1.1.1.1.1" class="ltx_p"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T1.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T1.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PUG: ImageNet Top-1 Accuracy across Factors of Variation</span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span></span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">ImageNet Val.</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Camera (Yaw,Pitch,Roll)</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Pose (Yaw,Pitch,Roll)</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Size</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Texture</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Light</span>
<span id="S3.T1.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Background</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T1.1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">81.5</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">(38.1, 33.1, 26.9)</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">(38.0, 23.6, 22.9)</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">35.7</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">27.0</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">13.6</span>
<span id="S3.T1.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">29.5</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ResNet101</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">82.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">(43.4, 35.9, 29.4)</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">(45.1, 26.7, 25.6)</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">39.7</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">31.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_center">14.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_center">32.8</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViTLarge</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">85.8</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center">(52.2, 40.4, 37.1)</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center">(52.4, 30.4, 28.4)</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center">46.4</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center">42.9</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_center">8.9</span>
<span id="S3.T1.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_center">34.6</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViTBasePretrained21k</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">84.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center">(37.5, 34.3, 31.7)</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center">(38.0, 21.8, 20.5)</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_center">33.0</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center">28.5</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_align_center">4.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.6.4.8" class="ltx_td ltx_align_center">26.6</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Swin</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">83.6</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center">(56.0, 45.6, 41.8)</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center">(56.9, 35.3, 34.2)</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center">52.9</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center">40.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_align_center">19.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.7.5.8" class="ltx_td ltx_align_center">42.0</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BiT (JFT300M)</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">80.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_align_center">(40.5, 32.3, 26.0)</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.4" class="ltx_td ltx_align_center">(42.1, 23.6, 22.8)</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.5" class="ltx_td ltx_align_center">37.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.6" class="ltx_td ltx_align_center">23.4</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.7" class="ltx_td ltx_align_center">6.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.8.6.8" class="ltx_td ltx_align_center">20.5</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DINOv2 (LVD-142M)</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">84.5</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.3" class="ltx_td ltx_align_center">(45.6, 41.1, 37.4)</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.4" class="ltx_td ltx_align_center">(47.5, 28.8, 28.5)</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.5" class="ltx_td ltx_align_center">43.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.6" class="ltx_td ltx_align_center">35.0</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.7" class="ltx_td ltx_align_center">6.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.9.7.8" class="ltx_td ltx_align_center">30.9</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Flava (PMD 70M)</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.5</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.3" class="ltx_td ltx_align_center">(31.7, 23.4, 17.6)</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.4" class="ltx_td ltx_align_center">(30.8, 17.6, 15.4)</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.5" class="ltx_td ltx_align_center">30.5</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.6" class="ltx_td ltx_align_center">24.2</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.7" class="ltx_td ltx_align_center">7.8</span>
<span id="S3.T1.1.1.1.1.1.1.1.10.8.8" class="ltx_td ltx_align_center">21.9</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIPViTB32 (400M)</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">62.9</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.3" class="ltx_td ltx_align_center">(41.7, 30.2, 22.1)</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.4" class="ltx_td ltx_align_center">(41.6, 23.8, 20.9)</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.5" class="ltx_td ltx_align_center">40.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.6" class="ltx_td ltx_align_center">34.4</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.7" class="ltx_td ltx_align_center">5.7</span>
<span id="S3.T1.1.1.1.1.1.1.1.11.9.8" class="ltx_td ltx_align_center">24.4</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIPViTB32 (2B)</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">66.6</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.3" class="ltx_td ltx_align_center">(44.0, 31.5, 24.1)</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.4" class="ltx_td ltx_align_center">(43.8, 24.8, 21.8)</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.5" class="ltx_td ltx_align_center">42.2</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.6" class="ltx_td ltx_align_center">34.7</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.7" class="ltx_td ltx_align_center">3.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.12.10.8" class="ltx_td ltx_align_center">26.0</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">CLIPViTL14 (400M)</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">72.8</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.3" class="ltx_td ltx_align_center ltx_border_bb">(52.3, 39.8, 35.7)</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.4" class="ltx_td ltx_align_center ltx_border_bb">(51.8, 29.0, 26.4)</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.5" class="ltx_td ltx_align_center ltx_border_bb">50.6</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.6" class="ltx_td ltx_align_center ltx_border_bb">41.1</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.7" class="ltx_td ltx_align_center ltx_border_bb">4.3</span>
<span id="S3.T1.1.1.1.1.1.1.1.13.11.8" class="ltx_td ltx_align_center ltx_border_bb">33.0</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Robustness measured by average top-1 accuracy across factors on PUG: ImageNet (We show on the second column the traditional ImageNet validation set accuracy for comparison). Pretraining dataset sizes are indicated in parenthesis with the default being ImageNet-1k. CLIP uses ViT-B32 or ViT-L14. Camera orientation and object pose indicate accuracy along (yaw, pitch, roll) axes.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>PUG: SPAR for VLMs</h3>

<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2308.03977/assets/x8.png" id="S3.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="263" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F6.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F6.1.1.1" class="ltx_tr">
<td id="S3.F6.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Caption</span>
</span>
</td>
<td id="S3.F6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.1.1.2.1.1" class="ltx_p" style="width:19.9pt;">Texture</span>
</span>
</td>
<td id="S3.F6.1.1.1.3" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.3.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.3.1.1" class="ltx_text ltx_inline-block" style="width:104.3pt;">
<span id="S3.F6.1.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:114.3pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.3.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.3.1.1.1.1.1" class="ltx_text">ViT-B-32 (OpenAI CLIP)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.4" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.4.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.4.1.1" class="ltx_text ltx_inline-block" style="width:99.7pt;">
<span id="S3.F6.1.1.1.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:109.7pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.4.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.4.1.1.1.1.1" class="ltx_text">ViT-B-32 (OpenClip 2B)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.5" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.5.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.5.1.1" class="ltx_text ltx_inline-block" style="width:103.5pt;">
<span id="S3.F6.1.1.1.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:113.5pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.5.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.5.1.1.1.1.1" class="ltx_text">ViT-L-14 (OpenAI CLIP)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.6" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.6.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.6.1.1" class="ltx_text ltx_inline-block" style="width:98.9pt;">
<span id="S3.F6.1.1.1.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:108.9pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.6.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.6.1.1.1.1.1" class="ltx_text">ViT-L-14 (OpenClip 2B)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.7" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.7.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.7.1.1" class="ltx_text ltx_inline-block" style="width:105.7pt;">
<span id="S3.F6.1.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:115.7pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.7.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.7.1.1.1.1.1" class="ltx_text">ViT-H-14 (OpenCLIP 2B)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.8" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.8.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.8.1.1" class="ltx_text ltx_inline-block" style="width:106.0pt;">
<span id="S3.F6.1.1.1.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:116.0pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.8.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.8.1.1.1.1.1" class="ltx_text">ViT-G-14 (OpenCLIP 2B)</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.9" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.9.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.9.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="S3.F6.1.1.1.9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.9.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.9.1.1.1.1.1" class="ltx_text">Flava</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.10" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.10.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.10.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="S3.F6.1.1.1.10.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.10.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.10.1.1.1.1.1" class="ltx_text">BLIP</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.11" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.11.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.11.1.1" class="ltx_text ltx_inline-block" style="width:20.4pt;">
<span id="S3.F6.1.1.1.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:30.4pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.11.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.11.1.1.1.1.1" class="ltx_text">XVLM</span></span>
</span></span></span></span></td>
<td id="S3.F6.1.1.1.12" class="ltx_td ltx_align_left"><span id="S3.F6.1.1.1.12.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.F6.1.1.1.12.1.1" class="ltx_text ltx_inline-block" style="width:30.8pt;">
<span id="S3.F6.1.1.1.12.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:40.8pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F6.1.1.1.12.1.1.1.1" class="ltx_p"><span id="S3.F6.1.1.1.12.1.1.1.1.1" class="ltx_text">NegCLIP</span></span>
</span></span></span></span></td>
</tr>
<tr id="S3.F6.1.2.2" class="ltx_tr">
<td id="S3.F6.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.2.2.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="S3.F6.1.2.2.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo in a [background] environment“</em></span>
</span>
</td>
<td id="S3.F6.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.2.2.2.1.1" class="ltx_p" style="width:19.9pt;">N/A</span>
</span>
</td>
<td id="S3.F6.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
<td id="S3.F6.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
<td id="S3.F6.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
<td id="S3.F6.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t">90.00</td>
<td id="S3.F6.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
<td id="S3.F6.1.2.2.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.2.2.8.1" class="ltx_text ltx_font_bold">100.00</span></td>
<td id="S3.F6.1.2.2.9" class="ltx_td ltx_align_left ltx_border_t">60.00</td>
<td id="S3.F6.1.2.2.10" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
<td id="S3.F6.1.2.2.11" class="ltx_td ltx_align_left ltx_border_t">90.00</td>
<td id="S3.F6.1.2.2.12" class="ltx_td ltx_align_left ltx_border_t">100.00</td>
</tr>
<tr id="S3.F6.1.3.3" class="ltx_tr">
<td id="S3.F6.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S3.F6.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.3.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.F6.1.3.3.1.1.1.1" class="ltx_text"><em id="S3.F6.1.3.3.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.F6.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.3.3.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="S3.F6.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t">39.61</td>
<td id="S3.F6.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">41.80</td>
<td id="S3.F6.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">78.44</td>
<td id="S3.F6.1.3.3.6" class="ltx_td ltx_align_left ltx_border_t">60.00</td>
<td id="S3.F6.1.3.3.7" class="ltx_td ltx_align_left ltx_border_t">70.23</td>
<td id="S3.F6.1.3.3.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.3.3.8.1" class="ltx_text ltx_font_bold">78.36</span></td>
<td id="S3.F6.1.3.3.9" class="ltx_td ltx_align_left ltx_border_t">31.41</td>
<td id="S3.F6.1.3.3.10" class="ltx_td ltx_align_left ltx_border_t">52.73</td>
<td id="S3.F6.1.3.3.11" class="ltx_td ltx_align_left ltx_border_t">44.84</td>
<td id="S3.F6.1.3.3.12" class="ltx_td ltx_align_left ltx_border_t">32.42</td>
</tr>
<tr id="S3.F6.1.4.4" class="ltx_tr">
<td id="S3.F6.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.4.4.1.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="S3.F6.1.4.4.2" class="ltx_td ltx_align_left">27.66</td>
<td id="S3.F6.1.4.4.3" class="ltx_td ltx_align_left">29.84</td>
<td id="S3.F6.1.4.4.4" class="ltx_td ltx_align_left">59.84</td>
<td id="S3.F6.1.4.4.5" class="ltx_td ltx_align_left">45.00</td>
<td id="S3.F6.1.4.4.6" class="ltx_td ltx_align_left">50.78</td>
<td id="S3.F6.1.4.4.7" class="ltx_td ltx_align_left"><span id="S3.F6.1.4.4.7.1" class="ltx_text ltx_font_bold">63.75</span></td>
<td id="S3.F6.1.4.4.8" class="ltx_td ltx_align_left">15.94</td>
<td id="S3.F6.1.4.4.9" class="ltx_td ltx_align_left">34.69</td>
<td id="S3.F6.1.4.4.10" class="ltx_td ltx_align_left">25.62</td>
<td id="S3.F6.1.4.4.11" class="ltx_td ltx_align_left">24.69</td>
</tr>
<tr id="S3.F6.1.5.5" class="ltx_tr">
<td id="S3.F6.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.5.5.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="S3.F6.1.5.5.2" class="ltx_td ltx_align_left">23.13</td>
<td id="S3.F6.1.5.5.3" class="ltx_td ltx_align_left">27.19</td>
<td id="S3.F6.1.5.5.4" class="ltx_td ltx_align_left">54.53</td>
<td id="S3.F6.1.5.5.5" class="ltx_td ltx_align_left">40.00</td>
<td id="S3.F6.1.5.5.6" class="ltx_td ltx_align_left">45.00</td>
<td id="S3.F6.1.5.5.7" class="ltx_td ltx_align_left"><span id="S3.F6.1.5.5.7.1" class="ltx_text ltx_font_bold">53.91</span></td>
<td id="S3.F6.1.5.5.8" class="ltx_td ltx_align_left">13.44</td>
<td id="S3.F6.1.5.5.9" class="ltx_td ltx_align_left">32.81</td>
<td id="S3.F6.1.5.5.10" class="ltx_td ltx_align_left">23.44</td>
<td id="S3.F6.1.5.5.11" class="ltx_td ltx_align_left">22.97</td>
</tr>
<tr id="S3.F6.1.6.6" class="ltx_tr">
<td id="S3.F6.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.6.6.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="S3.F6.1.6.6.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the (left/right) of the picture in a [background] environment“</em></span>
</span>
</td>
<td id="S3.F6.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.6.6.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="S3.F6.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t">19.84</td>
<td id="S3.F6.1.6.6.4" class="ltx_td ltx_align_left ltx_border_t">20.00</td>
<td id="S3.F6.1.6.6.5" class="ltx_td ltx_align_left ltx_border_t">39.38</td>
<td id="S3.F6.1.6.6.6" class="ltx_td ltx_align_left ltx_border_t">30.31</td>
<td id="S3.F6.1.6.6.7" class="ltx_td ltx_align_left ltx_border_t">35.94</td>
<td id="S3.F6.1.6.6.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.6.6.8.1" class="ltx_text ltx_font_bold">42.19</span></td>
<td id="S3.F6.1.6.6.9" class="ltx_td ltx_align_left ltx_border_t">14.69</td>
<td id="S3.F6.1.6.6.10" class="ltx_td ltx_align_left ltx_border_t">27.03</td>
<td id="S3.F6.1.6.6.11" class="ltx_td ltx_align_left ltx_border_t">22.66</td>
<td id="S3.F6.1.6.6.12" class="ltx_td ltx_align_left ltx_border_t">16.56</td>
</tr>
<tr id="S3.F6.1.7.7" class="ltx_tr">
<td id="S3.F6.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S3.F6.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.7.7.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.F6.1.7.7.1.1.1.1" class="ltx_text"><em id="S3.F6.1.7.7.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] and a [character] in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.F6.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.7.7.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="S3.F6.1.7.7.3" class="ltx_td ltx_align_left ltx_border_t">9.42</td>
<td id="S3.F6.1.7.7.4" class="ltx_td ltx_align_left ltx_border_t">11.88</td>
<td id="S3.F6.1.7.7.5" class="ltx_td ltx_align_left ltx_border_t">44.08</td>
<td id="S3.F6.1.7.7.6" class="ltx_td ltx_align_left ltx_border_t">25.36</td>
<td id="S3.F6.1.7.7.7" class="ltx_td ltx_align_left ltx_border_t">34.98</td>
<td id="S3.F6.1.7.7.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.7.7.8.1" class="ltx_text ltx_font_bold">50.66</span></td>
<td id="S3.F6.1.7.7.9" class="ltx_td ltx_align_left ltx_border_t">6.30</td>
<td id="S3.F6.1.7.7.10" class="ltx_td ltx_align_left ltx_border_t">22.29</td>
<td id="S3.F6.1.7.7.11" class="ltx_td ltx_align_left ltx_border_t">13.50</td>
<td id="S3.F6.1.7.7.12" class="ltx_td ltx_align_left ltx_border_t">7.32</td>
</tr>
<tr id="S3.F6.1.8.8" class="ltx_tr">
<td id="S3.F6.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.8.8.1.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="S3.F6.1.8.8.2" class="ltx_td ltx_align_left">3.54</td>
<td id="S3.F6.1.8.8.3" class="ltx_td ltx_align_left">6.03</td>
<td id="S3.F6.1.8.8.4" class="ltx_td ltx_align_left">23.00</td>
<td id="S3.F6.1.8.8.5" class="ltx_td ltx_align_left">14.16</td>
<td id="S3.F6.1.8.8.6" class="ltx_td ltx_align_left">14.59</td>
<td id="S3.F6.1.8.8.7" class="ltx_td ltx_align_left"><span id="S3.F6.1.8.8.7.1" class="ltx_text ltx_font_bold">28.54</span></td>
<td id="S3.F6.1.8.8.8" class="ltx_td ltx_align_left">1.93</td>
<td id="S3.F6.1.8.8.9" class="ltx_td ltx_align_left">7.02</td>
<td id="S3.F6.1.8.8.10" class="ltx_td ltx_align_left">2.49</td>
<td id="S3.F6.1.8.8.11" class="ltx_td ltx_align_left">2.94</td>
</tr>
<tr id="S3.F6.1.9.9" class="ltx_tr">
<td id="S3.F6.1.9.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F6.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.9.9.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="S3.F6.1.9.9.2" class="ltx_td ltx_align_left">2.89</td>
<td id="S3.F6.1.9.9.3" class="ltx_td ltx_align_left">4.94</td>
<td id="S3.F6.1.9.9.4" class="ltx_td ltx_align_left">16.98</td>
<td id="S3.F6.1.9.9.5" class="ltx_td ltx_align_left">11.21</td>
<td id="S3.F6.1.9.9.6" class="ltx_td ltx_align_left">12.21</td>
<td id="S3.F6.1.9.9.7" class="ltx_td ltx_align_left"><span id="S3.F6.1.9.9.7.1" class="ltx_text ltx_font_bold">21.89</span></td>
<td id="S3.F6.1.9.9.8" class="ltx_td ltx_align_left">1.11</td>
<td id="S3.F6.1.9.9.9" class="ltx_td ltx_align_left">5.67</td>
<td id="S3.F6.1.9.9.10" class="ltx_td ltx_align_left">2.48</td>
<td id="S3.F6.1.9.9.11" class="ltx_td ltx_align_left">3.28</td>
</tr>
<tr id="S3.F6.1.10.10" class="ltx_tr">
<td id="S3.F6.1.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.10.10.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="S3.F6.1.10.10.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the left and a [character] on the right in a [background] environment“</em></span>
</span>
</td>
<td id="S3.F6.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.10.10.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="S3.F6.1.10.10.3" class="ltx_td ltx_align_left ltx_border_t">4.82</td>
<td id="S3.F6.1.10.10.4" class="ltx_td ltx_align_left ltx_border_t">6.75</td>
<td id="S3.F6.1.10.10.5" class="ltx_td ltx_align_left ltx_border_t">22.20</td>
<td id="S3.F6.1.10.10.6" class="ltx_td ltx_align_left ltx_border_t">13.00</td>
<td id="S3.F6.1.10.10.7" class="ltx_td ltx_align_left ltx_border_t">20.94</td>
<td id="S3.F6.1.10.10.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.10.10.8.1" class="ltx_text ltx_font_bold">29.83</span></td>
<td id="S3.F6.1.10.10.9" class="ltx_td ltx_align_left ltx_border_t">3.43</td>
<td id="S3.F6.1.10.10.10" class="ltx_td ltx_align_left ltx_border_t">11.51</td>
<td id="S3.F6.1.10.10.11" class="ltx_td ltx_align_left ltx_border_t">7.42</td>
<td id="S3.F6.1.10.10.12" class="ltx_td ltx_align_left ltx_border_t">4.92</td>
</tr>
<tr id="S3.F6.1.11.11" class="ltx_tr">
<td id="S3.F6.1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" rowspan="2">
<span id="S3.F6.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.11.11.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.F6.1.11.11.1.1.1.1" class="ltx_text"><em id="S3.F6.1.11.11.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] textured with [texture1] and a [character] textured with [texture2] in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.F6.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F6.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.11.11.2.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="S3.F6.1.11.11.3" class="ltx_td ltx_align_left ltx_border_t">1.69</td>
<td id="S3.F6.1.11.11.4" class="ltx_td ltx_align_left ltx_border_t">3.03</td>
<td id="S3.F6.1.11.11.5" class="ltx_td ltx_align_left ltx_border_t">9.18</td>
<td id="S3.F6.1.11.11.6" class="ltx_td ltx_align_left ltx_border_t">6.37</td>
<td id="S3.F6.1.11.11.7" class="ltx_td ltx_align_left ltx_border_t">8.25</td>
<td id="S3.F6.1.11.11.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.F6.1.11.11.8.1" class="ltx_text ltx_font_bold">15.89</span></td>
<td id="S3.F6.1.11.11.9" class="ltx_td ltx_align_left ltx_border_t">1.73</td>
<td id="S3.F6.1.11.11.10" class="ltx_td ltx_align_left ltx_border_t">4.20</td>
<td id="S3.F6.1.11.11.11" class="ltx_td ltx_align_left ltx_border_t">2.50</td>
<td id="S3.F6.1.11.11.12" class="ltx_td ltx_align_left ltx_border_t">1.44</td>
</tr>
<tr id="S3.F6.1.12.12" class="ltx_tr">
<td id="S3.F6.1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.F6.1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F6.1.12.12.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="S3.F6.1.12.12.2" class="ltx_td ltx_align_left ltx_border_b">1.20</td>
<td id="S3.F6.1.12.12.3" class="ltx_td ltx_align_left ltx_border_b">2.24</td>
<td id="S3.F6.1.12.12.4" class="ltx_td ltx_align_left ltx_border_b">8.46</td>
<td id="S3.F6.1.12.12.5" class="ltx_td ltx_align_left ltx_border_b">6.14</td>
<td id="S3.F6.1.12.12.6" class="ltx_td ltx_align_left ltx_border_b">7.08</td>
<td id="S3.F6.1.12.12.7" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.F6.1.12.12.7.1" class="ltx_text ltx_font_bold">13.50</span></td>
<td id="S3.F6.1.12.12.8" class="ltx_td ltx_align_left ltx_border_b">1.50</td>
<td id="S3.F6.1.12.12.9" class="ltx_td ltx_align_left ltx_border_b">3.50</td>
<td id="S3.F6.1.12.12.10" class="ltx_td ltx_align_left ltx_border_b">1.89</td>
<td id="S3.F6.1.12.12.11" class="ltx_td ltx_align_left ltx_border_b">1.52</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Setup and zero-shot evaluation of CLIP models on PUG: SPAR with <span id="S3.F6.4.1" class="ltx_text ltx_font_bold">caption retrieval</span>. By using synthetic data, we can increase progressively the <span id="S3.F6.5.2" class="ltx_text ltx_font_italic">difficulty</span> of a scene. Our setup is presented in the image above the table in which we show 6 different types of image captioning. 1) caption for background scene recognition for which we have 10 different backgrounds which are easy to distinguish from each other. 2) caption for single animal class prediction, the model should predict the correct categories over the 32 possible animals and 10 backgrounds (for a total of 320 captions). 3) caption for single animal position prediction that increases the number of caption up to 640 and lead to a significant drop in accuracy for every models. 4) caption for two animals class prediction, the model should predict the correct categories of the two animals presented in the images (5120 captions). 5) caption for two animals positions prediction, the model should predict the position of the two animals in the picture (over a total of 10240 captions). 6) caption for two textured animals class prediction, the model should recognize a blue elephant from a red camel. The performances of several VLMs models are presented in the table for which each row corresponds to one of the scenario described previously.</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">As a third member of the PUG family, we introduce <em id="S3.SS4.p1.1.1" class="ltx_emph ltx_font_italic">PUG: SPAR (Scene, Position, Attribute and Relation)</em> for evaluating vision-language models (VLMs). In contrast to pure vision based models, VLMs should be able to predict the correct caption (from a given set of captions) that describe the content of a given image. Several benchmarks to evaluate VLM models already exist such as Winoground <cite class="ltx_cite ltx_citemacro_citep">(Thrush et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite> or ARO <cite class="ltx_cite ltx_citemacro_citep">(Yuksekgonul et al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>. However, recent works<cite class="ltx_cite ltx_citemacro_citep">(Thrush et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Diwan et al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> have highlighted an important limitation in these benchmarks: some image-caption pairs in Winoground might be even too difficult to solve for a human whereas ARO has been shown by <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> to be mostly solvable without even using the image information at all. Consider that for an image containing a horse eating grass, ARO will propose two captions: <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">"the horse eating the grass"</span> and <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_italic">"the grass eating the horse"</span>. The model should predict the correct caption between these two. However, the second caption is impossible, so even without looking at the image, any model can be confident that the first caption is the correct one.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p">Another shortcoming of current benchmarks is that most of them probe only if the model is correctly able to understand the relations or the attributes between objects. However it is not clear if the failures in finding the correct relations or attributes come from the model not understanding them or come from not understanding which objects are present in the scene. For example, to understand complex relations like: <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_italic">A photo of an elephant on the left and a camel on the right in a desert background</span>, the model should first be able to identify whether the background of the picture is an actual desert. Then, the model should identify whether there is an elephant on the picture. It should understand what is <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">an elephant on the left</span>. The model could be very effective at identifying individual elephants or camels, but it could unexpectedly fail when a camel and an elephant appear in the same picture. If the model does not fail in recognizing the animals, then we can probe the model to evaluate the position of each of them. We built PUG: SPAR with this goal of having a progressive evaluation scheme in which we can easily determine exactly what are the failure modes of a given VLM. From basic scene understanding to complex relations and attributes, our dataset offer a simple and yet effective way to get a better understanding of the capabilities and limitations of VLMs.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p">The dataset contains 43,560 images with the associated factors of variations: 10 backgrounds, 32 animals, 4 relations (left/right, botton/top) and 4 animal texture attributes (blue/red, grass/stone). We have images containing either 1) only the background (for scene recognition) 2) only one animal at different left/right or bottom/top position 3) two animals at different left/right or bottom/top position. And for each of the scenes (either single or multiple animals), we vary the texture of the animals and the background to evaluate the robustness of the model. Our setup and experiments are presented in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.4 PUG: SPAR for VLMs ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> in which we display some images from the dataset with the corresponding captions used for evaluations. In our benchmark, we used 6 different types of captions to evaluate the following: 1) Scene Recognition (first row) 2) Single animal classification (second row) 3) Single animal position detection (third row) 4) Multiple animals classification (forth row) 5) Multiple animal position detection (fifth row) 6) Multiple animal and textures prediction (sixth row). For each of them, we evaluate the top-1 retrieval accuracy of the correct captions within the set of captions associated to each setup. We evaluate multiple models on these setups: OpenAI CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>, OpenCLIP <cite class="ltx_cite ltx_citemacro_citep">(Ilharco et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>, Flava <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite>, BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib38" title="" class="ltx_ref">2022b</a>)</cite>, X-VLM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al., <a href="#bib.bib69" title="" class="ltx_ref">2021</a>)</cite> and NegCLIP <cite class="ltx_cite ltx_citemacro_citep">(Yuksekgonul et al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>. Most of the models are correctly able to solve the scene recognition task (which is not surprising since we used only 10 environments which are very different from each other). Concerning the simple object recognition task when using a single animal, the performances across models is highly variable. Our experiments also highlight that the VLM performance in a multiple animals detection setting are much worse than the performance in a single animal detection setting. Those experiments show that despite their successes, VLMs are far from having a good understanding of the world and that improving the robustness of these models is a needed step for real-world robustness.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p">Inspired by Winoground <cite class="ltx_cite ltx_citemacro_citep">(Thrush et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>)</cite>, we present an experimental setup in which we leverage hard-negative pair of images. Instead of performing caption retrieval within all captions associated to a given setup, we performed caption retrieval between the correct and the <span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_italic">hard negative</span> caption. For example, the hard negative caption of <span id="S3.SS4.p4.1.2" class="ltx_text ltx_font_italic">"An elephant on the left of the picture and a camel on the right of the picture"</span> will be <span id="S3.SS4.p4.1.3" class="ltx_text ltx_font_italic">"A camel on the left of the picture and an elephant on the right of the picture"</span>. In addition of switching the relation (left/right and bottom/top), we also provide hard negative captioning for the attributes (blue/red and grass/stone). In Table <a href="#S3.T2" title="Table 2 ‣ 3.4 PUG: SPAR for VLMs ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we present our results using the hard-negative pair. We clearly observe that none of the models are able to predict the correct captions, with many models being close to random performance (50%).</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Caption</span>
</span>
</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:19.9pt;">Values</span>
</span>
</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.3.1.1" class="ltx_text ltx_inline-block" style="width:37.6pt;">
<span id="S3.T2.1.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:47.6pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.3.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.3.1.1.1.1.1" class="ltx_text">B-32 CLIP</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.4.1.1" class="ltx_text ltx_inline-block" style="width:71.1pt;">
<span id="S3.T2.1.1.1.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:81.1pt;height:8.800000000000001pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.4.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.4.1.1.1.1.1" class="ltx_text">B-32 OpenClip 2B</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.5.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.5.1.1" class="ltx_text ltx_inline-block" style="width:36.8pt;">
<span id="S3.T2.1.1.1.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:46.8pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.5.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.5.1.1.1.1.1" class="ltx_text">L-14 CLIP</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.6.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.6.1.1" class="ltx_text ltx_inline-block" style="width:70.3pt;">
<span id="S3.T2.1.1.1.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:80.3pt;height:8.800000000000001pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.6.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.6.1.1.1.1.1" class="ltx_text">L-14 OpenClip 2B</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.7.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.7.1.1" class="ltx_text ltx_inline-block" style="width:77.1pt;">
<span id="S3.T2.1.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:87.1pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.7.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.7.1.1.1.1.1" class="ltx_text">H-14 OpenCLIP 2B</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.8.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.8.1.1" class="ltx_text ltx_inline-block" style="width:77.4pt;">
<span id="S3.T2.1.1.1.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:87.4pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.8.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.8.1.1.1.1.1" class="ltx_text">G-14 OpenCLIP 2B</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.9.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.9.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="S3.T2.1.1.1.9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.9.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.9.1.1.1.1.1" class="ltx_text">Flava</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.10.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.10.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="S3.T2.1.1.1.10.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.10.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.10.1.1.1.1.1" class="ltx_text">BLIP</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.11" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.11.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.11.1.1" class="ltx_text ltx_inline-block" style="width:20.4pt;">
<span id="S3.T2.1.1.1.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:30.4pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.11.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.11.1.1.1.1.1" class="ltx_text">XVLM</span></span>
</span></span></span></span></th>
<th id="S3.T2.1.1.1.12" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T2.1.1.1.12.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="S3.T2.1.1.1.12.1.1" class="ltx_text ltx_inline-block" style="width:30.8pt;">
<span id="S3.T2.1.1.1.12.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:40.8pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T2.1.1.1.12.1.1.1.1" class="ltx_p"><span id="S3.T2.1.1.1.12.1.1.1.1.1" class="ltx_text">NegCLIP</span></span>
</span></span></span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S3.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.T2.1.2.1.1.1.1.1" class="ltx_text"><em id="S3.T2.1.2.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the [position] of the picture in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.2.1.1" class="ltx_p" style="width:19.9pt;">Left/Right</span>
</span>
</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">49.53</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">47.66</td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">50.00</td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">46.72</td>
<td id="S3.T2.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">49.84</td>
<td id="S3.T2.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">50.31</td>
<td id="S3.T2.1.2.1.9" class="ltx_td ltx_align_left ltx_border_t">49.22</td>
<td id="S3.T2.1.2.1.10" class="ltx_td ltx_align_left ltx_border_t">51.88</td>
<td id="S3.T2.1.2.1.11" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.2.1.11.1" class="ltx_text ltx_font_bold">52.03</span></td>
<td id="S3.T2.1.2.1.12" class="ltx_td ltx_align_left ltx_border_t">48.91</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1.1" class="ltx_p" style="width:19.9pt;">Bottom/Top</span>
</span>
</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_left">54.84</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_left">52.34</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_left"><span id="S3.T2.1.3.2.4.1" class="ltx_text ltx_font_bold">66.87</span></td>
<td id="S3.T2.1.3.2.5" class="ltx_td ltx_align_left">60.62</td>
<td id="S3.T2.1.3.2.6" class="ltx_td ltx_align_left">56.56</td>
<td id="S3.T2.1.3.2.7" class="ltx_td ltx_align_left">58.91</td>
<td id="S3.T2.1.3.2.8" class="ltx_td ltx_align_left">50.47</td>
<td id="S3.T2.1.3.2.9" class="ltx_td ltx_align_left">54.84</td>
<td id="S3.T2.1.3.2.10" class="ltx_td ltx_align_left">53.28</td>
<td id="S3.T2.1.3.2.11" class="ltx_td ltx_align_left">54.06</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S3.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.T2.1.4.3.1.1.1.1" class="ltx_text"><em id="S3.T2.1.4.3.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the [position1] and a [character] on the [position2] in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.2.1.1" class="ltx_p" style="width:19.9pt;">Left/Right</span>
</span>
</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">53.88</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">55.62</td>
<td id="S3.T2.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">53.17</td>
<td id="S3.T2.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.4.3.6.1" class="ltx_text ltx_font_bold">56.23</span></td>
<td id="S3.T2.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t">55.74</td>
<td id="S3.T2.1.4.3.8" class="ltx_td ltx_align_left ltx_border_t">54.44</td>
<td id="S3.T2.1.4.3.9" class="ltx_td ltx_align_left ltx_border_t">54.41</td>
<td id="S3.T2.1.4.3.10" class="ltx_td ltx_align_left ltx_border_t">53.79</td>
<td id="S3.T2.1.4.3.11" class="ltx_td ltx_align_left ltx_border_t">55.02</td>
<td id="S3.T2.1.4.3.12" class="ltx_td ltx_align_left ltx_border_t">54.36</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.1.1.1" class="ltx_p" style="width:19.9pt;">Bottom/Top</span>
</span>
</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_left">51.15</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_left">53.84</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_left">54.06</td>
<td id="S3.T2.1.5.4.5" class="ltx_td ltx_align_left">53.51</td>
<td id="S3.T2.1.5.4.6" class="ltx_td ltx_align_left">57.24</td>
<td id="S3.T2.1.5.4.7" class="ltx_td ltx_align_left">56.49</td>
<td id="S3.T2.1.5.4.8" class="ltx_td ltx_align_left">55.23</td>
<td id="S3.T2.1.5.4.9" class="ltx_td ltx_align_left"><span id="S3.T2.1.5.4.9.1" class="ltx_text ltx_font_bold">60.26</span></td>
<td id="S3.T2.1.5.4.10" class="ltx_td ltx_align_left">58.87</td>
<td id="S3.T2.1.5.4.11" class="ltx_td ltx_align_left">54.09</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" rowspan="2">
<span id="S3.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.T2.1.6.5.1.1.1.1" class="ltx_text"><em id="S3.T2.1.6.5.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] textured with [texture1] and a [character] textured with [texture2] in a [background] environment“</em></span></span>
</span>
</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.5.2.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">52.77</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">53.63</td>
<td id="S3.T2.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t">54.43</td>
<td id="S3.T2.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t">56.94</td>
<td id="S3.T2.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t">55.48</td>
<td id="S3.T2.1.6.5.8" class="ltx_td ltx_align_left ltx_border_t">54.42</td>
<td id="S3.T2.1.6.5.9" class="ltx_td ltx_align_left ltx_border_t">54.22</td>
<td id="S3.T2.1.6.5.10" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.6.5.10.1" class="ltx_text ltx_font_bold">57.32</span></td>
<td id="S3.T2.1.6.5.11" class="ltx_td ltx_align_left ltx_border_t">56.19</td>
<td id="S3.T2.1.6.5.12" class="ltx_td ltx_align_left ltx_border_t">51.74</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<td id="S3.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.6.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_left ltx_border_b">52.79</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_left ltx_border_b">54.14</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_left ltx_border_b">56.31</td>
<td id="S3.T2.1.7.6.5" class="ltx_td ltx_align_left ltx_border_b">57.28</td>
<td id="S3.T2.1.7.6.6" class="ltx_td ltx_align_left ltx_border_b">56.62</td>
<td id="S3.T2.1.7.6.7" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T2.1.7.6.7.1" class="ltx_text ltx_font_bold">57.19</span></td>
<td id="S3.T2.1.7.6.8" class="ltx_td ltx_align_left ltx_border_b">54.53</td>
<td id="S3.T2.1.7.6.9" class="ltx_td ltx_align_left ltx_border_b">53.94</td>
<td id="S3.T2.1.7.6.10" class="ltx_td ltx_align_left ltx_border_b">54.26</td>
<td id="S3.T2.1.7.6.11" class="ltx_td ltx_align_left ltx_border_b">49.92</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>We present the performances of several VLMs with <span id="S3.T2.3.1" class="ltx_text ltx_font_bold">hard negatives captioning</span> on PUG: SPAR in which we perform retrieval between two captions: the correct caption and the hard-negative corresponding caption. In that instance, the model should choose the correct caption between both of them (the probability to get the correct one with a random model would be 50%). Interestingly, none of the model presented in this table seem to be able to get a real understanding of simple position (left, right, bottom, top) or colors.</figcaption>
</figure>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>PUG: AR4T</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Lastly, we introduce PUG: AR4T (Attributes and Relations for training). In contrast to PUG: SPAR which is only an evaluation benchmark, PUG: AR4T was created as an additional fine-tuning dataset for VLMs.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>The assets used to create PUG: SPAR and PUG: AR4T are different enough such that PUG: SPAR is still a good benchmark to evaluate models fine-tuned with PUG: AR4T.</span></span></span> As shown in the previous section, VLMs struggle to understand spatial relations or attributes and thus are good candidates for our fine-tuning scenario. PUG: AR4T contains 249,986 training images with captions and 23,216 test images<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We also run experiments with a version of this dataset which contain 1M images but as shown in Table <a href="#S3.T3" title="Table 3 ‣ 3.4.1 PUG: AR4T ‣ 3.4 PUG: SPAR for VLMs ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, adding more images does not increase performance on PUG: SPAR. We only release publicly the version of PUG:AR4T that contain 249,986 images.</span></span></span>. In Table <a href="#S3.T3" title="Table 3 ‣ 3.4.1 PUG: AR4T ‣ 3.4 PUG: SPAR for VLMs ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present CLIP fine-tuning results on the ARO and PUG: SPAR benchmark. We also compare our results against Syn-CLIP, which is CLIP fine-tuned on the SyVIC synthetic dataset. Our results are very similar to Syn-CLIP, but Syn-CLIP training requires several additional tricks to arrive at this performance (Section 3.2 in <cite class="ltx_cite ltx_citemacro_citep">(Cascante-Bonilla et al., <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>). On the other hand, the photo-realistic nature of PUG: AR4T enables us to match Syn-CLIP without any of these additional bells and whistles. However, even if we note some improvements on ARO, we are still far from having a model able to understand spatial relations. This is highlighted by the results given on our PUG: SPAR benchmark for which the improvement on single animal position prediction is still only above random chance while there is no improvement on the double animal location prediction task. This confirm the unreliability of the ARO benchmark highlighted by <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:76.4pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.5pt,25.1pt) scale(0.601796164730374,0.601796164730374) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="5">ARO</td>
<td id="S3.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2">PUG: SPAR (left/right)</td>
</tr>
<tr id="S3.T3.1.1.2.2" class="ltx_tr">
<td id="S3.T3.1.1.2.2.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T3.1.1.2.2.2" class="ltx_td ltx_align_left">VG-Relation</td>
<td id="S3.T3.1.1.2.2.3" class="ltx_td ltx_align_left">VG-Attribution</td>
<td id="S3.T3.1.1.2.2.4" class="ltx_td ltx_align_left">COCO-Order</td>
<td id="S3.T3.1.1.2.2.5" class="ltx_td ltx_align_left">Flickr30k-Order</td>
<td id="S3.T3.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_rr">Average</td>
<td id="S3.T3.1.1.2.2.7" class="ltx_td ltx_align_left">Single</td>
<td id="S3.T3.1.1.2.2.8" class="ltx_td ltx_align_left">Double</td>
</tr>
<tr id="S3.T3.1.1.3.3" class="ltx_tr">
<td id="S3.T3.1.1.3.3.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T3.1.1.3.3.2" class="ltx_td ltx_align_left">(Macro-Accuracy%)</td>
<td id="S3.T3.1.1.3.3.3" class="ltx_td ltx_align_left">(Macro-Accuracy%)</td>
<td id="S3.T3.1.1.3.3.4" class="ltx_td ltx_align_left">(Precision@1)</td>
<td id="S3.T3.1.1.3.3.5" class="ltx_td ltx_align_left">(Precision@1)</td>
<td id="S3.T3.1.1.3.3.6" class="ltx_td ltx_border_rr"></td>
<td id="S3.T3.1.1.3.3.7" class="ltx_td ltx_align_left">(Precision@1)</td>
<td id="S3.T3.1.1.3.3.8" class="ltx_td ltx_align_left">(Precision@1)</td>
</tr>
<tr id="S3.T3.1.1.4.4" class="ltx_tr">
<td id="S3.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.1.4.4.1.1" class="ltx_text ltx_font_bold">CLIP-ViT-B/32 (400M)</span></td>
<td id="S3.T3.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">59.16 | 55.50</td>
<td id="S3.T3.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t">62.18 | 61.52</td>
<td id="S3.T3.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_t">47.96</td>
<td id="S3.T3.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_t">59.98</td>
<td id="S3.T3.1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">57.32</td>
<td id="S3.T3.1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_t">49.84</td>
<td id="S3.T3.1.1.4.4.8" class="ltx_td ltx_align_left ltx_border_t">54.42 <span class="ltx_rule" style="width:0.0pt;height:11.2pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="S3.T3.1.1.5.5" class="ltx_tr">
<td id="S3.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r">   + <span id="S3.T3.1.1.5.5.1.1" class="ltx_text ltx_font_italic">FT w/ Syn-CLIP</span>
</td>
<td id="S3.T3.1.1.5.5.2" class="ltx_td ltx_align_left"><span id="S3.T3.1.1.5.5.2.1" class="ltx_text ltx_font_bold">71.40</span></td>
<td id="S3.T3.1.1.5.5.3" class="ltx_td ltx_align_left"><span id="S3.T3.1.1.5.5.3.1" class="ltx_text ltx_font_bold">66.94</span></td>
<td id="S3.T3.1.1.5.5.4" class="ltx_td ltx_align_left">59.06</td>
<td id="S3.T3.1.1.5.5.5" class="ltx_td ltx_align_left">70.96</td>
<td id="S3.T3.1.1.5.5.6" class="ltx_td ltx_align_left ltx_border_rr">67.09 <span id="S3.T3.1.1.5.5.6.1" class="ltx_text" style="color:#00E000;">(+9.77)</span>
</td>
<td id="S3.T3.1.1.5.5.7" class="ltx_td ltx_align_left">N/A</td>
<td id="S3.T3.1.1.5.5.8" class="ltx_td ltx_align_left">N/A</td>
</tr>
<tr id="S3.T3.1.1.6.6" class="ltx_tr">
<td id="S3.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r">   + <span id="S3.T3.1.1.6.6.1.1" class="ltx_text ltx_font_italic">FT w/ PUG:AR4T (200K)</span>
</td>
<td id="S3.T3.1.1.6.6.2" class="ltx_td ltx_align_left">68.36 | 75.18</td>
<td id="S3.T3.1.1.6.6.3" class="ltx_td ltx_align_left">65.54 | 64.44</td>
<td id="S3.T3.1.1.6.6.4" class="ltx_td ltx_align_left">57.80</td>
<td id="S3.T3.1.1.6.6.5" class="ltx_td ltx_align_left">69.74</td>
<td id="S3.T3.1.1.6.6.6" class="ltx_td ltx_align_left ltx_border_rr">65.36 <span id="S3.T3.1.1.6.6.6.1" class="ltx_text" style="color:#00E000;">(+8.04)</span>
</td>
<td id="S3.T3.1.1.6.6.7" class="ltx_td ltx_align_left">50.78<span id="S3.T3.1.1.6.6.7.1" class="ltx_text" style="color:#00E000;">(+0.94)</span>
</td>
<td id="S3.T3.1.1.6.6.8" class="ltx_td ltx_align_left">54.23<span id="S3.T3.1.1.6.6.8.1" class="ltx_text" style="color:#FF0000;">(-0.19)</span>
</td>
</tr>
<tr id="S3.T3.1.1.7.7" class="ltx_tr">
<td id="S3.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r">   + <span id="S3.T3.1.1.7.7.1.1" class="ltx_text ltx_font_italic">FT w/ PUG:AR4T (1M)</span>
</td>
<td id="S3.T3.1.1.7.7.2" class="ltx_td ltx_align_left">71.03 | 76.57</td>
<td id="S3.T3.1.1.7.7.3" class="ltx_td ltx_align_left">65.15 | 64.32</td>
<td id="S3.T3.1.1.7.7.4" class="ltx_td ltx_align_left"><span id="S3.T3.1.1.7.7.4.1" class="ltx_text ltx_font_bold">61.07</span></td>
<td id="S3.T3.1.1.7.7.5" class="ltx_td ltx_align_left"><span id="S3.T3.1.1.7.7.5.1" class="ltx_text ltx_font_bold">72.84</span></td>
<td id="S3.T3.1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_rr">
<span id="S3.T3.1.1.7.7.6.1" class="ltx_text ltx_font_bold">67.52</span> <span id="S3.T3.1.1.7.7.6.2" class="ltx_text" style="color:#00E000;">(+10.3)</span>
</td>
<td id="S3.T3.1.1.7.7.7" class="ltx_td ltx_align_left">50.16<span id="S3.T3.1.1.7.7.7.1" class="ltx_text" style="color:#00E000;">(+0.32)</span>
</td>
<td id="S3.T3.1.1.7.7.8" class="ltx_td ltx_align_left">54.19<span id="S3.T3.1.1.7.7.8.1" class="ltx_text" style="color:#FF0000;">(-0.23)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Fine-tuning CLIP on PUG: AR4T. For VG-Relation and Attribution, the results (Acc1 | Acc2) indicate macro-accuracy across all relations and attributes (Acc1), and macro-accuracy on the subset of relations and attributes present in both ARO and PUG (Acc2). For PUG: SPAR, we evaluate on images in which there is only one animal (Single) or two animals (Double) with the relation being left or right. We were not able to run SynCLIP on PUG: SPAR because the model was not public at the time of the publication.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">The fine-grained controllability of synthetic rendered image data makes it ideal for designing challenging evaluation benchmarks to better understand the properties and limitations of vision models, as well as for controlled training scenarios – if only it was closer to real data.
To this effect, we introduced PUG datasets for representation learning. By leveraging the photorealism of the Unreal Engine, we created 4 new datasets and showcased their utility for robust evaluation. We showed how PUG: Animals could be leveraged for OOD generalization and to study properties of the representation spaces. We developed PUG: ImageNet as a new challenging benchmark that researchers can easily use to assess and compare the robustness of image classifiers. With PUG: SPAR we provide a reliable benchmark for vision-language models while PUG:AR4T offer additional data that could be leveraged to fine-tune VLMs. Together, the PUG family of datasets represents a new standard of photorealism and control for synthetic image data.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas and Deny (2022)</span>
<span class="ltx_bibblock">
A. Abbas and S. Deny.

</span>
<span class="ltx_bibblock">Progress and limitations of deep networks to recognize objects in
unusual poses.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.08034</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alcorn et al. (2019)</span>
<span class="ltx_bibblock">
M. A. Alcorn, Q. Li, Z. Gong, C. Wang, L. Mai, W.-S. Ku, and A. Nguyen.

</span>
<span class="ltx_bibblock">Strike (with) a pose: Neural networks are easily fooled by strange
poses of familiar objects.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 4845–4854, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arjovsky et al. (2020)</span>
<span class="ltx_bibblock">
M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz.

</span>
<span class="ltx_bibblock">Invariant risk minimization, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Astolfi et al. (2023)</span>
<span class="ltx_bibblock">
P. Astolfi, A. Casanova, J. Verbeek, P. Vincent, A. Romero-Soriano, and
M. Drozdzal.

</span>
<span class="ltx_bibblock">Instance-conditioned gan data augmentation for representation
learning, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et al. (2023)</span>
<span class="ltx_bibblock">
S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet
classification, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal and Grover (2023)</span>
<span class="ltx_bibblock">
H. Bansal and A. Grover.

</span>
<span class="ltx_bibblock">Leaving reality to imagination: Robust classification via generated
datasets, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbu et al. (2019)</span>
<span class="ltx_bibblock">
A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and
B. Katz.

</span>
<span class="ltx_bibblock">Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouchacourt et al. (2021)</span>
<span class="ltx_bibblock">
D. Bouchacourt, M. Ibrahim, and A. Morcos.

</span>
<span class="ltx_bibblock">Grounding inductive biases in natural images: invariance stems from
variations in data.

</span>
<span class="ltx_bibblock">In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
Vaughan, editors, <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 34, pages 19566–19579. Curran Associates, Inc., 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/a2fe8c05877ec786290dd1450c3385cd-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2021/file/a2fe8c05877ec786290dd1450c3385cd-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cascante-Bonilla et al. (2023)</span>
<span class="ltx_bibblock">
P. Cascante-Bonilla, K. Shehada, J. S. Smith, S. Doveh, D. Kim, R. Panda,
G. Varol, A. Oliva, V. Ordonez, R. Feris, et al.

</span>
<span class="ltx_bibblock">Going beyond nouns with vision &amp; language models using synthetic
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17590</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2015)</span>
<span class="ltx_bibblock">
A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su, et al.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.03012</em>, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke et al. (2022)</span>
<span class="ltx_bibblock">
M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt,
L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi.

</span>
<span class="ltx_bibblock">Objaverse: A universe of annotated 3d objects.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08051</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
P. Dhariwal and A. Q. Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat GANs on image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Thirty-Fifth Conference on Neural Information Processing
Systems</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=AAWuCvzaVt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=AAWuCvzaVt</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diwan et al. (2022)</span>
<span class="ltx_bibblock">
A. Diwan, L. Berry, E. Choi, D. Harwath, and K. Mahowald.

</span>
<span class="ltx_bibblock">Why is winoground hard? investigating failures in visuolinguistic
compositionality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2236–2250, Abu Dhabi, United Arab
Emirates, Dec. 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.143" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.emnlp-main.143</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2017)</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun.

</span>
<span class="ltx_bibblock">CARLA: An open urban driving simulator.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Annual Conference on Robot Learning</em>,
pages 1–16, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
and N. Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eastwood and Williams (2018)</span>
<span class="ltx_bibblock">
C. Eastwood and C. K. I. Williams.

</span>
<span class="ltx_bibblock">A framework for the quantitative evaluation of disentangled
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=By-7dz-AZ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=By-7dz-AZ</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
EpicGames.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.unrealengine.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com</a>.

</span>
<span class="ltx_bibblock">Unreal Engine is a copyright of Epic Games, Inc. and its affiliates
(collectively, “Epic”). Any use of images, datasets, or other content
made available by Epic, including without limitation through the Unreal
Engine Marketplace or the Epic Games Launcher, in connection with your use of
the PUG environments and datasets we’ve outlined in this paper and released
publicly in connection hereto (the “PUG environments and datasets”) or
otherwise, is subject to the Epic Content License Agreement available at
<a target="_blank" href="https://www.unrealengine.com/en-US/eula/content" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/en-US/eula/content</a> or other agreement
between you and Epic.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh et al. (2019)</span>
<span class="ltx_bibblock">
K. Ethayarajh, D. Duvenaud, and G. Hirst.

</span>
<span class="ltx_bibblock">Towards understanding linear word analogies.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3253–3262, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1315</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1315</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2021)</span>
<span class="ltx_bibblock">
C. Gan, J. Schwartz, S. Alter, M. Schrimpf, J. Traer, J. De Freitas,
J. Kubilius, A. Bhandwaldar, N. Haber, M. Sano, et al.

</span>
<span class="ltx_bibblock">Threedworld: A platform for interactive multi-modal physical
simulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</em>,
2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandikota et al. (2023)</span>
<span class="ltx_bibblock">
R. Gandikota, J. Materzynska, J. Fiotto-Kaufman, and D. Bau.

</span>
<span class="ltx_bibblock">Erasing concepts from diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.07345</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et al. (2021)</span>
<span class="ltx_bibblock">
T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii,
and K. Crawford.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(12):86–92,
2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghifary et al. (2015)</span>
<span class="ltx_bibblock">
M. Ghifary, W. Kleijn, M. Zhang, and D. Balduzzi.

</span>
<span class="ltx_bibblock">Domain generalization for object recognition with multi-task
autoencoders.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision (ICCV)</em>,
pages 2551–2559, 2015.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:12825123" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:12825123</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gondal et al. (2019)</span>
<span class="ltx_bibblock">
M. W. Gondal, M. Wuthrich, D. Miladinovic, F. Locatello, M. Breidt,
V. Volchkov, J. Akpo, O. Bachem, B. Schölkopf, and S. Bauer.

</span>
<span class="ltx_bibblock">On the transfer of inductive bias from simulation to the real world:
a new disentanglement dataset.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2019/file/d97d404b6119214e4a7018391195240a-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2019/file/d97d404b6119214e4a7018391195240a-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2020)</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 63(11):139–144, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. QI.

</span>
<span class="ltx_bibblock">IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR
IMAGE RECOGNITION?

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=nUmCcZ5RKF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=nUmCcZ5RKF</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:6840–6851, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ibrahim et al. (2022)</span>
<span class="ltx_bibblock">
M. Ibrahim, Q. Garrido, A. Morcos, and D. Bouchacourt.

</span>
<span class="ltx_bibblock">The robustness limits of sota vision models to natural variation.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13604</em>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Idrissi et al. (2022)</span>
<span class="ltx_bibblock">
B. Y. Idrissi, D. Bouchacourt, R. Balestriero, I. Evtimov, C. Hazirbas,
N. Ballas, P. Vincent, M. Drozdzal, D. Lopez-Paz, and M. Ibrahim.

</span>
<span class="ltx_bibblock">Imagenet-x: Understanding model mistakes with factor of variation
annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.01866</em>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et al. (2021)</span>
<span class="ltx_bibblock">
G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave,
V. Shankar, H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and
L. Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5143773" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5143773</a>.

</span>
<span class="ltx_bibblock">If you use this software, please cite it as below.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahanian et al. (2022)</span>
<span class="ltx_bibblock">
A. Jahanian, X. Puig, Y. Tian, and P. Isola.

</span>
<span class="ltx_bibblock">Generative models as a data source for multiview representation
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=qhAeZjs7dCL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=qhAeZjs7dCL</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2023)</span>
<span class="ltx_bibblock">
S. Jain, H. Lawrence, A. Moitra, and A. Madry.

</span>
<span class="ltx_bibblock">Distilling model failures as directions in latent space.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=99RpBVpLiX" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=99RpBVpLiX</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick,
and R. Girshick.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2901–2910, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klee et al. (2023)</span>
<span class="ltx_bibblock">
D. Klee, O. Biza, R. Platt, and R. Walters.

</span>
<span class="ltx_bibblock">Image to sphere: Learning equivariant features for efficient pose
prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=_2bDpAtr7PI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=_2bDpAtr7PI</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolesnikov et al. (2020)</span>
<span class="ltx_bibblock">
A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and
N. Houlsby.

</span>
<span class="ltx_bibblock">Big transfer (bit): General visual representation learning, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee (2000)</span>
<span class="ltx_bibblock">
J. Lee.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Introduction to Topological Manifolds</em>.

</span>
<span class="ltx_bibblock">Graduate texts in mathematics. Springer, 2000.

</span>
<span class="ltx_bibblock">ISBN 9780387950266.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://books.google.fr/books?id=5LqQgkS3--MC" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.fr/books?id=5LqQgkS3--MC</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lenc and Vedaldi (2018)</span>
<span class="ltx_bibblock">
K. Lenc and A. Vedaldi.

</span>
<span class="ltx_bibblock">Understanding image representations by measuring their equivariance
and equivalence.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s11263-018-1098-y</span>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
D. Li, H. Ling, S. W. Kim, K. Kreis, S. Fidler, and A. Torralba.

</span>
<span class="ltx_bibblock">Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>, pages
21298–21308. IEEE, 2022a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR52688.2022.02064</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/CVPR52688.2022.02064" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR52688.2022.02064</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022b.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,
P. Perona, D. Ramanan, P. Doll’a r, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: common objects in context.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1405.0312, 2014.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1405.0312" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1405.0312</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Z. Lin, X. Chen, D. Pathak, P. Zhang, and D. Ramanan.

</span>
<span class="ltx_bibblock">Visualgptscore: Visio-linguistic reasoning with multimodal generative
pre-training scores, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
J. Liu, Z. Shen, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui.

</span>
<span class="ltx_bibblock">Towards out-of-distribution generalization: A survey, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
S. Madan, T. Sasaki, T.-M. Li, X. Boix, and H. Pfister.

</span>
<span class="ltx_bibblock">Small in-distribution changes in 3d perspective and lighting fool
both CNNs and transformers.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/2106.16198" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2106.16198</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madan et al. (2022)</span>
<span class="ltx_bibblock">
S. Madan, T. Henry, J. Dozier, H. Ho, N. Bhandari, T. Sasaki, F. Durand,
H. Pfister, and X. Boix.

</span>
<span class="ltx_bibblock">When and how convolutional neural networks generalize to
out-of-distribution category–viewpoint combinations.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 4(2):146–153, Feb 2022.

</span>
<span class="ltx_bibblock">ISSN 2522-5839.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s42256-021-00437-5</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1038/s42256-021-00437-5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s42256-021-00437-5</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2022)</span>
<span class="ltx_bibblock">
S. Mishra, R. Panda, C. P. Phoo, C.-F. R. Chen, L. Karlinsky, K. Saenko,
V. Saligrama, and R. S. Feris.

</span>
<span class="ltx_bibblock">Task2sim: Towards effective pre-training and transfer from synthetic
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 9194–9204, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. (2023)</span>
<span class="ltx_bibblock">
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov,
P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas,
W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma,
G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and
P. Bojanowski.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2017)</span>
<span class="ltx_bibblock">
W. Qiu, F. Zhong, Y. Zhang, S. Qiao, Z. Xiao, T. S. Kim, Y. Wang, and
A. Yuille.

</span>
<span class="ltx_bibblock">Unrealcv: Virtual worlds for computer vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia Open Source Software Competition</em>, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sagawa* et al. (2020)</span>
<span class="ltx_bibblock">
S. Sagawa*, P. W. Koh*, T. B. Hashimoto, and P. Liang.

</span>
<span class="ltx_bibblock">Distributionally robust neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=ryxGuJrFvS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=ryxGuJrFvS</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sariyildiz et al. (2023)</span>
<span class="ltx_bibblock">
M. B. Sariyildiz, K. Alahari, D. Larlus, and Y. Kalantidis.

</span>
<span class="ltx_bibblock">Fake it till you make it: Learning transferable representations from
synthetic imagenet clones.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et al. (2023)</span>
<span class="ltx_bibblock">
I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson.

</span>
<span class="ltx_bibblock">The curse of recursion: Training on generated data makes models
forget, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela.

</span>
<span class="ltx_bibblock">FLAVA: A foundational language and vision alignment model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Somepalli et al. (2022)</span>
<span class="ltx_bibblock">
G. Somepalli, V. Singla, M. Goldblum, J. Geiping, and T. Goldstein.

</span>
<span class="ltx_bibblock">Diffusion art or digital forgery? investigating data replication in
diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.03860</em>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Struckmeier et al. (2023)</span>
<span class="ltx_bibblock">
O. Struckmeier, K. Tiwari, and V. Kyrki.

</span>
<span class="ltx_bibblock">Autoencoding slow representations for semi-supervised data-efficient
regression.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>, pages 1–19, 2023.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szot et al. (2021)</span>
<span class="ltx_bibblock">
A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre,
M. Mukadam, D. Chaplot, O. Maksymets, A. Gokaslan, V. Vondrus, S. Dharur,
F. Meier, W. Galuba, A. Chang, Z. Kira, V. Koltun, J. Malik, M. Savva, and
D. Batra.

</span>
<span class="ltx_bibblock">Habitat 2.0: Training home assistants to rearrange their habitat,
2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tai et al. (2019)</span>
<span class="ltx_bibblock">
K. S. Tai, P. Bailis, and G. Valiant.

</span>
<span class="ltx_bibblock">Equivariant transformer networks.

</span>
<span class="ltx_bibblock">In K. Chaudhuri and R. Salakhutdinov, editors, <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA</em>, volume 97 of <em id="bib.bib56.2.2" class="ltx_emph ltx_font_italic">Proceedings of
Machine Learning Research</em>, pages 6086–6095. PMLR, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v97/tai19a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v97/tai19a.html</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thrush et al. (2022)</span>
<span class="ltx_bibblock">
T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross.

</span>
<span class="ltx_bibblock">Winoground: Probing vision and language models for visio-linguistic
compositionality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 5238–5248, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trabucco et al. (2023)</span>
<span class="ltx_bibblock">
B. Trabucco, K. Doherty, M. Gurinas, and R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Effective data augmentation with diffusion models, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trager et al. (2023)</span>
<span class="ltx_bibblock">
M. Trager, P. Perera, L. Zancato, A. Achille, P. Bhatia, and S. Soatto.

</span>
<span class="ltx_bibblock">Linear spaces of meanings: Compositional structures in
vision-language models, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio et al. (2021)</span>
<span class="ltx_bibblock">
A. Ushio, L. Espinosa Anke, S. Schockaert, and J. Camacho-Collados.

</span>
<span class="ltx_bibblock">BERT is to NLP what AlexNet is to CV: Can pre-trained
language models identify analogies?

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, Online, Aug. 2021.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
T. Wang, K. Lin, L. Li, C.-C. Lin, Z. Yang, H. Zhang, Z. Liu, and L. Wang.

</span>
<span class="ltx_bibblock">Equivariant similarity for vision-language foundation models, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng (2018)</span>
<span class="ltx_bibblock">
L. Weng.

</span>
<span class="ltx_bibblock">From autoencoder to beta-vae.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">lilianweng.github.io</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lilianweng.github.io/posts/2018-08-12-vae/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lilianweng.github.io/posts/2018-08-12-vae/</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wightman et al. (2021)</span>
<span class="ltx_bibblock">
R. Wightman, H. Touvron, and H. Jégou.

</span>
<span class="ltx_bibblock">Resnet strikes back: An improved training procedure in timm, 2021.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,
C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest,
and A. M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online,
Oct. 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2020)</span>
<span class="ltx_bibblock">
K. Xiao, L. Engstrom, A. Ilyas, and A. Madry.

</span>
<span class="ltx_bibblock">Noise or signal: The role of image backgrounds in object recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.09994</em>, 2020.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2022)</span>
<span class="ltx_bibblock">
S. Xie, A. S. Morcos, S.-C. Zhu, and R. Vedantam.

</span>
<span class="ltx_bibblock">COAT: Measuring object compositionality in emergent
representations.

</span>
<span class="ltx_bibblock">In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and
S. Sabato, editors, <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on
Machine Learning</em>, volume 162 of <em id="bib.bib66.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pages 24388–24413. PMLR, 17–23 Jul 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v162/xie22b.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v162/xie22b.html</a>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xingxuan Zhang (2022)</span>
<span class="ltx_bibblock">
R. X. H. Y. Z. S. P. C. Xingxuan Zhang, Yue He.

</span>
<span class="ltx_bibblock">Nico++: Towards better benchmarking for domain generalization, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuksekgonul et al. (2023)</span>
<span class="ltx_bibblock">
M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou.

</span>
<span class="ltx_bibblock">When and why vision-language models behave like bags-of-words, and
what to do about it?

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=KRLUvxh8uaX" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=KRLUvxh8uaX</a>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2021)</span>
<span class="ltx_bibblock">
Y. Zeng, X. Zhang, and H. Li.

</span>
<span class="ltx_bibblock">Multi-grained vision language pre-training: Aligning texts with
visual concepts.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.08276</em>, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Y. Zhang, H. Ling, J. Gao, K. Yin, J.-F. Lafleche, A. Barriuso, A. Torralba,
and S. Fidler.

</span>
<span class="ltx_bibblock">Datasetgan: Efficient labeled data factory with minimal human effort.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10145–10155, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Limitations and Future Work</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Limitations</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">In this work, we introduced 4 new datasets that were created using the Unreal Engine. We presented a set of case studies, demonstrating how these datasets can be leveraged to improve both evaluation and training for representation learning. Deeper work would be needed to fully unlock the potential these datasets can offer. In addition, we merely scratch the surface of what a powerful engine such as the Unreal Engine can offer. With advanced techniques such as Lumen, Nanite and Megascans, it is now possible to create even more realistic environments. In addition, the datasets we provide have a single simple label, whereas future uses could easily provide detailed rich labels for the entire scene underlying each generate image.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Future Work</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">A long vision for the PUG family would be to yield a series of benchmarks that can probe the robustness of vision models. PUG: ImageNet is a first step in this direction, however we might want to get more factors of variation such as weather and occlusion. A second take would be to increase the richness of the labelling by making available detailed segmentation masks and labels. Making a short video dataset in which we have complete control over the factors is also a promising future direction since AI research on video is still far behind what can be done with images. The active learning pipeline direction is also worth to explore since the model could bias the PUG environment to produces samples that are the best for a specific downstream task<cite class="ltx_cite ltx_citemacro_citep">[Mishra et al., <a href="#bib.bib45" title="" class="ltx_ref">2022</a>]</cite>.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>PUG Datasets</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">The datasets PUG: Animals, PUG: ImageNet, PUG: SPAR and PUG:AR4T are available under the <span id="A2.p1.1.1" class="ltx_text ltx_font_bold">cc-by-nc license with the restrictions that they should not be use to train generative AI models</span>. They are available to download on the following website: <a target="_blank" href="https://pug.metademolab.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pug.metademolab.com/</a>. The datasets can be read by a torchvision ImageFolder. We have one class by folder and all the images associated to one class are saved in this folder as png. There is a csv file associated to each dataset that map a filename (with unique ID) to its associated factors of variations. Examples of dataloaders are available at <a target="_blank" href="https://github.com/facebookresearch/PUG" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/PUG</a>.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Datasheet</h3>

<figure id="A2.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="A2.T4.1.1" class="ltx_text ltx_font_bold">Datasheet for PUG</span>, following the framework introduced by <cite class="ltx_cite ltx_citemacro_citet">Gebru et al. [<a href="#bib.bib21" title="" class="ltx_ref">2021</a>]</cite>.</figcaption>
<table id="A2.T4.3" class="ltx_tabular">
<tr id="A2.T4.3.1" class="ltx_tr">
<td id="A2.T4.3.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="A2.T4.3.1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Motivation</span></td>
</tr>
<tr id="A2.T4.3.2" class="ltx_tr">
<td id="A2.T4.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.2.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.2.1.1.1.1" class="ltx_text ltx_font_bold">For what purpose was the dataset created?</span></span>
</span>
</td>
<td id="A2.T4.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.2.2.1.1" class="ltx_p" style="width:227.6pt;">The 4 datasets we presented in this paper were created for representation learning research. PUG: Animals is a strong dataset for OOD research as well as for being able to better probe the representation of vision models. PUG: ImageNet was designed as an additional benchmark for ImageNet pretrained model to offer a better comprehension of vision models capabilities in term of robustness to specific factor of variations. PUG: SPAR showcase how synthetic data can be used to evaluate VLMs understanding while PUG: AR4T can be leveraged to fine-tune them.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.3" class="ltx_tr">
<td id="A2.T4.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.3.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Who created the dataset and on behalf of which entity?</span></span>
</span>
</td>
<td id="A2.T4.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.3.2.1.1" class="ltx_p" style="width:227.6pt;">This dataset was created by the FAIR team at Meta AI.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.4" class="ltx_tr">
<td id="A2.T4.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.4.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.4.1.1.1.1" class="ltx_text ltx_font_bold">Who funded the creation of the dataset?</span></span>
</span>
</td>
<td id="A2.T4.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.4.2.1.1" class="ltx_p" style="width:227.6pt;">Meta.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.5" class="ltx_tr">
<td id="A2.T4.3.5.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.5.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Composition</span></td>
</tr>
<tr id="A2.T4.3.6" class="ltx_tr">
<td id="A2.T4.3.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.6.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.6.1.1.1.1" class="ltx_text ltx_font_bold">What do the instances that comprise the dataset represent?</span></span>
</span>
</td>
<td id="A2.T4.3.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.6.2.1.1" class="ltx_p" style="width:227.6pt;">The instances represent images of animals in various environment for PUG: Animals. In contrast PUG: ImageNet contains 151 object classes (the full list is available in Appendix <a href="#A2.SS3" title="B.3 PUG: ImageNet ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>). PUG: SPAR uses the sames assets as PUG: Animal while PUG: AR4T use the same objects as PUG: ImageNet.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.7" class="ltx_tr">
<td id="A2.T4.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.7.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.7.1.1.1.1" class="ltx_text ltx_font_bold">How many instances are there in total?</span></span>
</span>
</td>
<td id="A2.T4.3.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.7.2.1.1" class="ltx_p" style="width:227.6pt;"><span id="A2.T4.3.7.2.1.1.1" class="ltx_text ltx_font_bold">PUG: Animals</span>: 215 040 images; see Appendix <a href="#A2.SS2" title="B.2 PUG: Animals ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.7.2.1.1.2" class="ltx_text ltx_font_bold">PUG: ImageNet</span>: 88,328 images; see Appendix <a href="#A2.SS3" title="B.3 PUG: ImageNet ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>. 
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.7.2.1.1.3" class="ltx_text ltx_font_bold">PUG: SPAR</span>: 43,560 images; see Appendix
<a href="#A2.SS4" title="B.4 PUG: SPAR ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4</span></a>.

<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.7.2.1.1.4" class="ltx_text ltx_font_bold">PUG: AR4T</span>: 249,986 images for training and 23,216 test images; see Appendix
<a href="#A2.SS5" title="B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.5</span></a>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.8" class="ltx_tr">
<td id="A2.T4.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.8.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.8.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</span></span>
</span>
</td>
<td id="A2.T4.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.8.2.1.1" class="ltx_p" style="width:227.6pt;">PUG: Animals contains all possible combination of factors of variations. In contrast PUG: ImageNet was sampled by changing only 1 factor at a time and is therefore a random sample of the distribution. Images in PUG: SPAR were sampled using all possible combination of factors of variations (with the exception that for the attributes the blue or grass animal is always on the left). Image-text pairs in PUG: AR4T were randomly sampled.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.9" class="ltx_tr">
<td id="A2.T4.3.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.9.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.9.1.1.1.1" class="ltx_text ltx_font_bold">What data does each instance consist of?</span></span>
</span>
</td>
<td id="A2.T4.3.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.9.2.1.1" class="ltx_p" style="width:227.6pt;">For PUG: Animals, PUG: ImageNet, PUG: SPAR we release images along the factor of variation. For PUG: SPAR, we release the script to generate the captions from the factors of variations. For PUG: AR4T, we release images along with corresponding captions.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.10" class="ltx_tr">
<td id="A2.T4.3.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.10.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.10.1.1.1.1" class="ltx_text ltx_font_bold">Is there a label or target associated with each instance?</span></span>
</span>
</td>
<td id="A2.T4.3.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.10.2.1.1" class="ltx_p" style="width:227.6pt;">Yes, a csv file. Each instance have a row in this csv files with all the factors of variation used to generate this image. For PUG: Animals, the csv files contains the following columns:
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.10.2.1.1.1" class="ltx_text ltx_font_typewriter">filename, world_name, character_name, character_scale, camera_yaw, character_texture</span>

<br class="ltx_break">
<br class="ltx_break">while for PUG: ImageNet, it contains: 
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.10.2.1.1.2" class="ltx_text ltx_font_typewriter">filename, world_name, character_name, character_label, character_rotation_yaw,
character_rotation_roll, character_rotation_pitch, character_scale, camera_roll,
camera_pitch, camera_yaw, character_texture,
scene_light</span>.

<br class="ltx_break">
<br class="ltx_break">For PUG: SPAR, the csv contains: 
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.10.2.1.1.3" class="ltx_text ltx_font_typewriter">filename, world_name, character_name, character2_name, character1_pos, character2_pos, character_texture, character2_texture</span>

<br class="ltx_break">
<br class="ltx_break">For PUG: AR4T, the csv contains: 
<br class="ltx_break">
<br class="ltx_break"><span id="A2.T4.3.10.2.1.1.4" class="ltx_text ltx_font_typewriter">Relation, Actor1Category, Actor2Category, Actor1Name, Actor2Name, Actor1Location, Actor2Location, Actor1Rotation, Actor2Rotation, Actor1Scale, Actor2Scale, Actor1Texture, Actor2Texture, Actor1Attribute, Actor2Attribute, Camera_roll,
Camera_pitch, Camera_yaw, caption, alt_caption, Level, World.Name, filename, filename_neg, filepath</span></span>
</span>
</td>
</tr>
<tr id="A2.T4.3.11" class="ltx_tr">
<td id="A2.T4.3.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.11.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.11.1.1.1.1" class="ltx_text ltx_font_bold">Is any information missing from individual instances?</span></span>
</span>
</td>
<td id="A2.T4.3.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.11.2.1.1" class="ltx_p" style="width:227.6pt;">No, all relevant information is included.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.12" class="ltx_tr">
<td id="A2.T4.3.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.12.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.12.1.1.1.1" class="ltx_text ltx_font_bold">Are relationships between individual instances made explicit?</span></span>
</span>
</td>
<td id="A2.T4.3.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.12.2.1.1" class="ltx_p" style="width:227.6pt;">N/A.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.13" class="ltx_tr">
<td id="A2.T4.3.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.13.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.13.1.1.1.1" class="ltx_text ltx_font_bold">Are there recommended data splits?</span></span>
</span>
</td>
<td id="A2.T4.3.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.13.2.1.1" class="ltx_p" style="width:227.6pt;">There is no specific split concerning PUG: Animals because this dataset should be used for OOD research. We primarily let the researchers choose their own held out or training/validation/testing split to train their models. In contrast, PUG:ImageNet and PUG:SPAR should only be used as an additional test set. For PUG: AR4T, splits are described in Appendix <a href="#A2.SS5" title="B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.5</span></a>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.14" class="ltx_tr">
<td id="A2.T4.3.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.14.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.14.1.1.1.1" class="ltx_text ltx_font_bold">Are there any errors, sources of noise, or redundancies in the dataset?</span></span>
</span>
</td>
<td id="A2.T4.3.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.14.2.1.1" class="ltx_p" style="width:227.6pt;">We did not explicitly filter for occlusion, so some images may contain occluded views. PUG: Animals and PUG:SPAR are very clean and each animal is easily identifiable. In contrast, PUG: ImageNet and PUG: AR4T leverage assets from Sketchfab and the asset quality vary significantly.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.15" class="ltx_tr">
<td id="A2.T4.3.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.15.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.15.1.1.1.1" class="ltx_text ltx_font_bold">Is the dataset self-contained, or does it link to or otherwise rely on external resources?</span></span>
</span>
</td>
<td id="A2.T4.3.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.15.2.1.1" class="ltx_p" style="width:227.6pt;">The dataset is self-contained however the assets that were used to build the dataset belongs to external sources which are listed in the github at <a target="_blank" href="https://github.com/facebookresearch/PUG" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/PUG</a>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.16" class="ltx_tr">
<td id="A2.T4.3.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.16.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.16.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that might be considered confidential?</span></span>
</span>
</td>
<td id="A2.T4.3.16.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.16.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.17" class="ltx_tr">
<td id="A2.T4.3.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.17.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.17.1.1.1.1" class="ltx_text ltx_font_bold">Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</span></span>
</span>
</td>
<td id="A2.T4.3.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.17.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.18" class="ltx_tr">
<td id="A2.T4.3.18.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.18.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Collection</span></td>
</tr>
<tr id="A2.T4.3.19" class="ltx_tr">
<td id="A2.T4.3.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.19.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.19.1.1.1.1" class="ltx_text ltx_font_bold">How was the data associated with each instance acquired?</span></span>
</span>
</td>
<td id="A2.T4.3.19.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.19.2.1.1" class="ltx_p" style="width:227.6pt;">The data (3D assets) were acquired through the Unreal Engine Marketplace <a target="_blank" href="https://www.unrealengine.com/marketplace/en-US/store" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/marketplace/en-US/store</a> and Sketchfab <a target="_blank" href="https://sketchfab.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sketchfab.com/</a>. Assets were then incorporated into the Unreal Engine to generate realistic 3D scenes and corresponding images. The 3D assets were manually selected to ensure high quality.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.20" class="ltx_tr">
<td id="A2.T4.3.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.20.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.20.1.1.1.1" class="ltx_text ltx_font_bold">What mechanisms or procedures were used to collect the data?</span></span>
</span>
</td>
<td id="A2.T4.3.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.20.2.1.1" class="ltx_p" style="width:227.6pt;">Manual human curation. Assets were manually collected.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.21" class="ltx_tr">
<td id="A2.T4.3.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.21.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.21.1.1.1.1" class="ltx_text ltx_font_bold">If the dataset is a sample from a larger set, what was the sampling strategy?</span></span>
</span>
</td>
<td id="A2.T4.3.21.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.21.2.1.1" class="ltx_p" style="width:227.6pt;">For PUG: Animals and PUG: SPAR, all combinations are included. For PUG: ImageNet and PUG: AR4T, a random sample of possible combinations is provided.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.22" class="ltx_tr">
<td id="A2.T4.3.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.22.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.22.1.1.1.1" class="ltx_text ltx_font_bold">Who was involved in the data collection process and how were they compensated?</span></span>
</span>
</td>
<td id="A2.T4.3.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.22.2.1.1" class="ltx_p" style="width:227.6pt;">Only the authors of this work were involved.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.23" class="ltx_tr">
<td id="A2.T4.3.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.23.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.23.1.1.1.1" class="ltx_text ltx_font_bold">Over what timeframe was the data collected?</span></span>
</span>
</td>
<td id="A2.T4.3.23.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.23.2.1.1" class="ltx_p" style="width:227.6pt;">The data were collected between June 2022 and June 2023</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.24" class="ltx_tr">
<td id="A2.T4.3.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.24.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.24.1.1.1.1" class="ltx_text ltx_font_bold">Were any ethical review processes conducted?</span></span>
</span>
</td>
<td id="A2.T4.3.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.24.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.25" class="ltx_tr">
<td id="A2.T4.3.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.25.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.25.1.1.1.1" class="ltx_text ltx_font_bold">Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?</span></span>
</span>
</td>
<td id="A2.T4.3.25.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.25.2.1.1" class="ltx_p" style="width:227.6pt;">Third parties: Unreal Engine Marketplace <a target="_blank" href="https://www.unrealengine.com/marketplace/en-US/store" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/marketplace/en-US/store</a> and Sketchfab <a target="_blank" href="https://sketchfab.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sketchfab.com/</a>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.26" class="ltx_tr">
<td id="A2.T4.3.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.26.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.26.1.1.1.1" class="ltx_text ltx_font_bold">Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.</span></span>
</span>
</td>
<td id="A2.T4.3.26.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.26.2.1.1" class="ltx_p" style="width:227.6pt;">There is no personally identifiable information in our datasets as they are purely synthetic and contain no images of people. We purchased 3D assets from different marketplaces where required, however we did not explicitly contact the individual creators.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.27" class="ltx_tr">
<td id="A2.T4.3.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.27.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.27.1.1.1.1" class="ltx_text ltx_font_bold">Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.</span></span>
</span>
</td>
<td id="A2.T4.3.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.27.2.1.1" class="ltx_p" style="width:227.6pt;">N/A. See above.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.28" class="ltx_tr">
<td id="A2.T4.3.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.28.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.28.1.1.1.1" class="ltx_text ltx_font_bold">If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).</span></span>
</span>
</td>
<td id="A2.T4.3.28.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.28.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.28.2.1.1" class="ltx_p" style="width:227.6pt;">N/A. See above.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.29" class="ltx_tr">
<td id="A2.T4.3.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.29.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.29.1.1.1.1" class="ltx_text ltx_font_bold">Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.</span></span>
</span>
</td>
<td id="A2.T4.3.29.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.29.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.29.2.1.1" class="ltx_p" style="width:227.6pt;">No data about specific individuals is included in these data. See above.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.30" class="ltx_tr">
<td id="A2.T4.3.30.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.30.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Preprocessing</span></td>
</tr>
<tr id="A2.T4.3.31" class="ltx_tr">
<td id="A2.T4.3.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.31.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.31.1.1.1.1" class="ltx_text ltx_font_bold">Was any preprocessing/cleaning/labeling of the data done?</span></span>
</span>
</td>
<td id="A2.T4.3.31.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.31.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.31.2.1.1" class="ltx_p" style="width:227.6pt;">N/A.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.32" class="ltx_tr">
<td id="A2.T4.3.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.32.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.32.1.1.1.1" class="ltx_text ltx_font_bold">Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data?</span></span>
</span>
</td>
<td id="A2.T4.3.32.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.32.2.1.1" class="ltx_p" style="width:227.6pt;">N/A.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.33" class="ltx_tr">
<td id="A2.T4.3.33.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.33.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.33.1.1.1.1" class="ltx_text ltx_font_bold">Is the software that was used to preprocess/clean/label the data available?</span></span>
</span>
</td>
<td id="A2.T4.3.33.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.33.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.33.2.1.1" class="ltx_p" style="width:227.6pt;">N/A.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.34" class="ltx_tr">
<td id="A2.T4.3.34.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.34.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Uses</span></td>
</tr>
<tr id="A2.T4.3.35" class="ltx_tr">
<td id="A2.T4.3.35.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.35.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.35.1.1.1.1" class="ltx_text ltx_font_bold">Has the dataset been used for any tasks already?</span></span>
</span>
</td>
<td id="A2.T4.3.35.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.35.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.35.2.1.1" class="ltx_p" style="width:227.6pt;">Yes, these data were used for the experiments that were presented in this paper.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.36" class="ltx_tr">
<td id="A2.T4.3.36.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.36.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.36.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.36.1.1.1.1" class="ltx_text ltx_font_bold">Is there a repository that links to any or all papers or systems that use the dataset?</span></span>
</span>
</td>
<td id="A2.T4.3.36.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.36.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.36.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.37" class="ltx_tr">
<td id="A2.T4.3.37.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.37.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.37.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.37.1.1.1.1" class="ltx_text ltx_font_bold">What (other) tasks could the dataset be used for?</span></span>
</span>
</td>
<td id="A2.T4.3.37.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.37.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.37.2.1.1" class="ltx_p" style="width:227.6pt;">These Datasets could be used widely for evaluating and training neural networks. For example, assessing disentanglement of models with respect to PUG: Animals factors of variation (e.g. with DCI metric <cite class="ltx_cite ltx_citemacro_citet">Eastwood and Williams [<a href="#bib.bib16" title="" class="ltx_ref">2018</a>]</cite>).</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.38" class="ltx_tr">
<td id="A2.T4.3.38.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.38.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.38.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.38.1.1.1.1" class="ltx_text ltx_font_bold">Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</span></span>
</span>
</td>
<td id="A2.T4.3.38.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.38.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.38.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.39" class="ltx_tr">
<td id="A2.T4.3.39.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.39.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.39.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.39.1.1.1.1" class="ltx_text ltx_font_bold">Are there tasks for which the dataset should not be used?</span></span>
</span>
</td>
<td id="A2.T4.3.39.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.39.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.39.2.1.1" class="ltx_p" style="width:227.6pt;">These datasets <span id="A2.T4.3.39.2.1.1.1" class="ltx_text ltx_font_bold">should not be used</span> for generative modelling purposes.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.40" class="ltx_tr">
<td id="A2.T4.3.40.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.40.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Distribution</span></td>
</tr>
<tr id="A2.T4.3.41" class="ltx_tr">
<td id="A2.T4.3.41.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.41.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.41.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.41.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?</span></span>
</span>
</td>
<td id="A2.T4.3.41.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.41.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.41.2.1.1" class="ltx_p" style="width:227.6pt;">Yes, the dataset will be publicly distributed.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.42" class="ltx_tr">
<td id="A2.T4.3.42.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.42.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.42.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.42.1.1.1.1" class="ltx_text ltx_font_bold">How will the dataset will be distributed?</span></span>
</span>
</td>
<td id="A2.T4.3.42.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.42.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.42.2.1.1" class="ltx_p" style="width:227.6pt;">Tarball on a website.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.43" class="ltx_tr">
<td id="A2.T4.3.43.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.43.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.43.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.43.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?</span></span>
</span>
</td>
<td id="A2.T4.3.43.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.43.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.43.2.1.1" class="ltx_p" style="width:227.6pt;">The license of the dataset is <span id="A2.T4.3.43.2.1.1.1" class="ltx_text ltx_font_bold">cc-by-nc with the mention that these data should not be used for generative AI purposes</span>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.44" class="ltx_tr">
<td id="A2.T4.3.44.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.44.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.44.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.44.1.1.1.1" class="ltx_text ltx_font_bold">Have any third parties imposed IP-based or other restrictions on the data associated with the instances?</span></span>
</span>
</td>
<td id="A2.T4.3.44.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.44.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.44.2.1.1" class="ltx_p" style="width:227.6pt;">See <cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib17" title="" class="ltx_ref">EpicGames </a></cite>.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.45" class="ltx_tr">
<td id="A2.T4.3.45.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.45.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.45.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.45.1.1.1.1" class="ltx_text ltx_font_bold">Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?</span></span>
</span>
</td>
<td id="A2.T4.3.45.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.45.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.45.2.1.1" class="ltx_p" style="width:227.6pt;">N/A</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.46" class="ltx_tr">
<td id="A2.T4.3.46.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2"><span id="A2.T4.3.46.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Maintenance</span></td>
</tr>
<tr id="A2.T4.3.47" class="ltx_tr">
<td id="A2.T4.3.47.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.47.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.47.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.47.1.1.1.1" class="ltx_text ltx_font_bold">Who will be supporting/hosting/maintaining the dataset?</span></span>
</span>
</td>
<td id="A2.T4.3.47.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.47.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.47.2.1.1" class="ltx_p" style="width:227.6pt;">Meta AI.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.48" class="ltx_tr">
<td id="A2.T4.3.48.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.48.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.48.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.48.1.1.1.1" class="ltx_text ltx_font_bold">How can the owner/curator/manager of the dataset be contacted?</span></span>
</span>
</td>
<td id="A2.T4.3.48.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.48.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.48.2.1.1" class="ltx_p" style="width:227.6pt;">Please contact the corresponding author of this paper.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.49" class="ltx_tr">
<td id="A2.T4.3.49.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.49.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.49.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.49.1.1.1.1" class="ltx_text ltx_font_bold">Is there an erratum?</span></span>
</span>
</td>
<td id="A2.T4.3.49.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.49.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.49.2.1.1" class="ltx_p" style="width:227.6pt;">No.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.50" class="ltx_tr">
<td id="A2.T4.3.50.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.50.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.50.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.50.1.1.1.1" class="ltx_text ltx_font_bold">Will the dataset be updated?</span></span>
</span>
</td>
<td id="A2.T4.3.50.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.50.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.50.2.1.1" class="ltx_p" style="width:227.6pt;">Yes the dataset will be updated with versioning.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.51" class="ltx_tr">
<td id="A2.T4.3.51.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.51.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.51.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.51.1.1.1.1" class="ltx_text ltx_font_bold">If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.</span></span>
</span>
</td>
<td id="A2.T4.3.51.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.51.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.51.2.1.1" class="ltx_p" style="width:227.6pt;">N/A.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.52" class="ltx_tr">
<td id="A2.T4.3.52.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.52.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.52.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.52.1.1.1.1" class="ltx_text ltx_font_bold">Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.</span></span>
</span>
</td>
<td id="A2.T4.3.52.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A2.T4.3.52.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.52.2.1.1" class="ltx_p" style="width:227.6pt;">It depends. If the dataset is updated because one of the asset creators has requested to remove their assets, we will not continue to host the dataset containing these assets. Only the newer version of the dataset which will not contain these assets will be available.</span>
</span>
</td>
</tr>
<tr id="A2.T4.3.53" class="ltx_tr">
<td id="A2.T4.3.53.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_l ltx_border_r ltx_border_t">
<span id="A2.T4.3.53.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.53.1.1.1" class="ltx_p" style="width:170.7pt;"><span id="A2.T4.3.53.1.1.1.1" class="ltx_text ltx_font_bold">If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?</span></span>
</span>
</td>
<td id="A2.T4.3.53.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A2.T4.3.53.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T4.3.53.2.1.1" class="ltx_p" style="width:227.6pt;">No mechanisms are in place yet, but they can contact the authors of this paper if they would like to contribute.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>PUG: Animals</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.1" class="ltx_p">PUG Animals contains 215 040 pre-rendered images using 70 animals assets, 64 backgrounds, 3 sizes, 4 textures, under 4 different camera orientations. To create PUG: Animals, we use Animals assets from the following bundle in the Epic Game Marketplace (
<a target="_blank" href="https://www.unrealengine.com/marketplace/en-US/product/complete-animals/reviews" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.unrealengine.com/marketplace/en-US/product/complete-animals/reviews</a>). The list of environments used can be found in the dataset folder or at <a target="_blank" href="https://github.com/facebookresearch/PUG" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/PUG</a>. Below, we list all the values for the factors of variation, we have used:</p>
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">World_Name</span> : ["Egypt", "Desert", "AmusementPark", "ArcadeClub", "Arena", "Battleground", "Catacombs", "Tableland", "EuropeanStreet",
"JunkYard", "OceanFloor", "Racetrack", "Ruins", "SciFiCity",
"SciFiGarage", "SpaceIsland", "SpaceHangar", "SpatialStation", "TokyoDay", "TokyoNight", "TrainStation",
"Bridge", "Beach", "BusStationInterior", "BusStationExterior", "Subway", "IndoorStairs", "Bar", "ScreeningCheckpoint",
"Circus", "Appartment", "Hallway", "TrashRoom", "FuturisticSubway", "Footbridge", "BoxingRing",
"Hangar", "Mansion", "ShoppingMall",
"ConferenceRoom", "SpacePort",
"VillageOutskirt","VillageSquare","Courtyard", "ElvenRuins", "Forge",
"Library", "Museum", "Gallery", "ModernGallery",
"Opera", "AncientAgora", "Restaurant", "RuralAustralia", "AustralianRoad",
"ShadyRoad", "SaltFlats", "Castle", "StylizedEgypt",
"Temple", "Snow", "Grass", "DryGrass", "Forest"],</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Character_Name</span> : ["Goldfish","Caribou","Elephant","Camel","Penguin","Cassowary","Zebra",
"Turtle","Bear","Beaver","Capybara","Crocodile","Armadillo","Cat","Gecko","Crow","GiantAnteater",
"GiantTortoise","KomodoDragon","Rhinoceros","Dolphin","EarlessSeal","FruitBat","Goat","Hippopotamus",
"Horse","Impala","Lion","Orca","Pig","Rabbit","Squirrel","Tapir","Wildbeest","Wolf","Anlylosaurus",
"BlackRockFish","Parasaurolophus","PoisonDartFrog","Spinosaurus","Triceraptos","Chicken",
"HarpyEagle","Ostrich","Raven","RedCrownedCrane","Robin","Seagull","Secretarybird","Shoebill","Swan",
"Toucan","Vulture","Ammonite","Ant","Scorpion","GoldBeetle","Hornet","SnowCrab","Tarantula","WhiteShark",
"Tuna","Arowana", "Ayu", "Betta", "Koi", "Pirarucu", "Salmon", "Cattle", "Jerboa"],</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Character_Scale</span> : [0.7, 1.0, 1.3],</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Camera_Yaw</span> : [0, 45, 225, 315],</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Character_Texture</span> : ["Default", "Sky", "Grass", "Asphalt"]</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS2.p2.1" class="ltx_p">PUG: Animals is built by using all combinations of the factor of variation above. In Figure <a href="#A2.F7" title="Figure 7 ‣ B.2 PUG: Animals ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we show random images from the PUG: animal dataset that highlight the diversity of this dataset.</p>
</div>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2308.03977/assets/x9.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="311" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Random images taken from the PUG: Animals dataset.</figcaption>
</figure>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib41" title="" class="ltx_ref">2023</a>]</cite>, we present a comparison in Table <a href="#A2.T5" title="Table 5 ‣ B.2 PUG: Animals ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with other datasets that are often used in OOD research:</p>
</div>
<figure id="A2.T5" class="ltx_table">
<div id="A2.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:28.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-480.4pt,48.8pt) scale(0.310961934187844,0.22581542484344) ;">
<table id="A2.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T5.1.1.1.1" class="ltx_tr">
<th id="A2.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="A2.T5.1.1.1.1.1.1" class="ltx_text">Image Data</span></th>
<td id="A2.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Colored MNIST</span></td>
<td id="A2.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">MNIST-R</span></td>
<td id="A2.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Waterbirds</span></td>
<td id="A2.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Biased-Cars</span></td>
<td id="A2.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Nico++</span></td>
<td id="A2.T5.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A2.T5.1.1.1.1.7.1" class="ltx_text ltx_font_bold">PUG: Animals</span></td>
</tr>
<tr id="A2.T5.1.1.2.2" class="ltx_tr">
<td id="A2.T5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Arjovsky et al. [<a href="#bib.bib3" title="" class="ltx_ref">2020</a>]</cite></td>
<td id="A2.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Ghifary et al. [<a href="#bib.bib22" title="" class="ltx_ref">2015</a>]</cite></td>
<td id="A2.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Sagawa* et al. [<a href="#bib.bib49" title="" class="ltx_ref">2020</a>]</cite></td>
<td id="A2.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Madan et al. [<a href="#bib.bib44" title="" class="ltx_ref">2022</a>]</cite></td>
<td id="A2.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite">Xingxuan Zhang [<a href="#bib.bib67" title="" class="ltx_ref">2022</a>]</cite></td>
<td id="A2.T5.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Ours</td>
</tr>
<tr id="A2.T5.1.1.3.3" class="ltx_tr">
<th id="A2.T5.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># Domains</th>
<td id="A2.T5.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="A2.T5.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="A2.T5.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="A2.T5.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="A2.T5.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="A2.T5.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64</td>
</tr>
<tr id="A2.T5.1.1.4.4" class="ltx_tr">
<th id="A2.T5.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># Categories</th>
<td id="A2.T5.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="A2.T5.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td id="A2.T5.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="A2.T5.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="A2.T5.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80</td>
<td id="A2.T5.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70</td>
</tr>
<tr id="A2.T5.1.1.5.5" class="ltx_tr">
<th id="A2.T5.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># Examples</th>
<td id="A2.T5.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="A2.T5.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6k</td>
<td id="A2.T5.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.8k</td>
<td id="A2.T5.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">450k</td>
<td id="A2.T5.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">230k</td>
<td id="A2.T5.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">215k</td>
</tr>
<tr id="A2.T5.1.1.6.6" class="ltx_tr">
<th id="A2.T5.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Shift Type</th>
<td id="A2.T5.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Color</td>
<td id="A2.T5.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Angle</td>
<td id="A2.T5.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Background</td>
<td id="A2.T5.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Views</td>
<td id="A2.T5.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Background</td>
<td id="A2.T5.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Back./Text./Size./View</td>
</tr>
<tr id="A2.T5.1.1.7.7" class="ltx_tr">
<th id="A2.T5.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Image Type</th>
<td id="A2.T5.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Digits</td>
<td id="A2.T5.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Digits</td>
<td id="A2.T5.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Birds</td>
<td id="A2.T5.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Synthetic Cars</td>
<td id="A2.T5.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Real Objects</td>
<td id="A2.T5.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Synthetic Animals</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparing PUG: Animals with other datasets traditionally used for OOD research. In contrast to other datasets that have variations across only a single domain, that have noisy annotations or that are to unrealistic, PUG: Animal over high quality images with reliable annotations across different domains such as the background,texture,size and view.</figcaption>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>PUG: ImageNet</h3>

<div id="A2.SS3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.p1.1" class="ltx_p">PUG: ImageNet contains 88,328 pre-rendered images using 724 assets representing 151 ImageNet classes with 64 backgrounds, 7 sizes, 10 textures, 18 different camera orientation, 18 different character orientation and 7 light intensity. Below is the values we used for each of these factors:</p>
</div>
<div id="A2.SS3.p2" class="ltx_para ltx_noindent">
<ul id="A2.I2" class="ltx_itemize">
<li id="A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i1.p1" class="ltx_para">
<p id="A2.I2.i1.p1.1" class="ltx_p"><span id="A2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">World_Name</span> : ["Egypt", "Desert", "AmusementPark", "ArcadeClub", "Arena", "Battleground", "Catacombs", "Tableland", "EuropeanStreet",
"JunkYard", "OceanFloor", "Racetrack", "Ruins", "SciFiCity",
"SciFiGarage", "SpaceIsland", "SpaceHangar", "SpatialStation", "TokyoDay", "TokyoNight", "TrainStation",
"Bridge", "Beach", "BusStationInterior", "BusStationExterior", "Subway", "IndoorStairs", "Bar", "ScreeningCheckpoint",
"Circus", "Appartment", "Hallway", "TrashRoom", "FuturisticSubway", "Footbridge", "BoxingRing",
"Hangar", "Mansion", "ShoppingMall",
"ConferenceRoom", "SpacePort",
"VillageOutskirt","VillageSquare","Courtyard", "ElvenRuins", "Forge",
"Library", "Museum", "Gallery", "ModernGallery",
"Opera", "AncientAgora", "Restaurant", "RuralAustralia", "AustralianRoad",
"ShadyRoad", "SaltFlats", "Castle", "StylizedEgypt",
"Temple", "Snow", "Grass", "DryGrass", "Forest"],</p>
</div>
</li>
<li id="A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i2.p1" class="ltx_para">
<p id="A2.I2.i2.p1.1" class="ltx_p"><span id="A2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Character_Name</span> : 724 Sketchfab assets (See github for the list)</p>
</div>
</li>
<li id="A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i3.p1" class="ltx_para">
<p id="A2.I2.i3.p1.1" class="ltx_p"><span id="A2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Character_Rotation_Yaw</span> : [0, 45, 135, 180, 225, 270],</p>
</div>
</li>
<li id="A2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i4.p1" class="ltx_para">
<p id="A2.I2.i4.p1.1" class="ltx_p"><span id="A2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Character_Rotation_Roll</span> : [45, 90, 135, 180, 225, 270],</p>
</div>
</li>
<li id="A2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i5.p1" class="ltx_para">
<p id="A2.I2.i5.p1.1" class="ltx_p"><span id="A2.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Character_Rotation_Pitch</span> : [45, 90, 135, 180, 225, 270],</p>
</div>
</li>
<li id="A2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i6.p1" class="ltx_para">
<p id="A2.I2.i6.p1.1" class="ltx_p"><span id="A2.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">Character_Scale</span> : [0.5, 0.6, 0.7, 0.8, 1.3, 1.6],</p>
</div>
</li>
<li id="A2.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i7.p1" class="ltx_para">
<p id="A2.I2.i7.p1.1" class="ltx_p"><span id="A2.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">Camera_Roll</span> : [45, 90, 135, 180, 225, 270],</p>
</div>
</li>
<li id="A2.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i8.p1" class="ltx_para">
<p id="A2.I2.i8.p1.1" class="ltx_p"><span id="A2.I2.i8.p1.1.1" class="ltx_text ltx_font_bold">Camera_Pitch</span> : [240, 260, 280, 300, 320, 340],</p>
</div>
</li>
<li id="A2.I2.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i9.p1" class="ltx_para">
<p id="A2.I2.i9.p1.1" class="ltx_p"><span id="A2.I2.i9.p1.1.1" class="ltx_text ltx_font_bold">Camera_Yaw</span> : [0, 45, 135, 180, 225, 270],</p>
</div>
</li>
<li id="A2.I2.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i10.p1" class="ltx_para">
<p id="A2.I2.i10.p1.1" class="ltx_p"><span id="A2.I2.i10.p1.1.1" class="ltx_text ltx_font_bold">Character_Texture</span> : ["Default", "Sky", "Green", "Gray", "Red", "Grass", "Color", "Black", "Curtain"],,</p>
</div>
</li>
<li id="A2.I2.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i11.p1" class="ltx_para ltx_noindent">
<p id="A2.I2.i11.p1.1" class="ltx_p"><span id="A2.I2.i11.p1.1.1" class="ltx_text ltx_font_bold">Scene_Light</span> : ["255,255,255,0","0,0,255,0", "0,255,0,0", "255,0,0,0", "0,255,255,0", "255,0,255,0", "255,255,0,0"] (The value for the lights are in RGBA format)</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS3.p3" class="ltx_para ltx_noindent">
<p id="A2.SS3.p3.1" class="ltx_p">To generate PUG:ImageNet, we change only one factor at a time for each assets. When changing the background (World_Name), all the other factors (Camera/Object Orientation, Size, Texture, Light) are at 0 or at their default value. When changing the other factors (Camera/Object Orientation, Size, Texture), the background is set to "SaltFlats_0" (Which is the most <span id="A2.SS3.p3.1.1" class="ltx_text ltx_font_italic">basic</span> background). When changing the light of the scene, we have used the environment "Opera".</p>
</div>
<div id="A2.SS3.p4" class="ltx_para ltx_noindent">
<p id="A2.SS3.p4.1" class="ltx_p">The assets in PUG:ImageNet were selected base on 151 ImageNet classes which are listed below:
<br class="ltx_break">[’BirdHouse’, ’Chest’, ’Bagel’, ’WarPlane’, ’Rocking_Chair’, ’Bridge’, ’Street_Sign’, ’Cabbage’, ’Pay_Phone’, ’Butternut_Squash’, ’JellyFish’, ’Jack_O_Lantern’, ’Bookcase’, ’Stonewall’, ’Punching_Bag’, ’Toaster’, ’Mushroom’, ’Frog’, ’Jeep’, ’Television’, ’Pineapple’, ’Vacuum’, ’Torch’, ’Carousel’, ’Desk’, ’WineBottle’, ’Wallet’, ’Dining_Table’, ’Military_uniform’, ’Car_Wheel’, ’Table_Lamb’, ’Digital_Watch’, ’Electric_Fan’, ’Sweatshirt’, ’Komodo_dragon’, ’Racket’, ’Cheeseburger’, ’Can_Opener’, ’Pomegranate’, ’Convertible’, ’Laptop’, ’Chicken_hen’, ’Wolf’, ’Bulletproof_vest’, ’Shield’, ’Bathtub’, ’Throne’, ’Lighter’, ’Bycicle’, ’Cofee_Mug’, ’Motor_Scooter’, ’Jean’, ’Soccer_Ball’, ’Vending_machine’, ’Hatchet’, ’Umbrella’, ’Bear’, ’Artichoke’, ’Vase’, ’Radiator’, ’SpaceShuttle’, ’Manhole_Cover’, ’Polaroid_Camera’, ’Traffic_Light’, ’Radio’, ’Soup_Bowl’, ’Zucchini’, ’Barrel’, ’Tennis_Ball’, ’Sunglasses’, ’Microwave’, ’Joystick’, ’Aircraft_Carrier’, ’Fox’, ’Submarine’, ’BasketBall’, ’Running_Shoe’, ’Chain-saw’, ’Piano’, ’Crate’, ’Loupe’, ’Minivan’, ’Shirt’, ’Remote_controler’, ’Airliner’, ’Sock’, ’Shovel’, ’Mask’, ’Tractor’, ’Sandal’, ’Wooden_Spoon’, ’Drum’, ’Goldfish’, ’Gasmask’, ’Mailbox’, ’Volley_Ball’, ’Banana’, ’Penguin’, ’Sliding_Door’, ’Pool_Table’, ’Burrito’, ’Candle’, ’Purse’, ’Canoe’, ’Typewriter_Keyboard’, ’Espresso_maker’, ’Carton’, ’Park_Bench’, ’Screen’, ’African_crocodile’, ’Cat’, ’Hay’, ’Elephant’, ’WaterBottle’, ’Modem’, ’Palace’, ’Ice_Cream’, ’Washer’, ’Sewing_Machine’, ’HairDryer’, ’Rabbit’, ’Dishwasher’, ’Bell_Pepper’, ’Ambulance’, ’French_Loaf’, ’Refrigerator’, ’Mouse’, ’Obelisk’, ’Starfish’, ’Brocolli’, ’Microphone’, ’Great_white_shark’, ’Power-drill’, ’Locomotive’, ’Perfume’, ’Whale’, ’Screwdriver’, ’Dial_telephone’, ’Backpack’, ’Harmonica’, ’Binocular’, ’Skirt’, ’Pizza’, ’Cowboy_Hat’, ’Computer_Keyboard’, ’Kangarou’, ’Baseball’, ’Tile_Roof’, ’Lawn_Mower’, ’Safe’, ’Cellular_telephone’]</p>
</div>
<div id="A2.SS3.p5" class="ltx_para ltx_noindent">
<p id="A2.SS3.p5.1" class="ltx_p">In Figure <a href="#A2.F8" title="Figure 8 ‣ B.3 PUG: ImageNet ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show random images taken from the PUG: ImageNet dataset.</p>
</div>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2308.03977/assets/x10.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="311" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Random images taken from the PUG: ImageNet dataset.</figcaption>
</figure>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>PUG: SPAR</h3>

<div id="A2.SS4.p1" class="ltx_para ltx_noindent">
<p id="A2.SS4.p1.1" class="ltx_p">PUG: SPAR contains 43,560 pre-rendered images using 32 animals assets, 10 backgrounds, 4 positions and 4 textures. In contrast with the other PUG datsets, PUG: SPAR contain up to two animal in a single scene. For generating the PUG: SPAR dataset, we utilize the same subset of assets as PUG: Animals.</p>
</div>
<div id="A2.SS4.p2" class="ltx_para ltx_noindent">
<ul id="A2.I3" class="ltx_itemize">
<li id="A2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I3.i1.p1" class="ltx_para">
<p id="A2.I3.i1.p1.1" class="ltx_p"><span id="A2.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">World_Name</span> : [
’Desert’, ’Arena’, ’OceanFloor’, ’Racetrack’, ’TokyoDay’, ’Circus’, ’BoxingRing’, ’AustralianRoad’, ’SaltFlats’, ’Museum’],</p>
</div>
</li>
<li id="A2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I3.i2.p1" class="ltx_para">
<p id="A2.I3.i2.p1.1" class="ltx_p"><span id="A2.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Character_Name</span> : [’Goldfish’, ’Caribou’, ’Elephant’, ’Camel’, ’Penguin’, ’Zebra’, ’Bear’, ’Beaver’, ’Cattle’, ’Armadillo’, ’Gecko’, ’Crow’, ’Scorpion’, ’GiantTortoise’, ’Tarantula’, ’Rhinoceros’, ’Dolphin’, ’EarlessSeal’, ’Goat’, ’Hippopotamus’, ’Horse’, ’Impala’, ’Lion’, ’Orca’, ’Pig’, ’Rabbit’, ’Squirrel’, ’Chicken’, ’WhiteShark’, ’Anlylosaurus’, ’BlackRockFish’, ’PoisonDartFrog’],</p>
</div>
</li>
<li id="A2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I3.i3.p1" class="ltx_para">
<p id="A2.I3.i3.p1.1" class="ltx_p"><span id="A2.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Character_pos</span> : ["Left/Right", "Bottom/Top"]</p>
</div>
</li>
<li id="A2.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I3.i4.p1" class="ltx_para ltx_noindent">
<p id="A2.I3.i4.p1.1" class="ltx_p"><span id="A2.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Character_Texture</span> : ["Default", "Blue/Red", "Grass/Stone"]</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS4.p3" class="ltx_para ltx_noindent">
<p id="A2.SS4.p3.1" class="ltx_p">In Figure <a href="#A2.F8" title="Figure 8 ‣ B.3 PUG: ImageNet ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we show random images taken from the PUG: SPAR.</p>
</div>
<figure id="A2.F9" class="ltx_figure"><img src="/html/2308.03977/assets/x11.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="313" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Random images taken from the PUG: SPAR dataset.</figcaption>
</figure>
</section>
<section id="A2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>PUG: AR4T</h3>

<section id="A2.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Assets and Environments</h5>

<div id="A2.SS5.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px1.p1.1" class="ltx_p">For generating the PUG: AR4T dataset, we utilize the same subset of Sketchfab assets as used in PUG: ImageNet. This leaves us with a set of 680 unique assets chosen from across 151 ImageNet categories, each manually inspected to quality control for photo-realism as much as possible. For this PUG dataset we are primarily concerned with object-object and object-attribute information, hence we utilize single camera and character orientation. Since Sketchfab assets differs widely in their scales, we first scaled down the longest edge of the asset bounding box to 150 pixels to normalize the order of magnitude of the asset dimensions, before any further scaling based on attributes. Next, we select a total of 26 unique environments as our background environments. The richness of some environments enables us to manually select different camera views in each environment as a novel environment view, and we generate a total of 28 visually unique backgrounds for our PUG: AR4T dataset. We provide each background with human-intelligible descriptive names for use in the dataset captions (Table <a href="#A2.T6" title="Table 6 ‣ Assets and Environments ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The PUG: AR4T dataset is composed with two subset described in the following sections.</p>
</div>
<figure id="A2.T6" class="ltx_table">
<div id="A2.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:207pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.5pt,43.2pt) scale(0.705385801674076,0.705385801674076) ;">
<table id="A2.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.1.1.1.1" class="ltx_tr">
<th id="A2.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<table id="A2.T6.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T6.1.1.1.1.1.1.1" class="ltx_tr">
<td id="A2.T6.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Environment Name</span></td>
</tr>
<tr id="A2.T6.1.1.1.1.1.1.2" class="ltx_tr">
<td id="A2.T6.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">(with camera view variant)</span></td>
</tr>
</table>
</th>
<th id="A2.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A2.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Descriptive Caption for PUG</span></th>
<th id="A2.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="A2.T6.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T6.1.1.1.1.3.1.1" class="ltx_tr">
<td id="A2.T6.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Environment Name</span></td>
</tr>
<tr id="A2.T6.1.1.1.1.3.1.2" class="ltx_tr">
<td id="A2.T6.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T6.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">(with camera view variant)</span></td>
</tr>
</table>
</th>
<th id="A2.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A2.T6.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Descriptive Caption for PUG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.1.1.2.1" class="ltx_tr">
<td id="A2.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Arena</td>
<td id="A2.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">arena</td>
<td id="A2.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IceRoad</td>
<td id="A2.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">icy road</td>
</tr>
<tr id="A2.T6.1.1.3.2" class="ltx_tr">
<td id="A2.T6.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">VillageOutskirt</td>
<td id="A2.T6.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">village</td>
<td id="A2.T6.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">Jungle</td>
<td id="A2.T6.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">jungle</td>
</tr>
<tr id="A2.T6.1.1.4.3" class="ltx_tr">
<td id="A2.T6.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">VillageSquare</td>
<td id="A2.T6.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">village</td>
<td id="A2.T6.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">Library</td>
<td id="A2.T6.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">library</td>
</tr>
<tr id="A2.T6.1.1.5.4" class="ltx_tr">
<td id="A2.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Battleground</td>
<td id="A2.T6.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">battleground</td>
<td id="A2.T6.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">Museum</td>
<td id="A2.T6.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">museum</td>
</tr>
<tr id="A2.T6.1.1.6.5" class="ltx_tr">
<td id="A2.T6.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Beach</td>
<td id="A2.T6.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">beach</td>
<td id="A2.T6.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">OceanFloor</td>
<td id="A2.T6.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">ocean floor</td>
</tr>
<tr id="A2.T6.1.1.7.6" class="ltx_tr">
<td id="A2.T6.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Bridge</td>
<td id="A2.T6.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">bridge</td>
<td id="A2.T6.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">AncientAgora</td>
<td id="A2.T6.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">castle outskirts</td>
</tr>
<tr id="A2.T6.1.1.8.7" class="ltx_tr">
<td id="A2.T6.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Circus</td>
<td id="A2.T6.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">circus</td>
<td id="A2.T6.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">Racetrack</td>
<td id="A2.T6.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">race track</td>
</tr>
<tr id="A2.T6.1.1.9.8" class="ltx_tr">
<td id="A2.T6.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Cliff</td>
<td id="A2.T6.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">cliff</td>
<td id="A2.T6.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">RuralAustralia</td>
<td id="A2.T6.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">rural wilderness</td>
</tr>
<tr id="A2.T6.1.1.10.9" class="ltx_tr">
<td id="A2.T6.1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Courtyard</td>
<td id="A2.T6.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r">courtyard</td>
<td id="A2.T6.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r">AustralianRoad</td>
<td id="A2.T6.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r">desert road</td>
</tr>
<tr id="A2.T6.1.1.11.10" class="ltx_tr">
<td id="A2.T6.1.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Egypt</td>
<td id="A2.T6.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r">egypt</td>
<td id="A2.T6.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r">SaltFlats</td>
<td id="A2.T6.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r">salt flats</td>
</tr>
<tr id="A2.T6.1.1.12.11" class="ltx_tr">
<td id="A2.T6.1.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ElvenRuins</td>
<td id="A2.T6.1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r">ruins</td>
<td id="A2.T6.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r">SpaceIsland</td>
<td id="A2.T6.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r">space</td>
</tr>
<tr id="A2.T6.1.1.13.12" class="ltx_tr">
<td id="A2.T6.1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">EuropeanStreet</td>
<td id="A2.T6.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r">european street</td>
<td id="A2.T6.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r">StylizedEgypt</td>
<td id="A2.T6.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r">egypt</td>
</tr>
<tr id="A2.T6.1.1.14.13" class="ltx_tr">
<td id="A2.T6.1.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">FightingArena</td>
<td id="A2.T6.1.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r">fighting arena</td>
<td id="A2.T6.1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r">Temple</td>
<td id="A2.T6.1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r">temple</td>
</tr>
<tr id="A2.T6.1.1.15.14" class="ltx_tr">
<td id="A2.T6.1.1.15.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Forge</td>
<td id="A2.T6.1.1.15.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">forge</td>
<td id="A2.T6.1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">TrainStation</td>
<td id="A2.T6.1.1.15.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">train station</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Environments and descriptive captions used in PUG: Attributes &amp; Relations</figcaption>
</figure>
</section>
<section id="A2.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">PUG: AR4T-Relations</h5>

<div id="A2.SS5.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px2.p1.1" class="ltx_p">For generating the PUG: AR4T-Relations subset, we utilize the spatial relationships from Visual Genome that are not symmetric, as noted in the ARO benchmark <cite class="ltx_cite ltx_citemacro_cite">Yuksekgonul et al. [<a href="#bib.bib68" title="" class="ltx_ref">2023</a>]</cite>. The set of relationships used in PUG: AR4T-Relations is given in Table <a href="#A2.T7" title="Table 7 ‣ PUG: AR4T-Relations ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. It consists of three unary relations (<span id="A2.SS5.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">at, in, inside</span>) and ten binary relations (<span id="A2.SS5.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">above, on, on top of, behind, in front of, below, beneath, under, to the left of, to the right of</span>.) For each relation, the corresponding objects are picked randomly from the set of assets, and placed in the scene based on the co-ordinates given in Table <a href="#A2.T7" title="Table 7 ‣ PUG: AR4T-Relations ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We add a random offset in the range [0-25] along each dimension for every individual object for every scene, so that the dataset does not contain shortcuts where the object locations can directly inform the underlying relation. The size of each asset is chosen randomly on a scale of 1-10, where 1 corresponds to 110 pixels and 10 to 200 pixels for the longest edge of the scaled asset.</p>
</div>
<figure id="A2.T7" class="ltx_table">
<div id="A2.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:234.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.6pt,20.6pt) scale(0.850237133542351,0.850237133542351) ;">
<table id="A2.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T7.1.1.1.1" class="ltx_tr">
<th id="A2.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="A2.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Relation</span></th>
<th id="A2.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="A2.T7.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T7.1.1.1.1.2.1.1" class="ltx_tr">
<td id="A2.T7.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Object 1 coordinates</span></td>
</tr>
<tr id="A2.T7.1.1.1.1.2.1.2" class="ltx_tr">
<td id="A2.T7.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">(wrt origin)</span></td>
</tr>
</table>
</th>
<th id="A2.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="A2.T7.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T7.1.1.1.1.3.1.1" class="ltx_tr">
<td id="A2.T7.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Object 2 coordinates</span></td>
</tr>
<tr id="A2.T7.1.1.1.1.3.1.2" class="ltx_tr">
<td id="A2.T7.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T7.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">(wrt origin)</span></td>
</tr>
</table>
</th>
<th id="A2.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="A2.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Negative Relation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T7.1.1.2.1" class="ltx_tr">
<td id="A2.T7.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">At</td>
<td id="A2.T7.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
<td id="A2.T7.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
</tr>
<tr id="A2.T7.1.1.3.2" class="ltx_tr">
<td id="A2.T7.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">In</td>
<td id="A2.T7.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
<td id="A2.T7.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
</tr>
<tr id="A2.T7.1.1.4.3" class="ltx_tr">
<td id="A2.T7.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Inside</td>
<td id="A2.T7.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
<td id="A2.T7.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">N/A</td>
</tr>
<tr id="A2.T7.1.1.5.4" class="ltx_tr">
<td id="A2.T7.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Above</td>
<td id="A2.T7.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Below, Beneath, Under</td>
</tr>
<tr id="A2.T7.1.1.6.5" class="ltx_tr">
<td id="A2.T7.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">On top of</td>
<td id="A2.T7.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Below, Beneath, Under</td>
</tr>
<tr id="A2.T7.1.1.7.6" class="ltx_tr">
<td id="A2.T7.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">On</td>
<td id="A2.T7.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Below, Beneath, Under</td>
</tr>
<tr id="A2.T7.1.1.8.7" class="ltx_tr">
<td id="A2.T7.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Below</td>
<td id="A2.T7.1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Above, On Top Of, On</td>
</tr>
<tr id="A2.T7.1.1.9.8" class="ltx_tr">
<td id="A2.T7.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Beneath</td>
<td id="A2.T7.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Above, On Top Of, On</td>
</tr>
<tr id="A2.T7.1.1.10.9" class="ltx_tr">
<td id="A2.T7.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Under</td>
<td id="A2.T7.1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 0)</td>
<td id="A2.T7.1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 0, 300)</td>
<td id="A2.T7.1.1.10.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Above, On Top Of, On</td>
</tr>
<tr id="A2.T7.1.1.11.10" class="ltx_tr">
<td id="A2.T7.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Behind</td>
<td id="A2.T7.1.1.11.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(150, 0, 0)</td>
<td id="A2.T7.1.1.11.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(-150, 0, 0)</td>
<td id="A2.T7.1.1.11.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">In front of</td>
</tr>
<tr id="A2.T7.1.1.12.11" class="ltx_tr">
<td id="A2.T7.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">In front of</td>
<td id="A2.T7.1.1.12.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(-150, 0, 0)</td>
<td id="A2.T7.1.1.12.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(150, 0, 0)</td>
<td id="A2.T7.1.1.12.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Behind</td>
</tr>
<tr id="A2.T7.1.1.13.12" class="ltx_tr">
<td id="A2.T7.1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">To the left of</td>
<td id="A2.T7.1.1.13.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, -150, 0)</td>
<td id="A2.T7.1.1.13.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(0, 150, 0)</td>
<td id="A2.T7.1.1.13.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">To the right of</td>
</tr>
<tr id="A2.T7.1.1.14.13" class="ltx_tr">
<td id="A2.T7.1.1.14.13.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">To the right of</td>
<td id="A2.T7.1.1.14.13.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">(0, 150, 0)</td>
<td id="A2.T7.1.1.14.13.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">(0, -150, 0)</td>
<td id="A2.T7.1.1.14.13.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">To the left of</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Relations, corresponding asset locations wrt to camera origin, and corresponding hard negative relation used in PUG: AR4T</figcaption>
</figure>
<div id="A2.SS5.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px2.p2.1" class="ltx_p">The caption for a scene is generated using one of two templates based on whether the spatial relationship in the scene is unary or binary:</p>
</div>
<div id="A2.SS5.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<ul id="A2.I4" class="ltx_itemize">
<li id="A2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I4.i1.p1" class="ltx_para">
<p id="A2.I4.i1.p1.1" class="ltx_p"><span id="A2.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Unary Relation:</span> <em id="A2.I4.i1.p1.1.2" class="ltx_emph ltx_font_italic">“[ImageNet class label of asset 1] [relation] [human-intelligible background description]”</em> e.g. Banana inside museum</p>
</div>
</li>
<li id="A2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I4.i2.p1" class="ltx_para ltx_noindent">
<p id="A2.I4.i2.p1.1" class="ltx_p"><span id="A2.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Binary Relation:</span> <em id="A2.I4.i2.p1.1.2" class="ltx_emph ltx_font_italic">“[ImageNet class label of asset 1] [relation] [ImageNet class label of asset 2] [(random) unary relation] [human-intelligible background description]”</em> e.g. Banana to the left of chair inside museum</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS5.SSS0.Px2.p4" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px2.p4.1" class="ltx_p">For each binary relation, we also sample the corresponding hard negative scene and caption, by replacing the relation with its negative from Table <a href="#A2.T7" title="Table 7 ‣ PUG: AR4T-Relations ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> such that the semantic meaning of the scene and caption represents a switch from the original relation between objects. For example, the negative of <em id="A2.SS5.SSS0.Px2.p4.1.1" class="ltx_emph ltx_font_italic">“Banana to the left of chair inside museum”</em> is given by <em id="A2.SS5.SSS0.Px2.p4.1.2" class="ltx_emph ltx_font_italic">“Banana to the right of chair inside museum”</em>.</p>
</div>
<div id="A2.SS5.SSS0.Px2.p5" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px2.p5.1" class="ltx_p">We sample a total of 310 binary relation scenes (155 random + 155 negatives), and 310 unary relation scenes for each background, thus leading to a dataset of 112,840 image-caption samples (28 backgrounds x (310 pairs x 10 binary relations + 310 pairs x 3 unary relations)). The PUG:Relations dataset generation process described above is summarized as psueduocode in Algorithm <a href="#alg1" title="Algorithm 1 ‣ PUG: AR4T-Relations ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="A2.SS5.SSS0.Px2.p6" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px2.p6.1" class="ltx_p">Lastly, we split the dataset into 101,920 train and 10,920 test image-caption samples, such that the test set is balanced by background and relations (28 backgrounds x 13 relations x 30 samples). We also release the subset of training and test samples that only contain pairs of objects in scenes along with their hard negatives, which contains 78,400 training and 8,400 test samples, or 39,200 training pairs and 4,200 test pairs. This subset enables Winoground <cite class="ltx_cite ltx_citemacro_cite">Thrush et al. [<a href="#bib.bib57" title="" class="ltx_ref">2022</a>]</cite> style evaluation as well as training with hard visual negative mining for VLMs in future work. The PUG:Relations dataset generation process described above is summarized as psueduocode in Algorithm <a href="#alg1" title="Algorithm 1 ‣ PUG: AR4T-Relations ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> PUG: AR4T-Relations subset generation</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">Unary = {At, In, Inside}

</div>
<div id="alg1.l2" class="ltx_listingline">Categories = {Set of ImageNet Class Labels}

</div>
<div id="alg1.l3" class="ltx_listingline">Environments = {Set of Unreal Environments}

</div>
<div id="alg1.l4" class="ltx_listingline">Relations = {Set of Relations}

</div>
<div id="alg1.l5" class="ltx_listingline">NegRelations = {Dictionary of relations as key, semantically negative relations as values}

</div>
<div id="alg1.l6" class="ltx_listingline">Assets = {Dictionary of Sketchfab assets, keys being ImageNet Class Labels}

</div>
<div id="alg1.l7" class="ltx_listingline">AssetLocations = {Function that returns locations of assets based on relation with a random offset}

</div>
<div id="alg1.l8" class="ltx_listingline">Dataset = <math id="alg1.l8.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="alg1.l8.m1.1a"><mi id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">\phi</annotation></semantics></math>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span id="alg1.l9.1" class="ltx_text ltx_font_bold">for</span> env <math id="alg1.l9.m1.1" class="ltx_Math" alttext="in" display="inline"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">​</mo><mi id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><times id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1"></times><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑖</ci><ci id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">in</annotation></semantics></math> Env <span id="alg1.l9.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">     <span id="alg1.l10.1" class="ltx_text ltx_font_bold">for</span> rel <math id="alg1.l10.m1.1" class="ltx_Math" alttext="in" display="inline"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1" xref="alg1.l10.m1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><times id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1"></times><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝑖</ci><ci id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">in</annotation></semantics></math> Rel <span id="alg1.l10.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l11" class="ltx_listingline">         <math id="alg1.l11.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l11.m1.1a"><mo id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\triangleright</annotation></semantics></math> For binary relations we also add the negative scene+caption to the dataset, thus

</div>
<div id="alg1.l12" class="ltx_listingline">         the effective number of samples per background is 2*15 = 30

</div>
<div id="alg1.l13" class="ltx_listingline">         <span id="alg1.l13.1" class="ltx_text ltx_font_bold">if</span> rel <math id="alg1.l13.m1.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="alg1.l13.m1.1a"><mo id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><in id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">\in</annotation></semantics></math> Unary <span id="alg1.l13.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l14" class="ltx_listingline">              num_samples = 15

</div>
<div id="alg1.l15" class="ltx_listingline">         <span id="alg1.l15.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg1.l16" class="ltx_listingline">              num_samples = 30

</div>
<div id="alg1.l17" class="ltx_listingline">         <span id="alg1.l17.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l17.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l18" class="ltx_listingline">         <span id="alg1.l18.1" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l18.m1.1" class="ltx_Math" alttext="i=1" display="inline"><semantics id="alg1.l18.m1.1a"><mrow id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml"><mi id="alg1.l18.m1.1.1.2" xref="alg1.l18.m1.1.1.2.cmml">i</mi><mo id="alg1.l18.m1.1.1.1" xref="alg1.l18.m1.1.1.1.cmml">=</mo><mn id="alg1.l18.m1.1.1.3" xref="alg1.l18.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><apply id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1"><eq id="alg1.l18.m1.1.1.1.cmml" xref="alg1.l18.m1.1.1.1"></eq><ci id="alg1.l18.m1.1.1.2.cmml" xref="alg1.l18.m1.1.1.2">𝑖</ci><cn type="integer" id="alg1.l18.m1.1.1.3.cmml" xref="alg1.l18.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">i=1</annotation></semantics></math> to num_samples <span id="alg1.l18.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l19" class="ltx_listingline">              cat1 = random.choice(Categories)

</div>
<div id="alg1.l20" class="ltx_listingline">              asset1 = random.choice(Assets[cat1])

</div>
<div id="alg1.l21" class="ltx_listingline">              <span id="alg1.l21.1" class="ltx_text ltx_font_bold">if</span> rel <math id="alg1.l21.m1.1" class="ltx_Math" alttext="\notin" display="inline"><semantics id="alg1.l21.m1.1a"><mo id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml">∉</mo><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><notin id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1"></notin></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">\notin</annotation></semantics></math> Unary <span id="alg1.l21.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l22" class="ltx_listingline">                  cat2 = random.choice(Categories)

</div>
<div id="alg1.l23" class="ltx_listingline">                  asset2 = random.choice(Assets[cat2])

</div>
<div id="alg1.l24" class="ltx_listingline">              <span id="alg1.l24.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l24.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l25" class="ltx_listingline">              location1, location2 = AssetLocations(rel)

</div>
<div id="alg1.l26" class="ltx_listingline">              scene1 = TorchMultiverse(asset1, asset2, location1, location2, env)

</div>
<div id="alg1.l27" class="ltx_listingline">              <span id="alg1.l27.1" class="ltx_text ltx_font_bold">if</span> rel <math id="alg1.l27.m1.1" class="ltx_Math" alttext="\notin" display="inline"><semantics id="alg1.l27.m1.1a"><mo id="alg1.l27.m1.1.1" xref="alg1.l27.m1.1.1.cmml">∉</mo><annotation-xml encoding="MathML-Content" id="alg1.l27.m1.1b"><notin id="alg1.l27.m1.1.1.cmml" xref="alg1.l27.m1.1.1"></notin></annotation-xml><annotation encoding="application/x-tex" id="alg1.l27.m1.1c">\notin</annotation></semantics></math> Unary <span id="alg1.l27.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l28" class="ltx_listingline">                  rel2 = random.choice(Unary)

</div>
<div id="alg1.l29" class="ltx_listingline">                  caption1 = cat1 + ’ ’ + rel + ’ ’ + cat2 + ’ ’ + rel2 + ’ ’ + env

</div>
<div id="alg1.l30" class="ltx_listingline">                  Dataset <math id="alg1.l30.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg1.l30.m1.1a"><mo id="alg1.l30.m1.1.1" xref="alg1.l30.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg1.l30.m1.1b"><union id="alg1.l30.m1.1.1.cmml" xref="alg1.l30.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg1.l30.m1.1c">\cup</annotation></semantics></math> {scene1, caption1}

</div>
<div id="alg1.l31" class="ltx_listingline">                  <math id="alg1.l31.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l31.m1.1a"><mo id="alg1.l31.m1.1.1" xref="alg1.l31.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l31.m1.1b"><ci id="alg1.l31.m1.1.1.cmml" xref="alg1.l31.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l31.m1.1c">\triangleright</annotation></semantics></math> Generate hard negative scene and caption

</div>
<div id="alg1.l32" class="ltx_listingline">                  scene1 = TorchMultiverse(asset1, asset2, location2, location1, env)

</div>
<div id="alg1.l33" class="ltx_listingline">                  caption2 = cat1 + ’ ’ + NegRelations[rel] + ’ ’ + cat2 + ’ ’ + rel2 + ’ ’ + env

</div>
<div id="alg1.l34" class="ltx_listingline">                  Dataset <math id="alg1.l34.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg1.l34.m1.1a"><mo id="alg1.l34.m1.1.1" xref="alg1.l34.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg1.l34.m1.1b"><union id="alg1.l34.m1.1.1.cmml" xref="alg1.l34.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg1.l34.m1.1c">\cup</annotation></semantics></math> {scene2, caption2}

</div>
<div id="alg1.l35" class="ltx_listingline">              <span id="alg1.l35.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg1.l36" class="ltx_listingline">                  caption1 = cat1 + ’ ’ + rel + ’ ’ + env

</div>
<div id="alg1.l37" class="ltx_listingline">                  Dataset <math id="alg1.l37.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg1.l37.m1.1a"><mo id="alg1.l37.m1.1.1" xref="alg1.l37.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg1.l37.m1.1b"><union id="alg1.l37.m1.1.1.cmml" xref="alg1.l37.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg1.l37.m1.1c">\cup</annotation></semantics></math> {scene1, caption1}

</div>
<div id="alg1.l38" class="ltx_listingline">              <span id="alg1.l38.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l38.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l39" class="ltx_listingline">         <span id="alg1.l39.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l39.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l40" class="ltx_listingline">     <span id="alg1.l40.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l40.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l41" class="ltx_listingline">
<span id="alg1.l41.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l41.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l42" class="ltx_listingline">
<span id="alg1.l42.1" class="ltx_text ltx_font_bold">return</span> Dataset

</div>
</div>
</figure>
</section>
<section id="A2.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">PUG: AR4T-Attributes</h5>

<div id="A2.SS5.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px3.p1.1" class="ltx_p">For PUG: AR4T-Attributes the selection of assets, relations between assets, and spatial locations of assets is done exactly as for PUG: AR4T-Relations. However, since the focus of this dataset is on the object-attribute pairs, the relations between objects are not represented in the corresponding scene caption in any form. The attribute for each object is chosen randomly from the set of 53 attributes described in Table <a href="#A2.T8" title="Table 8 ‣ PUG: AR4T-Attributes ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. For size based attributes, the asset’s material instance remains the same but its size is varied between 50 pixels (<span id="A2.SS5.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">short, small, little, tiny</span>) and 200 pixels (<span id="A2.SS5.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter">big, long, tall, large</span>). For all other attributes, the material instance of the object is changed in Unreal using Pytorch Multiverse.</p>
</div>
<div id="A2.SS5.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px3.p2.1" class="ltx_p">The caption of the scene is generated using one of two templates based on whether the spatial relationship in the scene is unary or binary:</p>
</div>
<div id="A2.SS5.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<ul id="A2.I5" class="ltx_itemize">
<li id="A2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I5.i1.p1" class="ltx_para">
<p id="A2.I5.i1.p1.1" class="ltx_p"><span id="A2.I5.i1.p1.1.1" class="ltx_text ltx_font_bold">Unary Relation:</span> <em id="A2.I5.i1.p1.1.2" class="ltx_emph ltx_font_italic">“[Attribute of asset 1] [ImageNet class label of asset 1] [relation] [human-intelligible background description]”</em> e.g. Green banana inside museum</p>
</div>
</li>
<li id="A2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I5.i2.p1" class="ltx_para ltx_noindent">
<p id="A2.I5.i2.p1.1" class="ltx_p"><span id="A2.I5.i2.p1.1.1" class="ltx_text ltx_font_bold">Binary Relation:</span> <em id="A2.I5.i2.p1.1.2" class="ltx_emph ltx_font_italic">“[Attribute of asset 1][ImageNet class label of asset 1] and [Attribute of asset 2][ImageNet class label of asset 2] [(random) unary relation] [human-intelligible background description]”</em> e.g. Green banana and large chair inside museum</p>
</div>
</li>
</ul>
</div>
<figure id="A2.T8" class="ltx_table">
<div id="A2.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:268.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.4pt,18.9pt) scale(0.876739836487244,0.876739836487244) ;">
<table id="A2.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T8.1.1.1.1" class="ltx_tr">
<th id="A2.T8.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Attribute Category</span></th>
<td id="A2.T8.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Attribute</span></td>
<td id="A2.T8.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Variants</span></td>
<th id="A2.T8.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Attribute Category</span></th>
<td id="A2.T8.1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Attribute</span></td>
<td id="A2.T8.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A2.T8.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Variants</span></td>
</tr>
<tr id="A2.T8.1.1.2.2" class="ltx_tr">
<th id="A2.T8.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="7"><span id="A2.T8.1.1.2.2.1.1" class="ltx_text">Material</span></th>
<td id="A2.T8.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Brick</td>
<td id="A2.T8.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3</td>
<th id="A2.T8.1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="12"><span id="A2.T8.1.1.2.2.4.1" class="ltx_text">Color</span></th>
<td id="A2.T8.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Black</td>
<td id="A2.T8.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.3.3" class="ltx_tr">
<td id="A2.T8.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Metal</td>
<td id="A2.T8.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Blue</td>
<td id="A2.T8.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.4.4" class="ltx_tr">
<td id="A2.T8.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Wood</td>
<td id="A2.T8.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Yellow</td>
<td id="A2.T8.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.5.5" class="ltx_tr">
<td id="A2.T8.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Glass</td>
<td id="A2.T8.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">White</td>
<td id="A2.T8.1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.6.6" class="ltx_tr">
<td id="A2.T8.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cloth</td>
<td id="A2.T8.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Green</td>
<td id="A2.T8.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.7.7" class="ltx_tr">
<td id="A2.T8.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Plastic</td>
<td id="A2.T8.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gray</td>
<td id="A2.T8.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.8.8" class="ltx_tr">
<td id="A2.T8.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Rock</td>
<td id="A2.T8.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="A2.T8.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Brown</td>
<td id="A2.T8.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.9.9" class="ltx_tr">
<th id="A2.T8.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="8"><span id="A2.T8.1.1.9.9.1.1" class="ltx_text">Size</span></th>
<td id="A2.T8.1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Big</td>
<td id="A2.T8.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Red</td>
<td id="A2.T8.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.10.10" class="ltx_tr">
<td id="A2.T8.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Long</td>
<td id="A2.T8.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Silver</td>
<td id="A2.T8.1.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.11.11" class="ltx_tr">
<td id="A2.T8.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tall</td>
<td id="A2.T8.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Orange</td>
<td id="A2.T8.1.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.12.12" class="ltx_tr">
<td id="A2.T8.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Large</td>
<td id="A2.T8.1.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pink</td>
<td id="A2.T8.1.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.13.13" class="ltx_tr">
<td id="A2.T8.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Short</td>
<td id="A2.T8.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gold</td>
<td id="A2.T8.1.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.14.14" class="ltx_tr">
<td id="A2.T8.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Small</td>
<td id="A2.T8.1.1.14.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<th id="A2.T8.1.1.14.14.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="4"><span id="A2.T8.1.1.14.14.3.1" class="ltx_text">Texture</span></th>
<td id="A2.T8.1.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Striped</td>
<td id="A2.T8.1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.15.15" class="ltx_tr">
<td id="A2.T8.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Little</td>
<td id="A2.T8.1.1.15.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Dark</td>
<td id="A2.T8.1.1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.16.16" class="ltx_tr">
<td id="A2.T8.1.1.16.16.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tiny</td>
<td id="A2.T8.1.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="A2.T8.1.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cloudy</td>
<td id="A2.T8.1.1.16.16.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="A2.T8.1.1.17.17" class="ltx_tr">
<th id="A2.T8.1.1.17.17.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="A2.T8.1.1.17.17.1.1" class="ltx_text ltx_font_bold">Total=</span></th>
<td id="A2.T8.1.1.17.17.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt">53</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Attributes and corresponding number of variants used in PUG: AR4T-Attributes</figcaption>
</figure>
<div id="A2.SS5.SSS0.Px3.p4" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px3.p4.1" class="ltx_p">For each binary relation, we also sample the corresponding hard negative scene and caption, by swapping the attributes between objects such that the semantic meaning of the scene and caption represents a switch from the original object-attribute associations. For example, the negative of <em id="A2.SS5.SSS0.Px3.p4.1.1" class="ltx_emph ltx_font_italic">“Green banana and large chair inside museum”</em> is given by <em id="A2.SS5.SSS0.Px3.p4.1.2" class="ltx_emph ltx_font_italic">“Large banana and green chair inside museum”</em>. For each of the 53 object attributes, we sample 2 scenes of it in conjugation with another object and attribute, and 2 scenes of the attribute in isolation in a scene. We repeat this for each possible background, thus leading to a dataset of 160,272 image-caption samples (28 backgrounds x (53 attributes x (53 attributes x 2 samples) + 2 samples)). The PUG:Attributes dataset generation process described above is summarized as psueduocode in Algorithm <a href="#alg2" title="Algorithm 2 ‣ Caption variants in PUG: AR4T ‣ B.5 PUG: AR4T ‣ Appendix B PUG Datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="A2.SS5.SSS0.Px3.p5" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px3.p5.1" class="ltx_p">Lastly, we split the dataset into 147,976 train and 12,296 test images. For each attribute pair in a scene, we select 4 image-caption samples in the test set (4 samples x 53 attributes x 53 attributes = 11,236 sample). And we sample the remaining test samples by selecting 20 image-caption samples for each attribute in isolation in a scene (20 samples x 53 attributes = 1,060 samples). Similar to PUG-attributes, we also separately release the subset of training and test set with scenes and captions containing only pairs of scenes with their corresponding hard negatives, leading to 146,158 training samples and 11,236 test samples, or 73,0739 training and 5,618 test sample pairs.</p>
</div>
</section>
<section id="A2.SS5.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Caption variants in PUG: AR4T</h5>

<div id="A2.SS5.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="A2.SS5.SSS0.Px4.p1.1" class="ltx_p">In order to emphasize the idea that each scene can have multiple descriptive captions associated with it, we utilize a simple template to generate alternate captions for scenes with binary relations that are semantically consistent but linguistically different. During fine-tuning, the model randomly sees either the original caption or the alternate caption. The presence of alternate captions also prevents the VLM to learn shortcuts between the position of the object descriptions in captions and the underlying spatial relationship or object-attribute association.</p>
</div>
<div id="A2.SS5.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<ul id="A2.I6" class="ltx_itemize">
<li id="A2.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I6.i1.p1" class="ltx_para">
<p id="A2.I6.i1.p1.1" class="ltx_p"><span id="A2.I6.i1.p1.1.1" class="ltx_text ltx_font_bold">PUG: Relations</span> <em id="A2.I6.i1.p1.1.2" class="ltx_emph ltx_font_italic">“[ImageNet class label of asset 2] [negative relation] [ImageNet class label of asset 1] [(random) unary relation] [human-intelligible background description]”</em> e.g. The alternate caption for ’Banana to the left of chair inside museum’ is ’Chair to the right of banana inside museum’</p>
</div>
</li>
<li id="A2.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I6.i2.p1" class="ltx_para ltx_noindent">
<p id="A2.I6.i2.p1.1" class="ltx_p"><span id="A2.I6.i2.p1.1.1" class="ltx_text ltx_font_bold">PUG: Attributes</span> <em id="A2.I6.i2.p1.1.2" class="ltx_emph ltx_font_italic">“[Attribute of asset 2][ImageNet class label of asset 2] and [Attribute of asset 1][ImageNet class label of asset 1] [(random) unary relation] [human-intelligible background description]”</em> e.g. The alternate caption for ’Green banana and large chair inside museum’ is ’Large chair and green banana inside museum’</p>
</div>
</li>
</ul>
</div>
<figure id="alg2" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg2.2.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span> PUG: AR4T-Attributes subset generation</figcaption>
<div id="alg2.3" class="ltx_listing ltx_listing">
<div id="alg2.l1" class="ltx_listingline">Unary = {At, In, Inside}

</div>
<div id="alg2.l2" class="ltx_listingline">Categories = {Set of ImageNet Class Labels}

</div>
<div id="alg2.l3" class="ltx_listingline">Environments = {Set of Unreal Environments}

</div>
<div id="alg2.l4" class="ltx_listingline">Relations = {Set of Relations}

</div>
<div id="alg2.l5" class="ltx_listingline">Attributes = {Set of Attributes}

</div>
<div id="alg2.l6" class="ltx_listingline">Assets = {Dictionary of Sketchfab assets, keys being ImageNet Class Labels}

</div>
<div id="alg2.l7" class="ltx_listingline">AssetLocations = {Function that returns locations of assets based on relation with a random offset}

</div>
<div id="alg2.l8" class="ltx_listingline">Dataset = <math id="alg2.l8.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="alg2.l8.m1.1a"><mi id="alg2.l8.m1.1.1" xref="alg2.l8.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="alg2.l8.m1.1b"><ci id="alg2.l8.m1.1.1.cmml" xref="alg2.l8.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l8.m1.1c">\phi</annotation></semantics></math>

</div>
<div id="alg2.l9" class="ltx_listingline">
<span id="alg2.l9.1" class="ltx_text ltx_font_bold">for</span> att1 <math id="alg2.l9.m1.1" class="ltx_Math" alttext="in" display="inline"><semantics id="alg2.l9.m1.1a"><mrow id="alg2.l9.m1.1.1" xref="alg2.l9.m1.1.1.cmml"><mi id="alg2.l9.m1.1.1.2" xref="alg2.l9.m1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg2.l9.m1.1.1.1" xref="alg2.l9.m1.1.1.1.cmml">​</mo><mi id="alg2.l9.m1.1.1.3" xref="alg2.l9.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l9.m1.1b"><apply id="alg2.l9.m1.1.1.cmml" xref="alg2.l9.m1.1.1"><times id="alg2.l9.m1.1.1.1.cmml" xref="alg2.l9.m1.1.1.1"></times><ci id="alg2.l9.m1.1.1.2.cmml" xref="alg2.l9.m1.1.1.2">𝑖</ci><ci id="alg2.l9.m1.1.1.3.cmml" xref="alg2.l9.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l9.m1.1c">in</annotation></semantics></math> Attributes <span id="alg2.l9.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg2.l10" class="ltx_listingline">     <span id="alg2.l10.1" class="ltx_text ltx_font_bold">for</span> att2 <math id="alg2.l10.m1.1" class="ltx_Math" alttext="in" display="inline"><semantics id="alg2.l10.m1.1a"><mrow id="alg2.l10.m1.1.1" xref="alg2.l10.m1.1.1.cmml"><mi id="alg2.l10.m1.1.1.2" xref="alg2.l10.m1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg2.l10.m1.1.1.1" xref="alg2.l10.m1.1.1.1.cmml">​</mo><mi id="alg2.l10.m1.1.1.3" xref="alg2.l10.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l10.m1.1b"><apply id="alg2.l10.m1.1.1.cmml" xref="alg2.l10.m1.1.1"><times id="alg2.l10.m1.1.1.1.cmml" xref="alg2.l10.m1.1.1.1"></times><ci id="alg2.l10.m1.1.1.2.cmml" xref="alg2.l10.m1.1.1.2">𝑖</ci><ci id="alg2.l10.m1.1.1.3.cmml" xref="alg2.l10.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l10.m1.1c">in</annotation></semantics></math> Attributes + [None] <span id="alg2.l10.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg2.l11" class="ltx_listingline">         <span id="alg2.l11.1" class="ltx_text ltx_font_bold">if</span> att2 == None <span id="alg2.l11.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l12" class="ltx_listingline">              num_samples = 20

</div>
<div id="alg2.l13" class="ltx_listingline">         <span id="alg2.l13.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg2.l14" class="ltx_listingline">              <math id="alg2.l14.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg2.l14.m1.1a"><mo id="alg2.l14.m1.1.1" xref="alg2.l14.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg2.l14.m1.1b"><ci id="alg2.l14.m1.1.1.cmml" xref="alg2.l14.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l14.m1.1c">\triangleright</annotation></semantics></math> For binary relations we also add the negative scene+caption to the dataset, thus

</div>
<div id="alg2.l15" class="ltx_listingline">              the effective number of samples per background and attribute is 2*2 = 4

</div>
<div id="alg2.l16" class="ltx_listingline">              num_samples = 2

</div>
<div id="alg2.l17" class="ltx_listingline">         <span id="alg2.l17.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l17.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l18" class="ltx_listingline">         <span id="alg2.l18.1" class="ltx_text ltx_font_bold">for</span> <math id="alg2.l18.m1.1" class="ltx_Math" alttext="i=1" display="inline"><semantics id="alg2.l18.m1.1a"><mrow id="alg2.l18.m1.1.1" xref="alg2.l18.m1.1.1.cmml"><mi id="alg2.l18.m1.1.1.2" xref="alg2.l18.m1.1.1.2.cmml">i</mi><mo id="alg2.l18.m1.1.1.1" xref="alg2.l18.m1.1.1.1.cmml">=</mo><mn id="alg2.l18.m1.1.1.3" xref="alg2.l18.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg2.l18.m1.1b"><apply id="alg2.l18.m1.1.1.cmml" xref="alg2.l18.m1.1.1"><eq id="alg2.l18.m1.1.1.1.cmml" xref="alg2.l18.m1.1.1.1"></eq><ci id="alg2.l18.m1.1.1.2.cmml" xref="alg2.l18.m1.1.1.2">𝑖</ci><cn type="integer" id="alg2.l18.m1.1.1.3.cmml" xref="alg2.l18.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l18.m1.1c">i=1</annotation></semantics></math> to num_samples <span id="alg2.l18.2" class="ltx_text ltx_font_bold">do</span>
env = random.choice(Environments)

</div>
<div id="alg2.l19" class="ltx_listingline">              <span id="alg2.l19.1" class="ltx_text ltx_font_bold">if</span> att2 == None <span id="alg2.l19.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l20" class="ltx_listingline">                  rel = random.choice(Unary)

</div>
<div id="alg2.l21" class="ltx_listingline">              <span id="alg2.l21.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg2.l22" class="ltx_listingline">                  rel = random.choice(Relations - Unary)

</div>
<div id="alg2.l23" class="ltx_listingline">              <span id="alg2.l23.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l23.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l24" class="ltx_listingline">              cat1 = random.choice(Categories)

</div>
<div id="alg2.l25" class="ltx_listingline">              asset1 = random.choice(Assets[cat1])

</div>
<div id="alg2.l26" class="ltx_listingline">              <span id="alg2.l26.1" class="ltx_text ltx_font_bold">if</span> att2 != None <span id="alg2.l26.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l27" class="ltx_listingline">                  cat2 = random.choice(Categories)

</div>
<div id="alg2.l28" class="ltx_listingline">                  asset2 = random.choice(Assets[cat2])

</div>
<div id="alg2.l29" class="ltx_listingline">              <span id="alg2.l29.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l29.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l30" class="ltx_listingline">              location1, location2 = AssetLocations(rel)

</div>
<div id="alg2.l31" class="ltx_listingline">              scene1 = TorchMultiverse(asset1, asset2, location1, location2, att1, att2, env)

</div>
<div id="alg2.l32" class="ltx_listingline">              <span id="alg2.l32.1" class="ltx_text ltx_font_bold">if</span> rel <math id="alg2.l32.m1.1" class="ltx_Math" alttext="\notin" display="inline"><semantics id="alg2.l32.m1.1a"><mo id="alg2.l32.m1.1.1" xref="alg2.l32.m1.1.1.cmml">∉</mo><annotation-xml encoding="MathML-Content" id="alg2.l32.m1.1b"><notin id="alg2.l32.m1.1.1.cmml" xref="alg2.l32.m1.1.1"></notin></annotation-xml><annotation encoding="application/x-tex" id="alg2.l32.m1.1c">\notin</annotation></semantics></math> Unary <span id="alg2.l32.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l33" class="ltx_listingline">                  rel2 = random.choice(Unary)

</div>
<div id="alg2.l34" class="ltx_listingline">                  caption1 = att1 + ’ ’ + cat1 + ’ and ’ + att2 + ’ ’ + cat2 + ’ ’ + rel2 + ’ ’ + env

</div>
<div id="alg2.l35" class="ltx_listingline">                  Dataset <math id="alg2.l35.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg2.l35.m1.1a"><mo id="alg2.l35.m1.1.1" xref="alg2.l35.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg2.l35.m1.1b"><union id="alg2.l35.m1.1.1.cmml" xref="alg2.l35.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg2.l35.m1.1c">\cup</annotation></semantics></math> {scene1, caption1}

</div>
<div id="alg2.l36" class="ltx_listingline">                  <math id="alg2.l36.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg2.l36.m1.1a"><mo id="alg2.l36.m1.1.1" xref="alg2.l36.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg2.l36.m1.1b"><ci id="alg2.l36.m1.1.1.cmml" xref="alg2.l36.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l36.m1.1c">\triangleright</annotation></semantics></math> Generate hard negative scene and caption by switching object-attribute

</div>
<div id="alg2.l37" class="ltx_listingline">                  associations

</div>
<div id="alg2.l38" class="ltx_listingline">                  scene2 = TorchMultiverse(asset1, asset2, location1, location2, att2, att1, env)

</div>
<div id="alg2.l39" class="ltx_listingline">                  caption2 = att2 + ’ ’ + cat1 + ’ and ’ + att1 + ’ ’ + cat2 + ’ ’ + rel2 + ’ ’ + env

</div>
<div id="alg2.l40" class="ltx_listingline">                  Dataset <math id="alg2.l40.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg2.l40.m1.1a"><mo id="alg2.l40.m1.1.1" xref="alg2.l40.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg2.l40.m1.1b"><union id="alg2.l40.m1.1.1.cmml" xref="alg2.l40.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg2.l40.m1.1c">\cup</annotation></semantics></math> {scene2, caption2}

</div>
<div id="alg2.l41" class="ltx_listingline">              <span id="alg2.l41.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg2.l42" class="ltx_listingline">                  caption1 = att1 + ’ ’ + cat1 + ’ ’ + rel + ’ ’ + env

</div>
<div id="alg2.l43" class="ltx_listingline">                  Dataset <math id="alg2.l43.m1.1" class="ltx_Math" alttext="\cup" display="inline"><semantics id="alg2.l43.m1.1a"><mo id="alg2.l43.m1.1.1" xref="alg2.l43.m1.1.1.cmml">∪</mo><annotation-xml encoding="MathML-Content" id="alg2.l43.m1.1b"><union id="alg2.l43.m1.1.1.cmml" xref="alg2.l43.m1.1.1"></union></annotation-xml><annotation encoding="application/x-tex" id="alg2.l43.m1.1c">\cup</annotation></semantics></math> {scene1, caption1}

</div>
<div id="alg2.l44" class="ltx_listingline">              <span id="alg2.l44.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l44.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l45" class="ltx_listingline">         <span id="alg2.l45.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l45.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg2.l46" class="ltx_listingline">     <span id="alg2.l46.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l46.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg2.l47" class="ltx_listingline">
<span id="alg2.l47.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l47.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg2.l48" class="ltx_listingline">
<span id="alg2.l48.1" class="ltx_text ltx_font_bold">return</span> Dataset

</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional experimental details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Equivariance study details</h3>

<div id="A3.SS1.p1" class="ltx_para ltx_noindent">
<p id="A3.SS1.p1.1" class="ltx_p">In section <a href="#S3.SS2" title="3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we used PUG: Animals to study how foundation vision-language models behave with respect to changes in factors of variations. We showed high image and text equivariance with respect to background, and text equivariance with respect to size and texture too. Here, we provide more details and results.
<br class="ltx_break">
<br class="ltx_break">In our study, we use the following pretrained models:</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p">BLIP with ViT base backbone from the Huggingface transformers library <cite class="ltx_cite ltx_citemacro_citep">[Wolf et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>]</cite>, trained on COCO dataset <cite class="ltx_cite ltx_citemacro_citep">[Lin et al., <a href="#bib.bib39" title="" class="ltx_ref">2014</a>]</cite>,</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p">NegCLIP from <cite class="ltx_cite ltx_citemacro_citep">[Yuksekgonul et al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>]</cite>,</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p id="A3.I1.i3.p1.1" class="ltx_p">As in <cite class="ltx_cite ltx_citemacro_citep">[Yuksekgonul et al., <a href="#bib.bib68" title="" class="ltx_ref">2023</a>]</cite> we use X-VLM pretrained on COCO dataset from <a target="_blank" href="https://github.com/zengyan-97/X-VLM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zengyan-97/X-VLM</a>,</p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p id="A3.I1.i4.p1.1" class="ltx_p">Flava with ViT-B/32 backbone from Huggingface transformers (<a target="_blank" href="https://huggingface.co/facebook/flava-full" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/facebook/flava-full</a>)</p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="A3.I1.i5.p1.1" class="ltx_p">CLIP models all come from OpenAI CLIP <a target="_blank" href="https://github.com/openai/CLIP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/CLIP</a>. We use the versions with ResNet50, ResNet101, ViT-L/14, ViT-B/16, ViT-B/32.</p>
</div>
</li>
</ul>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p id="A3.SS1.p2.3" class="ltx_p">We compute equivariance to each of the factors of variations. Since the text captions do not take into account camera and asset orientations, when we compute the equivariance with respect to the other factors we take only samples for a given orientation of the character and camera (0 for roll, pitch and yaw in both cases). Furthermore, in the text caption we replace sizes by three adjectives as follow: 0.7 is mapped to small, 1.0 to medium and 1.3 to big. Inspired by <cite class="ltx_cite ltx_citemacro_citep">[Bouchacourt et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>]</cite>, we compute equivariance as the alignment between embedding difference vectors. That is, we compute embedding difference vectors <math id="A3.SS1.p2.1.m1.1" class="ltx_Math" alttext="z_{i}-z_{j}" display="inline"><semantics id="A3.SS1.p2.1.m1.1a"><mrow id="A3.SS1.p2.1.m1.1.1" xref="A3.SS1.p2.1.m1.1.1.cmml"><msub id="A3.SS1.p2.1.m1.1.1.2" xref="A3.SS1.p2.1.m1.1.1.2.cmml"><mi id="A3.SS1.p2.1.m1.1.1.2.2" xref="A3.SS1.p2.1.m1.1.1.2.2.cmml">z</mi><mi id="A3.SS1.p2.1.m1.1.1.2.3" xref="A3.SS1.p2.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="A3.SS1.p2.1.m1.1.1.1" xref="A3.SS1.p2.1.m1.1.1.1.cmml">−</mo><msub id="A3.SS1.p2.1.m1.1.1.3" xref="A3.SS1.p2.1.m1.1.1.3.cmml"><mi id="A3.SS1.p2.1.m1.1.1.3.2" xref="A3.SS1.p2.1.m1.1.1.3.2.cmml">z</mi><mi id="A3.SS1.p2.1.m1.1.1.3.3" xref="A3.SS1.p2.1.m1.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.1.m1.1b"><apply id="A3.SS1.p2.1.m1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1"><minus id="A3.SS1.p2.1.m1.1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1.1"></minus><apply id="A3.SS1.p2.1.m1.1.1.2.cmml" xref="A3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.SS1.p2.1.m1.1.1.2.1.cmml" xref="A3.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="A3.SS1.p2.1.m1.1.1.2.2.cmml" xref="A3.SS1.p2.1.m1.1.1.2.2">𝑧</ci><ci id="A3.SS1.p2.1.m1.1.1.2.3.cmml" xref="A3.SS1.p2.1.m1.1.1.2.3">𝑖</ci></apply><apply id="A3.SS1.p2.1.m1.1.1.3.cmml" xref="A3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="A3.SS1.p2.1.m1.1.1.3.1.cmml" xref="A3.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="A3.SS1.p2.1.m1.1.1.3.2.cmml" xref="A3.SS1.p2.1.m1.1.1.3.2">𝑧</ci><ci id="A3.SS1.p2.1.m1.1.1.3.3.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.1.m1.1c">z_{i}-z_{j}</annotation></semantics></math> where <math id="A3.SS1.p2.2.m2.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="A3.SS1.p2.2.m2.1a"><msub id="A3.SS1.p2.2.m2.1.1" xref="A3.SS1.p2.2.m2.1.1.cmml"><mi id="A3.SS1.p2.2.m2.1.1.2" xref="A3.SS1.p2.2.m2.1.1.2.cmml">z</mi><mi id="A3.SS1.p2.2.m2.1.1.3" xref="A3.SS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.2.m2.1b"><apply id="A3.SS1.p2.2.m2.1.1.cmml" xref="A3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.2.m2.1.1.1.cmml" xref="A3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="A3.SS1.p2.2.m2.1.1.2.cmml" xref="A3.SS1.p2.2.m2.1.1.2">𝑧</ci><ci id="A3.SS1.p2.2.m2.1.1.3.cmml" xref="A3.SS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.2.m2.1c">z_{i}</annotation></semantics></math> and <math id="A3.SS1.p2.3.m3.1" class="ltx_Math" alttext="z_{j}" display="inline"><semantics id="A3.SS1.p2.3.m3.1a"><msub id="A3.SS1.p2.3.m3.1.1" xref="A3.SS1.p2.3.m3.1.1.cmml"><mi id="A3.SS1.p2.3.m3.1.1.2" xref="A3.SS1.p2.3.m3.1.1.2.cmml">z</mi><mi id="A3.SS1.p2.3.m3.1.1.3" xref="A3.SS1.p2.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.3.m3.1b"><apply id="A3.SS1.p2.3.m3.1.1.cmml" xref="A3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.3.m3.1.1.1.cmml" xref="A3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="A3.SS1.p2.3.m3.1.1.2.cmml" xref="A3.SS1.p2.3.m3.1.1.2">𝑧</ci><ci id="A3.SS1.p2.3.m3.1.1.3.cmml" xref="A3.SS1.p2.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.3.m3.1c">z_{j}</annotation></semantics></math> are the (normalized) embeddings of two images (or texts) of an object undergoing a given factor change. We then measure pairwise alignment as cosine similarity of embedding difference vectors (either between images pairs, text pairs or image-text pairs) corresponding to the same factor change. We report averaged cosine similarity of randomly paired vectors, and a higher value implies higher equivariance.
<br class="ltx_break">
<br class="ltx_break">Note that a model can present image equivariance but no text equivariance (caption and image are not guaranteed to be encoded in the same vector), or have high equivariance across modalities but no image or text equivariance and vice-versa.</p>
</div>
<figure id="A3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A3.F10.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2308.03977/assets/x12.png" id="A3.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="460" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Image equivariance</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A3.F10.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2308.03977/assets/x13.png" id="A3.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="460" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Text equivariance</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A3.F10.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2308.03977/assets/x14.png" id="A3.F10.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="460" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Across modalities equivariance</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Measuring foundation models equivariance thanks to PUG: Animals: all three factors.</figcaption>
</figure>
<div id="A3.SS1.p3" class="ltx_para">
<p id="A3.SS1.p3.1" class="ltx_p">In Figure <a href="#A3.F11" title="Figure 11 ‣ C.1 Equivariance study details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> we report image equivariance with respect to orientation of the camera yaw. We see that there is little to no equivariance to it, suggesting that image embeddings are more predictable when changing background than the camera yaw.</p>
</div>
<figure id="A3.F11" class="ltx_figure"><img src="/html/2308.03977/assets/x15.png" id="A3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="76" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Additional image equivariance results with respect to camera yaw.</figcaption>
</figure>
<div id="A3.SS1.p4" class="ltx_para ltx_noindent">
<p id="A3.SS1.p4.1" class="ltx_p">Note that foundation models representations belong to the hypersphere, yet our measurement of equivariance as parallelism (measured with cosine similarity) relies on Euclidean geometry. Still, cosine similarity is a starting point to showcase how PUG: Animals can be used to study models’ representational spaces. This could also explain the higher equivariance values of text representations: since textual captions follow the same template, embeddings might be close to each other (relative to image embeddings distances). In this case the hypersphere locally behaves like an Euclidean space <cite class="ltx_cite ltx_citemacro_citep">[Lee, <a href="#bib.bib35" title="" class="ltx_ref">2000</a>]</cite>, for which the Euclidean geometry is better suited. We leave for future work the exploration more complex equivariance metrics potentially based on spherical geometry to study foundation models’ representational spaces. Studying model’s representations is key to better understanding and improving them <cite class="ltx_cite ltx_citemacro_citet">Bouchacourt et al. [<a href="#bib.bib8" title="" class="ltx_ref">2021</a>], Xie et al. [<a href="#bib.bib66" title="" class="ltx_ref">2022</a>], Ushio et al. [<a href="#bib.bib60" title="" class="ltx_ref">2021</a>], Lenc and Vedaldi [<a href="#bib.bib36" title="" class="ltx_ref">2018</a>]</cite>. Our study showcases that PUG: Animals advantages (rich diversity of factors, knowledge of their values, control either one factor at a time but also all together) make it a great dataset to study state-of-the-art models representational properties.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Classification with held out sets</h3>

<div id="A3.SS2.p1" class="ltx_para ltx_noindent">
<p id="A3.SS2.p1.1" class="ltx_p">In section <a href="#S3.SS2.SSS0.Px1" title="Classification with held out sets ‣ 3.2 PUG: Animals ‣ 3 Photorealistic Unreal Graphics (PUG) environments and datasets ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we study how one can leverage PUG: Animsl to study OOD generalization in two settings: 1) Generalization on unseen factors 2) Generalization on unseen combination of factors.</p>
</div>
<div id="A3.SS2.p2" class="ltx_para ltx_noindent">
<p id="A3.SS2.p2.1" class="ltx_p">For this experiment, we use PUG: Animals with held out sets. Typically, we random select a number of number of animals (0, 10 or 20) within our 70 assets. Then for the remaining animals we decided to remove from PUG: Animals a number of background, object size or object texture. This give us a training set. Then the images that were excluded from this training set are put as a test or held out set in which we measure the performance of a model. This model is typically a Resnet50 trained for 100 epochs with AdamW as optimizer with a batch size of 2048.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Robustness of SOTA models additional details</h3>

<div id="A3.SS3.p1" class="ltx_para ltx_noindent">
<p id="A3.SS3.p1.1" class="ltx_p">In addition to evaluating robustness for the models in the main paper, in Table <a href="#A3.T9" title="Table 9 ‣ C.3 Robustness of SOTA models additional details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we provide an analysis of several additional models including the recent self-supervised DINOv2 model as well as BLIP a contrastive vision-language model.
Parenthesis indicate the pretraining dataset: ImageNet 21k, LVD-142M, JFT 300M, LAION 400M, and LAION 2B. Models without parethesis are pretrained on the standard ImageNet-1k dataset.
For ResNet models we use the publicly available pretrained checkpoints from the Timm package based on the training recipe from <cite class="ltx_cite ltx_citemacro_citet">Wightman et al. [<a href="#bib.bib63" title="" class="ltx_ref">2021</a>]</cite>. For the vision transformer models and Swin we use the pretrained models from the Timm package with patch size 16 for ViT and Swin Base with a patch size of 4 and window size of 7. For the BiT model we use the pretrained checkpoint trained on Google’s JFT 300M dataset from the Timm package with a ResNetv2 101 architecture. For DINOv2, we use the officially released repo to evaluate the base ViT architecture trained on 132 million samples <cite class="ltx_cite ltx_citemacro_citep">[Oquab et al., <a href="#bib.bib46" title="" class="ltx_ref">2023</a>]</cite>. For BLIP we use the checkpoint available in <a target="_blank" href="https://huggingface.co/docs/transformers/model_doc/blip" title="" class="ltx_ref ltx_href">HuggingFace</a> and evaluate the model using zero shot classification via the prompt ‘This is a photo of a [ ]’.
We evaluate CLIP model variants in a similar zero shot fashion and rely on OpenCLIP’s implementation. The parenthesis indicate the pretraining dataset size from the LAION dataset. We report the average accuracy as each factor (see columns of Table <a href="#A3.T9" title="Table 9 ‣ C.3 Robustness of SOTA models additional details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) varies.</p>
</div>
<div id="A3.SS3.p2" class="ltx_para ltx_noindent">
<p id="A3.SS3.p2.1" class="ltx_p">We also measure the relationship between standard in-distribution accuracy and robustness based on the accuracy as each factor in PUG:ImageNet varies. We measure Pearson’s correlation coefficient between ImageNet accuracy and accuracy for each factor in Table <a href="#A3.T10" title="Table 10 ‣ C.3 Robustness of SOTA models additional details ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. We find no statistically significant relationship between standard classification accuracy and factor robustness.</p>
</div>
<figure id="A3.T9" class="ltx_table">
<div id="A3.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:885.2pt;height:271pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A3.T9.1.1" class="ltx_p"><span id="A3.T9.1.1.1" class="ltx_text">
<span id="A3.T9.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A3.T9.1.1.1.1.1.1" class="ltx_tr">
<span id="A3.T9.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PUG: ImageNet</span>
<span id="A3.T9.1.1.1.1.1.1.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.9" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.10" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.11" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.1.1.12" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></span></span>
<span id="A3.T9.1.1.1.1.2.2" class="ltx_tr">
<span id="A3.T9.1.1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="A3.T9.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">ImageNet</span>
<span id="A3.T9.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Camera_Yaw</span>
<span id="A3.T9.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Camera_Pitch</span>
<span id="A3.T9.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Camera_Roll</span>
<span id="A3.T9.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Object_Yaw</span>
<span id="A3.T9.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Object_Pitch</span>
<span id="A3.T9.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Object_Roll</span>
<span id="A3.T9.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Object_Scale</span>
<span id="A3.T9.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Object_Texture</span>
<span id="A3.T9.1.1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Scene_Light</span>
<span id="A3.T9.1.1.1.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Background</span></span>
</span>
<span class="ltx_tbody">
<span id="A3.T9.1.1.1.1.3.1" class="ltx_tr">
<span id="A3.T9.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</span>
<span id="A3.T9.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">81.5</span>
<span id="A3.T9.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">38.1</span>
<span id="A3.T9.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">33.1</span>
<span id="A3.T9.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">26.9</span>
<span id="A3.T9.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">38.0</span>
<span id="A3.T9.1.1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">23.6</span>
<span id="A3.T9.1.1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">22.9</span>
<span id="A3.T9.1.1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">35.7</span>
<span id="A3.T9.1.1.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">27.0</span>
<span id="A3.T9.1.1.1.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t">13.6</span>
<span id="A3.T9.1.1.1.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t">29.5</span></span>
<span id="A3.T9.1.1.1.1.4.2" class="ltx_tr">
<span id="A3.T9.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ResNet101</span>
<span id="A3.T9.1.1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">82.3</span>
<span id="A3.T9.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">43.4</span>
<span id="A3.T9.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">35.9</span>
<span id="A3.T9.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">29.4</span>
<span id="A3.T9.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">45.1</span>
<span id="A3.T9.1.1.1.1.4.2.7" class="ltx_td ltx_align_center">26.7</span>
<span id="A3.T9.1.1.1.1.4.2.8" class="ltx_td ltx_align_center">25.6</span>
<span id="A3.T9.1.1.1.1.4.2.9" class="ltx_td ltx_align_center">39.7</span>
<span id="A3.T9.1.1.1.1.4.2.10" class="ltx_td ltx_align_center">31.1</span>
<span id="A3.T9.1.1.1.1.4.2.11" class="ltx_td ltx_align_center">14.1</span>
<span id="A3.T9.1.1.1.1.4.2.12" class="ltx_td ltx_align_center">32.8</span></span>
<span id="A3.T9.1.1.1.1.5.3" class="ltx_tr">
<span id="A3.T9.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BiT (JFT300M)</span>
<span id="A3.T9.1.1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">80.3</span>
<span id="A3.T9.1.1.1.1.5.3.3" class="ltx_td ltx_align_center">40.5</span>
<span id="A3.T9.1.1.1.1.5.3.4" class="ltx_td ltx_align_center">32.3</span>
<span id="A3.T9.1.1.1.1.5.3.5" class="ltx_td ltx_align_center">26.0</span>
<span id="A3.T9.1.1.1.1.5.3.6" class="ltx_td ltx_align_center">42.1</span>
<span id="A3.T9.1.1.1.1.5.3.7" class="ltx_td ltx_align_center">23.6</span>
<span id="A3.T9.1.1.1.1.5.3.8" class="ltx_td ltx_align_center">22.8</span>
<span id="A3.T9.1.1.1.1.5.3.9" class="ltx_td ltx_align_center">37.3</span>
<span id="A3.T9.1.1.1.1.5.3.10" class="ltx_td ltx_align_center">23.4</span>
<span id="A3.T9.1.1.1.1.5.3.11" class="ltx_td ltx_align_center">6.3</span>
<span id="A3.T9.1.1.1.1.5.3.12" class="ltx_td ltx_align_center">20.5</span></span>
<span id="A3.T9.1.1.1.1.6.4" class="ltx_tr">
<span id="A3.T9.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DINOv2 (LVD-142M)</span>
<span id="A3.T9.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">84.5</span>
<span id="A3.T9.1.1.1.1.6.4.3" class="ltx_td ltx_align_center">45.6</span>
<span id="A3.T9.1.1.1.1.6.4.4" class="ltx_td ltx_align_center">41.1</span>
<span id="A3.T9.1.1.1.1.6.4.5" class="ltx_td ltx_align_center">37.4</span>
<span id="A3.T9.1.1.1.1.6.4.6" class="ltx_td ltx_align_center">47.5</span>
<span id="A3.T9.1.1.1.1.6.4.7" class="ltx_td ltx_align_center">28.8</span>
<span id="A3.T9.1.1.1.1.6.4.8" class="ltx_td ltx_align_center">28.5</span>
<span id="A3.T9.1.1.1.1.6.4.9" class="ltx_td ltx_align_center">43.1</span>
<span id="A3.T9.1.1.1.1.6.4.10" class="ltx_td ltx_align_center">35.0</span>
<span id="A3.T9.1.1.1.1.6.4.11" class="ltx_td ltx_align_center">6.1</span>
<span id="A3.T9.1.1.1.1.6.4.12" class="ltx_td ltx_align_center">30.9</span></span>
<span id="A3.T9.1.1.1.1.7.5" class="ltx_tr">
<span id="A3.T9.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Flava (PMD 70M)</span>
<span id="A3.T9.1.1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.5</span>
<span id="A3.T9.1.1.1.1.7.5.3" class="ltx_td ltx_align_center">31.7</span>
<span id="A3.T9.1.1.1.1.7.5.4" class="ltx_td ltx_align_center">23.4</span>
<span id="A3.T9.1.1.1.1.7.5.5" class="ltx_td ltx_align_center">17.6</span>
<span id="A3.T9.1.1.1.1.7.5.6" class="ltx_td ltx_align_center">30.8</span>
<span id="A3.T9.1.1.1.1.7.5.7" class="ltx_td ltx_align_center">17.6</span>
<span id="A3.T9.1.1.1.1.7.5.8" class="ltx_td ltx_align_center">15.4</span>
<span id="A3.T9.1.1.1.1.7.5.9" class="ltx_td ltx_align_center">30.5</span>
<span id="A3.T9.1.1.1.1.7.5.10" class="ltx_td ltx_align_center">24.2</span>
<span id="A3.T9.1.1.1.1.7.5.11" class="ltx_td ltx_align_center">7.8</span>
<span id="A3.T9.1.1.1.1.7.5.12" class="ltx_td ltx_align_center">21.9</span></span>
<span id="A3.T9.1.1.1.1.8.6" class="ltx_tr">
<span id="A3.T9.1.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Swin</span>
<span id="A3.T9.1.1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">83.6</span>
<span id="A3.T9.1.1.1.1.8.6.3" class="ltx_td ltx_align_center">56.0</span>
<span id="A3.T9.1.1.1.1.8.6.4" class="ltx_td ltx_align_center">45.6</span>
<span id="A3.T9.1.1.1.1.8.6.5" class="ltx_td ltx_align_center">41.8</span>
<span id="A3.T9.1.1.1.1.8.6.6" class="ltx_td ltx_align_center">56.9</span>
<span id="A3.T9.1.1.1.1.8.6.7" class="ltx_td ltx_align_center">35.3</span>
<span id="A3.T9.1.1.1.1.8.6.8" class="ltx_td ltx_align_center">34.2</span>
<span id="A3.T9.1.1.1.1.8.6.9" class="ltx_td ltx_align_center">52.9</span>
<span id="A3.T9.1.1.1.1.8.6.10" class="ltx_td ltx_align_center">40.1</span>
<span id="A3.T9.1.1.1.1.8.6.11" class="ltx_td ltx_align_center">19.1</span>
<span id="A3.T9.1.1.1.1.8.6.12" class="ltx_td ltx_align_center">42.0</span></span>
<span id="A3.T9.1.1.1.1.9.7" class="ltx_tr">
<span id="A3.T9.1.1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViT-Base</span>
<span id="A3.T9.1.1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">84.3</span>
<span id="A3.T9.1.1.1.1.9.7.3" class="ltx_td ltx_align_center">37.5</span>
<span id="A3.T9.1.1.1.1.9.7.4" class="ltx_td ltx_align_center">34.3</span>
<span id="A3.T9.1.1.1.1.9.7.5" class="ltx_td ltx_align_center">31.7</span>
<span id="A3.T9.1.1.1.1.9.7.6" class="ltx_td ltx_align_center">38.0</span>
<span id="A3.T9.1.1.1.1.9.7.7" class="ltx_td ltx_align_center">21.8</span>
<span id="A3.T9.1.1.1.1.9.7.8" class="ltx_td ltx_align_center">20.5</span>
<span id="A3.T9.1.1.1.1.9.7.9" class="ltx_td ltx_align_center">33.0</span>
<span id="A3.T9.1.1.1.1.9.7.10" class="ltx_td ltx_align_center">28.5</span>
<span id="A3.T9.1.1.1.1.9.7.11" class="ltx_td ltx_align_center">4.1</span>
<span id="A3.T9.1.1.1.1.9.7.12" class="ltx_td ltx_align_center">26.6</span></span>
<span id="A3.T9.1.1.1.1.10.8" class="ltx_tr">
<span id="A3.T9.1.1.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViT-Large</span>
<span id="A3.T9.1.1.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">85.8</span>
<span id="A3.T9.1.1.1.1.10.8.3" class="ltx_td ltx_align_center">52.2</span>
<span id="A3.T9.1.1.1.1.10.8.4" class="ltx_td ltx_align_center">40.4</span>
<span id="A3.T9.1.1.1.1.10.8.5" class="ltx_td ltx_align_center">37.1</span>
<span id="A3.T9.1.1.1.1.10.8.6" class="ltx_td ltx_align_center">52.4</span>
<span id="A3.T9.1.1.1.1.10.8.7" class="ltx_td ltx_align_center">30.4</span>
<span id="A3.T9.1.1.1.1.10.8.8" class="ltx_td ltx_align_center">28.4</span>
<span id="A3.T9.1.1.1.1.10.8.9" class="ltx_td ltx_align_center">46.4</span>
<span id="A3.T9.1.1.1.1.10.8.10" class="ltx_td ltx_align_center">42.9</span>
<span id="A3.T9.1.1.1.1.10.8.11" class="ltx_td ltx_align_center">8.9</span>
<span id="A3.T9.1.1.1.1.10.8.12" class="ltx_td ltx_align_center">34.6</span></span>
<span id="A3.T9.1.1.1.1.11.9" class="ltx_tr">
<span id="A3.T9.1.1.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BLIP (100+M)</span>
<span id="A3.T9.1.1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">–</span>
<span id="A3.T9.1.1.1.1.11.9.3" class="ltx_td ltx_align_center">0.5</span>
<span id="A3.T9.1.1.1.1.11.9.4" class="ltx_td ltx_align_center">0.4</span>
<span id="A3.T9.1.1.1.1.11.9.5" class="ltx_td ltx_align_center">0.5</span>
<span id="A3.T9.1.1.1.1.11.9.6" class="ltx_td ltx_align_center">0.8</span>
<span id="A3.T9.1.1.1.1.11.9.7" class="ltx_td ltx_align_center">0.6</span>
<span id="A3.T9.1.1.1.1.11.9.8" class="ltx_td ltx_align_center">0.7</span>
<span id="A3.T9.1.1.1.1.11.9.9" class="ltx_td ltx_align_center">0.9</span>
<span id="A3.T9.1.1.1.1.11.9.10" class="ltx_td ltx_align_center">0.7</span>
<span id="A3.T9.1.1.1.1.11.9.11" class="ltx_td ltx_align_center">0.7</span>
<span id="A3.T9.1.1.1.1.11.9.12" class="ltx_td ltx_align_center">0.7</span></span>
<span id="A3.T9.1.1.1.1.12.10" class="ltx_tr">
<span id="A3.T9.1.1.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIPViTB32 (2B)</span>
<span id="A3.T9.1.1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">66.6</span>
<span id="A3.T9.1.1.1.1.12.10.3" class="ltx_td ltx_align_center">44.0</span>
<span id="A3.T9.1.1.1.1.12.10.4" class="ltx_td ltx_align_center">31.5</span>
<span id="A3.T9.1.1.1.1.12.10.5" class="ltx_td ltx_align_center">24.1</span>
<span id="A3.T9.1.1.1.1.12.10.6" class="ltx_td ltx_align_center">43.8</span>
<span id="A3.T9.1.1.1.1.12.10.7" class="ltx_td ltx_align_center">24.8</span>
<span id="A3.T9.1.1.1.1.12.10.8" class="ltx_td ltx_align_center">21.8</span>
<span id="A3.T9.1.1.1.1.12.10.9" class="ltx_td ltx_align_center">42.2</span>
<span id="A3.T9.1.1.1.1.12.10.10" class="ltx_td ltx_align_center">34.7</span>
<span id="A3.T9.1.1.1.1.12.10.11" class="ltx_td ltx_align_center">3.3</span>
<span id="A3.T9.1.1.1.1.12.10.12" class="ltx_td ltx_align_center">26.0</span></span>
<span id="A3.T9.1.1.1.1.13.11" class="ltx_tr">
<span id="A3.T9.1.1.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIPViTB32 (400M)</span>
<span id="A3.T9.1.1.1.1.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">62.9</span>
<span id="A3.T9.1.1.1.1.13.11.3" class="ltx_td ltx_align_center">41.7</span>
<span id="A3.T9.1.1.1.1.13.11.4" class="ltx_td ltx_align_center">30.2</span>
<span id="A3.T9.1.1.1.1.13.11.5" class="ltx_td ltx_align_center">22.1</span>
<span id="A3.T9.1.1.1.1.13.11.6" class="ltx_td ltx_align_center">41.6</span>
<span id="A3.T9.1.1.1.1.13.11.7" class="ltx_td ltx_align_center">23.8</span>
<span id="A3.T9.1.1.1.1.13.11.8" class="ltx_td ltx_align_center">20.9</span>
<span id="A3.T9.1.1.1.1.13.11.9" class="ltx_td ltx_align_center">40.1</span>
<span id="A3.T9.1.1.1.1.13.11.10" class="ltx_td ltx_align_center">34.4</span>
<span id="A3.T9.1.1.1.1.13.11.11" class="ltx_td ltx_align_center">5.7</span>
<span id="A3.T9.1.1.1.1.13.11.12" class="ltx_td ltx_align_center">24.4</span></span>
<span id="A3.T9.1.1.1.1.14.12" class="ltx_tr">
<span id="A3.T9.1.1.1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CLIPViTL14 (2B)</span>
<span id="A3.T9.1.1.1.1.14.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.3</span>
<span id="A3.T9.1.1.1.1.14.12.3" class="ltx_td ltx_align_center">49.7</span>
<span id="A3.T9.1.1.1.1.14.12.4" class="ltx_td ltx_align_center">34.9</span>
<span id="A3.T9.1.1.1.1.14.12.5" class="ltx_td ltx_align_center">28.2</span>
<span id="A3.T9.1.1.1.1.14.12.6" class="ltx_td ltx_align_center">50.3</span>
<span id="A3.T9.1.1.1.1.14.12.7" class="ltx_td ltx_align_center">26.3</span>
<span id="A3.T9.1.1.1.1.14.12.8" class="ltx_td ltx_align_center">25.3</span>
<span id="A3.T9.1.1.1.1.14.12.9" class="ltx_td ltx_align_center">46.8</span>
<span id="A3.T9.1.1.1.1.14.12.10" class="ltx_td ltx_align_center">39.4</span>
<span id="A3.T9.1.1.1.1.14.12.11" class="ltx_td ltx_align_center">4.8</span>
<span id="A3.T9.1.1.1.1.14.12.12" class="ltx_td ltx_align_center">30.8</span></span>
<span id="A3.T9.1.1.1.1.15.13" class="ltx_tr">
<span id="A3.T9.1.1.1.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">CLIPViTL14 (400M)</span>
<span id="A3.T9.1.1.1.1.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">72.8</span>
<span id="A3.T9.1.1.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_bb">52.3</span>
<span id="A3.T9.1.1.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_bb">39.8</span>
<span id="A3.T9.1.1.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_bb">35.7</span>
<span id="A3.T9.1.1.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_bb">51.8</span>
<span id="A3.T9.1.1.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_bb">29.0</span>
<span id="A3.T9.1.1.1.1.15.13.8" class="ltx_td ltx_align_center ltx_border_bb">26.4</span>
<span id="A3.T9.1.1.1.1.15.13.9" class="ltx_td ltx_align_center ltx_border_bb">50.6</span>
<span id="A3.T9.1.1.1.1.15.13.10" class="ltx_td ltx_align_center ltx_border_bb">41.1</span>
<span id="A3.T9.1.1.1.1.15.13.11" class="ltx_td ltx_align_center ltx_border_bb">4.3</span>
<span id="A3.T9.1.1.1.1.15.13.12" class="ltx_td ltx_align_center ltx_border_bb">33.0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Robustness measured by average accuracy across factors. We report zero shot classification accuracy for BLIP, Flava, all CLIP models. The pretraining dataset is indicated in parenthesis next to each model name with ImageNet-1k being the default unless otherwise indicated.</figcaption>
</figure>
<figure id="A3.T10" class="ltx_table">
<table id="A3.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T10.1.1.1" class="ltx_tr">
<th id="A3.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">factor</th>
<th id="A3.T10.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">correlation</th>
<th id="A3.T10.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">pvalue</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T10.1.2.1" class="ltx_tr">
<th id="A3.T10.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Object Pitch</th>
<td id="A3.T10.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.29</td>
<td id="A3.T10.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">0.45</td>
</tr>
<tr id="A3.T10.1.3.2" class="ltx_tr">
<th id="A3.T10.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Camera Roll</th>
<td id="A3.T10.1.3.2.2" class="ltx_td ltx_align_right">0.61</td>
<td id="A3.T10.1.3.2.3" class="ltx_td ltx_align_right">0.08</td>
</tr>
<tr id="A3.T10.1.4.3" class="ltx_tr">
<th id="A3.T10.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Camera Pitch</th>
<td id="A3.T10.1.4.3.2" class="ltx_td ltx_align_right">0.53</td>
<td id="A3.T10.1.4.3.3" class="ltx_td ltx_align_right">0.14</td>
</tr>
<tr id="A3.T10.1.5.4" class="ltx_tr">
<th id="A3.T10.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Camera Yaw</th>
<td id="A3.T10.1.5.4.2" class="ltx_td ltx_align_right">0.12</td>
<td id="A3.T10.1.5.4.3" class="ltx_td ltx_align_right">0.76</td>
</tr>
<tr id="A3.T10.1.6.5" class="ltx_tr">
<th id="A3.T10.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Background</th>
<td id="A3.T10.1.6.5.2" class="ltx_td ltx_align_right">0.43</td>
<td id="A3.T10.1.6.5.3" class="ltx_td ltx_align_right">0.25</td>
</tr>
<tr id="A3.T10.1.7.6" class="ltx_tr">
<th id="A3.T10.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Object Yaw</th>
<td id="A3.T10.1.7.6.2" class="ltx_td ltx_align_right">0.18</td>
<td id="A3.T10.1.7.6.3" class="ltx_td ltx_align_right">0.64</td>
</tr>
<tr id="A3.T10.1.8.7" class="ltx_tr">
<th id="A3.T10.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Object Texture</th>
<td id="A3.T10.1.8.7.2" class="ltx_td ltx_align_right">-0.10</td>
<td id="A3.T10.1.8.7.3" class="ltx_td ltx_align_right">0.81</td>
</tr>
<tr id="A3.T10.1.9.8" class="ltx_tr">
<th id="A3.T10.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Scene Light</th>
<td id="A3.T10.1.9.8.2" class="ltx_td ltx_align_right">0.53</td>
<td id="A3.T10.1.9.8.3" class="ltx_td ltx_align_right">0.14</td>
</tr>
<tr id="A3.T10.1.10.9" class="ltx_tr">
<th id="A3.T10.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Object Scale</th>
<td id="A3.T10.1.10.9.2" class="ltx_td ltx_align_right">-0.05</td>
<td id="A3.T10.1.10.9.3" class="ltx_td ltx_align_right">0.90</td>
</tr>
<tr id="A3.T10.1.11.10" class="ltx_tr">
<th id="A3.T10.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Object Roll</th>
<td id="A3.T10.1.11.10.2" class="ltx_td ltx_align_right ltx_border_bb">0.45</td>
<td id="A3.T10.1.11.10.3" class="ltx_td ltx_align_right ltx_border_bb">0.22</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>We compute the correlation between standard ImageNet classification and robustness based on the accuracy for each factor. We find no statistically significant relationship for standard classification and factor robustness.</figcaption>
</figure>
<section id="A3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.1 </span>Performances</h4>

<div id="A3.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A3.SS3.SSS1.p1.1" class="ltx_p">To understand if the differences in performance between the real ImageNet and our PUG dataset is caused by a sim-to-real gap or by the factor of variations, we show below the zero-shot accuracy obtained with a pretrained resnet101 for each class in PUG: ImageNet. There is only 3 classes for which there is not a single configurations of the factors that lead to a correct classification. For all the other classes, there is always at least one configuration for which the network is correctly predicting the class. In that instance, we assume that if the objects in a given class are correctly predicted at least one time, the failures in predicting the correct class for the same objects with different factors is probably due to the changes of factors.</p>
</div>
<div id="A3.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="A3.SS3.SSS1.p2.1" class="ltx_p"><span id="A3.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Zero-shot top-1 accuracy per class </span>Soccer_Ball: 100.00 , Pineapple: 95.62 , Barrel: 95.00 , Cellular_telephone: 89.45 , Pomegranate: 88.12 , Jack_O_Lantern: 85.94 , Vase: 85.62 , BirdHouse: 81.88 , BasketBall: 81.25 , Sewing_Machine: 80.94 , Umbrella: 80.00 , Washer: 78.75 , Pool_Table: 76.88 , Baseball: 76.88 , Safe: 76.25 , Cabbage: 75.78 , Cofee_Mug: 75.31 , Mask: 74.06 , Brocolli: 73.44 , Starfish: 72.50 , Rocking_Chair: 71.25 , Punching_Bag: 70.94 , Chicken_hen: 69.38 , WineBottle: 66.88 , Gasmask: 66.56 , Joystick: 64.06 , Television: 63.44 , Chest: 63.44 , Elephant: 62.50 , Bell_Pepper: 61.46 , Cheeseburger: 60.62 , Pay_Phone: 60.00 , Tennis_Ball: 58.44 , Jean: 56.25 , Binocular: 55.86 , Racket: 55.62 , Motor_Scooter: 55.00 , Hay: 54.06 , Park_Bench: 53.44 , Bookcase: 53.44 , Zucchini: 52.08 , Banana: 50.00 , Sliding_Door: 48.75 , Military_uniform: 47.50 , Ambulance: 47.50 , Pizza: 47.19 , Tractor: 46.88 , Dishwasher: 46.88 , Cowboy_Hat: 46.35 , Drum: 45.31 , Typewriter_Keyboard: 44.92 , Toaster: 44.69 , Obelisk: 44.38 , Laptop: 44.38 , Throne: 43.75 , Backpack: 43.44 , Shield: 41.88 , Artichoke: 41.80 , Penguin: 41.56 , Bathtub: 40.31 , WaterBottle: 40.00 , SpaceShuttle: 37.19 , Bagel: 36.88 , Bear: 36.25 , Vacuum: 35.94 , Radiator: 35.62 , Shovel: 35.55 , Refrigerator: 35.00 , Running_Shoe: 34.38 , Goldfish: 34.38 , Crate: 34.38 , Polaroid_Camera: 33.98 , Table_Lamb: 33.75 , Bulletproof_vest: 33.75 , Microphone: 33.12 , Traffic_Light: 32.50 , Carton: 31.25 , Volley_Ball: 30.62 , Vending_machine: 30.62 , Lawn_Mower: 29.38 , Car_Wheel: 29.38 , Harmonica: 28.12 , Lighter: 27.50 , Carousel: 27.34 , Mailbox: 27.19 , Airliner: 27.19 , Butternut_Squash: 26.95 , Sweatshirt: 26.56 , Sock: 25.62 , French_Loaf: 25.00 , Dial_telephone: 24.61 , Rabbit: 24.06 , Remote_controler: 22.81 , Modem: 22.50 , Chain-saw: 21.35 , Screwdriver: 20.31 , Power-drill: 19.69 , Electric_Fan: 19.06 , HairDryer: 18.75 , Purse: 18.12 , Wallet: 17.50 , Sunglasses: 17.50 , Minivan: 17.50 , Cat: 15.94 , Microwave: 15.62 , Candle: 15.62 , Mushroom: 15.31 , Dining_Table: 14.06 , Ice_Cream: 13.75 , Perfume: 13.44 , Komodo_dragon: 13.44 , Bycicle: 13.12 , Wooden_Spoon: 12.81 , JellyFish: 12.81 , Canoe: 12.81 , Radio: 12.19 , Desk: 12.19 , African_crocodile: 11.88 , Hatchet: 11.25 , Sandal: 10.00 , Stonewall: 9.69 , Burrito: 9.38 , Palace: 9.06 , Mouse: 7.50 , Convertible: 7.19 , Espresso_maker: 6.88 , Can_Opener: 6.56 , Jeep: 6.25 , Fox: 6.25 , Tile_Roof: 5.86 , Street_Sign: 5.62 , WarPlane: 5.47 , Frog: 5.47 , Wolf: 5.31 , Whale: 5.00 , Torch: 5.00 , Soup_Bowl: 4.69 , Great_white_shark: 4.38 , Kangarou: 3.91 , Digital_Watch: 2.81 , Skirt: 2.50 , Computer_Keyboard: 2.50 , Piano: 1.88 , Manhole_Cover: 1.88 , Bridge: 0.94 , Aircraft_Carrier: 0.39 , Screen: 0.31 , Locomotive: 0.31 , Submarine: 0.00 , Shirt: 0.00 , Loupe: 0.00</p>
</div>
</section>
</section>
<section id="A3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Additional PUG:SPAR experiments</h3>

<div id="A3.SS4.p1" class="ltx_para ltx_noindent">
<p id="A3.SS4.p1.1" class="ltx_p">Instead of using all the background environments presented in the main
part of the paper, we also introduce a much simple setup in which we have a single background (thus we do not need the background information in the caption anymore). The background that we choose is the simplest one named "salt flats". This is also the background for which the retrieval accuracy is the highest across all the backgrounds. In table <a href="#A3.T11" title="Table 11 ‣ C.4 Additional PUG:SPAR experiments ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, we present the performances of several VLMs when using this single environment. We can observe a significant boost in accuracy for the single object detection task for which the best model achieve 94% accuracy (this value is to contrast with the 78% accuracy obtained across all the background). This show that VLMs are definitively not robust to background changes. However as in the previous case, when probing for the positional information, the performance of the model is still decreasing significantly. We also illustrate in Figure <a href="#A3.F12" title="Figure 12 ‣ C.4 Additional PUG:SPAR experiments ‣ Appendix C Additional experimental details ‣ PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> very simple failure cases on the best model.</p>
</div>
<figure id="A3.T11" class="ltx_table">
<table id="A3.T11.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.T11.1.1.1" class="ltx_tr">
<td id="A3.T11.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Caption</span>
</span>
</td>
<td id="A3.T11.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.1.1.2.1.1" class="ltx_p" style="width:19.9pt;">Texture</span>
</span>
</td>
<td id="A3.T11.1.1.1.3" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.3.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.3.1.1" class="ltx_text ltx_inline-block" style="width:104.3pt;">
<span id="A3.T11.1.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:114.3pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.3.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.3.1.1.1.1.1" class="ltx_text">ViT-B-32 (OpenAI CLIP)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.4" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.4.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.4.1.1" class="ltx_text ltx_inline-block" style="width:99.7pt;">
<span id="A3.T11.1.1.1.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:109.7pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.4.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.4.1.1.1.1.1" class="ltx_text">ViT-B-32 (OpenClip 2B)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.5" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.5.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.5.1.1" class="ltx_text ltx_inline-block" style="width:103.5pt;">
<span id="A3.T11.1.1.1.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:113.5pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.5.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.5.1.1.1.1.1" class="ltx_text">ViT-L-14 (OpenAI CLIP)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.6" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.6.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.6.1.1" class="ltx_text ltx_inline-block" style="width:98.9pt;">
<span id="A3.T11.1.1.1.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:108.9pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.6.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.6.1.1.1.1.1" class="ltx_text">ViT-L-14 (OpenClip 2B)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.7" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.7.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.7.1.1" class="ltx_text ltx_inline-block" style="width:105.7pt;">
<span id="A3.T11.1.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:115.7pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.7.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.7.1.1.1.1.1" class="ltx_text">ViT-H-14 (OpenCLIP 2B)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.8" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.8.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.8.1.1" class="ltx_text ltx_inline-block" style="width:106.0pt;">
<span id="A3.T11.1.1.1.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:116.0pt;height:10pt;vertical-align:-2.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.8.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.8.1.1.1.1.1" class="ltx_text">ViT-G-14 (OpenCLIP 2B)</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.9" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.9.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.9.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="A3.T11.1.1.1.9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.9.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.9.1.1.1.1.1" class="ltx_text">Flava</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.10" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.10.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.10.1.1" class="ltx_text ltx_inline-block" style="width:13.8pt;">
<span id="A3.T11.1.1.1.10.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:23.8pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.10.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.10.1.1.1.1.1" class="ltx_text">BLIP</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.11" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.11.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.11.1.1" class="ltx_text ltx_inline-block" style="width:20.4pt;">
<span id="A3.T11.1.1.1.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:30.4pt;height:6.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.11.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.11.1.1.1.1.1" class="ltx_text">XVLM</span></span>
</span></span></span></span></td>
<td id="A3.T11.1.1.1.12" class="ltx_td ltx_align_left"><span id="A3.T11.1.1.1.12.1" class="ltx_text ltx_inline-block" style="width:10.0pt;">   <span id="A3.T11.1.1.1.12.1.1" class="ltx_text ltx_inline-block" style="width:30.8pt;">
<span id="A3.T11.1.1.1.12.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:40.8pt;height:8.699999999999999pt;vertical-align:-1.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A3.T11.1.1.1.12.1.1.1.1" class="ltx_p"><span id="A3.T11.1.1.1.12.1.1.1.1.1" class="ltx_text">NegCLIP</span></span>
</span></span></span></span></td>
</tr>
<tr id="A3.T11.1.2.2" class="ltx_tr">
<td id="A3.T11.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="A3.T11.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.2.2.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A3.T11.1.2.2.1.1.1.1" class="ltx_text"><em id="A3.T11.1.2.2.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character]“</em></span></span>
</span>
</td>
<td id="A3.T11.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.2.2.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">57.81</td>
<td id="A3.T11.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">75.78</td>
<td id="A3.T11.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">91.41</td>
<td id="A3.T11.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t">88.28</td>
<td id="A3.T11.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t">94.53</td>
<td id="A3.T11.1.2.2.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.2.2.8.1" class="ltx_text ltx_font_bold">94.53</span></td>
<td id="A3.T11.1.2.2.9" class="ltx_td ltx_align_left ltx_border_t">64.06</td>
<td id="A3.T11.1.2.2.10" class="ltx_td ltx_align_left ltx_border_t">78.91</td>
<td id="A3.T11.1.2.2.11" class="ltx_td ltx_align_left ltx_border_t">67.19</td>
<td id="A3.T11.1.2.2.12" class="ltx_td ltx_align_left ltx_border_t">64.84</td>
</tr>
<tr id="A3.T11.1.3.3" class="ltx_tr">
<td id="A3.T11.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.3.3.1.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="A3.T11.1.3.3.2" class="ltx_td ltx_align_left">48.44</td>
<td id="A3.T11.1.3.3.3" class="ltx_td ltx_align_left">54.69</td>
<td id="A3.T11.1.3.3.4" class="ltx_td ltx_align_left">71.88</td>
<td id="A3.T11.1.3.3.5" class="ltx_td ltx_align_left">70.31</td>
<td id="A3.T11.1.3.3.6" class="ltx_td ltx_align_left">75.00</td>
<td id="A3.T11.1.3.3.7" class="ltx_td ltx_align_left"><span id="A3.T11.1.3.3.7.1" class="ltx_text ltx_font_bold">84.38</span></td>
<td id="A3.T11.1.3.3.8" class="ltx_td ltx_align_left">42.19</td>
<td id="A3.T11.1.3.3.9" class="ltx_td ltx_align_left">53.12</td>
<td id="A3.T11.1.3.3.10" class="ltx_td ltx_align_left">48.44</td>
<td id="A3.T11.1.3.3.11" class="ltx_td ltx_align_left">48.44</td>
</tr>
<tr id="A3.T11.1.4.4" class="ltx_tr">
<td id="A3.T11.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.4.4.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="A3.T11.1.4.4.2" class="ltx_td ltx_align_left">39.06</td>
<td id="A3.T11.1.4.4.3" class="ltx_td ltx_align_left">45.31</td>
<td id="A3.T11.1.4.4.4" class="ltx_td ltx_align_left">67.19</td>
<td id="A3.T11.1.4.4.5" class="ltx_td ltx_align_left">68.75</td>
<td id="A3.T11.1.4.4.6" class="ltx_td ltx_align_left">76.56</td>
<td id="A3.T11.1.4.4.7" class="ltx_td ltx_align_left"><span id="A3.T11.1.4.4.7.1" class="ltx_text ltx_font_bold">78.12</span></td>
<td id="A3.T11.1.4.4.8" class="ltx_td ltx_align_left">39.06</td>
<td id="A3.T11.1.4.4.9" class="ltx_td ltx_align_left">56.25</td>
<td id="A3.T11.1.4.4.10" class="ltx_td ltx_align_left">45.31</td>
<td id="A3.T11.1.4.4.11" class="ltx_td ltx_align_left">50.00</td>
</tr>
<tr id="A3.T11.1.5.5" class="ltx_tr">
<td id="A3.T11.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.5.5.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="A3.T11.1.5.5.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the (left/right) of the picture“</em></span>
</span>
</td>
<td id="A3.T11.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.5.5.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.5.5.3" class="ltx_td ltx_align_left ltx_border_t">29.69</td>
<td id="A3.T11.1.5.5.4" class="ltx_td ltx_align_left ltx_border_t">37.50</td>
<td id="A3.T11.1.5.5.5" class="ltx_td ltx_align_left ltx_border_t">39.06</td>
<td id="A3.T11.1.5.5.6" class="ltx_td ltx_align_left ltx_border_t">40.62</td>
<td id="A3.T11.1.5.5.7" class="ltx_td ltx_align_left ltx_border_t">50.00</td>
<td id="A3.T11.1.5.5.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.5.5.8.1" class="ltx_text ltx_font_bold">51.56</span></td>
<td id="A3.T11.1.5.5.9" class="ltx_td ltx_align_left ltx_border_t">32.81</td>
<td id="A3.T11.1.5.5.10" class="ltx_td ltx_align_left ltx_border_t">42.19</td>
<td id="A3.T11.1.5.5.11" class="ltx_td ltx_align_left ltx_border_t">34.38</td>
<td id="A3.T11.1.5.5.12" class="ltx_td ltx_align_left ltx_border_t">31.25</td>
</tr>
<tr id="A3.T11.1.6.6" class="ltx_tr">
<td id="A3.T11.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.6.6.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="A3.T11.1.6.6.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the (bottom/top) of the picture“</em></span>
</span>
</td>
<td id="A3.T11.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.6.6.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t">29.69</td>
<td id="A3.T11.1.6.6.4" class="ltx_td ltx_align_left ltx_border_t">23.44</td>
<td id="A3.T11.1.6.6.5" class="ltx_td ltx_align_left ltx_border_t">45.31</td>
<td id="A3.T11.1.6.6.6" class="ltx_td ltx_align_left ltx_border_t">43.75</td>
<td id="A3.T11.1.6.6.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.6.6.7.1" class="ltx_text ltx_font_bold">46.88</span></td>
<td id="A3.T11.1.6.6.8" class="ltx_td ltx_align_left ltx_border_t">45.31</td>
<td id="A3.T11.1.6.6.9" class="ltx_td ltx_align_left ltx_border_t">23.44</td>
<td id="A3.T11.1.6.6.10" class="ltx_td ltx_align_left ltx_border_t">34.38</td>
<td id="A3.T11.1.6.6.11" class="ltx_td ltx_align_left ltx_border_t">34.38</td>
<td id="A3.T11.1.6.6.12" class="ltx_td ltx_align_left ltx_border_t">23.44</td>
</tr>
<tr id="A3.T11.1.7.7" class="ltx_tr">
<td id="A3.T11.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="A3.T11.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.7.7.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A3.T11.1.7.7.1.1.1.1" class="ltx_text"><em id="A3.T11.1.7.7.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] and a [character]“</em></span></span>
</span>
</td>
<td id="A3.T11.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.7.7.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.7.7.3" class="ltx_td ltx_align_left ltx_border_t">24.56</td>
<td id="A3.T11.1.7.7.4" class="ltx_td ltx_align_left ltx_border_t">32.47</td>
<td id="A3.T11.1.7.7.5" class="ltx_td ltx_align_left ltx_border_t">68.85</td>
<td id="A3.T11.1.7.7.6" class="ltx_td ltx_align_left ltx_border_t">59.67</td>
<td id="A3.T11.1.7.7.7" class="ltx_td ltx_align_left ltx_border_t">72.66</td>
<td id="A3.T11.1.7.7.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.7.7.8.1" class="ltx_text ltx_font_bold">80.96</span></td>
<td id="A3.T11.1.7.7.9" class="ltx_td ltx_align_left ltx_border_t">18.55</td>
<td id="A3.T11.1.7.7.10" class="ltx_td ltx_align_left ltx_border_t">40.58</td>
<td id="A3.T11.1.7.7.11" class="ltx_td ltx_align_left ltx_border_t">24.71</td>
<td id="A3.T11.1.7.7.12" class="ltx_td ltx_align_left ltx_border_t">21.04</td>
</tr>
<tr id="A3.T11.1.8.8" class="ltx_tr">
<td id="A3.T11.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.8.8.1.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="A3.T11.1.8.8.2" class="ltx_td ltx_align_left">10.84</td>
<td id="A3.T11.1.8.8.3" class="ltx_td ltx_align_left">18.55</td>
<td id="A3.T11.1.8.8.4" class="ltx_td ltx_align_left">38.57</td>
<td id="A3.T11.1.8.8.5" class="ltx_td ltx_align_left">30.66</td>
<td id="A3.T11.1.8.8.6" class="ltx_td ltx_align_left">34.28</td>
<td id="A3.T11.1.8.8.7" class="ltx_td ltx_align_left"><span id="A3.T11.1.8.8.7.1" class="ltx_text ltx_font_bold">51.56</span></td>
<td id="A3.T11.1.8.8.8" class="ltx_td ltx_align_left">9.86</td>
<td id="A3.T11.1.8.8.9" class="ltx_td ltx_align_left">10.94</td>
<td id="A3.T11.1.8.8.10" class="ltx_td ltx_align_left">3.12</td>
<td id="A3.T11.1.8.8.11" class="ltx_td ltx_align_left">8.01</td>
</tr>
<tr id="A3.T11.1.9.9" class="ltx_tr">
<td id="A3.T11.1.9.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A3.T11.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.9.9.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="A3.T11.1.9.9.2" class="ltx_td ltx_align_left">9.38</td>
<td id="A3.T11.1.9.9.3" class="ltx_td ltx_align_left">18.16</td>
<td id="A3.T11.1.9.9.4" class="ltx_td ltx_align_left">31.35</td>
<td id="A3.T11.1.9.9.5" class="ltx_td ltx_align_left">30.47</td>
<td id="A3.T11.1.9.9.6" class="ltx_td ltx_align_left">30.18</td>
<td id="A3.T11.1.9.9.7" class="ltx_td ltx_align_left"><span id="A3.T11.1.9.9.7.1" class="ltx_text ltx_font_bold">47.85</span></td>
<td id="A3.T11.1.9.9.8" class="ltx_td ltx_align_left">6.84</td>
<td id="A3.T11.1.9.9.9" class="ltx_td ltx_align_left">11.33</td>
<td id="A3.T11.1.9.9.10" class="ltx_td ltx_align_left">5.96</td>
<td id="A3.T11.1.9.9.11" class="ltx_td ltx_align_left">8.98</td>
</tr>
<tr id="A3.T11.1.10.10" class="ltx_tr">
<td id="A3.T11.1.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.10.10.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="A3.T11.1.10.10.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the left and a [character] on the right“</em></span>
</span>
</td>
<td id="A3.T11.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.10.10.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.10.10.3" class="ltx_td ltx_align_left ltx_border_t">14.11</td>
<td id="A3.T11.1.10.10.4" class="ltx_td ltx_align_left ltx_border_t">14.92</td>
<td id="A3.T11.1.10.10.5" class="ltx_td ltx_align_left ltx_border_t">38.10</td>
<td id="A3.T11.1.10.10.6" class="ltx_td ltx_align_left ltx_border_t">30.85</td>
<td id="A3.T11.1.10.10.7" class="ltx_td ltx_align_left ltx_border_t">40.83</td>
<td id="A3.T11.1.10.10.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.10.10.8.1" class="ltx_text ltx_font_bold">42.14</span></td>
<td id="A3.T11.1.10.10.9" class="ltx_td ltx_align_left ltx_border_t">13.71</td>
<td id="A3.T11.1.10.10.10" class="ltx_td ltx_align_left ltx_border_t">23.99</td>
<td id="A3.T11.1.10.10.11" class="ltx_td ltx_align_left ltx_border_t">14.01</td>
<td id="A3.T11.1.10.10.12" class="ltx_td ltx_align_left ltx_border_t">14.21</td>
</tr>
<tr id="A3.T11.1.11.11" class="ltx_tr">
<td id="A3.T11.1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.11.11.1.1.1" class="ltx_p" style="width:85.4pt;"><em id="A3.T11.1.11.11.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] on the bottom and a [character] on the top“</em></span>
</span>
</td>
<td id="A3.T11.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.11.11.2.1.1" class="ltx_p" style="width:19.9pt;">Default</span>
</span>
</td>
<td id="A3.T11.1.11.11.3" class="ltx_td ltx_align_left ltx_border_t">12.00</td>
<td id="A3.T11.1.11.11.4" class="ltx_td ltx_align_left ltx_border_t">15.42</td>
<td id="A3.T11.1.11.11.5" class="ltx_td ltx_align_left ltx_border_t">34.07</td>
<td id="A3.T11.1.11.11.6" class="ltx_td ltx_align_left ltx_border_t">35.48</td>
<td id="A3.T11.1.11.11.7" class="ltx_td ltx_align_left ltx_border_t">44.05</td>
<td id="A3.T11.1.11.11.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.11.11.8.1" class="ltx_text ltx_font_bold">45.26</span></td>
<td id="A3.T11.1.11.11.9" class="ltx_td ltx_align_left ltx_border_t">10.99</td>
<td id="A3.T11.1.11.11.10" class="ltx_td ltx_align_left ltx_border_t">26.51</td>
<td id="A3.T11.1.11.11.11" class="ltx_td ltx_align_left ltx_border_t">14.62</td>
<td id="A3.T11.1.11.11.12" class="ltx_td ltx_align_left ltx_border_t">8.17</td>
</tr>
<tr id="A3.T11.1.12.12" class="ltx_tr">
<td id="A3.T11.1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" rowspan="2">
<span id="A3.T11.1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.12.12.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A3.T11.1.12.12.1.1.1.1" class="ltx_text"><em id="A3.T11.1.12.12.1.1.1.1.1" class="ltx_emph ltx_font_italic">“A photo of a [character] textured with [texture1] and a [character] textured with [texture2]“</em></span></span>
</span>
</td>
<td id="A3.T11.1.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A3.T11.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.12.12.2.1.1" class="ltx_p" style="width:19.9pt;">Blue/Red</span>
</span>
</td>
<td id="A3.T11.1.12.12.3" class="ltx_td ltx_align_left ltx_border_t">4.44</td>
<td id="A3.T11.1.12.12.4" class="ltx_td ltx_align_left ltx_border_t">6.65</td>
<td id="A3.T11.1.12.12.5" class="ltx_td ltx_align_left ltx_border_t">19.15</td>
<td id="A3.T11.1.12.12.6" class="ltx_td ltx_align_left ltx_border_t">15.52</td>
<td id="A3.T11.1.12.12.7" class="ltx_td ltx_align_left ltx_border_t">20.56</td>
<td id="A3.T11.1.12.12.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T11.1.12.12.8.1" class="ltx_text ltx_font_bold">29.84</span></td>
<td id="A3.T11.1.12.12.9" class="ltx_td ltx_align_left ltx_border_t">6.15</td>
<td id="A3.T11.1.12.12.10" class="ltx_td ltx_align_left ltx_border_t">10.28</td>
<td id="A3.T11.1.12.12.11" class="ltx_td ltx_align_left ltx_border_t">5.44</td>
<td id="A3.T11.1.12.12.12" class="ltx_td ltx_align_left ltx_border_t">4.13</td>
</tr>
<tr id="A3.T11.1.13.13" class="ltx_tr">
<td id="A3.T11.1.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="A3.T11.1.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A3.T11.1.13.13.1.1.1" class="ltx_p" style="width:19.9pt;">Grass/Stone</span>
</span>
</td>
<td id="A3.T11.1.13.13.2" class="ltx_td ltx_align_left ltx_border_b">3.43</td>
<td id="A3.T11.1.13.13.3" class="ltx_td ltx_align_left ltx_border_b">5.44</td>
<td id="A3.T11.1.13.13.4" class="ltx_td ltx_align_left ltx_border_b">15.73</td>
<td id="A3.T11.1.13.13.5" class="ltx_td ltx_align_left ltx_border_b">17.24</td>
<td id="A3.T11.1.13.13.6" class="ltx_td ltx_align_left ltx_border_b">19.05</td>
<td id="A3.T11.1.13.13.7" class="ltx_td ltx_align_left ltx_border_b"><span id="A3.T11.1.13.13.7.1" class="ltx_text ltx_font_bold">27.32</span></td>
<td id="A3.T11.1.13.13.8" class="ltx_td ltx_align_left ltx_border_b">5.54</td>
<td id="A3.T11.1.13.13.9" class="ltx_td ltx_align_left ltx_border_b">7.76</td>
<td id="A3.T11.1.13.13.10" class="ltx_td ltx_align_left ltx_border_b">6.35</td>
<td id="A3.T11.1.13.13.11" class="ltx_td ltx_align_left ltx_border_b">4.03</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Setup and zero-shot evaluation of CLIP models on PUG: SPAR with <span id="A3.T11.3.1" class="ltx_text ltx_font_bold">caption retrieval in a single environment</span>. In contrast with the figure presented in the main paper, we present the result only using the salt flats environment. The motivation for this experiment is to showcase the failures mode of VLMs in a very simple setup in which the model robustness to background does not impact the prediction.</figcaption>
</figure>
<figure id="A3.F12" class="ltx_figure"><img src="/html/2308.03977/assets/x16.png" id="A3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="311" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Failures mode of a OpenCLIP ViT-G-14. Our PUG: SPAR dataset provides very simple images and captions and yet even large models are failing on them.</figcaption>
</figure>
</section>
<section id="A3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>CLIP fine-tuning details</h3>

<div id="A3.SS5.p1" class="ltx_para ltx_noindent">
<p id="A3.SS5.p1.1" class="ltx_p">We utilize the OpenCLIP framework <cite class="ltx_cite ltx_citemacro_citep">[Ilharco et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>]</cite> for all our CLIP experiments. The ViT-B/32 model is used as the image encoder for all our experiments. The CLIP model we fine-tune is the OpenAI 400M pre-trained model <span id="A3.SS5.p1.1.1" class="ltx_text ltx_font_italic">(‘ViT-B-32’, ‘openai’)<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note"><span id="footnote11.1.1.1" class="ltx_text ltx_font_upright">11</span></span><span id="footnote11.5" class="ltx_text ltx_font_upright">NOTE: We do not perform any training on the proprietary 400M dataset from OpenAI. We strictly only use the pre-trained models released, and fine-tune them on our PUG datasets.</span></span></span></span></span>. Fine-tuning on PUG: AR4T is done for 10 epochs on the 200K dataset while training is done for 2 epochs on the 1M dataset.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.03976" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.03977" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.03977">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.03977" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.03978" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 13:48:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
