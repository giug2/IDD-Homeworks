<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19205] TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains</title><meta property="og:description" content="In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets.
It is im…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19205">

<!--Generated on Sun May  5 20:28:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yoonsik Kim 
<br class="ltx_break">NAVER Cloud AI 
<br class="ltx_break">Seongnam-si, Gyeonggi-do, Korea 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">yoonsik.kim90@navercorp.com</span> 
<br class="ltx_break">&amp;Moonbin Yim 
<br class="ltx_break">NAVER Cloud AI 
<br class="ltx_break">Seongnam-si, Gyeonggi-do, Korea 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">moonbin.yim@navercorp.com</span> 
<br class="ltx_break">Ka Yeon Song
<br class="ltx_break">NAVER Cloud AI 
<br class="ltx_break">Seongnam-si, Gyeonggi-do, Korea 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">kayeon.song@navercorp.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">In this paper, we establish a benchmark for table visual question answering, referred to as the TableVQA-Bench, derived from pre-existing table question-answering (QA) and table structure recognition datasets.
It is important to note that existing datasets have not incorporated images or QA pairs, which are two crucial components of TableVQA.
As such, the primary objective of this paper is to obtain these necessary components.
Specifically, images are sourced either through the application of a <span id="id4.id1.1" class="ltx_text ltx_font_italic">stylesheet</span> or by employing the proposed table rendering system.
QA pairs are generated by exploiting the large language model (LLM) where the input is a text-formatted table.
Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs.
We comprehensively compare the performance of various multi-modal large language models (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs from our experiments.
Moreover, we discover that the number of vision queries plays a significant role in TableVQA performance. To further analyze the capabilities of MLLMs in comparison to their LLM backbones, we investigate by presenting image-formatted tables to MLLMs and text-formatted tables to LLMs, respectively.
Our findings suggest that processing visual inputs is more challenging than text inputs, as evidenced by the lower performance of MLLMs, despite generally requiring higher computational costs than LLMs.
The proposed TableVQA-Bench and evaluation codes are available at <a target="_blank" href="https://github.com/naver-ai/tablevqabench" title="" class="ltx_ref ltx_href">https://github.com/naver-ai/tablevqabench</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Tabular data is one of the most prevalent formats for representing structured text, playing a significant role in the efficient delivery of text-based information.
A large proportion of these tables can be found in image form, created from text sources, such as HTML and markdown formats.
Therefore, understanding visual tabular data can be deemed a crucial endeavor within the realm of the visual documentation domain.
In light of recent advancements in multi-modal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, it is now possible to harbor this capability within a single model. However, despite its significance, the evaluation of visual table data has been less vigorous due to the absence of evaluation datasets.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Meanwhile, in natural language processing (NLP), textual table question answering (TableQA) datasets have been widely proposed.
For instance, Panupong <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> provide WikiTableQuestion (WTQ) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> that is a question-answering task based on a text-based table. Chen <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">et al.</em> also release TabFact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> dataset determining whether a statement is entailed or refuted with a given table. Unfortunately, these datasets do not provide table images, making it challenging to apply them directly to table visual question answering.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.19205/assets/figures/samples.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="385" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Samples of the proposed TableVQA-Bench. TableVQA-Bench incorporates four domains of table datasets: VWTQ, VWTQ-Syn, VTabFact, and FinTabNetQA. The images of VWTQ-Syn and VTabFact are generated by our rendering system.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we construct a new TableVQA-Bench dataset as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> by leveraging existing TableQA and table structure recognition (TSR) datasets.
As for the TableQA dataset, real table images are sourced by attaching a <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">stylesheet</span> of original source (Wikipedia) into HTML that contains both the content and style of the table.
Acquired images can be contaminated, given that Wikipedia is often utilized as a primary source for constructing the web-crawled base for pre-training data, as suggested by Pix2Struct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To circumvent this issue, the proposed table rendering system is also utilized to obtain synthetic table images.
As for TSR dataset, QA pairs are required for constructing TableVQA.
To generate QA pairs, we propose to exploit GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> by feeding the text-formatted table as an input.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Through comparisons among MLLMs on TableVQA-Bench, we found that GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> outperforms other methods including commercial and open-sourced models across all table domains. We also observed that preserving the original information of visual features can be a crucial factor for TableVQA. For example, GPT-4V and CogVLM achieved enhanced performance when the resolution of the input image was higher.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.19205/assets/figures/input_format_visualization2.png" id="S1.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Examples of Table Formats.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.19205/assets/figures/performance_inputformat.png" id="S1.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Performance based on Input Formats.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">
We present visualized examples with various formats having the same content.
The evaluations of GPT-4 families <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> were conducted on VWTQ, which consists of 750 samples.
The accuracy of a vision-formatted table gets lower performance than the accuracy of a text-formatted table and severely depends on the aspect ratio of the input image.
Since HTML format effectively represents multi-row and multi-column configurations, it can achieve better performance than markdown format.
</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To provide a better analysis of the model’s capability, we conduct a comprehensive investigation of table formats and their performance. As illustrated in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, text-formatted tables, including HTML and markdown, tended to outperform their vision-formatted counterparts.
Furthermore, to enhance the analysis, a two-stage approach is explored, which initially involves extracting content from images for HTML representation and subsequently applying it to the TableQA task.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The significance of benchmarks for assessing the performance of MLLMs has grown as MLLMs advance rapidly. MMBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> evaluates perception and reasoning across approximately 3,000 questions in 20 different ability dimensions, including the ‘image-text understanding’ dimension. SEED-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is categorized into 12 evaluation dimensions with about 19,000 questions covering scenes, detection, OCR, and various other types. SEED-Bench-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> increases the number of questions to 24K to its predecessor, and the complexity of questions has been heightened to represent multi-modal content on both input and output sides. MathVista <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is a mathematically specialized evaluation set, consisting of 6,141 subjective and objective questions. This dataset encompasses questions related to seven types of mathematical reasoning and covers five primary tasks, incorporating a small portion in tabular format. Recently, chart question-answering benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have been introduced, examining specific domains of tasks. While these aforementioned datasets may partially encompass or relate to TableVQA, they do not primarily focus on TableVQA. Therefore, a dataset meticulously designed for the thorough investigation of TableVQA is indispensable and our TableVQA-Bench dutifully fulfills this requirement. Furthermore, we believe that the extensive investigation provided in this paper will be helpful in interpreting the table-related performance in previous datasets.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2404.19205/assets/figures/main.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="386" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Overview of constructing the proposed TableVQA-Bench. HTML* denotes that it incorporates both content and style of tables, while HTML only contains content.
</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>TableVQA-Bench</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">As illustrated in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Related Works ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we construct the TableVQA-Bench. TableVQA-Bench encompasses VWTQ, VTabFact, and FinTabNetQA, which are extended from pre-existing databases such as WTQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, TabFact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> correspondingly.
The components of TableVQA consist of three parts; table image, text-representation (HTML), and QA pairs <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="S3.p1.1.m1.1a"><mo stretchy="false" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\{</annotation></semantics></math><span id="S3.p1.2.2" class="ltx_text ltx_font_italic">IMG</span>, <span id="S3.p1.2.3" class="ltx_text ltx_font_italic">HTML</span>, <span id="S3.p1.2.1" class="ltx_text ltx_font_italic">QA<math id="S3.p1.2.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S3.p1.2.1.m1.1a"><mo stretchy="false" id="S3.p1.2.1.m1.1.1" xref="S3.p1.2.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.p1.2.1.m1.1b"><ci id="S3.p1.2.1.m1.1.1.cmml" xref="S3.p1.2.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.1.m1.1c">\}</annotation></semantics></math></span>.
To acquire images for VWTQ and VTabFact, we source images by attaching the <span id="S3.p1.2.4" class="ltx_text ltx_font_italic">stylesheet</span> of Wikipedia or by utilizing our table rendering system.
Conversely, FinTabNet is devoid of the QA pair, which is generated by employing the GPT-4.
In the final stage of these processes, any samples with more than 50 table rows are methodically filtered out and the authors carry out a meticulous review.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>VWTQ</h3>

<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.19205/assets/x1.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.sf1.4.2" class="ltx_text" style="font-size:90%;">Before attaching <span id="S3.F4.sf1.4.2.1" class="ltx_text ltx_font_italic">stylesheet</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.19205/assets/figures/after_style_resized.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.sf2.4.2" class="ltx_text" style="font-size:90%;">After attaching <span id="S3.F4.sf2.4.2.1" class="ltx_text ltx_font_italic">stylesheet</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">
The captured images whether the <span id="S3.F4.4.2.1" class="ltx_text ltx_font_italic">stylesheet</span> is attached or not.
</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">VWTQ is constructed by incorporating an image collection into the WTQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> while maintaining its QA pairs and accuracy-based evaluation metric.
As shown in Fig. <a href="#S3.F4.sf1" title="In Figure 4 ‣ 3.1 VWTQ ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, WTQ provides HTML that represents both the content and style of a table.
To reproduce the original table images from Wikipedia, we applied the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">stylesheet</span> of Wikipedia to the HTML.
Finally, we obtained the images by capturing screenshots, which are presented in Fig. <a href="#S3.F4.sf2" title="In Figure 4 ‣ 3.1 VWTQ ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>.
Since images from Wikipedia can be web-crawled to gather pre-training data for MLLMs, we also generate table images using our table rendering system.
It takes HTML as input and generates tables with various styles, featuring random attributes, as detailed in Section <a href="#S3.SS4" title="3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
The datasets generated from the attaching Wikipedia <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">stylesheet</span> and our rendering system have been named VWTQ and VWTQ-Synthesized (VWTQ-Syn), respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>VTabFact</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">TabFact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> represents a verification task that verifies whether a statement derived from a table is either entailed or refuted, thus categorizing it as a variant of the TableQA task.
In our empirical experiments, it was observed that prompts framed as “True or False” yielded higher efficacy compared to those framed as “entailed or refuted”. Consequently, we replace the answer format to “True” or “False” accordingly and we employ the evaluation metric as accuracy following the TabFact.
Given that TabFact has not provided the original HTML format of the tables, the acquisition of images is feasible only through the utilization of the proposed rendering system. It takes pseudo-HTML as an input, which is converted from the simple CSV file, and generates the images.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>FinTabNetQA</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">FinTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is a dataset for TSR task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that extracts an HTML format from a given table image. Unlike WTQ and TabFact, which use Wikipedia as their data source, FinTabNet’s sources are the annual reports of S&amp;P 500 companies, allowing it to evaluate tables from new domains. For the construction of FinTabNetQA, a generation process of QA pairs is required, and we utilized GPT-4 with HTML as an input.
During the generation process, two issues were encountered and resolved in the following manners:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The first question is often answered in the first non-header cell.
This issue persisted even with the use of additional instructions, thus we opted to generate numerous QA pairs from a single table and conducted random sampling from QA pairs.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">We observed inconsistent inclusion of scale units, such as thousand, million, and billion at the answer. Particularly when the scale unit is in thousands, most generated answers often do not include the scale unit.
We rectify this issue with a meticulous human revision procedure.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In terms of the evaluation metric, we employ accuracy. It should be noted that the majority of financial tables encompass scale units, for instance, thousand, million, billion, trillion, and percentage.
For the FinTabNetQA, the accuracy measure referred to as <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">relieved-accuracy</span> is employed, whereby these units are intentionally excluded during evaluation.
To provide an illustrative example, when the ground truth is “128 million”, predictions such as “128 million”, “128,000,000” and “128” are all approved as accurate responses.
This methodology is justified due to the fact that MLLMs presently fail to attain substantial performance in a strict accuracy evaluation.
Both the <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">strict-accuracy</span> and the <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">relieved-accuracy</span> scripts will be made available for further research.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2404.19205/assets/figures/table_render.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">The generated image according to the change of the attributes. To represent the table’s margin, we denote the dashed-box as the captured table image with a white margin.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Table Rendering System</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Our rendering framework employs a rule-based methodology for rendering table images, engaging diverse styles applied to HTML sources.
This framework bifurcates into two principal phases: style generation and image generation.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In the first stage, style tags are added to the original HTML to generate a styled HTML where the most of original HTML only incorporates the structure of the table. Leveraging the Bootstrap framework<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://getbootstrap.com</span></span></span>, the system facilitates a diverse representation of table styles encompassing elements such as cells, borders, and texts. The specific style attributes include:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Table</span>: background-color and margin</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Cell</span>: background-color and padding</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Border</span>: border-width, border-style, and border-color</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Text</span>: font-family, font-size, text-align, and color</p>
</div>
</li>
</ul>
<p id="S3.SS4.p2.2" class="ltx_p">where these components are randomly determined.
Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.3 FinTabNetQA ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the example when each attribute is changed from the default setting.
The second phase, image generation, involves rendering the styled HTML within a web browser to capture a screenshot. Utilizing the Puppeteer library<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://pptr.dev</span></span></span>, we obtain rendered images by randomly selecting parameters such as image dimensions and JPEG quality.
To generate diverse table images, most attributes are randomly determined. However, certain attribute combinations may yield images that appear unnatural. To mitigate this, a human review process is conducted to filter out such anomalous images.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Statistics of TableVQA-Bench.</span></figcaption>
<div id="S3.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:105.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.0pt,1.0pt) scale(0.98058567913226,0.98058567913226) ;">
<table id="S3.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.1.1" class="ltx_tr">
<td id="S3.T1.4.1.1.1.1" class="ltx_td ltx_border_tt" style="padding:1.5pt 4.0pt;"></td>
<th id="S3.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Real Image</th>
<th id="S3.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Human Generated QA</th>
<th id="S3.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">#Image</th>
<th id="S3.T1.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">#QA</th>
</tr>
<tr id="S3.T1.4.1.2.2" class="ltx_tr">
<td id="S3.T1.4.1.2.2.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:1.5pt 4.0pt;">VWTQ</td>
<td id="S3.T1.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">✓</td>
<td id="S3.T1.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">✓</td>
<td id="S3.T1.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">315</td>
<td id="S3.T1.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">750</td>
</tr>
<tr id="S3.T1.4.1.3.3" class="ltx_tr">
<td id="S3.T1.4.1.3.3.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">VWTQ-Syn</td>
<td id="S3.T1.4.1.3.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✗</td>
<td id="S3.T1.4.1.3.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✓</td>
<td id="S3.T1.4.1.3.3.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">150</td>
<td id="S3.T1.4.1.3.3.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">250</td>
</tr>
<tr id="S3.T1.4.1.4.4" class="ltx_tr">
<td id="S3.T1.4.1.4.4.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">VTabFact</td>
<td id="S3.T1.4.1.4.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✗</td>
<td id="S3.T1.4.1.4.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✓</td>
<td id="S3.T1.4.1.4.4.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">224</td>
<td id="S3.T1.4.1.4.4.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">250</td>
</tr>
<tr id="S3.T1.4.1.5.5" class="ltx_tr">
<td id="S3.T1.4.1.5.5.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">FinTabNetQA</td>
<td id="S3.T1.4.1.5.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✓</td>
<td id="S3.T1.4.1.5.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">✗</td>
<td id="S3.T1.4.1.5.5.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">205</td>
<td id="S3.T1.4.1.5.5.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">250</td>
</tr>
<tr id="S3.T1.4.1.6.6" class="ltx_tr">
<td id="S3.T1.4.1.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding:1.5pt 4.0pt;">Total</td>
<td id="S3.T1.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:1.5pt 4.0pt;">-</td>
<td id="S3.T1.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:1.5pt 4.0pt;">-</td>
<td id="S3.T1.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:1.5pt 4.0pt;">894</td>
<td id="S3.T1.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:1.5pt 4.0pt;">1,500</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_question_wtq_real.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_question_wtq_render.png" id="S3.F6.sf1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_question_tabfact_render.png" id="S3.F6.sf1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_question_fintabnet_real.png" id="S3.F6.sf1.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Question Length</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_answer_wtq_real.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_answer_wtq_render.png" id="S3.F6.sf2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_answer_tabfact_render.png" id="S3.F6.sf2.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_answer_fintabnet_real.png" id="S3.F6.sf2.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Answer Length</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_aspect_wtq_real.png" id="S3.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_aspect_wtq_render.png" id="S3.F6.sf3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_aspect_tabfact_render.png" id="S3.F6.sf3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_aspect_fintabnet_real.png" id="S3.F6.sf3.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">Aspect Ratio</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numrow_wtq_real.png" id="S3.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numrow_wtq_render.png" id="S3.F6.sf4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numrow_tabfact_render.png" id="S3.F6.sf4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numrow_fintabnet_real.png" id="S3.F6.sf4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">Number of Rows</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numcell_wtq_real.png" id="S3.F6.sf5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numcell_wtq_render.png" id="S3.F6.sf5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numcell_tabfact_render.png" id="S3.F6.sf5.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numcell_fintabnet_real.png" id="S3.F6.sf5.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S3.F6.sf5.3.2" class="ltx_text" style="font-size:90%;">Number of Cells</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtoken_wtq_real.png" id="S3.F6.sf6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtoken_wtq_render.png" id="S3.F6.sf6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtoken_tabfact_render.png" id="S3.F6.sf6.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtoken_fintabnet_real.png" id="S3.F6.sf6.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S3.F6.sf6.3.2" class="ltx_text" style="font-size:90%;">Number of Text Tokens</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf7" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtokensp_wtq_real.png" id="S3.F6.sf7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtokensp_wtq_render.png" id="S3.F6.sf7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtokensp_tabfact_render.png" id="S3.F6.sf7.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2404.19205/assets/figures/eda/eda_numtokensp_fintabnet_real.png" id="S3.F6.sf7.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="86" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S3.F6.sf7.3.2" class="ltx_text" style="font-size:90%;">Number of Text Tokens with Special Tokens</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">The distribution is analyzed with respect to each feature. For the quantification of text tokens, we utilize the Viucuna-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> tokenizer.</span></figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Data Statistics</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides data statistics, comprising a total of 894 images and 1500 QA pairs for evaluation. VWTQ includes 750 QA pairs gathered from purely authentic data. An equal quantity of QA pairs is amassed from partial real data, originating from VWTQ-syn, VTabFact-syn, and FintabNetQA. The QA pairs of VWTQ-syn are sampled from VWTQ.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">The distribution of each dataset is examined and visualized for analytical purposes in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The observed statistics in Fig. <a href="#S3.F6.sf1" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a> reveal that the length of the questions originating from FintabNetQA is generally longer than the other datasets. This trend is possibly due to its machine-generated characteristics, where GPT-4 tends to construct more elaborate question structures. As shown in Fig. <a href="#S3.F6.sf2" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, the answer length distribution for VTabFact seems to branch out into two distinctive categories, with “true” or “false” being its definitive responses. Frequent instances of elongated answers in FintabNetQA primarily occur due to the common inclusion of units.
As shown in Fig. <a href="#S3.F6.sf3" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, <a href="#S3.F6.sf4" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a>, and <a href="#S3.F6.sf5" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(e)</span></a>, a prominent correlation between the number of rows and the aspect ratio can be established. VWTQ is distinctively characterized by the presence of numerous tables with lengthy rows. While comparing the number of rows, FintabNetQA often exhibits a larger aspect ratio. This might be attributed to two possible explanations: 1) the cell height is relatively larger, and 2) the cell content is abundant, leading to an increase in the number of line breaks.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.3" class="ltx_p">As illustrated in Fig. <a href="#S3.F6.sf6" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(f)</span></a> and <a href="#S3.F6.sf7" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(g)</span></a>, our analysis extends to examining the token length with the Vicuna-7B tokenizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> when tables are encoded in HTML format. We found that the tokenizer does not incorporate HTML tags such as <math id="S3.SS5.p3.1.m1.1" class="ltx_Math" alttext="&lt;td&gt;" display="inline"><semantics id="S3.SS5.p3.1.m1.1a"><mrow id="S3.SS5.p3.1.m1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS5.p3.1.m1.1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS5.p3.1.m1.1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.1.m1.1.1.1.1.1" xref="S3.SS5.p3.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS5.p3.1.m1.1.1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.1.1.3.cmml">d</mi></mrow><mo fence="true" lspace="0em" id="S3.SS5.p3.1.m1.1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS5.p3.1.m1.1.1.2.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1.2">expectation</csymbol><apply id="S3.SS5.p3.1.m1.1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1.1"><times id="S3.SS5.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1.1.1.1"></times><ci id="S3.SS5.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.1.1.2">𝑡</ci><ci id="S3.SS5.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.1.1.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">&lt;td&gt;</annotation></semantics></math>, <math id="S3.SS5.p3.2.m2.1" class="ltx_Math" alttext="&lt;tr&gt;" display="inline"><semantics id="S3.SS5.p3.2.m2.1a"><mrow id="S3.SS5.p3.2.m2.1.1.1" xref="S3.SS5.p3.2.m2.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS5.p3.2.m2.1.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS5.p3.2.m2.1.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.1.1.2" xref="S3.SS5.p3.2.m2.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.2.m2.1.1.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS5.p3.2.m2.1.1.1.1.3" xref="S3.SS5.p3.2.m2.1.1.1.1.3.cmml">r</mi></mrow><mo fence="true" lspace="0em" id="S3.SS5.p3.2.m2.1.1.1.3" xref="S3.SS5.p3.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS5.p3.2.m2.1.1.2.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1.2">expectation</csymbol><apply id="S3.SS5.p3.2.m2.1.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1.1"><times id="S3.SS5.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1.1.1"></times><ci id="S3.SS5.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.1.1.2">𝑡</ci><ci id="S3.SS5.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS5.p3.2.m2.1.1.1.1.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">&lt;tr&gt;</annotation></semantics></math>, and <math id="S3.SS5.p3.3.m3.1" class="ltx_Math" alttext="&lt;th&gt;" display="inline"><semantics id="S3.SS5.p3.3.m3.1a"><mrow id="S3.SS5.p3.3.m3.1.1.1" xref="S3.SS5.p3.3.m3.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS5.p3.3.m3.1.1.1.2" xref="S3.SS5.p3.3.m3.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS5.p3.3.m3.1.1.1.1" xref="S3.SS5.p3.3.m3.1.1.1.1.cmml"><mi id="S3.SS5.p3.3.m3.1.1.1.1.2" xref="S3.SS5.p3.3.m3.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p3.3.m3.1.1.1.1.1" xref="S3.SS5.p3.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS5.p3.3.m3.1.1.1.1.3" xref="S3.SS5.p3.3.m3.1.1.1.1.3.cmml">h</mi></mrow><mo fence="true" lspace="0em" id="S3.SS5.p3.3.m3.1.1.1.3" xref="S3.SS5.p3.3.m3.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.1b"><apply id="S3.SS5.p3.3.m3.1.1.2.cmml" xref="S3.SS5.p3.3.m3.1.1.1"><csymbol cd="latexml" id="S3.SS5.p3.3.m3.1.1.2.1.cmml" xref="S3.SS5.p3.3.m3.1.1.1.2">expectation</csymbol><apply id="S3.SS5.p3.3.m3.1.1.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1.1.1"><times id="S3.SS5.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1.1.1.1"></times><ci id="S3.SS5.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS5.p3.3.m3.1.1.1.1.2">𝑡</ci><ci id="S3.SS5.p3.3.m3.1.1.1.1.3.cmml" xref="S3.SS5.p3.3.m3.1.1.1.1.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.1c">&lt;th&gt;</annotation></semantics></math> as individual tokens. Although incorporating these tags as special tokens slightly increases the vocabulary size, it significantly reduces the number of required input tokens.
Typically, open-sourced MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> integrate vision queries and text queries by concatenating them before feeding them into the LLMs.
Consequently, comparing the length of text tokens with the length of vision tokens becomes feasible when tables are represented in both image and text formats.
As shown in Table <a href="#S4.T2" title="Table 2 ‣ Evaluation Protocol. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the length of vision tokens varies widely, ranging from 32 to 1445. It is observed that the efficiency of image-formatted tables significantly decreases compared to those text-formatted with special tokens when the length of a vision query exceeds 1,000 tokens.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Evaluation Protocol.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">In the inference phase, minor prompt tuning was conducted for each model in order to acquire a suitable answer format for subsequent evaluation. In instances where answer parsing was required, rule-based methods are deployed.
The chosen metric for evaluation is accuracy, the specifics of which are explained in Section 3.
When the rule-based parsing fails to acquire a properly formatted answer, we also evaluate its performance using a modified accuracy metric.
This metric specifically assesses whether the answer is contained within the response.
These aforementioned processes will be incorporated into the upcoming project page.
</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.2.1" class="ltx_text" style="font-size:90%;">The architecture of open-sourced MLLMs. <math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T2.2.2.1.m1.1b"><mi id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1c"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1d">\alpha</annotation></semantics></math> denotes an additional number of vision tokens that feed to the cross-attention layer of LLM.</span></figcaption>
<div id="S4.T2.3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:161.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.1pt,18.4pt) scale(0.813998327484151,0.813998327484151) ;">
<table id="S4.T2.3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.1.2.1" class="ltx_tr">
<th id="S4.T2.3.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Models</th>
<th id="S4.T2.3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">Size</th>
<th id="S4.T2.3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">LLM Branch</th>
<th id="S4.T2.3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">Size</th>
<th id="S4.T2.3.3.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Vision Branch</th>
<th id="S4.T2.3.3.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Size</th>
<th id="S4.T2.3.3.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">#Vision-Queries</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.1.3.1" class="ltx_tr">
<td id="S4.T2.3.3.1.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:1.5pt 4.0pt;">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T2.3.3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">12.1B</td>
<td id="S4.T2.3.3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">FlanT5-XXL</td>
<td id="S4.T2.3.3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">11B</td>
<td id="S4.T2.3.3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">EVA-CLIP-g/14</td>
<td id="S4.T2.3.3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">1B</td>
<td id="S4.T2.3.3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;">32</td>
</tr>
<tr id="S4.T2.3.3.1.4.2" class="ltx_tr">
<td id="S4.T2.3.3.1.4.2.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S4.T2.3.3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">8.2B</td>
<td id="S4.T2.3.3.1.4.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T2.3.3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">7B</td>
<td id="S4.T2.3.3.1.4.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">EVA-CLIP-g/14</td>
<td id="S4.T2.3.3.1.4.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">1B</td>
<td id="S4.T2.3.3.1.4.2.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">32</td>
</tr>
<tr id="S4.T2.3.3.1.5.3" class="ltx_tr">
<td id="S4.T2.3.3.1.5.3.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T2.3.3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">17B</td>
<td id="S4.T2.3.3.1.5.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T2.3.3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">7B</td>
<td id="S4.T2.3.3.1.5.3.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">EVA-02-CLIP-E/14</td>
<td id="S4.T2.3.3.1.5.3.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">4.4B</td>
<td id="S4.T2.3.3.1.5.3.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">256</td>
</tr>
<tr id="S4.T2.3.3.1.6.4" class="ltx_tr">
<td id="S4.T2.3.3.1.6.4.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">CogVLM-1k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T2.3.3.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">17B</td>
<td id="S4.T2.3.3.1.6.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T2.3.3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">7B</td>
<td id="S4.T2.3.3.1.6.4.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">EVA-02-CLIP-E/14</td>
<td id="S4.T2.3.3.1.6.4.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">4.4B</td>
<td id="S4.T2.3.3.1.6.4.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">1225</td>
</tr>
<tr id="S4.T2.3.3.1.1" class="ltx_tr">
<td id="S4.T2.3.3.1.1.2" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">CogVLM-Agent-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">17B</td>
<td id="S4.T2.3.3.1.1.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Vicuna-7B</td>
<td id="S4.T2.3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">7B</td>
<td id="S4.T2.3.3.1.1.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Mixed</td>
<td id="S4.T2.3.3.1.1.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">4.4B</td>
<td id="S4.T2.3.3.1.1.1" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">256+<math id="S4.T2.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T2.3.3.1.1.1.m1.1a"><mi id="S4.T2.3.3.1.1.1.m1.1.1" xref="S4.T2.3.3.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.1.1.m1.1b"><ci id="S4.T2.3.3.1.1.1.m1.1.1.cmml" xref="S4.T2.3.3.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.1.1.m1.1c">\alpha</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.3.3.1.7.5" class="ltx_tr">
<td id="S4.T2.3.3.1.7.5.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S4.T2.3.3.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">8.2B</td>
<td id="S4.T2.3.3.1.7.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">LLaMA-7B</td>
<td id="S4.T2.3.3.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">7B</td>
<td id="S4.T2.3.3.1.7.5.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">CLIP ViT-L/14</td>
<td id="S4.T2.3.3.1.7.5.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">0.3B</td>
<td id="S4.T2.3.3.1.7.5.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">64</td>
</tr>
<tr id="S4.T2.3.3.1.8.6" class="ltx_tr">
<td id="S4.T2.3.3.1.8.6.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">SPHINX-v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S4.T2.3.3.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">15.7B</td>
<td id="S4.T2.3.3.1.8.6.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">LLaMA-13B</td>
<td id="S4.T2.3.3.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">13B</td>
<td id="S4.T2.3.3.1.8.6.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Mixed</td>
<td id="S4.T2.3.3.1.8.6.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">2.7B</td>
<td id="S4.T2.3.3.1.8.6.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">289</td>
</tr>
<tr id="S4.T2.3.3.1.9.7" class="ltx_tr">
<td id="S4.T2.3.3.1.9.7.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">SPHINX-v1-1k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S4.T2.3.3.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">15.7B</td>
<td id="S4.T2.3.3.1.9.7.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">LLaMA-13B</td>
<td id="S4.T2.3.3.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">13B</td>
<td id="S4.T2.3.3.1.9.7.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Mixed</td>
<td id="S4.T2.3.3.1.9.7.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">2.7B</td>
<td id="S4.T2.3.3.1.9.7.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">1445</td>
</tr>
<tr id="S4.T2.3.3.1.10.8" class="ltx_tr">
<td id="S4.T2.3.3.1.10.8.1" class="ltx_td ltx_align_left" style="padding:1.5pt 4.0pt;">LLaVA-v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.3.3.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">13.4B</td>
<td id="S4.T2.3.3.1.10.8.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">Vicuna-13B</td>
<td id="S4.T2.3.3.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">13B</td>
<td id="S4.T2.3.3.1.10.8.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">CLIP ViT-L/14</td>
<td id="S4.T2.3.3.1.10.8.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">304M</td>
<td id="S4.T2.3.3.1.10.8.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">576</td>
</tr>
<tr id="S4.T2.3.3.1.11.9" class="ltx_tr">
<td id="S4.T2.3.3.1.11.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1.5pt 4.0pt;">Qwen-VL(-Chat) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S4.T2.3.3.1.11.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;">9.6B</td>
<td id="S4.T2.3.3.1.11.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">Qwen-7B</td>
<td id="S4.T2.3.3.1.11.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;">7.7B</td>
<td id="S4.T2.3.3.1.11.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">OpenCLIP ViT-G/14</td>
<td id="S4.T2.3.3.1.11.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">1.9B</td>
<td id="S4.T2.3.3.1.11.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">256</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Compared Models.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.2" class="ltx_p">Comparative analysis is conducted on MLLMs, including commercial models such as Gemini-ProV<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>gemini-pro-vision and gemini-pro are employed for MLLM and LLM, respectively.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and GPT-4V<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>gpt-4-vision-preview and gpt-4-1106-preview are employed for MLLMs and LLM, respectively. For gpt-4-vision-preview, we adopt ‘auto’ as a detail option</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and several open-source models as outlined in Table <a href="#S4.T2" title="Table 2 ‣ Evaluation Protocol. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Since SPHINX-MoE and SPHINX-v2 have not been published, we exploited huggingface models<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/Alpha-VLLM/LLaMA2-Accessory/tree/main/finetune/mm/SPHINX</span></span></span>.
To examine the capabilities of their underlying LLMs on TableQA, Vicuna-7B-v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Vicuna-13B-v1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Gemini-Pro <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, GPT-3.5, and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are evaluated by feeding them HTML-encoded tables as input.
We also employ two-stage inference methods. We extract the HTML of tables using MLLMs and then conduct the QA task with LLMs where these methods are denoted as GPT-4V <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">\rightarrow</annotation></semantics></math> GPT-4 and Gemini-ProV <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mo stretchy="false" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">\rightarrow</annotation></semantics></math> Gemini-Pro. We expect this to reveal the correlation between textual and visual modalities.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We present the comprehensive comparisons of multi-modal inputs in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The average score is achieved from the sample average.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">Accuracy scores on TableVQA-Bench. Scores of both text and vision modalities are reported.
The notation ‘-1k’ indicates that the number of vision queries is approximately 1k.
CogAgent-VQA* denotes the scores evaluated by the modified accuracy metric.
The highest scores in each section are represented in <span id="S4.T3.6.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
<div id="S4.T3.2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:430.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.8pt,37.4pt) scale(0.851491503128928,0.851491503128928) ;">
<table id="S4.T3.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.2.3.1" class="ltx_tr">
<th id="S4.T3.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Input Modality</span></th>
<th id="S4.T3.2.2.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S4.T3.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">VWTQ</span></td>
<td id="S4.T3.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.4.1" class="ltx_text ltx_font_bold">VWTQ-Syn</span></td>
<td id="S4.T3.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.5.1" class="ltx_text ltx_font_bold">VTabFact</span></td>
<td id="S4.T3.2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.6.1" class="ltx_text ltx_font_bold">FinTabNetQA</span></td>
<td id="S4.T3.2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.3.1.7.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="S4.T3.2.2.2.4.2" class="ltx_tr" style="background-color:#D0F8D0;">
<th id="S4.T3.2.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 4.0pt;" colspan="7"><span id="S4.T3.2.2.2.4.2.1.1" class="ltx_text ltx_font_italic" style="background-color:#D0F8D0;">Multi-modal Large Language Models (MLLMs)</span></th>
</tr>
<tr id="S4.T3.2.2.2.5.3" class="ltx_tr">
<th id="S4.T3.2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;" rowspan="16"><span id="S4.T3.2.2.2.5.3.1.1" class="ltx_text">Vision</span></th>
<th id="S4.T3.2.2.2.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T3.2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.5.3.3.1" class="ltx_text ltx_font_bold">42.5</span></td>
<td id="S4.T3.2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.5.3.4.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S4.T3.2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.5.3.5.1" class="ltx_text ltx_font_bold">68.0</span></td>
<td id="S4.T3.2.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.5.3.6.1" class="ltx_text ltx_font_bold">79.6</span></td>
<td id="S4.T3.2.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.5.3.7.1" class="ltx_text ltx_font_bold">54.5</span></td>
</tr>
<tr id="S4.T3.2.2.2.6.4" class="ltx_tr">
<th id="S4.T3.2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">Gemini-ProV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<td id="S4.T3.2.2.2.6.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">26.7</td>
<td id="S4.T3.2.2.2.6.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">33.2</td>
<td id="S4.T3.2.2.2.6.4.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">55.6</td>
<td id="S4.T3.2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">60.8</td>
<td id="S4.T3.2.2.2.6.4.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">38.3</td>
</tr>
<tr id="S4.T3.2.2.2.7.5" class="ltx_tr">
<th id="S4.T3.2.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">SPHINX-MoE-1k</th>
<td id="S4.T3.2.2.2.7.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">27.2</td>
<td id="S4.T3.2.2.2.7.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">33.6</td>
<td id="S4.T3.2.2.2.7.5.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">61.6</td>
<td id="S4.T3.2.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">36.0</td>
<td id="S4.T3.2.2.2.7.5.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">35.5</td>
</tr>
<tr id="S4.T3.2.2.2.8.6" class="ltx_tr">
<th id="S4.T3.2.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">SPHINX-v2-1k</th>
<td id="S4.T3.2.2.2.8.6.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">25.3</td>
<td id="S4.T3.2.2.2.8.6.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">28.0</td>
<td id="S4.T3.2.2.2.8.6.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">66.8</td>
<td id="S4.T3.2.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">31.2</td>
<td id="S4.T3.2.2.2.8.6.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">33.7</td>
</tr>
<tr id="S4.T3.2.2.2.9.7" class="ltx_tr">
<th id="S4.T3.2.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">QWEN-VL-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T3.2.2.2.9.7.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">19.0</td>
<td id="S4.T3.2.2.2.9.7.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">23.2</td>
<td id="S4.T3.2.2.2.9.7.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">60.4</td>
<td id="S4.T3.2.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">29.6</td>
<td id="S4.T3.2.2.2.9.7.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">28.4</td>
</tr>
<tr id="S4.T3.2.2.2.10.8" class="ltx_tr">
<th id="S4.T3.2.2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">QWEN-VL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T3.2.2.2.10.8.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">17.2</td>
<td id="S4.T3.2.2.2.10.8.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">21.2</td>
<td id="S4.T3.2.2.2.10.8.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">52.0</td>
<td id="S4.T3.2.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">34.0</td>
<td id="S4.T3.2.2.2.10.8.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">26.5</td>
</tr>
<tr id="S4.T3.2.2.2.11.9" class="ltx_tr">
<th id="S4.T3.2.2.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">SPHINX-MoE</th>
<td id="S4.T3.2.2.2.11.9.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">15.3</td>
<td id="S4.T3.2.2.2.11.9.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">16.8</td>
<td id="S4.T3.2.2.2.11.9.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">58.8</td>
<td id="S4.T3.2.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">2.8</td>
<td id="S4.T3.2.2.2.11.9.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">20.7</td>
</tr>
<tr id="S4.T3.2.2.2.12.10" class="ltx_tr">
<th id="S4.T3.2.2.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">SPHINX-v1-1k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="S4.T3.2.2.2.12.10.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">13.2</td>
<td id="S4.T3.2.2.2.12.10.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">17.2</td>
<td id="S4.T3.2.2.2.12.10.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">58.0</td>
<td id="S4.T3.2.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">3.2</td>
<td id="S4.T3.2.2.2.12.10.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">19.7</td>
</tr>
<tr id="S4.T3.2.2.2.13.11" class="ltx_tr">
<th id="S4.T3.2.2.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T3.2.2.2.13.11.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">10.7</td>
<td id="S4.T3.2.2.2.13.11.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">14.4</td>
<td id="S4.T3.2.2.2.13.11.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">56.8</td>
<td id="S4.T3.2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">2.8</td>
<td id="S4.T3.2.2.2.13.11.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">17.7</td>
</tr>
<tr id="S4.T3.2.2.2.14.12" class="ltx_tr">
<th id="S4.T3.2.2.2.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T3.2.2.2.14.12.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">12.4</td>
<td id="S4.T3.2.2.2.14.12.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">12.4</td>
<td id="S4.T3.2.2.2.14.12.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">55.6</td>
<td id="S4.T3.2.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">0.8</td>
<td id="S4.T3.2.2.2.14.12.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">17.7</td>
</tr>
<tr id="S4.T3.2.2.2.15.13" class="ltx_tr">
<th id="S4.T3.2.2.2.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">CogVLM-1k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<td id="S4.T3.2.2.2.15.13.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">9.7</td>
<td id="S4.T3.2.2.2.15.13.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">11.6</td>
<td id="S4.T3.2.2.2.15.13.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">52.0</td>
<td id="S4.T3.2.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">4.8</td>
<td id="S4.T3.2.2.2.15.13.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">16.3</td>
</tr>
<tr id="S4.T3.2.2.2.16.14" class="ltx_tr">
<th id="S4.T3.2.2.2.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">SPHINX-v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="S4.T3.2.2.2.16.14.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">7.1</td>
<td id="S4.T3.2.2.2.16.14.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">9.6</td>
<td id="S4.T3.2.2.2.16.14.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">55.2</td>
<td id="S4.T3.2.2.2.16.14.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">1.2</td>
<td id="S4.T3.2.2.2.16.14.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">14.5</td>
</tr>
<tr id="S4.T3.2.2.2.17.15" class="ltx_tr">
<th id="S4.T3.2.2.2.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">CogAgent-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T3.2.2.2.17.15.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">0.3</td>
<td id="S4.T3.2.2.2.17.15.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">0.8</td>
<td id="S4.T3.2.2.2.17.15.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">58.4</td>
<td id="S4.T3.2.2.2.17.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">22.8</td>
<td id="S4.T3.2.2.2.17.15.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">13.8</td>
</tr>
<tr id="S4.T3.2.2.2.18.16" class="ltx_tr">
<th id="S4.T3.2.2.2.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">InstructBLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T3.2.2.2.18.16.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">5.9</td>
<td id="S4.T3.2.2.2.18.16.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">6.4</td>
<td id="S4.T3.2.2.2.18.16.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">50.4</td>
<td id="S4.T3.2.2.2.18.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">0.4</td>
<td id="S4.T3.2.2.2.18.16.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">12.5</td>
</tr>
<tr id="S4.T3.2.2.2.19.17" class="ltx_tr">
<th id="S4.T3.2.2.2.19.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<td id="S4.T3.2.2.2.19.17.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">5.2</td>
<td id="S4.T3.2.2.2.19.17.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">5.6</td>
<td id="S4.T3.2.2.2.19.17.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">51.6</td>
<td id="S4.T3.2.2.2.19.17.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">0.4</td>
<td id="S4.T3.2.2.2.19.17.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">12.2</td>
</tr>
<tr id="S4.T3.2.2.2.20.18" class="ltx_tr">
<th id="S4.T3.2.2.2.20.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">CogVLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<td id="S4.T3.2.2.2.20.18.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">0.8</td>
<td id="S4.T3.2.2.2.20.18.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">0.8</td>
<td id="S4.T3.2.2.2.20.18.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">40.8</td>
<td id="S4.T3.2.2.2.20.18.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">1.2</td>
<td id="S4.T3.2.2.2.20.18.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">7.5</td>
</tr>
<tr id="S4.T3.2.2.2.21.19" class="ltx_tr">
<th id="S4.T3.2.2.2.21.19.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;"></th>
<th id="S4.T3.2.2.2.21.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">CogAgent-VQA* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S4.T3.2.2.2.21.19.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">37.2</td>
<td id="S4.T3.2.2.2.21.19.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">41.2</td>
<td id="S4.T3.2.2.2.21.19.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">58.4</td>
<td id="S4.T3.2.2.2.21.19.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">22.8</td>
<td id="S4.T3.2.2.2.21.19.7" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">39.0</td>
</tr>
<tr id="S4.T3.2.2.2.22.20" class="ltx_tr" style="background-color:#D0F8D0;">
<th id="S4.T3.2.2.2.22.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 4.0pt;" colspan="7"><span id="S4.T3.2.2.2.22.20.1.1" class="ltx_text ltx_font_italic" style="background-color:#D0F8D0;">Table Structure Reconstruction<span id="S4.T3.2.2.2.22.20.1.1.1" class="ltx_text ltx_font_upright" style="background-color:#D0F8D0;"> + </span>Large Language Models (LLMs)</span></th>
</tr>
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;" rowspan="2"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text">Vision</span></th>
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> <math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">45.2</span></td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">55.6</span></td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">78.0</span></td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold">95.2</span></td>
<td id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.1.1.1.1.7.1" class="ltx_text ltx_font_bold">60.7</span></td>
</tr>
<tr id="S4.T3.2.2.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">Gemini-ProV <math id="S4.T3.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.1.m1.1c">\rightarrow</annotation></semantics></math> Gemini-Pro <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<td id="S4.T3.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">34.8</td>
<td id="S4.T3.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">40.4</td>
<td id="S4.T3.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">71.0</td>
<td id="S4.T3.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">75.6</td>
<td id="S4.T3.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">48.6</td>
</tr>
<tr id="S4.T3.2.2.2.23.21" class="ltx_tr" style="background-color:#D0F8D0;">
<th id="S4.T3.2.2.2.23.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 4.0pt;" colspan="7"><span id="S4.T3.2.2.2.23.21.1.1" class="ltx_text ltx_font_italic" style="background-color:#D0F8D0;">Large Language Models (LLMs)</span></th>
</tr>
<tr id="S4.T3.2.2.2.24.22" class="ltx_tr">
<th id="S4.T3.2.2.2.24.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;" rowspan="5"><span id="S4.T3.2.2.2.24.22.1.1" class="ltx_text">Text</span></th>
<th id="S4.T3.2.2.2.24.22.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;">GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T3.2.2.2.24.22.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.24.22.3.1" class="ltx_text ltx_font_bold">68.1</span></td>
<td id="S4.T3.2.2.2.24.22.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.24.22.4.1" class="ltx_text ltx_font_bold">69.6</span></td>
<td id="S4.T3.2.2.2.24.22.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.24.22.5.1" class="ltx_text ltx_font_bold">80.0</span></td>
<td id="S4.T3.2.2.2.24.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.24.22.6.1" class="ltx_text ltx_font_bold">98.8</span></td>
<td id="S4.T3.2.2.2.24.22.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.0pt;"><span id="S4.T3.2.2.2.24.22.7.1" class="ltx_text ltx_font_bold">75.5</span></td>
</tr>
<tr id="S4.T3.2.2.2.25.23" class="ltx_tr">
<th id="S4.T3.2.2.2.25.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">Gemini-Pro <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<td id="S4.T3.2.2.2.25.23.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">56.4</td>
<td id="S4.T3.2.2.2.25.23.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">61.2</td>
<td id="S4.T3.2.2.2.25.23.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">69.6</td>
<td id="S4.T3.2.2.2.25.23.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">96.4</td>
<td id="S4.T3.2.2.2.25.23.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">66.1</td>
</tr>
<tr id="S4.T3.2.2.2.26.24" class="ltx_tr">
<th id="S4.T3.2.2.2.26.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">GPT-3.5</th>
<td id="S4.T3.2.2.2.26.24.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">50.5</td>
<td id="S4.T3.2.2.2.26.24.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">54.4</td>
<td id="S4.T3.2.2.2.26.24.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">68.0</td>
<td id="S4.T3.2.2.2.26.24.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">93.2</td>
<td id="S4.T3.2.2.2.26.24.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">61.2</td>
</tr>
<tr id="S4.T3.2.2.2.27.25" class="ltx_tr">
<th id="S4.T3.2.2.2.27.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">Vicuna-13B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S4.T3.2.2.2.27.25.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">32.8</td>
<td id="S4.T3.2.2.2.27.25.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">39.2</td>
<td id="S4.T3.2.2.2.27.25.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">57.6</td>
<td id="S4.T3.2.2.2.27.25.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">84.8</td>
<td id="S4.T3.2.2.2.27.25.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">46.7</td>
</tr>
<tr id="S4.T3.2.2.2.28.26" class="ltx_tr">
<th id="S4.T3.2.2.2.28.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;">Vicuna-7B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<td id="S4.T3.2.2.2.28.26.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">21.5</td>
<td id="S4.T3.2.2.2.28.26.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">34.4</td>
<td id="S4.T3.2.2.2.28.26.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">54.0</td>
<td id="S4.T3.2.2.2.28.26.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;">68.8</td>
<td id="S4.T3.2.2.2.28.26.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">37.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Comparisons between MLLMs.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">Among MLLMs, commercial models outperform open-source alternatives. To elaborate further, the high performance of GPT-4V can be attributed to the use of GPT-4 in creating QA in FintabNetQA.
However, GPT-4V demonstrates the highest performance across all datasets, not just this specific instance.
On TableVQA, we also find that the pivotal role is played by the number of vision queries.
In a specific comparison, SPHINX-MoE-1k, SPHINX-v1-1k, and CogVLM-1k surpass SPHINX-MoE, SHPHINX, and CogVLM, respectively.
These findings, along with observations from Fig. <a href="#S3.F6.sf7" title="In Figure 6 ‣ 3.4 Table Rendering System ‣ 3 TableVQA-Bench ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(g)</span></a>, indicate that vision input generally requires a higher number of queries than text input to achieve promising performance.
Notably, despite LLaVA-1.5 has not been trained on OCR-abundant documents, it exhibits competitive performance to models that included such documents in their training sets.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">MLLMs vs. LLMs.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">From a performance perspective, the text modality outperforms the vision modality as an input source. Specifically, on average, GPT-4 achieves a performance enhancement of 21 % points more than GPT-4V, while Gemini-pro outperforms Gemini-proV by 27.8 % points.
Similarly, open-sourced MLLMs generally have lower performance than their backbone LLMs such as Vicuna-7B and Vicuna-13B.
Although the spatial information in vision inputs might enable easier comprehension of the instance’s location relation, a performance critically dependent on the aspect ratio cannot be overlooked, as seen in Fig. <a href="#S1.F2.sf2" title="In Figure 2 ‣ 1 Introduction ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.
Such findings indicate that in terms of performance, using text inputs still might be advantageous if both vision and text tables are presented.
Meanwhile, even in non-GPT models such as Gemini-Pro and Vicuna-13B, a high level of performance is obtained on FintabNetQA, suggesting that the inherent complexity of the QA pair in the dataset is relatively low.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">The performance of TSR. TEDs evaluates the scores of both the structure and content of the table. A higher value indicates better performance.</span></figcaption>
<div id="S4.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:60.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.7pt,5.6pt) scale(0.845436003975169,0.845436003975169) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.1.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;"></th>
<th id="S4.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">VQWTQ</th>
<th id="S4.T4.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">VWTQ-Syn</th>
<th id="S4.T4.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">VTabFact</th>
<th id="S4.T4.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">FinTabNetQA</th>
<th id="S4.T4.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 4.0pt;">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.2.1" class="ltx_tr">
<th id="S4.T4.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">TSR SoTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S4.T4.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T4.4.1.2.1.2.1" class="ltx_text ltx_font_bold">89.7</span></td>
<td id="S4.T4.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T4.4.1.2.1.3.1" class="ltx_text ltx_font_bold">84.5</span></td>
<td id="S4.T4.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T4.4.1.2.1.4.1" class="ltx_text ltx_font_bold">76.8</span></td>
<td id="S4.T4.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 4.0pt;">52.0</td>
<td id="S4.T4.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 4.0pt;"><span id="S4.T4.4.1.2.1.6.1" class="ltx_text ltx_font_bold">80.4</span></td>
</tr>
<tr id="S4.T4.4.1.3.2" class="ltx_tr">
<th id="S4.T4.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 4.0pt;">Gemini-ProV</th>
<td id="S4.T4.4.1.3.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">72.7</td>
<td id="S4.T4.4.1.3.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">78.4</td>
<td id="S4.T4.4.1.3.2.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">73.0</td>
<td id="S4.T4.4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 4.0pt;">65.8</td>
<td id="S4.T4.4.1.3.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.0pt;">72.6</td>
</tr>
<tr id="S4.T4.4.1.4.3" class="ltx_tr">
<th id="S4.T4.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;">GPT-4V</th>
<td id="S4.T4.4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">64.0</td>
<td id="S4.T4.4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">76.7</td>
<td id="S4.T4.4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">72.8</td>
<td id="S4.T4.4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 4.0pt;"><span id="S4.T4.4.1.4.3.5.1" class="ltx_text ltx_font_bold">72.6</span></td>
<td id="S4.T4.4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 4.0pt;">69.0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2404.19205/assets/figures/working_examples.jpg" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="665" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Examples of qualitative evaluation. The examples are sampled from VWTQ (top) and FinTabNetQA (bottom). FinTabNetQA is evaluated with the <span id="S4.F7.4.2.1" class="ltx_text ltx_font_italic">relieved-accuracy</span> where scale units are intentionally excluded at the evaluation.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Two-stage Inference.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">Two-stage inference leads to significant performance enhancements within the same vision input on both GPT and Gemini families. Despite such enhancements, it is evident that the performance still falls short compared to when text input is used.
While it might be feasible to conduct experiments extracting HTML and answers through prompt tuning in the single MLLM, unfortunately, we were unable to obtain results in our desired format.
Employing TEDs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> evaluation metric, we compare the MLLMs’ performance on TSR with that of the state-of-the-art (SoTA) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
For a fair comparison, we utilize the SoTA model trained only on PubTabNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which can be regarded as a held-out dataset for TableVQA-Bench.
As shown in Table <a href="#S4.T4" title="Table 4 ‣ MLLMs vs. LLMs. ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the SoTA model usually performs better than MLLMs.
These findings indicate that MLLMs exhibit limitations in efficiently extracting information from visual tables.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Qualitative Evaluation.</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">We present qualitative results in Fig. <a href="#S4.F7" title="Figure 7 ‣ MLLMs vs. LLMs. ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The incorrect answers are usually derived from words not presented in the table, which may be attributed to the limitations of OCR capability. A longer length of the vision query appears to alleviate these issues, as demonstrated by the correct answers in the second example.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2404.19205/assets/figures/gpt4v_details.jpg" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">The evaluation is conducted on VWTQ, with 20 instances for each aspect ratio. GPT-4V offers three input image resolution options: ‘auto’, ‘high’, and ‘low’. The ‘high’ setting requires more computational resources for inference compared to the ‘low’.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">GPT-4V Details.</h4>

<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.1" class="ltx_p">The size of table images can vary significantly depending on their content. In this experiment, we explored the impact on model performance when preserving or not preserving the original size of table images. The GPT-4V offers a ‘high’ option that preserves the input resolution, in contrast to a ‘low’ option that appears to resize the image to a fixed size without preserving the original resolution. Additionally, an ‘auto’ option exists that adaptively determines the resolution based on the input image. For each image ratio, we sampled 20 instances and then measured the performance across these resolution modes. As can be seen in Fig. <a href="#S4.F8" title="Figure 8 ‣ Qualitative Evaluation. ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the ‘low’ demonstrated relatively lower performance.
Hence, maintaining the original resolution constitutes a critical factor for accuracy, which is similarly observed in comparisons among MLLMs.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present the TableVQA-Bench, a comprehensive benchmark specifically designed for evaluating table visual question-answering capabilities.
To ensure a wide-ranging domain, we have leveraged a multitude of pre-existing table-related tasks, procuring essential elements such as images and question-answer pairs.
Our study includes an extensive evaluation of various models on the TableVQA-Bench.
Through a comparison among MLLMs, it was observed that GPT-4V outperformed other methods across all evaluated domains.
Based on observations from the comparison with LLMs and the application of a two-stage inference approach, we believe there is significant potential for further enhancements in MLLMs’ performance on visual table understanding tasks.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We greatly appreciate Bado Lee and YoungSang Yoo for their help with the initial project setup.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Gpt-4v(ision) system card (2023), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:263218031" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:263218031</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., Wang, W.Y.: Tabfact: A large-scale dataset for table-based fact verification. In: International Conference on Learning Representations (2020), <a target="_blank" href="https://openreview.net/forum?id=rkeJRhNYDH" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=rkeJRhNYDH</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March 2023), <a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-03-30-vicuna/</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al.: Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914 (2023)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kim, D., Kim, Y., Kim, D., Lim, Y., Kim, G., Kil, T.: Scob: Universal text understanding via character-wise supervised contrastive learning with online text rendering for bridging domain gap. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 19562–19573 (2023)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kim, G., Lee, H., Kim, D., Jung, H., Park, S., Kim, Y., Yun, S., Kil, T., Lee, B., Park, S.: Cream: Visually-situated natural language understanding with contrastive reading model and frozen large language models. arXiv preprint arXiv:2305.15080 (2023)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Lee, K., Joshi, M., Turc, I.R., Hu, H., Liu, F., Eisenschlos, J.M., Khandelwal, U., Shaw, P., Chang, M.W., Toutanova, K.: Pix2struct: Screenshot parsing as pretraining for visual language understanding. In: International Conference on Machine Learning. pp. 18893–18912. PMLR (2023)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., Shan, Y.: Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint arXiv:2311.17092 (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Liu, F., Wang, X., Yao, W., Chen, J., Song, K., Cho, S., Yacoob, Y., Yu, D.: Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774 (2023)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 (2023)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Masry, A., Do, X.L., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In: Findings of the Association for Computational Linguistics: ACL 2022. pp. 2263–2279 (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure understanding with transformers. arXiv preprint arXiv:2203.01017 (2022)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Pasupat, P., Liang, P.: Compositional semantic parsing on semi-structured tables. In: Zong, C., Strube, M. (eds.) Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1470–1480. Association for Computational Linguistics, Beijing, China (Jul 2015). https://doi.org/10.3115/v1/P15-1142, <a target="_blank" href="https://aclanthology.org/P15-1142" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P15-1142</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Xu, Z., Du, S., Qi, Y., Xu, C., Yuan, C., Guo, J.: Chartbench: A benchmark for complex visual reasoning in charts. arXiv preprint arXiv:2312.15915 (2023)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 (2023)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zheng, X., Burdick, D., Popa, L., Zhong, P., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. Winter Conference for Applications in Computer Vision (WACV) (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: data, model, and evaluation. In: European conference on computer vision. pp. 564–580. Springer (2020)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhong, X., ShafieiBavani, E., Jimeno Yepes, A.: Image-based table recognition: data, model, and evaluation. In: European conference on computer vision. pp. 564–580. Springer (2020)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.19204" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.19205" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.19205">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19205" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.19206" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:28:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
