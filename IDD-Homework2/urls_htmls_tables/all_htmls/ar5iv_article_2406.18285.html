<article class="ltx_document ltx_authors_1line">
 <span class="ltx_note ltx_role_institutetext" id="id1">
  <sup class="ltx_note_mark">
   1
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     1
    </sup>
    <span class="ltx_note_type">
     institutetext:
    </span>
    Dept. of Computer, Control, and Management Engineering
    <br class="ltx_break"/>
    Sapienza University of Rome, Rome (Italy),
    <span class="ltx_note ltx_role_email" id="id1.1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        email:
       </span>
       {lastname}@diag.uniroma1.it
      </span>
     </span>
    </span>
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_role_institutetext" id="id2">
  <sup class="ltx_note_mark">
   2
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     2
    </sup>
    <span class="ltx_note_type">
     institutetext:
    </span>
    School of Engineering, University of Basilicata, Potenza (Italy),
    <span class="ltx_note ltx_role_email" id="id2.1">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_note_type">
        email:
       </span>
       vincenzo.suriani@unibas.it
      </span>
     </span>
    </span>
   </span>
  </span>
 </span>
 <span class="ltx_note ltx_role_institutetext" id="id3">
  <sup class="ltx_note_mark">
   3
  </sup>
  <span class="ltx_note_outer">
   <span class="ltx_note_content">
    <sup class="ltx_note_mark">
     3
    </sup>
    <span class="ltx_note_type">
     institutetext:
    </span>
    Dept. of International Humanities and Social Sciences,
International University of Rome, Rome (Italy),
    <span class="ltx_note ltx_role_email" id="id3.1">
     <sup class="ltx_note_mark">
      3
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_note_type">
        email:
       </span>
       domenico.bloisi@unint.eu
      </span>
     </span>
    </span>
   </span>
  </span>
 </span>
 <h1 class="ltx_title ltx_title_document">
  LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    M. Brienza
   </span>
   <span class="ltx_author_notes">
    11
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0009-0000-1549-9500" target="_blank" title="ORCID identifier">
      0009-0000-1549-9500
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    E. Musumeci
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0009-0004-2359-5032" target="_blank" title="ORCID identifier">
      0009-0004-2359-5032
     </a>
    </span>
    11
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    V. Suriani
   </span>
   <span class="ltx_author_notes">
    22
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000-0003-1199-8358" target="_blank" title="ORCID identifier">
      0000-0003-1199-8358
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    D. Affinita
   </span>
   <span class="ltx_author_notes">
    11
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <br class="ltx_break"/>
    A. Pennisi
   </span>
   <span class="ltx_author_notes">
    33
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000-0002-9081-0765" target="_blank" title="ORCID identifier">
      0000-0002-9081-0765
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    D. Nardi
   </span>
   <span class="ltx_author_notes">
    11
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000-0001-6606-200X" target="_blank" title="ORCID identifier">
      0000-0001-6606-200X
     </a>
    </span>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <br class="ltx_break"/>
    D. D. Bloisi
   </span>
   <span class="ltx_author_notes">
    33
    <span class="ltx_contact ltx_role_orcid">
     <a class="ltx_ref" href="https://orcid.org/0000-0003-0339-8651" target="_blank" title="ORCID identifier">
      0000-0003-0339-8651
     </a>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id1.id1">
   The deployment of robots into human scenarios necessitates advanced planning strategies, particularly when we ask robots to operate in dynamic, unstructured environments. RoboCup offers the chance to deploy robots in one of those scenarios, a human-shaped game represented by a soccer match. In such scenarios, robots must operate using predefined behaviors that can fail in unpredictable conditions.
This paper introduces a novel application of Large Language Models (LLMs) to address the challenge of generating actionable plans in such settings, specifically within the context of the RoboCup Standard Platform League (SPL) competitions where robots are required to autonomously execute soccer strategies that emerge from the interactions of individual agents.
In particular, we propose a multi-role approach leveraging the capabilities of LLMs to generate and refine plans for a robotic soccer team.
The potential of the proposed method is demonstrated through an experimental evaluation, carried out simulating multiple matches where robots with AI-generated plans play against robots running human-built code.
  </p>
 </div>
 <div class="ltx_keywords">
  <h6 class="ltx_title ltx_title_keywords">
   Keywords:
  </h6>
  Humanoid Robotics Planning and Reasoning Team Coordination Methods
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="555" id="S1.F1.g1" src="/html/2406.18285/assets/images/into_image.png" width="479"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">
     The LLCoach architecture includes high-level plan generation by the VLM coach, which is then refined to a low-level plan, executable in robot matches.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent advancements in Language Modeling, particularly the proliferation of technologies associated with Large Language Models (LLMs), have unlocked numerous ways to enhance embodied AI systems with common sense knowledge
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    . The expansive scope of LLMs serves as a vast repository of information, facilitating multi-modal interactions with textual and visual data.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    When appropriately queried, LLMs provide access to semantic cues that are crucial for bridging the conceptual gap between abstract planning domains and intricate and unpredictable real-world environments
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    . Consequently, they are increasingly integrated into embodied AI systems
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In this work, we present a first attempt to use generative AI in the context of robot soccer for creating successful game plans using an approach to leverage the LLMs’ chain of thought and generate game tactics by impersonating a coach.
In particular, we present a multi-role pipeline, featuring four stages in which the capabilities of a foundation model are exploited to generate and refine a plan for a soccer robot (see Fig.
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ).
Robot soccer offers an ideal environment for planning within a multi-robot, highly dynamic environment
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    .
Here, we focus on the RoboCup Standard Platform League (SPL), which involves teams of Aldebaran NAO V6 humanoid robots competing autonomously in soccer matches. Participating teams are tasked with executing multi-agent strategies aimed at securing victory in the matches.
It is worth noting that, in SPL matches, the applicability of commonsense knowledge is constrained, given that a detailed understanding of game rules and the capabilities of robots during soccer matches is often specialized knowledge. Access to such information, including the actions available to each agent, is typically facilitated through a Retrieval-Augmented Generation (RAG) system
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    .
The contribution of this work is two-fold:
   </p>
   <ol class="ltx_enumerate" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       A methodology for extracting high-level strategies from videos of RoboCup soccer matches.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       A pipeline for creating detailed, multi-agent plans that can be directly executed by a NAO robot using its available set of basic actions.
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Experimental results are obtained in a simulated environment by performing multiple matches where the robots with the AI-generated plans play against the robots running the code
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SPQRTeam/spqr2023" target="_blank" title="">
        https://github.com/SPQRTeam/spqr2023
       </a>
      </span>
     </span>
    </span>
    used by SPQR Team in RoboCup 2023.
The code and the additional material mentioned in this paper are publicly available at
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sites.google.com/diag.uniroma1.it/llcoach/" target="_blank" title="">
     https://sites.google.com/diag.uniroma1.it/llcoach/
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    The rest of the paper is organized as follows. Section
    <a class="ltx_ref" href="#S2" title="2 Related Work ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    presents a brief overview of related work, while
Section
    <a class="ltx_ref" href="#S3" title="3 Proposed Approach ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    showcases the methodology and the proposed architecture. Experimental results are shown in Section
    <a class="ltx_ref" href="#S4" title="4 Experimental Results ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    . Finally, Section
    <a class="ltx_ref" href="#S5" title="5 Conclusions and Future Directions ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    draws the conclusions and future directions.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    Before the advent of the LLMs, few approaches have been proposed for coaching teams of robots. The first attempt is represented by a language to coach a RoboCup team
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    , where COACH UNILANG is presented for Simulated 3D League. More recently, in SPL,
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    presented an approach to conditioning the robot’s behavior before or during the time of the match.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    More recently, LLMs have been used as planners for both structured
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ]
    </cite>
    and unstructured environments
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    .
However, when using LLMs as planners, especially for embodied AI (where visual cues must be conveyed through text), there is the possibility of incurring hallucinations, causing skewed or imprecise results.
Hallucination is a phenomenon typically observed when the provided textual inputs are too structured and long. For such a reason, it proves beneficial to provide examples of desired output and to define roles for the LLM in a request, by instructing the model on its role and providing constraints on how to perform its task. Demanding to each role a very specific task in a pipeline composed of multiple steps allows to obtain a better final result that would be too complex for the model with a single, lengthy, and complex query.
Thanks to the introduction of VLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    , LLMs can get both textual and visual inputs reducing the length of the context that must be provided textually in scenarios where visual cues are essential to understand the requested task. Such an approach alleviates the issue of hallucination in applications of embodied AI.
However, due to their considerable size in terms of parameters and the consequent computational demands for a single training or inference step, LLMs are less likely to be employed in embedded systems. Moreover, fine-tuning them is an exceedingly resource-intensive endeavor, seldom justifiable for enhancing the accuracy of tasks reliant on specialized commonsense knowledge.
   </p>
  </div>
  <div class="ltx_para" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    To better fit unknown domains, Retrieval-Augmented Generation (RAG) is implemented within intricate pipelines. Here, segments of potentially lengthy documents are indexed based on their semantics in an embedding space and stored within a Vector Index.
This approach facilitates enhanced and expedited customization of LLM query results compared to traditional fine-tuning methods. Moreover, additional pieces of information can be seamlessly integrated into the supplementary knowledge base by appending them to the Semantic Database. This capability enables, for instance, the generation of documents from semantically akin templates
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    RAG-based systems offer interesting applications to robotics. In
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    , RAG-Driver is presented as a novel multimodal RAG LLM for high-performance, explainable, and generalizable autonomous driving. RAGs in robotics offer the possibility to enrich the available commonsense knowledge with more specialized knowledge about the environment or the robotic agent’s capabilities.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Proposed Approach
  </h2>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="478" id="S3.F2.g1" src="/html/2406.18285/assets/images/Process_horizontal.png" width="568"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">
     The pipeline is subdivided into an offline component and an online one, executed in real-time. The offline component collects plans by passing video frames to the coach VLM, and refines them using a multi-role LLM pipeline, using only actions obtained by RAG. The online component retrieves and executes the most fitting plan according to the world model, shared between robots.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Fig.
    <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Proposed Approach ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    shows our multi-role LLM-based planning sequential pipeline, featuring four steps, namely
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">
     Action Retrieval
    </em>
    ,
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">
     Coach VLM
    </em>
    ,
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.3">
     Plan Refinement
    </em>
    , and
    <em class="ltx_emph ltx_font_italic" id="S3.p1.1.4">
     Plan Synchronizer
    </em>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">
     Input.
    </span>
    Although commonsense knowledge is usually enough for most generic tasks, in the case of robot soccer, specific knowledge must be provided. Thus, the prompts for the LLM-based generation steps along the pipeline are constructed from case-specific information.
In particular, case-specific knowledge includes:
   </p>
   <ul class="ltx_itemize" id="S3.I1">
    <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i1.p1">
      <p class="ltx_p" id="S3.I1.i1.p1.1">
       The
       <span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">
        domain
       </span>
       , containing a natural language description of the application (a soccer match between autonomous robots), the definition of tokens representing the locations of waypoints, described in natural language as a discrete distribution of several relevant locations. This allows managing the complexity of planning in a continuous coordinate space by delegating the grounding of waypoint locations (into actual spatial coordinates) to the robot control framework, where a world model will be available.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i2.p1">
      <p class="ltx_p" id="S3.I1.i2.p1.1">
       A
       <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">
        planning goal
       </span>
       : ”
       <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">
        The own team should score a goal in the opponent’s goal.
       </span>
       ”
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S3.I1.i3.p1">
      <p class="ltx_p" id="S3.I1.i3.p1.1">
       A description of robot team formations and agent capabilities in natural language, containing tokens associated with the actions they can perform.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">
     Actions
    </span>
    are described using natural language, following a structure similar to STRIPS actions (similar to Planning Domain Definition Language
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    ) where tokens used from the domain and agent descriptions are used to give the LLM a better understanding of their pre-conditions and effects.
   </p>
  </div>
  <div class="ltx_para" id="S3.p4">
   <p class="ltx_p" id="S3.p4.1">
    To prevent hallucination, the generation process is subdivided into several subsequent generation steps, centered on the interaction with an LLM or a VLM, each with a properly engineered prompt to minimize its size and increase the quality of the information provided to the model.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Action Retrieval
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     To minimize the size of the prompts sent to the LLM or VLM modules requiring an understanding of agent actions, in an attempt to prevent hallucination, actions are stored in a Semantic Database, where each data node corresponds to the vector embedding of their natural language description. The semantic database leverages a Vector Index for efficient storage and retrieval of action definitions.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     The Action Retrieval module retrieves actions from the database based on their semantic similarity with the input plan. A
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">
      Sentence Transformer
     </em>
     , specifically the MPNet model
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     , is employed to compute this semantic similarity. This embedding space is crucial for both populating the database and performing retrieval operations.
Actions stored in the database follow a STRIPS-like template:
     <span class="ltx_text" id="S3.SS1.p2.1.2" style="font-size:90%;">
     </span>
    </p>
    <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p2.2" style="font-size:90%;">
ACTION_ID: the name of the action
DESCRIPTION: a high-level description that explains what
the action does.
ARGS: ARG_NAME : ARG_VALUE specifies action arguments.
PRECONDITIONS: conditions that must be satisfied to execute the action.
EFFECTS: the expected result
</pre>
    <p class="ltx_p" id="S3.SS1.p2.3">
     During retrieval, the planning domain and goal are also embedded within this same space, so that only relevant actions may be extracted using a k-nearest neighbor search based on cosine similarity, measuring the alignment between the required task and the actions stored in the database. This enables the extraction of pertinent actions only, minimizing the prompt length for subsequent pipeline steps and improving its quality.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Coach VLM
   </h3>
   <section class="ltx_subsubsection" id="S3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.1
     </span>
     Visual Analysis.
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS1.p1">
     <p class="ltx_p" id="S3.SS2.SSS1.p1.1">
      To acquire the necessary frame information for the coach’s utilization of the VLM to generate the desired output, we leverage MARIO
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib2" title="">
        2
       </a>
       ]
      </cite>
      , a tool specialized in soccer match video analysis.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS1.p2">
     <p class="ltx_p" id="S3.SS2.SSS1.p2.1">
      MARIO extracts a representation of the soccer game using its tracking capabilities and converts it into a 2D plan view applying a homography geometrical transformation.
This representation is useful
to characterize video frames in a way that a VLM can easily understand. In particular, each robot is marked with a dot representing the color of the robot’s jersey, which indicates the team color (see the bottom-left of Fig.
      <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Proposed Approach ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      ).
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F3">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="388" id="S3.F3.sf1.g1" src="/html/2406.18285/assets/images/action_2.png" width="598"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S3.F3.sf1.2.1.1" style="font-size:90%;">
           (a)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2">
        <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S3.F3.sf2.g1" src="/html/2406.18285/assets/images/simrobot2.png" width="598"/>
        <figcaption class="ltx_caption ltx_centering">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S3.F3.sf2.2.1.1" style="font-size:90%;">
           (b)
          </span>
         </span>
        </figcaption>
       </figure>
      </div>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">
        Figure 3
       </span>
       :
      </span>
      <span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">
       MARIO visual tool, transforming robot pose (original (a), simulation (b)).
      </span>
     </figcaption>
    </figure>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.2
     </span>
     VLM module.
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS2.p1">
     <p class="ltx_p" id="S3.SS2.SSS2.p1.1">
      The
      <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">
       coach
      </span>
      module exploits the capabilities of GPT4-Vision
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib9" title="">
        9
       </a>
       ]
      </cite>
      , a recent Visual Language Model (VLM). A video frame resulting from the visual analysis of a relevant action, like the one in Fig.
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.2.1 Visual Analysis. ‣ 3.2 Coach VLM ‣ 3 Proposed Approach ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      , is provided to the Visual Language Model, along with a textual prompt.
While the video frame provides a detailed view of the current configuration of agents and obstacles on the field, the textual prompt is used to instruct the VLM on its task and constraints. In particular, the coach is instructed to perform two tasks:
     </p>
     <ol class="ltx_enumerate" id="S3.I2">
      <li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="S3.I2.i1.p1">
        <p class="ltx_p" id="S3.I2.i1.p1.1">
         Generating a schematic description of the video frame.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="S3.I2.i2.p1">
        <p class="ltx_p" id="S3.I2.i2.p1.1">
         Generating a high-level plan in natural language.
        </p>
       </div>
      </li>
     </ol>
     <p class="ltx_p" id="S3.SS2.SSS2.p1.2">
      The prompt for the first task is based on the following template.
      <span class="ltx_text" id="S3.SS2.SSS2.p1.2.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS2.p1.3" style="font-size:90%;">
As a coach assisting the red team, provide a precise summary using the
formatting below. Each player’s position should be related to designated
waypoints on the field.
SCENARIO:
[ROLE_OWN_TEAM] is at [WAYPOINT]
[ROLE_OPPONENT_TEAM] is at [WAYPOINT]
...
BALL is at [WAYPOINT]
</pre>
     <p class="ltx_p" id="S3.SS2.SSS2.p1.4">
      Given the proven capabilities of LLMs as few-shot learners
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib12" title="">
        12
       </a>
       ]
      </cite>
      , the prompt is enriched by an example of an ideal output structure.
Then, a description of the robot roles in the multi-agent team is added, followed by a list of the available movement waypoints, with a semantic description for each one. Upper-case words are used in an attempt to increase attention over specific tokens.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS2.SSS2.p2">
     <p class="ltx_p" id="S3.SS2.SSS2.p2.1">
      The second task consists of generating a high-level plan in natural language, describing the optimal strategy to achieve the planning goal, given the same inputs as the first task. The prompt follows the template:
      <span class="ltx_text" id="S3.SS2.SSS2.p2.1.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS2.p2.2" style="font-size:90%;">
After describing the SCENARIO, provide coaching advice.
Your suggestions should utilize the following actions: [ACTIONS]
Recommend player movements to achieve the objective [PLANNING_GOAL]
Your attitude is to perform the following tactics [TACTICS]
</pre>
     <p class="ltx_p" id="S3.SS2.SSS2.p2.3">
      Here:
     </p>
     <ul class="ltx_itemize" id="S3.I3">
      <li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I3.i1.p1">
        <p class="ltx_p" id="S3.I3.i1.p1.1">
         <span class="ltx_text ltx_font_italic" id="S3.I3.i1.p1.1.1">
          [ACTIONS]
         </span>
         is replaced with actions extracted at the retrieval step.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I3.i2.p1">
        <p class="ltx_p" id="S3.I3.i2.p1.1">
         <span class="ltx_text ltx_font_italic" id="S3.I3.i2.p1.1.1">
          [PLANNING_GOAL]
         </span>
         is the original planning goal for the current task, which consists simply in ”
         <span class="ltx_text ltx_font_italic" id="S3.I3.i2.p1.1.2">
          The own team should score a goal in the opponent’s goal.
         </span>
         ”
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I3.i3.p1">
        <p class="ltx_p" id="S3.I3.i3.p1.1">
         <span class="ltx_text ltx_font_italic" id="S3.I3.i3.p1.1.1">
          [TACTICS]
         </span>
         allow additional customization of the planning goal (which is instead fixed).
        </p>
       </div>
      </li>
     </ul>
     <p class="ltx_p" id="S3.SS2.SSS2.p2.4">
      In addition to the task, several constraints over the desired output are added:
      <span class="ltx_text" id="S3.SS2.SSS2.p2.4.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS2.p2.5" style="font-size:90%;">
Respect the sequential execution of actions,considering that the previous
action changes the game situation. Constraints on passing: a robot
cannot pass or kick the ball if it has passed it before, receive
the ball only if you are at the target location otherwise, consider
robot movement actions.
</pre>
     <p class="ltx_p" id="S3.SS2.SSS2.p2.6">
      Then an example of ideal output is provided, to enforce a specific structure:
      <span class="ltx_text" id="S3.SS2.SSS2.p2.6.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS2.p2.7" style="font-size:90%;">
COACH ADVICE:
[give the actions to be performed avoiding conditionals
but describing the precise steps to be performed in sequence.]
</pre>
     <p class="ltx_p" id="S3.SS2.SSS2.p2.8">
      The output of the VLM is the concatenation of the responses to the two tasks:
     </p>
     <ol class="ltx_enumerate" id="S3.I4">
      <li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        1.
       </span>
       <div class="ltx_para" id="S3.I4.i1.p1">
        <p class="ltx_p" id="S3.I4.i1.p1.1">
         A structured description of the scenario represented in the input video frame.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        2.
       </span>
       <div class="ltx_para" id="S3.I4.i2.p1">
        <p class="ltx_p" id="S3.I4.i2.p1.1">
         A multi-agent high-level plan, featuring all the relevant agents detected in the provided snapshots, that represents the suggested strategy for the task, for the given domain, extracted actions, described agents, and planning goal.
        </p>
       </div>
      </li>
     </ol>
     <p class="ltx_p" id="S3.SS2.SSS2.p2.9">
      Therefore, the VLM returns the initial configuration of the agents and obstacles in the video frame, and an ideal strategy to pursue the planning goal.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.2.3
     </span>
     Role Retrieval.
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS3.p1">
     <p class="ltx_p" id="S3.SS2.SSS3.p1.1">
      A description of the relevant roles in the multi-agent team is provided to the Coach VLM in the input prompt. In this way, the Coach module implicitly identifies the roles of the robots in the multi-agent strategy, by mapping the robot in the video frame to the role assigned in the high-level plan, based on its relevance in the strategy.
To enable the coach to do so, relevant roles, such as the goalie (the goalkeeper), the jolly (tasked with moving around to receive passes), or the striker (tasked with managing the ball) are defined in natural language in the input prompt. For example, the striker role is described as follows:
      <span class="ltx_text" id="S3.SS2.SSS3.p1.1.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS3.p1.2" style="font-size:90%;">
- STRIKER: The striker robot is responsible for scoring goals, and is
allowed to perform the following actions: pass_the_ball and kick_to_goal
...
</pre>
     <p class="ltx_p" id="S3.SS2.SSS3.p1.3">
      Similarly, it is possible to describe waypoints (locations reachable by robots on the field) in natural language:
      <span class="ltx_text" id="S3.SS2.SSS3.p1.3.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS2.SSS3.p1.4" style="font-size:90%;">
OUR_GOAL: Our team’s goal area.
...
</pre>
     <p class="ltx_p" id="S3.SS2.SSS3.p1.5">
      Having discretized the field in a distribution of waypoints, it is possible to identify robot roles by comparing their position on the field with the assignment to waypoints that the VLM returns in its first task.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Plan Refinement
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     The plan obtained up until now is a high-level abstract plan, containing tokens representing real-world entities, such as agents, the ball, waypoint locations on the field, and actions. Still, to be executed, two problems have to be solved:
    </p>
    <ul class="ltx_itemize" id="S3.I5">
     <li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I5.i1.p1">
       <p class="ltx_p" id="S3.I5.i1.p1.1">
        The plan must be translated into a more structured version, so that a parser may be used to build a Finite-State Machine that is readily executable by the robotic agents.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I5.i2.p1">
       <p class="ltx_p" id="S3.I5.i2.p1.1">
        Given that several agents concur with the execution of this plan, a hint on the synchronization between the actions of different agents must be provided.
       </p>
      </div>
     </li>
    </ul>
   </div>
   <section class="ltx_subsubsection" id="S3.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.3.1
     </span>
     Plan Grounding.
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS1.p1">
     <p class="ltx_p" id="S3.SS3.SSS1.p1.1">
      The first issue can be solved by transforming the original high-level abstract plan, into a more structured version of the plan. To this aim, the LLM is instructed to satisfy the following task:
      <span class="ltx_text" id="S3.SS3.SSS1.p1.1.1" style="font-size:90%;">
      </span>
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS1.p1.2" style="font-size:90%;">
Generate a high-level plan for the team using the coach’s recommendations
in the shortest number of steps using the planning domain, the planning
goal, and the scenario. Don’t write actions for the opponent team players.
</pre>
    </div>
    <div class="ltx_para" id="S3.SS3.SSS1.p2">
     <p class="ltx_p" id="S3.SS3.SSS1.p2.1">
      This simple task prompt is then followed by:
     </p>
     <ul class="ltx_itemize" id="S3.I6">
      <li class="ltx_item" id="S3.I6.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i1.p1">
        <p class="ltx_p" id="S3.I6.i1.p1.1">
         The same planning domain provided to the VLM.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i2.p1">
        <p class="ltx_p" id="S3.I6.i2.p1.1">
         The actions retrieved by the action database.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i3.p1">
        <p class="ltx_p" id="S3.I6.i3.p1.1">
         Constraints about the structure of the plan, to make sure that actions are written as
         <code class="ltx_verbatim ltx_font_typewriter" id="S3.I6.i3.p1.1.1">
          ACTION_ID AGENT_ID ARGUMENTS
         </code>
         (where
         <code class="ltx_verbatim ltx_font_typewriter" id="S3.I6.i3.p1.1.2">
          AGENT_ID
         </code>
         is the agent acting), followed by a list of possible values for the
         <code class="ltx_verbatim ltx_font_typewriter" id="S3.I6.i3.p1.1.3">
          ACTION_ID
         </code>
         token (the retrieved actions).
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i4" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i4.p1">
        <p class="ltx_p" id="S3.I6.i4.p1.1">
         A list of possible values for the
         <code class="ltx_verbatim ltx_font_typewriter" id="S3.I6.i4.p1.1.1">
          AGENT_ID
         </code>
         field, containing all the relevant roles for the plan, and descriptions of the agents, in natural language.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i5" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i5.p1">
        <p class="ltx_p" id="S3.I6.i5.p1.1">
         Several more fine-grained constraints to ensure that the result is desirable.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i6" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i6.p1">
        <p class="ltx_p" id="S3.I6.i6.p1.1">
         The ”scenario”, is the configuration of elements on the field extracted from the output of the VLM.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S3.I6.i7" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S3.I6.i7.p1">
        <p class="ltx_p" id="S3.I6.i7.p1.1">
         Finally, the high-level plan to be refined, presented as ”COACH RECOMMENDATIONS”
        </p>
       </div>
      </li>
     </ul>
     <p class="ltx_p" id="S3.SS3.SSS1.p2.2">
      The result is a more structured version of the high-level plan returned by the ”coach” VLM, with action names and arguments correctly grounded in the domain tokens.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.3.2
     </span>
     Plan Synchronizer.
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS2.p1">
     <p class="ltx_p" id="S3.SS3.SSS2.p1.1">
      At this stage, actions in the generated plan feature multiple agents but still follow a sequential order. To obtain a multi-agent plan, there should be some indication of how actions performed by different agents at the same time should be synchronized. For this reason, the component of
      <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.1">
       Plan Parallelization
      </span>
      instructs the LLM to return a plan where actions that are executed by different agents at the same time are put together in a ”
      <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS2.p1.1.2">
       JOIN{...}
      </code>
      ” block. The prompt provided to the LLM is then further enriched with a positive example of a valid result and several negative examples of results to avoid, with a brief explanation of the reason that makes them invalid. The resulting plan will then look like:
     </p>
     <pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS2.p1.2">
pass_the_ball STRIKER {’SENDER’: STRIKER, ’RECEIVER’: JOLLY}
JOIN {pass_the_ball JOLLY {’SENDER’: STRIKER, ’RECEIVER’: JOLLY},
      kick_to_goal JOLLY {}}
</pre>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Plan Execution
   </h3>
   <div class="ltx_para" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     The offline portion of the pipeline is used to generate a collection of plans from a given set of video frames, as in Fig.
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Proposed Approach ‣ LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
Collected plans are then selected based on their similarity to the real-world scenario.
The similarity between the two scenarios can be performed using a clustering technique where the current game scenario is obtained by the global model perceived by the robot.
An accurate generation of plans based on various game situations allows for the handling of almost all game situations during matches and allows the robots to apply when needed a suitable behavior that maximizes the likelihood of scoring a goal.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experimental Results
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    This section illustrates the experimental results obtained from the evaluation in a simulated environment of the results of the current work, with the primary objective of assessing its effectiveness in generating high-quality plans collected by processing video frames from real RoboCup SPL matches.
   </p>
  </div>
  <section class="ltx_subsubsection" id="S4.SS0.SSS1">
   <h4 class="ltx_title ltx_title_subsubsection">
    <span class="ltx_tag ltx_tag_subsubsection">
     4.0.1
    </span>
    Prompt Engineering.
   </h4>
   <div class="ltx_para" id="S4.SS0.SSS1.p1">
    <p class="ltx_p" id="S4.SS0.SSS1.p1.1">
     To obtain the desired results, an initial phase of prompt refinement was required. The prompt of the VLM ”coach” module could be more information-rich with a smaller risk of hallucination, given the multi-modal input. Providing both a video frame and textual input allows the coach to give better insight into the subsequent stages along the pipeline. The accuracy of the output at this stage is key, as any error at this stage could mislead the entire generation pipeline. The application domain constitutes a critical component of the prompt for this module, particularly in a use case where the objective is to describe the location of the players based on both an image and a text description.
The main obstacle is the representation of 2D coordinates within the provided frame, which cannot be easily achieved using a VLM due to its difficulty in working on cartesian coordinates.
Instead of coordinates, tokens representing waypoints are used as a way of discretizing the field, by marking significant field locations. The description of these tokens should not mislead the VLM and the significance of waypoints must be explained in natural language. For example, an earlier iteration of the prompt contained the waypoint ”KICKING_POSITION”: initially described as a ”vantage point”, whose semantics are far from having a geometric value. The final version of the prompt avoids such bias-inducing mistakes by describing it as a ”favorable position”. This improved dramatically the results of the scenario description task.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS0.SSS1.p2">
    <p class="ltx_p" id="S4.SS0.SSS1.p2.1">
     The second task assigned to the coach consisted of generating a plan given the situation depicted in the provided input frame. Ideally, the plan should be structured according to an easily parsable template. A first attempt at generating a structured plan directly using the VLM resulted in heavy hallucination. To overcome this problem, the VLM was instead tasked with generating a high-level plan, as if it were a soccer coach. To exert more control on the overall strategy, the possibility to customize the overall ”tactics” was added, for example by specifying whether passes were desirable or if the strategy should be offensive or defensive. A subsequent plan refinement module was added, to refine the high-level plan into a parsable plan. This module used only a textual input, therefore it was key to minimize the input token count, to avoid hallucination.
    </p>
   </div>
  </section>
  <section class="ltx_subsubsection" id="S4.SS0.SSS2">
   <h4 class="ltx_title ltx_title_subsubsection">
    <span class="ltx_tag ltx_tag_subsubsection">
     4.0.2
    </span>
    Setup.
   </h4>
   <div class="ltx_para" id="S4.SS0.SSS2.p1">
    <p class="ltx_p" id="S4.SS0.SSS2.p1.1">
     The VLM Coach module relies on the OpenAI
     <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS2.p1.1.1">
      GPT 4 Turbo
     </em>
     model with vision while the text-only modules feature the latest OpenAI
     <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS2.p1.1.2">
      GPT 3.5 Turbo
     </em>
     model
     <em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS2.p1.1.3">
      gpt-3.5-turbo-0125
     </em>
     . Tests were conducted in a simulated environment based on SimRobot
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib6" title="">
       6
      </a>
      ]
     </cite>
     . Pertinent frames were extracted from historical SPL matches, publicly accessible on the web. For every frame, the robot configuration was replicated within the simulation, reflecting each robot’s pose
     <math alttext="p_{i}=(x,y,\theta)" class="ltx_Math" display="inline" id="S4.SS0.SSS2.p1.1.m1.3">
      <semantics id="S4.SS0.SSS2.p1.1.m1.3a">
       <mrow id="S4.SS0.SSS2.p1.1.m1.3.4" xref="S4.SS0.SSS2.p1.1.m1.3.4.cmml">
        <msub id="S4.SS0.SSS2.p1.1.m1.3.4.2" xref="S4.SS0.SSS2.p1.1.m1.3.4.2.cmml">
         <mi id="S4.SS0.SSS2.p1.1.m1.3.4.2.2" xref="S4.SS0.SSS2.p1.1.m1.3.4.2.2.cmml">
          p
         </mi>
         <mi id="S4.SS0.SSS2.p1.1.m1.3.4.2.3" xref="S4.SS0.SSS2.p1.1.m1.3.4.2.3.cmml">
          i
         </mi>
        </msub>
        <mo id="S4.SS0.SSS2.p1.1.m1.3.4.1" xref="S4.SS0.SSS2.p1.1.m1.3.4.1.cmml">
         =
        </mo>
        <mrow id="S4.SS0.SSS2.p1.1.m1.3.4.3.2" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml">
         <mo id="S4.SS0.SSS2.p1.1.m1.3.4.3.2.1" stretchy="false" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml">
          (
         </mo>
         <mi id="S4.SS0.SSS2.p1.1.m1.1.1" xref="S4.SS0.SSS2.p1.1.m1.1.1.cmml">
          x
         </mi>
         <mo id="S4.SS0.SSS2.p1.1.m1.3.4.3.2.2" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml">
          ,
         </mo>
         <mi id="S4.SS0.SSS2.p1.1.m1.2.2" xref="S4.SS0.SSS2.p1.1.m1.2.2.cmml">
          y
         </mi>
         <mo id="S4.SS0.SSS2.p1.1.m1.3.4.3.2.3" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml">
          ,
         </mo>
         <mi id="S4.SS0.SSS2.p1.1.m1.3.3" xref="S4.SS0.SSS2.p1.1.m1.3.3.cmml">
          θ
         </mi>
         <mo id="S4.SS0.SSS2.p1.1.m1.3.4.3.2.4" stretchy="false" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS0.SSS2.p1.1.m1.3b">
        <apply id="S4.SS0.SSS2.p1.1.m1.3.4.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4">
         <eq id="S4.SS0.SSS2.p1.1.m1.3.4.1.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.1">
         </eq>
         <apply id="S4.SS0.SSS2.p1.1.m1.3.4.2.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.2">
          <csymbol cd="ambiguous" id="S4.SS0.SSS2.p1.1.m1.3.4.2.1.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.2">
           subscript
          </csymbol>
          <ci id="S4.SS0.SSS2.p1.1.m1.3.4.2.2.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.2.2">
           𝑝
          </ci>
          <ci id="S4.SS0.SSS2.p1.1.m1.3.4.2.3.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.2.3">
           𝑖
          </ci>
         </apply>
         <vector id="S4.SS0.SSS2.p1.1.m1.3.4.3.1.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.4.3.2">
          <ci id="S4.SS0.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS2.p1.1.m1.1.1">
           𝑥
          </ci>
          <ci id="S4.SS0.SSS2.p1.1.m1.2.2.cmml" xref="S4.SS0.SSS2.p1.1.m1.2.2">
           𝑦
          </ci>
          <ci id="S4.SS0.SSS2.p1.1.m1.3.3.cmml" xref="S4.SS0.SSS2.p1.1.m1.3.3">
           𝜃
          </ci>
         </vector>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS0.SSS2.p1.1.m1.3c">
        p_{i}=(x,y,\theta)
       </annotation>
      </semantics>
     </math>
     as extracted from the MARIO, ensuring a faithful recreation of real-world scenarios within the simulated environment. Starting from the same initial configuration, the evolution of plans was considered, comparing the pre-existing behaviors and the plans generated by the presented pipeline. For simplicity, only offensive scenarios were taken into account due to their complexity and larger action space compared to defensive scenarios.
    </p>
   </div>
  </section>
  <section class="ltx_subsubsection" id="S4.SS0.SSS3">
   <h4 class="ltx_title ltx_title_subsubsection">
    <span class="ltx_tag ltx_tag_subsubsection">
     4.0.3
    </span>
    Evaluation.
   </h4>
   <div class="ltx_para" id="S4.SS0.SSS3.p1">
    <p class="ltx_p" id="S4.SS0.SSS3.p1.1">
     The proposed approach is evaluated using the success rate metric, measuring the team’s ability to score goals in offensive situations during gameplay. To assess the effectiveness of our method, we compared two approaches: the baseline approach, relying on the predefined behaviors of the SPQR Team 2023 code release, and the dynamic plans generated by the Large Language Model (LLCoach). In addition to the success rate, we considered the average number of passes and average scoring time in seconds as supplementary metrics to illustrate the evolution of gameplay dynamics over different settings.
Considering two different situations from the final match BHuman - HTWK we recreated the robot configurations running eight simulations in total. It is worth noting that, since several API calls are required to obtain and refine the plan throughout the multi-role pipeline, the overall execution time is affected mainly by the inference times of LLM/VLM agents.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T1">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S4.T1.2.1.1">
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.1.1.1">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1">
         Metrics
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.2">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.2.1">
         SPQR Human-written Code
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.2.1.1.3">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.3.1">
         LLCoach
        </span>
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T1.2.2.1">
       <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.2.1.1">
        Success Rate
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.2">
        30%
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.3">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.2.1.3.1">
         90%
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.2.3.2">
       <td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.3.2.1">
        Avg. no. of passes
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.3.2.2">
        0.60
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.3.2.3">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.3.2.3.1">
         1.33
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.2.4.3">
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.2.4.3.1">
        Avg. scoring time
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.4.3.2">
        66 sec.
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.4.3.3">
        <span class="ltx_text ltx_font_bold" id="S4.T1.2.4.3.3.1">
         29.7 sec.
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">
       Table 1
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">
      Comparison of success rate, average passes, and seconds played obtained by comparing eight simulations with LLCoach and the human-written code.
     </span>
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusions and Future Directions
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we presented a novel approach to plan generation, exploiting the common knowledge in Large Language Models. The proposed approach represents a first attempt at using generative AI in the context of robot soccer. In particular, we described a complete pipeline, in four stages, where a high-level plan, is progressively refined into a multi-agent plan. The first stage, a Visual Language Model acting as a coach, generates plans from video frames, extracted from videos RoboCup SPL matches. Collected plans can then be run by real NAO robots, as shown by the promising results conducted in the context of RoboCup SPL matches.
As future work, we intend to use videos from human soccer matches to create policies for soccer robot players. Our aim is to reduce the gap between robot behaviors and human moves in order to achieve the 2050 RoboCup goal.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Acknowledgement
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    This work has been carried out while Michele Brienza was enrolled in the Italian National Doctorate on Artificial Intelligence run by Sapienza University of Rome. This work has been supported by PNRR MUR project PE0000013-FAIR.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     Balaguer, A., Benara, V., de Freitas Cunha, R.L., de M. Estevão Filho, R., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: Rag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture (2024)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     Bloisi, D.D., Pennisi, A., Zampino, C., Biancospino, F., Laus, F., Stefano, G.D., Brienza, M., Romano, R.: Mario: Modular and extensible architecture for computing visual statistics in robocup spl (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     Dorbala, V.S., Mullen, J.F., Manocha, D.: Can an embodied agent find your “cat-shaped mug”? llm-based zero-shot object navigation. IEEE Robotics and Automation Letters
     <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">
      9
     </span>
     (5), 4083–4090 (2024)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     Ghallab, M., Knoblock, C., Wilkins, D., Barrett, A., Christianson, D., Friedman, M., Kwok, C., Golden, K., Penberthy, S., Smith, D., Sun, Y., Weld, D.: Pddl - the planning domain definition language (08 1998)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     Huang, W., Abbeel, P., Pathak, D., Mordatch, I.: Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In: International Conference on Machine Learning. pp. 9118–9147. PMLR (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     Laue, T., Spiess, K., Röfer, T.: Simrobot – a general physical robot simulator and its application in robocup. In: Bredenfeld, A., Jacoff, A., Noda, I., Takahashi, Y. (eds.) RoboCup 2005: Robot Soccer World Cup IX. pp. 173–183. Springer Berlin Heidelberg, Berlin, Heidelberg (2006)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     Musumeci, E., Brienza, M., Suriani, V., Nardi, D., Bloisi, D.D.: Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain (2024)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     Musumeci, E., Suriani, V., Antonioni, E., Nardi, D., Bloisi, D.D.: Adaptive team behavior planning using human coach commands. In: Eguchi, A., Lau, N., Paetzel-Prüsmann, M., Wanichanon, T. (eds.) RoboCup 2022: Robot World Cup XXV. pp. 112–123. Springer International Publishing, Cham (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     OpenAI: Gpt-4v(ision) system card (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     Reis, L.P., Lau, N.: Coach unilang-a standard language for coaching a (robo) soccer team. Springer (2002)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     Silver, T., Dan, S., Srinivas, K., Tenenbaum, J.B., Kaelbling, L.P., Katz, M.: Generalized planning in pddl domains with pretrained large language models (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     Song, C.H., Wu, J., Washington, C., Sadler, B.M., Chao, W.L., Su, Y.: Llm-planner: Few-shot grounded planning for embodied agents with large language models (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     Song, K., Tan, X., Qin, T., Lu, J., Liu, T.: Mpnet: Masked and permuted pre-training for language understanding. In: Advances in Neural Information Processing Systems (2020)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     Suriani, V., Musumeci, E., Nardi, D., Bloisi, D.D.: Play everywhere: A temporal logic based game environment independent approach for playing soccer with robots. In: RoboCup 2023: Robot World Cup XXVI. pp. 3–14. Springer (2024)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     Yuan, J., Sun, S., Omeiza, D., Zhao, B., Newman, P., Kunze, L., Gadd, M.: Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modal large language model. arXiv preprint arXiv:2402.10828 (2024)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     Zhao, Z., Lee, W.S., Hsu, D.: Large language models as commonsense knowledge for large-scale task planning. In: Advances in Neural Information Processing Systems. vol. 36, pp. 31967–31987. Curran Associates, Inc. (2023)
    </span>
   </li>
  </ul>
 </section>
</article>
