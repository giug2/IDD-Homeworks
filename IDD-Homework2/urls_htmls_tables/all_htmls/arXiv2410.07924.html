<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results</title>
<!--Generated on Thu Oct 10 13:43:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Multiple Sclerosis Segmentation Brain MRI Lesion Detection Deep Learning Medical Imaging." lang="en" name="keywords"/>
<base href="/html/2410.07924v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S1" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.SS1" title="In 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>MSLesSeg Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.SS2" title="In 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Dataset preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.SS3" title="In 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Lesion delineation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S3" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Competition Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S3.SS1" title="In 3 Competition Overview ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Competition Protocol and Duration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S3.SS2" title="In 3 Competition Overview ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluation metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S4" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Participants and Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S5" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Competition Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S6" title="In ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S6.SS0.SSS1" title="In 6 Conclusion ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.0.1 </span>Acknowledgements</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Department of Mathematics and Computer Science, University of Catania, Catania, Italy </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Biomedical and Biotechnological Sciences, University of Catania, Catania, Italy </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Department of Drug and Health Sciences, University of Catania, Catania, Italy </span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>UOC Radiologia, ARNAS Garibaldi, Catania, Italy </span></span></span><span class="ltx_note ltx_role_institutetext" id="id5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Centro Sclerosi Multipla, UOC Neurologia con Stroke Unit, Azienda Ospedaliera per l’Emergenza Cannizzaro, Catania, Italy</span></span></span>
<h1 class="ltx_title ltx_title_document">ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessia Rondinella 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6825-8708" title="ORCID identifier">0000-0002-6825-8708</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Guarnera
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7703-3367" title="ORCID identifier">0000-0002-7703-3367</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elena Crispino
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-9289-4926" title="ORCID identifier">0000-0001-9289-4926</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giulia Russo
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6616-7856" title="ORCID identifier">0000-0001-6616-7856</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Clara Di Lorenzo
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Davide Maimone
</span><span class="ltx_author_notes">55
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1906-7953" title="ORCID identifier">0000-0003-1906-7953</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Pappalardo
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-1668-3320" title="ORCID identifier">0000-0003-1668-3320</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastiano Battiato
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6127-2470" title="ORCID identifier">0000-0001-6127-2470</a></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This report summarizes the outcomes of the ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation (MSLesSeg). The competition aimed to develop methods capable of automatically segmenting multiple sclerosis lesions in MRI scans. Participants were provided with a novel annotated dataset comprising a heterogeneous cohort of MS patients, featuring both baseline and follow-up MRI scans acquired at different hospitals.
MSLesSeg focuses on developing algorithms that can independently segment multiple sclerosis lesions of an unexamined cohort of patients. This segmentation approach aims to overcome current benchmarks by eliminating user interaction and ensuring robust lesion detection at different timepoints, encouraging innovation and promoting methodological advances.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Multiple Sclerosis Segmentation Brain MRI Lesion Detection Deep Learning Medical Imaging.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multiple Sclerosis (MS) is a chronic neurological disease affecting millions of people worldwide. MS, causes inflammation in various areas of the brain, resulting in damage to the myelin, the fatty tissue that surrounds and insulates nerve fibers, which is crucial for proper neurological function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib1" title="">1</a>]</cite>. Magnetic Resonance Imaging (MRI) is a fundamental tool for diagnosing MS, monitoring its progression, and evaluating responses to treatments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib2" title="">2</a>]</cite>. Accurate segmentation of MS lesions is essential for volumetric quantification of lesion load, which helps in clinical decision-making and patient management. However, manual segmentation of MS lesions on MRI scans is a labor-intensive and time-consuming process that requires significant expertise. Moreover, large datasets with manual segmentations carried out by experts are limited, posing a significant challenge for the development and validation of automated segmentation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib3" title="">3</a>]</cite>. This highlights the urgent need for developing automated methods capable of segmenting MS lesions, which would significantly enhance clinical management and treatment optimization for MS patients <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib4" title="">4</a>]</cite>.
Recent advances in Deep Learning (DL) have shown significant potential in automating this task, leading to more consistent and efficient lesion segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib6" title="">6</a>]</cite>. As known, Deep learning methods require the availability of huge amount of data, therefore the lack of large labeled datasets represents a limitation to these studies. The scarcity of large, annotated datasets restricts the training and validation of deep learning models, potentially impacting their generalizability and performance on unseen data.
Our initiative aims to fill existing gaps in fully automated MS lesion segmentation by providing participants with an extensively annotated MRI dataset derived from a heterogeneous cohort of MS patients. A notable strength of our initiative lies in the substantial number of patients scans, surpassing publicly available datasets used for MS lesion segmentation. Furthermore, our dataset is particularly advantageous because it is representative of real-world scenarios, with authentic MS patients; in fact, it is acquired "in daily practice", reflecting a heterogeneous and unconstrained acquisition environment. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">1</span></a> presents a numerical comparison of the main datasets used for MS lesion segmentation, highlighting the advantages of the MSLesSeg dataset in relation to existing datasets.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison between the proposed dataset and the main dataset used in MS Lesion segmentation. <span class="ltx_text ltx_font_italic" id="S1.T1.8.1">Dataset</span> indicates the name of the dataset, <span class="ltx_text ltx_font_italic" id="S1.T1.9.2">N. patients</span> denotes the number of patients in each dataset. <span class="ltx_text ltx_font_italic" id="S1.T1.10.3">M/F</span> denotes the Male/Female ratio. <span class="ltx_text ltx_font_italic" id="S1.T1.11.4">N. timepoints</span> is the number of timepoints. <span class="ltx_text ltx_font_italic" id="S1.T1.12.5">Age Mean (SD)</span> is the mean age (and standard deviation) in years, at baseline. <span class="ltx_text ltx_font_italic" id="S1.T1.13.6">Training cases</span> and <span class="ltx_text ltx_font_italic" id="S1.T1.14.7">Testing cases</span> are the division of data into training and testing sets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.15" style="width:438.1pt;height:100pt;vertical-align:-10.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S1.T1.15.1"><span class="ltx_text" id="S1.T1.15.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T1.15.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S1.T1.15.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.1">Dataset</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.2">N.</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.3">M/F</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.4">N.</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.5">Age</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.6">Follow-Up</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.7">Training</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.1.1.8">Testing</span></span>
<span class="ltx_tr" id="S1.T1.15.1.1.1.2.2">
<span class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r" id="S1.T1.15.1.1.1.2.2.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.2">patients</span>
<span class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.3"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.4">timepoints</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.5">Mean (SD)</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.6">Mean (SD)</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.7">cases</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S1.T1.15.1.1.1.2.2.8">cases</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S1.T1.15.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.1">ISBI2015</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.2">19</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.3">4/15</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.4">2-6</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.5">43.5 (±10.3)</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.6">1.0 (±0.18)</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.7">5</span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.15.1.1.1.3.1.8">14</span></span>
<span class="ltx_tr" id="S1.T1.15.1.1.1.4.2">
<span class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S1.T1.15.1.1.1.4.2.1">MSSEG-2016</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.2">53</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.3">15/38</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.4">1</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.5">45.4 (±10.3)</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.6">N.A.</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.7">15</span>
<span class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.15.1.1.1.4.2.8">38</span></span>
<span class="ltx_tr" id="S1.T1.15.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S1.T1.15.1.1.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.1.1">MSLesSeg</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.2.1">75</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.3.1">27/48</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.4.1">1-4</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.5.1">37 (±10.3)</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.6"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.6.1">1.27 (±0.62)</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.7.1">53</span></span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S1.T1.15.1.1.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S1.T1.15.1.1.1.5.3.8.1">22</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The goal of the MSLesSeg competition is to promote the development of algorithms capable of autonomously segmenting MS lesions in unseen MRI data series (T1-w, T2-w, and FLAIR). By ensuring robust lesion detection across different timepoints, this competition aims to push the boundaries of current benchmarks, encouraging innovation and promoting methodological advancements in the field of MS lesion segmentation.
We are confident that the MSLesSeg competition represents a significant step forward in the pursuit of automated MS lesion segmentation. By providing a diverse and comprehensive dataset, we aim to inspire the development of advanced algorithms that can significantly improve the clinical management of MS and optimize treatment strategies for patients worldwide.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The remainder of this paper is structured as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2" title="2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the dataset used in the competition, detailing the preprocessing steps and the lesion annotation process. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S3" title="3 Competition Overview ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of the competition, including its description, protocol, duration, and the metrics used for evaluating the results. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S4" title="4 Participants and Methods ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">4</span></a> contains information about the participating teams and descriptions of the methods they proposed. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S5" title="5 Competition Results ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">5</span></a> presents the results achieved by the participants, along with the final leaderboard. Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S6" title="6 Conclusion ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper with a discussion of the competition’s outcomes and future directions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>MSLesSeg Dataset</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">For this competition, we provided participants with the MSLesSeg Dataset, a comprehensively annotated dataset containing labeled MRI scans acquired from various MRI scanners. The dataset includes MRI series from 75 patients aged between 18 and 59 years, with an average age of 37 (±10.3) years. Of these patients, 48 are women and 27 are men. The MRI scans were acquired at multiple timepoints, ranging from 1 to 4 per patient. Specifically, 50 patients had 1 timepoints, 15 patients had 2 timepoints, 5 patients had 3 timepoints and 5 patients had 4 timepoints. The interval between consecutive timepoints is approximately 1.27 (±0.62) years. In total, the dataset comprises 115 MRI data series. Each timepoint includes three different scan modalities: T1-weighted (T1-w), T2-weighted (T2-w), and Fluid-Attenuated Inversion Recovery (FLAIR). The dataset was meticulously preprocessed and annotated by experts, with lesion annotations performed on the FLAIR sequences and complementary T1-w and T2-w sequences used for comprehensive lesion characterization. The dataset was divided into training and test sets, with 53 scans allocated to the training set and 22 scans reserved for the test set.
The study was approved by the corresponding Hospital Ethics Committee and all patients gave their informed consent.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Dataset preprocessing</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">All MRI scans in the dataset underwent a series of preprocessing steps to ensure uniformity. At first, all scans were fully anonymized to protect patient privacy and then the scans were converted from the DICOM format to the NIFTI format, which is widely used in neuroimaging due to its compatibility and ease of use.
Subsequently, each MRI modality was co-registered to the <math alttext="1mm^{3}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">1</mn><mo id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">m</mi><mo id="S2.SS2.p1.1.m1.1.1.1a" xref="S2.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><msup id="S2.SS2.p1.1.m1.1.1.4" xref="S2.SS2.p1.1.m1.1.1.4.cmml"><mi id="S2.SS2.p1.1.m1.1.1.4.2" xref="S2.SS2.p1.1.m1.1.1.4.2.cmml">m</mi><mn id="S2.SS2.p1.1.m1.1.1.4.3" xref="S2.SS2.p1.1.m1.1.1.4.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn id="S2.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.2">1</cn><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">𝑚</ci><apply id="S2.SS2.p1.1.m1.1.1.4.cmml" xref="S2.SS2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.4.1.cmml" xref="S2.SS2.p1.1.m1.1.1.4">superscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.4.2.cmml" xref="S2.SS2.p1.1.m1.1.1.4.2">𝑚</ci><cn id="S2.SS2.p1.1.m1.1.1.4.3.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.4.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">1mm^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> MNI152 isotropic template using FMRIB’s Linear Image Registration Tool (FLIRT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib7" title="">7</a>]</cite>, a fully automated instrument for brain image registration, ensuring that all scans are aligned to a common reference space.
After registration, brain extraction was performed using the Brain Extraction Tool (BET) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib8" title="">8</a>]</cite>. BET effectively removes non-brain tissues, such as the skull and scalp, isolating the brain for subsequent analysis. This preprocessing pipeline ensures that all MRI scans are standardized, facilitating the development and evaluation of MS lesion segmentation algorithms.
The preprocessing pipeline is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F1" title="Figure 1 ‣ 2.2 Dataset preprocessing ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">1</span></a>, while a proper example of images after preprocessing is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F2" title="Figure 2 ‣ 2.2 Dataset preprocessing ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="342" id="S2.F1.g1" src="extracted/5916746/preprocessing.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Preprocessing steps: all brain MRI (T1-w, T2-w and FLAIR) were aligned to the standard <math alttext="1mm^{3}" class="ltx_Math" display="inline" id="S2.F1.2.m1.1"><semantics id="S2.F1.2.m1.1b"><mrow id="S2.F1.2.m1.1.1" xref="S2.F1.2.m1.1.1.cmml"><mn id="S2.F1.2.m1.1.1.2" xref="S2.F1.2.m1.1.1.2.cmml">1</mn><mo id="S2.F1.2.m1.1.1.1" xref="S2.F1.2.m1.1.1.1.cmml">⁢</mo><mi id="S2.F1.2.m1.1.1.3" xref="S2.F1.2.m1.1.1.3.cmml">m</mi><mo id="S2.F1.2.m1.1.1.1b" xref="S2.F1.2.m1.1.1.1.cmml">⁢</mo><msup id="S2.F1.2.m1.1.1.4" xref="S2.F1.2.m1.1.1.4.cmml"><mi id="S2.F1.2.m1.1.1.4.2" xref="S2.F1.2.m1.1.1.4.2.cmml">m</mi><mn id="S2.F1.2.m1.1.1.4.3" xref="S2.F1.2.m1.1.1.4.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.2.m1.1c"><apply id="S2.F1.2.m1.1.1.cmml" xref="S2.F1.2.m1.1.1"><times id="S2.F1.2.m1.1.1.1.cmml" xref="S2.F1.2.m1.1.1.1"></times><cn id="S2.F1.2.m1.1.1.2.cmml" type="integer" xref="S2.F1.2.m1.1.1.2">1</cn><ci id="S2.F1.2.m1.1.1.3.cmml" xref="S2.F1.2.m1.1.1.3">𝑚</ci><apply id="S2.F1.2.m1.1.1.4.cmml" xref="S2.F1.2.m1.1.1.4"><csymbol cd="ambiguous" id="S2.F1.2.m1.1.1.4.1.cmml" xref="S2.F1.2.m1.1.1.4">superscript</csymbol><ci id="S2.F1.2.m1.1.1.4.2.cmml" xref="S2.F1.2.m1.1.1.4.2">𝑚</ci><cn id="S2.F1.2.m1.1.1.4.3.cmml" type="integer" xref="S2.F1.2.m1.1.1.4.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.m1.1d">1mm^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.F1.2.m1.1e">1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> MNI space, then brain tissue was extracted.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="158" id="S2.F2.sf1.g1" src="extracted/5916746/flair_axial.png" width="132"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S2.F2.sf1.3.2" style="font-size:80%;">Flair</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="158" id="S2.F2.sf2.g1" src="extracted/5916746/t1_axial.png" width="132"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S2.F2.sf2.3.2" style="font-size:80%;">T1-W</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="158" id="S2.F2.sf3.g1" src="extracted/5916746/t2_axial.png" width="132"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.sf3.2.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S2.F2.sf3.3.2" style="font-size:80%;">T2-W</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample axial MRI images of the brain of an MS patient of the MSLesSeg Dataset in each modality of acquisition <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F2.sf1" title="In Figure 2 ‣ 2.2 Dataset preprocessing ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2(a)</span></a> FLAIR, <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F2.sf2" title="In Figure 2 ‣ 2.2 Dataset preprocessing ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2(b)</span></a> T1-weighted, <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F2.sf3" title="In Figure 2 ‣ 2.2 Dataset preprocessing ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2(c)</span></a> T2-weighted</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Lesion delineation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The dataset was manually annotated to obtain the ground-truth masks of hyperintense lesions on FLAIR for each timepoint of every patient. Manual segmentation of MS lesions on MRI is inherently challenging due to the complexity of the task and the inherent variability among different expert annotators.
To achieve a reliable ground-truth, the task was carried out by three experts. Lesion segmentation was performed primarily on the FLAIR modality, with comprehensive cross-checks on T2-w and T1-w modalities. The manual segmentation was conducted by a junior rater, who was trained by two senior experts: a senior neuroradiologist and a senior neurologist, both with extensive experience in MS.
Several training sessions were held between the junior rater and the two senior experts to establish a consistent lesion segmentation strategy and to introduce the junior rater with the segmentation tool, JIM9 <span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.xinapse.com/j-im-9-software/</span></span></span>. JIM9 is a sophisticated medical image processing software known for its advanced capabilities in image registration, segmentation, and analysis.
After the training sessions, the junior rater began labeling the 115 MRI data series. Throughout the process, additional meetings were held with the senior experts to validate the annotated scans. This iterative approach ensured that the annotations were accurate and consistent.
The manual segmentation protocol required independent segmentation for each patient and each timepoint to avoid bias. Segmentation was performed on the FLAIR images registered to the <math alttext="1mm^{3}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mn id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">m</mi><mo id="S2.SS3.p1.1.m1.1.1.1a" xref="S2.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><msup id="S2.SS3.p1.1.m1.1.1.4" xref="S2.SS3.p1.1.m1.1.1.4.cmml"><mi id="S2.SS3.p1.1.m1.1.1.4.2" xref="S2.SS3.p1.1.m1.1.1.4.2.cmml">m</mi><mn id="S2.SS3.p1.1.m1.1.1.4.3" xref="S2.SS3.p1.1.m1.1.1.4.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><cn id="S2.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1.2">1</cn><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">𝑚</ci><apply id="S2.SS3.p1.1.m1.1.1.4.cmml" xref="S2.SS3.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.4.1.cmml" xref="S2.SS3.p1.1.m1.1.1.4">superscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.4.2.cmml" xref="S2.SS3.p1.1.m1.1.1.4.2">𝑚</ci><cn id="S2.SS3.p1.1.m1.1.1.4.3.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1.4.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">1mm^{3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">1 italic_m italic_m start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> isotropic MNI template, with T2-w and T1-w images used to confirm the presence of ambiguous or challenging lesions. After validation by the two senior experts, the resulting mask was deemed as ground-truth.
An example of a ground-truth mask is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F3" title="Figure 3 ‣ 2.3 Lesion delineation ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="144" id="S2.F3.sf1.g1" src="extracted/5916746/flair_8.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S2.F3.sf1.3.2" style="font-size:80%;">FLAIR</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="144" id="S2.F3.sf2.g1" src="extracted/5916746/mask_8.png" width="419"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S2.F3.sf2.3.2" style="font-size:80%;">Mask</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of a (b) segmentation mask generated from the (a) FLAIR sequence. The images are presented in axial, coronal, and sagittal views.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Competition Overview</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this competition, the objective is the automatic segmentation of MS lesions. To tackle this task, we provided participants with MRI scans in three modalities: FLAIR, T1-w, and T2-w, along with their corresponding segmentation masks, as a ground-truth. The segmentation masks are binary, where white pixels represent the regions of the image corresponding to MS lesions, and black pixels represent the background.
Participants were asked to use any or all of these modalities, as well as the provided ground-truth, to develop deep learning-based algorithms capable of automatically generating MS lesion segmentation masks. MS lesions are represented as clusters of pixels in the ground-truth masks and can vary greatly in size and shape. These lesions are often very challenging to detect with the naked eye on MRI scans and require expert interpretation.
The goal is to develop deep learning algorithms that can segment MS lesions in a fully automated manner. An example of a segmentation mask is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S2.F3.sf2" title="In Figure 3 ‣ 2.3 Lesion delineation ‣ 2 Dataset ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">3(b)</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Competition Protocol and Duration</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">During the competition, a dedicated website <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://iplab.dmi.unict.it/mfs/ms-les-seg/</span></span></span> was used to host all the rules and details useful for participants. This website will remain active even after the competition concludes, serving as a resource for participants and the wider research community.
The competition, which spanned three months, began with a team registration phase. Participants, after registering, were required to sign a Data Licence Agreement (DLA) before obtaining the training set; after DLA, the list of participants was published on the competition website.
After releasing the training set, participants were invited to develop methods for the automatic segmentation of MS lesions. Only fully automated methods were allowed. To ensure fair comparison among the participants, the data used during the training phase was limited to the dataset provided by the competition; no additional data was permitted. Afterwards, the test dataset was released, which included only the FLAIR, T1-w, and T2-w scans for each patient, but not the ground-truth masks.
Alongside the release of the test dataset, we provided an evaluation script. This script allowed participants to calculate their scores using their methods on the training set scans by using a subset as a validation set, as they did not have access to the ground-truth of the test set scans. This same script was used by the organizers to generate the final leaderboard.
During the competition, each team was allowed to upload up to three submissions, with only the last submission being considered for the final results.
Participants then moved to the submission phase, where they were required to submit a repository containing the segmentation masks generated by their proposed solutions. Additionally, each team was asked to produce a 1-2 page summary detailing their technical solutions to the competition.
After the submission deadline, scores were calculated only for participants who uploaded both the summary and at least one result file. All results were publicly displayed on the competition website, where the final leaderboard was also published at the end of the competition.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation metrics</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The participants’ methods were evaluated based on their accuracy in segmenting MS lesions, measuring the overlap between the predicted lesions by the proposed method and the actual lesions in the ground-truth mask. We used a metric based on overlap, specifically the Mean Dice Score (DSC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib9" title="">9</a>]</cite>, to determine the participants’ ranking in the final leaderboard.
The evaluation of the models was conducted by comparing the submitted predicted segmentation masks of each teams with the ground-truth maskss. The DSC was the metric used, defined as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="DSC=\frac{2TP}{2TP+FP+FN}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">D</mi><mo id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml">S</mi><mo id="S3.E1.m1.1.1.2.1a" xref="S3.E1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.2.4" xref="S3.E1.m1.1.1.2.4.cmml">C</mi></mrow><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mn id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">2</mn><mo id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">T</mi><mo id="S3.E1.m1.1.1.3.2.1a" xref="S3.E1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.2.4" xref="S3.E1.m1.1.1.3.2.4.cmml">P</mi></mrow><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mrow id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml"><mn id="S3.E1.m1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.3.3.2.2.cmml">2</mn><mo id="S3.E1.m1.1.1.3.3.2.1" xref="S3.E1.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.3.3.2.3.cmml">T</mi><mo id="S3.E1.m1.1.1.3.3.2.1a" xref="S3.E1.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.2.4" xref="S3.E1.m1.1.1.3.3.2.4.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S3.E1.m1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml"><mi id="S3.E1.m1.1.1.3.3.4.2" xref="S3.E1.m1.1.1.3.3.4.2.cmml">F</mi><mo id="S3.E1.m1.1.1.3.3.4.1" xref="S3.E1.m1.1.1.3.3.4.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.4.3" xref="S3.E1.m1.1.1.3.3.4.3.cmml">N</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝐷</ci><ci id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">𝑆</ci><ci id="S3.E1.m1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.2.4">𝐶</ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><divide id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3"></divide><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><times id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"></times><cn id="S3.E1.m1.1.1.3.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.2">2</cn><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑇</ci><ci id="S3.E1.m1.1.1.3.2.4.cmml" xref="S3.E1.m1.1.1.3.2.4">𝑃</ci></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><plus id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></plus><apply id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2"><times id="S3.E1.m1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.3.3.2.1"></times><cn id="S3.E1.m1.1.1.3.3.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.2.2">2</cn><ci id="S3.E1.m1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.3.3.2.3">𝑇</ci><ci id="S3.E1.m1.1.1.3.3.2.4.cmml" xref="S3.E1.m1.1.1.3.3.2.4">𝑃</ci></apply><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">𝐹</ci><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">𝑃</ci></apply><apply id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4"><times id="S3.E1.m1.1.1.3.3.4.1.cmml" xref="S3.E1.m1.1.1.3.3.4.1"></times><ci id="S3.E1.m1.1.1.3.3.4.2.cmml" xref="S3.E1.m1.1.1.3.3.4.2">𝐹</ci><ci id="S3.E1.m1.1.1.3.3.4.3.cmml" xref="S3.E1.m1.1.1.3.3.4.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">DSC=\frac{2TP}{2TP+FP+FN}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_D italic_S italic_C = divide start_ARG 2 italic_T italic_P end_ARG start_ARG 2 italic_T italic_P + italic_F italic_P + italic_F italic_N end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">where TP, FP, and FN represent the number of true positive, false positive, and false negative pixels, respectively. The DSC is a widely used metric for measuring the similarity between two samples, especially in medical image segmentation tasks such as lesion segmentation, where precise delineation of objects is crucial. The DSC ranges from 0 to 1, where 0 indicates no overlap between the predicted and ground-truth masks, and 1 indicates a perfect overlap.
Specifically, we calculated the DSC for each data series in the test set using the formula (<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S3.E1" title="In 3.2 Evaluation metrics ‣ 3 Competition Overview ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">1</span></a>). The final performance ranking of a participant was determined by the overall mean DSC across all test set data series.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Participants and Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">A total of 28 teams registered for the competition, with 15 teams submitting their predictions for the final phase. Some teams were unable to submit their predictions due to the strict time constraints. Another possible reason could be the challenging nature of the dataset, which might have discouraged some participants.
Overall, we are very pleased with the contributions of the participants and hope that all participants, including those who were unable to submit their results on time, will continue working with the dataset to improve their segmentation methods. Below, we briefly describe the methods proposed by each team:
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">MSUniCaTeam.</span>
The team, composed of Andrea Loddo, Alessandro Pani, Lorenzo Putzu, Luca Zedda, and Cecilia Di Ruberto, all from the University of Cagliari, proposed an ensemble method using Swin-UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib10" title="">10</a>]</cite>, UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib11" title="">11</a>]</cite>, and SegResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib12" title="">12</a>]</cite> models. They preprocessed the data by merging contrasts, random cropping, and normalizing intensity. Each model was trained with the same number of epochs and batch size. They used Dice Loss for model training. The final predictions were produced by combining the models’ outputs through a weighted average based on the DSC from the validation set, enhancing the robustness of the final segmentation results.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.2">LST.</span>
The team, composed of Domen Preloznik and Ziga Spiclin from the Laboratory of Imaging Technologies, University of Ljubljana, further preprocessed the dataset using ANTsPy <span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/ANTsX/ANTsPy</span></span></span> to register T1-w images to the MNI ICBM 2009c atlas and computed mean transformations. They transformed all modalities and masks using this mean transformation, cropped images, and rescaled them. They trained their model using the nnU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite> framework with custom configurations, experimenting with various initialization and training strategies. For evaluation, they used lesion-specific metrics from the Lesion-metrics repository <span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/Pubec/lesion-metrics</span></span></span> and generated segmentation masks using the best-performing model, applying inverse transformations to return images to their native space.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.3">UBHM.</span>
This team, consisting of Dana Dascalaescu from the University of Bucharest, Iuliana Georgescu from Helmholtz Munich, and Radu Tudor Ionescu from the University of Bucharest, employed the UNet++ architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib14" title="">14</a>]</cite> to detect MS lesions, experimenting with different backbones. They used a combination of binary cross-entropy and Dice loss functions for training. In their two-stage approach, they first trained a classifier to distinguish slices with lesions and then applied a UNet++ model only to those slices. They also experimented with a patch-level approach, dividing slices into 2x2 parts and training separate models for each part. Finally, they created ensembles of the different models using a soft voting scheme to boost performance.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.4">UGIVIA-UIB.</span>
This team, composed of Aina Maria Tur Serrano, Gabriel Moyà Alcover, and Francisco José Perales López, all from the University of the Balearic Islands, focused on using only the FLAIR sequences, resizing 2D slices through cropping and padding. They employed a modified 2D U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib15" title="">15</a>]</cite>, adjusting it to handle the resized input. The model consists of an encoder-decoder structure with a total of 19 convolutional layers. Data augmentation techniques, such as rotation, scaling, and shearing, were applied to increase the training set by generating 4000 new images. The final segmentation masks were produced by resizing the predictions back to their original size.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.5">MadSeg.</span>
The team, composed of Liang Shang, Zhengyang Lou, William Sethares, Vivek Prabhakaran, Veena Nair, Nagesh Adluru, all from the University of Wisconsin-Madison, employed two labeling strategies to enhance model training: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL). MSL classifies lesions into small, medium, and large categories based on their volume, while DBL labels lesion voxels according to their distance from non-lesion areas. They trained models using 5-fold cross-validation with various training schemes: the default scheme with nnU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite> and a combination of Dice+CrossEntropy loss, a focal scheme with Dice+Focal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib16" title="">16</a>]</cite> loss, an MSCSA scheme integrating the attention-based MSCSA module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib17" title="">17</a>]</cite>, and two variants with Res UNet and Mamba-based LightM-UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib18" title="">18</a>]</cite>. For postprocessing, they created ensembles of predictions for each labeling strategy, combining results and applying linear interpolation for small lesions and relying on the DBL ensemble for large lesions. Finally, they filtered out small lesions based on a probability threshold to obtain the final prediction results.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.6">BrainS.</span>
The team, composed of Edoardo Coppola, Mattia Savardi, Alberto Signoroni, and Sergio Benini, all from the University of Brescia, used the nnU-Net self-configuring framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite> for segmentation. Initially, they conducted a 5-fold cross-validation using only FLAIR images, followed by another 5-fold cross-validation using T1-w, T2-w, and FLAIR images. The multi-modality approach showed improved performance wrt single-modality approach. After confirming the benefit of using multiple modalities, they performed a statistical analysis on the training and test sets to check for covariate shifts, which indicated no significant distribution shift. Finally, they involved an experienced neuroradiologist to evaluate the model results and selected the best-performing model with a good precision-recall balance for the final evaluation.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.7">BeckLab.</span>
The team, composed of Francesco La Rosa, Emma Dereskewicz, and Erin S. Beck, all from the Icahn School of Medicine at Mount Sinai, trained two nnU-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite> 3D models with a 5-fold cross-validation on the training dataset, using the default hyperparameter configuration. The first model used only FLAIR images, while the second model used T1w, T2w, and FLAIR images. After visual inspection of the validation masks, they found that combining the outputs from both models enhanced MS lesion detection and improved overall lesion segmentation. This combined mask was used as the final output of their method.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.8">M3S: MeetMIALMS.</span>
The team, composed of Pedro M. Gordaliza from the CIBM Center for Biomedical Imaging, University of Lausanne and Lausanne University Hospital, Federico Spagnolo from the Translational Imaging in Neurology, Maxence Wynen from the Catholic University of Louvain, Nataliia Molchanova from the University of Lausanne and Lausanne University Hospital, University of Applied Sciences of Western Switzerland, CIBM Center for Biomedical Imaging, and Meritxell Bach Cuadra from the CIBM Center for Biomedical Imaging, University of Lausanne and Lausanne University Hospital, employed the nnU-Net framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite>, utilizing both U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib15" title="">15</a>]</cite> and the U-Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib19" title="">19</a>]</cite> architectures. The nnU-Net automatically configures network architecture, training, and testing pipelines for a given dataset, featuring a five-level 3D CNN, extensive data augmentation, and model ensembling to enhance generalization. The U-Mamba integrates CNN and State Space Sequence Models (SSMs) to combine local feature extraction with long-range dependency capture. They explored three models: baseline nnU-Net, nnUNetUMambaBot (with the U-Mamba blocks at the network bottleneck), and nnUNetUMambaEnc (with the U-Mamba blocks throughout the entire architecture), using FLAIR images alone and in combination with T1-w and T2-w images. T1-w and T2-w images were registered to FLAIR images using ANTS. Six models were trained, and the ensemble achieving the highest DSC on a patient-wise basis was selected. The nnUNet-FLAIR model demonstrated the best performance with a DSC of 0.773 and was used for the test dataset.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.9">Student.</span>
The team, composed of Kwaku Agyapong from the Université Côte d’Azur, used T2-w and FLAIR modalities from the MRI scans, extracting 2D axial views for training and validation. A U-Net architecture with EfficientNet-b0 as the backbone was employed. They used weighted binary cross-entropy loss and the DSC for evaluation. Data augmentation and callbacks like learning rate reduction and early stopping were implemented to improve performance. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.10">MMS.</span>
The team, composed of Aswathi Varma, Benedikt Wiestler, and Daniel Scholz, all from the TU Munich, utilized FLAIR and T2-w images for their segmentation approach, leveraging the nnU-Net framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite> enhanced by a novel augmentation technique called Global Intensity Non-linear (GIN) augmentations. GIN involves using randomly initialized convolution layers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib20" title="">20</a>]</cite> and non-linear functions like ReLU to generate diverse appearances for training images while preserving anatomical structures. This method helps the network focus on shape invariant information. For each training image, ten GIN-augmented views were created and used along with the original images to train the nnU-Net segmentation network, employing a 5-fold cross-validation with a 3D full-resolution configuration.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.11">BronTeam.</span>
The team, composed of Lehel Dénes-Fazakas, Barbara Simon, Adam Hartveg, László Szilágyi, all from Obuda University, adapted a U-Net network for the competition, consisting of four encoder and four decoder blocks with a bridge in between. Each block performed 2+1 dimensional convolutions, using 3D convolution operations. Training involved the Adam optimizer, using a composite loss function combining Categorical Cross Entropy and Dice loss.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.12">INTELLIGO Labs.</span>
The team, composed of Federico Cunico, Giorgio Dolci, Ilaria Boscolo Galazzo, Gloria Menegaz, and Marco Cristani, all from the University of Verona, implemented a 3D U-Net model with attention gates on residual connections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib21" title="">21</a>]</cite> processing only the FLAIR scan. The architecture included 3 encoder layers, a bottleneck layer, and 3 decoder layers, expanding from a 64-dimensional space in the encoder to a 256-dimensional space in the bottleneck before decoding to the final output. The weights were initialized using the Kaiming normal distribution, and the model was trained using reduced precision (16-bit) to manage memory limitations, allowing a larger batch size. A grid search was conducted to find the optimal seed for weight initialization.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.13">LA2I2F.</span>
The team, composed of Lauro Snidaro, Federico Urli, Ehsan Rassekh and Michele Somero, all from the University of Udine, proposed a 3D U-Net model, known as 3dSeUnet, designed for volumetric data segmentation. The architecture includes an encoder with convolutional blocks and squeeze-and-excitation (ssCE3d) layers, a bottleneck for processing downsampled features, and a decoder with transposed convolutions and skip connections. The custom ssCE3d layer enhances feature recalibration using global average pooling and a multi-layer perceptron (MLP). Their custom BasnetLoss combines binary cross-entropy (BCE), structural similarity index (SSIM), and intersection over union (IoU) losses to guide training. They used the RMSprop optimizer with a learning rate of 0.0001, aiming for accurate and perceptually similar segmentations with high overlap with the ground truth.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.14">Golestan.</span>
The team, consisting of Majid Ziaratban from Golestan University, proposed a segmentation approach consisting of three main steps: preprocessing, segmentation, and post-processing. In preprocessing, each slice of an MRI from the training set was extracted and saved as a single image, generating a new dataset of over 126,000 images. Twenty combinations were created for each image with potential lesions. For segmentation, they used a 2D U-Net-based network. During post-processing, the segmentation results of the 20 combinations per slice were combined, and all segmented slices were assembled to produce the final mask.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.15">AdasLab.</span>
The team, composed of Marcos Díaz-Hurtado from the Universitat Oberta de Catalunya, Ferran Prados Carrasco from the Universitat Oberta de Catalunya, Jordi Casas Roma from the Universidad Autónoma de Barcelona, Eloy Martínez-Heras from the Neuroimmunology and Multiple Sclerosis Unit, Laboratory of Advanced Imaging in Neuroimmunological Diseases, Hospital Clinic Barcelona, Institut d’Investigacions Biomèdiques August Pi i Sunyer and Universitat de Barcelona, employed a method based on the nnU-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#bib.bib13" title="">13</a>]</cite>. This approach handles both cross-sectional and longitudinal imaging data with only FLAIR images. They also employ a Monte Carlo strategy to generate synthetic lesion masks, simulating different lesion scenarios by applying changes like erosion, removal, and dilation. This method creates a robust training set, improving the model’s generalization and reliability.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Competition Results</h2>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Final leaderboard, displaying the results obtained by each participating team in the MS lesion segmentation competition. The columns include the following information: the rank position of each team (column <span class="ltx_text ltx_font_italic" id="S5.T2.7.1">Ranks</span>), the team name (column <span class="ltx_text ltx_font_italic" id="S5.T2.8.2">Team Name</span>), the organization they represent (column <span class="ltx_text ltx_font_italic" id="S5.T2.9.3">Organization</span>), a summary of the methods they employed (column <span class="ltx_text ltx_font_italic" id="S5.T2.10.4">Methods</span>), the imaging modalities used (column <span class="ltx_text ltx_font_italic" id="S5.T2.11.5">Modalities</span>), and the mean DSC achieved on the test set (column <span class="ltx_text ltx_font_italic" id="S5.T2.12.6">Mean DSC</span>). This comprehensive overview highlights the diversity of approaches and the performance metrics achieved in the competition.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.13" style="width:487.7pt;height:613pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T2.13.1"><span class="ltx_text" id="S5.T2.13.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.13.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T2.13.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.1">Rank</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.2">Team Name</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.3">Organization</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.4">Methods</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.5">Modalities</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.1.1.6">Mean DSC</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.1">1</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.2">MadSeg</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.3">University of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.4">Ensembles of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.2.2.6">0.7146</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.3.3">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.3.3.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.3.3.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.3.3.3">Wisconsin-Madison</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.3.3.4">labeling strategies</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.3.3.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.3.3.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.4.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.1">2</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.2">BrainS</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.3">University of Brescia</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.4">Statistical analysis</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.4.4.6">0.7083</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.5.5">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.5.5.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.5.5.2"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.5.5.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.5.5.4">and nnU-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.5.5.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.5.5.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.6.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.1">3</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.2">M3S:</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.3">CIBM Center for</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.4">Modified nnU-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.6.6.6">0.7079</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.7.7">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.7.7.1"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.7.7.2">MeetMIALMS</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.7.7.3">Biomedical Imaging,</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.7.7.4">labeling strategies</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.7.7.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.7.7.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.8.8">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.8.8.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.8.8.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.8.8.3">Switzerland</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.8.8.4"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.8.8.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.8.8.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.9.9">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.1">4</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.2">AdasLab</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.3">Universitat</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.4">augmentation technique</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.5">FLAIR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.9.9.6">0.6974</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.10.10">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.10.10.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.10.10.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.10.10.3">Oberta de Catalunya</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.10.10.4">and nnU-Net</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.10.10.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.10.10.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.11.11">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.1">5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.2">MMS</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.3">TU Munich</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.4">augmentation technique</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.5">FLAIR, T2-w</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.11.11.6">0.6859</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.12.12">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.12.12.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.12.12.2"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.12.12.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.12.12.4">and nnU-Net</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.12.12.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.12.12.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.13.13">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.1">6</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.2">LST</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.3">Faculty of ,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.4">ANTsPy preprocessing,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.13.13.6">0.6783</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.14.14">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.14.14.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.14.14.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.14.14.3">Electrical Engineering</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.14.14.4">nnU-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.14.14.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.14.14.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.15.15">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.15.15.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.15.15.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.15.15.3">University of Ljubljana</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.15.15.4"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.15.15.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.15.15.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.16.16">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.1">7</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.2">BeckLab</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.3">Icahn School of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.4">nnU-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.16.16.6">0.6754</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.17.17">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.17.17.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.17.17.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.17.17.3">Medicine at Mount Sinai</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.17.17.4"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.17.17.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.17.17.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.18.18">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.1">8</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.2">MSUniCaTeam</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.3">University of Cagliari</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.4">Ensemble of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.18.18.6">0.6508</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.19.19">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.19.19.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.19.19.2"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.19.19.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.19.19.4">Swin-UNETR, UNETR,</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.19.19.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.19.19.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.20.20">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.20.20.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.20.20.2"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.20.20.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.20.20.4">SegResNet</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.20.20.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.20.20.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.21.21">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.1">9</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.2">Golestan</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.3">Golestan university</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.4">2D U-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.5">NA</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.21.21.6">0.6503</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.22.22">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.1">10</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.2">LA2I2F</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.3">University of Udine</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.4">3D U-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.5">FLAIR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.22.22.6">0.6446</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.23.23">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.23.23.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.23.23.2"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.23.23.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.23.23.4">with custom loss</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.23.23.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.23.23.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.24.24">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.1">11</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.2">UBHM</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.3">University of Bucharest,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.4">UNet++,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.5">FLAIR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.24.24.6">0.6357</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.25.25">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.25.25.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.25.25.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.25.25.3">Helmholtz Munich</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.25.25.4">ensembles of models</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.25.25.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.25.25.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.26.26">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.1">12</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.2">UGIVIA-UIB</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.3">University of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.4">2D U-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.5">FLAIR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.26.26.6">0.6101</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.27.27">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.27.27.1"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.27.27.2">Team</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.27.27.3">the Balearic Islands</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.27.27.4"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.27.27.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.27.27.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.28.28">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.1">13</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.2">BronTeam</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.3">John von Neumann</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.4">3D U-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.5">FLAIR, T2-w,</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.28.28.6">0.5683</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.29.29">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.29.29.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.29.29.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.29.29.3">Faculty of Informatics,</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.29.29.4"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.29.29.5">T1-w</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.29.29.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.30.30">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.30.30.1"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.30.30.2"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.30.30.3">Hungary</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.30.30.4"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.30.30.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.30.30.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.31.31">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.1">14</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.2">INTELLIGO</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.3">University of Verona</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.4">3D U-Net</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.5">FLAIR</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.31.31.6">0.5471</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.32.32">
<span class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.32.32.1"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.32.32.2">Labs</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.32.32.3"></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.13.1.1.1.32.32.4">with attention gate</span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.32.32.5"></span>
<span class="ltx_td ltx_border_r" id="S5.T2.13.1.1.1.32.32.6"></span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.33.33">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.1">15</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.2">Student</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.3">University of</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.4">EfficientNet-b0</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.5">FLAIR, T2-w</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.13.1.1.1.33.33.6">0.4985</span></span>
<span class="ltx_tr" id="S5.T2.13.1.1.1.34.34">
<span class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T2.13.1.1.1.34.34.1"></span>
<span class="ltx_td ltx_border_b ltx_border_r" id="S5.T2.13.1.1.1.34.34.2"></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.13.1.1.1.34.34.3">Cote d’Azur</span>
<span class="ltx_td ltx_border_b ltx_border_r" id="S5.T2.13.1.1.1.34.34.4"></span>
<span class="ltx_td ltx_border_b ltx_border_r" id="S5.T2.13.1.1.1.34.34.5"></span>
<span class="ltx_td ltx_border_b ltx_border_r" id="S5.T2.13.1.1.1.34.34.6"></span></span>
</span>
</span></span></p>
</span></div>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.5">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07924v1#S5.T2" title="Table 2 ‣ 5 Competition Results ‣ ICPR 2024 Competition on Multiple Sclerosis Lesion Segmentation - Methods and Results"><span class="ltx_text ltx_ref_tag">2</span></a> reports the final leaderboard. Each team was allowed to make up to three submissions, but the table displays only the result of the last submission for each team. Submissions are ranked according to the mean DSC metric. The online leaderboard can be viewed on the competition website.
Looking at the results in the table, we can see that the team <span class="ltx_text ltx_font_italic" id="S5.p1.5.1">MadSeg</span>, with all members from the University of Wisconsin-Madison, achieved the best performance. Their approach involved multi-size and distance-based labeling strategies for data augmentation. They trained several models, each with different loss functions and architectures. The predictions from these models were ensembled and post-processed to enhance segmentation results. This sophisticated strategy led them to attain a DSC of <math alttext="0.7146" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">0.7146</mn><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><cn id="S5.p1.1.m1.1.1.cmml" type="float" xref="S5.p1.1.m1.1.1">0.7146</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">0.7146</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">0.7146</annotation></semantics></math> on the test set, which is <math alttext="0.0063" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">0.0063</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn id="S5.p1.2.m2.1.1.cmml" type="float" xref="S5.p1.2.m2.1.1">0.0063</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">0.0063</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">0.0063</annotation></semantics></math> points above <span class="ltx_text ltx_font_italic" id="S5.p1.5.2">BrainS</span>, the second-place team.
<span class="ltx_text ltx_font_italic" id="S5.p1.5.3">BrainS</span>, with all members from the University of Brescia, utilized a method based on the nnU-Net framework, performing 5-fold cross-validation with both single-modality (FLAIR) and multi-modality (T1w, T2w, FLAIR) approaches. Their method confirmed the benefits of multi-modality through statistical analysis, and they selected the best model based on neuroradiologist evaluation and precision-recall balance, achieving a DSC of <math alttext="0.7083" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mn id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">0.7083</mn><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><cn id="S5.p1.3.m3.1.1.cmml" type="float" xref="S5.p1.3.m3.1.1">0.7083</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">0.7083</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">0.7083</annotation></semantics></math>.
The third-place team, <span class="ltx_text ltx_font_italic" id="S5.p1.5.4">M3S: MeetMIALMS</span>, with members from Catholic University of Louvain, Translational Imaging in Neurology, University of Lausanne and Lausanne University Hospital, University of Applied Sciences of Western Switzerland, and CIBM Center for Biomedical Imaging, used an ensemble of networks based on customized nnU-Net architectures, leveraging FLAIR modality alone and in combination with T1-w and T2-w modalities. They experimented with various model configurations and MRI sequences, ultimately selecting nnUNet-FLAIR, which achieved the highest DSC for their validation set. <span class="ltx_text ltx_font_italic" id="S5.p1.5.5">M3S: MeetMIALMS</span> secured a DSC of <math alttext="0.7079" class="ltx_Math" display="inline" id="S5.p1.4.m4.1"><semantics id="S5.p1.4.m4.1a"><mn id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml">0.7079</mn><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><cn id="S5.p1.4.m4.1.1.cmml" type="float" xref="S5.p1.4.m4.1.1">0.7079</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">0.7079</annotation><annotation encoding="application/x-llamapun" id="S5.p1.4.m4.1d">0.7079</annotation></semantics></math>, just <math alttext="0.0004" class="ltx_Math" display="inline" id="S5.p1.5.m5.1"><semantics id="S5.p1.5.m5.1a"><mn id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml">0.0004</mn><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><cn id="S5.p1.5.m5.1.1.cmml" type="float" xref="S5.p1.5.m5.1.1">0.0004</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">0.0004</annotation><annotation encoding="application/x-llamapun" id="S5.p1.5.m5.1d">0.0004</annotation></semantics></math> points behind <span class="ltx_text ltx_font_italic" id="S5.p1.5.6">BrainS</span>, demonstrating the competitiveness of the top-performing teams. A closer examination of the results reveals that the most challenging lesions for segmentation were typically those with smaller sizes and those located near the ventricles. These lesions often have lower contrast against surrounding brain tissues, making them difficult to distinguish. Additionally, periventricular lesions, i.e. adjacent to the ventricles, are particularly challenging due to their proximity to areas with high-intensity signals such as the cerebrospinal fluid in FLAIR sequences, which can obscure lesion boundaries.
Another common source of segmentation errors involved hyperintensities in the white matter, visible in FLAIR images, which can be incorrectly identified as MS lesions. These hyperintensities, which may result from aging or other neurological conditions, frequently mimic the signal characteristics of MS lesions. The presence of these non-MS hyperintensities, especially in models that use only FLAIR images, poses a significant challenge, as they can lead to false-positive segmentations.
A key strength of the dataset used in this competition is the inclusion of T1-w and T2-w scans, which provide complementary information to confirm the presence and location of MS lesions. These additional modalities help distinguish true MS lesions from other high-intensity areas or non-lesional hyperintensities. This is reflected in the approaches of the three top-performing teams, all of which utilized all three available MRI modalities (FLAIR, T1-w, and T2-w) to achieve more accurate lesion segmentation. By leveraging the combined information from multiple sequences, these models were better able to mitigate the challenges posed by small lesions and periventricular regions, and differentiate true lesions from other hyperintensities.
Interestingly, models that incorporated more advanced techniques, such as synthetic data augmentation, ensemble learning, and innovative labeling methods, further enhanced their ability to differentiate true MS lesions from challenging cases. However, even with these techniques, the consistent segmentation of small lesions and lesions near high-intensity regions remained difficult, highlighting areas for potential future improvement. From an organizational standpoint, the competition proceeded smoothly without significant issues or unexpected difficulties. The submission process, which involved submitting a repository containing the segmentation masks generated by the proposed solutions on the test set, worked well. We plan to organize the competition again next year, but this time with a larger dataset and additional tasks for efficient MRI-based MS lesion analysis.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper summarizes the results of the successful submissions for the first competition on multiple sclerosis lesion segmentation. The outcomes of this competition demonstrate that deep learning algorithms can achieve promising results in MS lesion segmentation. However, these algorithms still need refinement to improve the detection of small and challenging lesions. For broader clinical adoption, model interpretability is crucial. Models that provide insights into their decision-making processes, such as via saliency maps or attention mechanisms, are more likely to be trusted in clinical environments. Future iterations of the MSLesSeg competition should emphasize interpretability alongside segmentation accuracy to facilitate the integration of these advanced models into clinical practice, ultimately enhancing patient care and treatment management. Furthermore, scalability is key for clinical implementation. The top-performing models, using techniques like ensemble learning and synthetic data augmentation, require significant computational resources. Nonetheless, ongoing advancements in hardware and optimization techniques can help mitigate these demands. Future research should focus on balancing accuracy and efficiency to ensure applicability across diverse clinical settings. In conclusion, this competition highlights new opportunities and challenges in the analysis of MS lesions from MRI scans. We anticipate that in the near future, deep learning algorithms will further demonstrate their potential to enhance the clinical management of MS and optimize treatment strategies for patients worldwide. Moreover, with the release of our public dataset, we expect a surge in research activity in this critical area, driving further advancements and innovations.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S6.SS0.SSS1.p1">
<p class="ltx_p" id="S6.SS0.SSS1.p1.1">Alessia Rondinella is a PhD candidate enrolled in the National PhD in Artificial Intelligence, XXXVII cycle, course on Health and life sciences, organized by Università Campus Bio-Medico di Roma. Francesco Guarnera is funded by the PNRR MUR project PE0000013-FAIR.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Lassmann, H., Multiple Sclerosis Pathology, Cold Spring Harbor perspectives in medicine 8 (3) (2018) a028936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Filippi, M., Bar-Or, A., Piehl, F., Preziosa, P., Solari, A., Vukusic, S., Rocca, M. A., Multiple sclerosis, Nature Reviews Disease Primers 4 (43) (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Molyneux, P., Miller, D., Filippi, M., Yousry, T., Radü, E., Adèr, H., Barkhof, F., Visual analysis of serial T2-weighted MRI in multiple sclerosis: intra-and interobserver reproducibility, Neuroradiology 41 (12) (1999) 882–888.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shoeibi, A., Khodatars, M., Jafari, M., Moridian, P., Rezaei, M., Alizadehsani, R., Khozeimeh, F., Gorriz, J. M., Heras, J. Panahiazar, M., and others, Applications of deep learning techniques for automated multiple sclerosis detection using magnetic resonance imaging: A review, Computers in Biology and Medicine 136 (2021) 104697.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Rondinella, A., Crispino, E., Guarnera, F., Giudice, O., Ortis, A., Russo, G., Di Lorenzo, C., Maimone, D., Pappalardo, F., Battiato, S., "Boosting multiple sclerosis lesion segmentation through attention mechanism." Computers in Biology and Medicine 161 (2023): 107021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Rondinella, A., Guarnera, F., Giudice, O., Ortis, A., Russo, G., Crispino, E., Pappalardo, F., Battiato, S., "Enhancing Multiple Sclerosis Lesion Segmentation in Multimodal MRI Scans with Diffusion Models." 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jenkinson, M., Bannister, P., Brady, J. M. and Smith, S. M., Improved Optimisation for the Robust and Accurate Linear Registration and Motion Correction of Brain Images. NeuroImage, 17(2), 825-841, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Smith, S.M., Fast robust automated brain extraction. Human Brain Mapping, 17(3):143-155, November 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dice, L. R., Measures of the Amount of Ecologic Association Between Species, Ecology 26 (3) (1945) 297–302.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, Holger R., Xu, D., "Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images." International MICCAI brainlesion workshop. Cham: Springer International Publishing, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H. R., Xu, D., "Unetr: Transformers for 3d medical image segmentation." Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Myronenko, A., "3D MRI brain tumor segmentation using autoencoder regularization." Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4. Springer International Publishing, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P. F., Kohl, S. A. A., Petersen, J., Maier-Hein, K. H., "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nature methods 18.2 (2021): 203-211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Zhou, Z., Rahman S. M. M., Tajbakhsh, N., Liang, J., "Unet++: A nested u-net architecture for medical image segmentation." Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4. Springer International Publishing, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ronneberger, O., Philipp F., Thomas B., "U-net: Convolutional networks for biomedical image segmentation." Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings. Springer International Publishing, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Lin, T., Goyal, P., Girshick, R., He, K., Dollár, P., "Focal loss for dense object detection." Proceedings of the IEEE international conference on computer vision. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Shang, L., Liu, Y., Lou, Z., Quan, S., Adluru, N., Guan, B., Sethares, W. A., "Vision Backbone Enhancement via Multi-Stage Cross-Scale Attention." arXiv:2308.05872 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Liao, W., Zhu, Y., Wang, X., Pan, C., Wang, Y., Ma, L,. "Lightm-unet: Mamba assists in lightweight unet for medical image segmentation." arXiv preprint arXiv:2403.05246 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ma, J., Feifei L., Bo W., "U-mamba: Enhancing long-range dependency for biomedical image segmentation." arXiv preprint arXiv:2401.04722 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Choi, S., Das, D., Choi, S., Yang, S., Park, H., Yun, S., "Progressive random convolutions for single domain generalization." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N. Y., Kainz, B., and others. "Attention u-net: Learning where to look for the pancreas." arXiv preprint arXiv:1804.03999 (2018).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 13:43:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
