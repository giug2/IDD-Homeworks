<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.05967] CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark</title><meta property="og:description" content="Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. H…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.05967">

<!--Generated on Fri Jul  5 17:58:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CVQA: Culturally-diverse Multilingual 
<br class="ltx_break">Visual Question Answering Benchmark</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.1" class="ltx_text ltx_font_bold">David Romero<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup></span>
</span><span class="ltx_author_notes">Equal Contribution
<span class="ltx_contact ltx_role_affiliation">Core Authors (MBZUAI)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.1" class="ltx_text ltx_font_bold">Chenyang Lyu<sup id="id2.1.1.1" class="ltx_sup"><span id="id2.1.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Core Authors (MBZUAI)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text ltx_font_bold">Haryo Akbarianto Wibowo</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Core Authors (MBZUAI)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.id1" class="ltx_text ltx_font_bold">Teresa Lynn</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_text ltx_font_bold">Injy Hamed</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id6.1.id1" class="ltx_text ltx_font_bold">Aditya Nanda Kishore</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.1.id1" class="ltx_text ltx_font_bold">Aishik Mandal</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id8.1.id1" class="ltx_text ltx_font_bold">Alina Dragonetti</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id9.1.id1" class="ltx_text ltx_font_bold">Artem Abzaliev</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id10.1.id1" class="ltx_text ltx_font_bold">Atnafu Lambebo Tonja</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id11.1.id1" class="ltx_text ltx_font_bold">Bontu Fufa Balcha</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id12.1.id1" class="ltx_text ltx_font_bold">Chenxi Whitehouse</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id13.1.id1" class="ltx_text ltx_font_bold">Christian Salamea</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id14.1.id1" class="ltx_text ltx_font_bold">Dan John Velasco</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id15.1.id1" class="ltx_text ltx_font_bold">David Ifeoluwa Adelani</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id16.1.id1" class="ltx_text ltx_font_bold">David Le Meur</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id17.1.id1" class="ltx_text ltx_font_bold">Emilio Villa-Cueva</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id18.1.id1" class="ltx_text ltx_font_bold">Fajri Koto</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id19.1.id1" class="ltx_text ltx_font_bold">Fauzan Farooqui</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id20.1.id1" class="ltx_text ltx_font_bold">Frederico Belcavello</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id21.1.id1" class="ltx_text ltx_font_bold">Ganzorig Batnasan</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id22.1.id1" class="ltx_text ltx_font_bold">Gisela Vallejo</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id23.1.id1" class="ltx_text ltx_font_bold">Grainne Caulfield</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id24.1.id1" class="ltx_text ltx_font_bold">Guido Ivetta</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id25.1.id1" class="ltx_text ltx_font_bold">Haiyue Song</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id26.1.id1" class="ltx_text ltx_font_bold">Henok Biadglign Ademtew</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id27.1.id1" class="ltx_text ltx_font_bold">Hernán Maina</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id28.1.id1" class="ltx_text ltx_font_bold">Holy Lovenia</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id29.1.id1" class="ltx_text ltx_font_bold">Israel Abebe Azime</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id30.1.id1" class="ltx_text ltx_font_bold">Jan Christian Blaise Cruz</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id31.1.id1" class="ltx_text ltx_font_bold">Jay Gala</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id32.1.id1" class="ltx_text ltx_font_bold">Jiahui Geng</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id33.1.id1" class="ltx_text ltx_font_bold">Jesus-German Ortiz-Barajas</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id34.1.id1" class="ltx_text ltx_font_bold">Jinheon Baek</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id35.1.id1" class="ltx_text ltx_font_bold">Jocelyn Dunstan</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id36.1.id1" class="ltx_text ltx_font_bold">Laura Alonso Alemany</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id37.1.id1" class="ltx_text ltx_font_bold">Kumaranage Ravindu Yasas Nagasinghe</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id38.1.id1" class="ltx_text ltx_font_bold">Luciana Benotti</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id39.1.id1" class="ltx_text ltx_font_bold">Luis Fernando D’Haro</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id40.1.id1" class="ltx_text ltx_font_bold">Marcelo Viridiano</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id41.1.id1" class="ltx_text ltx_font_bold">Marcos Estecha-Garitagoitia</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id42.1.id1" class="ltx_text ltx_font_bold">Maria Camila Buitrago Cabrera</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id43.1.id1" class="ltx_text ltx_font_bold">Mario Rodríguez-Cantelar</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id44.1.id1" class="ltx_text ltx_font_bold">Mélanie Jouitteau</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id45.1.id1" class="ltx_text ltx_font_bold">Mihail Mihaylov</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id46.1.id1" class="ltx_text ltx_font_bold">Mohamed Fazli Mohamed Imam</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id47.1.id1" class="ltx_text ltx_font_bold">Muhammad Farid Adilazuarda</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id48.1.id1" class="ltx_text ltx_font_bold">Munkhjargal Gochoo</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id49.1.id1" class="ltx_text ltx_font_bold">Munkh-Erdene Otgonbold</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id50.1.id1" class="ltx_text ltx_font_bold">Naome Etori</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id51.1.id1" class="ltx_text ltx_font_bold">Olivier Niyomugisha</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id52.1.id1" class="ltx_text ltx_font_bold">Paula Mónica Silva</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id53.1.id1" class="ltx_text ltx_font_bold">Pranjal Chitale</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id54.1.id1" class="ltx_text ltx_font_bold">Raj Dabre</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id55.1.id1" class="ltx_text ltx_font_bold">Rendi Chevi</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id56.1.id1" class="ltx_text ltx_font_bold">Ruochen Zhang</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id57.1.id1" class="ltx_text ltx_font_bold">Ryandito Diandaru</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id58.1.id1" class="ltx_text ltx_font_bold">Samuel Cahyawijaya</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id59.1.id1" class="ltx_text ltx_font_bold">Santiago Góngora</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id60.1.id1" class="ltx_text ltx_font_bold">Soyeong Jeong</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id61.1.id1" class="ltx_text ltx_font_bold">Sukannya Purkayastha</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id62.1.id1" class="ltx_text ltx_font_bold">Tatsuki Kuribayashi</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id63.1.id1" class="ltx_text ltx_font_bold">Thanmay Jayakumar</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id64.1.id1" class="ltx_text ltx_font_bold">Tiago Timponi Torrent</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id65.1.id1" class="ltx_text ltx_font_bold">Toqeer Ehsan</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id66.1.id1" class="ltx_text ltx_font_bold">Vladimir Araujo</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id67.1.id1" class="ltx_text ltx_font_bold">Yova Kementchedjhieva</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id68.1.id1" class="ltx_text ltx_font_bold">Zara Burzo</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id69.1.id1" class="ltx_text ltx_font_bold">Zheng Wei Lim</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id70.1.id1" class="ltx_text ltx_font_bold">Zheng Xin Yong</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span id="id71.1.id1" class="ltx_text ltx_font_bold">Oana Ignat</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id72.1.id1" class="ltx_text ltx_font_bold">Joan Nwatu</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id73.1.id1" class="ltx_text ltx_font_bold">Rada Mihalcea</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id74.1.id1" class="ltx_text ltx_font_bold">Thamar Solorio</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Core Authors (MBZUAI)
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id75.1.id1" class="ltx_text ltx_font_bold">Alham Fikri Aji</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Core Authors (MBZUAI)
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id76.id1" class="ltx_p">Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/datasets/afaji/cvqa</span></span></span>, a new <span id="id76.id1.1" class="ltx_text ltx_font_bold">C</span>ulturally-diverse multilingual <span id="id76.id1.2" class="ltx_text ltx_font_bold">V</span>isual <span id="id76.id1.3" class="ltx_text ltx_font_bold">Q</span>uestion <span id="id76.id1.4" class="ltx_text ltx_font_bold">A</span>nswering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2406.05967/assets/cvqa_image.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We propose CVQA, a large-scale multilingual VQA benchmark, representing the cultures of 28 countries and 26 different languages across 10 diverse categories, comprising 9k samples.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is a task that requires AI systems to answer textual questions based on a given context image. VQA serves as an essential measure for assessing the understanding and reasoning capabilities of Multimodal Large Language Models (MLLMs) across diverse images and texts. With the rapid development of MLLMs, significant improvements have been observed, including support for multiple languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. However, there is still a lack of VQA benchmarks that capture a diverse set of languages and cultural contexts. Specifically, most VQA benchmarks only cover the English language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. While some work has been undertaken on multilingual VQA, it either covers a limited set of popular languages or is producing questions via translation/generation of text from the original Western-centric images, thus failing to capture cultural nuances inherent in different languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To address these limitations, we propose CVQA: a novel, large-scale, multilingual, culturally nuanced VQA benchmark that includes a diverse set of languages, including many that are underrepresented and understudied. CVQA follows the grassroots crowd-sourcing collaboration approaches taken by Masakhane for Africa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, NusaCrowd for Indonesia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and AI4Bharat for India <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In our case, however, we collaborate across communities, rather than within one particular community, in order to maximize cultural and linguistic representation. Consequently, our data consists of 9k questions across 28 countries, covering 26 languages. We also sub-categorize CVQA based on Country-Language pairs, resulting in 33 distinct pairs, which is substantially more extensive than existing VQA benchmarks. Furthermore, each sample in CVQA falls into one of 10 diverse categories (see Table <a href="#S2.T1" title="Table 1 ‣ Categories ‣ 2.1 Dataset Collection Design ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and is annotated and validated by fluent speakers and those familiar with the respective cultures, ensuring high quality and diversity. Lastly, CVQA is written in both English and local languages, enabling us to benchmark multilingual MLLMs and English-only MLLMs.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this study, we benchmark CVQA across various MLLMs and find that it presents a significant challenge for open MLLMs, which most of the time achieve no more than 50% accuracy. Additionally, we observe a notable degradation in model performance when questions are asked in native languages, particularly those in understudied languages such as Breton from France and Javanese from Indonesia, highlighting a significant gap in understanding multilingual prompts. We further conduct several ablation studies to analyze the models’ performance across different question categories, regions, languages, and image sources.
Our contributions can be summarised as follows:</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">First, we introduce CVQA, a new culturally diverse multilingual visual question answering dataset consisting of over 9,000 questions from across 28 countries and 26 languages.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Second, we provide extensive documentation on our process to crowdsource such large dataset across numerous communities, including annotation guidelines.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">Finally, we provide an initial set of evaluations on this benchmark, to serve as a baseline for future research on vision-language models that are culturally diverse.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We note that efforts to enhance cultural awareness in models are increasingly gaining attention. As such, our work contributes to the growing interest within the community and can encourage further initiatives to broaden the limited world view currently captured by MLLMs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>CVQA Data Collection</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">The construction of our CVQA dataset involved a detailed annotation process that aims at creating a culturally diverse and linguistically comprehensive dataset for Visual Question Answering. It is worth noting that, while defining culture is challenging, we follow <cite class="ltx_cite ltx_citemacro_citet">Adilazuarda et al. [<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> by using common-ground knowledge (e.g., information surrounding local dishes, history, places, etc. that is generally shared by the people within the region) as a proxy of culture. In this section, we now turn to outline the detailed procedures followed during the data collection and annotation phases.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset Collection Design</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Country-Language Pair Subset</h4>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">CVQA is a multilingual, multiple-choice locally-nuanced visual question-answering dataset. The format is similar to commonly used visual QA data such as VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, VQA-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Yet, in contrast to them, we gathered images and created question-answer pairs based on the cultures of various locations. Moreover,
for each location, the question-answer pairs were created in their respective local languages, along with parallel English translations. Some languages are shared across different locations (e.g., Mexico-Spanish vs Spain-Spanish), and vice-versa, different languages are shared across the same location (e.g., Indonesia-Indonesian vs Indonesia-Javanese). Therefore, to capture them, we group our CVQA dataset into several subsets based on this Country-Language pair, rather than simply on language or location only.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotators</h4>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">To elicit image collectors and annotation contributions to this project, we reached out to our network, which included both linguistic groups and NLP communities. Annotators needed to be fluent speakers of the language in question and be accustomed to the cultures of the locations for which they provided data. To promote data collection, contributors with significant contributions, either by contributing at least 100 validated question-answer pairs and/or managing several subsets, are rewarded as co-authors in this paper. The annotator demographic statistics can be seen in Figure <a href="#A4.F8" title="Figure 8 ‣ Appendix D CVQA Annotator Demographic ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, Appendix <a href="#A4" title="Appendix D CVQA Annotator Demographic ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>. Our annotators are predominantly native speakers, with around 89% residing in the respective country for over 16 years. The age group distribution shows a significant concentration in the 18-30 age bracket, with about one-third female representation. Overall, the demographic profile highlights diversity in terms of age, with high levels of cultural familiarity and language proficiency.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Categories</h4>

<figure id="S2.T1" class="ltx_table ltx_align_floatright">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Categories in our Dataset. To save space in some of our results, we might refer them by shorthand version in brackets.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:154.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,21.8pt) scale(0.779815872064185,0.779815872064185) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.1.2.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">1. Vehicles and Transportation (Vehicles)</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_nopad_r ltx_align_left">2. Cooking and Food (Food)</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.1.4.3.1" class="ltx_td ltx_nopad_r ltx_align_left">3. People and Everyday Life (People)</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_nopad_r ltx_align_left">4. Sports and Recreation (Sports)</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.1.6.5.1" class="ltx_td ltx_nopad_r ltx_align_left">5. Plants and Animals (Plants &amp; Animals)</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.1.7.6.1" class="ltx_td ltx_nopad_r ltx_align_left">6. Objects, Materials, and Clothing (Objects)</td>
</tr>
<tr id="S2.T1.1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.1.8.7.1" class="ltx_td ltx_nopad_r ltx_align_left">7. Brands and Products (Brands)</td>
</tr>
<tr id="S2.T1.1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1" class="ltx_td ltx_nopad_r ltx_align_left">8. Geography, Buildings, and Landmarks (Geography)</td>
</tr>
<tr id="S2.T1.1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.1.10.9.1" class="ltx_td ltx_nopad_r ltx_align_left">9. Tradition, Art, and History (Tradition)</td>
</tr>
<tr id="S2.T1.1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.1.11.10.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">10. Public Figure and Pop-Culture (Pop Culture)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">For the categorization of questions of our CVQA dataset, we incorporate 10 diverse categories to ensure a culturally-comprehensive representative set of visual questions, which are shown in Table <a href="#S2.T1" title="Table 1 ‣ Categories ‣ 2.1 Dataset Collection Design ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We mainly adopt the categorization from the OK-VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, with some modifications to fit the theme of our project. Specifically, the categories from the OK-VQA dataset used in our CVQA dataset are 1) to 7). We split the original category of <span id="S2.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">Geography, History, Language and Culture</span> into 2 separate categories of 8) and 9). In addition, we added a new category of 10) considering the effect that cultural icons and media have on everyday life.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Annotation Process</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">We developed concise annotation guidelines (in English) that are suitable for all Country-Language subset teams. Here we provide an overview of the key steps that annotators followed during the dataset creation process. The full guidelines are provided in Appendix <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:AnnoGuidelines</span>.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image Selection and Preparation</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">For each Country-Language pair, annotators were instructed to select images that depict diverse cultural aspects pertinent to their cultural backgrounds among one of the 10 categories. We did not enforce balance across categories considering the different variations of cultural knowledge. We strongly recommend that annotators use their own personal images to avoid accidental data leakage from existing online sources. However, we noted that this request was not always possible, since some images are extremely hard to come by (e.g., photos of public figures or landmarks that are far from the annotator’s location). Therefore, we also allowed them to use images from our pre-defined list of open-use licensing sources<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>common.wikimedia.org, Flickr, GapMinder, Unsplash, Pixabay</span></span></span>. For self-made images, we asked the annotators whether they were willing to make the image available for commercial or research purposes. For images from existing online sources, we applied the original license.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px1.p2.1" class="ltx_p">We requested annotators to avoid using sensitive images that would perpetuate stereotypes. In addition, the annotators were also requested to anonymize faces that were not public figures or fictional characters, as well as text that could reveal the answer to the accompanying questions.
We also post-processed all images to remove all metadata such as geo-location, device type, and so on.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.05967/assets/full-statistics.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="550" height="244" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Statistics of the CVQA Benchmark</figcaption>
</figure>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Question Creation</h4>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">The questions associated with each image had to be culturally relevant and formulated such that they would require the context of the image in order to be answerable. A maximum of three question-answer pairs could be provided for each image. Each question was accompanied by one correct answer and three distractors that were reasonably plausible, yet incorrect, thus forming a multiple-choice format.</p>
</div>
<div id="S2.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px2.p2.1" class="ltx_p">While we follow the existing VQA benchmarks in terms of using a multiple-choice format, we are also aware that multiple-choice has some flaws when used to measure a model’s performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Hence, we made sure that CVQA is also convertible into free-text open-ended QA, by instructing the annotators to ensure that the question would be answerable even without the accompanying multiple choices (i.e., not through a deductive method). Moreover, to accommodate the multilingual aspect of the benchmark, each question-answer pair was created in the local language and manually translated into English.</p>
</div>
<div id="S2.SS2.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px2.p3.1" class="ltx_p">Annotators were advised to create questions that promoted an understanding and appreciation of different cultures without perpetuating stereotypes. Typical questions ranged from simple identification queries (e.g., “What is the name of this food?”) to more complex ones involving multi-hop reasoning or local common-sense knowledge (e.g., “What is the color of the t-shirt the youngest member of this group is wearing?”).</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotation Examples and Training</h4>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">The annotation guidelines provided multiple examples of well-formulated questions and answers to help guide annotator efforts (See Appendix <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:AnnoGuidelines</span>). These examples helped clarify the level of specificity and cultural relevance expected in the annotations. We provided a tutorial to annotators on how to edit and blur sensitive information in the images.
To confirm understanding, we spot-checked the annotators’ collected data throughout the annotation period and informed them if some of their data did not follow the guidelines.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Validation</h4>

<div id="S2.SS2.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS0.Px4.p1.1" class="ltx_p">The last step in the CVQA data creation was the validation process. Each entry was validated by another annotator of the same Country-Language pair. The validators were instructed to ensure that each question followed the guidelines. Based on our spot-checking, common mistakes that we encouraged the validators to check were typos and grammatical mistakes, non-cultural questions, questions that could be answered without the image, as well as incorrectly-sourced images. More information on the annotation platform is provided in Appendix <a href="#A2" title="Appendix B Annotation Platform ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Statistics</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">To ensure sufficient question variation, we set the minimum number of questions to be included in CVQA to be at least 200 questions per Country-Language subset. In the end, we gathered 9,044 total questions across all subsets. Some statistics of our collected data are shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.3 Data Statistics ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our CVQA covers a diverse set of languages and locations spread across the globe. We also capture languages written in various scripts. While Latin is the dominant script (used in 20 Country-Language pairs), the remaining scripts are diverse; covering Arabic, Amharic, Bengali, Chinese, Cyrillic, Hangul, Japanese, Perso-Arabic, Sinhalese, and Tamil.
The Country-Language pairs and corresponding scripts are shown in Appendix <a href="#A5" title="Appendix E Country-Language Pairs and Scripts ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. CVQA covers several less commonly studied languages and regions, such as Ireland-Irish, Indonesia-Minangkabau, Indonesia-Javanese, France-Breton, Nigeria-Igbo and Mongolia-Mongolian.
</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Question distribution across the subset and categories are shown in Figure <a href="#S2.F2" title="Figure 2 ‣ Image Selection and Preparation ‣ 2.2 Annotation Process ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Whether the image is coming from an external or personal source varies depending on the subset. We also note that the category with the most personal images is <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Cooking and Food</span>, which we assume is due to the ease of obtaining such images. In contrast, the category with the least amount of personal images is <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">Public Figures and Pop Culture</span>, as it is less likely for people to have personal photos under this category.</p>
</div>
<figure id="S2.T2" class="ltx_table ltx_align_floatright">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>CVQA Data Statistics</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">No. of images</td>
<td id="S2.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">4,560</td>
</tr>
<tr id="S2.T2.1.2.2" class="ltx_tr">
<td id="S2.T2.1.2.2.1" class="ltx_td ltx_align_left">No. of questions</td>
<td id="S2.T2.1.2.2.2" class="ltx_td ltx_align_right">9,044</td>
</tr>
<tr id="S2.T2.1.3.3" class="ltx_tr">
<td id="S2.T2.1.3.3.1" class="ltx_td ltx_align_left">No. of countries</td>
<td id="S2.T2.1.3.3.2" class="ltx_td ltx_align_right">28</td>
</tr>
<tr id="S2.T2.1.4.4" class="ltx_tr">
<td id="S2.T2.1.4.4.1" class="ltx_td ltx_align_left">No. of languages</td>
<td id="S2.T2.1.4.4.2" class="ltx_td ltx_align_right">26</td>
</tr>
<tr id="S2.T2.1.5.5" class="ltx_tr">
<td id="S2.T2.1.5.5.1" class="ltx_td ltx_align_left">No. of Country-Language pairs</td>
<td id="S2.T2.1.5.5.2" class="ltx_td ltx_align_right">33</td>
</tr>
<tr id="S2.T2.1.6.6" class="ltx_tr">
<td id="S2.T2.1.6.6.1" class="ltx_td ltx_align_left">Avg. question per image</td>
<td id="S2.T2.1.6.6.2" class="ltx_td ltx_align_right">1.98</td>
</tr>
<tr id="S2.T2.1.7.7" class="ltx_tr">
<td id="S2.T2.1.7.7.1" class="ltx_td ltx_align_left">Avg. words per question</td>
<td id="S2.T2.1.7.7.2" class="ltx_td ltx_align_right">10.2</td>
</tr>
<tr id="S2.T2.1.8.8" class="ltx_tr">
<td id="S2.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_border_bb">Avg. words per option</td>
<td id="S2.T2.1.8.8.2" class="ltx_td ltx_align_right ltx_border_bb">1.90</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">To investigate the question variations, we categorize each question into question types of “what”, “how”, “why”, “where”, “who”, and “which” questions. We categorize the questions by simple string-matching performed on the English questions.
While not perfect, we argue that this method should be able to capture the trend of the questions. As shown in Figure <a href="#S2.F2" title="Figure 2 ‣ Image Selection and Preparation ‣ 2.2 Annotation Process ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the majority of the questions fall into “what” questions. Question distribution across different Country-Language pairs varies, with an interesting finding that India-Bengali has a lot of “how” questions. Across categories, perhaps unsurprisingly, the <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">Geography and Landmark</span> category has noticeably more “where” and “which” (e.g., in which city) questions, whereas the <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">Public Figure and Pop Culture</span> category has more “who” questions. By looking at the most frequently used words (Figure <a href="#A3.F7" title="Figure 7 ‣ Appendix C Most-Frequent Words in the Questions ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) across each category, we can see the general theme of the types of questions being asked. For example, questions in the <span id="S2.SS3.p3.1.3" class="ltx_text ltx_font_italic">Cooking and Food</span> category often enquire about dish names, ingredients, or tastes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Models.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">To evaluate performance on our CVQA benchmark, we select a range of multimodal vision-language models with multilingual and monolingual English-only capabilities. For monolingual English-only models, we test CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> a contrastive-learning-based model, trained with approximately 400 million images and English-only text pairs from the web, where we use its <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">vit-large-patch14-336</span> version. We also use InstructBLIP(4.1B)  <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, an English-only instruction-aware vision model based on BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, trained with 13 held-in datasets covering different tasks in English. For multilingual models, we evaluate LLaVA-1.5 (7B) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> based on Llama-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, and mBLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> a BLIP-2 based model that covers 96 languages (where we evaluate two model variations, mBLIP mT0-XL (4.9B) and mBLIP BLOOMZ (8.3B)). Lastly, we employ M-CLIP <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> a multilingual CLIP-based model that supports 68 languages, where we use its <span id="S3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">XLM-Roberta-Large-Vit-B-32</span> version. We also evaluate the most advanced closed-source MLLMs, such as GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and Gemini-1.5-Flash <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Framework.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">We perform a zero-shot evaluation with two types of prompts, as follows: a location-aware prompt, which specifies the country, the question, and the options, (e.g., <span id="S3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">“Location: </span>{<span id="S3.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">country</span>}<span id="S3.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">. Question: </span>{<span id="S3.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">question</span>}<span id="S3.SS0.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_italic"> Options: </span>{<span id="S3.SS0.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_italic">options</span>}<span id="S3.SS0.SSS0.Px2.p1.1.7" class="ltx_text ltx_font_italic"> Short Answer:”</span>); and a location-agnostic prompt, which follows the same template but does not specify the country in the prompt (e.g., <span id="S3.SS0.SSS0.Px2.p1.1.8" class="ltx_text ltx_font_italic">“Question: </span>{<span id="S3.SS0.SSS0.Px2.p1.1.9" class="ltx_text ltx_font_italic">question</span>}<span id="S3.SS0.SSS0.Px2.p1.1.10" class="ltx_text ltx_font_italic"> Options: </span>{<span id="S3.SS0.SSS0.Px2.p1.1.11" class="ltx_text ltx_font_italic">options</span>}<span id="S3.SS0.SSS0.Px2.p1.1.12" class="ltx_text ltx_font_italic"> Short Answer:”</span>). Additionally, due to the multilingual nature of CVQA, for each prompt, we evaluate using the English-only and local language question-option pairs. For the generative-based models, LLaVA, mBLIP and InstructBLIP, the image and the prompts are used as the input. The models then produce output probabilities and we treat the highest probability for the options (A,B,C,D) as the prediction (following MMLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>). On the other hand, for embedding-based models like CLIP and M-CLIP, we use the embedding-level similarity between the image and the combination of question and each answer candidate texts (<span id="S3.SS0.SSS0.Px2.p1.1.13" class="ltx_text ltx_font_italic">Question+Option-1</span>,…,<span id="S3.SS0.SSS0.Px2.p1.1.14" class="ltx_text ltx_font_italic">Question+Option-4</span>) to select the one with the highest similarity as the correct answer. We use accuracy to measure the performance, following the existing multiple-choice VQA tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we discuss the performance of existing MLLMs on the CVQA benchmark.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average performance of MLLMs on our CVQA dataset with English prompts (EN) and local language prompts (LOC).</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:47pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.0pt,3.9pt) scale(0.854328774553438,0.854328774553438) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">LLaVA-1.5-7B</span></td>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">M-CLIP</span></td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">CLIP</span></td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">mBLIP-mT0</span></td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">mBLIP-BLOOMZ</span></td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold">InstructBLIP</span></td>
<td id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Gemini-1.5-Flash</span></td>
<td id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.1.8.1" class="ltx_text ltx_font_bold">GPT-4o</span></td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.2.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.3.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.4.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.5.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.6.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.7.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.8.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.9.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.10" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.10.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.11" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.11.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.12" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.12.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.13" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.13.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.14" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.14.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T3.1.1.2.2.15" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.15.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T3.1.1.2.2.16" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.2.2.16.1" class="ltx_text ltx_font_bold">LOC</span></td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<td id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">48.9</td>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">36.5</td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">36.9</td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">33.4</td>
<td id="S4.T3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">42.3</td>
<td id="S4.T3.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.5</td>
<td id="S4.T3.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.5</td>
<td id="S4.T3.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.2</td>
<td id="S4.T3.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">39.4</td>
<td id="S4.T3.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">33.0</td>
<td id="S4.T3.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">47.8</td>
<td id="S4.T3.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">32.7</td>
<td id="S4.T3.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">67.1</td>
<td id="S4.T3.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">67.2</td>
<td id="S4.T3.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">79.1</td>
<td id="S4.T3.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">79.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>LLaVA-1.5-7B and InstructBLIP results on various VQA datasets, where the results on the other datasets are taken from <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:20.7pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-358.6pt,16.8pt) scale(0.376790380196748,0.376790380196748) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">VQAv2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">GQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">VizWiz</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">SciQA-IMG</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</th>
<th id="S4.T4.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1.6.1" class="ltx_text ltx_font_bold">TextVQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</th>
<th id="S4.T4.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.1.7.1" class="ltx_text ltx_font_bold">CVQA (EN)</span></th>
<th id="S4.T4.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.1.1.1.1.8.1" class="ltx_text ltx_font_bold">CVQA (LOC)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LLaVA-1.5-7B</th>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">78.5</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">62.0</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">50.0</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">66.8</td>
<td id="S4.T4.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S4.T4.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">48.9</td>
<td id="S4.T4.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">36.5</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">InstructBLIP</th>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">49.2</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">34.5</td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">60.5</td>
<td id="S4.T4.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb">50.1</td>
<td id="S4.T4.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_bb">47.8</td>
<td id="S4.T4.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_bb">32.7</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Main Results</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">The overall performance on our CVQA dataset of various open and closed-source MLLMs are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Among open models, LLaVA-1.5-7B achieves the best performance, but still significantly lagging behind closed models by more than 10%. However, Table <a href="#S4.T4" title="Table 4 ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that LLaVA-1.5-7B indeed achieves better performance on other established English VQA benchmarks, highlighting that culturally-specific questions that we collect in CVQA are challenging even for the best-performing open model (LLaVA-1.5-7B). The performance is even worse when the question is asked in local languages, emphasizing the models’ lower capability in handling non-English prompts.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">The experimental results also highlight a substantial performance gap between open and closed-source MLLMs. Closed models like GPT-4o and Gemini-1.5-Flash demonstrate superior performance, with GPT-4o achieving the highest accuracy in both English (79.1%) and local language (79.4%) prompts. In contrast, open models like InstructBLIP and mBLIP-mT0 exhibit lower performance, particularly in local language prompts, indicating a need for more diverse training data and refined fine-tuning processes. While proprietary models show superior performance, it is hard to fully explain why, due to their closed nature. Additionally, their results are not reproducible. Therefore, we use open models in the rest of our experiments.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance per Country-Language.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">To see the capability of MLLMs in solving questions for each country and language, we report accuracy performance for Country-Language pairs in Figure <a href="#S4.F3" title="Figure 3 ‣ Performance per Country-Language. ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
From this, we observe that all models struggle with questions in local languages, demonstrating the challenges for current MLLMs. In other words, across all models, their performance drops in local language questions compared to their performance in English questions.
For instance, in the case of Brazil-Portuguese, LLaVA-1.5-7B achieved a score of 60.73% for English and 51.16% for Portuguese. Moreover, in Mongolia-Mongolian, all models struggled, with LLaVA-1.5-7B reaching only 40% for English and 27.62% for Mongolian, suggesting challenges in less resource-rich language environments. It is worth noting that, these multilingual MLLMs do not originally support some of the languages, which also explains the significant performance drop for those languages. In contrast, in languages that are more frequently studied in NLP and have more abundant training resources, the performance gap between English and local languages, such as Spanish, tends to be smaller <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2406.05967/assets/lang_count_results_eng.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="364" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Using English-only question-option pairs </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2406.05967/assets/lang_count_results_loc.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="362" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Using local-language question-option pairs</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model performance per Country-Language pair. The <span id="S4.F3.2.1" class="ltx_text" style="color:#00A2FF;">blue</span> lines indicate separation by continent. All models show similar behaviour in the majority of cases, despite having different sizes.</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance Across Categories.</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">We show the breakdown performances of models per category in Table <a href="#S4.T5" title="Table 5 ‣ Performance Across Categories. ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where the categories themselves are described in Section <a href="#S2.SS1" title="2.1 Dataset Collection Design ‣ 2 CVQA Data Collection ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. Note that the category
<span id="S4.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">People and Everyday Life</span>
consistently achieves the highest accuracy across most models, with LLaVA-1.5-7B obtaining 58.6% in English prompts. This can be possibly attributed to the extensive training data available for everyday human activity and interaction, which widely existed in many visual-related datasets.
Conversely, the <span id="S4.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">Cooking &amp; Food</span>
and <span id="S4.SS0.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">Pop Culture</span>
categories exhibit lower accuracy across models, especially in local language prompts. This demonstrates that the high diversity in food and pop culture across different cultures poses a great challenge for the generalization of MLLMs.
</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy of models across categories. Per category, the best performing models on English (EN) and local language (LOC) question-option pairs are bolded and underlined, respectively.</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:202.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.0pt,7.4pt) scale(0.931457394139503,0.931457394139503) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Categories</span></th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">LLaVA-1.5-7B</span></th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">M-CLIP</span></th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">CLIP</span></th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.5.1" class="ltx_text ltx_font_bold">mBLIP-mT0</span></th>
<th id="S4.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.6.1" class="ltx_text ltx_font_bold">mBLIP-BLOOMZ</span></th>
<th id="S4.T5.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T5.1.1.1.1.7.1" class="ltx_text ltx_font_bold">InstructBLIP</span></th>
</tr>
<tr id="S4.T5.1.1.2.2" class="ltx_tr">
<th id="S4.T5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.1.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.2.1" class="ltx_text ltx_font_bold">LOC</span></th>
<th id="S4.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.3.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.4.1" class="ltx_text ltx_font_bold">LOC</span></th>
<th id="S4.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.5.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.6.1" class="ltx_text ltx_font_bold">LOC</span></th>
<th id="S4.T5.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.7.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.8.1" class="ltx_text ltx_font_bold">LOC</span></th>
<th id="S4.T5.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.9.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.10.1" class="ltx_text ltx_font_bold">LOC</span></th>
<th id="S4.T5.1.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.11.1" class="ltx_text ltx_font_bold">EN</span></th>
<th id="S4.T5.1.1.2.2.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column"><span id="S4.T5.1.1.2.2.12.1" class="ltx_text ltx_font_bold">LOC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.3.1" class="ltx_tr">
<th id="S4.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Brands</th>
<td id="S4.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.3.1.2.1" class="ltx_text ltx_font_bold">47.7</span></td>
<td id="S4.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.3.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">36.5</span></td>
<td id="S4.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">37.8</td>
<td id="S4.T5.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">35.4</td>
<td id="S4.T5.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">40.1</td>
<td id="S4.T5.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">31.3</td>
<td id="S4.T5.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">33.5</td>
<td id="S4.T5.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">31.3</td>
<td id="S4.T5.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">40.3</td>
<td id="S4.T5.1.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t">36.5</td>
<td id="S4.T5.1.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t">46.9</td>
<td id="S4.T5.1.1.3.1.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">32.2</td>
</tr>
<tr id="S4.T5.1.1.4.2" class="ltx_tr">
<th id="S4.T5.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Food</th>
<td id="S4.T5.1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.2.2.1" class="ltx_text ltx_font_bold">45.9</span></td>
<td id="S4.T5.1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.2.3.1" class="ltx_text ltx_framed ltx_framed_underline">32.5</span></td>
<td id="S4.T5.1.1.4.2.4" class="ltx_td ltx_align_center">33.1</td>
<td id="S4.T5.1.1.4.2.5" class="ltx_td ltx_align_center">29.1</td>
<td id="S4.T5.1.1.4.2.6" class="ltx_td ltx_align_center">40.8</td>
<td id="S4.T5.1.1.4.2.7" class="ltx_td ltx_align_center">30.9</td>
<td id="S4.T5.1.1.4.2.8" class="ltx_td ltx_align_center">27.8</td>
<td id="S4.T5.1.1.4.2.9" class="ltx_td ltx_align_center">26.9</td>
<td id="S4.T5.1.1.4.2.10" class="ltx_td ltx_align_center">37.4</td>
<td id="S4.T5.1.1.4.2.11" class="ltx_td ltx_align_center">30.7</td>
<td id="S4.T5.1.1.4.2.12" class="ltx_td ltx_align_center">43.8</td>
<td id="S4.T5.1.1.4.2.13" class="ltx_td ltx_nopad_r ltx_align_center">32.0</td>
</tr>
<tr id="S4.T5.1.1.5.3" class="ltx_tr">
<th id="S4.T5.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Geography</th>
<td id="S4.T5.1.1.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.5.3.2.1" class="ltx_text ltx_font_bold">46.1</span></td>
<td id="S4.T5.1.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.5.3.3.1" class="ltx_text ltx_framed ltx_framed_underline">37.9</span></td>
<td id="S4.T5.1.1.5.3.4" class="ltx_td ltx_align_center">36.5</td>
<td id="S4.T5.1.1.5.3.5" class="ltx_td ltx_align_center">35.6</td>
<td id="S4.T5.1.1.5.3.6" class="ltx_td ltx_align_center">41.0</td>
<td id="S4.T5.1.1.5.3.7" class="ltx_td ltx_align_center">32.7</td>
<td id="S4.T5.1.1.5.3.8" class="ltx_td ltx_align_center">31.1</td>
<td id="S4.T5.1.1.5.3.9" class="ltx_td ltx_align_center">32.6</td>
<td id="S4.T5.1.1.5.3.10" class="ltx_td ltx_align_center">35.1</td>
<td id="S4.T5.1.1.5.3.11" class="ltx_td ltx_align_center">32.6</td>
<td id="S4.T5.1.1.5.3.12" class="ltx_td ltx_align_center">43.6</td>
<td id="S4.T5.1.1.5.3.13" class="ltx_td ltx_nopad_r ltx_align_center">32.5</td>
</tr>
<tr id="S4.T5.1.1.6.4" class="ltx_tr">
<th id="S4.T5.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Objects</th>
<td id="S4.T5.1.1.6.4.2" class="ltx_td ltx_align_center">50.5</td>
<td id="S4.T5.1.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">34.7</span></td>
<td id="S4.T5.1.1.6.4.4" class="ltx_td ltx_align_center">36.4</td>
<td id="S4.T5.1.1.6.4.5" class="ltx_td ltx_align_center">32.7</td>
<td id="S4.T5.1.1.6.4.6" class="ltx_td ltx_align_center">39.0</td>
<td id="S4.T5.1.1.6.4.7" class="ltx_td ltx_align_center">24.2</td>
<td id="S4.T5.1.1.6.4.8" class="ltx_td ltx_align_center">32.4</td>
<td id="S4.T5.1.1.6.4.9" class="ltx_td ltx_align_center">30.9</td>
<td id="S4.T5.1.1.6.4.10" class="ltx_td ltx_align_center">42.3</td>
<td id="S4.T5.1.1.6.4.11" class="ltx_td ltx_align_center">31.8</td>
<td id="S4.T5.1.1.6.4.12" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.6.4.12.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S4.T5.1.1.6.4.13" class="ltx_td ltx_nopad_r ltx_align_center">31.6</td>
</tr>
<tr id="S4.T5.1.1.7.5" class="ltx_tr">
<th id="S4.T5.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">People</th>
<td id="S4.T5.1.1.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.5.2.1" class="ltx_text ltx_font_bold">58.6</span></td>
<td id="S4.T5.1.1.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.5.3.1" class="ltx_text ltx_framed ltx_framed_underline">38.7</span></td>
<td id="S4.T5.1.1.7.5.4" class="ltx_td ltx_align_center">45.7</td>
<td id="S4.T5.1.1.7.5.5" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T5.1.1.7.5.6" class="ltx_td ltx_align_center">44.9</td>
<td id="S4.T5.1.1.7.5.7" class="ltx_td ltx_align_center">36.2</td>
<td id="S4.T5.1.1.7.5.8" class="ltx_td ltx_align_center">34.4</td>
<td id="S4.T5.1.1.7.5.9" class="ltx_td ltx_align_center">34.1</td>
<td id="S4.T5.1.1.7.5.10" class="ltx_td ltx_align_center">47.1</td>
<td id="S4.T5.1.1.7.5.11" class="ltx_td ltx_align_center">37.6</td>
<td id="S4.T5.1.1.7.5.12" class="ltx_td ltx_align_center">58.3</td>
<td id="S4.T5.1.1.7.5.13" class="ltx_td ltx_nopad_r ltx_align_center">38.1</td>
</tr>
<tr id="S4.T5.1.1.8.6" class="ltx_tr">
<th id="S4.T5.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Plants &amp; Animals</th>
<td id="S4.T5.1.1.8.6.2" class="ltx_td ltx_align_center">54.8</td>
<td id="S4.T5.1.1.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.8.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">39.4</span></td>
<td id="S4.T5.1.1.8.6.4" class="ltx_td ltx_align_center">40.6</td>
<td id="S4.T5.1.1.8.6.5" class="ltx_td ltx_align_center">31.9</td>
<td id="S4.T5.1.1.8.6.6" class="ltx_td ltx_align_center">45.9</td>
<td id="S4.T5.1.1.8.6.7" class="ltx_td ltx_align_center">27.9</td>
<td id="S4.T5.1.1.8.6.8" class="ltx_td ltx_align_center">35.4</td>
<td id="S4.T5.1.1.8.6.9" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T5.1.1.8.6.10" class="ltx_td ltx_align_center">46.4</td>
<td id="S4.T5.1.1.8.6.11" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T5.1.1.8.6.12" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.8.6.12.1" class="ltx_text ltx_font_bold">56.8</span></td>
<td id="S4.T5.1.1.8.6.13" class="ltx_td ltx_nopad_r ltx_align_center">34.8</td>
</tr>
<tr id="S4.T5.1.1.9.7" class="ltx_tr">
<th id="S4.T5.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pop Culture</th>
<td id="S4.T5.1.1.9.7.2" class="ltx_td ltx_align_center">43.5</td>
<td id="S4.T5.1.1.9.7.3" class="ltx_td ltx_align_center">36.7</td>
<td id="S4.T5.1.1.9.7.4" class="ltx_td ltx_align_center">32.6</td>
<td id="S4.T5.1.1.9.7.5" class="ltx_td ltx_align_center">31.0</td>
<td id="S4.T5.1.1.9.7.6" class="ltx_td ltx_align_center">44.5</td>
<td id="S4.T5.1.1.9.7.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.9.7.7.1" class="ltx_text ltx_framed ltx_framed_underline">37.1</span></td>
<td id="S4.T5.1.1.9.7.8" class="ltx_td ltx_align_center">30.6</td>
<td id="S4.T5.1.1.9.7.9" class="ltx_td ltx_align_center">31.2</td>
<td id="S4.T5.1.1.9.7.10" class="ltx_td ltx_align_center">35.8</td>
<td id="S4.T5.1.1.9.7.11" class="ltx_td ltx_align_center">30.7</td>
<td id="S4.T5.1.1.9.7.12" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.9.7.12.1" class="ltx_text ltx_font_bold">45.0</span></td>
<td id="S4.T5.1.1.9.7.13" class="ltx_td ltx_nopad_r ltx_align_center">35.5</td>
</tr>
<tr id="S4.T5.1.1.10.8" class="ltx_tr">
<th id="S4.T5.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sports</th>
<td id="S4.T5.1.1.10.8.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.10.8.2.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S4.T5.1.1.10.8.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.10.8.3.1" class="ltx_text ltx_framed ltx_framed_underline">40.0</span></td>
<td id="S4.T5.1.1.10.8.4" class="ltx_td ltx_align_center">40.2</td>
<td id="S4.T5.1.1.10.8.5" class="ltx_td ltx_align_center">32.6</td>
<td id="S4.T5.1.1.10.8.6" class="ltx_td ltx_align_center">45.7</td>
<td id="S4.T5.1.1.10.8.7" class="ltx_td ltx_align_center">34.6</td>
<td id="S4.T5.1.1.10.8.8" class="ltx_td ltx_align_center">33.3</td>
<td id="S4.T5.1.1.10.8.9" class="ltx_td ltx_align_center">30.7</td>
<td id="S4.T5.1.1.10.8.10" class="ltx_td ltx_align_center">41.8</td>
<td id="S4.T5.1.1.10.8.11" class="ltx_td ltx_align_center">34.4</td>
<td id="S4.T5.1.1.10.8.12" class="ltx_td ltx_align_center">49.7</td>
<td id="S4.T5.1.1.10.8.13" class="ltx_td ltx_nopad_r ltx_align_center">36.5</td>
</tr>
<tr id="S4.T5.1.1.11.9" class="ltx_tr">
<th id="S4.T5.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Tradition</th>
<td id="S4.T5.1.1.11.9.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.11.9.2.1" class="ltx_text ltx_font_bold">48.8</span></td>
<td id="S4.T5.1.1.11.9.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.11.9.3.1" class="ltx_text ltx_framed ltx_framed_underline">37.1</span></td>
<td id="S4.T5.1.1.11.9.4" class="ltx_td ltx_align_center">36.6</td>
<td id="S4.T5.1.1.11.9.5" class="ltx_td ltx_align_center">34.9</td>
<td id="S4.T5.1.1.11.9.6" class="ltx_td ltx_align_center">42.4</td>
<td id="S4.T5.1.1.11.9.7" class="ltx_td ltx_align_center">31.8</td>
<td id="S4.T5.1.1.11.9.8" class="ltx_td ltx_align_center">31.6</td>
<td id="S4.T5.1.1.11.9.9" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T5.1.1.11.9.10" class="ltx_td ltx_align_center">39.0</td>
<td id="S4.T5.1.1.11.9.11" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T5.1.1.11.9.12" class="ltx_td ltx_align_center">46.5</td>
<td id="S4.T5.1.1.11.9.13" class="ltx_td ltx_nopad_r ltx_align_center">30.5</td>
</tr>
<tr id="S4.T5.1.1.12.10" class="ltx_tr">
<th id="S4.T5.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Vehicles</th>
<td id="S4.T5.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_bb">50.9</td>
<td id="S4.T5.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.12.10.3.1" class="ltx_text ltx_framed ltx_framed_underline">43.8</span></td>
<td id="S4.T5.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_bb">38.4</td>
<td id="S4.T5.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_bb">39.4</td>
<td id="S4.T5.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_bb">40.5</td>
<td id="S4.T5.1.1.12.10.7" class="ltx_td ltx_align_center ltx_border_bb">32.6</td>
<td id="S4.T5.1.1.12.10.8" class="ltx_td ltx_align_center ltx_border_bb">37.4</td>
<td id="S4.T5.1.1.12.10.9" class="ltx_td ltx_align_center ltx_border_bb">35.7</td>
<td id="S4.T5.1.1.12.10.10" class="ltx_td ltx_align_center ltx_border_bb">42.1</td>
<td id="S4.T5.1.1.12.10.11" class="ltx_td ltx_align_center ltx_border_bb">33.3</td>
<td id="S4.T5.1.1.12.10.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.12.10.12.1" class="ltx_text ltx_font_bold">52.6</span></td>
<td id="S4.T5.1.1.12.10.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">33.3</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Impact of External Image Source.</h4>

<figure id="S4.T7" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Accuracy of different models divided by image source</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T7.1" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:68.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.1pt,2.0pt) scale(0.943220225007604,0.943220225007604) ;">
<table id="S4.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Image Source</span></th>
<th id="S4.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.2.1" class="ltx_text ltx_font_bold">LLaVA-1.5-7B</span></th>
<th id="S4.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">M-CLIP</span></th>
<th id="S4.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">CLIP</span></th>
<th id="S4.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.5.1" class="ltx_text ltx_font_bold">mBLIP-mT0</span></th>
<th id="S4.T7.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.6.1" class="ltx_text ltx_font_bold">mBLIP-BLOOMZ</span></th>
<th id="S4.T7.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.1.1.1.1.7.1" class="ltx_text ltx_font_bold">InstructBLIP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.1.2.1" class="ltx_tr">
<th id="S4.T7.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T7.1.1.2.1.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.2.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.3.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.1.1.2.1.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.4.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.5" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.5.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.1.1.2.1.6" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.6.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.7" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.7.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.1.1.2.1.8" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.8.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.9" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.9.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.1.1.2.1.10" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.10.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.11" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.11.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.1.1.2.1.12" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.2.1.12.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.1.1.2.1.13" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T7.1.1.2.1.13.1" class="ltx_text ltx_font_bold">LOC</span></td>
</tr>
<tr id="S4.T7.1.1.3.2" class="ltx_tr">
<th id="S4.T7.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Self-made Image</th>
<th id="S4.T7.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">47.0</th>
<th id="S4.T7.1.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">34.0</th>
<th id="S4.T7.1.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">37.9</th>
<th id="S4.T7.1.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">34.0</th>
<th id="S4.T7.1.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">41.5</th>
<th id="S4.T7.1.1.3.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">30.4</th>
<th id="S4.T7.1.1.3.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">34.4</th>
<th id="S4.T7.1.1.3.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">31.9</th>
<th id="S4.T7.1.1.3.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">40.5</th>
<th id="S4.T7.1.1.3.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">34.2</th>
<th id="S4.T7.1.1.3.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">49.1</th>
<th id="S4.T7.1.1.3.2.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">31.1</th>
</tr>
<tr id="S4.T7.1.1.4.3" class="ltx_tr">
<th id="S4.T7.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Web Image</th>
<td id="S4.T7.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">48.0</td>
<td id="S4.T7.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">37.0</td>
<td id="S4.T7.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">36.8</td>
<td id="S4.T7.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">33.6</td>
<td id="S4.T7.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">44.1</td>
<td id="S4.T7.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">33.1</td>
<td id="S4.T7.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb">34.0</td>
<td id="S4.T7.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_bb">32.8</td>
<td id="S4.T7.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_bb">39.6</td>
<td id="S4.T7.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_bb">33.5</td>
<td id="S4.T7.1.1.4.3.12" class="ltx_td ltx_align_center ltx_border_bb">46.7</td>
<td id="S4.T7.1.1.4.3.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">33.1</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Location-aware and location-agnostic results</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T7.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:68.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.5pt,2.4pt) scale(0.93323930132797,0.93323930132797) ;">
<table id="S4.T7.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.2.1.1.1" class="ltx_tr">
<th id="S4.T7.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T7.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Prompt type</span></th>
<th id="S4.T7.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.2.1" class="ltx_text ltx_font_bold">LLaVA-1.5-7B</span></th>
<th id="S4.T7.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.3.1" class="ltx_text ltx_font_bold">M-CLIP</span></th>
<th id="S4.T7.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.4.1" class="ltx_text ltx_font_bold">CLIP</span></th>
<th id="S4.T7.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.5.1" class="ltx_text ltx_font_bold">mBLIP-mT0</span></th>
<th id="S4.T7.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.6.1" class="ltx_text ltx_font_bold">mBLIP-BLOOMZ</span></th>
<th id="S4.T7.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.2.1.1.1.7.1" class="ltx_text ltx_font_bold">InstructBLIP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.2.1.2.1" class="ltx_tr">
<th id="S4.T7.2.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T7.2.1.2.1.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.2.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.3.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.2.1.2.1.4" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.4.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.5" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.5.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.2.1.2.1.6" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.6.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.7" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.7.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.2.1.2.1.8" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.8.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.9" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.9.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.2.1.2.1.10" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.10.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.11" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.11.1" class="ltx_text ltx_font_bold">LOC</span></td>
<td id="S4.T7.2.1.2.1.12" class="ltx_td ltx_align_center"><span id="S4.T7.2.1.2.1.12.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T7.2.1.2.1.13" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T7.2.1.2.1.13.1" class="ltx_text ltx_font_bold">LOC</span></td>
</tr>
<tr id="S4.T7.2.1.3.2" class="ltx_tr">
<th id="S4.T7.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Location-aware</th>
<th id="S4.T7.2.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">48.9</th>
<th id="S4.T7.2.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">36.5</th>
<th id="S4.T7.2.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">36.9</th>
<th id="S4.T7.2.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">33.4</th>
<th id="S4.T7.2.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">42.3</th>
<th id="S4.T7.2.1.3.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">31.5</th>
<th id="S4.T7.2.1.3.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">31.5</th>
<th id="S4.T7.2.1.3.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">31.2</th>
<th id="S4.T7.2.1.3.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">39.4</th>
<th id="S4.T7.2.1.3.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">33.0</th>
<th id="S4.T7.2.1.3.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">47.8</th>
<th id="S4.T7.2.1.3.2.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">32.7</th>
</tr>
<tr id="S4.T7.2.1.4.3" class="ltx_tr">
<th id="S4.T7.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Location-agnostic</th>
<td id="S4.T7.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">47.7</td>
<td id="S4.T7.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">35.4</td>
<td id="S4.T7.2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">37.5</td>
<td id="S4.T7.2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">33.8</td>
<td id="S4.T7.2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">43.1</td>
<td id="S4.T7.2.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">31.5</td>
<td id="S4.T7.2.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb">34.2</td>
<td id="S4.T7.2.1.4.3.9" class="ltx_td ltx_align_center ltx_border_bb">32.2</td>
<td id="S4.T7.2.1.4.3.10" class="ltx_td ltx_align_center ltx_border_bb">39.8</td>
<td id="S4.T7.2.1.4.3.11" class="ltx_td ltx_align_center ltx_border_bb">33.6</td>
<td id="S4.T7.2.1.4.3.12" class="ltx_td ltx_align_center ltx_border_bb">47.5</td>
<td id="S4.T7.2.1.4.3.13" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">32.1</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</figure>
<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">The performance of various models on self-made versus web images is shown in Table <a href="#S4.T7" title="Table 7 ‣ Impact of External Image Source. ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. One of the interesting findings is the performance variability across image sources for different models. For self-made images, the accuracy of some models such as LLaVA-1.5-7B and CLIP tends to be lower compared to web images. For instance, LLaVA-1.5-7B achieves a 47.0% accuracy in English prompts on self-made images but slightly higher at 48.0% on web images. CLIP shows an accuracy of 44.1% in English prompts on web images compared to 41.5% on self-made images. While this trend is not consistent across the other models, the results still indicate that web images might be more representative of the data these models (such as LLaVA-1.5-7B and CLIP) were trained on, leading to better performance.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Location-Aware vs Location-Agnostic Prompt.</h4>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px5.p1.1" class="ltx_p">The performance of the various models on location-aware versus location-agnostic prompts is shown in Table <a href="#S4.T7" title="Table 7 ‣ Impact of External Image Source. ‣ 4 Results ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. While the inclusion of location information has a varied impact on different models, the overall difference between both prompt options is marginal, suggesting no significant effect of including location information on MLLMs.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance without Multiple Choice Options.</h4>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px6.p1.1" class="ltx_p">Most of the evaluations we conduct on CVQA are under a multiple-choice setting.
However, the multiple-choice setting is often brittle towards option ordering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, and not very natural with respect to real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. In this paragraph, we explore the model’s performance on CVQA in an open-ended QA setting. To evaluate in this setting, we prompt the models without giving them the options (e.g., “In which city is this monument located?”). Then, the answer is selected by choosing the model’s highest probability of generating the full answer phrase of one of the options <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (e.g., Jakarta, Bandung, Bali, Surabaya). This way, it is robust towards ordering unlike predicting the answer letter (e.g., A), while also not giving the model multiple-choice options that can be indirectly used for deductive reasoning. Our result shows that LLaVA-1.5-7B achieved a noticeable performance drop when prompted without multiple choice, from 48.9% to just 30% average performance. This notes that in a more practical scenario, these models might be even more unreliable in cultural understanding.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Our new benchmark dataset represents a diverse worldview through the inclusion of different languages and regions not covered in previous datasets. But we acknowledge that even CVQA is not comprehensive, as it covers only a fraction of the world’s languages and regions. CVQA also lacks an English-centric baseline, which could arguably provide an interesting comparison with the rest of the regions. Additionally, our data scale prevents using CVQA to train new models, limiting its use for benchmarking purposes only.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">We note that each region has different characteristics of questions and difficulty—some regions are more likely to provide simpler, identity “what is” questions, whereas other regions might use questions that require deeper cultural knowledge. Therefore, comparing performance across languages/countries might not always be fair.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">Culture is hard to define, and our CVQA ultimately serves only as a proxy to benchmark the model’s understanding of culture through local common knowledge. However, this by no means captures all cultural nuances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Additionally, our location granularity captures country-level cultural knowledge. However, it might be interesting to capture cultural awareness at a more granular level, such as city-level, since each city might have variations in cultural common knowledge. Similarly, other demographic factors such as age might play a role in common knowledge.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">In this section we discussed the following aspects: 1) the fact that this dataset cannot be considered as a comprehensive representation of the world languages and regions; 2) the different levels of question complexity; 3) a bounded definition of culture. While these limitations might be relevant, we consider them as plausible lines for future work and outside the scope of this initial effort.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Substantial progress has been made in recent years on both datasets and methodologies for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Since the introduction of early open-ended VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, various formats like multiple-choice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, span extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and free-text generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> have been developed. Among these, multiple-choice datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> are the most commonly used, likely due to their simplicity in evaluation and comparison. The development of these datasets has significantly accelerated research progress, serving as both training data and testbeds, especially the recently introduced ScienceQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and MathVista <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> designed for evaluating MLLMs. The evolution of VQA methodologies has been revolutionary, transitioning from statistical machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to neural-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, and advanced MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> trained on massive multimodal data. Early VQA systems often required supervised learning and were limited to specific domains, but recent models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> are capable of zero-shot or few-shot learning, demonstrating strong performance. Despite this progress, significant limitations remain. Most VQA datasets focus primarily on English and a few major world languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, leading to language bias and under-representation of many languages and cultures. Additionally, the images in these datasets predominantly reflect Western scenes and styles, lacking the diversity needed to represent real-world scenarios across different cultures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Some efforts have been made to create multilingual VQA datasets, such as FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, MCVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, xGQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, MaXM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, MTVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, and MaRVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. However, these datasets are still limited in terms of the number of languages and the cultural diversity of the images and questions, or being a translation of existing English data.
On the other hand, there have been initiatives to create culturally-diverse datasets and benchmarks under text-only modality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Our proposed benchmark aims to fill the gap that covers both textual and visual modality by creating a large-scale, culturally-and-linguistically diverse dataset that will enable the development of more inclusive and robust VQA models.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We proposed CVQA, a novel, human-written visual QA benchmark dataset that captures cultural nuances across a diverse set of languages and locations. CVQA encompasses 10 question categories, with each question written in both English and the native language. This allowed us to benchmark both multilingual visual models and English-only models. We provided insights into our dataset’s question types and commonly used terms for each category.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">We then performed benchmarks on various visual models, including both multilingual and English-only models. Our benchmark demonstrated that CVQA presented challenges for open-source models. These models generally performed worse when queried in local languages compared to English, indicating poorer performance in handling multilingual queries. The performance is also considerably lower when we do not provide the multiple choice setting, which is a more realistic use case for this technology. We hope that publishing CVQA encourages the AI community to pay more attention to non-English-centric models and benchmarking, thereby advancing progress in multilingual, multimodal research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adilazuarda et al. [2024]</span>
<span class="ltx_bibblock">
M. F. Adilazuarda, S. Mukherjee, P. Lavania, S. Singh, A. Dwivedi, A. F. Aji, J. O’Neill, A. Modi, and M. Choudhury.

</span>
<span class="ltx_bibblock">Towards measuring and modeling "culture" in LLMs: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.15412</em>, 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Computer Vision (ICCV)</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al. [2023]</span>
<span class="ltx_bibblock">
Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, et al.

</span>
<span class="ltx_bibblock">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.04023</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cahyawijaya et al. [2023]</span>
<span class="ltx_bibblock">
S. Cahyawijaya, H. Lovenia, A. F. Aji, G. Winata, B. Wilie, F. Koto, R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, J. Santoso, D. Moeljadi, C. Wirawan, F. Hudi, M. S. Wicaksono, I. Parmonangan, I. Alfina, I. F. Putra, S. Rahmadani, Y. Oenang, A. Septiandri, J. Jaya, K. Dhole, A. Suryani, R. A. Putri, D. Su, K. Stevens, M. N. Nityasya, M. Adilazuarda, R. Hadiwijaya, R. Diandaru, T. Yu, V. Ghifari, W. Dai, Y. Xu, D. Damapuspita, H. Wibowo, C. Tho, I. Karo Karo, T. Fatyanosa, Z. Ji, G. Neubig, T. Baldwin, S. Ruder, P. Fung, H. Sujaini, S. Sakti, and A. Purwarianti.

</span>
<span class="ltx_bibblock">NusaCrowd: Open source initiative for Indonesian NLP resources.

</span>
<span class="ltx_bibblock">In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 13745–13818, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-acl.868</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.findings-acl.868" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.findings-acl.868</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlsson et al. [2022]</span>
<span class="ltx_bibblock">
F. Carlsson, P. Eisen, F. Rekathati, and M. Sahlgren.

</span>
<span class="ltx_bibblock">Cross-lingual and multilingual CLIP.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Language Resources and Evaluation Conference (LREC)</em>, pages 6848–6854, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. [2023]</span>
<span class="ltx_bibblock">
S. Changpinyo, L. Xue, M. Yarom, A. Thapliyal, I. Szpektor, J. Amelot, X. Chen, and R. Soricut.

</span>
<span class="ltx_bibblock">MaXM: Towards multilingual visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 2667–2682, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2023]</span>
<span class="ltx_bibblock">
W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi.

</span>
<span class="ltx_bibblock">InstructBLIP: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwivedi et al. [2023]</span>
<span class="ltx_bibblock">
A. Dwivedi, P. Lavania, and A. Modi.

</span>
<span class="ltx_bibblock">Eticor: Corpus for analyzing llms for etiquettes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 6921–6931, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2015]</span>
<span class="ltx_bibblock">
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2021]</span>
<span class="ltx_bibblock">
L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geigle et al. [2023]</span>
<span class="ltx_bibblock">
G. Geigle, A. Jain, R. Timofte, and G. Glavaš.

</span>
<span class="ltx_bibblock">mBLIP: Efficient bootstrapping of multilingual vision-LLMs.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.06930</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. [2020]</span>
<span class="ltx_bibblock">
D. Gupta, P. Lenka, A. Ekbal, and P. Bhattacharyya.

</span>
<span class="ltx_bibblock">A unified framework for multilingual and code-mixed visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</em>, pages 900–913, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. [2018]</span>
<span class="ltx_bibblock">
D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. [2021]</span>
<span class="ltx_bibblock">
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning [2019]</span>
<span class="ltx_bibblock">
D. A. Hudson and C. D. Manning.

</span>
<span class="ltx_bibblock">GQA: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 6700–6709, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jha et al. [2023]</span>
<span class="ltx_bibblock">
A. Jha, A. M. Davani, C. K. Reddy, S. Dave, V. Prabhakaran, and S. Dev.

</span>
<span class="ltx_bibblock">Seegull: A stereotype benchmark with broad geo-cultural coverage leveraging generative models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 9851–9870, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabra et al. [2023]</span>
<span class="ltx_bibblock">
A. Kabra, E. Liu, S. Khanuja, A. F. Aji, G. Winata, S. Cahyawijaya, A. Aremu, P. Ogayo, and G. Neubig.

</span>
<span class="ltx_bibblock">Multi-lingual and multi-cultural figurative language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 8269–8284, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kunchukuttan et al. [2020]</span>
<span class="ltx_bibblock">
A. Kunchukuttan, D. Kakwani, S. Golla, G. N. C., A. Bhattacharyya, M. M. Khapra, and P. Kumar.

</span>
<span class="ltx_bibblock">Ai4bharat-indicnlp corpus: Monolingual corpora and word embeddings for indic languages, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leong et al. [2023]</span>
<span class="ltx_bibblock">
W. Q. Leong, J. G. Ngui, Y. Susanto, H. Rengarajan, K. Sarveswaran, and W. C. Tjhi.

</span>
<span class="ltx_bibblock">Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao.

</span>
<span class="ltx_bibblock">LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2023a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao.

</span>
<span class="ltx_bibblock">Multimodal foundation models: From specialists to general-purpose assistants.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Computer Graphics and Vision</em>, 16(1-2):1–214, 2024.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning (ICML)</em>, pages 19730–19742, 2023b.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and D. Elliott.

</span>
<span class="ltx_bibblock">Visually grounded reasoning across languages and cultures.

</span>
<span class="ltx_bibblock">In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 10467–10485, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.818</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.818" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.818</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023a]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Y. Li, and Y. J. Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</em>, 2023a.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023b]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Q. Wu, and Y. J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2023b.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2022]</span>
<span class="ltx_bibblock">
P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2023]</span>
<span class="ltx_bibblock">
P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao.

</span>
<span class="ltx_bibblock">MathVista: Evaluating mathematical reasoning of foundation models in visual contexts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2024]</span>
<span class="ltx_bibblock">
C. Lyu, M. Wu, and A. F. Aji.

</span>
<span class="ltx_bibblock">Beyond probabilities: Unveiling the misalignment in evaluating large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.13887</em>, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz [2014]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes based on uncertain input.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of NeurIPS</em>, 2014.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. [2015]</span>
<span class="ltx_bibblock">
M. Malinowski, M. Rohrbach, and M. Fritz.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions about images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. [2019]</span>
<span class="ltx_bibblock">
K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi.

</span>
<span class="ltx_bibblock">OK-VQA: A visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proccedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 3195–3204, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. [2022]</span>
<span class="ltx_bibblock">
M. Mathew, V. Bagal, R. P. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar.

</span>
<span class="ltx_bibblock">InfographicVQA.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, pages 1697–1706, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al. [2023]</span>
<span class="ltx_bibblock">
A. Mukherjee, C. Raj, Z. Zhu, and A. Anastasopoulos.

</span>
<span class="ltx_bibblock">Global voices, local biases: Socio-cultural prejudices across languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 15828–15845, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orife et al. [2020]</span>
<span class="ltx_bibblock">
I. Orife, J. Kreutzer, B. Sibanda, D. Whitenack, K. Siminyu, L. Martinus, J. T. Ali, J. Abbott, V. Marivate, S. Kabongo, M. Meressa, E. Murhabazi, O. Ahia, E. van Biljon, A. Ramkilowan, A. Akinfaderin, A. Öktem, W. Akin, G. Kioko, K. Degila, H. Kamper, B. Dossou, C. Emezue, K. Ogueji, and A. Bashir.

</span>
<span class="ltx_bibblock">Masakhane – machine translation for africa, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pezeshkpour and Hruschka [2023]</span>
<span class="ltx_bibblock">
P. Pezeshkpour and E. Hruschka.

</span>
<span class="ltx_bibblock">Large language models sensitivity to the order of options in multiple-choice questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11483</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. [2022]</span>
<span class="ltx_bibblock">
J. Pfeiffer, G. Geigle, A. Kamath, J.-M. Steitz, S. Roth, I. Vulić, and I. Gurevych.

</span>
<span class="ltx_bibblock">xGQA: Cross-lingual visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2497–2511, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning (ICML)</em>, pages 8748–8763, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson and Wingate [2023]</span>
<span class="ltx_bibblock">
J. Robinson and D. Wingate.

</span>
<span class="ltx_bibblock">Leveraging large language models for multiple choice question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma and Jalal [2021]</span>
<span class="ltx_bibblock">
H. Sharma and A. S. Jalal.

</span>
<span class="ltx_bibblock">A survey of methods, datasets and evaluation metrics for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, 116:104327, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2019]</span>
<span class="ltx_bibblock">
A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach.

</span>
<span class="ltx_bibblock">Towards VQA models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 8317–8326, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2024]</span>
<span class="ltx_bibblock">
J. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao, Y. Wang, Y. Liu, H. Liu, X. Bai, and C. Huang.

</span>
<span class="ltx_bibblock">MTVQA: Benchmarking multilingual text-centric visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.11985</em>, 2024.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2023]</span>
<span class="ltx_bibblock">
G. Team.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024]</span>
<span class="ltx_bibblock">
B. Wang, Z. Liu, X. Huang, F. Jiao, Y. Ding, A. Aw, and N. F. Chen.

</span>
<span class="ltx_bibblock">Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning, 2024.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wibowo et al. [2023]</span>
<span class="ltx_bibblock">
H. A. Wibowo, E. H. Fuadi, M. N. Nityasya, R. E. Prasojo, and A. F. Aji.

</span>
<span class="ltx_bibblock">Copal-id: Indonesian language reasoning with local culture and nuances.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.01012</em>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2021]</span>
<span class="ltx_bibblock">
L. Xu, H. Huang, and J. Liu.

</span>
<span class="ltx_bibblock">SUTD-TrafficQA: A question answering benchmark and an efficient network for video reasoning over traffic events.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 9878–9888, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2021]</span>
<span class="ltx_bibblock">
A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid.

</span>
<span class="ltx_bibblock">Just ask: Learning to answer questions from millions of narrated videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 1686–1697, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2023]</span>
<span class="ltx_bibblock">
W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang.

</span>
<span class="ltx_bibblock">MM-Vet: Evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.02490</em>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. [2023]</span>
<span class="ltx_bibblock">
X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen.

</span>
<span class="ltx_bibblock">MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16502</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2024a]</span>
<span class="ltx_bibblock">
B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su.

</span>
<span class="ltx_bibblock">GPT-4V(ision) is a generalist web agent, if grounded.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.01614</em>, 2024a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2024b]</span>
<span class="ltx_bibblock">
C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang.

</span>
<span class="ltx_bibblock">Large language models are not robust multiple choice selectors.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Conference on Learning Representations (ICLR)</em>, 2024b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2016]</span>
<span class="ltx_bibblock">
Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7W: Grounded Question Answering in Images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p">See pages 1 of <a href="guideline.pdf" title="" class="ltx_ref">guideline.pdf</a>
See pages 2- of <a href="guideline.pdf" title="" class="ltx_ref">guideline.pdf</a></p>
</div>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Annotation Platform</h2>

<figure id="A2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.05967/assets/jotform-input1.png" id="A2.F4.1.g1" class="ltx_graphics ltx_img_square" width="598" height="659" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.05967/assets/jotform-input2.png" id="A2.F4.2.g1" class="ltx_graphics ltx_img_square" width="598" height="668" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Annotation interface for inputting image and questions</figcaption>
</figure>
<figure id="A2.F5" class="ltx_figure ltx_align_center"><img src="/html/2406.05967/assets/jotform-valid1.png" id="A2.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Annotation interface for validation. Contributors can comment, edit, and star the entries</figcaption>
</figure>
<figure id="A2.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.05967/assets/jotform-vallid2.png" id="A2.F6.1.g1" class="ltx_graphics ltx_img_square" width="598" height="581" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.05967/assets/jotform-valid3.png" id="A2.F6.2.g1" class="ltx_graphics ltx_img_square" width="598" height="568" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>During validation, contributors can preview the submission from other contributors</figcaption>
</figure>
<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">We use JotForm as our annotation platform. For question entry, contributors can upload and write questions in both languages in the form. The interface can be seen in Figure <a href="#A2.F4" title="Figure 4 ‣ Appendix B Annotation Platform ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. During validation, contributors can see all the data submitted by other contributors (Figure <a href="#A2.F5" title="Figure 5 ‣ Appendix B Annotation Platform ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). They can select the entry to see detailed preview of the entry (Figure <a href="#A2.F6" title="Figure 6 ‣ Appendix B Annotation Platform ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). They can then either edit the data directly, provide comments, or confirm the data by starring the entry.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Most-Frequent Words in the Questions</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">Figure <a href="#A3.F7" title="Figure 7 ‣ Appendix C Most-Frequent Words in the Questions ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows word clouds for the most frequent words in CVQA per category. We exclude stopwords as well as ‘picture’, ‘photo’, and ‘image’ from the list, since most questions contain these words. In this VQA context, we can treat them as stopwords.</p>
</div>
<figure id="A3.F7" class="ltx_figure"><img src="/html/2406.05967/assets/word-cloud.png" id="A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Word Cloud in CVQA per category</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>CVQA Annotator Demographic</h2>

<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p">Figure <a href="#A4.F8" title="Figure 8 ‣ Appendix D CVQA Annotator Demographic ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates the demographic statistics of the annotators, based on an anonymous questionnaire we provided. At the time of writing, we have information for 36 out of 76 annotators. As such, this breakdown is a rough representation of the annotation group.</p>
</div>
<figure id="A4.F8" class="ltx_figure"><img src="/html/2406.05967/assets/demographic2.png" id="A4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Annotator demographic statistics</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Country-Language Pairs and Scripts</h2>

<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.1" class="ltx_p">In Table <a href="#A5.T8" title="Table 8 ‣ Appendix E Country-Language Pairs and Scripts ‣ CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we provide information on the script used in each Country-Language pair.</p>
</div>
<figure id="A5.T8" class="ltx_table">
<table id="A5.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.T8.1.1.1" class="ltx_tr">
<td id="A5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="A5.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Country</span></td>
<td id="A5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A5.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="A5.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A5.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Script</span></td>
</tr>
<tr id="A5.T8.1.2.2" class="ltx_tr">
<td id="A5.T8.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="A5.T8.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Africa</span></td>
</tr>
<tr id="A5.T8.1.3.3" class="ltx_tr">
<td id="A5.T8.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Egypt</td>
<td id="A5.T8.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Egyptian Arabic</td>
<td id="A5.T8.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Arabic</td>
</tr>
<tr id="A5.T8.1.4.4" class="ltx_tr">
<td id="A5.T8.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Ethiopia</td>
<td id="A5.T8.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r">Amharic</td>
<td id="A5.T8.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r">Amharic</td>
</tr>
<tr id="A5.T8.1.5.5" class="ltx_tr">
<td id="A5.T8.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Ethiopia</td>
<td id="A5.T8.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r">Oromo</td>
<td id="A5.T8.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.6.6" class="ltx_tr">
<td id="A5.T8.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Nigeria</td>
<td id="A5.T8.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r">Igbo</td>
<td id="A5.T8.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.7.7" class="ltx_tr">
<td id="A5.T8.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="A5.T8.1.7.7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Asia</span></td>
</tr>
<tr id="A5.T8.1.8.8" class="ltx_tr">
<td id="A5.T8.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">China</td>
<td id="A5.T8.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Chinese</td>
<td id="A5.T8.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Chinese</td>
</tr>
<tr id="A5.T8.1.9.9" class="ltx_tr">
<td id="A5.T8.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">India</td>
<td id="A5.T8.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r">Bengali</td>
<td id="A5.T8.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r">Bengali</td>
</tr>
<tr id="A5.T8.1.10.10" class="ltx_tr">
<td id="A5.T8.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">India</td>
<td id="A5.T8.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r">Tamil</td>
<td id="A5.T8.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r">Tamil</td>
</tr>
<tr id="A5.T8.1.11.11" class="ltx_tr">
<td id="A5.T8.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Indonesia</td>
<td id="A5.T8.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r">Indonesian</td>
<td id="A5.T8.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.12.12" class="ltx_tr">
<td id="A5.T8.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Indonesia</td>
<td id="A5.T8.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r">Javanese</td>
<td id="A5.T8.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.13.13" class="ltx_tr">
<td id="A5.T8.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Indonesia</td>
<td id="A5.T8.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r">Minangkabau</td>
<td id="A5.T8.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.14.14" class="ltx_tr">
<td id="A5.T8.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Indonesia</td>
<td id="A5.T8.1.14.14.2" class="ltx_td ltx_align_left ltx_border_r">Sundanese</td>
<td id="A5.T8.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.15.15" class="ltx_tr">
<td id="A5.T8.1.15.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Japan</td>
<td id="A5.T8.1.15.15.2" class="ltx_td ltx_align_left ltx_border_r">Japanese</td>
<td id="A5.T8.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r">Japanese</td>
</tr>
<tr id="A5.T8.1.16.16" class="ltx_tr">
<td id="A5.T8.1.16.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">South Korea</td>
<td id="A5.T8.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r">Korean</td>
<td id="A5.T8.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r">Hangul</td>
</tr>
<tr id="A5.T8.1.17.17" class="ltx_tr">
<td id="A5.T8.1.17.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Malaysia</td>
<td id="A5.T8.1.17.17.2" class="ltx_td ltx_align_left ltx_border_r">Malay</td>
<td id="A5.T8.1.17.17.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.18.18" class="ltx_tr">
<td id="A5.T8.1.18.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Mongolia</td>
<td id="A5.T8.1.18.18.2" class="ltx_td ltx_align_left ltx_border_r">Mongolian</td>
<td id="A5.T8.1.18.18.3" class="ltx_td ltx_align_left ltx_border_r">Cyrillic</td>
</tr>
<tr id="A5.T8.1.19.19" class="ltx_tr">
<td id="A5.T8.1.19.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Pakistan</td>
<td id="A5.T8.1.19.19.2" class="ltx_td ltx_align_left ltx_border_r">Urdu</td>
<td id="A5.T8.1.19.19.3" class="ltx_td ltx_align_left ltx_border_r">Perso-Arabic</td>
</tr>
<tr id="A5.T8.1.20.20" class="ltx_tr">
<td id="A5.T8.1.20.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Philippines</td>
<td id="A5.T8.1.20.20.2" class="ltx_td ltx_align_left ltx_border_r">Filipino</td>
<td id="A5.T8.1.20.20.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.21.21" class="ltx_tr">
<td id="A5.T8.1.21.21.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Singapore</td>
<td id="A5.T8.1.21.21.2" class="ltx_td ltx_align_left ltx_border_r">Chinese</td>
<td id="A5.T8.1.21.21.3" class="ltx_td ltx_align_left ltx_border_r">Chinese</td>
</tr>
<tr id="A5.T8.1.22.22" class="ltx_tr">
<td id="A5.T8.1.22.22.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Sri Lanka</td>
<td id="A5.T8.1.22.22.2" class="ltx_td ltx_align_left ltx_border_r">Sinhala</td>
<td id="A5.T8.1.22.22.3" class="ltx_td ltx_align_left ltx_border_r">Sinhalese</td>
</tr>
<tr id="A5.T8.1.23.23" class="ltx_tr">
<td id="A5.T8.1.23.23.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="A5.T8.1.23.23.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Europe</span></td>
</tr>
<tr id="A5.T8.1.24.24" class="ltx_tr">
<td id="A5.T8.1.24.24.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Bulgaria</td>
<td id="A5.T8.1.24.24.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Bulgarian</td>
<td id="A5.T8.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cyrillic</td>
</tr>
<tr id="A5.T8.1.25.25" class="ltx_tr">
<td id="A5.T8.1.25.25.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">France</td>
<td id="A5.T8.1.25.25.2" class="ltx_td ltx_align_left ltx_border_r">Breton</td>
<td id="A5.T8.1.25.25.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.26.26" class="ltx_tr">
<td id="A5.T8.1.26.26.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Ireland</td>
<td id="A5.T8.1.26.26.2" class="ltx_td ltx_align_left ltx_border_r">Irish</td>
<td id="A5.T8.1.26.26.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.27.27" class="ltx_tr">
<td id="A5.T8.1.27.27.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Norway</td>
<td id="A5.T8.1.27.27.2" class="ltx_td ltx_align_left ltx_border_r">Norwegian</td>
<td id="A5.T8.1.27.27.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.28.28" class="ltx_tr">
<td id="A5.T8.1.28.28.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Romania</td>
<td id="A5.T8.1.28.28.2" class="ltx_td ltx_align_left ltx_border_r">Romanian</td>
<td id="A5.T8.1.28.28.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.29.29" class="ltx_tr">
<td id="A5.T8.1.29.29.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Russia</td>
<td id="A5.T8.1.29.29.2" class="ltx_td ltx_align_left ltx_border_r">Russian</td>
<td id="A5.T8.1.29.29.3" class="ltx_td ltx_align_left ltx_border_r">Cyrillic</td>
</tr>
<tr id="A5.T8.1.30.30" class="ltx_tr">
<td id="A5.T8.1.30.30.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Spain</td>
<td id="A5.T8.1.30.30.2" class="ltx_td ltx_align_left ltx_border_r">Spanish</td>
<td id="A5.T8.1.30.30.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.31.31" class="ltx_tr">
<td id="A5.T8.1.31.31.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span id="A5.T8.1.31.31.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Latin America</span></td>
</tr>
<tr id="A5.T8.1.32.32" class="ltx_tr">
<td id="A5.T8.1.32.32.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Argentina</td>
<td id="A5.T8.1.32.32.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Spanish</td>
<td id="A5.T8.1.32.32.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Latin</td>
</tr>
<tr id="A5.T8.1.33.33" class="ltx_tr">
<td id="A5.T8.1.33.33.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Brazil</td>
<td id="A5.T8.1.33.33.2" class="ltx_td ltx_align_left ltx_border_r">Portuguese</td>
<td id="A5.T8.1.33.33.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.34.34" class="ltx_tr">
<td id="A5.T8.1.34.34.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Chile</td>
<td id="A5.T8.1.34.34.2" class="ltx_td ltx_align_left ltx_border_r">Spanish</td>
<td id="A5.T8.1.34.34.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.35.35" class="ltx_tr">
<td id="A5.T8.1.35.35.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Colombia</td>
<td id="A5.T8.1.35.35.2" class="ltx_td ltx_align_left ltx_border_r">Spanish</td>
<td id="A5.T8.1.35.35.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.36.36" class="ltx_tr">
<td id="A5.T8.1.36.36.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Ecuador</td>
<td id="A5.T8.1.36.36.2" class="ltx_td ltx_align_left ltx_border_r">Spanish</td>
<td id="A5.T8.1.36.36.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.37.37" class="ltx_tr">
<td id="A5.T8.1.37.37.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Mexico</td>
<td id="A5.T8.1.37.37.2" class="ltx_td ltx_align_left ltx_border_r">Spanish</td>
<td id="A5.T8.1.37.37.3" class="ltx_td ltx_align_left ltx_border_r">Latin</td>
</tr>
<tr id="A5.T8.1.38.38" class="ltx_tr">
<td id="A5.T8.1.38.38.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Uruguay</td>
<td id="A5.T8.1.38.38.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Spanish</td>
<td id="A5.T8.1.38.38.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Latin</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>The list of Country-Language pairs covered in CVQA and their corresponding scripts.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.05966" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.05967" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.05967">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.05967" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.05968" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 17:58:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
