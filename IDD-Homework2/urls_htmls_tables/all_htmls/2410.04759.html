<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM</title>
<!--Generated on Mon Oct  7 05:26:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04759v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S1" title="In Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S2" title="In Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">RELATED WORK</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S2.SS1" title="In II RELATED WORK ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Traffic Regulation in Autonomous Driving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S2.SS2" title="In II RELATED WORK ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Retrieval-Augmented Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S2.SS3" title="In II RELATED WORK ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Decision-Making of Autonomous Driving</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S3" title="In Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S3.SS1" title="In III Method ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Retrieval-Augmented Generation with Traffic Regulations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S3.SS2" title="In III Method ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Reasoning Agent</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4" title="In Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.SS1" title="In IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Traffic Regulation Retrieval (TRR) Agent and RAG</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.SS2" title="In IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Hypothesized Scenarios</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.SS3" title="In IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Real-world Scenarios</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S5" title="In Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">CONCLUSION</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianhui Cai<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">1∗</span></sup>, Yifan Liu<sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">1∗</span></sup>, Zewei Zhou<sup class="ltx_sup" id="id12.12.id3"><span class="ltx_text ltx_font_italic" id="id12.12.id3.1">1</span></sup>, Haoxuan Ma<sup class="ltx_sup" id="id13.13.id4"><span class="ltx_text ltx_font_italic" id="id13.13.id4.1">1</span></sup>, 
<br class="ltx_break"/>Seth Z. Zhao<sup class="ltx_sup" id="id14.14.id5"><span class="ltx_text ltx_font_italic" id="id14.14.id5.1">1</span></sup>, Zhiwen Wu<sup class="ltx_sup" id="id15.15.id6"><span class="ltx_text ltx_font_italic" id="id15.15.id6.1">1</span></sup> and Jiaqi Ma<sup class="ltx_sup" id="id16.16.id7"><span class="ltx_text ltx_font_italic" id="id16.16.id7.1">1†</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id17.17.id1"><span class="ltx_text ltx_font_italic" id="id17.17.id1.1">1</span></sup>The authors are with the UCLA Mobility Lab and Mobility Center of Excellence, Los Angeles, Los Angeles, USA.
<span class="ltx_text ltx_font_typewriter" id="id18.18.id2" style="font-size:90%;">{tianhui, bmmliu, zeweizhou, haoxuanma, zhiwenwu1212, sethzhao506, jiaqima}@ucla.edu</span>* indicates equal contribution.<sup class="ltx_sup" id="id19.19.id1"><span class="ltx_text ltx_font_italic" id="id19.19.id1.1">†</span></sup>Corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">This work presents an interpretable decision-making framework for autonomous vehicles that integrates traffic regulations, norms, and safety guidelines comprehensively and enables seamless adaptation to different regions. While traditional rule-based methods struggle to incorporate the full scope of traffic rules, we develop a Traffic Regulation Retrieval (TRR) Agent based on Retrieval-Augmented Generation (RAG) to automatically retrieve relevant traffic rules and guidelines from extensive regulation documents and relevant records based on the ego vehicle’s situation. Given the semantic complexity of the retrieved rules, we also design a reasoning module powered by a Large Language Model (LLM) to interpret these rules, differentiate between mandatory rules and safety guidelines, and assess actions on legal compliance and safety. Additionally, the reasoning is designed to be interpretable, enhancing both transparency and reliability. The framework demonstrates robust performance on both hypothesized and real-world cases across diverse scenarios, along with the ability to adapt to different regions with ease.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">utonomous driving has advanced significantly in recent years, showing its promising potential for enhancing safety and efficiency, and is gradually being integrated into everyday life <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib6" title="">6</a>]</cite>. For autonomous vehicles to seamlessly integrate into transportation systems designed for humans, they are expected to fulfill several key requirements: (1) adhere to traffic rules (traffic regulations, laws, and social norms) for humans; and (2) earn and maintain public trust through transparent, safe, and reliable operations around humans. Based on these requirements, it is crucial that the decision-making process of autonomous systems ensure compliance with traffic rules and adherence to safety guidelines. Additionally, ensuring that this rule-aware decision-making process is interpretable is equally important. Interpretability is essential not only for identifying the applicable rules but also for fostering trust and ensuring accountability for users.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One of the major challenges in achieving rule-aware decision-making is the complexity of identifying which rules apply in a given driving situation. Traffic rules are diverse and complex, encompassing thousands of regulations from laws, driving handbooks, or driving norms of different regions. Among these traffic rules, the ego vehicle must consider various factors, such as the actions of other road users, current road conditions, and environmental context—to identify the ones that are relevant to the specific scenario. Changes in any of these factors might require different rules or a re-prioritization of existing ones. Previous efforts have focused on selecting key rules and creating hand-crafted decision-making rules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib9" title="">9</a>]</cite>, however, this manual encoding approach struggles to handle a large number of traffic rules and cannot easily adapt to regulations in different regions without additional effort.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Another challenge in rule-aware decision-making is the semantic complexity and context dependence of traffic rules, making accurate interpretation and compliance difficult for models. Traffic rules range from signage interpretation to specific driving behaviors, necessitating varied integration into decision-making processes. For instance, legal codes are strict constraints, while local norms and safety guidelines may require flexible applications based on context. Therefore, intelligently understanding and incorporating hand-crafted rules into the decision-making system is crucial for autonomous vehicles’ seamless integration into human traffic systems. This is a challenging task for conventional AI systems that are trained for specific tasks, but it becomes possible with the powerful comprehension and reasoning capabilities of Large Language Models (LLMs).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address these gaps, this paper introduces a novel interpretable decision maker for traffic rule compliance, which incorporates a Traffic Regulation Retrieval Agent built upon Retrieval-Augmented Generation (RAG) and a reasoning module using LLM, specifically GPT-4o. The reasoning module assesses the actions on two levels: (1) whether the action is compliant, meaning it follows all mandatory traffic rules; and (2) whether the action is considered a safe behavior, meaning it adheres to both mandatory traffic rules and safety guidelines. This dual-level assessment ensures a comprehensive evaluation and decision making of both legal compliance and adherence to safe driving practices. Additionally, for enhanced interpretability, intermediate reasoning information, such as the traffic rules used in the reasoning process, is also output, providing transparency in the evaluator’s decision-making process. Experiments are conducted on both hypothesized and real-world cases, with our framework consistently demonstrating robust results.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a novel LLM-driven decision-making framework for autonomous vehicles that integrates traffic rules, featuring an interpretable Reasoning Agent that assesses actions for compliance and safety, ensuring rule-adherent decision outputs and seamless adaptation to different regions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop a Traffic Regulation Retrieval System which thoroughly retrieves traffic rules based on ego vehicle’s current situation from extensive traffic regulation documents and relevant records.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">To further validate our approach, we establish a comprehensive benchmark that encompasses both hypothetical and real-world scenarios to evaluate the agent’s ability to incorporate traffic regulations into its decision-making. The experimental results demonstrate strong performance in challenging situations, highlighting the framework’s adaptability across different regions.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">RELATED WORK</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Traffic Regulation in Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Various techniques have been applied to integrate traffic regulations into autonomous driving systems. Early approaches included rule-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib10" title="">10</a>]</cite> and finite state machines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib11" title="">11</a>]</cite>, which encoded traffic laws through explicit if-then rules or state transitions. To handle complex scenarios, more sophisticated methods emerged: behavior trees created hierarchical decision-making structures capable of representing and executing traffic rules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib12" title="">12</a>]</cite>, and formal methods using temporal logics like LTL or MTL provided rigorous frameworks for specifying and verifying compliance with traffic laws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib13" title="">13</a>]</cite>. However, these methods often struggled with the ambiguity and regional variations of real-world traffic rules, leading to challenges in creating autonomous vehicles that could adapt to different regulatory environments.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding natural language and interpreting complex scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib19" title="">19</a>]</cite>. By leveraging these abilities, LLMs can process and integrate traffic rules in a more flexible and context-aware manner, eliminating the need for rule-based encoding. For example, LLaDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib20" title="">20</a>]</cite> utilizes LLMs to interpret traffic rules from local handbooks, enabling autonomous vehicles to adjust tasks and motion plans accordingly. Similarly, Agent-Driver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib21" title="">21</a>]</cite> incorporates traffic rules into an LLM-based cognitive framework, storing and referencing them during planning. However, ensuring LLMs accurately apply relevant traffic rules without hallucinations or misinterpretations remains a key challenge.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Retrieval-Augmented Generation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib22" title="">22</a>]</cite> addresses LLM hallucinations and improves information retrieval accuracy by combining neural retrieval with a sequence-to-sequence generator, producing outputs based on relevant documents. Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib24" title="">24</a>]</cite> have demonstrated RAG’s effectiveness in enhancing LLM accuracy and factual correctness across domains like current events, language modeling, and open-domain question answering. These findings elicit RAG’s potential for enhancing LLM-based autonomous driving systems’ compliance with traffic regulations. Its dynamic retrieval capability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib22" title="">22</a>]</cite> enables real-time access to region-specific traffic rules, addressing the challenge of adapting to diverse regulatory environments. The enhanced factual grounding provided by RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib23" title="">23</a>]</cite> can reduce hallucinations in LLMs, mitigating the risk of fabricating or misapplying traffic rules. RAG’s ability to handle complex and contextual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib24" title="">24</a>]</cite> is well-suited for interpreting nuanced traffic regulations with multiple conditions or exceptions. Furthermore, the transparency inherent in RAG’s retrieval process can improve the interpretability of decision-making in autonomous driving systems, a crucial factor for regulatory compliance and public trust.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Decision-Making of Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Decision-making methods of autonomous driving have evolved from rule-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib25" title="">25</a>]</cite> to learning-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib26" title="">26</a>]</cite>. Learning-based methods demonstrate greater adaptability than the former in dynamic driving environments and free autonomous vehicles from the constraints of complex hand-crafted rules. Two typical learning methods are imitation learning (IL) and reinforcement learning (RL). The IL focuses on imitating the expert’s decision but faces different distribution in online deployment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib28" title="">28</a>]</cite>. On the contrary, the RL explores and learns in the online interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib29" title="">29</a>]</cite>, but such trial-and-error is inefficient. Furthermore, GPT-Driver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib30" title="">30</a>]</cite> introduced the GPT to the autonomous vehicle (AV), which reformulates the planning as a language modeling problem. Nonetheless, in the human driving environment structured by traffic rules, AVs not only need to ensure safety but also follow these rules while driving with human-driven vehicles. The integration of diverse semantic traffic rules into decision-making using a unified model remains underexplored.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S2.F1.g1" src="extracted/5906163/figures/main_pipeline.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F1.4.2" style="font-size:90%;">Overview of Driving with Regulation Framework<span class="ltx_text ltx_font_medium" id="S2.F1.4.2.1">. The framework consists of two main components: the Traffic Rules Retrieval Agent and Reasoning Agent. The Traffic Rules Retrieval Agent retrieves relevant rules from traffic regulation documents based on the generated traffic rule retrieval query. The Reasoning Agent then identifies the applicable rules from the retrieved set and performs compliance and safety checks based on those applicable rules.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our proposed method, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S2.F1" title="Figure 1 ‣ II-C Decision-Making of Autonomous Driving ‣ II RELATED WORK ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">1</span></a>, comprises two main components: a Traffic Rules Retrieval Agent that retrieves relevant traffic rules from regulation documents using a retrieval query, and a Reasoning Agent that assesses the traffic rule adherence of each action in the action set based on environment information, ego vehicle’s state and retrieved traffic rules.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We first perform an environment analysis to generate the retrieval query for the Traffic Rules Retrieval Agent and provide the environment information input for the Reasoning Agent. To extract more regulation-related features beyond the common perception output, we use a Vision Language Model (VLM), GPT-4o, to analyze the environment based on the ego vehicle’s camera images. The analysis follows a carefully designed Chain-of-Thought (CoT) process: the VLM first performs a broad environment overview and checks general road information, then it conducts a detailed analysis concentrating on critical elements such as other road users, traffic elements, and lane markings, especially those relevant to the vehicle’s global planning output (e.g., ‘Right,’ ‘Left,’ or ‘Forward’). The VLM then generates a concise retrieval query that summarizes the current scenario for the Traffic Rules Retrieval Agent. An example output of Environment Analysis is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F3" title="Figure 3 ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We extract an action set containing all possible actions from the Action Space based on the Global Planning output. For simplicity and to maintain the focus of this work on traffic rule adherence, the Action Space only consists of a predefined set of actions: turning right, turning left, going forward (with current speed, acceleration, or deceleration), changing lane to the left, and changing lane to the right. The extraction process selects the actions that align with the Global Planning output. For example, if the Global Planning output is “LEFT,” the action set would include turning left with current speed, an acceleration, or a deceleration.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Retrieval-Augmented Generation with Traffic Regulations</span>
</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S3.F2.g1" src="extracted/5906163/figures/rag_flow_diagram_v2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">Illustration of the proposed Traffic Regulation Retrieval (TRR) Agent.<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1"> The retrieval results are obtained through the similarity score between scene description and well-curated regulation documents with a pre-defined relevance metric.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To enhance the model’s understanding of localized traffic rules and norms, and to fully consider all the related rules from available sources, we developed the Traffic Regulation Retrieval (TRR) Agent, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S3.F2" title="Figure 2 ‣ III-A Retrieval-Augmented Generation with Traffic Regulations ‣ III Method ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Since different regions have varying sources of traffic rules, we take the United States as an example to demonstrate how the TRR Agent fully considers available sources. Due to constitutional reasons, traffic regulations in the U.S. are set by each state rather than the federal government. Cities also establish local rules for management. To ensure comprehensive coverage, the TRR includes both state and local regulations. Additionally, case laws that provide reference for the U.S. judiciary systems and driving manuals that provide extra safety guidelines along with regulations are also considered as important sources and are included in the TRR. Therefore we design TRR with an integration of a comprehensive collection of regulatory documents as below:</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">State-Level Traffic Law</span>: Laws regulate vehicle operations and ensure road safety established by state legislatures and enforced statewide.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">State-Level Driving Manual</span>: Published by each state’s DMV, this manual details state traffic laws and safe driving practices. It includes driving safety guidelines in the form of text and illustration images.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">City-Level Traffic Regulation</span>: Set by local governments, rules to address specific needs such as parking, speed limits, and lane usage to manage local traffic and ensure safety.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">State-Level Court Cases</span>: Judicial rulings on traffic-related cases clarify laws and influence enforcement.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Traffic Norms</span>: Widely recognized behaviors that drivers follow to ensure smooth and safe road interactions. These norms are essential for autonomous vehicles to align with human driving behaviors and societal expectations. This paper does not focus on building a repository of records for such norms but we will use examples to illustrate that our framework still applies.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">Upon evaluating retrieval performance against traditional inverted index-based retrieval methods relying on keyword inputs such as BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib31" title="">31</a>]</cite> and Taily <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib32" title="">32</a>]</cite>, we found that embedding-based algorithms, which utilize informative long queries and retrieve based on paragraph similarity, significantly outperform in terms of completeness and efficiency. Integrated into the TRR Agent, embedding-based methods handle the complexities of driving scenarios more effectively.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">Each document or record is reformatted into markdown with hierarchical headers to improve clarity, enabling better interpretation by the subsequent Reasoning Agent. In addition to textual content, figures, especially widely used in state-level handbooks, which clarify regulations with visual examples, are integrated into the TRR Agent. This integration is particularly important when regulatory details are embedded in images but are not explicitly described in the accompanying text. To address this, figures are transformed into text labels and appended at the end of the relevant paragraphs, and appropriately restored at the end of the retrieval process.</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.1">During the retrieval process, we first produce vector embeddings for both the regulatory documents and the previously generated traffic rule retrieval query, and then apply FAISS similarity search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib33" title="">33</a>]</cite> to determine their relevance. A cascading retrieval pipeline from paragraph-level to sentence-level helps ensure that the results are both comprehensive and concise. After paragraph-level embeddings across the entire data source, a top-k selection is applied to choose the most relevant paragraphs, forming a new niche database. To address the sparsity issue that can affect search accuracy due to the large size of the tokenized traffic book, we re-embed the selected paragraphs at the sentence level. This second-level embedding provides better, more accurate indexing and searching capabilities by focusing on the most pertinent sections. This approach allows for dynamic adaptation by prioritizing the relevance of available regulations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p11">
<p class="ltx_p" id="S3.SS1.p11.1">In the end, the TRR Agent aggregates the selected sentences from traffic regulation and state-level laws, rules from city regulations, and court cases will be combined together as well as the attribute images to produce a comprehensive result which is later provided to the Reasoning Agent.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Reasoning Agent</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The Reasoning Agent is responsible for determining whether each action in the action set complies with traffic rules, leveraging an LLM (GPT-4o) with a CoT prompting method. The Reasoning Agent receives three key inputs: (1) the current environment information from the environment analysis (2) ego vehicle’s action set, and (3) a set of retrieved traffic rules from TRR Agent.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In the reasoning process, the agent first filters the retrieved traffic rules to identify those that are most applicable to both the current situation and the ego vehicle’s intended action. These rules are then categorized into either mandatory rules, which must be followed to ensure legal compliance, or safety guidelines, which represent best practices that, while not legally required, are advisable for optimal driving behavior.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The Reasoning Agent proceeds to check for compliance with mandatory rules. If the current action violates any mandatory rule, the agent concludes that the action is non-compliant; otherwise, it is marked as compliant. The model then evaluates safety by checking both mandatory rules and safety guidelines, if any safety guidelines are retrieved. If the action complies with both, it is marked as safe; otherwise, it is marked as unsafe.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The Reasoning Agent outputs a binary compliance and safety decision for each action in the action set, along with a clear explanation referencing each applicable rule, detailing why the action complies or does not comply to increase the interpretability of the reasoning process. The framework then selects the actions that are marked as both compliant and safe as the final output for decision-making. An example output of the Reasoning Agent is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F3" title="Figure 3 ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">EXPERIMENTS</span>
</h2>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S4.F3.g1" src="extracted/5906163/figures/result_hypo_v3.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.4.2" style="font-size:90%;">Pipeline of processing the selected scenario “Left turn at the intersection with steady green circle”. The correct action is labeled in <span class="ltx_text ltx_font_bold" id="S4.F3.4.2.1" style="color:#009900;">green</span> background.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To validate our proposed method and access its effectiveness in leveraging regulation for decision-making, we develop a comprehensive benchmark that contains hypothesized and real-world scenarios as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F3" title="Figure 3 ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">3</span></a>. Hypothesized scenarios offer greater diversity, while real-world data experiment demonstrates the framework’s practical performance in real driving conditions. We primarily evaluated scenarios in the region of Boston City.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Traffic Regulation Retrieval (TRR) Agent and RAG</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The document collection we used in the TRR Agent follows the architecture shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S3.F2" title="Figure 2 ‣ III-A Retrieval-Augmented Generation with Traffic Regulations ‣ III Method ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">2</span></a> includes:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Boston City Traffic Rules and Regulations</span>: Article IV (One-Way Regulations) and V (Operation of Vehicles).</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Massachusetts State General Law</span>: Chapter 89 (Law of the road), section 1 to section 12.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Massachusetts State Driver’s Manual</span>: Chapter 4 (Rules of the Road) and Chapter 5 (Special Driving Situations).</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Massachusetts State Selected Court Cases</span>: Twelve selected traffic violation cases among the state.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Selected US Traffic Norms</span>: Ten selected driving norms without regulation violation.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">We used the model “text-embedding-ada-002” from OpenAI for the paragraph level retrieval with a threshold of 0.28 and “paraphrase-MiniLM-L6-v2” from SentenceTransformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib34" title="">34</a>]</cite> for the sentence level retrieval and collected top-5 retrieved sentences.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Hypothesized Scenarios</span>
</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="525" id="S4.F4.g1" src="extracted/5906163/figures/table_result_hypo_new.png" width="550"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.4.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.5.2" style="font-size:90%;">Selected results from the experiment on hypothesized Scenarios. Correct reasoning is labeled in <span class="ltx_text ltx_font_bold" id="S4.F4.5.2.1" style="color:#009900;">green</span> and incorrect ones are labeled in <span class="ltx_text ltx_font_bold" id="S4.F4.5.2.2" style="color:#B80000;">red</span>. Our scenario-action reasoning accuracy achieves 100% out of 76 pairs and our decision-making accuracy is 100% out of 30 scenarios, compared to the baseline of 76%.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The hypothesized scenarios, described in text format, include 30 situations that cover both common scenes—such as turning or passing intersections—and rare cases like passing a stopped school bus on a divided road or yielding to an emergency vehicle approaching from behind, which are generally not covered by real-world datasets. These scenarios were identified manually by the researchers by reviewing Boston’s regulation codes and driving manual, as they are potentially challenging for human or autonomous drivers.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S4.F5.g1" src="extracted/5906163/figures/real_world_boston_4_v1.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.5.2" style="font-size:90%;">Inference results on real-world scenarios from nuScenes dataset. Actions in <span class="ltx_text ltx_font_bold" id="S4.F5.5.2.1" style="color:#009900;">green</span> boxes represent the final decision-making outputs, which are both compliant and safe. Results demonstrate that our framework successfully retrieves relevant rules and interprets and assesses actions based on those rules. The correct action is labeled in <span class="ltx_text ltx_font_bold" id="S4.F5.5.2.2" style="color:#009900;">green</span> background. (Due to space constraints, some actions and reasoning details are omitted.)</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We assessed our framework’s performance across 30 hypothesized scenarios, with and without data from the TRR Agent, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F4" title="Figure 4 ‣ IV-B Hypothesized Scenarios ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">4</span></a>. In scenarios lacking specific local regulations or reliant on traffic norms, the LLM effectively uses its extensive pretrained knowledge to make correct decisions. However, in scenarios requiring adherence to detailed city-level or state-level regulations or judicial precedents, LLM alone is inadequate for ensuring safety. Incorporating TRR Agent, which includes localized regulations and judicial decisions, improves scenario-action reasoning accuracy from 82% to 100% and decision-making accuracy from 76% to 100% across these scenarios. This highlights the importance of integrating comprehensive legal and judicial information into the LLM framework to effectively tackle complex real-world driving situations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Real-world Scenarios</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To evaluate our framework’s performance on real-world data, we tested it on the nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#bib.bib35" title="">35</a>]</cite>, a real-world autonomous driving dataset that provides annotated sensor data from urban driving scenes collected in Boston and Singapore. Since it is not designed for traffic regulation-related tasks, it does not contain traffic rule annotations. To address this, we manually reviewed the camera images and selected samples with a strong relevance to traffic regulations, where the actions are more constrained or influenced by traffic rules. For each sample, we annotated compliance and safety labels for actions in the action set, identifying the compliant and safe action as the ground truth for decision-making output. To ensure a meaningful evaluation and avoid unbalance due to repeated or overly similar scenarios, we carefully selected samples where either different traffic rules applied or there were variations in how the same rule was applied due to scenario-specific factors directly related to the regulation. As a result, we identified 17 diverse samples from the validation set for evaluation, and our model produced correct outputs for 15 out of the 17 samples, along with accurate reasoning.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="161" id="S4.F6.g1" src="extracted/5906163/figures/singapore_case_v1.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Inference result on real-world scenarios from Singapore. The framework’s output aligns with Singapore’s traffic regulations, which differ from those in Boston. This demonstrates that our model can effectively adapt to different regions with varying regulations.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Examples of output from our framework are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F5" title="Figure 5 ‣ IV-B Hypothesized Scenarios ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">5</span></a>. In (a) and (b), we present two scenarios: a crosswalk without pedestrians and a crosswalk with pedestrians. For the crosswalk without pedestrians, the model outputs “compliant but not safe” when the ego vehicle accelerates forward, aligning with common sense. In the scenario with pedestrians, accelerating forward is not compliant with traffic rules, and our framework correctly identifies this, outputting the correct compliance judgment. These two examples demonstrate our model’s ability to reason based on multiple environmental factors, accurately adjusting its evaluation according to changes in the scenario. In (c), we further demonstrate a case where multiple traffic elements and rules must be considered simultaneously. In this scenario, the vehicle is turning right at a red traffic light where there is no “No Turn on Red” sign, making the turn legally permissible. However, a pedestrian is crossing the crosswalk in front of the vehicle, requiring the vehicle to yield. Thus, turning right without yielding is non-compliant with traffic regulations. As shown in the final output, our model successfully identifies this and outputs “non-compliant” for the case. In (d), we present a case where the ego vehicle is approaching a work zone and should reduce its speed, which our model successfully identifies, outputting the action “go forward with deceleration.” This is a scenario that previous rule-based methods struggle to handle, as they typically select only key rules due to the need for manually hand-crafting the rules, often omitting specific cases such as regulations for work zones.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04759v1#S4.F6" title="Figure 6 ‣ IV-C Real-world Scenarios ‣ IV EXPERIMENTS ‣ Driving with Regulation: Interpretable Decision-Making for Autonomous Vehicles with Retrieval-Augmented Reasoning via LLM"><span class="ltx_text ltx_ref_tag">6</span></a>, we demonstrate a case in Singapore to show that our model can adapt to different regions with ease. In this scenario, the ego vehicle attempts to turn right at a red traffic light. While right turns on red are legal in Boston, they are illegal in Singapore. As shown, our model correctly outputs “non-compliant,” aligning with Singapore’s traffic regulations. Unlike previous rule-based approaches, which require re-creating rules for each new region, our model seamlessly adapts to the new scenario by simply switching the traffic regulation documents from Boston’s to Singapore’s.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">CONCLUSION</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper introduces an interpretable LLM-driven, traffic rules-aware decision-making framework that integrates a Traffic Rules Retrieval (TRR) Agent and a Reasoning Agent. Experiments conducted on both hypothesized and real-world scenarios confirm the strong performance of our approach and its seamless adaptation to different regions. We believe this framework will markedly improve vehicle safety and reliability in autonomous driving systems, enhancing trust among regulators and the public. Future work will expand the testing of the framework to more regions and diversify our test scenarios. Additionally, developing a comprehensive real-world dataset for traffic rule-related tasks is important for future research and advancements in the field.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Zhao, W. Zhao, B. Deng, Z. Wang, F. Zhang, W. Zheng, W. Cao, J. Nan, Y. Lian, and A. F. Burke, “Autonomous driving system: A comprehensive survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Expert Systems with Applications</em>, p. 122836, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X. Han, Z. Meng, X. Xia, X. Liao, Y. He, Z. Zheng, Y. Wang, H. Xiang, Z. Zhou, L. Gao, <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Foundation intelligence for smart infrastructure services in transportation 5.0,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">IEEE Transactions on Intelligent Vehicles</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, “A survey on trajectory-prediction methods for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Intelligent Vehicles</em>, vol. 7, no. 3, pp. 652–674, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Li, S. Z. Zhao, C. Xu, C. Tang, C. Li, M. Ding, M. Tomizuka, and W. Zhan, “Pre-training on synthetic driving data for trajectory prediction,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, “Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Q. Li, Z. Peng, L. Feng, Z. Liu, C. Duan, W. Mo, and B. Zhou, “Scenarionet: Open-source platform for large-scale traffic scenario simulation and modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
W. Xiao, N. Mehdipour, A. Collin, A. Y. Bin-Nun, E. Frazzoli, R. D. Tebbens, and C. Belta, “Rule-based optimal control for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems</em>, 2021, pp. 143–154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Manas, S. Zwicklbauer, and A. Paschke, “Robust traffic rules and knowledge representation for conflict resolution in autonomous driving.” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">RuleML+ RR (Companion)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Sun, C. M. Poskitt, J. Sun, Y. Chen, and Z. Yang, “Lawbreaker: An approach for specifying traffic laws and fuzzing autonomous vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering</em>, 2022, pp. 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Li, Y. Bai, P. Cai, L. Wen, D. Fu, B. Zhang, X. Yang, X. Cai, T. Ma, J. Guo, X. Gao, M. Dou, Y. Li, B. Shi, Y. Liu, L. He, and Y. Qiao, “Towards knowledge-driven autonomous driving,” 2023. [Online]. Available: https://arxiv.org/abs/2312.04316

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Bae, S. Joo, J. Pyo, J.-S. Yoon, K. Lee, and T.-Y. Kuc, “Finite state machine based vehicle system for autonomous driving in urban environments,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2020 20th International Conference on Control, Automation and Systems (ICCAS)</em>, pp. 1181–1186, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID:227278676

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T. G. Tadewos, L. Shamgah, and A. Karimoddini, “Automatic safe behaviour tree synthesis for autonomous agents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">2019 IEEE 58th Conference on Decision and Control (CDC)</em>, 2019, pp. 2776–2781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Maierhofer, A.-K. Rettinger, E. C. Mayer, and M. Althoff, “Formalization of interstate traffic rules in temporal logic,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2020 IEEE Intelligent Vehicles Symposium (IV)</em>, 2020, pp. 752–759.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Wen, Y. Liu, G. C. R. Bethala, Z. Peng, H. Lin, Y.-S. Liu, and Y. Fang, “Enhancing socially-aware robot navigation through bidirectional natural language conversation,” 2024. [Online]. Available: https://arxiv.org/abs/2409.04965

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Z. Li, Z. Gou, X. Zhang, Z. Liu, S. Li, Y. Hu, C. Ling, Z. Zhang, and L. Zhao, “Teg-db: A comprehensive dataset and benchmark of textual-edge graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2406.10310</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Zhang, D. Jin, C. Gu, F. Hong, Z. Cai, J. Huang, C. Zhang, X. Guo, L. Yang, Y. He, <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>, “Large motion model for unified multi-modal motion generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">arXiv preprint arXiv:2404.01284</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, P. Luo, A. Geiger, and H. Li, “Drivelm: Driving with graph visual question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2312.14150</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Malla, C. Choi, I. Dwivedi, J. H. Choi, and J. Li, “Drama: Joint risk localization and captioning in driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023, pp. 1043–1052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Wei, Z. Wang, Y. Lu, C. Xu, C. Liu, H. Zhao, S. Chen, and Y. Wang, “Editable scene simulation for autonomous driving via collaborative llm-agents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B. Li, Y. Wang, J. Mao, B. Ivanovic, S. Veer, K. Leung, and M. Pavone, “Driving everywhere with large language model policy adaptation,” 2024. [Online]. Available: https://arxiv.org/abs/2402.05932

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Mao, J. Ye, Y. Qian, M. Pavone, and Y. Wang, “A language agent for autonomous driving,” 2024. [Online]. Available: https://arxiv.org/abs/2311.10813

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” 2021. [Online]. Available: https://arxiv.org/abs/2005.11401

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, D. De Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. Rae, E. Elsen, and L. Sifre, “Improving language models by retrieving from trillions of tokens,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 39th International Conference on Machine Learning</em>, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162.   PMLR, 17–23 Jul 2022, pp. 2206–2240. [Online]. Available: https://proceedings.mlr.press/v162/borgeaud22a.html

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman, “Webgpt: Browser-assisted question-answering with human feedback,” 2022. [Online]. Available: https://arxiv.org/abs/2112.09332

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
X. Wang, X. Qi, P. Wang, and J. Yang, “Decision making framework for autonomous vehicles driving behavior in complex scenarios via hierarchical state machine,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Autonomous Intelligent Systems</em>, vol. 1, pp. 1–12, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23, no. 6, pp. 4909–4926, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1812.03079</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Tang, K. Yuan, S. Li, S. Yang, Z. Zhou, and Y. Huang, “Personalized decision-making and control for automated vehicles based on generative adversarial imitation learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2023, pp. 4806–4812.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K. Yuan, Y. Huang, S. Yang, Z. Zhou, Y. Wang, D. Cao, and H. Chen, “Evolutionary decision-making and planning for autonomous driving based on safe and rational exploration and exploitation,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Engineering</em>, vol. 33, pp. 108–120, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Mao, Y. Qian, H. Zhao, and Y. Wang, “Gpt-driver: Learning to drive with gpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2310.01415</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Ding and T. Suel, “Faster top-k document retrieval using block-max indexes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</em>, 2011, pp. 993–1002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
R. Aly, D. Hiemstra, and T. Demeester, “Taily: Shard selection using the tail of score distributions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</em>, 2013, pp. 673–682.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Johnson, M. Douze, and H. Jégou, “Billion-scale similarity search with gpus,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1702.08734</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>.   Association for Computational Linguistics, 11 2019. [Online]. Available: http://arxiv.org/abs/1908.10084

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 11 621–11 631.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 05:26:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
