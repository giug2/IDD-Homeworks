<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09698] Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation</title><meta property="og:description" content="Recent advances in Large Language Models (LLMs) have demonstrated significant potential in the field of Recommendation Systems (RSs). Most existing studies have focused on converting user behavior logs into textual proâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09698">

<!--Generated on Thu Sep  5 13:48:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuyang Ye<sup id="id1.1.id1" class="ltx_sup">1</sup>,Zhi Zheng<sup id="id2.2.id2" class="ltx_sup">2</sup>, Yishan Shen<sup id="id3.3.id3" class="ltx_sup">3</sup>, Tianshu Wang<sup id="id4.4.id4" class="ltx_sup">4</sup>, Hengruo Zhang<sup id="id5.5.id5" class="ltx_sup">4</sup>, 
<br class="ltx_break">Peijun Zhu<sup id="id6.6.id6" class="ltx_sup">5</sup>, Runlong Yu<sup id="id7.7.id7" class="ltx_sup">6</sup>, Kai Zhang<sup id="id8.8.id8" class="ltx_sup">2</sup>, Hui Xiong<sup id="id9.9.id9" class="ltx_sup">7</sup>
</span><span class="ltx_author_notes">Corresponding Author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Recent advances in Large Language Models (LLMs) have demonstrated significant potential in the field of Recommendation Systems (RSs). Most existing studies have focused on converting user behavior logs into textual prompts and leveraging techniques such as prompt tuning to enable LLMs for recommendation tasks. Meanwhile, research interest has recently grown in multimodal recommendation systems that integrate data from images, text, and other sources using modality fusion techniques. This introduces new challenges to the existing LLM-based recommendation paradigm which relies solely on text modality information. Moreover, although Multimodal Large Language Models (MLLMs) capable of processing multi-modal inputs have emerged, how to equip MLLMs with multi-modal recommendation capabilities remains largely unexplored. To this end, in this paper, we propose the Multimodal Large Language Model-enhanced Multimodal Sequential Recommendation (MLLM-MSR) model. To capture the dynamic user preference, we design a two-stage user preference summarization method. Specifically, we first utilize an MLLM-based item-summarizer to extract image feature given an item and convert the image into text. Then, we employ a recurrent user preference summarization generation paradigm to capture the dynamic changes in user preferences based on an LLM-based user-summarizer. Finally, to enable the MLLM for multi-modal recommendation task, we propose to fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT) techniques. Extensive evaluations across various datasets validate the effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt to the evolving dynamics of user preferences.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The development of Large Language Models (LLMs) has significantly enhanced the capacity for natural language understandingÂ <cite class="ltx_cite ltx_citemacro_citep">(Floridi and Chiriatti <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Achiam etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Touvron etÂ al. <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, which has been instrumental in advancing recommendation systems (RSs). LLMs have demonstrated remarkable improvements in processing complex user preferences due to its strong semantic understanding and summarization ability. These attributes significantly enhance personalization and accuracy in recommendations<cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al. <a href="#bib.bib45" title="" class="ltx_ref">2023</a>; Zhang etÂ al. <a href="#bib.bib49" title="" class="ltx_ref">2024a</a>; Ren etÂ al. <a href="#bib.bib37" title="" class="ltx_ref">2024</a>)</cite>, particularly in Sequential Recommendations (SRs) where extracting long historical preferences is crucialÂ <cite class="ltx_cite ltx_citemacro_citep">(Hou etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2023</a>; Zheng etÂ al. <a href="#bib.bib54" title="" class="ltx_ref">2024</a>; Zhai etÂ al. <a href="#bib.bib48" title="" class="ltx_ref">2023</a>; Li etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Simultaneously, beyond solely modeling textual information, there has been a growing interest in leveraging multimodal informationÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib31" title="" class="ltx_ref">2023b</a>)</cite>. Techniques such as multimodal fusion and gated multimodal units have been utilized to integrate data from various sourcesâ€”images, videos, and audioâ€”enriching the context for recommendations. This offers a deeper understanding of user-item interactions and naturally leads to the exploration of Multimodal Large Language Models (MLLMs) for enhancing multimodal recommendation systemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Tong <a href="#bib.bib51" title="" class="ltx_ref">2020</a>; Liu etÂ al. <a href="#bib.bib33" title="" class="ltx_ref">2021b</a>; Zhou and Miao <a href="#bib.bib55" title="" class="ltx_ref">2024</a>; Kim etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite>. MLLMs merge multimodal information into a unified textual semantic space, enhancing the systemâ€™s ability to understand and interpret complex data inputs, thereby can significantly improving recommendation accuracyÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib32" title="" class="ltx_ref">2024c</a>; Zhang etÂ al. <a href="#bib.bib50" title="" class="ltx_ref">2024b</a>)</cite>. The application of MLLMs in sequential recommender systems presents a promising avenue for dynamically adapting to user preferences and handling the intricate interplay of multimodal data, which holds considerable untapped potential.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">However, integrating Multimodal Large Language Models (MLLMs) into multimodal sequential recommendation systems introduces a set of notable challenges. First, the inherent complexity and computational demands of processing sequential multimodal data, particularly with multiple ordered image inputs, significantly constrain the scalability and efficiency of these systems <cite class="ltx_cite ltx_citemacro_citep">(Yue etÂ al. <a href="#bib.bib47" title="" class="ltx_ref">2024</a>; Koh, Fried, and Salakhutdinov <a href="#bib.bib23" title="" class="ltx_ref">2024</a>)</cite>. Moreover, conventional MLLMs often exhibit limitations in comprehending the temporal dynamics of user interactions and preferences, particularly in the context of sequential multimodal interactions <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Liu etÂ al. <a href="#bib.bib32" title="" class="ltx_ref">2024c</a>)</cite>. This critical limitation undermines the systemsâ€™ capacity to accurately capture and reflect the evolving nature of user interests over time. Furthermore, fine-tuning multimodal large language models (MLLMs) for specific recommendation scenarios while avoiding overfitting and preserving the generalizability gained during pre-training presents a significant challenge <cite class="ltx_cite ltx_citemacro_citep">(Borisov etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2022</a>; Yin etÂ al. <a href="#bib.bib46" title="" class="ltx_ref">2023</a>; Li, Zhang, and Chen <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. These hurdles underscore the need for innovative approaches that can navigate the complexities of multimodal sequential data, ensuring that MLLMs can be effectively leveraged to enhance recommendation systems.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">To address these challenges, this paper introduces the Multimodal Large Language Model-enhanced Multimodal Sequential Recommendation (MLLM-MSR), a pioneering approach that leverages the capabilities of MLLMs to enhance and integrate multimodal item data effectively. Specifically, we introduce a Multimodal User Preferences Inference approach, which merges traditional multimodal fusion with sequence modeling techniques with MLLMs. Initially, we employ MLLMs to transform visual and textual data of each item into a cohesive textual description, preserving the integrity of the information as demonstrated by a preliminary study. Subsequently, utilizing the enriched item information processed through the MLLM, we develop an innovative LLM-based recurrent method to infer user preferences, capturing the temporal dynamics of these preferences. This method addresses the above mentioned challenges in processing sequential image inputs by harnessing superior text process capabilities of LLMs and improves the interpretability of recommendation compared with traditional representation based approaches, by providing detailed user preference. Further, we fine-tune an MLLM to function as a recommender, utilizing a carefully designed set of prompts that integrate this enriched item data, inferred user preferences, and the ground-truth of user-item interactions. This process of Supervised Fine-Tuning (SFT) on an open-source MLLM, equips the model with the ability to accurately match user preferences with potential items, thereby enhancing the personalization and accuracy of recommendations. To validate the effectiveness of MLLM-MSR, we conduct extensive experiments across three publicly available datasets from various domains, which confirm the superior performance of our approach. The major contributions of this paper are summarized as follows:</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">To best of our knowledge, our work is the first attempt to fine-tune multimodal large models to address the challenges of sequential multimodal recommendation, where our fine-tune strategies achieving significant improvements in recommendation performance.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">We introduce a novel image summarizing method based on MLLMs to recurrently summarize user preferences on multi modality, facilitating a deeper understanding of user interactions and interests over time.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Our approach is extensively validated across various datasets, demonstrating its effectiveness in enhancing the accuracy and interpretability of recommendations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Work</h2>

<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Multimodal Sequential Recommendation</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">Sequential Recommenders (SRs) have progressed from matrix-based models to sophisticated neural architectures. Initially, Factorizing Personalized Markov Chains (FPMC) integrated matrix factorization with Markov chains to model sequential behavior <cite class="ltx_cite ltx_citemacro_citep">(Rendle, Freudenthaler, and Schmidt-Thieme <a href="#bib.bib38" title="" class="ltx_ref">2010</a>)</cite>. The transition to neural models started with GRU4Rec, which employed gated recurrent units for session-based recommendations <cite class="ltx_cite ltx_citemacro_citep">(Hidasi etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2015</a>)</cite>. Subsequently, SASRec utilized self-attention mechanisms to handle long-term dependencies in user-item interactions <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>, and BERT4Rec introduced transformers to SRs, significantly enhancing performance with deep bidirectional training <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="Sx2.SSx1.p2" class="ltx_para">
<p id="Sx2.SSx1.p2.1" class="ltx_p">Furthermore, the evolution of multimodal information-enhanced SRs has leveraged additional contextual information to improve recommendation quality. Fusion methods in SRs are categorized into early, late, and hybrid approaches <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. Early fusion techniques involve invasive methods that integrate various modalities at the input level, enhancing initial feature representation through techniques like concatenation and gating <cite class="ltx_cite ltx_citemacro_citep">(Tang and Wang <a href="#bib.bib42" title="" class="ltx_ref">2018</a>; Sun etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2019</a>; Lei, Ji, and Li <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. Besides, non-invasive early fusion employs attention mechanisms to merge multiple attributes before processing <cite class="ltx_cite ltx_citemacro_citep">(Rendle etÂ al. <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Liu etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2021a</a>)</cite>. In contrast, late fusion merges feature sequences from separate modules before the final stage, as evidenced in <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a href="#bib.bib52" title="" class="ltx_ref">2019</a>; Ji etÂ al. <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Du etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite>. Hybrid fusion methods flexibly combine modality fusion and sequential modeling by evaluating inter-modality relationships, offering a versatile fusion strategy <cite class="ltx_cite ltx_citemacro_citep">(Zhao, Lee, and Wu <a href="#bib.bib53" title="" class="ltx_ref">2020</a>; Hu etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">LLM for Recommendation</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">The integration of Large Language Models (LLMs) into recommendation systems has been profoundly influenced by foundational models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> and GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, which demonstrated the potential of LLMs in processing vast amounts of textual data to understand user behaviors deeply. This foundation has been expanded upon by subsequent models like BERT4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> and innovations such as RLMRec <cite class="ltx_cite ltx_citemacro_citep">(Ren etÂ al. <a href="#bib.bib37" title="" class="ltx_ref">2024</a>)</cite>, which tailor LLM capabilities to generate personalized, context-aware recommendations by analyzing detailed user-item interactions.</p>
</div>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">In the current landscape, LLM applications in recommendation systems are categorized into three main approaches: embeddings-based, token-based, and direct model applicationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al. <a href="#bib.bib45" title="" class="ltx_ref">2023</a>; Cao etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2024</a>)</cite>. Embeddings-based applications, such as <cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2022</a>; Liu etÂ al. <a href="#bib.bib30" title="" class="ltx_ref">2024b</a>)</cite>, use LLMs to extract rich feature representations from item and user data, enhancing the systemâ€™s understanding of user preferences. Token-based approaches, highlighted in works like <cite class="ltx_cite ltx_citemacro_citep">(Zhai etÂ al. <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite>, focus on generating tokens that capture semantic meanings and potential user preferences, integrating this data into the recommendation logic. Lastly, direct model applications <cite class="ltx_cite ltx_citemacro_citep">(Hou etÂ al. <a href="#bib.bib16" title="" class="ltx_ref">2024</a>; Geng etÂ al. <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> involve using LLMs as end-to-end solutions where the models directly generate recommendations based on user queries and profiles, offering a streamlined and potentially more powerful system architecture. Additionally, there has been an emergence of multimodal LLM-based recommendation frameworks designed to handle scenarios involving multimodal information. These frameworks integrate and process diverse data types such as images, text, and video to enhance the recommendation systemâ€™s accuracy and user experienceÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib32" title="" class="ltx_ref">2024c</a>; Zhang etÂ al. <a href="#bib.bib49" title="" class="ltx_ref">2024a</a>)</cite>.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Preliminary</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">In this section, we will give the definition of our research problem and conduct a preliminary study to discuss the effectiveness of image summarizing approach.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Problem Definition</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.8" class="ltx_p">We first introduce the problem formulation of the Sequential Multimodal Recommendation problem. The dataset used in this work contains the interaction records between users and items. Given a user <math id="Sx3.SSx1.p1.1.m1.1" class="ltx_Math" alttext="u" display="inline"><semantics id="Sx3.SSx1.p1.1.m1.1a"><mi id="Sx3.SSx1.p1.1.m1.1.1" xref="Sx3.SSx1.p1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.1.m1.1b"><ci id="Sx3.SSx1.p1.1.m1.1.1.cmml" xref="Sx3.SSx1.p1.1.m1.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.1.m1.1c">u</annotation></semantics></math>, let us first define the historical user behavior sequence of <math id="Sx3.SSx1.p1.2.m2.1" class="ltx_Math" alttext="u" display="inline"><semantics id="Sx3.SSx1.p1.2.m2.1a"><mi id="Sx3.SSx1.p1.2.m2.1.1" xref="Sx3.SSx1.p1.2.m2.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.2.m2.1b"><ci id="Sx3.SSx1.p1.2.m2.1.1.cmml" xref="Sx3.SSx1.p1.2.m2.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.2.m2.1c">u</annotation></semantics></math> as <math id="Sx3.SSx1.p1.3.m3.3" class="ltx_Math" alttext="S_{u}=[I^{1}_{u},\ldots,I^{n}_{u}]" display="inline"><semantics id="Sx3.SSx1.p1.3.m3.3a"><mrow id="Sx3.SSx1.p1.3.m3.3.3" xref="Sx3.SSx1.p1.3.m3.3.3.cmml"><msub id="Sx3.SSx1.p1.3.m3.3.3.4" xref="Sx3.SSx1.p1.3.m3.3.3.4.cmml"><mi id="Sx3.SSx1.p1.3.m3.3.3.4.2" xref="Sx3.SSx1.p1.3.m3.3.3.4.2.cmml">S</mi><mi id="Sx3.SSx1.p1.3.m3.3.3.4.3" xref="Sx3.SSx1.p1.3.m3.3.3.4.3.cmml">u</mi></msub><mo id="Sx3.SSx1.p1.3.m3.3.3.3" xref="Sx3.SSx1.p1.3.m3.3.3.3.cmml">=</mo><mrow id="Sx3.SSx1.p1.3.m3.3.3.2.2" xref="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml"><mo stretchy="false" id="Sx3.SSx1.p1.3.m3.3.3.2.2.3" xref="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml">[</mo><msubsup id="Sx3.SSx1.p1.3.m3.2.2.1.1.1" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.cmml"><mi id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.2" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.2.cmml">I</mi><mi id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.3" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.3.cmml">u</mi><mn id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.3" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.3.cmml">1</mn></msubsup><mo id="Sx3.SSx1.p1.3.m3.3.3.2.2.4" xref="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="Sx3.SSx1.p1.3.m3.1.1" xref="Sx3.SSx1.p1.3.m3.1.1.cmml">â€¦</mi><mo id="Sx3.SSx1.p1.3.m3.3.3.2.2.5" xref="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml">,</mo><msubsup id="Sx3.SSx1.p1.3.m3.3.3.2.2.2" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.cmml"><mi id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.2" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.2.cmml">I</mi><mi id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.3" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.3.cmml">u</mi><mi id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.3" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.3.cmml">n</mi></msubsup><mo stretchy="false" id="Sx3.SSx1.p1.3.m3.3.3.2.2.6" xref="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.3.m3.3b"><apply id="Sx3.SSx1.p1.3.m3.3.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3"><eq id="Sx3.SSx1.p1.3.m3.3.3.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.3"></eq><apply id="Sx3.SSx1.p1.3.m3.3.3.4.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.4"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.3.3.4.1.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.4">subscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.3.3.4.2.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.4.2">ğ‘†</ci><ci id="Sx3.SSx1.p1.3.m3.3.3.4.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.4.3">ğ‘¢</ci></apply><list id="Sx3.SSx1.p1.3.m3.3.3.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2"><apply id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1">subscript</csymbol><apply id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1">superscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.2">ğ¼</ci><cn type="integer" id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.2.3">1</cn></apply><ci id="Sx3.SSx1.p1.3.m3.2.2.1.1.1.3.cmml" xref="Sx3.SSx1.p1.3.m3.2.2.1.1.1.3">ğ‘¢</ci></apply><ci id="Sx3.SSx1.p1.3.m3.1.1.cmml" xref="Sx3.SSx1.p1.3.m3.1.1">â€¦</ci><apply id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2">subscript</csymbol><apply id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.1.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2">superscript</csymbol><ci id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.2.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.2">ğ¼</ci><ci id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.2.3">ğ‘›</ci></apply><ci id="Sx3.SSx1.p1.3.m3.3.3.2.2.2.3.cmml" xref="Sx3.SSx1.p1.3.m3.3.3.2.2.2.3">ğ‘¢</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.3.m3.3c">S_{u}=[I^{1}_{u},\ldots,I^{n}_{u}]</annotation></semantics></math>, where <math id="Sx3.SSx1.p1.4.m4.1" class="ltx_Math" alttext="I^{i}" display="inline"><semantics id="Sx3.SSx1.p1.4.m4.1a"><msup id="Sx3.SSx1.p1.4.m4.1.1" xref="Sx3.SSx1.p1.4.m4.1.1.cmml"><mi id="Sx3.SSx1.p1.4.m4.1.1.2" xref="Sx3.SSx1.p1.4.m4.1.1.2.cmml">I</mi><mi id="Sx3.SSx1.p1.4.m4.1.1.3" xref="Sx3.SSx1.p1.4.m4.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.4.m4.1b"><apply id="Sx3.SSx1.p1.4.m4.1.1.cmml" xref="Sx3.SSx1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="Sx3.SSx1.p1.4.m4.1.1.1.cmml" xref="Sx3.SSx1.p1.4.m4.1.1">superscript</csymbol><ci id="Sx3.SSx1.p1.4.m4.1.1.2.cmml" xref="Sx3.SSx1.p1.4.m4.1.1.2">ğ¼</ci><ci id="Sx3.SSx1.p1.4.m4.1.1.3.cmml" xref="Sx3.SSx1.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.4.m4.1c">I^{i}</annotation></semantics></math> represents the <math id="Sx3.SSx1.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx3.SSx1.p1.5.m5.1a"><mi id="Sx3.SSx1.p1.5.m5.1.1" xref="Sx3.SSx1.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.5.m5.1b"><ci id="Sx3.SSx1.p1.5.m5.1.1.cmml" xref="Sx3.SSx1.p1.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.5.m5.1c">i</annotation></semantics></math>-th item with which the user has interacted, through actions such as clicking, purchasing, or watching, and <math id="Sx3.SSx1.p1.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="Sx3.SSx1.p1.6.m6.1a"><mi id="Sx3.SSx1.p1.6.m6.1.1" xref="Sx3.SSx1.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.6.m6.1b"><ci id="Sx3.SSx1.p1.6.m6.1.1.cmml" xref="Sx3.SSx1.p1.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.6.m6.1c">n</annotation></semantics></math> denotes the length of the user behavior sequence. In addition, each item corresponds to a textual description <math id="Sx3.SSx1.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="Sx3.SSx1.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p1.7.m7.1.1" xref="Sx3.SSx1.p1.7.m7.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.7.m7.1b"><ci id="Sx3.SSx1.p1.7.m7.1.1.cmml" xref="Sx3.SSx1.p1.7.m7.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.7.m7.1c">\mathcal{W}</annotation></semantics></math> and a image <math id="Sx3.SSx1.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="Sx3.SSx1.p1.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="Sx3.SSx1.p1.8.m8.1.1" xref="Sx3.SSx1.p1.8.m8.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p1.8.m8.1b"><ci id="Sx3.SSx1.p1.8.m8.1.1.cmml" xref="Sx3.SSx1.p1.8.m8.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p1.8.m8.1c">\mathcal{I}</annotation></semantics></math> (e.g., product diagram, video cover). Consequently, our problem can be formulated as follows.</p>
</div>
<div id="Thmdefinition1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinition1.1.1.1" class="ltx_text ltx_font_bold">Definition 1</span></span><span id="Thmdefinition1.2.2" class="ltx_text ltx_font_bold"> (Multimodal Sequential Recommendation)</span>
</h6>
<div id="Thmdefinition1.p1" class="ltx_para">
<p id="Thmdefinition1.p1.7" class="ltx_p"><span id="Thmdefinition1.p1.7.7" class="ltx_text ltx_font_italic">Given a user <math id="Thmdefinition1.p1.1.1.m1.1" class="ltx_Math" alttext="u" display="inline"><semantics id="Thmdefinition1.p1.1.1.m1.1a"><mi id="Thmdefinition1.p1.1.1.m1.1.1" xref="Thmdefinition1.p1.1.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.1.1.m1.1b"><ci id="Thmdefinition1.p1.1.1.m1.1.1.cmml" xref="Thmdefinition1.p1.1.1.m1.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.1.1.m1.1c">u</annotation></semantics></math> with the corresponding historical behavior sequence <math id="Thmdefinition1.p1.2.2.m2.1" class="ltx_Math" alttext="S^{u}" display="inline"><semantics id="Thmdefinition1.p1.2.2.m2.1a"><msup id="Thmdefinition1.p1.2.2.m2.1.1" xref="Thmdefinition1.p1.2.2.m2.1.1.cmml"><mi id="Thmdefinition1.p1.2.2.m2.1.1.2" xref="Thmdefinition1.p1.2.2.m2.1.1.2.cmml">S</mi><mi id="Thmdefinition1.p1.2.2.m2.1.1.3" xref="Thmdefinition1.p1.2.2.m2.1.1.3.cmml">u</mi></msup><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.2.2.m2.1b"><apply id="Thmdefinition1.p1.2.2.m2.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.2.2.m2.1.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1">superscript</csymbol><ci id="Thmdefinition1.p1.2.2.m2.1.1.2.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1.2">ğ‘†</ci><ci id="Thmdefinition1.p1.2.2.m2.1.1.3.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.2.2.m2.1c">S^{u}</annotation></semantics></math>, including both textual and visual data, and a candidate item <math id="Thmdefinition1.p1.3.3.m3.1" class="ltx_Math" alttext="I_{c}" display="inline"><semantics id="Thmdefinition1.p1.3.3.m3.1a"><msub id="Thmdefinition1.p1.3.3.m3.1.1" xref="Thmdefinition1.p1.3.3.m3.1.1.cmml"><mi id="Thmdefinition1.p1.3.3.m3.1.1.2" xref="Thmdefinition1.p1.3.3.m3.1.1.2.cmml">I</mi><mi id="Thmdefinition1.p1.3.3.m3.1.1.3" xref="Thmdefinition1.p1.3.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.3.3.m3.1b"><apply id="Thmdefinition1.p1.3.3.m3.1.1.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.3.3.m3.1.1.1.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1">subscript</csymbol><ci id="Thmdefinition1.p1.3.3.m3.1.1.2.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1.2">ğ¼</ci><ci id="Thmdefinition1.p1.3.3.m3.1.1.3.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.3.3.m3.1c">I_{c}</annotation></semantics></math>, the objective of the Multimodal Sequential Recommendation is to predict the probability of next interacted item <math id="Thmdefinition1.p1.4.4.m4.1" class="ltx_Math" alttext="I^{n+1}_{u}" display="inline"><semantics id="Thmdefinition1.p1.4.4.m4.1a"><msubsup id="Thmdefinition1.p1.4.4.m4.1.1" xref="Thmdefinition1.p1.4.4.m4.1.1.cmml"><mi id="Thmdefinition1.p1.4.4.m4.1.1.2.2" xref="Thmdefinition1.p1.4.4.m4.1.1.2.2.cmml">I</mi><mi id="Thmdefinition1.p1.4.4.m4.1.1.3" xref="Thmdefinition1.p1.4.4.m4.1.1.3.cmml">u</mi><mrow id="Thmdefinition1.p1.4.4.m4.1.1.2.3" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.cmml"><mi id="Thmdefinition1.p1.4.4.m4.1.1.2.3.2" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.2.cmml">n</mi><mo id="Thmdefinition1.p1.4.4.m4.1.1.2.3.1" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.1.cmml">+</mo><mn id="Thmdefinition1.p1.4.4.m4.1.1.2.3.3" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.4.4.m4.1b"><apply id="Thmdefinition1.p1.4.4.m4.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.4.4.m4.1.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1">subscript</csymbol><apply id="Thmdefinition1.p1.4.4.m4.1.1.2.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.4.4.m4.1.1.2.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1">superscript</csymbol><ci id="Thmdefinition1.p1.4.4.m4.1.1.2.2.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2.2">ğ¼</ci><apply id="Thmdefinition1.p1.4.4.m4.1.1.2.3.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3"><plus id="Thmdefinition1.p1.4.4.m4.1.1.2.3.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.1"></plus><ci id="Thmdefinition1.p1.4.4.m4.1.1.2.3.2.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.2">ğ‘›</ci><cn type="integer" id="Thmdefinition1.p1.4.4.m4.1.1.2.3.3.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2.3.3">1</cn></apply></apply><ci id="Thmdefinition1.p1.4.4.m4.1.1.3.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.4.4.m4.1c">I^{n+1}_{u}</annotation></semantics></math> (for example, the probability of clicking) with the candidate item <math id="Thmdefinition1.p1.5.5.m5.1" class="ltx_Math" alttext="I^{c}_{u}" display="inline"><semantics id="Thmdefinition1.p1.5.5.m5.1a"><msubsup id="Thmdefinition1.p1.5.5.m5.1.1" xref="Thmdefinition1.p1.5.5.m5.1.1.cmml"><mi id="Thmdefinition1.p1.5.5.m5.1.1.2.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.2.cmml">I</mi><mi id="Thmdefinition1.p1.5.5.m5.1.1.3" xref="Thmdefinition1.p1.5.5.m5.1.1.3.cmml">u</mi><mi id="Thmdefinition1.p1.5.5.m5.1.1.2.3" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.5.5.m5.1b"><apply id="Thmdefinition1.p1.5.5.m5.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.5.5.m5.1.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1">subscript</csymbol><apply id="Thmdefinition1.p1.5.5.m5.1.1.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1"><csymbol cd="ambiguous" id="Thmdefinition1.p1.5.5.m5.1.1.2.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1">superscript</csymbol><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.2">ğ¼</ci><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3">ğ‘</ci></apply><ci id="Thmdefinition1.p1.5.5.m5.1.1.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.5.5.m5.1c">I^{c}_{u}</annotation></semantics></math> for the user <math id="Thmdefinition1.p1.6.6.m6.1" class="ltx_Math" alttext="u" display="inline"><semantics id="Thmdefinition1.p1.6.6.m6.1a"><mi id="Thmdefinition1.p1.6.6.m6.1.1" xref="Thmdefinition1.p1.6.6.m6.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.6.6.m6.1b"><ci id="Thmdefinition1.p1.6.6.m6.1.1.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.6.6.m6.1c">u</annotation></semantics></math>, denoted as <math id="Thmdefinition1.p1.7.7.m7.1" class="ltx_Math" alttext="g_{u}:I_{c}\rightarrow\mathbb{R}" display="inline"><semantics id="Thmdefinition1.p1.7.7.m7.1a"><mrow id="Thmdefinition1.p1.7.7.m7.1.1" xref="Thmdefinition1.p1.7.7.m7.1.1.cmml"><msub id="Thmdefinition1.p1.7.7.m7.1.1.2" xref="Thmdefinition1.p1.7.7.m7.1.1.2.cmml"><mi id="Thmdefinition1.p1.7.7.m7.1.1.2.2" xref="Thmdefinition1.p1.7.7.m7.1.1.2.2.cmml">g</mi><mi id="Thmdefinition1.p1.7.7.m7.1.1.2.3" xref="Thmdefinition1.p1.7.7.m7.1.1.2.3.cmml">u</mi></msub><mo lspace="0.278em" rspace="0.278em" id="Thmdefinition1.p1.7.7.m7.1.1.1" xref="Thmdefinition1.p1.7.7.m7.1.1.1.cmml">:</mo><mrow id="Thmdefinition1.p1.7.7.m7.1.1.3" xref="Thmdefinition1.p1.7.7.m7.1.1.3.cmml"><msub id="Thmdefinition1.p1.7.7.m7.1.1.3.2" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2.cmml"><mi id="Thmdefinition1.p1.7.7.m7.1.1.3.2.2" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2.2.cmml">I</mi><mi id="Thmdefinition1.p1.7.7.m7.1.1.3.2.3" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2.3.cmml">c</mi></msub><mo stretchy="false" id="Thmdefinition1.p1.7.7.m7.1.1.3.1" xref="Thmdefinition1.p1.7.7.m7.1.1.3.1.cmml">â†’</mo><mi id="Thmdefinition1.p1.7.7.m7.1.1.3.3" xref="Thmdefinition1.p1.7.7.m7.1.1.3.3.cmml">â„</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.7.7.m7.1b"><apply id="Thmdefinition1.p1.7.7.m7.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1"><ci id="Thmdefinition1.p1.7.7.m7.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.1">:</ci><apply id="Thmdefinition1.p1.7.7.m7.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.2"><csymbol cd="ambiguous" id="Thmdefinition1.p1.7.7.m7.1.1.2.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.2">subscript</csymbol><ci id="Thmdefinition1.p1.7.7.m7.1.1.2.2.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.2.2">ğ‘”</ci><ci id="Thmdefinition1.p1.7.7.m7.1.1.2.3.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.2.3">ğ‘¢</ci></apply><apply id="Thmdefinition1.p1.7.7.m7.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3"><ci id="Thmdefinition1.p1.7.7.m7.1.1.3.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.1">â†’</ci><apply id="Thmdefinition1.p1.7.7.m7.1.1.3.2.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="Thmdefinition1.p1.7.7.m7.1.1.3.2.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2">subscript</csymbol><ci id="Thmdefinition1.p1.7.7.m7.1.1.3.2.2.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2.2">ğ¼</ci><ci id="Thmdefinition1.p1.7.7.m7.1.1.3.2.3.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.2.3">ğ‘</ci></apply><ci id="Thmdefinition1.p1.7.7.m7.1.1.3.3.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.7.7.m7.1c">g_{u}:I_{c}\rightarrow\mathbb{R}</annotation></semantics></math>.</span></p>
</div>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Effectiveness of Multiple Images Summary</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">As highlighted in the Introduction, current multimodal large language models (MLLMs) face challenges in processing multiple image inputs, limiting their effectiveness for sequential multimodal analysis. To overcome this problem, we introduce an image summary approach that leverages MLLMs to convert and summarize image content. The efficacy of this technique is evaluated using the basic sequential recommender, GRU4Rec, on real-world datasets (detailed in the Experiment section). In our approach, we employed simple prompts like â€Please summarize the imageâ€ with LLaVAÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib29" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib28" title="" class="ltx_ref">2024a</a>)</cite> to generate image summaries. These summaries were transformed into latent vectors using BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, which then fed into the GRU4Rec model. This method is benchmarked against direct image representations from VGG19Â <cite class="ltx_cite ltx_citemacro_citep">(Simonyan <a href="#bib.bib40" title="" class="ltx_ref">2014</a>)</cite>, assessing performance via the AUC metric.</p>
</div>
<figure id="Sx3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of GRU4Rec with Different Inputs</figcaption>
<table id="Sx3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T1.1.1.1" class="ltx_tr">
<th id="Sx3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Dataset</th>
<th id="Sx3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Microlens</th>
<th id="Sx3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Baby</th>
<th id="Sx3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Games</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T1.1.2.1" class="ltx_tr">
<td id="Sx3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Image Summary</td>
<td id="Sx3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.7281</td>
<td id="Sx3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.7318</td>
<td id="Sx3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.7451</td>
</tr>
<tr id="Sx3.T1.1.3.2" class="ltx_tr">
<td id="Sx3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b">VGG19 Features</td>
<td id="Sx3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">0.7154</td>
<td id="Sx3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b">0.7383</td>
<td id="Sx3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b">0.7532</td>
</tr>
</tbody>
</table>
</figure>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">The performance is detailed in TableÂ <a href="#Sx3.T1" title="Table 1 â€£ Effectiveness of Multiple Images Summary â€£ Preliminary â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Results show that using image summaries allows the GRU4Rec model to perform comparably to direct processing with VGG19, confirming that our image summary approach preserves necessary semantic information in sequential modeling. This preliminary validation underscores the effectiveness of our method in addressing the challenges associated with processing multiple ordered images.</p>
</div>
<figure id="Sx3.F1" class="ltx_figure"><img src="/html/2408.09698/assets/figures/framework.png" id="Sx3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The schematic framework of MLLM-MSR</figcaption>
</figure>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Technical Details</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">This section will introduce the technical details of our proposed MLLM-MSR framework, which contains two main components as Multimodal User Preferences Inference and Tuning MLLM based Recommender, illustrated as FigureÂ <a href="#Sx3.F1" title="Figure 1 â€£ Effectiveness of Multiple Images Summary â€£ Preliminary â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="Sx4.F2" class="ltx_figure"><img src="/html/2408.09698/assets/figures/Preference.png" id="Sx4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="393" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of recurrent user preference inference.</figcaption>
</figure>
<figure id="Sx4.F3" class="ltx_figure"><img src="/html/2408.09698/assets/figures/recommender.png" id="Sx4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example of MLLM-based sequential recommendation</figcaption>
</figure>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Multimodal User Preferences Inference</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">In the context of sequential recommendation, a common approach is to learn user representations and predict future interactions with candidate items via calculating affinity scores. Unlike traditional methods that utilize embeddings, LLMs typically analyze user preferences and interaction probabilities directly at token level. This section will detail how our method employs Multimodal Large Language Models (MLLMs) to specifically address challenges associated with multimodal recommendation scenarios.</p>
</div>
<section id="Sx4.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Multimodal Item Summarization</h4>

<div id="Sx4.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p1.1" class="ltx_p">To effectively predict user preferences, it is crucial to analyze historical item sequences. In multimodal recommendation scenarios, handling multiple image inputs presents a significant challenge for MLLMs, especially in maintaining the sequence of these inputs and aligning textual information with corresponding images. To overcome these issues, we propose a Multimodal Item Summarization approach, which simplifies the processing by summarizing multimodal information of images into unified textual descriptions by designing effective prompts to integrate the multimodal data of items.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p2.1" class="ltx_p">Our prompt design adheres to foundational methods of multimodal information fusion. Item information can be separated into textual descriptions and image. Hence, In the initial phase, distinct prompts (i.e., text summarization and image description prompt) are used to guide Multimodal Large Language Models (MLLMs) to process these modalities independently, to ensure a more thorough comprehension and detailed feature extraction from each modality, ensuring nuanced characteristics often missed in unified analyses are captured. To ensure both modalities contribute equally to item modeling, the outputs of text summarization and image description are calibrated to similar lengths.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p3.1" class="ltx_p">After independently analyzing each modality, our design integrates insights from both textual and visual information using a fusion prompt. This approach aligns with traditional multimodal recommendation strategies that emphasize synthesizing diverse data types to create a comprehensive item profile, enhances the multifaceted understanding of the item <cite class="ltx_cite ltx_citemacro_citep">(BaltruÅ¡aitis, Ahuja, and Morency <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Huang, Xiao, and Yu <a href="#bib.bib19" title="" class="ltx_ref">2019</a>; Zhang and Tong <a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Recurrent User Preference Inference</h4>

<div id="Sx4.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p1.1" class="ltx_p">In the Sequential Multimodal Recommendation framework, achieving detailed personalization relies on an accurate understanding of user preferences. The advent of Multimodal Large Language Models (MLLMs) marks a significant advancement in understanding multimodal information. However, as we introduced above, they are struggle in dealing with sequential multimodal data. Although our multimodal item summarization method effectively integrates multimodal information into a unified item summary, this complexity still leads to unstable and random outputs when the historical sequence becomes long, leading to excessively long prompts. Consequently, this results in suboptimal performance in sequential recommendation systems.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p2.1" class="ltx_p">To address these challenges, our method, inspired by Recurrent Neural Networks (RNNs), employs prompted sequence modeling to iteratively capture user preferences through interaction sequences. In RNNs, each output is influenced by both the current input and the previous state, facilitating contextual awareness across sequences. We segment item interactions into several blocks, each covering interactions within a defined session, converting long multimodal sequences into concise textual narratives that sequentially represent the userâ€™s historical interactions. This segmentation enables our approach to dynamically represent user preferences, effectively overcoming the limitations of MLLMs in processing sequential and multimodal data. By incorporating prompt-driven modules in each session, our method integrates insights from previous interactions to refine the understanding of current user preferences continuously. This iterative process is essential for accurately capturing the dynamic of user preferences and offers more interpretable descriptions than traditional representation-based models, enhancing the potential for detailed case studies.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p3.1" class="ltx_p">Specifically, as we have initially generated item multimodal summaries for each item, we pair these summaries with prompts that guide the LLMs to infer user preferences based on a sequential narrative. For example, the initial prompt in the first block is designed to summarize the userâ€™s initial interests from a chronological list of item interactions at the first timestamp. Subsequently, at following session, the prompt for updating the summarized preference is showed as FigureÂ <a href="#Sx4.F2" title="Figure 2 â€£ Technical Details â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our approach uses prompted sequence modeling to iteratively understand user preferences through detailed analysis of each interaction, thus effectively manage challenges with long, multimodal sequences.</p>
</div>
</section>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Tuning MLLM based Recommender</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">After gathering user preferences using the methods described above, we can propose a supervised fine-tuning of an open-sourced Multimodal Large Language Model, such as LLaVA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf</span></span></span>, which excels at understanding images. This model would be used to build a multimodal recommender system through SFT. In line with the definition of sequential recommendation, given a user-item interaction, the MLLM-based recommender system utilizes the prompt contained the obtained user preferences, the textual description and image of the given item and the designed system instruction prompt to predict the probability that the user will interact with the candidate item. Specifically, the prompt designed for the tuned MLLM recommender module is illustrated in FigureÂ <a href="#Sx4.F3" title="Figure 3 â€£ Technical Details â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where we restrict the output to only include â€™yesâ€™ or â€™noâ€™ to avoid irrelevant information about the predicted label. Thus, the probability of item interaction can be calculated from the probability score of the predicted first new token as follows:</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<table id="Sx4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx4.E1.m1.3" class="ltx_Math" alttext="p=\frac{p(\text{'yes'})}{p(\text{'yes'})+p(\text{'no'})}" display="block"><semantics id="Sx4.E1.m1.3a"><mrow id="Sx4.E1.m1.3.4" xref="Sx4.E1.m1.3.4.cmml"><mi id="Sx4.E1.m1.3.4.2" xref="Sx4.E1.m1.3.4.2.cmml">p</mi><mo id="Sx4.E1.m1.3.4.1" xref="Sx4.E1.m1.3.4.1.cmml">=</mo><mfrac id="Sx4.E1.m1.3.3" xref="Sx4.E1.m1.3.3.cmml"><mrow id="Sx4.E1.m1.1.1.1" xref="Sx4.E1.m1.1.1.1.cmml"><mi id="Sx4.E1.m1.1.1.1.3" xref="Sx4.E1.m1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx4.E1.m1.1.1.1.2" xref="Sx4.E1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx4.E1.m1.1.1.1.4.2" xref="Sx4.E1.m1.1.1.1.1a.cmml"><mo stretchy="false" id="Sx4.E1.m1.1.1.1.4.2.1" xref="Sx4.E1.m1.1.1.1.1a.cmml">(</mo><mtext id="Sx4.E1.m1.1.1.1.1" xref="Sx4.E1.m1.1.1.1.1.cmml">â€™yesâ€™</mtext><mo stretchy="false" id="Sx4.E1.m1.1.1.1.4.2.2" xref="Sx4.E1.m1.1.1.1.1a.cmml">)</mo></mrow></mrow><mrow id="Sx4.E1.m1.3.3.3" xref="Sx4.E1.m1.3.3.3.cmml"><mrow id="Sx4.E1.m1.3.3.3.4" xref="Sx4.E1.m1.3.3.3.4.cmml"><mi id="Sx4.E1.m1.3.3.3.4.2" xref="Sx4.E1.m1.3.3.3.4.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx4.E1.m1.3.3.3.4.1" xref="Sx4.E1.m1.3.3.3.4.1.cmml">â€‹</mo><mrow id="Sx4.E1.m1.3.3.3.4.3.2" xref="Sx4.E1.m1.2.2.2.1a.cmml"><mo stretchy="false" id="Sx4.E1.m1.3.3.3.4.3.2.1" xref="Sx4.E1.m1.2.2.2.1a.cmml">(</mo><mtext id="Sx4.E1.m1.2.2.2.1" xref="Sx4.E1.m1.2.2.2.1.cmml">â€™yesâ€™</mtext><mo stretchy="false" id="Sx4.E1.m1.3.3.3.4.3.2.2" xref="Sx4.E1.m1.2.2.2.1a.cmml">)</mo></mrow></mrow><mo id="Sx4.E1.m1.3.3.3.3" xref="Sx4.E1.m1.3.3.3.3.cmml">+</mo><mrow id="Sx4.E1.m1.3.3.3.5" xref="Sx4.E1.m1.3.3.3.5.cmml"><mi id="Sx4.E1.m1.3.3.3.5.2" xref="Sx4.E1.m1.3.3.3.5.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx4.E1.m1.3.3.3.5.1" xref="Sx4.E1.m1.3.3.3.5.1.cmml">â€‹</mo><mrow id="Sx4.E1.m1.3.3.3.5.3.2" xref="Sx4.E1.m1.3.3.3.2a.cmml"><mo stretchy="false" id="Sx4.E1.m1.3.3.3.5.3.2.1" xref="Sx4.E1.m1.3.3.3.2a.cmml">(</mo><mtext id="Sx4.E1.m1.3.3.3.2" xref="Sx4.E1.m1.3.3.3.2.cmml">â€™noâ€™</mtext><mo stretchy="false" id="Sx4.E1.m1.3.3.3.5.3.2.2" xref="Sx4.E1.m1.3.3.3.2a.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="Sx4.E1.m1.3b"><apply id="Sx4.E1.m1.3.4.cmml" xref="Sx4.E1.m1.3.4"><eq id="Sx4.E1.m1.3.4.1.cmml" xref="Sx4.E1.m1.3.4.1"></eq><ci id="Sx4.E1.m1.3.4.2.cmml" xref="Sx4.E1.m1.3.4.2">ğ‘</ci><apply id="Sx4.E1.m1.3.3.cmml" xref="Sx4.E1.m1.3.3"><divide id="Sx4.E1.m1.3.3.4.cmml" xref="Sx4.E1.m1.3.3"></divide><apply id="Sx4.E1.m1.1.1.1.cmml" xref="Sx4.E1.m1.1.1.1"><times id="Sx4.E1.m1.1.1.1.2.cmml" xref="Sx4.E1.m1.1.1.1.2"></times><ci id="Sx4.E1.m1.1.1.1.3.cmml" xref="Sx4.E1.m1.1.1.1.3">ğ‘</ci><ci id="Sx4.E1.m1.1.1.1.1a.cmml" xref="Sx4.E1.m1.1.1.1.4.2"><mtext id="Sx4.E1.m1.1.1.1.1.cmml" xref="Sx4.E1.m1.1.1.1.1">â€™yesâ€™</mtext></ci></apply><apply id="Sx4.E1.m1.3.3.3.cmml" xref="Sx4.E1.m1.3.3.3"><plus id="Sx4.E1.m1.3.3.3.3.cmml" xref="Sx4.E1.m1.3.3.3.3"></plus><apply id="Sx4.E1.m1.3.3.3.4.cmml" xref="Sx4.E1.m1.3.3.3.4"><times id="Sx4.E1.m1.3.3.3.4.1.cmml" xref="Sx4.E1.m1.3.3.3.4.1"></times><ci id="Sx4.E1.m1.3.3.3.4.2.cmml" xref="Sx4.E1.m1.3.3.3.4.2">ğ‘</ci><ci id="Sx4.E1.m1.2.2.2.1a.cmml" xref="Sx4.E1.m1.3.3.3.4.3.2"><mtext id="Sx4.E1.m1.2.2.2.1.cmml" xref="Sx4.E1.m1.2.2.2.1">â€™yesâ€™</mtext></ci></apply><apply id="Sx4.E1.m1.3.3.3.5.cmml" xref="Sx4.E1.m1.3.3.3.5"><times id="Sx4.E1.m1.3.3.3.5.1.cmml" xref="Sx4.E1.m1.3.3.3.5.1"></times><ci id="Sx4.E1.m1.3.3.3.5.2.cmml" xref="Sx4.E1.m1.3.3.3.5.2">ğ‘</ci><ci id="Sx4.E1.m1.3.3.3.2a.cmml" xref="Sx4.E1.m1.3.3.3.5.3.2"><mtext id="Sx4.E1.m1.3.3.3.2.cmml" xref="Sx4.E1.m1.3.3.3.2">â€™noâ€™</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.E1.m1.3c">p=\frac{p(\text{'yes'})}{p(\text{'yes'})+p(\text{'no'})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="Sx4.SSx2.p3" class="ltx_para">
<p id="Sx4.SSx2.p3.1" class="ltx_p">To construct a multimodal sequential recommender system based on Multimodal Large Language Models (MLLMs), we implement supervised fine-tuning to optimize the model parameters. This fine-tuning process involves adjusting the model to minimize the discrepancy between predicted and actual user interactions. Our dataset construction strategy employs negative sampling, a commonly used training technique in recommendation systems, wherein each positive user-item interaction is coupled with multiple negative samples representing items with which the user did not interact. This methodology aids the model in distinguishing between relevant and irrelevant items through contrastive learning, thereby improving its predictive accuracy.</p>
</div>
<div id="Sx4.SSx2.p4" class="ltx_para">
<p id="Sx4.SSx2.p4.1" class="ltx_p">The model is trained on a dataset comprising sequences of user-item interactions, with each interaction encapsulated as a sequence of user preferences, item descriptions, and images. The fine-tuning leverages the next token prediction paradigm, training the model to predict the subsequent token in a sequence based on preceding tokens. This ensures the generation of coherent and contextually pertinent outputs from the input sequences. The supervised fine-tuning loss function is defined as:</p>
</div>
<div id="Sx4.SSx2.p5" class="ltx_para">
<table id="Sx4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx4.E2.m1.2" class="ltx_Math" alttext="L=-\sum_{i=1}^{L}\log P(v_{i}|v_{&lt;i},\mathcal{I})," display="block"><semantics id="Sx4.E2.m1.2a"><mrow id="Sx4.E2.m1.2.2.1" xref="Sx4.E2.m1.2.2.1.1.cmml"><mrow id="Sx4.E2.m1.2.2.1.1" xref="Sx4.E2.m1.2.2.1.1.cmml"><mi id="Sx4.E2.m1.2.2.1.1.3" xref="Sx4.E2.m1.2.2.1.1.3.cmml">L</mi><mo id="Sx4.E2.m1.2.2.1.1.2" xref="Sx4.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="Sx4.E2.m1.2.2.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.cmml"><mo id="Sx4.E2.m1.2.2.1.1.1a" xref="Sx4.E2.m1.2.2.1.1.1.cmml">âˆ’</mo><mrow id="Sx4.E2.m1.2.2.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.cmml"><munderover id="Sx4.E2.m1.2.2.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" id="Sx4.E2.m1.2.2.1.1.1.1.2.2.2" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.2" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.1" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.3" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="Sx4.E2.m1.2.2.1.1.1.1.2.3" xref="Sx4.E2.m1.2.2.1.1.1.1.2.3.cmml">L</mi></munderover><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.cmml"><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.cmml"><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.3.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="Sx4.E2.m1.2.2.1.1.1.1.1.3a" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.cmml">â¡</mo><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.3.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="Sx4.E2.m1.2.2.1.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml">v</mi><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mrow id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="Sx4.E2.m1.1.1" xref="Sx4.E2.m1.1.1.cmml">â„</mi></mrow></mrow><mo stretchy="false" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="Sx4.E2.m1.2.2.1.2" xref="Sx4.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.E2.m1.2b"><apply id="Sx4.E2.m1.2.2.1.1.cmml" xref="Sx4.E2.m1.2.2.1"><eq id="Sx4.E2.m1.2.2.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.2"></eq><ci id="Sx4.E2.m1.2.2.1.1.3.cmml" xref="Sx4.E2.m1.2.2.1.1.3">ğ¿</ci><apply id="Sx4.E2.m1.2.2.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1"><minus id="Sx4.E2.m1.2.2.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1"></minus><apply id="Sx4.E2.m1.2.2.1.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1"><apply id="Sx4.E2.m1.2.2.1.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx4.E2.m1.2.2.1.1.1.1.2.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="Sx4.E2.m1.2.2.1.1.1.1.2.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx4.E2.m1.2.2.1.1.1.1.2.2.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="Sx4.E2.m1.2.2.1.1.1.1.2.2.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3"><eq id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="Sx4.E2.m1.2.2.1.1.1.1.2.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.2.3">ğ¿</ci></apply><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1"><times id="Sx4.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.2"></times><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3"><log id="Sx4.E2.m1.2.2.1.1.1.1.1.3.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.1"></log><ci id="Sx4.E2.m1.2.2.1.1.1.1.1.3.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.2">ğ‘£</ci><ci id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><list id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><apply id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="Sx4.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="Sx4.E2.m1.1.1.cmml" xref="Sx4.E2.m1.1.1">â„</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.E2.m1.2c">L=-\sum_{i=1}^{L}\log P(v_{i}|v_{&lt;i},\mathcal{I}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx4.SSx2.p5.5" class="ltx_p">where <math id="Sx4.SSx2.p5.1.m1.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="Sx4.SSx2.p5.1.m1.1a"><msub id="Sx4.SSx2.p5.1.m1.1.1" xref="Sx4.SSx2.p5.1.m1.1.1.cmml"><mi id="Sx4.SSx2.p5.1.m1.1.1.2" xref="Sx4.SSx2.p5.1.m1.1.1.2.cmml">v</mi><mi id="Sx4.SSx2.p5.1.m1.1.1.3" xref="Sx4.SSx2.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p5.1.m1.1b"><apply id="Sx4.SSx2.p5.1.m1.1.1.cmml" xref="Sx4.SSx2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx2.p5.1.m1.1.1.1.cmml" xref="Sx4.SSx2.p5.1.m1.1.1">subscript</csymbol><ci id="Sx4.SSx2.p5.1.m1.1.1.2.cmml" xref="Sx4.SSx2.p5.1.m1.1.1.2">ğ‘£</ci><ci id="Sx4.SSx2.p5.1.m1.1.1.3.cmml" xref="Sx4.SSx2.p5.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p5.1.m1.1c">v_{i}</annotation></semantics></math> represents the <math id="Sx4.SSx2.p5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx4.SSx2.p5.2.m2.1a"><mi id="Sx4.SSx2.p5.2.m2.1.1" xref="Sx4.SSx2.p5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p5.2.m2.1b"><ci id="Sx4.SSx2.p5.2.m2.1.1.cmml" xref="Sx4.SSx2.p5.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p5.2.m2.1c">i</annotation></semantics></math>-th token of the prompt text, <math id="Sx4.SSx2.p5.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="Sx4.SSx2.p5.3.m3.1a"><mi id="Sx4.SSx2.p5.3.m3.1.1" xref="Sx4.SSx2.p5.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p5.3.m3.1b"><ci id="Sx4.SSx2.p5.3.m3.1.1.cmml" xref="Sx4.SSx2.p5.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p5.3.m3.1c">L</annotation></semantics></math> denotes the prompt length and <math id="Sx4.SSx2.p5.4.m4.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="Sx4.SSx2.p5.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Sx4.SSx2.p5.4.m4.1.1" xref="Sx4.SSx2.p5.4.m4.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p5.4.m4.1b"><ci id="Sx4.SSx2.p5.4.m4.1.1.cmml" xref="Sx4.SSx2.p5.4.m4.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p5.4.m4.1c">\mathcal{I}</annotation></semantics></math> is the given image. The probability <math id="Sx4.SSx2.p5.5.m5.2" class="ltx_Math" alttext="P(v_{i}|v_{&lt;i},\mathcal{I})" display="inline"><semantics id="Sx4.SSx2.p5.5.m5.2a"><mrow id="Sx4.SSx2.p5.5.m5.2.2" xref="Sx4.SSx2.p5.5.m5.2.2.cmml"><mi id="Sx4.SSx2.p5.5.m5.2.2.3" xref="Sx4.SSx2.p5.5.m5.2.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="Sx4.SSx2.p5.5.m5.2.2.2" xref="Sx4.SSx2.p5.5.m5.2.2.2.cmml">â€‹</mo><mrow id="Sx4.SSx2.p5.5.m5.2.2.1.1" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.cmml"><mo stretchy="false" id="Sx4.SSx2.p5.5.m5.2.2.1.1.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.cmml">(</mo><mrow id="Sx4.SSx2.p5.5.m5.2.2.1.1.1" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.cmml"><msub id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.cmml"><mi id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.2.cmml">v</mi><mi id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.3" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.2.cmml">|</mo><mrow id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.2.cmml"><msub id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.cmml"><mi id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.2.cmml">v</mi><mrow id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.cmml"><mi id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.2.cmml"></mi><mo id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.1" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.3" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.2" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="Sx4.SSx2.p5.5.m5.1.1" xref="Sx4.SSx2.p5.5.m5.1.1.cmml">â„</mi></mrow></mrow><mo stretchy="false" id="Sx4.SSx2.p5.5.m5.2.2.1.1.3" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p5.5.m5.2b"><apply id="Sx4.SSx2.p5.5.m5.2.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2"><times id="Sx4.SSx2.p5.5.m5.2.2.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.2"></times><ci id="Sx4.SSx2.p5.5.m5.2.2.3.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.3">ğ‘ƒ</ci><apply id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1"><csymbol cd="latexml" id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.2">conditional</csymbol><apply id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.1.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3">subscript</csymbol><ci id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.2">ğ‘£</ci><ci id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.3.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.3.3">ğ‘–</ci></apply><list id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1"><apply id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.1.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.2">ğ‘£</ci><apply id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3"><lt id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.1.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.2.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.2">absent</csymbol><ci id="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.3.cmml" xref="Sx4.SSx2.p5.5.m5.2.2.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="Sx4.SSx2.p5.5.m5.1.1.cmml" xref="Sx4.SSx2.p5.5.m5.1.1">â„</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p5.5.m5.2c">P(v_{i}|v_{&lt;i},\mathcal{I})</annotation></semantics></math> is calculated using MLLMs within the next token prediction framework, which maximizes the likelihood of the ground truth tokens given the prompt. This ensures the model learns to accurately predict the subsequent token based on the provided context, which is critical for generating precise and contextually aware recommendations. Specifically, we employ LoRAÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> to adhere to a parameter-efficient fine-tuning framework (PEFT), which accelerates the training process.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">In this section, we detail the comprehensive experiment to validate the effectiveness of our proposed Multimodal Large Language Model for Sequential Multimodal Recommendation (MLLM-MSR).</p>
</div>
<figure id="Sx5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The Statistics of Datasets</figcaption>
<table id="Sx5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx5.T2.1.1.1" class="ltx_tr">
<th id="Sx5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Dataset</th>
<td id="Sx5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Microlens</td>
<td id="Sx5.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Amazon-Baby</td>
<td id="Sx5.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Amazon-Game</td>
</tr>
<tr id="Sx5.T2.1.2.2" class="ltx_tr">
<th id="Sx5.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">#User</th>
<td id="Sx5.T2.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">25411</td>
<td id="Sx5.T2.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">41081</td>
<td id="Sx5.T2.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">38808</td>
</tr>
<tr id="Sx5.T2.1.3.3" class="ltx_tr">
<th id="Sx5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">#Item</th>
<td id="Sx5.T2.1.3.3.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">20276</td>
<td id="Sx5.T2.1.3.3.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">14393</td>
<td id="Sx5.T2.1.3.3.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">13379</td>
</tr>
<tr id="Sx5.T2.1.4.4" class="ltx_tr">
<th id="Sx5.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">#Interaction</th>
<td id="Sx5.T2.1.4.4.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">223263</td>
<td id="Sx5.T2.1.4.4.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">400876</td>
<td id="Sx5.T2.1.4.4.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">352136</td>
</tr>
<tr id="Sx5.T2.1.5.5" class="ltx_tr">
<th id="Sx5.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">#Avg Seqlen</th>
<td id="Sx5.T2.1.5.5.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.35</td>
<td id="Sx5.T2.1.5.5.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">13.65</td>
<td id="Sx5.T2.1.5.5.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">13.23</td>
</tr>
<tr id="Sx5.T2.1.6.6" class="ltx_tr">
<th id="Sx5.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">Sparsity</th>
<td id="Sx5.T2.1.6.6.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">99.96%</td>
<td id="Sx5.T2.1.6.6.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">99.93%</td>
<td id="Sx5.T2.1.6.6.4" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">99.93%</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Sx5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The performance of different methods.</figcaption>
<table id="Sx5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx5.T3.1.1.1" class="ltx_tr">
<th id="Sx5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"></th>
<th id="Sx5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="3">Microlens</th>
<th id="Sx5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="3">Video-Games</th>
<th id="Sx5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="3">Amazon-Baby</th>
</tr>
<tr id="Sx5.T3.1.2.2" class="ltx_tr">
<th id="Sx5.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"></th>
<th id="Sx5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">AUC</th>
<th id="Sx5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">HR@5</th>
<th id="Sx5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">MRR@5</th>
<th id="Sx5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">AUC</th>
<th id="Sx5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">HR@5</th>
<th id="Sx5.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">MRR@5</th>
<th id="Sx5.T3.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">AUC</th>
<th id="Sx5.T3.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">HR@5</th>
<th id="Sx5.T3.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.5pt;padding-bottom:0.5pt;">MRR@5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx5.T3.1.3.1" class="ltx_tr">
<th id="Sx5.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GRU4Rec</th>
<td id="Sx5.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">72.55</td>
<td id="Sx5.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">53.32</td>
<td id="Sx5.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">33.36</td>
<td id="Sx5.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">73.45</td>
<td id="Sx5.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">59.32</td>
<td id="Sx5.T3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">34.56</td>
<td id="Sx5.T3.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">73.68</td>
<td id="Sx5.T3.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.21</td>
<td id="Sx5.T3.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">35.81</td>
</tr>
<tr id="Sx5.T3.1.4.2" class="ltx_tr">
<th id="Sx5.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">SASRec</th>
<td id="Sx5.T3.1.4.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">74.02</td>
<td id="Sx5.T3.1.4.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.88</td>
<td id="Sx5.T3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">36.57</td>
<td id="Sx5.T3.1.4.2.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">71.06</td>
<td id="Sx5.T3.1.4.2.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">43.81</td>
<td id="Sx5.T3.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">23.94</td>
<td id="Sx5.T3.1.4.2.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">80.50</td>
<td id="Sx5.T3.1.4.2.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">70.09</td>
<td id="Sx5.T3.1.4.2.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">52.34</td>
</tr>
<tr id="Sx5.T3.1.5.3" class="ltx_tr">
<th id="Sx5.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">MGAT</th>
<td id="Sx5.T3.1.5.3.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">71.23</td>
<td id="Sx5.T3.1.5.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">42.37</td>
<td id="Sx5.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">30.84</td>
<td id="Sx5.T3.1.5.3.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">72.21</td>
<td id="Sx5.T3.1.5.3.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">48.71</td>
<td id="Sx5.T3.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">32.17</td>
<td id="Sx5.T3.1.5.3.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">73.19</td>
<td id="Sx5.T3.1.5.3.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">46.42</td>
<td id="Sx5.T3.1.5.3.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">31.92</td>
</tr>
<tr id="Sx5.T3.1.6.4" class="ltx_tr">
<th id="Sx5.T3.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">MMGCN</th>
<td id="Sx5.T3.1.6.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">73.35</td>
<td id="Sx5.T3.1.6.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">54.47</td>
<td id="Sx5.T3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">34.14</td>
<td id="Sx5.T3.1.6.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">74.91</td>
<td id="Sx5.T3.1.6.4.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">55.63</td>
<td id="Sx5.T3.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">35.19</td>
<td id="Sx5.T3.1.6.4.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">74.88</td>
<td id="Sx5.T3.1.6.4.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">57.13</td>
<td id="Sx5.T3.1.6.4.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">36.13</td>
</tr>
<tr id="Sx5.T3.1.7.5" class="ltx_tr">
<th id="Sx5.T3.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">GRU4Rec<sub id="Sx5.T3.1.7.5.1.1" class="ltx_sub">F</sub>
</th>
<td id="Sx5.T3.1.7.5.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">75.31</td>
<td id="Sx5.T3.1.7.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">60.28</td>
<td id="Sx5.T3.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">33.25</td>
<td id="Sx5.T3.1.7.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">75.79</td>
<td id="Sx5.T3.1.7.5.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">62.38</td>
<td id="Sx5.T3.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">36.39</td>
<td id="Sx5.T3.1.7.5.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">76.41</td>
<td id="Sx5.T3.1.7.5.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">63.84</td>
<td id="Sx5.T3.1.7.5.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">35.77</td>
</tr>
<tr id="Sx5.T3.1.8.6" class="ltx_tr">
<th id="Sx5.T3.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">SASRec<sub id="Sx5.T3.1.8.6.1.1" class="ltx_sub">F</sub>
</th>
<td id="Sx5.T3.1.8.6.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">77.69</td>
<td id="Sx5.T3.1.8.6.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">63.03</td>
<td id="Sx5.T3.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">37.42</td>
<td id="Sx5.T3.1.8.6.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">77.51</td>
<td id="Sx5.T3.1.8.6.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">66.64</td>
<td id="Sx5.T3.1.8.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">43.55</td>
<td id="Sx5.T3.1.8.6.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.94</td>
<td id="Sx5.T3.1.8.6.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">72.08</td>
<td id="Sx5.T3.1.8.6.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.32</td>
</tr>
<tr id="Sx5.T3.1.9.7" class="ltx_tr">
<th id="Sx5.T3.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">MMSR</th>
<td id="Sx5.T3.1.9.7.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">78.87</td>
<td id="Sx5.T3.1.9.7.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">69.85</td>
<td id="Sx5.T3.1.9.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">55.21</td>
<td id="Sx5.T3.1.9.7.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">79.01</td>
<td id="Sx5.T3.1.9.7.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">71.95</td>
<td id="Sx5.T3.1.9.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.52</td>
<td id="Sx5.T3.1.9.7.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.19</td>
<td id="Sx5.T3.1.9.7.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">70.34</td>
<td id="Sx5.T3.1.9.7.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.38</td>
</tr>
<tr id="Sx5.T3.1.10.8" class="ltx_tr">
<th id="Sx5.T3.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">Trans2D</th>
<td id="Sx5.T3.1.10.8.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">70.23</td>
<td id="Sx5.T3.1.10.8.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">51.91</td>
<td id="Sx5.T3.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">31.21</td>
<td id="Sx5.T3.1.10.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">72.20</td>
<td id="Sx5.T3.1.10.8.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">56.70</td>
<td id="Sx5.T3.1.10.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">39.28</td>
<td id="Sx5.T3.1.10.8.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">66.60</td>
<td id="Sx5.T3.1.10.8.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">28.94</td>
<td id="Sx5.T3.1.10.8.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">45.91</td>
</tr>
<tr id="Sx5.T3.1.11.9" class="ltx_tr">
<th id="Sx5.T3.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">LLaVa</th>
<td id="Sx5.T3.1.11.9.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">52.75</td>
<td id="Sx5.T3.1.11.9.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">28.21</td>
<td id="Sx5.T3.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">17.33</td>
<td id="Sx5.T3.1.11.9.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">53.57</td>
<td id="Sx5.T3.1.11.9.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">33.24</td>
<td id="Sx5.T3.1.11.9.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">18.73</td>
<td id="Sx5.T3.1.11.9.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">55.86</td>
<td id="Sx5.T3.1.11.9.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">34.82</td>
<td id="Sx5.T3.1.11.9.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">20.96</td>
</tr>
<tr id="Sx5.T3.1.12.10" class="ltx_tr">
<th id="Sx5.T3.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">TALLREC</th>
<td id="Sx5.T3.1.12.10.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.25</td>
<td id="Sx5.T3.1.12.10.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">71.23</td>
<td id="Sx5.T3.1.12.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">58.22</td>
<td id="Sx5.T3.1.12.10.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">81.79</td>
<td id="Sx5.T3.1.12.10.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">72.36</td>
<td id="Sx5.T3.1.12.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">57.38</td>
<td id="Sx5.T3.1.12.10.8" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">82.16</td>
<td id="Sx5.T3.1.12.10.9" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">74.31</td>
<td id="Sx5.T3.1.12.10.10" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">60.15</td>
</tr>
<tr id="Sx5.T3.1.13.11" class="ltx_tr">
<th id="Sx5.T3.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MLLM-MSR</th>
<td id="Sx5.T3.1.13.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.2.1" class="ltx_text ltx_font_bold">83.17</span></td>
<td id="Sx5.T3.1.13.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.3.1" class="ltx_text ltx_font_bold">78.23</span></td>
<td id="Sx5.T3.1.13.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.4.1" class="ltx_text ltx_font_bold">60.38</span></td>
<td id="Sx5.T3.1.13.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.5.1" class="ltx_text ltx_font_bold">84.39</span></td>
<td id="Sx5.T3.1.13.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.6.1" class="ltx_text ltx_font_bold">77.42</span></td>
<td id="Sx5.T3.1.13.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.7.1" class="ltx_text ltx_font_bold">63.25</span></td>
<td id="Sx5.T3.1.13.11.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.8.1" class="ltx_text ltx_font_bold">85.69</span></td>
<td id="Sx5.T3.1.13.11.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.9.1" class="ltx_text ltx_font_bold">79.58</span></td>
<td id="Sx5.T3.1.13.11.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="Sx5.T3.1.13.11.10.1" class="ltx_text ltx_font_bold">63.77</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Experimental Setup</h3>

<section id="Sx5.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Dataset Description</h4>

<div id="Sx5.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.SSSx1.p1.1" class="ltx_p">Our experimental evaluation utilized three open-source, real-world datasets from diverse recommendation system domains. These datasets include the <span id="Sx5.SSx1.SSSx1.p1.1.1" class="ltx_text ltx_font_italic">Microlens Dataset<cite class="ltx_cite ltx_citemacro_citep"><span id="Sx5.SSx1.SSSx1.p1.1.1.1.1" class="ltx_text ltx_font_upright">(</span>Ni etÂ al.<span id="Sx5.SSx1.SSSx1.p1.1.1.2.2.1.1" class="ltx_text ltx_font_upright"> </span><a href="#bib.bib36" title="" class="ltx_ref">2023</a><span id="Sx5.SSx1.SSSx1.p1.1.1.3.3" class="ltx_text ltx_font_upright">)</span></cite></span>, featuring user-item interactions, video introductions, and video cover images; the <span id="Sx5.SSx1.SSSx1.p1.1.2" class="ltx_text ltx_font_italic">Amazon-Baby Dataset</span>; and the <span id="Sx5.SSx1.SSSx1.p1.1.3" class="ltx_text ltx_font_italic">Amazon-Game Dataset<cite class="ltx_cite ltx_citemacro_citep"><span id="Sx5.SSx1.SSSx1.p1.1.3.1.1" class="ltx_text ltx_font_upright">(</span>He and McAuley<span id="Sx5.SSx1.SSSx1.p1.1.3.2.2.1.1" class="ltx_text ltx_font_upright"> </span><a href="#bib.bib13" title="" class="ltx_ref">2016</a>; McAuley etÂ al.<span id="Sx5.SSx1.SSSx1.p1.1.3.2.2.1.1" class="ltx_text ltx_font_upright"> </span><a href="#bib.bib34" title="" class="ltx_ref">2015</a><span id="Sx5.SSx1.SSSx1.p1.1.3.3.3" class="ltx_text ltx_font_upright">)</span></cite></span>, all of them contain user-item interactions, product descriptions, and images. These selections enabled a thorough analysis across different recommendation systems. We preprocessed each dataset by removing infrequent users and items to ensure user history sequences met our minimum length criteria. Additionally, we implemented a 1:1 ratio for negative sampling during training and a 1:20 ratio for evaluation. Further details on these datasets are provided in TableÂ <a href="#Sx5.T2" title="Table 2 â€£ Experiments â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="Sx5.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Baseline Methods</h4>

<div id="Sx5.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx5.SSx1.SSSx2.p1.1" class="ltx_p">To evaluate the effectiveness of our proposed LMM-MSR method, we selected some compared methods which can be categorized into following groups:</p>
</div>
<div id="Sx5.SSx1.SSSx2.p2" class="ltx_para">
<ul id="Sx5.I2" class="ltx_itemize">
<li id="Sx5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I2.i1.p1" class="ltx_para">
<p id="Sx5.I2.i1.p1.1" class="ltx_p"><span id="Sx5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Basic SR Models</span>: These models use item attributes including IDs and textual information. We selectively integrate the most effective information from these attributes to achieve optimal performance. <span id="Sx5.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">GRU4RecÂ <cite class="ltx_cite ltx_citemacro_citep">(Hidasi etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2015</a>)</cite></span>: Utilizes Gated Recurrent Units (GRU) to model the sequential dependencies between items. <span id="Sx5.I2.i1.p1.1.3" class="ltx_text ltx_font_italic">SASRecÂ <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite></span>: Employs a self-attention mechanism to capture long-term dependencies.</p>
</div>
</li>
<li id="Sx5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I2.i2.p1" class="ltx_para">
<p id="Sx5.I2.i2.p1.1" class="ltx_p"><span id="Sx5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Multimodal recommendation model</span>: <span id="Sx5.I2.i2.p1.1.2" class="ltx_text ltx_font_italic">MMGCNÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al. <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite></span>: Integrates multimodal features into a graph-based framework using a message-passing scheme. <span id="Sx5.I2.i2.p1.1.3" class="ltx_text ltx_font_italic">MGATÂ <cite class="ltx_cite ltx_citemacro_citep">(Monti, Bronstein, and Bresson <a href="#bib.bib35" title="" class="ltx_ref">2017</a>)</cite></span>: Employs a graph attention network to disentangle personal interests by modality.</p>
</div>
</li>
<li id="Sx5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I2.i3.p1" class="ltx_para">
<p id="Sx5.I2.i3.p1.1" class="ltx_p"><span id="Sx5.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Multimodal feature enhanced SR models</span>: <span id="Sx5.I2.i3.p1.1.2" class="ltx_text ltx_font_italic">GRU4Rec<sub id="Sx5.I2.i3.p1.1.2.1" class="ltx_sub">F</sub></span>, <span id="Sx5.I2.i3.p1.1.3" class="ltx_text ltx_font_italic">SASRec<sub id="Sx5.I2.i3.p1.1.3.1" class="ltx_sub">F</sub></span>: Adaptations of GRU4Rec and SASRec with multimodal feature enhancements. <span id="Sx5.I2.i3.p1.1.4" class="ltx_text ltx_font_italic">Trans2DÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao, Lee, and Wu <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite></span>: Utilizes holistic fusion to integrate features across different dimensions.
<span id="Sx5.I2.i3.p1.1.5" class="ltx_text ltx_font_italic">MMSRÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite></span>: Depolys a graph-based approach for adaptive fusion of multi-modal features, which dynamically adjusts the fusion order of modalities based on their sequential relationships.</p>
</div>
</li>
<li id="Sx5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I2.i4.p1" class="ltx_para">
<p id="Sx5.I2.i4.p1.1" class="ltx_p"><span id="Sx5.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">LLM based SR models</span>: <span id="Sx5.I2.i4.p1.1.2" class="ltx_text ltx_font_italic">TALLRECÂ <cite class="ltx_cite ltx_citemacro_citep">(Bao etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite></span>: Uses LLMs for sequence recommendation through SFT, exclusively processing textual inputs. <span id="Sx5.I2.i4.p1.1.3" class="ltx_text ltx_font_italic">LLaVA w/o SFT</span>: Utilizes LLaVA as the recommender without a specific fine-tuning for recommendation.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx5.SSx1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Metrics</h4>

<div id="Sx5.SSx1.SSSx3.p1" class="ltx_para">
<p id="Sx5.SSx1.SSSx3.p1.1" class="ltx_p">To assess the performance of baseline methods and our proposed MLLM-MSR for multimodal sequential recommendations, we employed AUC, HR@5, and NDCG@5 as evaluation metrics. To ensure a fair comparison, we standardized the size of the candidate item sets across all baseline methods and our approach.</p>
</div>
</section>
<section id="Sx5.SSx1.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Implementation Details</h4>

<div id="Sx5.SSx1.SSSx4.p1" class="ltx_para">
<p id="Sx5.SSx1.SSSx4.p1.1" class="ltx_p">Our experiments were performed on a Linux server equipped with eight A800 80GB GPUs. We utilized Llava-v1.6-mistral-7b for image description and recommendation tasks, and Llama3-8b-instructÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</span></span></span> for summarizing user preferences. For the Supervised Fine-Tuning (SFT) process, we employed the PyTorch Lightning library, using LoRA with a rank of 8. The optimization was handled by the AdamW optimizer with a learning rate of 2e-5 and a batch size of 1, setting gradient accumulation steps at 8 and epochs at 10. For distributed training, we implemented Deepspeed [28] with ZeRO stage 2. Additionally, we set the maximum token length for MLLMs at 512 and the number of items per block in recurrent preference inference at 3.</p>
</div>
</section>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Performance Analysis</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">The performance of the compared methods and our MLLM-MSR is presented in TableÂ <a href="#Sx5.T3" title="Table 3 â€£ Experiments â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where all results were obtained using 5-fold cross-validation and various random seeds, and achieved a 95% confidence level. It is evident that, in our evaluation, MLLM-MSR consistently outperforms all other metrics in terms of both classification and ranking, underscoring the personalization accuracy of our recommendation system. We also observed additional insights:
Firstly, compared to basic sequential recommendation (SR) models, our adaptations that incorporate multimodal inputs, particularly SASRec, show significantly better results. This emphasizes the critical role of multimodal integration within the sequential recommendation framework and confirms the efficacy of self-attention-based models in handling both multi-modality and sequential inputs.
Moreover, MMSR distinguishes itself from other non-LLM baselines, highlighting the importance of integrating multimodal fusion modules with sequential modeling components in SR tasks, thereby indirectly supporting our prompt design idea for user preference inference.
Conversely, purely multimodal recommendation models such as MMGCN and MGAT exhibit lower performance due to their lack of dedicated sequential modeling components. This indicates that for optimal effectiveness in SR, the integration of both multimodal and sequential processing capabilities is essential.
Lastly, within the realm of large language model (LLM)-based SR models, our approach significantly outperforms LLaVA without specific fine-tuning. This success validates the effectiveness of our strategically designed prompts for SR tasks. Additionally, our method outperforms TALLREC, demonstrating our success in integrating multimodal information and unlocking the potential of large multimodal models compared to other LLM-based approaches using only textual information. This comparative advantage underscores the integration of advanced MLLM training techniques and the strategic application of multimodal data processing in enhancing sequential recommendation systems.</p>
</div>
</section>
<section id="Sx5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Ablation Study</h3>

<div id="Sx5.SSx3.p1" class="ltx_para">
<p id="Sx5.SSx3.p1.1" class="ltx_p">To evaluate the individual contributions of certain components within our MLLM-MSR framework, we developed several variants of MLLM-MSR, each described as follows:</p>
</div>
<div id="Sx5.SSx3.p2" class="ltx_para">
<ul id="Sx5.I3" class="ltx_itemize">
<li id="Sx5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I3.i1.p1" class="ltx_para">
<p id="Sx5.I3.i1.p1.1" class="ltx_p"><span id="Sx5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">MLLM-MSR<sub id="Sx5.I3.i1.p1.1.1.1" class="ltx_sub">R</sub></span>: This variant employs direct user preference inference instead of the recurrent method. It uses the entire chronological order of historical interaction data to infer user preferences.</p>
</div>
</li>
<li id="Sx5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx5.I3.i2.p1" class="ltx_para">
<p id="Sx5.I3.i2.p1.1" class="ltx_p"><span id="Sx5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">MLLM-MSR<sub id="Sx5.I3.i2.p1.1.1.1" class="ltx_sub"><math id="Sx5.I3.i2.p1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="Sx5.I3.i2.p1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx5.I3.i2.p1.1.1.1.m1.1.1" xref="Sx5.I3.i2.p1.1.1.1.m1.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="Sx5.I3.i2.p1.1.1.1.m1.1b"><ci id="Sx5.I3.i2.p1.1.1.1.m1.1.1.cmml" xref="Sx5.I3.i2.p1.1.1.1.m1.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.I3.i2.p1.1.1.1.m1.1c">\mathcal{I}</annotation></semantics></math></sub></span>: In this version, we omit the image summarization component in item summarization. It relies solely on textual data for user preference inference while still incorporating image information in the recommender module, leveraging capabilities that are readily achievable with current MLLMs.</p>
</div>
</li>
</ul>
</div>
<figure id="Sx5.F4" class="ltx_figure"><img src="/html/2408.09698/assets/figures/Ablation.png" id="Sx5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The performance of MLLM-MSR and its variants.</figcaption>
</figure>
<div id="Sx5.SSx3.p3" class="ltx_para">
<p id="Sx5.SSx3.p3.2" class="ltx_p">As FigureÂ <a href="#Sx5.F4" title="Figure 4 â€£ Ablation Study â€£ Experiments â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows, in the evaluations across three diverse datasets, our primary model, MLLM-MSR, consistently outperformed its variants, MLLM-MSR<sub id="Sx5.SSx3.p3.2.3" class="ltx_sub">R</sub> and MLLM-MSR<sub id="Sx5.SSx3.p3.1.1" class="ltx_sub"><math id="Sx5.SSx3.p3.1.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="Sx5.SSx3.p3.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx5.SSx3.p3.1.1.m1.1.1" xref="Sx5.SSx3.p3.1.1.m1.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p3.1.1.m1.1b"><ci id="Sx5.SSx3.p3.1.1.m1.1.1.cmml" xref="Sx5.SSx3.p3.1.1.m1.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p3.1.1.m1.1c">\mathcal{I}</annotation></semantics></math></sub>, demonstrating the essential roles of its key components.
The MLLM-MSR<sub id="Sx5.SSx3.p3.2.4" class="ltx_sub">R</sub> variant, which employed direct user preference inference, achieved suboptimal performance. This result validates the importance of our modelâ€™s recurrent method in capturing the dynamic evolution of user preferences, indicates our methods can reflect current interests more accurately and reduce the negative impact of lengthy prompts. Besides, the worse performance of MLLM-MSR<sub id="Sx5.SSx3.p3.2.2" class="ltx_sub"><math id="Sx5.SSx3.p3.2.2.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="Sx5.SSx3.p3.2.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx5.SSx3.p3.2.2.m1.1.1" xref="Sx5.SSx3.p3.2.2.m1.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p3.2.2.m1.1b"><ci id="Sx5.SSx3.p3.2.2.m1.1.1.cmml" xref="Sx5.SSx3.p3.2.2.m1.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p3.2.2.m1.1c">\mathcal{I}</annotation></semantics></math></sub> variant, which excluded image summaries and depended solely on textual data for user preference inference, illustrated the significance of integrating multimodal data. This integration is crucial to understand user preferences across different modalities, thereby significantly compensating for the incompleteness of textual information.</p>
</div>
<figure id="Sx5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.09698/assets/figures/Blocksize_Auc.png" id="Sx5.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="455" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.09698/assets/figures/Blocksize_MRR.png" id="Sx5.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="456" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance of MLLM-MSR under different block size.</figcaption>
</figure>
</section>
<section id="Sx5.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Parameter Analysis</h3>

<div id="Sx5.SSx4.p1" class="ltx_para">
<p id="Sx5.SSx4.p1.1" class="ltx_p">In this section, we first analyze the optimal block size for the recurrent user preference inference component of our MLLM-MSR model. As FigureÂ <a href="#Sx5.F5" title="Figure 5 â€£ Ablation Study â€£ Experiments â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows, striking the right balance on the block size is crucial; Too small a block size simplifies the approach to direct inference, potentially missing the dynamic evolution of user preferences due to the limited contextual span. Conversely, too large a block size leads to long prompts, increasing computational load and reducing the number of blocks available to effectively capture temporal dynamics, thereby diminishing the systemâ€™s adaptive capabilities. Optimal block sizing ensures the model processes sequential data efficiently and adapts dynamically to changes in user behavior.</p>
</div>
<div id="Sx5.SSx4.p2" class="ltx_para">
<p id="Sx5.SSx4.p2.1" class="ltx_p">Additionally, we evaluate the impact of context length on the predictive performance of our model. By fixing the output length during user preference generation, we assess how different context lengths affect recommendation outcomes. The results are shown in FigureÂ <a href="#Sx5.F6" title="Figure 6 â€£ Parameter Analysis â€£ Experiments â€£ Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We found short context lengths cause a loss of information, resulting in suboptimal predictions. However, once the context length reaches a certain threshold, the results stabilize, indicating that the large model has strong summarizing capabilities and can capture all necessary information within a specific optimal range. This demonstrates the importance of selecting a proper context length to maximize information utility without incurring unnecessary computational complexity.</p>
</div>
<figure id="Sx5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.09698/assets/figures/Length_Auc.png" id="Sx5.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="446" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.09698/assets/figures/Length_MRR.png" id="Sx5.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="455" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance of MLLM-MSR under different user preference summary length.</figcaption>
</figure>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this study, our proposed model, the Multimodal Large Language Model-enhanced Multimodal Sequential Recommendation (MLLM-MSR), effectively leverages MLLMs for multimodal sequential recommendation. Through a novel two-stage user preference summarization process and the implementation of SFT techniques, MLLM-MSR showcases a robust ability to adapt to and predict dynamic user preferences across various datasets. Our experimental results validate the outstanding performance of MLLM-MSR compared to existing methods, particularly in its adaptability to evolving preferences. This paper introduces a innovative use of MLLMs that enriches the recommendation process by integrating diverse modalities and enhances the personalization and accuracy of the recommendations, and meanwhile providing added interpretability through detailed user preference analysis.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam etÂ al. (2023)</span>
<span class="ltx_bibblock">
Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F.Â L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaltruÅ¡aitis, Ahuja, and Morency (2018)</span>
<span class="ltx_bibblock">
BaltruÅ¡aitis, T.; Ahuja, C.; and Morency, L.-P. 2018.

</span>
<span class="ltx_bibblock">Multimodal Machine Learning: A Survey and Taxonomy.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 41(2): 423â€“443.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bao, K.; Zhang, J.; Zhang, Y.; Wang, W.; Feng, F.; and He, X. 2023.

</span>
<span class="ltx_bibblock">Tallrec: An effective and efficient tuning framework to align large language model with recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th ACM Conference on Recommender Systems</em>, 1007â€“1014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borisov etÂ al. (2022)</span>
<span class="ltx_bibblock">
Borisov, V.; SeÃŸler, K.; Leemann, T.; Pawelczyk, M.; and Kasneci, G. 2022.

</span>
<span class="ltx_bibblock">Language models are realistic tabular data generators.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.06280</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Brown, T.Â B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 33, 1877â€“1901.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Cao, Y.; Mehta, N.; Yi, X.; Keshavan, R.; Heldt, L.; Hong, L.; Chi, E.Â H.; and Sathiamoorthy, M. 2024.

</span>
<span class="ltx_bibblock">Aligning Large Language Models with Recommendation Knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.00245</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui etÂ al. (2022)</span>
<span class="ltx_bibblock">
Cui, Z.; Ma, J.; Zhou, C.; Zhou, J.; and Yang, H. 2022.

</span>
<span class="ltx_bibblock">M6-rec: Generative pretrained language models are open-ended recommender systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.08084</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2023)</span>
<span class="ltx_bibblock">
Du, X.; Yuan, H.; Zhao, P.; Qu, J.; Zhuang, F.; Liu, G.; Liu, Y.; and Sheng, V.Â S. 2023.

</span>
<span class="ltx_bibblock">Frequency enhanced hybrid attention network for sequential recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 78â€“88.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floridi and Chiriatti (2020)</span>
<span class="ltx_bibblock">
Floridi, L.; and Chiriatti, M. 2020.

</span>
<span class="ltx_bibblock">GPT-3: Its nature, scope, limits, and consequences.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Minds and Machines</em>, 30: 681â€“694.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Gao, P.; Zareian, A.; Maji, S.; Darrell, T.; and Rohrbach, M. 2021.

</span>
<span class="ltx_bibblock">CLIP: Connecting Text and Images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1â€“10.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Geng, S.; Liu, S.; Fu, Z.; Ge, Y.; and Zhang, Y. 2022.

</span>
<span class="ltx_bibblock">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt, and Predict Paradigm (P5).

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th ACM Conference on Recommender Systems</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and McAuley (2016)</span>
<span class="ltx_bibblock">
He, R.; and McAuley, J. 2016.

</span>
<span class="ltx_bibblock">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">proceedings of the 25th international conference on world wide web</em>, 507â€“517.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidasi etÂ al. (2015)</span>
<span class="ltx_bibblock">
Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D. 2015.

</span>
<span class="ltx_bibblock">Session-based recommendations with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.06939</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hou, Y.; He, Z.; McAuley, J.; and Zhao, W.Â X. 2023.

</span>
<span class="ltx_bibblock">Learning vector-quantized item representation for transferable sequential recommenders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Web Conference 2023</em>, 1162â€“1171.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Hou, Y.; Zhang, J.; Lin, Z.; Lu, H.; Xie, R.; McAuley, J.; and Zhao, W.Â X. 2024.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for recommender systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">European Conference on Information Retrieval</em>, 364â€“381. Springer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hu, E.Â J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hu, H.; Guo, W.; Liu, Y.; and Kan, M.-Y. 2023.

</span>
<span class="ltx_bibblock">Adaptive multi-modalities fusion in sequential recommendation systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>, 843â€“853.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang, Xiao, and Yu (2019)</span>
<span class="ltx_bibblock">
Huang, Z.; Xiao, W.; and Yu, Y. 2019.

</span>
<span class="ltx_bibblock">A Survey on Deep Learning Based Recommender Systems: From Traditional Methods to Recent Advances in Multimedia Recommendations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM International Conference on Multimedia</em>, 2401â€“2409. ACM.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ji, W.; Sun, Y.; Chen, T.; and Wang, X. 2020.

</span>
<span class="ltx_bibblock">Two-stage sequential recommendation via bidirectional attentive behavior embedding and long/short-term integration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Knowledge Graph (ICKG)</em>, 449â€“457. IEEE.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang and McAuley (2018)</span>
<span class="ltx_bibblock">
Kang, W.-C.; and McAuley, J. 2018.

</span>
<span class="ltx_bibblock">Self-attentive sequential recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on data mining (ICDM)</em>, 197â€“206. IEEE.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kim, Y.; Kim, T.; Shin, W.-Y.; and Kim, S.-W. 2024.

</span>
<span class="ltx_bibblock">MONET: Modality-Embracing Graph Convolutional Network and Target-Aware Attention for Multimedia Recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>, 332â€“340.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh, Fried, and Salakhutdinov (2024)</span>
<span class="ltx_bibblock">
Koh, J.Â Y.; Fried, D.; and Salakhutdinov, R.Â R. 2024.

</span>
<span class="ltx_bibblock">Generating images with multimodal language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei, Ji, and Li (2019)</span>
<span class="ltx_bibblock">
Lei, C.; Ji, S.; and Li, Z. 2019.

</span>
<span class="ltx_bibblock">Tissa: A time slice self-attention approach for modeling sequential user behaviors.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">The World Wide Web Conference</em>, 2964â€“2970.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li, Zhang, and Chen (2023)</span>
<span class="ltx_bibblock">
Li, L.; Zhang, Y.; and Chen, L. 2023.

</span>
<span class="ltx_bibblock">Prompt distillation for efficient llm-based recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>, 1348â€“1357.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Li, X.; Chen, C.; Zhao, X.; Zhang, Y.; and Xing, C. 2023.

</span>
<span class="ltx_bibblock">E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.02443</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Liu, C.; Li, X.; Cai, G.; Dong, Z.; Zhu, H.; and Shang, L. 2021a.

</span>
<span class="ltx_bibblock">Noninvasive self-attention for side information fusion in sequential recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 35, 4249â€“4256.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Liu, H.; Li, C.; Li, Y.; Li, B.; Zhang, Y.; Shen, S.; and Lee, Y.Â J. 2024a.

</span>
<span class="ltx_bibblock">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Liu, H.; Li, C.; Wu, Q.; and Lee, Y.Â J. 2023a.

</span>
<span class="ltx_bibblock">Visual Instruction Tuning.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Liu, Q.; Chen, N.; Sakai, T.; and Wu, X.-M. 2024b.

</span>
<span class="ltx_bibblock">Once: Boosting content-based recommendation with both open-and closed-source large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>, 452â€“461.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Liu, Q.; Hu, J.; Xiao, Y.; Gao, J.; and Zhao, X. 2023b.

</span>
<span class="ltx_bibblock">Multimodal recommender systems: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.03883</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024c)</span>
<span class="ltx_bibblock">
Liu, Y.; Wang, Y.; Sun, L.; and Yu, P.Â S. 2024c.

</span>
<span class="ltx_bibblock">Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.08670</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Liu, Y.; Yang, S.; Lei, C.; Wang, G.; Tang, H.; Zhang, J.; Sun, A.; and Miao, C. 2021b.

</span>
<span class="ltx_bibblock">Pre-training graph transformer with multimodal side information for recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</em>, 2853â€“2861.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McAuley etÂ al. (2015)</span>
<span class="ltx_bibblock">
McAuley, J.; Targett, C.; Shi, Q.; and Van DenÂ Hengel, A. 2015.

</span>
<span class="ltx_bibblock">Image-based recommendations on styles and substitutes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</em>, 43â€“52.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monti, Bronstein, and Bresson (2017)</span>
<span class="ltx_bibblock">
Monti, F.; Bronstein, M.; and Bresson, X. 2017.

</span>
<span class="ltx_bibblock">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ni, Y.; Cheng, Y.; Liu, X.; Fu, J.; Li, Y.; He, X.; Zhang, Y.; and Yuan, F. 2023.

</span>
<span class="ltx_bibblock">A Content-Driven Micro-Video Recommendation Dataset at Scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15379</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ren, X.; Wei, W.; Xia, L.; Su, L.; Cheng, S.; Wang, J.; Yin, D.; and Huang, C. 2024.

</span>
<span class="ltx_bibblock">Representation learning with large language models for recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Web Conference 2024</em>, 3464â€“3475.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle, Freudenthaler, and Schmidt-Thieme (2010)</span>
<span class="ltx_bibblock">
Rendle, S.; Freudenthaler, C.; and Schmidt-Thieme, L. 2010.

</span>
<span class="ltx_bibblock">Factorizing personalized markov chains for next-basket recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th international conference on World wide web</em>, 811â€“820.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle etÂ al. (2019)</span>
<span class="ltx_bibblock">
Rendle, S.; Krichene, W.; Zhang, L.; and Anderson, J. 2019.

</span>
<span class="ltx_bibblock">DIF-SR: Differential Sequential Recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Conference on Recommender Systems</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan (2014)</span>
<span class="ltx_bibblock">
Simonyan, K. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>, 1441â€“1450.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wang (2018)</span>
<span class="ltx_bibblock">
Tang, J.; and Wang, K. 2018.

</span>
<span class="ltx_bibblock">Personalized top-n sequential recommendation via convolutional sequence embedding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the eleventh ACM international conference on web search and data mining</em>, 565â€“573.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; RoziÃ¨re, B.; Goyal, N.; Hambro, E.; Azhar, F.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2019)</span>
<span class="ltx_bibblock">
Wei, Y.; Zhao, X.; Liu, G.; Zhu, Z.; Zhuang, Y.; Qin, J.; and Wang, J. 2019.

</span>
<span class="ltx_bibblock">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM International Conference on Multimedia</em>, 1437â€“1445. ACM.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Shen, T.; Qin, C.; Zhu, C.; Zhu, H.; Liu, Q.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">A Survey on Large Language Models for Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.19860</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yin, B.; Xie, J.; Qin, Y.; Ding, Z.; Feng, Z.; Li, X.; and Lin, W. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th ACM Conference on Recommender Systems</em>, 599â€“601.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yue, X.; Ni, Y.; Zhang, K.; Zheng, T.; Liu, R.; Zhang, G.; Stevens, S.; Jiang, D.; Ren, W.; Sun, Y.; etÂ al. 2024.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9556â€“9567.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhai, J.; Zheng, X.; Wang, C.-D.; Li, H.; and Tian, Y. 2023.

</span>
<span class="ltx_bibblock">Knowledge prompt-tuning for sequential recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>, 6451â€“6461.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Zhang, C.; Zhang, H.; Wu, S.; Wu, D.; Xu, T.; Gao, Y.; Hu, Y.; and Chen, E. 2024a.

</span>
<span class="ltx_bibblock">NoteLLM-2: Multimodal Large Representation Models for Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.16789</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Zhang, C.; Zhang, H.; Wu, S.; Wu, D.; Xu, T.; Gao, Y.; Hu, Y.; and Chen, E. 2024b.

</span>
<span class="ltx_bibblock">NoteLLM-2: Multimodal Large Representation Models for Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.16789</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Tong (2020)</span>
<span class="ltx_bibblock">
Zhang, S.-Y.; and Tong, H. 2020.

</span>
<span class="ltx_bibblock">Deep Learning for Multimodal Integration: A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 53(1): 1â€“35.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhang, T.; Zhao, P.; Liu, Y.; Sheng, V.Â S.; Xu, J.; Wang, D.; Liu, G.; Zhou, X.; etÂ al. 2019.

</span>
<span class="ltx_bibblock">Feature-level deeper self-attention network for sequential recommendation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, 4320â€“4326.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao, Lee, and Wu (2020)</span>
<span class="ltx_bibblock">
Zhao, Z.; Lee, D.; and Wu, X. 2020.

</span>
<span class="ltx_bibblock">Trans2D: A Holistic Fusion Model for Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.04563</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zheng, Z.; Chao, W.; Qiu, Z.; Zhu, H.; and Xiong, H. 2024.

</span>
<span class="ltx_bibblock">Harnessing Large Language Models for Text-Rich Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.13325</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Miao (2024)</span>
<span class="ltx_bibblock">
Zhou, X.; and Miao, C. 2024.

</span>
<span class="ltx_bibblock">Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation With Interpretability.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09697" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09698" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09698">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09698" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09699" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:48:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
