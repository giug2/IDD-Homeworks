<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.03681] A Survey on Participant Selection for Federated Learning in Mobile Networks</title><meta property="og:description" content="Federated Learning (FL) is an efficient distributed machine learning paradigm that employs private datasets in a privacy-preserving manner. The main challenges of FL are that end devices usually possess various computa‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey on Participant Selection for Federated Learning in Mobile Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey on Participant Selection for Federated Learning in Mobile Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.03681">

<!--Generated on Wed Mar 13 16:16:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning,  machine learning,  participant selection.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Survey on Participant Selection for Federated Learning in Mobile Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Behnaz Soltani<sup id="id1.1.id1" class="ltx_sup">1</sup>, Venus Haghighi<sup id="id2.2.id2" class="ltx_sup">1</sup>, Adnan Mahmood<sup id="id3.3.id3" class="ltx_sup">1</sup>, Quan Z. Sheng<sup id="id4.4.id4" class="ltx_sup">1</sup>, Lina Yao<sup id="id5.5.id5" class="ltx_sup">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.6.id1" class="ltx_text ltx_affiliation_institution"><sup id="id6.6.id1.1" class="ltx_sup">1</sup>Macquarie University, Sydney, Australia</span><span id="id7.7.id2" class="ltx_text ltx_affiliation_institution"><sup id="id7.7.id2.1" class="ltx_sup">2</sup>University of New South Wales, Sydney, Australia</span><span id="id8.8.id3" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>
<div class="ltx_dates">(2022)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">Federated Learning (FL) is an efficient distributed machine learning paradigm that employs private datasets in a privacy-preserving manner. The main challenges of FL are that end devices usually possess various computation and communication capabilities and their training data are not independent and identically distributed (non-IID). Due to limited communication bandwidth and unstable availability of such devices in a mobile network, only a fraction of end devices (also referred to as the <em id="id9.id1.1" class="ltx_emph ltx_font_italic">participants</em> or <em id="id9.id1.2" class="ltx_emph ltx_font_italic">clients</em> in a FL process) can be selected in each round.
Hence, it is of paramount importance to utilize an efficient participant selection scheme to maximize the performance of FL including final model accuracy and training time. In this paper, we provide a review of participant selection techniques for FL. First, we introduce FL and highlight the main challenges during participant selection. Then, we
review
the
existing studies and categorize them based on their solutions. Finally, we provide some future directions on participant selection for FL based on our analysis of the state-of-the-art in this topic area.</p>
</div>
<div class="ltx_keywords">Federated learning, machine learning, participant selection.
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">journalyear: </span>2022</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">conference: </span>Workshop on Mobility in the Evolving Internet Architecture; October 21,
2022; Sydney, Australia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies¬†Machine learning</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies¬†Distributed computing methodologies</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Mobile devices such as vehicles and smart phones are constantly generating a massive amount of data, which could be utilized for machine learning in a bid to achieve smart mobile applications. However, transmitting private data to a centralized or an edge server for training may lead to privacy issue and could cause long communication latency and large resource cost¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">verbraeken2020survey, </a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A decentralized machine learning approach called Federated Learning (FL) has been proposed by Google that enables cooperative learning on devices without sharing the local data¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>. Clients train the model on-device in a privacy-preserving manner using their local datasets and transfer the local model parameters to the FL server for aggregation.
As a result, FL enables user privacy preservation, low communication costs, and transmission latency reduction owing to transmitting only model parameters to the server for aggregation. Furthermore, in time-critical systems such as autonomous vehicles, making real time decisions locally at end devices significantly decreases response time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Employing various datasets of different clients using FL leads to model accuracy improvement.
Participant selection is an emerging challenge in the management of FL that possesses a profound impact on the performance of the model training, especially in the scenarios with a huge number of participants and limited wireless channels. In FL, due to dynamic environments and the limited network bandwidth, only a fraction of clients can be participated for training in each round. Hence, proper selection is of paramount importance in FL to achieve the desired accuracy with fast convergence.
In FedAvg¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite> algorithm introduced by Google, participants are determined uniformly at random in each round. However, due to the various computation and communication resources and heterogeneous datasets, devices may have different contributions on the training performance.
To evaluate the performance of FL, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">training time</span> and <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">final model accuracy</span> are the most important factors. Mostly, there are trade-offs between the model accuracy and convergence time in FL. For example, participants with high quality data may have poor network connection or computation capacities, while those with low quality data may possess rich resources to perform training process.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To the best of our knowledge, there are no existing works that conduct a literature review of the participant selection in FL. The main contributions of this study are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We discuss the important challenges pertinent to participant selection process in FL.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We review the state-of-the-art on participant selection by categorizing them vis-√†-vis different approaches.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We identify some open research directions of participant selection in mobile networks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federation learning (FL), in essence, encompasses a number of steps which are delineated as follows:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Initialization</em> ‚Äì The aggregation server determines the FL task, generates a randomly or pretrained global model, and adjusts learning parameters (e.g., the number of rounds and the learning rates).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Participant Selection</em> ‚Äì The FL server selects <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mi id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><ci id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">n</annotation></semantics></math> number of clients amongst the volunteers to participate in the training process.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><em id="S2.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Global Distribution</em> ‚Äì The server disseminates the global model parameters to the selected participants to train and update the shared model.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><em id="S2.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Local Training</em> ‚Äì Selected participants train the shared model using their local data samples and upload local model parameters to the server.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p"><em id="S2.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">Aggregation</em> ‚Äì After receiving the local models, the server aggregates them and computes a new global model.</p>
</div>
</li>
</ol>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Steps 2, 3, and 4 repeat until the model converges or reaches a desirable accuracy and are portrayed in Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2. Federated Learning ‚Ä£ A Survey on Participant Selection for Federated Learning in Mobile Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The most popular FL algorithm is FedAvg¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite> that randomly samples a subset of clients. However, due to different computation and communication capabilities and various data samples, randomly selected participants could degrade the performance of FL process.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2207.03681/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>An Overview of Federated Learning Process.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Challenges of Participant Selection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There are several significant challenges in participant selection approaches that affect the performance of FL process:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Device Heterogeneity:</span> Devices have different computation, communication, and storage capabilities that may negatively affect the performance of training process¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">luo2021tackling, </a>)</cite>. In synchronous FL with heterogeneous resources, per-round training time is determined by the slowest clients (i.e., <em id="S2.I2.i1.p1.1.2" class="ltx_emph ltx_font_italic">stragglers</em>) since all the participants are required to wait for the slowest clients to complete their training tasks.
On the other hand, end devices with limited resources may increase dropouts during training process and affect the accuracy of the model. Therefore, selecting clients without considering their resource capabilities may lead to longer training time and degrade the final model accuracy.
</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Data Heterogeneity</span>: Since data is generated independently based on clients‚Äô behaviours, data distribution on end devices are usually non-Independent and Identically Distributed (non-IID). Hence, local datasets may not be representative of the population distribution, which can lead to the biased model update and degrade the model accuracy¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">mcmahan2017communication, </a>; <a href="#bib.bib4" title="" class="ltx_ref">li2019convergence, </a>; <a href="#bib.bib5" title="" class="ltx_ref">wang2020optimizing, </a>; <a href="#bib.bib6" title="" class="ltx_ref">zhang2021client, </a>)</cite>.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Dynamicity:</span> Clients might be unavailable due to the high-mobility environment, poor network condition, and energy constraints¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib7" title="" class="ltx_ref">zhou2022role, </a>; <a href="#bib.bib8" title="" class="ltx_ref">kang2019incentive, </a>; <a href="#bib.bib9" title="" class="ltx_ref">kang2020reliable, </a>)</cite>. Furthermore, due to the channel fading in wireless networks, a fraction of local model updates may be lost. Therefore, a dynamic environment with high mobility devices has a major impact on the performance of FL process.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p"><span id="S2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Trustworthiness:</span> Since the FL server has no knowledge about the local training process, malicious devices may launch attacks and manipulate the result of the training task. Therefore, it is of utmost importance to identify malicious clients and eliminate them from the learning process.</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p"><span id="S2.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Fairness:</span> Devices with poor capabilities are less likely to be selected to participate in the training process, which leads to selection bias and degrade the model accuracy¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">huang2022stochastic, </a>)</cite>. Fairness enables clients with diverse datasets to participate in the FL process and improve the model accuracy and convergence speed. Therefore, in order to minimize the model bias and generalize the global model, all the end devices should have a chance to participate in the FL process.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Participant selection in FL</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Over the past few years, researchers in both academia and industry have proposed a number of research studies pertinent to participant selection in the FL process. The same have been classified into eight
appropriate categories and are discussed as follows:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Threshold-based Selection.</span> Participants may be allowed to complete the local training within a specific deadline.
FedCS¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib11" title="" class="ltx_ref">nishio2019client, </a>)</cite> is one of the first studies on client selection in FL that manages participant selection process based on the resource capabilities. The proposed method aims to select as many clients as possible that can complete the training steps within a specific deadline to reach the desired performance¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>. First, the random clients are asked to inform their resource information, i.e., computation and computation capacities, wireless channel states, and the size of their data. Afterward, the server computes the required time to complete the training task for each client using aforementioned information. However, due to dynamically changing resource and network conditions, the static time threshold cannot guarantee the efficiency of this approach. Thus, an adaptive deadline determination algorithm for mobile devices is proposed in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib12" title="" class="ltx_ref">lee2021adaptive, </a>)</cite>, wherein clients are adaptively determined at each round instead of using a fixed deadline. This approach could decrease the convergence time by up to 50% in contrast to the conventional fixed threshold schemes. </p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">A multicriteria-based client selection strategy is proposed in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">abdulrahman2020fedmccs, </a>)</cite> in which time, CPU, memory, and energy for all the end devices are considered to predict their respective capability to perform a training task. The authors employ stratified-based sampling to establish a homogeneous group of clients based on the time zones. Subsequently, considering the resource utilization and the resource capabilities to complete the FL task within a specific time frame, the authors formulate a bilevel optimization problem to maximize the number of selected clients. To predict the resource utilization, linear regression according to the history of the past rounds is employed. Furthermore, due to imbalanced class distribution, the authors prioritize participants with the highest event rate (i.e., the ratio of abnormal data samples pertaining to minority class to the total data samples) </p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Reputation-based Selection.</span> Unreliable clients may (a) perform undesirable behaviours intentionally, e.g., data poisoning, model poisoning, and sybil attacks, or (b) unintentionally due to high mobility environments and unstable network connections. Therefore, it is of paramount importance to design an efficient participant selection approach for reliable model training. Reputation is a metric to evaluate trustworthiness of entities based on their past interactions¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib14" title="" class="ltx_ref">mahmood2021trust, </a>; <a href="#bib.bib15" title="" class="ltx_ref">mahmood2022trust, </a>; <a href="#bib.bib16" title="" class="ltx_ref">liu2011novel, </a>)</cite>. Recently, a couple of studies employed reputation as a metric to select reliable devices for participating in FL process. A reputation-based client selection scheme is proposed in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">kang2019incentive, </a>; <a href="#bib.bib9" title="" class="ltx_ref">kang2020reliable, </a>)</cite> that employs multiweight subjective logic by taking into account both the direct interactions and recommendations from other task publishers. The proposed approach considers three weight attributes: interaction frequency, interaction timeliness, and interaction effects. The reputation is stored in a blockchain to achieve secure reputation management.
</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">zou2021reputation, </a>)</cite>, a reputation-based regional FL framework is introduced for intelligent transportation systems in which vehicles are divided into multiple regions each of which possesses its own learning model. Roadside Units (RSUs) are selected as the leaders and perform aggregation in each region. Owing to the dynamic nature of transportation systems, devices may move away from their regions and join a new region. Each leader computes the reputation of vehicles based on a) honesty degree, b) accuracy contribution, and c) interaction timeliness, and selects vehicles with high reputation to join FL process. In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">song2021reputation, </a>)</cite>, the authors introduce a reputation model based on beta distribution in order to measure the trustworthiness of the end devices. The trust value is evaluated using the contribution of participants to the global model. They propose a reputation-based scheduling scheme that jointly considers trustworthiness and fairness based on the reputation value and successful transmission rate.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Probability Allocation-based Selection.</span>
Different end devices can have various probabilities of being selected for a training task. The authors in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">luo2021tackling, </a>)</cite> design a client sampling strategy to reduce total training time. They take both system and data heterogeneity into consideration since clients with high-quality data may have poor communication resources, whereas, those with high communication capabilities may have low-quality data. Their approach provides a new convergence upper bound for arbitrary client selection probabilities and generates a non-convex training time minimization problem.
Their approach significantly reduces the convergence time for achieving the same target loss compared to several baselines.
</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">huang2022stochastic, </a>)</cite>, a stochastic client selection algorithm under a volatile context is investigated in which the selected clients might not be capable of returning their local models for aggregation due to different reasons, including but not limited to, limited computing resources, network failure, and user unwillingness. However, selecting clients with the lowest failure probability may violate selection fairness owing to the selection of a particular group of clients repeatedly and hurt the model accuracy. Therefore, the authors study the trade-off between selecting participants with the lowest failure likelihood and selection fairness, and formulate the client selection problem by taking both factors into account. Since the problem cannot be solved offline due to unknown status of the participants, an adversary bandit-based online scheduling solution is employed. The proposed method is able to increase convergence time while maintaining the model accuracy level.
</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">The authors in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">perazzone2022communication, </a>)</cite> derive a novel convergence upper bound for non-convex loss functions using FL with arbitrary device selection probabilities. They design a stochastic optimization problem that aims to minimize a weighted sum of the convergence bound and communication time. The proposed participant selection method solves the problem using the Lyapunov drift-plus-penalty framework based on current channel conditions without the knowledge of channel statistics.
In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">wu2022node, </a>)</cite>, an aggregation algorithm is designed to determine the optimal subset of local model updates by excluding adverse local updates. Moreover, a Probabilistic Node Selection framework (FedPNS) is proposed that dynamically adjusts the selection probability for devices based on their contribution to the model pertaining to the data distribution that is determined using the output of the aggregation algorithm.</p>
</div>
<div id="S3.p9" class="ltx_para ltx_noindent">
<p id="S3.p9.1" class="ltx_p"><span id="S3.p9.1.1" class="ltx_text ltx_font_bold">Reinforcement Learning-based Selection</span>. In reinforcement learning, an agent learns to attain an objective in a complex and uncertain environment and maximizes its rewards. A framework based on multi-armed bandit for online client selection is introduced in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib21" title="" class="ltx_ref">xia2020multi, </a>)</cite> to minimize the training latency including both local computation time and data transmission time in two scenarios: 1) an ideal scenario in which clients possess balanced and IID datasets and are always available, and 2) a non-ideal scenario in which clients may be
unavailable and the distribution of datasets is non-IID. In the non-IID scenario, both the fairness and availability constraint is addressed.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">huang2020efficiency, </a>)</cite>, the authors define an offline client selection problem with long-term fairness constraints. The constraint adjusts the minimum average selection rate of every participant to maintain fairness for the system. Due to the indeterminate availability of clients until the start of a round and also time-coupling fairness constraint, the authors utilize the Lyapunov optimization framework to transfer the offline problem into an online optimization problem, where the participation rate of clients is evaluated using dynamic queues. In the proposed method, the model exchange time (i.e., the time spent between global model distribution and uploading all the local models) of each participant before each communication round is estimated using Contextual Combinatorial Multi Arm Bandit (C<sup id="S3.p10.1.1" class="ltx_sup">2</sup>MAB) model. The authors aim to minimize the long-term model exchange time.</p>
</div>
<div id="S3.p11" class="ltx_para">
<p id="S3.p11.1" class="ltx_p">An experience-driven controlled framework called FAVOR is proposed in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib5" title="" class="ltx_ref">wang2020optimizing, </a>)</cite> that aims to determine the best fraction of devices in each round to minimize the number of rounds and tackle the data non-IID distribution. The authors formulate client selection for FL process as a deep reinforcement learning problem. To select devices in each communication round, a double deep Q-learning mechanism is proposed in order to improve global model accuracy and reduce the number of communication rounds.</p>
</div>
<div id="S3.p12" class="ltx_para ltx_noindent">
<p id="S3.p12.1" class="ltx_p"><span id="S3.p12.1.1" class="ltx_text ltx_font_bold">Group-based Selection (GS).</span> Clients are divided into several groups and then those within the same group
are
selected for each communication round. In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">ma2021client, </a>)</cite>, a grouping based participant selection mechanism is introduced in which participants are split into various groups based on group earth mover‚Äôs distance (GEMD) to balance the label distribution of the clients. This new metric evaluates similarity between global distribution and local data distributions. A smaller GEMD means that the training data of the selected clients are closer to IID distribution. Therefore, selecting a group of clients with the smallest GEMD can improve the performance of FL.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of the existing participant selection works.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:414.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-372.9pt,356.4pt) scale(0.367681618790105,0.367681618790105) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t">No.</th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.2.1.1" class="ltx_p">Methods</span>
</span>
</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Device Heterogeneity</th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data Heterogeneity</th>
<th id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Fairness</th>
<th id="S3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dynamicity</th>
<th id="S3.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Trustworthiness</th>
<th id="S3.T1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Objective</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_tt"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib11" title="" class="ltx_ref">nishio2019client, </a>)</cite></th>
<th id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="width:85.4pt;">
<span id="S3.T1.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.2.1.1" class="ltx_p">Threshold-based</span>
</span>
</th>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">‚úì</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">‚úó</td>
<td id="S3.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">‚úó</td>
<td id="S3.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">‚úó</td>
<td id="S3.T1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">‚úó</td>
<td id="S3.T1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">Maximizing the number of participants</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib12" title="" class="ltx_ref">lee2021adaptive, </a>)</cite></th>
<th id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Maximizing the number of participants</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">abdulrahman2020fedmccs, </a>)</cite></th>
<th id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Maximizing the number of participants</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">kang2019incentive, </a>; <a href="#bib.bib9" title="" class="ltx_ref">kang2020reliable, </a>)</cite></th>
<th id="S3.T1.1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.4.2.1.1" class="ltx_p">Reputation-based</span>
</span>
</th>
<td id="S3.T1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Slecting trusted participants to improve the reliablity of FL</td>
</tr>
<tr id="S3.T1.1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">zou2021reputation, </a>)</cite></th>
<th id="S3.T1.1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.6.5.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Reliable participant selection to improve accuracy of knowledge</td>
</tr>
<tr id="S3.T1.1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">song2021reputation, </a>)</cite></th>
<th id="S3.T1.1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.7.6.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Improving the reliability and convergence performance</td>
</tr>
<tr id="S3.T1.1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">luo2021tackling, </a>)</cite></th>
<th id="S3.T1.1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.7.2.1.1" class="ltx_p">Probability allocation-based</span>
</span>
</th>
<td id="S3.T1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Minimizing total learning time</td>
</tr>
<tr id="S3.T1.1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">huang2022stochastic, </a>)</cite></th>
<th id="S3.T1.1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.9.8.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Addressing the trade-off between the lowest failure
probability and fairness</td>
</tr>
<tr id="S3.T1.1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">perazzone2022communication, </a>)</cite></th>
<th id="S3.T1.1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.10.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.10.9.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Minimizing communication time for speeding up the convergence</td>
</tr>
<tr id="S3.T1.1.1.11.10" class="ltx_tr">
<th id="S3.T1.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">wu2022node, </a>)</cite></th>
<th id="S3.T1.1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.11.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.11.10.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Excluding adverse local models from participating devices</td>
</tr>
<tr id="S3.T1.1.1.12.11" class="ltx_tr">
<th id="S3.T1.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib21" title="" class="ltx_ref">xia2020multi, </a>)</cite></th>
<th id="S3.T1.1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.12.11.2.1.1" class="ltx_p">Reinforcement learning-based</span>
</span>
</th>
<td id="S3.T1.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.12.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.12.11.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Minimizing the total training time</td>
</tr>
<tr id="S3.T1.1.1.13.12" class="ltx_tr">
<th id="S3.T1.1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">huang2020efficiency, </a>)</cite></th>
<th id="S3.T1.1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.13.12.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Minimizing exchange time with fairness guarantee</td>
</tr>
<tr id="S3.T1.1.1.14.13" class="ltx_tr">
<th id="S3.T1.1.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib5" title="" class="ltx_ref">wang2020optimizing, </a>)</cite></th>
<th id="S3.T1.1.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.14.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.14.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.14.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.14.13.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Reducing the number of communication round under non-IID setting</td>
</tr>
<tr id="S3.T1.1.1.15.14" class="ltx_tr">
<th id="S3.T1.1.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">ma2021client, </a>)</cite></th>
<th id="S3.T1.1.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.15.14.2.1.1" class="ltx_p">Group-based</span>
</span>
</th>
<td id="S3.T1.1.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.15.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.15.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.15.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.15.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.15.14.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Balancing the label distribution of participants</td>
</tr>
<tr id="S3.T1.1.1.16.15" class="ltx_tr">
<th id="S3.T1.1.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">zhao2022participant, </a>)</cite></th>
<th id="S3.T1.1.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.16.15.2.1.1" class="ltx_p">Weight Divergence-based</span>
</span>
</th>
<td id="S3.T1.1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.16.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.16.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.16.15.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.16.15.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.16.15.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Addressing trade-off between accuracy contribution and training time</td>
</tr>
<tr id="S3.T1.1.1.17.16" class="ltx_tr">
<th id="S3.T1.1.1.17.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">zhang2021client, </a>)</cite></th>
<th id="S3.T1.1.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" style="width:85.4pt;"></th>
<td id="S3.T1.1.1.17.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.17.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.17.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.17.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.17.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.17.16.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Selecting clients with lower non-IID degree of data</td>
</tr>
<tr id="S3.T1.1.1.18.17" class="ltx_tr">
<th id="S3.T1.1.1.18.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">wang2021device, </a>)</cite></th>
<th id="S3.T1.1.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.18.17.2.1.1" class="ltx_p">Offloading-Aware</span>
</span>
</th>
<td id="S3.T1.1.1.18.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.18.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.18.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.18.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.18.17.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.18.17.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">The optimal combination of client selection and data offloading configuration</td>
</tr>
<tr id="S3.T1.1.1.19.18" class="ltx_tr">
<th id="S3.T1.1.1.19.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_ll ltx_border_rr ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib26" title="" class="ltx_ref">fraboni2021clustered, </a>)</cite></th>
<th id="S3.T1.1.1.19.18.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="width:85.4pt;">
<span id="S3.T1.1.1.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.19.18.2.1.1" class="ltx_p">Clustering-based</span>
</span>
</th>
<td id="S3.T1.1.1.19.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.19.18.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">‚úì</td>
<td id="S3.T1.1.1.19.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.19.18.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.19.18.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">‚úó</td>
<td id="S3.T1.1.1.19.18.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">Unbiased participant selection</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.p13" class="ltx_para ltx_noindent">
<p id="S3.p13.1" class="ltx_p"><span id="S3.p13.1.1" class="ltx_text ltx_font_bold">Weight Divergence-based Selection.</span> Weight divergence in training can be an indicator to identify the distribution of local datasets. The authors in <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">zhao2022participant, </a>)</cite> introduce a client selection utility that tries to deal with the trade-off between accuracy and execution time in each round. The change of weight between two adjacent rounds
is
defined as a utility for fast convergence. In addition, since clients with large data volume may negatively affect the training time,
the ratio of the local data size to the total size
is also added as a coefficient to the client‚Äôs utility in order that if local data constitutes a significant amount of total data, utility reduces.
Since it is not always necessary to select participants in every round, this study also designs a feedback control component that dynamically adjusts the frequency of client selection.
</p>
</div>
<div id="S3.p14" class="ltx_para">
<p id="S3.p14.1" class="ltx_p">The authors in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">zhang2021client, </a>)</cite> design a participant selection algorithm to tackle the accuracy reduction owing to non-IID datasets. They identify the degree of non-IID data using the weight divergence. The weight divergence is evaluated between the client model and auxiliary model
which is trained using public or purchased datasets at the aggregation server.
In this approach, participants that have lower non-IID degree should be selected with higher frequency for training.</p>
</div>
<div id="S3.p15" class="ltx_para ltx_noindent">
<p id="S3.p15.1" class="ltx_p"><span id="S3.p15.1.1" class="ltx_text ltx_font_bold">Offloading-Aware Selection.</span> Device-to-device (D2D) offloading of local data processing from resource-limited devices to resource-rich devices can enable local training with more diversified data distribution. Participant sampling with data offloading are combined in¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">wang2021device, </a>)</cite> to maximize training accuracy.
Devices with large contributions to model training are chosen for training and other devices may send their data to the selected participants based on their data similarity. Two data samples are considered similar if they have identical labels and their feature vectors have limited difference. Data offloading is only performed between trusted and single-hop neighbors. To determine participants, the proposed method uses graph convolutional networks (GCNs) to learn the relationship between FL accuracy, network attributes, and offloading topology, which maximizes training accuracy.</p>
</div>
<div id="S3.p16" class="ltx_para ltx_noindent">
<p id="S3.p16.1" class="ltx_p"><span id="S3.p16.1.1" class="ltx_text ltx_font_bold">Clustering-based Selection.</span> Various clients can be selected with different data distributions using clustered sampling. In¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib26" title="" class="ltx_ref">fraboni2021clustered, </a>)</cite>, an unbiased clustered sampling scheme for client selection is introduced that decreases weights‚Äô variance for the client aggregation and ensures that clients with unique distributions are more likely to be selected. The authors introduce two clustered sampling schemes: 1) clustered sampling based on sample size, and 2) clustered sampling based on similarity. They experimentally show that clustered sampling leads to better and faster convergence.</p>
</div>
<div id="S3.p17" class="ltx_para">
<p id="S3.p17.1" class="ltx_p">In Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3. Participant selection in FL ‚Ä£ A Survey on Participant Selection for Federated Learning in Mobile Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we summarize the existing studies and their approaches, and draw a comparison of them in terms of solving different challenges.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Future Research Directions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Participant selection in FL is still in its infancy and there are
some open challenges that require to be addressed. We have identified the following
main research directions:
</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Optimal and Adaptive Reputation Threshold.</span> Existing reputation-based studies consider a pre-defined threshold in order to distinguish malicious devices from honest ones.
If this threshold is adjusted too high, honest devices may lose the opportunity to participate in the FL process. On the other hand, if the aforementioned threshold is set too low, malicious devices may be selected for training and manipulate the model. It is, therefore, essential to optimize the threshold value, particularly, in safety-critical applications.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Hierarchical Aggregation.</span> All the reviewed approaches use centralized aggregation, wherein clients are required to transmit their local model parameters to a single aggregator server. When the number of devices increase, limited network bandwidth becomes a scalability bottleneck in centralized FL. On the other hand, communication between end devices and a remote server might be intermittent or unavailable.
In hierarchical FL, end devices are divided into a number of clusters and participants transmit their local models to the cluster head for intermediate model aggregation. All cluster heads communicate with the FL server for global aggregation. This approach reduces the device dropout and improves scalability. To reduce communication cost and improve performance of FL, an efficient mechanism for determining the cluster structures is of paramount importance.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Highly Dynamic Environment.</span> In mobile networks, i.e., the Internet of Vehicles with high mobility nodes and a highly dynamic topology, intermittent connectivity of devices with the FL server along with a constant change in data transmission time may degrade the performance of FL process. After client selection, some participants may get out of network coverage and significantly affect the system behaviour. It is important to handle high mobility environment and minimize the training performance degradation.
Decentralized FL can be a proper solution for high mobility clients that may
not have access to the aggregation server. In decentralized FL, clients can only share their local model parameters with nearby devices using device-to-device communication without relying on a single server¬†<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib27" title="" class="ltx_ref">barbieri2022decentralized, </a>)</cite>. Device selection algorithms in decentralized FL can have a significant impact on the performance of FL performance. Social trust values can be integrated with other selection criteria to determine the best nearby clients.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Asynchronous Training.</span> All existing works are based on synchronous training, wherein the aggregation server has to wait for receiving all the local models before aggregation. Therefore, <em id="S4.p5.1.2" class="ltx_emph ltx_font_italic">stragglers</em> prolong the training time owing to data and device heterogeneity. In asynchronous FL, the server is not affected by <em id="S4.p5.1.3" class="ltx_emph ltx_font_italic">stragglers</em> and can update the global model without waiting to collect all the local models. Participant selection in asynchronous FL needs to be studied so that appropriate selection approaches are proposed.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Participant selection in federated learning (FL) has a significant impact on the final model accuracy and training time of FL.
In this paper,
we explore the main FL challenges of the participant selection process. We review the existing approaches and conduct a comparison of the existing studies in terms of solving different challenges. Finally, we identify some research directions and hope to stimulate further research in this important topic.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim
Verbelen, and Jan¬†S Rellermeyer.

</span>
<span class="ltx_bibblock">A survey on distributed machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 53(2):1‚Äì33, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise¬†Aguera
y¬†Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273‚Äì1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, and Leandros Tassiulas.

</span>
<span class="ltx_bibblock">Tackling system and statistical heterogeneity for federated learning
with adaptive client sampling.

</span>
<span class="ltx_bibblock">pages 1‚Äì10, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.

</span>
<span class="ltx_bibblock">On the convergence of fedavg on non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.02189</span>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hao Wang, Zakhary Kaplan, Di¬†Niu, and Baochun Li.

</span>
<span class="ltx_bibblock">Optimizing federated learning on non-iid data with reinforcement
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2020-IEEE Conference on Computer
Communications</span>, pages 1698‚Äì1707. IEEE, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wenyu Zhang, Xiumin Wang, Pan Zhou, Weiwei Wu, and Xinglin Zhang.

</span>
<span class="ltx_bibblock">Client selection for federated learning with non-iid data in mobile
edge computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 9:24462‚Äì24474, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yipeng Zhou, Yao Fu, Zhenxiao Luo, Miao Hu, Di¬†Wu, Quan¬†Z Sheng, and Shui Yu.

</span>
<span class="ltx_bibblock">The role of communication time in the convergence of federated edge
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Vehicular Technology</span>, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang.

</span>
<span class="ltx_bibblock">Incentive mechanism for reliable federated learning: A joint
optimization approach to combining reputation and contract theory.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 6(6):10700‚Äì10714, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and Mohsen
Guizani.

</span>
<span class="ltx_bibblock">Reliable federated learning for mobile networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 27(2):72‚Äì80, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Tiansheng Huang, Weiwei Lin, Li¬†Shen, Keqin Li, and Albert¬†Y Zomaya.

</span>
<span class="ltx_bibblock">Stochastic client selection for federated learning with volatile
clients.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Takayuki Nishio and Ryo Yonetani.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources
in mobile edge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE international conference on communications (ICC)</span>, pages
1‚Äì7. IEEE, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jaewook Lee, Haneul Ko, and Sangheon Pack.

</span>
<span class="ltx_bibblock">Adaptive deadline determination for mobile device selection in
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Vehicular Technology</span>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sawsan AbdulRahman, Hanine Tout, Azzam Mourad, and Chamseddine Talhi.

</span>
<span class="ltx_bibblock">Fedmccs: Multicriteria client selection model for optimal iot
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(6):4723‚Äì4735, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Adnan Mahmood, Quan¬†Z Sheng, Sarah¬†Ali Siddiqui, Subhash Sagar, Wei¬†Emma Zhang,
Hajime Suzuki, and Wei Ni.

</span>
<span class="ltx_bibblock">When trust meets the internet of vehicles: opportunities, challenges,
and future prospects.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2021 IEEE 7th International Conference on Collaboration and
Internet Computing (CIC)</span>, pages 60‚Äì67. IEEE, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Adnan Mahmood, Sarah¬†Ali Siddiqui, Quan¬†Z Sheng, Wei¬†Emma Zhang, Hajime Suzuki,
and Wei Ni.

</span>
<span class="ltx_bibblock">Trust on wheels: Towards secure and resource efficient IoV
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Computing</span>, pages 1‚Äì22, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Yining Liu, Keqiu Li, Yingwei Jin, Yong Zhang, and Wenyu Qu.

</span>
<span class="ltx_bibblock">A novel reputation computation model based on subjective logic for
mobile ad hoc networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 27(5):547‚Äì554, 2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yue Zou, Fei Shen, Feng Yan, Jing Lin, and Yunzhou Qiu.

</span>
<span class="ltx_bibblock">Reputation-based regional federated learning for knowledge trading in
blockchain-enhanced IoV.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2021 IEEE Wireless Communications and Networking Conference
(WCNC)</span>, pages 1‚Äì6. IEEE, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhendong Song, Hongguang Sun, Howard¬†H Yang, Xijun Wang, Yan Zhang, and Tony¬†QS
Quek.

</span>
<span class="ltx_bibblock">Reputation-based federated learning for secure wireless networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 9(2):1212‚Äì1226, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jake Perazzone, Shiqiang Wang, Mingyue Ji, and Kevin Chan.

</span>
<span class="ltx_bibblock">Communication-efficient device scheduling for federated learning
using stochastic optimization.

</span>
<span class="ltx_bibblock">pages 1‚Äì10, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Hongda Wu and Ping Wang.

</span>
<span class="ltx_bibblock">Node selection toward faster convergence for federated learning on
non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Network Science and Engineering</span>, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wenchao Xia, Tony¬†QS Quek, Kun Guo, Wanli Wen, Howard¬†H Yang, and Hongbo Zhu.

</span>
<span class="ltx_bibblock">Multi-armed bandit-based client scheduling for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>,
19(11):7108‚Äì7123, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Tiansheng Huang, Weiwei Lin, Wentai Wu, Ligang He, Keqin Li, and Albert¬†Y
Zomaya.

</span>
<span class="ltx_bibblock">An efficiency-boosting client selection scheme for federated learning
with fairness guarantee.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
32(7):1552‚Äì1564, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jiahua Ma, Xinghua Sun, Wenchao Xia, Xijun Wang, Xiang Chen, and Hongbo Zhu.

</span>
<span class="ltx_bibblock">Client selection based on label quantity information for federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2021 IEEE 32nd Annual International Symposium on Personal,
Indoor and Mobile Radio Communications (PIMRC)</span>, pages 1‚Äì6. IEEE, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jianxin Zhao, Xinyu Chang, Yanhao Feng, Chi¬†Harold Liu, and Ningbo Liu.

</span>
<span class="ltx_bibblock">Participant selection for federated learning with heterogeneous data
in intelligent transport system.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Su¬†Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang,
and Christopher¬†G Brinton.

</span>
<span class="ltx_bibblock">Device sampling for heterogeneous federated learning: Theory,
algorithms, and implementation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2021-IEEE Conference on Computer
Communications</span>, pages 1‚Äì10. IEEE, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi.

</span>
<span class="ltx_bibblock">Clustered sampling: Low-variance and improved representativity for
clients selection in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
3407‚Äì3416. PMLR, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Luca Barbieri, Stefano Savazzi, Mattia Brambilla, and Monica Nicoli.

</span>
<span class="ltx_bibblock">Decentralized federated learning for extended sensing in 6g connected
vehicles.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Vehicular Communications</span>, 33:100396, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.03680" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.03681" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.03681">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.03681" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.03682" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 16:16:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
