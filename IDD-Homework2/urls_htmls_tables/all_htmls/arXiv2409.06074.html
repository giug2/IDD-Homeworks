<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SVS-GAN: Leveraging GANs for Semantic Video Synthesis</title>
<!--Generated on Mon Sep  9 20:58:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.06074v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S1" title="In SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S2" title="In SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S2.SS1" title="In 2 Related Work ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Semantic Image Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S2.SS2" title="In 2 Related Work ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Video-to-Video</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S3" title="In SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S3.SS1" title="In 3 Method ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Generator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S3.SS2" title="In 3 Method ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Discriminators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S3.SS3" title="In 3 Method ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Losses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4" title="In SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.SS1" title="In 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.SS2" title="In 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.SS3" title="In 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.SS4" title="In 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.SS5" title="In 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S5" title="In SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SVS-GAN: Leveraging GANs for Semantic Video Synthesis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Khaled M. Seyam
<br class="ltx_break"/>Institute of Signal Processing and System Theory, University of Stuttgart, Germany
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">khaled.seyam@iss.uni-stuttgart.de</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julian Wiederer
<br class="ltx_break"/>Mercedes-Benz AG, Stuttgart, Germany
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">julian.wiederer@mercedes-benz.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Markus Braun
<br class="ltx_break"/>Mercedes-Benz AG, Stuttgart, Germany
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1" style="font-size:90%;">markus.ma.braun@mercedes-benz.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bin Yang
<br class="ltx_break"/>Institute of Signal Processing and System Theory, University of Stuttgart, Germany
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1" style="font-size:90%;">bin.yang@iss.uni-stuttgart.de</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">In recent years, there has been a growing interest in Semantic Image Synthesis (SIS) through the use of Generative Adversarial Networks (GANs) and diffusion models. This field has seen innovations such as the implementation of specialized loss functions tailored for this task, diverging from the more general approaches in Image-to-Image (I2I) translation. While the concept of Semantic Video Synthesis (SVS)—the generation of temporally coherent, realistic sequences of images from semantic maps—is newly formalized in this paper, some existing methods have already explored aspects of this field. Most of these approaches rely on generic loss functions designed for video-to-video translation or require additional data to achieve temporal coherence. In this paper, we introduce the SVS-GAN, a framework specifically designed for SVS, featuring a custom architecture and loss functions. Our approach includes a triple-pyramid generator that utilizes SPADE blocks. Additionally, we employ a U-Net-based network for the image discriminator, which performs semantic segmentation for the OASIS loss. Through this combination of tailored architecture and objective engineering, our framework aims to bridge the existing gap between SIS and SVS, outperforming current state-of-the-art models on datasets like Cityscapes and KITTI-360.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="420" id="S1.F1.g1" src="x1.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.4.2" style="font-size:90%;">Illustration of our SVS-GAN’s capabilities in generating high-fidelity videos that closely mimic the details of the initial reference image while following the given sequence of semantic maps, demonstrating the model’s effectiveness in accurately synthesizing the 30<sup class="ltx_sup" id="S1.F1.4.2.1">th</sup> frame within the sequence.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Video generation has been drawing more attention in recent years using GANs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib37" title="">37</a>]</cite> or diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib9" title="">9</a>]</cite>, but it faces increasing challenges since it involves processing much larger amounts of data in comparison with research areas working with non-temporal/single-shot 2D images. Despite these challenges, there have been significant developments in models designed for perception applications<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib39" title="">39</a>]</cite>, like autonomous driving, which requires extensive video training data along with their ground truth labels. This progress highlights the growing importance of video generative models, particularly in the area of controllable generation. Approaches within this area allow for precise placement and manipulation of generated content to meet specific requirements, which is crucial for creating realistic simulations, customizing outputs for specific applications, and ensuring consistency across generated sequences. Improving these models would enable researchers to augment the data available for training and testing, as well as generate controlled outputs for new and diverse scenarios, such as relevant but rare corner cases in autonomous driving and data generation under domain shifts.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Video generation techniques are divided into two main approaches: unconditional<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib33" title="">33</a>]</cite> and conditional<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib14" title="">14</a>]</cite>. Unconditional video generation creates videos from random noise without any guiding input, while conditional video generation relies on specific inputs to direct the creation process. These inputs can range from text descriptions and object bounding boxes to detailed scene layouts, which help in dictating the structure and content of the generated video.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we introduce Semantic Video Synthesis (SVS), a form of conditional video generation using semantic map sequences to produce realistic, temporally coherent videos that align accurately with the sequence of semantic maps. This task poses greater complexity than Semantic Image Synthesis (SIS), which generates single images from semantic maps. SVS’s primary challenge is maintaining visual consistency across frames, crucial for realistic outputs. However, GPU limitations restrict handling detailed, long video sequences. Despite these challenges, SVS provides per-pixel control over video outputs, offering better precision in video appearance and layout compared to textual inputs, thus enhancing content and geometry customization in video creation.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite the fact that SVS has not explicitly been defined as a research area before, the following generic methods already tackle it either using extra modalities along the semantic map or without focusing explicitly on semantic maps as conditional input. The first approach, a Video-to-Video (V2V) synthesis method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite>, follows a general strategy for video generation similar to the Image-to-Image (I2I) translation framework used in pix2pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib17" title="">17</a>]</cite>. This approach does not utilize tailored losses or architecture modifications that leverage semantic inputs, leading to poor performance since it fails to capitalize on the rich structural data within the maps. The second study, World-Consistent Video-to-Video (WC-V2V)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib29" title="">29</a>]</cite>, enhances the generator architecture by incorporating SPADE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib30" title="">30</a>]</cite> blocks and focuses on using semantic maps as input. However, it also integrates additional 3D world inputs, thereby deviating from the primary goal of SVS, which is to generate realistic videos directly from semantic map sequences without additional data.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address the mentioned limitations and motivated by the advancements in SIS, we introduce SVS-GAN, a framework designed to bridge the gap between SVS and SIS. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">1</span></a> showcases results from our model, which processes a sequence of semantic maps alongside a reference image <math alttext="I_{1}" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><msub id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">I</mi><mn id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1">subscript</csymbol><ci id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">𝐼</ci><cn id="S1.p5.1.m1.1.1.3.cmml" type="integer" xref="S1.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">I_{1}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>—either real or generated by any SIS framework. SVS-GAN then autoregressively generates the subsequent sequence of frames.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our framework introduces a novel triple pyramid generator architecture tailored specifically for SVS. Additionally, we are the first to introduce the OASIS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib34" title="">34</a>]</cite> loss in video generation, which significantly improves semantic alignment metrics. This is facilitated by using our encoder-decoder discriminator, which is capable of segmenting images. Our evaluations demonstrate that SVS-GAN surpasses existing state-of-the-art approaches in image quality, temporal coherence, and semantic alignment without requiring additional inputs and by considering only one previous frame. Additionally, our model operates efficiently on a single GPU at a resolution of <math alttext="1024\times 512" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mn id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">1024</mn><mo id="S1.p6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S1.p6.1.m1.1.1.1.cmml">×</mo><mn id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><times id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></times><cn id="S1.p6.1.m1.1.1.2.cmml" type="integer" xref="S1.p6.1.m1.1.1.2">1024</cn><cn id="S1.p6.1.m1.1.1.3.cmml" type="integer" xref="S1.p6.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">1024\times 512</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">1024 × 512</annotation></semantics></math>, achieving an inference time of 40ms per frame.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our architecture is versatile enough to be applied to any SVS task, yet we have specifically targeted driving scenarios due to their complexity and growing relevance in the field of autonomous driving. These scenarios are particularly challenging because they involve dual movement dynamics: the motion of the camera, represented by the ego vehicle, and the interactions of other vehicles and pedestrians. We validate our framework on the Cityscapes Sequence<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib2" title="">2</a>]</cite> and KITTI-360<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib24" title="">24</a>]</cite> datasets, achieving state-of-the-art performance on both. A detailed ablation study highlights the impact of each component in our framework.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Data generation plays a crucial role in the field of machine learning, serving as a data augmentor for training various discriminative models. This includes a wide array of data types like images, videos, and additional sensor outputs such as LiDAR. Within neural networks, two main approaches stand out for data generation: GANs and diffusion models. Recently, diffusion models have made significant improvements in producing realistic outputs, surpassing GANs in some aspects. However, they are hampered by long training and inference times, and in scenarios such as SIS, their output quality remains comparable to that of GANs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Semantic Image Synthesis</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.2">SIS falls under the broader category of I2I translation, where the conditioning image is a semantic map. The concept of I2I using GANs was pioneered by the pix2pix framework, which introduced an approach to translate images from one domain <math alttext="X" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_X</annotation></semantics></math> to another domain <math alttext="Y" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_Y</annotation></semantics></math> without specific domain constraints<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib17" title="">17</a>]</cite>. This framework could perform various tasks, such as image colorization and SIS. Subsequent developments in I2I mainly focused on application-specific enhancements. Several studies, including those on SPADE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib30" title="">30</a>]</cite>, OASIS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib34" title="">34</a>]</cite>, and DP-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib23" title="">23</a>]</cite>, have concentrated solely on SIS, evolving architectures and loss functions tailored to generate realistic images that closely align with the semantic map. SIS typically functions within a supervised learning framework, relying on paired datasets. However, the scarcity of such data has encouraged interest in Unsupervised SIS (USIS)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib4" title="">4</a>]</cite> and semi-supervised approaches<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">SPADE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib30" title="">30</a>]</cite> emerged as a solution in SIS by integrating a spatially adaptive denormalization layer into the generator. The authors demonstrated that directly feeding the semantic map to the generator was not optimal. Instead, SPADE modifies the normalization process by utilizing segmentation maps to adaptively scale and shift the normalized output. This technique enables the generation of images that adhere more faithfully to the input semantic layout, preserving spatial information typically lost in standard normalization layers.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Previously, many frameworks presented the discriminator with the real or fake image alongside its corresponding semantic map. However, this approach did not guarantee that the discriminator would leverage the semantic map as a guide in the discrimination process. OASIS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib34" title="">34</a>]</cite> addressed this by implementing an encoder-decoder architecture in the discriminator, that not only produces real/fake prediction, but segments the image. This structure encourages the discriminator to learn image segmentation, while the generator tries to fool the discriminator by creating images that can be accurately segmented by it.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Video-to-Video</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.3">The definition of V2V synthesis has varied across studies, with some interpreting it as a method for converting between any two video domains<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite>, and others specifically linking it to SVS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib29" title="">29</a>]</cite>. In our study, we will use V2V to denote the transformation of a video from one arbitrary domain <math alttext="X" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_X</annotation></semantics></math> to another <math alttext="Y" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_Y</annotation></semantics></math>. Similarly, we will define SVS in a manner similar to SIS, with the domain <math alttext="X" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_X</annotation></semantics></math> limited to sequences of semantic maps.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The initial application of GANs to these tasks was in 2018<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite>, which explored video transformations such as from pose to dance, from sketches to faces, and SVS. The generator <math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_G</annotation></semantics></math> was designed to consider both the two previous frames from the source and target domains, along with the current source image, to produce the next frame in the target sequence. The work presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite> approached the task as an autoregressive next-frame prediction, incorporating a specialized module to calculate optical flow<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib28" title="">28</a>]</cite> for image warping, complemented by an additional module for generating rapidly-moving foreground objects that the optical flow could not track. Subsequently, Fast-V2V<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib43" title="">43</a>]</cite> was introduced, adding to the system spatial-temporal compression techniques to accelerate inference and minimize GPU load. Additionally, the scarcity of paired datasets motivated the development of unsupervised frameworks<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">A later development in this field was the introduction of WC-V2V, which prioritized maintaining consistency across the entire world in the video rather than ensuring only frame-to-frame continuity<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib29" title="">29</a>]</cite>. Unlike traditional V2V that processes just the two preceding frames, WC-V2V utilizes the entirety of preceding frames to synthesize the current frame, necessitating a 3D world representation derived from actual images using structure from motion (SfM)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib36" title="">36</a>]</cite>. This requirement, however, limits its applicability to pure SVS, as creating a reliable 3D model from real images can be impractical.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Our main contributions are threefold:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">We formally define SVS and introduce a dedicated framework for pure SVS. Utilizing a sequence of semantic maps as inputs, our framework generates a corresponding sequence of realistic, and spatially and temporally coherent video frames.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">We develop a novel triple-pyramid generator architecture that leverages semantic maps and information from the past frame to predict visually coherent and semantically accurate frames.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">We integrate the OASIS loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib34" title="">34</a>]</cite> into our SVS framework, enhancing the alignment of synthesized videos with their corresponding semantic maps.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Our contributions have led to state-of-the-art performance on datasets such as Cityscapes and KITTI-360, excelling in metrics that assess image quality, video quality, and adherence to the semantic map.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S3.F2.g1" src="x2.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.14.7.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.12.6" style="font-size:90%;">Architectural illustration highlighting the generator <math alttext="G" class="ltx_Math" display="inline" id="S3.F2.7.1.m1.1"><semantics id="S3.F2.7.1.m1.1b"><mi id="S3.F2.7.1.m1.1.1" xref="S3.F2.7.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.1.m1.1c"><ci id="S3.F2.7.1.m1.1.1.cmml" xref="S3.F2.7.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.1.m1.1d">G</annotation><annotation encoding="application/x-llamapun" id="S3.F2.7.1.m1.1e">italic_G</annotation></semantics></math> and the discriminators <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.F2.8.2.m2.1"><semantics id="S3.F2.8.2.m2.1b"><msub id="S3.F2.8.2.m2.1.1" xref="S3.F2.8.2.m2.1.1.cmml"><mi id="S3.F2.8.2.m2.1.1.2" xref="S3.F2.8.2.m2.1.1.2.cmml">D</mi><mi id="S3.F2.8.2.m2.1.1.3" xref="S3.F2.8.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.8.2.m2.1c"><apply id="S3.F2.8.2.m2.1.1.cmml" xref="S3.F2.8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.8.2.m2.1.1.1.cmml" xref="S3.F2.8.2.m2.1.1">subscript</csymbol><ci id="S3.F2.8.2.m2.1.1.2.cmml" xref="S3.F2.8.2.m2.1.1.2">𝐷</ci><ci id="S3.F2.8.2.m2.1.1.3.cmml" xref="S3.F2.8.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.2.m2.1d">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.8.2.m2.1e">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.F2.9.3.m3.1"><semantics id="S3.F2.9.3.m3.1b"><msub id="S3.F2.9.3.m3.1.1" xref="S3.F2.9.3.m3.1.1.cmml"><mi id="S3.F2.9.3.m3.1.1.2" xref="S3.F2.9.3.m3.1.1.2.cmml">D</mi><mi id="S3.F2.9.3.m3.1.1.3" xref="S3.F2.9.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.9.3.m3.1c"><apply id="S3.F2.9.3.m3.1.1.cmml" xref="S3.F2.9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.9.3.m3.1.1.1.cmml" xref="S3.F2.9.3.m3.1.1">subscript</csymbol><ci id="S3.F2.9.3.m3.1.1.2.cmml" xref="S3.F2.9.3.m3.1.1.2">𝐷</ci><ci id="S3.F2.9.3.m3.1.1.3.cmml" xref="S3.F2.9.3.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.3.m3.1d">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.9.3.m3.1e">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>. The generator features an optical flow and occlusion map network illustrated at the top, followed by a triple-pyramid structure comprising a warped image encoder, a semantic map encoder, and an image generation decoder. The decoder employs SPADE blocks to incorporate the semantic map and utilizes <math alttext="\widehat{OC}_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.F2.10.4.m4.1"><semantics id="S3.F2.10.4.m4.1b"><msub id="S3.F2.10.4.m4.1.1" xref="S3.F2.10.4.m4.1.1.cmml"><mover accent="true" id="S3.F2.10.4.m4.1.1.2" xref="S3.F2.10.4.m4.1.1.2.cmml"><mrow id="S3.F2.10.4.m4.1.1.2.2" xref="S3.F2.10.4.m4.1.1.2.2.cmml"><mi id="S3.F2.10.4.m4.1.1.2.2.2" xref="S3.F2.10.4.m4.1.1.2.2.2.cmml">O</mi><mo id="S3.F2.10.4.m4.1.1.2.2.1" xref="S3.F2.10.4.m4.1.1.2.2.1.cmml">⁢</mo><mi id="S3.F2.10.4.m4.1.1.2.2.3" xref="S3.F2.10.4.m4.1.1.2.2.3.cmml">C</mi></mrow><mo id="S3.F2.10.4.m4.1.1.2.1" xref="S3.F2.10.4.m4.1.1.2.1.cmml">^</mo></mover><mrow id="S3.F2.10.4.m4.1.1.3" xref="S3.F2.10.4.m4.1.1.3.cmml"><mrow id="S3.F2.10.4.m4.1.1.3.2" xref="S3.F2.10.4.m4.1.1.3.2.cmml"><mi id="S3.F2.10.4.m4.1.1.3.2.2" xref="S3.F2.10.4.m4.1.1.3.2.2.cmml">i</mi><mo id="S3.F2.10.4.m4.1.1.3.2.1" xref="S3.F2.10.4.m4.1.1.3.2.1.cmml">−</mo><mn id="S3.F2.10.4.m4.1.1.3.2.3" xref="S3.F2.10.4.m4.1.1.3.2.3.cmml">1</mn></mrow><mo id="S3.F2.10.4.m4.1.1.3.1" stretchy="false" xref="S3.F2.10.4.m4.1.1.3.1.cmml">→</mo><mi id="S3.F2.10.4.m4.1.1.3.3" xref="S3.F2.10.4.m4.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.F2.10.4.m4.1c"><apply id="S3.F2.10.4.m4.1.1.cmml" xref="S3.F2.10.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F2.10.4.m4.1.1.1.cmml" xref="S3.F2.10.4.m4.1.1">subscript</csymbol><apply id="S3.F2.10.4.m4.1.1.2.cmml" xref="S3.F2.10.4.m4.1.1.2"><ci id="S3.F2.10.4.m4.1.1.2.1.cmml" xref="S3.F2.10.4.m4.1.1.2.1">^</ci><apply id="S3.F2.10.4.m4.1.1.2.2.cmml" xref="S3.F2.10.4.m4.1.1.2.2"><times id="S3.F2.10.4.m4.1.1.2.2.1.cmml" xref="S3.F2.10.4.m4.1.1.2.2.1"></times><ci id="S3.F2.10.4.m4.1.1.2.2.2.cmml" xref="S3.F2.10.4.m4.1.1.2.2.2">𝑂</ci><ci id="S3.F2.10.4.m4.1.1.2.2.3.cmml" xref="S3.F2.10.4.m4.1.1.2.2.3">𝐶</ci></apply></apply><apply id="S3.F2.10.4.m4.1.1.3.cmml" xref="S3.F2.10.4.m4.1.1.3"><ci id="S3.F2.10.4.m4.1.1.3.1.cmml" xref="S3.F2.10.4.m4.1.1.3.1">→</ci><apply id="S3.F2.10.4.m4.1.1.3.2.cmml" xref="S3.F2.10.4.m4.1.1.3.2"><minus id="S3.F2.10.4.m4.1.1.3.2.1.cmml" xref="S3.F2.10.4.m4.1.1.3.2.1"></minus><ci id="S3.F2.10.4.m4.1.1.3.2.2.cmml" xref="S3.F2.10.4.m4.1.1.3.2.2">𝑖</ci><cn id="S3.F2.10.4.m4.1.1.3.2.3.cmml" type="integer" xref="S3.F2.10.4.m4.1.1.3.2.3">1</cn></apply><ci id="S3.F2.10.4.m4.1.1.3.3.cmml" xref="S3.F2.10.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.4.m4.1d">\widehat{OC}_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.10.4.m4.1e">over^ start_ARG italic_O italic_C end_ARG start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to modulate features passed through the skip connections using the occlusion mask fusion module. On the right, the discriminator <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.F2.11.5.m5.1"><semantics id="S3.F2.11.5.m5.1b"><msub id="S3.F2.11.5.m5.1.1" xref="S3.F2.11.5.m5.1.1.cmml"><mi id="S3.F2.11.5.m5.1.1.2" xref="S3.F2.11.5.m5.1.1.2.cmml">D</mi><mi id="S3.F2.11.5.m5.1.1.3" xref="S3.F2.11.5.m5.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.11.5.m5.1c"><apply id="S3.F2.11.5.m5.1.1.cmml" xref="S3.F2.11.5.m5.1.1"><csymbol cd="ambiguous" id="S3.F2.11.5.m5.1.1.1.cmml" xref="S3.F2.11.5.m5.1.1">subscript</csymbol><ci id="S3.F2.11.5.m5.1.1.2.cmml" xref="S3.F2.11.5.m5.1.1.2">𝐷</ci><ci id="S3.F2.11.5.m5.1.1.3.cmml" xref="S3.F2.11.5.m5.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.5.m5.1d">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.11.5.m5.1e">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> employs an encoder/decoder setup for image segmentation using the OASIS loss, while above, <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.F2.12.6.m6.1"><semantics id="S3.F2.12.6.m6.1b"><msub id="S3.F2.12.6.m6.1.1" xref="S3.F2.12.6.m6.1.1.cmml"><mi id="S3.F2.12.6.m6.1.1.2" xref="S3.F2.12.6.m6.1.1.2.cmml">D</mi><mi id="S3.F2.12.6.m6.1.1.3" xref="S3.F2.12.6.m6.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.12.6.m6.1c"><apply id="S3.F2.12.6.m6.1.1.cmml" xref="S3.F2.12.6.m6.1.1"><csymbol cd="ambiguous" id="S3.F2.12.6.m6.1.1.1.cmml" xref="S3.F2.12.6.m6.1.1">subscript</csymbol><ci id="S3.F2.12.6.m6.1.1.2.cmml" xref="S3.F2.12.6.m6.1.1.2">𝐷</ci><ci id="S3.F2.12.6.m6.1.1.3.cmml" xref="S3.F2.12.6.m6.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.6.m6.1d">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.12.6.m6.1e">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> functions as a video patch discriminator, aiming for temporal consistency.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.3">Our model addresses the task of SVS by predicting the next frame based on a given semantic map through an autoregressive video generation process. Each frame <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">I</mi><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝐼</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> depends on the previously generated image, denoted as <math alttext="I_{i-1}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">I</mi><mrow id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mi id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.p1.2.m2.1.1.3.1" xref="S3.p1.2.m2.1.1.3.1.cmml">−</mo><mn id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝐼</ci><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><minus id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3.1"></minus><ci id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">𝑖</ci><cn id="S3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">I_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and aligns with the current semantic map <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">S</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑆</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. This approach allows the creation of videos with variable lengths that are tailored to the input semantic maps.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Our new framework for SVS consists of three primary components:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Generator (<math alttext="G" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.1.m1.1"><semantics id="S3.I1.i1.p1.1.1.m1.1a"><mi id="S3.I1.i1.p1.1.1.m1.1.1" xref="S3.I1.i1.p1.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.1.m1.1b"><ci id="S3.I1.i1.p1.1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.1.m1.1d">italic_G</annotation></semantics></math>)</span>: The generator takes as input both past and current semantic maps (<math alttext="S_{i-1}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m1.1"><semantics id="S3.I1.i1.p1.2.m1.1a"><msub id="S3.I1.i1.p1.2.m1.1.1" xref="S3.I1.i1.p1.2.m1.1.1.cmml"><mi id="S3.I1.i1.p1.2.m1.1.1.2" xref="S3.I1.i1.p1.2.m1.1.1.2.cmml">S</mi><mrow id="S3.I1.i1.p1.2.m1.1.1.3" xref="S3.I1.i1.p1.2.m1.1.1.3.cmml"><mi id="S3.I1.i1.p1.2.m1.1.1.3.2" xref="S3.I1.i1.p1.2.m1.1.1.3.2.cmml">i</mi><mo id="S3.I1.i1.p1.2.m1.1.1.3.1" xref="S3.I1.i1.p1.2.m1.1.1.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.2.m1.1.1.3.3" xref="S3.I1.i1.p1.2.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m1.1b"><apply id="S3.I1.i1.p1.2.m1.1.1.cmml" xref="S3.I1.i1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.2.m1.1.1.1.cmml" xref="S3.I1.i1.p1.2.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.2.m1.1.1.2.cmml" xref="S3.I1.i1.p1.2.m1.1.1.2">𝑆</ci><apply id="S3.I1.i1.p1.2.m1.1.1.3.cmml" xref="S3.I1.i1.p1.2.m1.1.1.3"><minus id="S3.I1.i1.p1.2.m1.1.1.3.1.cmml" xref="S3.I1.i1.p1.2.m1.1.1.3.1"></minus><ci id="S3.I1.i1.p1.2.m1.1.1.3.2.cmml" xref="S3.I1.i1.p1.2.m1.1.1.3.2">𝑖</ci><cn id="S3.I1.i1.p1.2.m1.1.1.3.3.cmml" type="integer" xref="S3.I1.i1.p1.2.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m1.1c">S_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m1.1d">italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m2.1"><semantics id="S3.I1.i1.p1.3.m2.1a"><msub id="S3.I1.i1.p1.3.m2.1.1" xref="S3.I1.i1.p1.3.m2.1.1.cmml"><mi id="S3.I1.i1.p1.3.m2.1.1.2" xref="S3.I1.i1.p1.3.m2.1.1.2.cmml">S</mi><mi id="S3.I1.i1.p1.3.m2.1.1.3" xref="S3.I1.i1.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m2.1b"><apply id="S3.I1.i1.p1.3.m2.1.1.cmml" xref="S3.I1.i1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.3.m2.1.1.1.cmml" xref="S3.I1.i1.p1.3.m2.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.3.m2.1.1.2.cmml" xref="S3.I1.i1.p1.3.m2.1.1.2">𝑆</ci><ci id="S3.I1.i1.p1.3.m2.1.1.3.cmml" xref="S3.I1.i1.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m2.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m2.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) to calculate the optical flow. It also utilizes appearance information from the previous image <math alttext="I_{i-1}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.4.m3.1"><semantics id="S3.I1.i1.p1.4.m3.1a"><msub id="S3.I1.i1.p1.4.m3.1.1" xref="S3.I1.i1.p1.4.m3.1.1.cmml"><mi id="S3.I1.i1.p1.4.m3.1.1.2" xref="S3.I1.i1.p1.4.m3.1.1.2.cmml">I</mi><mrow id="S3.I1.i1.p1.4.m3.1.1.3" xref="S3.I1.i1.p1.4.m3.1.1.3.cmml"><mi id="S3.I1.i1.p1.4.m3.1.1.3.2" xref="S3.I1.i1.p1.4.m3.1.1.3.2.cmml">i</mi><mo id="S3.I1.i1.p1.4.m3.1.1.3.1" xref="S3.I1.i1.p1.4.m3.1.1.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.4.m3.1.1.3.3" xref="S3.I1.i1.p1.4.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m3.1b"><apply id="S3.I1.i1.p1.4.m3.1.1.cmml" xref="S3.I1.i1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m3.1.1.1.cmml" xref="S3.I1.i1.p1.4.m3.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.4.m3.1.1.2.cmml" xref="S3.I1.i1.p1.4.m3.1.1.2">𝐼</ci><apply id="S3.I1.i1.p1.4.m3.1.1.3.cmml" xref="S3.I1.i1.p1.4.m3.1.1.3"><minus id="S3.I1.i1.p1.4.m3.1.1.3.1.cmml" xref="S3.I1.i1.p1.4.m3.1.1.3.1"></minus><ci id="S3.I1.i1.p1.4.m3.1.1.3.2.cmml" xref="S3.I1.i1.p1.4.m3.1.1.3.2">𝑖</ci><cn id="S3.I1.i1.p1.4.m3.1.1.3.3.cmml" type="integer" xref="S3.I1.i1.p1.4.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m3.1c">I_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.4.m3.1d">italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and uses the current map <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.5.m4.1"><semantics id="S3.I1.i1.p1.5.m4.1a"><msub id="S3.I1.i1.p1.5.m4.1.1" xref="S3.I1.i1.p1.5.m4.1.1.cmml"><mi id="S3.I1.i1.p1.5.m4.1.1.2" xref="S3.I1.i1.p1.5.m4.1.1.2.cmml">S</mi><mi id="S3.I1.i1.p1.5.m4.1.1.3" xref="S3.I1.i1.p1.5.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m4.1b"><apply id="S3.I1.i1.p1.5.m4.1.1.cmml" xref="S3.I1.i1.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m4.1.1.1.cmml" xref="S3.I1.i1.p1.5.m4.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.5.m4.1.1.2.cmml" xref="S3.I1.i1.p1.5.m4.1.1.2">𝑆</ci><ci id="S3.I1.i1.p1.5.m4.1.1.3.cmml" xref="S3.I1.i1.p1.5.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m4.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.5.m4.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to ensure the generated frame adheres to the specified semantic guidance.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Image Discriminator (<math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.1.m1.1"><semantics id="S3.I1.i2.p1.1.1.m1.1a"><msub id="S3.I1.i2.p1.1.1.m1.1.1" xref="S3.I1.i2.p1.1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.1.m1.1.1.2.cmml">D</mi><mi id="S3.I1.i2.p1.1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.1.m1.1b"><apply id="S3.I1.i2.p1.1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i2.p1.1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.1.m1.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math>)</span>: This component evaluates individual frames independently to determine their realism and conformity to the semantic map.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Video Discriminator (<math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.1.m1.1"><semantics id="S3.I1.i3.p1.1.1.m1.1a"><msub id="S3.I1.i3.p1.1.1.m1.1.1" xref="S3.I1.i3.p1.1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.1.m1.1.1.2.cmml">D</mi><mi id="S3.I1.i3.p1.1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.1.m1.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.1.m1.1b"><apply id="S3.I1.i3.p1.1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.1.m1.1.1.2">𝐷</ci><ci id="S3.I1.i3.p1.1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.1.m1.1c">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>)</span>: The video discriminator assesses groups of generated frames together to evaluate their temporal and spatial coherence. This ensures the sequence of generated frames is smooth and transitions are logically consistent, contributing to a realistic video sequence.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generator</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">Our generator architecture is designed to be efficient while producing realistic results, utilizing about one-fourth of the parameters of frameworks like V2V<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite>. It contains two primary segments: an optical flow predictor and a triple-pyramid network for next frame prediction. While our generator architecture incorporates an optical flow network similar to that used in V2V, our method diverges by integrating optical flow and occlusion maps not merely for warping the previous frame and blending it with the generated image, but by embedding these elements into deeper layers of the frame synthesis process for enhanced generation. The optical flow predictor functions as an encoder-decoder network incorporating ResNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib10" title="">10</a>]</cite> blocks to process inputs (<math alttext="S_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">S</mi><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">−</mo><mn id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑆</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><minus id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1"></minus><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝑖</ci><cn id="S3.SS1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">S_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑆</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>). This network generates the predicted optical flow <math alttext="OF_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">O</mi><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">F</mi><mrow id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.3.3.2" xref="S3.SS1.p1.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.3.2.2" xref="S3.SS1.p1.3.m3.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p1.3.m3.1.1.3.3.2.1" xref="S3.SS1.p1.3.m3.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p1.3.m3.1.1.3.3.2.3" xref="S3.SS1.p1.3.m3.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.3.m3.1.1.3.3.1" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p1.3.m3.1.1.3.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑂</ci><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">𝐹</ci><apply id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3"><ci id="S3.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.1">→</ci><apply id="S3.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.2"><minus id="S3.SS1.p1.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.2.1"></minus><ci id="S3.SS1.p1.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p1.3.m3.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">OF_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_O italic_F start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and an occlusion map <math alttext="OC_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">O</mi><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">C</mi><mrow id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml"><mrow id="S3.SS1.p1.4.m4.1.1.3.3.2" xref="S3.SS1.p1.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.3.2.2" xref="S3.SS1.p1.4.m4.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p1.4.m4.1.1.3.3.2.1" xref="S3.SS1.p1.4.m4.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p1.4.m4.1.1.3.3.2.3" xref="S3.SS1.p1.4.m4.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.4.m4.1.1.3.3.1" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p1.4.m4.1.1.3.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></times><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑂</ci><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">𝐶</ci><apply id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3"><ci id="S3.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3.1">→</ci><apply id="S3.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3.2"><minus id="S3.SS1.p1.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3.2.1"></minus><ci id="S3.SS1.p1.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p1.4.m4.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">OC_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_O italic_C start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Here, <math alttext="OF_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">O</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">F</mi><mrow id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml"><mrow id="S3.SS1.p1.5.m5.1.1.3.3.2" xref="S3.SS1.p1.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.3.2.2" xref="S3.SS1.p1.5.m5.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p1.5.m5.1.1.3.3.2.1" xref="S3.SS1.p1.5.m5.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p1.5.m5.1.1.3.3.2.3" xref="S3.SS1.p1.5.m5.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.5.m5.1.1.3.3.1" stretchy="false" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></times><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑂</ci><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">𝐹</ci><apply id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3"><ci id="S3.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.1">→</ci><apply id="S3.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.2"><minus id="S3.SS1.p1.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.2.1"></minus><ci id="S3.SS1.p1.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p1.5.m5.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">OF_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_O italic_F start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math> depicts the movement of each pixel from <math alttext="I_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">I</mi><mrow id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p1.6.m6.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">−</mo><mn id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝐼</ci><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><minus id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.1"></minus><ci id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">𝑖</ci><cn id="S3.SS1.p1.6.m6.1.1.3.3.cmml" type="integer" xref="S3.SS1.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">I_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">𝐼</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, while <math alttext="OC_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">O</mi><mo id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">C</mi><mrow id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml"><mrow id="S3.SS1.p1.8.m8.1.1.3.3.2" xref="S3.SS1.p1.8.m8.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.3.2.2" xref="S3.SS1.p1.8.m8.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p1.8.m8.1.1.3.3.2.1" xref="S3.SS1.p1.8.m8.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p1.8.m8.1.1.3.3.2.3" xref="S3.SS1.p1.8.m8.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.8.m8.1.1.3.3.1" stretchy="false" xref="S3.SS1.p1.8.m8.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><times id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1"></times><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝑂</ci><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.2">𝐶</ci><apply id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3"><ci id="S3.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.1">→</ci><apply id="S3.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.2"><minus id="S3.SS1.p1.8.m8.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.2.1"></minus><ci id="S3.SS1.p1.8.m8.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p1.8.m8.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p1.8.m8.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p1.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">OC_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_O italic_C start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math> identifies areas potentially occluded in the transition and requiring regeneration.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.9">Utilizing <math alttext="OF_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">O</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">F</mi><mrow id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml"><mrow id="S3.SS1.p2.1.m1.1.1.3.3.2" xref="S3.SS1.p2.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.3.2.2" xref="S3.SS1.p2.1.m1.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p2.1.m1.1.1.3.3.2.1" xref="S3.SS1.p2.1.m1.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p2.1.m1.1.1.3.3.2.3" xref="S3.SS1.p2.1.m1.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.1.m1.1.1.3.3.1" stretchy="false" xref="S3.SS1.p2.1.m1.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑂</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">𝐹</ci><apply id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3"><ci id="S3.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.1">→</ci><apply id="S3.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.2"><minus id="S3.SS1.p2.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.2.1"></minus><ci id="S3.SS1.p2.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p2.1.m1.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">OF_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_O italic_F start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, the previous image <math alttext="I_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">I</mi><mrow id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.2.m2.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">−</mo><mn id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐼</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><minus id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.1"></minus><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">𝑖</ci><cn id="S3.SS1.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">I_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> is warped to produce the warped image <math alttext="WI_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">W</mi><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">I</mi><mrow id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.cmml">i</mi><mo id="S3.SS1.p2.3.m3.1.1.3.3.1" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">−</mo><mn id="S3.SS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></times><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑊</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">𝐼</ci><apply id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3"><minus id="S3.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.1"></minus><ci id="S3.SS1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2">𝑖</ci><cn id="S3.SS1.p2.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">WI_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_W italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> by shifting each pixel according to its corresponding motion vector in <math alttext="OF_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">O</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">F</mi><mrow id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml"><mrow id="S3.SS1.p2.4.m4.1.1.3.3.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.3.2.2" xref="S3.SS1.p2.4.m4.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3.2.1" xref="S3.SS1.p2.4.m4.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p2.4.m4.1.1.3.3.2.3" xref="S3.SS1.p2.4.m4.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.4.m4.1.1.3.3.1" stretchy="false" xref="S3.SS1.p2.4.m4.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p2.4.m4.1.1.3.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑂</ci><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝐹</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3"><ci id="S3.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.1">→</ci><apply id="S3.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2"><minus id="S3.SS1.p2.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2.1"></minus><ci id="S3.SS1.p2.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p2.4.m4.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">OF_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_O italic_F start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. This warped image feeds into the first pyramid of our network, an image encoder, designed to extract image features for constructing the current image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝐼</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in a U-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib31" title="">31</a>]</cite> configuration with skip connections (refer to Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S3.F2" title="Figure 2 ‣ 3 Method ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">2</span></a>). The modulation of these skip connections by <math alttext="OC_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">O</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2" xref="S3.SS1.p2.6.m6.1.1.3.2.cmml">C</mi><mrow id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml"><mrow id="S3.SS1.p2.6.m6.1.1.3.3.2" xref="S3.SS1.p2.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.3.2.2" xref="S3.SS1.p2.6.m6.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p2.6.m6.1.1.3.3.2.1" xref="S3.SS1.p2.6.m6.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p2.6.m6.1.1.3.3.2.3" xref="S3.SS1.p2.6.m6.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.6.m6.1.1.3.3.1" stretchy="false" xref="S3.SS1.p2.6.m6.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p2.6.m6.1.1.3.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><times id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1"></times><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝑂</ci><apply id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.2">𝐶</ci><apply id="S3.SS1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3"><ci id="S3.SS1.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.1">→</ci><apply id="S3.SS1.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.2"><minus id="S3.SS1.p2.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.2.1"></minus><ci id="S3.SS1.p2.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p2.6.m6.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p2.6.m6.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">OC_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">italic_O italic_C start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is used to determine the balance between the generated content and the information carried over from <math alttext="WI_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">W</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">I</mi><mrow id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.3.2" xref="S3.SS1.p2.7.m7.1.1.3.3.2.cmml">i</mi><mo id="S3.SS1.p2.7.m7.1.1.3.3.1" xref="S3.SS1.p2.7.m7.1.1.3.3.1.cmml">−</mo><mn id="S3.SS1.p2.7.m7.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><times id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></times><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝑊</ci><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">𝐼</ci><apply id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3"><minus id="S3.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.1"></minus><ci id="S3.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2">𝑖</ci><cn id="S3.SS1.p2.7.m7.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p2.7.m7.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">WI_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_W italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="OC_{i-1\rightarrow i}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><mrow id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">O</mi><mo id="S3.SS1.p2.8.m8.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">C</mi><mrow id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml"><mrow id="S3.SS1.p2.8.m8.1.1.3.3.2" xref="S3.SS1.p2.8.m8.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.3.2.2" xref="S3.SS1.p2.8.m8.1.1.3.3.2.2.cmml">i</mi><mo id="S3.SS1.p2.8.m8.1.1.3.3.2.1" xref="S3.SS1.p2.8.m8.1.1.3.3.2.1.cmml">−</mo><mn id="S3.SS1.p2.8.m8.1.1.3.3.2.3" xref="S3.SS1.p2.8.m8.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.8.m8.1.1.3.3.1" stretchy="false" xref="S3.SS1.p2.8.m8.1.1.3.3.1.cmml">→</mo><mi id="S3.SS1.p2.8.m8.1.1.3.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><times id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1"></times><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑂</ci><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">𝐶</ci><apply id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3"><ci id="S3.SS1.p2.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.1">→</ci><apply id="S3.SS1.p2.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.2"><minus id="S3.SS1.p2.8.m8.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.2.1"></minus><ci id="S3.SS1.p2.8.m8.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.2.2">𝑖</ci><cn id="S3.SS1.p2.8.m8.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p2.8.m8.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p2.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">OC_{i-1\rightarrow i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">italic_O italic_C start_POSTSUBSCRIPT italic_i - 1 → italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, with pixel values ranging from 0 to 1, indicates whether a pixel should feature newly generated content or retain features from the warped previous image. This dynamic scaling optimizes the integration of these inputs to construct <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><msub id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">𝐼</ci><ci id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, thereby effectively managing the blend based on pixel-specific occlusion.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The second pyramid, another encoder, accepts <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑆</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as input and produces per-level features for the SPADE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib30" title="">30</a>]</cite> blocks within the generator’s decoder. Similar to DP-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib23" title="">23</a>]</cite>, the bottleneck features, which provide the largest field of view, are concatenated to the features across all levels to maintain a global perspective. This configuration enables more accurate denormalization by integrating global context and detailed local features, thereby improving the fidelity and consistency of the generated images relative to the input semantic maps.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.5">The third pyramid, functioning as the decoder, employs multiple SPADE ResNet blocks. These blocks utilize the features extracted from <math alttext="WI_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">W</mi><mo id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.3.2.cmml">I</mi><mrow id="S3.SS1.p4.1.m1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3.3.2" xref="S3.SS1.p4.1.m1.1.1.3.3.2.cmml">i</mi><mo id="S3.SS1.p4.1.m1.1.1.3.3.1" xref="S3.SS1.p4.1.m1.1.1.3.3.1.cmml">−</mo><mn id="S3.SS1.p4.1.m1.1.1.3.3.3" xref="S3.SS1.p4.1.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑊</ci><apply id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.3.2">𝐼</ci><apply id="S3.SS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3"><minus id="S3.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3.1"></minus><ci id="S3.SS1.p4.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3.2">𝑖</ci><cn id="S3.SS1.p4.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p4.1.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">WI_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_W italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝑆</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to generate the current image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝐼</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, ensuring fidelity to the appearance of <math alttext="I_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.1"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">I</mi><mrow id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml"><mi id="S3.SS1.p4.4.m4.1.1.3.2" xref="S3.SS1.p4.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p4.4.m4.1.1.3.1" xref="S3.SS1.p4.4.m4.1.1.3.1.cmml">−</mo><mn id="S3.SS1.p4.4.m4.1.1.3.3" xref="S3.SS1.p4.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">𝐼</ci><apply id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3"><minus id="S3.SS1.p4.4.m4.1.1.3.1.cmml" xref="S3.SS1.p4.4.m4.1.1.3.1"></minus><ci id="S3.SS1.p4.4.m4.1.1.3.2.cmml" xref="S3.SS1.p4.4.m4.1.1.3.2">𝑖</ci><cn id="S3.SS1.p4.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS1.p4.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">I_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.1d">italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> and adherence to the semantic guidance from <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5.1"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">S</mi><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝑆</ci><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m5.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Discriminators</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">The primary objective of the discriminator is to ensure spatial-temporal coherence and fidelity to the semantic map. This is achieved through the use of two discriminators: <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝐷</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>. The image discriminator <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝐷</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> concentrates on validating each image’s realism and adherence to the semantic map, while the video discriminator <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">D</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝐷</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> ensures temporal coherence across a video sequence.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.9"><math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> is structured as an encoder-decoder network similar to DP-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib23" title="">23</a>]</cite>, utilizing the OASIS loss to effectively guide the generator. It accepts as input either a real image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">I</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝐼</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> or a generated image <math alttext="\hat{I}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">I</mi><mo id="S3.SS2.p2.3.m3.1.1.2.1" xref="S3.SS2.p2.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><ci id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2.1">^</ci><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝐼</ci></apply><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\hat{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and produces a predicted segmentation map <math alttext="\hat{S}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">S</mi><mo id="S3.SS2.p2.4.m4.1.1.2.1" xref="S3.SS2.p2.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><ci id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2.1">^</ci><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">𝑆</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\hat{S}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">over^ start_ARG italic_S end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. In the case of <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐼</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝐷</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> is tasked with segmenting the image into correct semantic labels. Conversely, when evaluating <math alttext="\hat{I}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><msub id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mover accent="true" id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2.2" xref="S3.SS2.p2.7.m7.1.1.2.2.cmml">I</mi><mo id="S3.SS2.p2.7.m7.1.1.2.1" xref="S3.SS2.p2.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">subscript</csymbol><apply id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"><ci id="S3.SS2.p2.7.m7.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.2.1">^</ci><ci id="S3.SS2.p2.7.m7.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2.2">𝐼</ci></apply><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\hat{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, it attempts to identify an additional class indicative of the ’fake’ class. This segmentation-based loss specifically provides detailed, pixel-level feedback, enhancing the semantic alignment of the generated images with the input map <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m8.1"><semantics id="S3.SS2.p2.8.m8.1a"><msub id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">S</mi><mi id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">𝑆</ci><ci id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. This loss-driven feedback forces the generator to refine its output to closely match <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m9.1"><semantics id="S3.SS2.p2.9.m9.1a"><msub id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">S</mi><mi id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝑆</ci><ci id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m9.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">For <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math>, we employ a network model similar to V2V<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite>, which functions as a patchwise discriminator to check temporal coherence. In our framework, we utilize two video discriminators, with each one targeting a different temporal scale to ensure coverage of longer temporal sequences. This means each discriminator analyzes frames at varying rates to assess broader temporal coherence. Each <math alttext="D_{V}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐷</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">D_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> examines three frames simultaneously to evaluate their temporal alignment, thereby enhancing the overall realism of the video.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Losses</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our model architecture employs a comprehensive suite of loss functions, each designed to optimize specific characteristics of the generated outputs. These loss functions are briefly summarized below, with detailed formulations and additional discussions available in the appendix.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.14"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.2.2">OASIS Adversarial Loss <math alttext="\mathcal{L}_{D_{I}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.1.m1.1"><semantics id="S3.I2.i1.p1.1.1.m1.1a"><msub id="S3.I2.i1.p1.1.1.m1.1.1" xref="S3.I2.i1.p1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I2.i1.p1.1.1.m1.1.1.2" xref="S3.I2.i1.p1.1.1.m1.1.1.2.cmml">ℒ</mi><msub id="S3.I2.i1.p1.1.1.m1.1.1.3" xref="S3.I2.i1.p1.1.1.m1.1.1.3.cmml"><mi id="S3.I2.i1.p1.1.1.m1.1.1.3.2" xref="S3.I2.i1.p1.1.1.m1.1.1.3.2.cmml">D</mi><mi id="S3.I2.i1.p1.1.1.m1.1.1.3.3" xref="S3.I2.i1.p1.1.1.m1.1.1.3.3.cmml">I</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.1.m1.1b"><apply id="S3.I2.i1.p1.1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.1.m1.1.1.1.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1.2">ℒ</ci><apply id="S3.I2.i1.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.1.m1.1.1.3.1.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I2.i1.p1.1.1.m1.1.1.3.2.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1.3.2">𝐷</ci><ci id="S3.I2.i1.p1.1.1.m1.1.1.3.3.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.1.m1.1c">\mathcal{L}_{D_{I}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{G_{I}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.2.m2.1"><semantics id="S3.I2.i1.p1.2.2.m2.1a"><msub id="S3.I2.i1.p1.2.2.m2.1.1" xref="S3.I2.i1.p1.2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I2.i1.p1.2.2.m2.1.1.2" xref="S3.I2.i1.p1.2.2.m2.1.1.2.cmml">ℒ</mi><msub id="S3.I2.i1.p1.2.2.m2.1.1.3" xref="S3.I2.i1.p1.2.2.m2.1.1.3.cmml"><mi id="S3.I2.i1.p1.2.2.m2.1.1.3.2" xref="S3.I2.i1.p1.2.2.m2.1.1.3.2.cmml">G</mi><mi id="S3.I2.i1.p1.2.2.m2.1.1.3.3" xref="S3.I2.i1.p1.2.2.m2.1.1.3.3.cmml">I</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.2.m2.1b"><apply id="S3.I2.i1.p1.2.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.2.2.m2.1.1.1.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.2.2.m2.1.1.2.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1.2">ℒ</ci><apply id="S3.I2.i1.p1.2.2.m2.1.1.3.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.I2.i1.p1.2.2.m2.1.1.3.1.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1.3">subscript</csymbol><ci id="S3.I2.i1.p1.2.2.m2.1.1.3.2.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1.3.2">𝐺</ci><ci id="S3.I2.i1.p1.2.2.m2.1.1.3.3.cmml" xref="S3.I2.i1.p1.2.2.m2.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.2.m2.1c">\mathcal{L}_{G_{I}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib34" title="">34</a>]</cite>:</span> <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m1.1"><semantics id="S3.I2.i1.p1.3.m1.1a"><msub id="S3.I2.i1.p1.3.m1.1.1" xref="S3.I2.i1.p1.3.m1.1.1.cmml"><mi id="S3.I2.i1.p1.3.m1.1.1.2" xref="S3.I2.i1.p1.3.m1.1.1.2.cmml">D</mi><mi id="S3.I2.i1.p1.3.m1.1.1.3" xref="S3.I2.i1.p1.3.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m1.1b"><apply id="S3.I2.i1.p1.3.m1.1.1.cmml" xref="S3.I2.i1.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.3.m1.1.1.1.cmml" xref="S3.I2.i1.p1.3.m1.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.3.m1.1.1.2.cmml" xref="S3.I2.i1.p1.3.m1.1.1.2">𝐷</ci><ci id="S3.I2.i1.p1.3.m1.1.1.3.cmml" xref="S3.I2.i1.p1.3.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m1.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m1.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> acts as a semantic segmentation network, generating a semantic map <math alttext="\hat{S}_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.4.m2.1"><semantics id="S3.I2.i1.p1.4.m2.1a"><msub id="S3.I2.i1.p1.4.m2.1.1" xref="S3.I2.i1.p1.4.m2.1.1.cmml"><mover accent="true" id="S3.I2.i1.p1.4.m2.1.1.2" xref="S3.I2.i1.p1.4.m2.1.1.2.cmml"><mi id="S3.I2.i1.p1.4.m2.1.1.2.2" xref="S3.I2.i1.p1.4.m2.1.1.2.2.cmml">S</mi><mo id="S3.I2.i1.p1.4.m2.1.1.2.1" xref="S3.I2.i1.p1.4.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.I2.i1.p1.4.m2.1.1.3" xref="S3.I2.i1.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.4.m2.1b"><apply id="S3.I2.i1.p1.4.m2.1.1.cmml" xref="S3.I2.i1.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m2.1.1.1.cmml" xref="S3.I2.i1.p1.4.m2.1.1">subscript</csymbol><apply id="S3.I2.i1.p1.4.m2.1.1.2.cmml" xref="S3.I2.i1.p1.4.m2.1.1.2"><ci id="S3.I2.i1.p1.4.m2.1.1.2.1.cmml" xref="S3.I2.i1.p1.4.m2.1.1.2.1">^</ci><ci id="S3.I2.i1.p1.4.m2.1.1.2.2.cmml" xref="S3.I2.i1.p1.4.m2.1.1.2.2">𝑆</ci></apply><ci id="S3.I2.i1.p1.4.m2.1.1.3.cmml" xref="S3.I2.i1.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.4.m2.1c">\hat{S}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.4.m2.1d">over^ start_ARG italic_S end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The discriminator’s loss, <math alttext="L_{D_{I}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.5.m3.1"><semantics id="S3.I2.i1.p1.5.m3.1a"><msub id="S3.I2.i1.p1.5.m3.1.1" xref="S3.I2.i1.p1.5.m3.1.1.cmml"><mi id="S3.I2.i1.p1.5.m3.1.1.2" xref="S3.I2.i1.p1.5.m3.1.1.2.cmml">L</mi><msub id="S3.I2.i1.p1.5.m3.1.1.3" xref="S3.I2.i1.p1.5.m3.1.1.3.cmml"><mi id="S3.I2.i1.p1.5.m3.1.1.3.2" xref="S3.I2.i1.p1.5.m3.1.1.3.2.cmml">D</mi><mi id="S3.I2.i1.p1.5.m3.1.1.3.3" xref="S3.I2.i1.p1.5.m3.1.1.3.3.cmml">I</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.5.m3.1b"><apply id="S3.I2.i1.p1.5.m3.1.1.cmml" xref="S3.I2.i1.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.5.m3.1.1.1.cmml" xref="S3.I2.i1.p1.5.m3.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.5.m3.1.1.2.cmml" xref="S3.I2.i1.p1.5.m3.1.1.2">𝐿</ci><apply id="S3.I2.i1.p1.5.m3.1.1.3.cmml" xref="S3.I2.i1.p1.5.m3.1.1.3"><csymbol cd="ambiguous" id="S3.I2.i1.p1.5.m3.1.1.3.1.cmml" xref="S3.I2.i1.p1.5.m3.1.1.3">subscript</csymbol><ci id="S3.I2.i1.p1.5.m3.1.1.3.2.cmml" xref="S3.I2.i1.p1.5.m3.1.1.3.2">𝐷</ci><ci id="S3.I2.i1.p1.5.m3.1.1.3.3.cmml" xref="S3.I2.i1.p1.5.m3.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.5.m3.1c">L_{D_{I}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.5.m3.1d">italic_L start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, uses cross-entropy to ensure precise segmentation of real images <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.6.m4.1"><semantics id="S3.I2.i1.p1.6.m4.1a"><msub id="S3.I2.i1.p1.6.m4.1.1" xref="S3.I2.i1.p1.6.m4.1.1.cmml"><mi id="S3.I2.i1.p1.6.m4.1.1.2" xref="S3.I2.i1.p1.6.m4.1.1.2.cmml">I</mi><mi id="S3.I2.i1.p1.6.m4.1.1.3" xref="S3.I2.i1.p1.6.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.6.m4.1b"><apply id="S3.I2.i1.p1.6.m4.1.1.cmml" xref="S3.I2.i1.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.6.m4.1.1.1.cmml" xref="S3.I2.i1.p1.6.m4.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.6.m4.1.1.2.cmml" xref="S3.I2.i1.p1.6.m4.1.1.2">𝐼</ci><ci id="S3.I2.i1.p1.6.m4.1.1.3.cmml" xref="S3.I2.i1.p1.6.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.6.m4.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.6.m4.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> by comparing <math alttext="\hat{S}_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.7.m5.1"><semantics id="S3.I2.i1.p1.7.m5.1a"><msub id="S3.I2.i1.p1.7.m5.1.1" xref="S3.I2.i1.p1.7.m5.1.1.cmml"><mover accent="true" id="S3.I2.i1.p1.7.m5.1.1.2" xref="S3.I2.i1.p1.7.m5.1.1.2.cmml"><mi id="S3.I2.i1.p1.7.m5.1.1.2.2" xref="S3.I2.i1.p1.7.m5.1.1.2.2.cmml">S</mi><mo id="S3.I2.i1.p1.7.m5.1.1.2.1" xref="S3.I2.i1.p1.7.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.I2.i1.p1.7.m5.1.1.3" xref="S3.I2.i1.p1.7.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.7.m5.1b"><apply id="S3.I2.i1.p1.7.m5.1.1.cmml" xref="S3.I2.i1.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.7.m5.1.1.1.cmml" xref="S3.I2.i1.p1.7.m5.1.1">subscript</csymbol><apply id="S3.I2.i1.p1.7.m5.1.1.2.cmml" xref="S3.I2.i1.p1.7.m5.1.1.2"><ci id="S3.I2.i1.p1.7.m5.1.1.2.1.cmml" xref="S3.I2.i1.p1.7.m5.1.1.2.1">^</ci><ci id="S3.I2.i1.p1.7.m5.1.1.2.2.cmml" xref="S3.I2.i1.p1.7.m5.1.1.2.2">𝑆</ci></apply><ci id="S3.I2.i1.p1.7.m5.1.1.3.cmml" xref="S3.I2.i1.p1.7.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.7.m5.1c">\hat{S}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.7.m5.1d">over^ start_ARG italic_S end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.8.m6.1"><semantics id="S3.I2.i1.p1.8.m6.1a"><msub id="S3.I2.i1.p1.8.m6.1.1" xref="S3.I2.i1.p1.8.m6.1.1.cmml"><mi id="S3.I2.i1.p1.8.m6.1.1.2" xref="S3.I2.i1.p1.8.m6.1.1.2.cmml">S</mi><mi id="S3.I2.i1.p1.8.m6.1.1.3" xref="S3.I2.i1.p1.8.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.8.m6.1b"><apply id="S3.I2.i1.p1.8.m6.1.1.cmml" xref="S3.I2.i1.p1.8.m6.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.8.m6.1.1.1.cmml" xref="S3.I2.i1.p1.8.m6.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.8.m6.1.1.2.cmml" xref="S3.I2.i1.p1.8.m6.1.1.2">𝑆</ci><ci id="S3.I2.i1.p1.8.m6.1.1.3.cmml" xref="S3.I2.i1.p1.8.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.8.m6.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.8.m6.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. For generated images <math alttext="\hat{I}_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.9.m7.1"><semantics id="S3.I2.i1.p1.9.m7.1a"><msub id="S3.I2.i1.p1.9.m7.1.1" xref="S3.I2.i1.p1.9.m7.1.1.cmml"><mover accent="true" id="S3.I2.i1.p1.9.m7.1.1.2" xref="S3.I2.i1.p1.9.m7.1.1.2.cmml"><mi id="S3.I2.i1.p1.9.m7.1.1.2.2" xref="S3.I2.i1.p1.9.m7.1.1.2.2.cmml">I</mi><mo id="S3.I2.i1.p1.9.m7.1.1.2.1" xref="S3.I2.i1.p1.9.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.I2.i1.p1.9.m7.1.1.3" xref="S3.I2.i1.p1.9.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.9.m7.1b"><apply id="S3.I2.i1.p1.9.m7.1.1.cmml" xref="S3.I2.i1.p1.9.m7.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.9.m7.1.1.1.cmml" xref="S3.I2.i1.p1.9.m7.1.1">subscript</csymbol><apply id="S3.I2.i1.p1.9.m7.1.1.2.cmml" xref="S3.I2.i1.p1.9.m7.1.1.2"><ci id="S3.I2.i1.p1.9.m7.1.1.2.1.cmml" xref="S3.I2.i1.p1.9.m7.1.1.2.1">^</ci><ci id="S3.I2.i1.p1.9.m7.1.1.2.2.cmml" xref="S3.I2.i1.p1.9.m7.1.1.2.2">𝐼</ci></apply><ci id="S3.I2.i1.p1.9.m7.1.1.3.cmml" xref="S3.I2.i1.p1.9.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.9.m7.1c">\hat{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.9.m7.1d">over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.10.m8.1"><semantics id="S3.I2.i1.p1.10.m8.1a"><msub id="S3.I2.i1.p1.10.m8.1.1" xref="S3.I2.i1.p1.10.m8.1.1.cmml"><mi id="S3.I2.i1.p1.10.m8.1.1.2" xref="S3.I2.i1.p1.10.m8.1.1.2.cmml">D</mi><mi id="S3.I2.i1.p1.10.m8.1.1.3" xref="S3.I2.i1.p1.10.m8.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.10.m8.1b"><apply id="S3.I2.i1.p1.10.m8.1.1.cmml" xref="S3.I2.i1.p1.10.m8.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.10.m8.1.1.1.cmml" xref="S3.I2.i1.p1.10.m8.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.10.m8.1.1.2.cmml" xref="S3.I2.i1.p1.10.m8.1.1.2">𝐷</ci><ci id="S3.I2.i1.p1.10.m8.1.1.3.cmml" xref="S3.I2.i1.p1.10.m8.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.10.m8.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.10.m8.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> aims to assign each pixel to an additional fake label class. Meanwhile, the generator’s loss, <math alttext="L_{G_{I}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.11.m9.1"><semantics id="S3.I2.i1.p1.11.m9.1a"><msub id="S3.I2.i1.p1.11.m9.1.1" xref="S3.I2.i1.p1.11.m9.1.1.cmml"><mi id="S3.I2.i1.p1.11.m9.1.1.2" xref="S3.I2.i1.p1.11.m9.1.1.2.cmml">L</mi><msub id="S3.I2.i1.p1.11.m9.1.1.3" xref="S3.I2.i1.p1.11.m9.1.1.3.cmml"><mi id="S3.I2.i1.p1.11.m9.1.1.3.2" xref="S3.I2.i1.p1.11.m9.1.1.3.2.cmml">G</mi><mi id="S3.I2.i1.p1.11.m9.1.1.3.3" xref="S3.I2.i1.p1.11.m9.1.1.3.3.cmml">I</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.11.m9.1b"><apply id="S3.I2.i1.p1.11.m9.1.1.cmml" xref="S3.I2.i1.p1.11.m9.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.11.m9.1.1.1.cmml" xref="S3.I2.i1.p1.11.m9.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.11.m9.1.1.2.cmml" xref="S3.I2.i1.p1.11.m9.1.1.2">𝐿</ci><apply id="S3.I2.i1.p1.11.m9.1.1.3.cmml" xref="S3.I2.i1.p1.11.m9.1.1.3"><csymbol cd="ambiguous" id="S3.I2.i1.p1.11.m9.1.1.3.1.cmml" xref="S3.I2.i1.p1.11.m9.1.1.3">subscript</csymbol><ci id="S3.I2.i1.p1.11.m9.1.1.3.2.cmml" xref="S3.I2.i1.p1.11.m9.1.1.3.2">𝐺</ci><ci id="S3.I2.i1.p1.11.m9.1.1.3.3.cmml" xref="S3.I2.i1.p1.11.m9.1.1.3.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.11.m9.1c">L_{G_{I}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.11.m9.1d">italic_L start_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, is designed to deceive <math alttext="D_{I}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.12.m10.1"><semantics id="S3.I2.i1.p1.12.m10.1a"><msub id="S3.I2.i1.p1.12.m10.1.1" xref="S3.I2.i1.p1.12.m10.1.1.cmml"><mi id="S3.I2.i1.p1.12.m10.1.1.2" xref="S3.I2.i1.p1.12.m10.1.1.2.cmml">D</mi><mi id="S3.I2.i1.p1.12.m10.1.1.3" xref="S3.I2.i1.p1.12.m10.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.12.m10.1b"><apply id="S3.I2.i1.p1.12.m10.1.1.cmml" xref="S3.I2.i1.p1.12.m10.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.12.m10.1.1.1.cmml" xref="S3.I2.i1.p1.12.m10.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.12.m10.1.1.2.cmml" xref="S3.I2.i1.p1.12.m10.1.1.2">𝐷</ci><ci id="S3.I2.i1.p1.12.m10.1.1.3.cmml" xref="S3.I2.i1.p1.12.m10.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.12.m10.1c">D_{I}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.12.m10.1d">italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math> by producing images <math alttext="\hat{I}_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.13.m11.1"><semantics id="S3.I2.i1.p1.13.m11.1a"><msub id="S3.I2.i1.p1.13.m11.1.1" xref="S3.I2.i1.p1.13.m11.1.1.cmml"><mover accent="true" id="S3.I2.i1.p1.13.m11.1.1.2" xref="S3.I2.i1.p1.13.m11.1.1.2.cmml"><mi id="S3.I2.i1.p1.13.m11.1.1.2.2" xref="S3.I2.i1.p1.13.m11.1.1.2.2.cmml">I</mi><mo id="S3.I2.i1.p1.13.m11.1.1.2.1" xref="S3.I2.i1.p1.13.m11.1.1.2.1.cmml">^</mo></mover><mi id="S3.I2.i1.p1.13.m11.1.1.3" xref="S3.I2.i1.p1.13.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.13.m11.1b"><apply id="S3.I2.i1.p1.13.m11.1.1.cmml" xref="S3.I2.i1.p1.13.m11.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.13.m11.1.1.1.cmml" xref="S3.I2.i1.p1.13.m11.1.1">subscript</csymbol><apply id="S3.I2.i1.p1.13.m11.1.1.2.cmml" xref="S3.I2.i1.p1.13.m11.1.1.2"><ci id="S3.I2.i1.p1.13.m11.1.1.2.1.cmml" xref="S3.I2.i1.p1.13.m11.1.1.2.1">^</ci><ci id="S3.I2.i1.p1.13.m11.1.1.2.2.cmml" xref="S3.I2.i1.p1.13.m11.1.1.2.2">𝐼</ci></apply><ci id="S3.I2.i1.p1.13.m11.1.1.3.cmml" xref="S3.I2.i1.p1.13.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.13.m11.1c">\hat{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.13.m11.1d">over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> that are misclassified as real and accurately mapped to their corresponding semantic classes <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.14.m12.1"><semantics id="S3.I2.i1.p1.14.m12.1a"><msub id="S3.I2.i1.p1.14.m12.1.1" xref="S3.I2.i1.p1.14.m12.1.1.cmml"><mi id="S3.I2.i1.p1.14.m12.1.1.2" xref="S3.I2.i1.p1.14.m12.1.1.2.cmml">S</mi><mi id="S3.I2.i1.p1.14.m12.1.1.3" xref="S3.I2.i1.p1.14.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.14.m12.1b"><apply id="S3.I2.i1.p1.14.m12.1.1.cmml" xref="S3.I2.i1.p1.14.m12.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.14.m12.1.1.1.cmml" xref="S3.I2.i1.p1.14.m12.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.14.m12.1.1.2.cmml" xref="S3.I2.i1.p1.14.m12.1.1.2">𝑆</ci><ci id="S3.I2.i1.p1.14.m12.1.1.3.cmml" xref="S3.I2.i1.p1.14.m12.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.14.m12.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.14.m12.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, using also cross-entropy, therefore enhancing the realism and semantic accuracy of the generated images.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Adversarial Loss <math alttext="\mathcal{L}_{adv}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.1.m1.1"><semantics id="S3.I2.i2.p1.1.1.m1.1a"><msub id="S3.I2.i2.p1.1.1.m1.1.1" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I2.i2.p1.1.1.m1.1.1.2" xref="S3.I2.i2.p1.1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I2.i2.p1.1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.1.m1.1.1.3.cmml"><mi id="S3.I2.i2.p1.1.1.m1.1.1.3.2" xref="S3.I2.i2.p1.1.1.m1.1.1.3.2.cmml">a</mi><mo id="S3.I2.i2.p1.1.1.m1.1.1.3.1" xref="S3.I2.i2.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i2.p1.1.1.m1.1.1.3.3" xref="S3.I2.i2.p1.1.1.m1.1.1.3.3.cmml">d</mi><mo id="S3.I2.i2.p1.1.1.m1.1.1.3.1a" xref="S3.I2.i2.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i2.p1.1.1.m1.1.1.3.4" xref="S3.I2.i2.p1.1.1.m1.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.1.m1.1b"><apply id="S3.I2.i2.p1.1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.2">ℒ</ci><apply id="S3.I2.i2.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.3"><times id="S3.I2.i2.p1.1.1.m1.1.1.3.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.3.1"></times><ci id="S3.I2.i2.p1.1.1.m1.1.1.3.2.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.3.2">𝑎</ci><ci id="S3.I2.i2.p1.1.1.m1.1.1.3.3.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.3.3">𝑑</ci><ci id="S3.I2.i2.p1.1.1.m1.1.1.3.4.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.1.m1.1c">\mathcal{L}_{adv}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib29" title="">29</a>]</cite>:</span> Applied to video sequences, this loss utilizes a discriminator to distinguish between real and synthetic videos, enforcing that generated videos are indistinguishable from real videos, thus promoting temporal stability and coherence.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.3"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">VGG Loss <math alttext="\mathcal{L}_{VGG}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.1.1.m1.1"><semantics id="S3.I2.i3.p1.1.1.m1.1a"><msub id="S3.I2.i3.p1.1.1.m1.1.1" xref="S3.I2.i3.p1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I2.i3.p1.1.1.m1.1.1.2" xref="S3.I2.i3.p1.1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I2.i3.p1.1.1.m1.1.1.3" xref="S3.I2.i3.p1.1.1.m1.1.1.3.cmml"><mi id="S3.I2.i3.p1.1.1.m1.1.1.3.2" xref="S3.I2.i3.p1.1.1.m1.1.1.3.2.cmml">V</mi><mo id="S3.I2.i3.p1.1.1.m1.1.1.3.1" xref="S3.I2.i3.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i3.p1.1.1.m1.1.1.3.3" xref="S3.I2.i3.p1.1.1.m1.1.1.3.3.cmml">G</mi><mo id="S3.I2.i3.p1.1.1.m1.1.1.3.1a" xref="S3.I2.i3.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i3.p1.1.1.m1.1.1.3.4" xref="S3.I2.i3.p1.1.1.m1.1.1.3.4.cmml">G</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.1.m1.1b"><apply id="S3.I2.i3.p1.1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.1.1.m1.1.1.1.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i3.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.2">ℒ</ci><apply id="S3.I2.i3.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.3"><times id="S3.I2.i3.p1.1.1.m1.1.1.3.1.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.3.1"></times><ci id="S3.I2.i3.p1.1.1.m1.1.1.3.2.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.3.2">𝑉</ci><ci id="S3.I2.i3.p1.1.1.m1.1.1.3.3.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.3.3">𝐺</ci><ci id="S3.I2.i3.p1.1.1.m1.1.1.3.4.cmml" xref="S3.I2.i3.p1.1.1.m1.1.1.3.4">𝐺</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.1.m1.1c">\mathcal{L}_{VGG}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_V italic_G italic_G end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib19" title="">19</a>]</cite>:</span> Measures the perceptual difference between the feature maps of generated <math alttext="\hat{I}_{i}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.2.m1.1"><semantics id="S3.I2.i3.p1.2.m1.1a"><msub id="S3.I2.i3.p1.2.m1.1.1" xref="S3.I2.i3.p1.2.m1.1.1.cmml"><mover accent="true" id="S3.I2.i3.p1.2.m1.1.1.2" xref="S3.I2.i3.p1.2.m1.1.1.2.cmml"><mi id="S3.I2.i3.p1.2.m1.1.1.2.2" xref="S3.I2.i3.p1.2.m1.1.1.2.2.cmml">I</mi><mo id="S3.I2.i3.p1.2.m1.1.1.2.1" xref="S3.I2.i3.p1.2.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.I2.i3.p1.2.m1.1.1.3" xref="S3.I2.i3.p1.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.2.m1.1b"><apply id="S3.I2.i3.p1.2.m1.1.1.cmml" xref="S3.I2.i3.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.2.m1.1.1.1.cmml" xref="S3.I2.i3.p1.2.m1.1.1">subscript</csymbol><apply id="S3.I2.i3.p1.2.m1.1.1.2.cmml" xref="S3.I2.i3.p1.2.m1.1.1.2"><ci id="S3.I2.i3.p1.2.m1.1.1.2.1.cmml" xref="S3.I2.i3.p1.2.m1.1.1.2.1">^</ci><ci id="S3.I2.i3.p1.2.m1.1.1.2.2.cmml" xref="S3.I2.i3.p1.2.m1.1.1.2.2">𝐼</ci></apply><ci id="S3.I2.i3.p1.2.m1.1.1.3.cmml" xref="S3.I2.i3.p1.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.2.m1.1c">\hat{I}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.2.m1.1d">over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and real images <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.3.m2.1"><semantics id="S3.I2.i3.p1.3.m2.1a"><msub id="S3.I2.i3.p1.3.m2.1.1" xref="S3.I2.i3.p1.3.m2.1.1.cmml"><mi id="S3.I2.i3.p1.3.m2.1.1.2" xref="S3.I2.i3.p1.3.m2.1.1.2.cmml">I</mi><mi id="S3.I2.i3.p1.3.m2.1.1.3" xref="S3.I2.i3.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.3.m2.1b"><apply id="S3.I2.i3.p1.3.m2.1.1.cmml" xref="S3.I2.i3.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.3.m2.1.1.1.cmml" xref="S3.I2.i3.p1.3.m2.1.1">subscript</csymbol><ci id="S3.I2.i3.p1.3.m2.1.1.2.cmml" xref="S3.I2.i3.p1.3.m2.1.1.2">𝐼</ci><ci id="S3.I2.i3.p1.3.m2.1.1.3.cmml" xref="S3.I2.i3.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.3.m2.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.3.m2.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> obtained from various layers of a pre-trained VGG network<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib32" title="">32</a>]</cite>, aiming to reduce these discrepancies to enhance visual similarity.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Feature Matching Loss <math alttext="\mathcal{L}_{FM}" class="ltx_Math" display="inline" id="S3.I2.i4.p1.1.1.m1.1"><semantics id="S3.I2.i4.p1.1.1.m1.1a"><msub id="S3.I2.i4.p1.1.1.m1.1.1" xref="S3.I2.i4.p1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I2.i4.p1.1.1.m1.1.1.2" xref="S3.I2.i4.p1.1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I2.i4.p1.1.1.m1.1.1.3" xref="S3.I2.i4.p1.1.1.m1.1.1.3.cmml"><mi id="S3.I2.i4.p1.1.1.m1.1.1.3.2" xref="S3.I2.i4.p1.1.1.m1.1.1.3.2.cmml">F</mi><mo id="S3.I2.i4.p1.1.1.m1.1.1.3.1" xref="S3.I2.i4.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i4.p1.1.1.m1.1.1.3.3" xref="S3.I2.i4.p1.1.1.m1.1.1.3.3.cmml">M</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i4.p1.1.1.m1.1b"><apply id="S3.I2.i4.p1.1.1.m1.1.1.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i4.p1.1.1.m1.1.1.1.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i4.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1.2">ℒ</ci><apply id="S3.I2.i4.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1.3"><times id="S3.I2.i4.p1.1.1.m1.1.1.3.1.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1.3.1"></times><ci id="S3.I2.i4.p1.1.1.m1.1.1.3.2.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1.3.2">𝐹</ci><ci id="S3.I2.i4.p1.1.1.m1.1.1.3.3.cmml" xref="S3.I2.i4.p1.1.1.m1.1.1.3.3">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i4.p1.1.1.m1.1c">\mathcal{L}_{FM}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i4.p1.1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_F italic_M end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib22" title="">22</a>]</cite></span>: Aligns intermediate representations of real and generated images by minimizing the L1-norm difference between feature maps extracted from multiple discriminator layers.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.4"><span class="ltx_text ltx_font_bold" id="S3.I2.i5.p1.1.1">Flow and Warping Loss <math alttext="L_{Flow}" class="ltx_Math" display="inline" id="S3.I2.i5.p1.1.1.m1.1"><semantics id="S3.I2.i5.p1.1.1.m1.1a"><msub id="S3.I2.i5.p1.1.1.m1.1.1" xref="S3.I2.i5.p1.1.1.m1.1.1.cmml"><mi id="S3.I2.i5.p1.1.1.m1.1.1.2" xref="S3.I2.i5.p1.1.1.m1.1.1.2.cmml">L</mi><mrow id="S3.I2.i5.p1.1.1.m1.1.1.3" xref="S3.I2.i5.p1.1.1.m1.1.1.3.cmml"><mi id="S3.I2.i5.p1.1.1.m1.1.1.3.2" xref="S3.I2.i5.p1.1.1.m1.1.1.3.2.cmml">F</mi><mo id="S3.I2.i5.p1.1.1.m1.1.1.3.1" xref="S3.I2.i5.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.1.1.m1.1.1.3.3" xref="S3.I2.i5.p1.1.1.m1.1.1.3.3.cmml">l</mi><mo id="S3.I2.i5.p1.1.1.m1.1.1.3.1a" xref="S3.I2.i5.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.1.1.m1.1.1.3.4" xref="S3.I2.i5.p1.1.1.m1.1.1.3.4.cmml">o</mi><mo id="S3.I2.i5.p1.1.1.m1.1.1.3.1b" xref="S3.I2.i5.p1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.1.1.m1.1.1.3.5" xref="S3.I2.i5.p1.1.1.m1.1.1.3.5.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.1.1.m1.1b"><apply id="S3.I2.i5.p1.1.1.m1.1.1.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i5.p1.1.1.m1.1.1.1.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i5.p1.1.1.m1.1.1.2.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.2">𝐿</ci><apply id="S3.I2.i5.p1.1.1.m1.1.1.3.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3"><times id="S3.I2.i5.p1.1.1.m1.1.1.3.1.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3.1"></times><ci id="S3.I2.i5.p1.1.1.m1.1.1.3.2.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3.2">𝐹</ci><ci id="S3.I2.i5.p1.1.1.m1.1.1.3.3.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3.3">𝑙</ci><ci id="S3.I2.i5.p1.1.1.m1.1.1.3.4.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3.4">𝑜</ci><ci id="S3.I2.i5.p1.1.1.m1.1.1.3.5.cmml" xref="S3.I2.i5.p1.1.1.m1.1.1.3.5">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.1.1.m1.1c">L_{Flow}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i5.p1.1.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_F italic_l italic_o italic_w end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib29" title="">29</a>]</cite>:</span> This loss function evaluates the accuracy of predicted optical flows by comparing them with pseudo ground truth flows derived from FlowNet2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib16" title="">16</a>]</cite>. Additionally, it incorporates a warping loss that assesses the consistency of <math alttext="WI_{i-1}" class="ltx_Math" display="inline" id="S3.I2.i5.p1.2.m1.1"><semantics id="S3.I2.i5.p1.2.m1.1a"><mrow id="S3.I2.i5.p1.2.m1.1.1" xref="S3.I2.i5.p1.2.m1.1.1.cmml"><mi id="S3.I2.i5.p1.2.m1.1.1.2" xref="S3.I2.i5.p1.2.m1.1.1.2.cmml">W</mi><mo id="S3.I2.i5.p1.2.m1.1.1.1" xref="S3.I2.i5.p1.2.m1.1.1.1.cmml">⁢</mo><msub id="S3.I2.i5.p1.2.m1.1.1.3" xref="S3.I2.i5.p1.2.m1.1.1.3.cmml"><mi id="S3.I2.i5.p1.2.m1.1.1.3.2" xref="S3.I2.i5.p1.2.m1.1.1.3.2.cmml">I</mi><mrow id="S3.I2.i5.p1.2.m1.1.1.3.3" xref="S3.I2.i5.p1.2.m1.1.1.3.3.cmml"><mi id="S3.I2.i5.p1.2.m1.1.1.3.3.2" xref="S3.I2.i5.p1.2.m1.1.1.3.3.2.cmml">i</mi><mo id="S3.I2.i5.p1.2.m1.1.1.3.3.1" xref="S3.I2.i5.p1.2.m1.1.1.3.3.1.cmml">−</mo><mn id="S3.I2.i5.p1.2.m1.1.1.3.3.3" xref="S3.I2.i5.p1.2.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.2.m1.1b"><apply id="S3.I2.i5.p1.2.m1.1.1.cmml" xref="S3.I2.i5.p1.2.m1.1.1"><times id="S3.I2.i5.p1.2.m1.1.1.1.cmml" xref="S3.I2.i5.p1.2.m1.1.1.1"></times><ci id="S3.I2.i5.p1.2.m1.1.1.2.cmml" xref="S3.I2.i5.p1.2.m1.1.1.2">𝑊</ci><apply id="S3.I2.i5.p1.2.m1.1.1.3.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I2.i5.p1.2.m1.1.1.3.1.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3">subscript</csymbol><ci id="S3.I2.i5.p1.2.m1.1.1.3.2.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3.2">𝐼</ci><apply id="S3.I2.i5.p1.2.m1.1.1.3.3.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3.3"><minus id="S3.I2.i5.p1.2.m1.1.1.3.3.1.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3.3.1"></minus><ci id="S3.I2.i5.p1.2.m1.1.1.3.3.2.cmml" xref="S3.I2.i5.p1.2.m1.1.1.3.3.2">𝑖</ci><cn id="S3.I2.i5.p1.2.m1.1.1.3.3.3.cmml" type="integer" xref="S3.I2.i5.p1.2.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.2.m1.1c">WI_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i5.p1.2.m1.1d">italic_W italic_I start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> with their subsequent frames <math alttext="I_{i}" class="ltx_Math" display="inline" id="S3.I2.i5.p1.3.m2.1"><semantics id="S3.I2.i5.p1.3.m2.1a"><msub id="S3.I2.i5.p1.3.m2.1.1" xref="S3.I2.i5.p1.3.m2.1.1.cmml"><mi id="S3.I2.i5.p1.3.m2.1.1.2" xref="S3.I2.i5.p1.3.m2.1.1.2.cmml">I</mi><mi id="S3.I2.i5.p1.3.m2.1.1.3" xref="S3.I2.i5.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.3.m2.1b"><apply id="S3.I2.i5.p1.3.m2.1.1.cmml" xref="S3.I2.i5.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i5.p1.3.m2.1.1.1.cmml" xref="S3.I2.i5.p1.3.m2.1.1">subscript</csymbol><ci id="S3.I2.i5.p1.3.m2.1.1.2.cmml" xref="S3.I2.i5.p1.3.m2.1.1.2">𝐼</ci><ci id="S3.I2.i5.p1.3.m2.1.1.3.cmml" xref="S3.I2.i5.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.3.m2.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i5.p1.3.m2.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="L_{Flow}" class="ltx_Math" display="inline" id="S3.I2.i5.p1.4.m3.1"><semantics id="S3.I2.i5.p1.4.m3.1a"><msub id="S3.I2.i5.p1.4.m3.1.1" xref="S3.I2.i5.p1.4.m3.1.1.cmml"><mi id="S3.I2.i5.p1.4.m3.1.1.2" xref="S3.I2.i5.p1.4.m3.1.1.2.cmml">L</mi><mrow id="S3.I2.i5.p1.4.m3.1.1.3" xref="S3.I2.i5.p1.4.m3.1.1.3.cmml"><mi id="S3.I2.i5.p1.4.m3.1.1.3.2" xref="S3.I2.i5.p1.4.m3.1.1.3.2.cmml">F</mi><mo id="S3.I2.i5.p1.4.m3.1.1.3.1" xref="S3.I2.i5.p1.4.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.4.m3.1.1.3.3" xref="S3.I2.i5.p1.4.m3.1.1.3.3.cmml">l</mi><mo id="S3.I2.i5.p1.4.m3.1.1.3.1a" xref="S3.I2.i5.p1.4.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.4.m3.1.1.3.4" xref="S3.I2.i5.p1.4.m3.1.1.3.4.cmml">o</mi><mo id="S3.I2.i5.p1.4.m3.1.1.3.1b" xref="S3.I2.i5.p1.4.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.I2.i5.p1.4.m3.1.1.3.5" xref="S3.I2.i5.p1.4.m3.1.1.3.5.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.4.m3.1b"><apply id="S3.I2.i5.p1.4.m3.1.1.cmml" xref="S3.I2.i5.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i5.p1.4.m3.1.1.1.cmml" xref="S3.I2.i5.p1.4.m3.1.1">subscript</csymbol><ci id="S3.I2.i5.p1.4.m3.1.1.2.cmml" xref="S3.I2.i5.p1.4.m3.1.1.2">𝐿</ci><apply id="S3.I2.i5.p1.4.m3.1.1.3.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3"><times id="S3.I2.i5.p1.4.m3.1.1.3.1.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3.1"></times><ci id="S3.I2.i5.p1.4.m3.1.1.3.2.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3.2">𝐹</ci><ci id="S3.I2.i5.p1.4.m3.1.1.3.3.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3.3">𝑙</ci><ci id="S3.I2.i5.p1.4.m3.1.1.3.4.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3.4">𝑜</ci><ci id="S3.I2.i5.p1.4.m3.1.1.3.5.cmml" xref="S3.I2.i5.p1.4.m3.1.1.3.5">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.4.m3.1c">L_{Flow}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i5.p1.4.m3.1d">italic_L start_POSTSUBSCRIPT italic_F italic_l italic_o italic_w end_POSTSUBSCRIPT</annotation></semantics></math> aims to minimize motion discrepancies between consecutive frames, enhancing the realism and temporal consistency of the video output.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The following describes the implementation details, introduces the datasets and metrics used throughout the experiments, and finally presents the results.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">We trained our model using a single NVIDIA A6000 GPU and employed a progressive learning approach that includes both increasing the sequence length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib41" title="">41</a>]</cite> and the spatial resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib20" title="">20</a>]</cite>. The base model is initially trained over 20 epochs at a constant resolution, with the sequence length starting at 6 frames and progressively increasing to 12, 24, and finally 30 frames, with increments every 5 epochs. After completing these 20 epochs, training continues for another 20 epochs with a decaying learning rate. Following this initial 40-epoch training period, an additional SPADE block is integrated into the generator’s decoder to facilitate spatial progression. This enhancement enables the model to handle higher resolutions. Subsequently, the model undergoes further training for an additional 8 epochs with the sequence length progressively increasing. Throughout this process, the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib21" title="">21</a>]</cite> is utilized with parameters <math alttext="\beta_{1}=0.5" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><msub id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2">𝛽</ci><cn id="S4.SS1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.2.3">1</cn></apply><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\beta_{1}=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.5</annotation></semantics></math> and <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><msub id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2.2" xref="S4.SS1.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S4.SS1.p1.2.m2.1.1.2.3" xref="S4.SS1.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><eq id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></eq><apply id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2.2">𝛽</ci><cn id="S4.SS1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S4.SS1.p1.2.m2.1.1.2.3">2</cn></apply><cn id="S4.SS1.p1.2.m2.1.1.3.cmml" type="float" xref="S4.SS1.p1.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We trained our model on two driving datasets: Cityscapes Sequence and KITTI-360. These datasets were chosen for their challenging scenarios, which include the movement of the ego vehicle (the camera) as well as the dynamic movement of other objects such as cars and pedestrians. Although our model was specifically trained on these driving datasets, it is designed to generalize to non-driving scenarios as well.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Cityscapes Sequence<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib2" title="">2</a>]</cite>:</span> The Cityscapes Sequence dataset extends the well-known Cityscapes dataset, specifically tailored for video-based urban scene analysis. It encompasses 150,000 high-resolution frames (2048 x 1024 pixels), drawn from video sequences captured across 30 cities at 17 frames per second. Each sequence consists of 30 frames. Notably, ground truth semantic labels are provided only for a single frame in each sequence. To address this limitation, we employed OneFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib18" title="">18</a>]</cite>, which was trained on the Cityscapes dataset, to produce segmentation and instance maps for all images. Although these segmentations are not fully accurate, this pseudo ground truth is sufficient to enable our model to generate realistic videos.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.2">Our model is trained using the 2,975 training sequences and is assessed on the 500 validation sequences. We start by training our model on resolution <math alttext="512\times 256" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS2.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><cn id="S4.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.2">512</cn><cn id="S4.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">512\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">512 × 256</annotation></semantics></math> as our base and then add an extra SPADE block to reach resolution <math alttext="1024\times 512" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">1024</mn><mo id="S4.SS2.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><cn id="S4.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1.2">1024</cn><cn id="S4.SS2.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">1024\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">1024 × 512</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">KITTI-360 Dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib24" title="">24</a>]</cite>:</span>
The KITTI-360 dataset enhances the original KITTI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib7" title="">7</a>]</cite> by providing detailed 3D urban scene perception for autonomous driving research. This dataset comprises over 80,000 images (1400 x 425 pixels) alongside more than 80,000 dense 3D laser scans, covering approximately 75 km of urban and rural roads.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Annotations include 2D and 3D bounding boxes for object detection and semantic labels for both images and 3D point clouds. Data capture involves two color video cameras and two laser scanners.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.2">The sequences in this dataset vary in length, with a minimum sequence length of 136 frames and a maximum of 8294 frames taken at 11 frames per second. This gives us the ability to test our model on long sequences while only trained on sequences of length 30. We start with a resolution <math alttext="512\times 128" class="ltx_Math" display="inline" id="S4.SS2.p6.1.m1.1"><semantics id="S4.SS2.p6.1.m1.1a"><mrow id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mn id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS2.p6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p6.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p6.1.m1.1.1.3" xref="S4.SS2.p6.1.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><times id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1"></times><cn id="S4.SS2.p6.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.p6.1.m1.1.1.2">512</cn><cn id="S4.SS2.p6.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p6.1.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">512\times 128</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.1.m1.1d">512 × 128</annotation></semantics></math> and reach <math alttext="1024\times 256" class="ltx_Math" display="inline" id="S4.SS2.p6.2.m2.1"><semantics id="S4.SS2.p6.2.m2.1a"><mrow id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mn id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">1024</mn><mo id="S4.SS2.p6.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p6.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p6.2.m2.1.1.3" xref="S4.SS2.p6.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><times id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1.1"></times><cn id="S4.SS2.p6.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.p6.2.m2.1.1.2">1024</cn><cn id="S4.SS2.p6.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p6.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">1024\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p6.2.m2.1d">1024 × 256</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We evaluate our SVS-GAN using several key metrics to assess both visual quality and semantic accuracy. Fréchet Inception Distance (FID)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib11" title="">11</a>]</cite>, calculated using feature vectors from the Inception-v3 network<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib35" title="">35</a>]</cite> trained on ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib3" title="">3</a>]</cite>, quantifies how closely generated images match real images in content and style. Fréchet Video Distance (FVD)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib38" title="">38</a>]</cite> is used to measure how closely generated videos mimic real ones. Traditionally, FVD employs FVD<sub class="ltx_sub" id="S4.SS3.p1.1.1">i3d</sub> features from the I3D model, which focuses on frame appearance and includes some temporal elements. However, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib6" title="">6</a>]</cite> demonstrates that FVD<sub class="ltx_sub" id="S4.SS3.p1.1.2">cd</sub>, using features from the VideoMAE-v2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib40" title="">40</a>]</cite> network, provides a more precise assessment of temporal coherence, offering a more accurate evaluation of video realism. Mean Intersection over Union (MIoU) evaluates the semantic accuracy, using predictions from the DRN-D-105 network<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib42" title="">42</a>]</cite> to measure the overlap of semantic labels with ground truths. Lower FID and FVD scores, along with higher MIoU values, indicate improved performance of the generated videos. For a comprehensive exploration of these metrics, see the supplementary material.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S4.F3.g1" src="x3.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">
Comparison of model performances on the Cityscapes sequence dataset: On the two displayed scenes our model, compared to V2V and WC-V2V models, demonstrates enhanced detail capture, particularly in the road surfaces and vehicles.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.2.1.1.1"><span class="ltx_text" id="S4.T1.2.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.2"><span class="ltx_text" id="S4.T1.2.1.1.2.1" style="font-size:90%;">FID ↓</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.3">
<span class="ltx_text" id="S4.T1.2.1.1.3.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T1.2.1.1.3.2"><span class="ltx_text" id="S4.T1.2.1.1.3.2.1" style="font-size:90%;">i3d</span></sub><span class="ltx_text" id="S4.T1.2.1.1.3.3" style="font-size:90%;"> ↓</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.4">
<span class="ltx_text" id="S4.T1.2.1.1.4.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T1.2.1.1.4.2"><span class="ltx_text" id="S4.T1.2.1.1.4.2.1" style="font-size:90%;">cd</span></sub><span class="ltx_text" id="S4.T1.2.1.1.4.3" style="font-size:90%;"> ↓</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.5"><span class="ltx_text" id="S4.T1.2.1.1.5.1" style="font-size:90%;">MIoU ↑</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.6"><span class="ltx_text" id="S4.T1.2.1.1.6.1" style="font-size:90%;">FPS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.2.1.1"><span class="ltx_text" id="S4.T1.2.2.1.1.1" style="font-size:90%;">V2V</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.2"><span class="ltx_text" id="S4.T1.2.2.1.2.1" style="font-size:90%;">69.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.3"><span class="ltx_text" id="S4.T1.2.2.1.3.1" style="font-size:90%;">126.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.4"><span class="ltx_text" id="S4.T1.2.2.1.4.1" style="font-size:90%;">97.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.5"><span class="ltx_text" id="S4.T1.2.2.1.5.1" style="font-size:90%;">55.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.2.2.1.6"><span class="ltx_text" id="S4.T1.2.2.1.6.1" style="font-size:90%;">4.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.3.2.1"><span class="ltx_text" id="S4.T1.2.3.2.1.1" style="font-size:90%;">WC-V2V</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.2"><span class="ltx_text" id="S4.T1.2.3.2.2.1" style="font-size:90%;">49.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.3"><span class="ltx_text" id="S4.T1.2.3.2.3.1" style="font-size:90%;">127.36</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.4"><span class="ltx_text" id="S4.T1.2.3.2.4.1" style="font-size:90%;">110.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.5"><span class="ltx_text" id="S4.T1.2.3.2.5.1" style="font-size:90%;">62.0</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.3.2.6"><span class="ltx_text" id="S4.T1.2.3.2.6.1" style="font-size:90%;">3.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.4.3.1"><span class="ltx_text" id="S4.T1.2.4.3.1.1" style="font-size:90%;">Fast-V2V</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3.2"><span class="ltx_text" id="S4.T1.2.4.3.2.1" style="font-size:90%;">89.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3.3"><span class="ltx_text" id="S4.T1.2.4.3.3.1" style="font-size:90%;">322.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3.4"><span class="ltx_text" id="S4.T1.2.4.3.4.1" style="font-size:90%;">223.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.4.3.5"><span class="ltx_text" id="S4.T1.2.4.3.5.1" style="font-size:90%;">35.1</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.4.3.6"><span class="ltx_text" id="S4.T1.2.4.3.6.1" style="font-size:90%;">24.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.2.5.4.1"><span class="ltx_text" id="S4.T1.2.5.4.1.1" style="font-size:90%;">SVS-GAN</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.2.1" style="font-size:90%;">42.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.3.1" style="font-size:90%;">85.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.4.1" style="font-size:90%;">66.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.5.1" style="font-size:90%;">66.0</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.2.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.4.6.1" style="font-size:90%;">25.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Quantitative results on the Cityscapes sequence dataset.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1"><span class="ltx_text" id="S4.T2.2.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.2"><span class="ltx_text" id="S4.T2.2.1.1.2.1" style="font-size:90%;">FID (↓)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.3">
<span class="ltx_text" id="S4.T2.2.1.1.3.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T2.2.1.1.3.2"><span class="ltx_text" id="S4.T2.2.1.1.3.2.1" style="font-size:90%;">i3d</span></sub><span class="ltx_text" id="S4.T2.2.1.1.3.3" style="font-size:90%;"> (↓)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.4">
<span class="ltx_text" id="S4.T2.2.1.1.4.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T2.2.1.1.4.2"><span class="ltx_text" id="S4.T2.2.1.1.4.2.1" style="font-size:90%;">cd</span></sub><span class="ltx_text" id="S4.T2.2.1.1.4.3" style="font-size:90%;"> (↓)</span>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.5"><span class="ltx_text" id="S4.T2.2.1.1.5.1" style="font-size:90%;">MIoU (↑)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.2.1.1"><span class="ltx_text" id="S4.T2.2.2.1.1.1" style="font-size:90%;">V2V</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.2"><span class="ltx_text" id="S4.T2.2.2.1.2.1" style="font-size:90%;">23.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.3"><span class="ltx_text" id="S4.T2.2.2.1.3.1" style="font-size:90%;">1201.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.1.4"><span class="ltx_text" id="S4.T2.2.2.1.4.1" style="font-size:90%;">123.05</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.1.5"><span class="ltx_text" id="S4.T2.2.2.1.5.1" style="font-size:90%;">36.13</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.2.3.2.1"><span class="ltx_text" id="S4.T2.2.3.2.1.1" style="font-size:90%;">SVS-GAN</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.2.1" style="font-size:90%;">19.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.3.1" style="font-size:90%;">1103.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.4.1" style="font-size:90%;">96.18</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.5.1" style="font-size:90%;">45.08</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Quantitative results on the KITTI-360 dataset.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S4.F4.g1" src="x4.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Our model demonstrates the ability to produce accurate results on long sequence lengths using the KITTI-360 dataset. The figure shows that even with rare classes like the truck, SVS-GAN maintains temporal consistency.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Our model not only demonstrates accurate video generation but also achieves this with significantly fewer parameters and improved inference times, as evidenced by both quantitative and qualitative results. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.F3" title="Figure 3 ‣ 4.3 Metrics ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the capabilities of our framework by displaying the reference frame, the 10th, 20th, and last frames of each sequence alongside their semantic maps. In the first sequence, our model accurately preserves car details, closely aligning with the ground truth. Notably, our model uniquely reproduces the bicycle in the final frame, despite it not being visible in the reference image. This is a result of our discriminator prioritizing rare objects (the OASIS loss) and our generator’s use of a triple-pyramid structure to balance global and local image aspects.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Conversely, WC-V2V struggles with the appearance of the road and lane colors, as seen in the first and second scenes. We can see an issue in V2V where the color of the van on the right changes between the 10th and 20th frames, indicating a lack of temporal coherence. WC-V2V performs better in rendering buildings due to its world view assumption, as buildings are static and give this approach an edge. However, obtaining a 3D world representation is not applicable in pure SVS frameworks.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">The second scene in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.F3" title="Figure 3 ‣ 4.3 Metrics ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">3</span></a> reveals a significant issue with WC-V2V: it assumes a static world representation, unaffected by time, leading to deteriorated results with dynamic objects in the scene. This limitation is illustrated where the people crossing the street behind the bus are missing from the WC-V2V output. This is because WC-V2V relies on the coloration of the 3D world representation when receiving new data. Since the 3D world is assumed to be constant, with only the movement of the ego-vehicle considered, WC-V2V fails in dynamic scenarios.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Additionally, in the second scene, V2V’s reliance on a separate foreground network that does not depend on previous frames causes the flickering and inconsistent appearance of cars throughout the video. While this avoids the problem of complicated optical flow calculations for the movement of the cars, it introduces a larger issue where the appearance of the cars changes completely and is not coherent with the rest of the objects in the image.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Similarly, V2V struggles with maintaining visual consistency for dynamically moving objects, such as the two people crossing the street. This example highlights our framework’s capability in managing dynamic scenes and corner cases not represented in the training dataset. Accurately adhering to the semantic map in these scenarios is crucial, as they are essential for training robust frameworks that can generalize to critical real-world conditions often underrepresented in training datasets.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">The qualitative results are supported by quantitative evidence presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.T1" title="Table 1 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">1</span></a>, where our approach achieves best results throughout all metrics in comparison with the baselines on Cityscapes. Our model exhibits a decrease in FID, indicating better frame-wise visual quality and diversity. Moreover, significant improvements in both FVD<sub class="ltx_sub" id="S4.SS4.p6.1.1">cd</sub> and FVD<sub class="ltx_sub" id="S4.SS4.p6.1.2">i3d</sub> underscore our model’s enhanced temporal coherence, with generated videos exhibiting realistic, smooth transitions. Additionally, our MIoU score exceeds others by over 4 points, demonstrating more accurate adherence to segmentation maps and confirming the model’s effectiveness in maintaining visual and semantic integrity. Lastly, our framework not only significantly outperforms Fast-V2V<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib43" title="">43</a>]</cite> but also achieves this at a higher resolution compared to the <math alttext="512\times 256" class="ltx_Math" display="inline" id="S4.SS4.p6.1.m1.1"><semantics id="S4.SS4.p6.1.m1.1a"><mrow id="S4.SS4.p6.1.m1.1.1" xref="S4.SS4.p6.1.m1.1.1.cmml"><mn id="S4.SS4.p6.1.m1.1.1.2" xref="S4.SS4.p6.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS4.p6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS4.p6.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS4.p6.1.m1.1.1.3" xref="S4.SS4.p6.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.1.m1.1b"><apply id="S4.SS4.p6.1.m1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1"><times id="S4.SS4.p6.1.m1.1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1.1"></times><cn id="S4.SS4.p6.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p6.1.m1.1.1.2">512</cn><cn id="S4.SS4.p6.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.p6.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.1.m1.1c">512\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p6.1.m1.1d">512 × 256</annotation></semantics></math> resolution of Fast-V2V, while both maintain a comparable frame rate of 25 FPS.</p>
</div>
<div class="ltx_para" id="S4.SS4.p7">
<p class="ltx_p" id="S4.SS4.p7.1">Further validation comes from the KITTI-360 dataset, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.F4" title="Figure 4 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">4</span></a>. Despite being trained on sequences with a maximum length of 30 frames, our model is capable of generating videos that exceed 1000 frames in length. This demonstrates the model’s ability to generalize beyond its training constraints and maintain high-quality video generation over extended sequences.</p>
</div>
<div class="ltx_para" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1">The figure displays the generated 348th and 353rd frames, which include a rare class (truck). Our model successfully generates the truck with appropriate structure and detail, illustrating its capability to handle infrequent objects effectively. Additionally, it maintains temporal coherence between these frames which is critical for realistic video generation, ensuring that objects and their movements remain consistent over time.</p>
</div>
<div class="ltx_para" id="S4.SS4.p9">
<p class="ltx_p" id="S4.SS4.p9.1">Quantitative results presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.T2" title="Table 2 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrate that our SVS-GAN framework outperforms V2V on the KITTI-360 dataset. SVS-GAN not only achieves lower FID and FVD scores, indicating better image quality and spatial-temporal coherence, but it also shows a significant 8-point improvement in MIoU, further validating its enhanced capability to maintain semantic integrity in generated videos.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To assess the impact of each modification, we performed an ablation study on the Cityscapes dataset. Due to long training times, we focused on the base model with a <math alttext="512\times 256" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mn id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS5.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><times id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></times><cn id="S4.SS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1.2">512</cn><cn id="S4.SS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">512\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">512 × 256</annotation></semantics></math> resolution, excluding spatial progressive learning. Consequently, the results differ from those presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.T1" title="Table 1 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Our results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.T3" title="Table 3 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">3</span></a> illustrate the impact of each change where configuration A serves as our baseline. Adding the OASIS loss and the encoder/decoder discriminator in configuration B increased the MIoU by approximately 8 points, as the OASIS loss encourages the generator to produce results that the discriminator can easily segment. However, the image/video quality metrics did not show significant improvement, suggesting potential issues with the generator architecture. Key issues include the direct use of the semantic map as input along the reference image. Specifically, the SPADE paper<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#bib.bib30" title="">30</a>]</cite> highlighted that this approach is sub-optimal for managing semantic maps, effectively ’washing away’ semantic details essential for high-quality generation. Furthermore, the generated images are directly merged with the previously warped images without additional refinement, and the foreground module generating the cars fails to consider previous frames, resulting in flickering that severely undermines temporal coherence.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">After modifying the generator architecture in framework C and solving the mentioned issues, we observed a decrease in FVD<sub class="ltx_sub" id="S4.SS5.p3.1.1">i3d</sub> by more than 30 points and a reduction in FID. This indicates that our new generator significantly enhances temporal consistency and video quality. Additionally, the MIoU increased, indicating that our generator receives better guidance with SPADE blocks. Interestingly, our new generator has significantly fewer parameters and produces better quality. This optimization was achieved through our design choices, such as employing fewer ResNet blocks and utilizing skip connections that facilitate better feature integration across the network. Additionally, by considering only one past frame for appearance , as opposed to V2V and WC-V2V which use two past frames, our model’s complexity was reduced while maintaining high performance.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T3.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.3"><span class="ltx_text" id="S4.T3.1.1.3.1" style="font-size:90%;">FID (↓)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.4">
<span class="ltx_text" id="S4.T3.1.1.4.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T3.1.1.4.2"><span class="ltx_text" id="S4.T3.1.1.4.2.1" style="font-size:90%;">i3d</span></sub><span class="ltx_text" id="S4.T3.1.1.4.3" style="font-size:90%;"> (↓)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.5"><span class="ltx_text" id="S4.T3.1.1.5.1" style="font-size:90%;">MIoU (↑)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1">
<math alttext="G" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.m1.1.1" mathsize="90%" xref="S4.T3.1.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:90%;"> Params</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.2.1.1"><span class="ltx_text" id="S4.T3.1.2.1.1.1" style="font-size:90%;">A</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2"><span class="ltx_text" id="S4.T3.1.2.1.2.1" style="font-size:90%;">46.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3"><span class="ltx_text" id="S4.T3.1.2.1.3.1" style="font-size:90%;">110.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4"><span class="ltx_text" id="S4.T3.1.2.1.4.1" style="font-size:90%;">58.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5"><span class="ltx_text" id="S4.T3.1.2.1.5.1" style="font-size:90%;">411.3 M</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.3.2.1"><span class="ltx_text" id="S4.T3.1.3.2.1.1" style="font-size:90%;">B: A+OASIS</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2"><span class="ltx_text" id="S4.T3.1.3.2.2.1" style="font-size:90%;">46.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3"><span class="ltx_text" id="S4.T3.1.3.2.3.1" style="font-size:90%;">105.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4"><span class="ltx_text" id="S4.T3.1.3.2.4.1" style="font-size:90%;">66.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5"><span class="ltx_text" id="S4.T3.1.3.2.5.1" style="font-size:90%;">411.3 M</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.4.3.1"><span class="ltx_text" id="S4.T3.1.4.3.1.1" style="font-size:90%;">C: B+SPADE</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.2.1" style="font-size:90%;">45.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.3.1" style="font-size:90%;">72.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.4.1" style="font-size:90%;">68.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.3.5"><span class="ltx_text" id="S4.T3.1.4.3.5.1" style="font-size:90%;">105.9 M</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Ablation study results on the Cityscapes dataset.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T4.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.3"><span class="ltx_text" id="S4.T4.1.1.3.1" style="font-size:90%;">FID (↓)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.4">
<span class="ltx_text" id="S4.T4.1.1.4.1" style="font-size:90%;">FVD</span><sub class="ltx_sub" id="S4.T4.1.1.4.2"><span class="ltx_text" id="S4.T4.1.1.4.2.1" style="font-size:90%;">i3d</span></sub><span class="ltx_text" id="S4.T4.1.1.4.3" style="font-size:90%;"> (↓)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.5"><span class="ltx_text" id="S4.T4.1.1.5.1" style="font-size:90%;">MIoU (↑)</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1">
<math alttext="G" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.m1.1.1" mathsize="90%" xref="S4.T4.1.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">italic_G</annotation></semantics></math><span class="ltx_text" id="S4.T4.1.1.1.1" style="font-size:90%;"> Params (M)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1"><span class="ltx_text" id="S4.T4.1.2.1.1.1" style="font-size:90%;">C</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2"><span class="ltx_text" id="S4.T4.1.2.1.2.1" style="font-size:90%;">45.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3"><span class="ltx_text" id="S4.T4.1.2.1.3.1" style="font-size:90%;">72.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.1.4.1" style="font-size:90%;">68.47</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.1.2.1.5"><span class="ltx_text" id="S4.T4.1.2.1.5.1" style="font-size:90%;">105.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.3.2.1"><span class="ltx_text" id="S4.T4.1.3.2.1.1" style="font-size:90%;">D</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2"><span class="ltx_text" id="S4.T4.1.3.2.2.1" style="font-size:90%;">48.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3"><span class="ltx_text" id="S4.T4.1.3.2.3.1" style="font-size:90%;">88.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4"><span class="ltx_text" id="S4.T4.1.3.2.4.1" style="font-size:90%;">66.01</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.3.2.5"><span class="ltx_text" id="S4.T4.1.3.2.5.1" style="font-size:90%;">105.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.1.4.3.1"><span class="ltx_text" id="S4.T4.1.4.3.1.1" style="font-size:90%;">E</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.3.2.1" style="font-size:90%;">43.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.3.3.1" style="font-size:90%;">70.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.4.3.4"><span class="ltx_text" id="S4.T4.1.4.3.4.1" style="font-size:90%;">67.97</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.1.4.3.5"><span class="ltx_text" id="S4.T4.1.4.3.5.1" style="font-size:90%;">153.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Comparing different generator architecture modifications.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">We have performed multiple other experiments to validate our generator architecture. In our final architecture, we produce the optical flow from the two semantic maps and then warp the image before passing it to the image encoder. We tested feeding the image as is to the encoder and warping the features of the image that get fed to the decoder (Framework D). We found that our method performs better when looking at all the metrics as seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06074v1#S4.T4" title="Table 4 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ SVS-GAN: Leveraging GANs for Semantic Video Synthesis"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">We have also tested in framework E the use of an extra SPADE block in each layer to take the style of the warped input image, similar to how we did with the semantic map. Although this modification led to slight improvements in both FVD<sub class="ltx_sub" id="S4.SS5.p5.1.1">i3d</sub> and FID compared to framework C, the MIoU slightly worsened. Furthermore, this approach resulted in almost a 50% increase in the number of generator parameters, so we opted to go with our decided architecture.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we defined Semantic Video Synthesis (SVS) and developed a GAN framework capable of generating realistic, time-coherent videos from corresponding semantic maps. Through a dedicated architecture and losses tailored for the SVS application, we achieved state-of-the-art results on the Cityscapes sequence dataset while maintaining a more efficient architecture. Additionally, we are the first to apply this task to the KITTI-360 dataset, successfully generating long sequences of realistic temporally coherent videos from the corresponding semantic maps.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Despite the success of our method in producing realistic, time-coherent results, we identified a limitation in maintaining world-consistency. Each generated frame relies only on the previous frame, without knowledge of the preceding frames. This can result in inconsistencies, such as a car appearing in different colors if it reappears after several frames. Addressing this issue without incorporating a 3D world representation as input is a critical area for future research. Furthermore, another significant limitation is the handling of optical flow, particularly in scenarios where a vehicle maneuvers around corners, such as turning right or left, or navigating roundabouts, where flow estimation becomes much more challenging. Additionally, managing domain shifts throughout extended video sequences presents a critical area for future exploration.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Tariq Berrada, Jakob Verbeek, Camille Couprie, and Karteek Alahari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Unlocking pre-trained image backbones for semantic image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.13314</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">The cityscapes dataset for semantic urban scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, pages 3213–3223, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">2009 IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
George Eskandar, Mohamed Abdelsamad, Karim Armanious, and Bin Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Usis: Unsupervised semantic image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">Computers &amp; Graphics</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 111:14–23, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
George Eskandar, Shuai Zhang, Mohamed Abdelsamad, Mark Youssef, Diandian Guo, and Bin Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">A semi-paired approach for label-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">2023 IEEE International Conference on Image Processing (ICIP)</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pages 2225–2229. IEEE, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">On the content bias in fréchet video distance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, pages 7277–7288, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark suite.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">2012 IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, pages 3354–3361. IEEE, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Photorealistic video generation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.06662</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 33:6840–6851, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Hou-Ning Hu, Yung-Hsu Yang, Tobias Fischer, Trevor Darrell, Fisher Yu, and Min Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Monocular quasi-dense 3d object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 45(2):1992–2008, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Li Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Animate anyone: Consistent and controllable image-to-video synthesis for character animation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 8153–8163, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Ping Hu, Fabian Caba, Oliver Wang, Zhe Lin, Stan Sclaroff, and Federico Perazzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Temporally distributed networks for fast video semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, pages 8818–8827, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Flownet 2.0: Evolution of optical flow estimation with deep networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 2462–2470, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Image-to-image translation with conditional adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 1125–1134, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Oneformer: One transformer to rule universal image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, pages 2989–2998, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Perceptual losses for real-time style transfer and super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, pages 694–711. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Progressive growing of gans for improved quality, stability, and variation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">arXiv preprint arXiv:1710.10196</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Autoencoding beyond pixels using a learned similarity metric.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 1558–1566. PMLR, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Shijie Li, Ming-Ming Cheng, and Juergen Gall.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Dual pyramid generative adversarial networks for semantic image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">arXiv preprint arXiv:2210.04085</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Yiyi Liao, Jun Xie, and Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 45(3):3292–3310, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Kangning Liu, Shuhang Gu, Andrés Romero, and Radu Timofte.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Unsupervised multimodal video-to-video translation via self-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span><span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">, pages 1030–1040, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">2023 IEEE international conference on robotics and automation (ICRA)</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, pages 2774–2781. IEEE, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
H Christopher Longuet-Higgins.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">A computer algorithm for reconstructing a scene from two projections.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Nature</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 293(5828):133–135, 1981.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Bruce D Lucas and Takeo Kanade.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">An iterative image registration technique with an application to stereo vision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">IJCAI’81: 7th international joint conference on Artificial intelligence</span><span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">, volume 2, pages 674–679, 1981.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Arun Mallya, Ting-Chun Wang, Karan Sapra, and Ming-Yu Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">World-consistent video-to-video synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VIII 16</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, pages 359–378. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Semantic image synthesis with spatially-adaptive normalization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, pages 2337–2346, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, pages 3626–3636, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Vadim Sushko, Edgar Schönfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">You only need adversarial supervision for semantic image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">arXiv preprint arXiv:2012.04781</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Rethinking the inception architecture for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">, pages 2818–2826, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Carlo Tomasi and Takeo Kanade.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Shape and motion from image streams under orthography: a factorization method.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">International journal of computer vision</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 9:137–154, 1992.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Mocogan: Decomposing motion and content for video generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib37.5.3" style="font-size:90%;">, pages 1526–1535, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Fvd: A new metric for video generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Hao Wang, Weining Wang, and Jing Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Temporal memory attention for video semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib39.4.2" style="font-size:90%;">2021 IEEE International Conference on Image Processing (ICIP)</span><span class="ltx_text" id="bib.bib39.5.3" style="font-size:90%;">, pages 2254–2258. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Videomae v2: Scaling video masked autoencoders with dual masking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib40.5.3" style="font-size:90%;">, pages 14549–14560, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Video-to-video synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.3.1" style="font-size:90%;">arXiv preprint arXiv:1808.06601</span><span class="ltx_text" id="bib.bib41.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Dilated residual networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">, pages 472–480, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Long Zhuo, Guangcong Wang, Shikai Li, Wayne Wu, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Fast-vid2vid: Spatial-temporal compression for video-to-video synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib43.4.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib43.5.3" style="font-size:90%;">, pages 289–305. Springer, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  9 20:58:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
