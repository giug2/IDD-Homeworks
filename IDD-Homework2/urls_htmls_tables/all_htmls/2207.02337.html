<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.02337] Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms</title><meta property="og:description" content="The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.02337">

<!--Generated on Wed Mar 13 12:52:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Ehsan Hallaji </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Electrical and Computer Engineering, University of Windsor, 401 Sunset Avenue, Windsor, ON N9B 3P4, Canada, <span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>hallaji@uwindsor.ca</span></span></span>
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Roozbeh Razavi-Far </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Department of Electrical and Computer Engineering and School of Computer Science, University of Windsor, 401 Sunset Avenue, Windsor, ON N9B 3P4, Canada, <span id="id4.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>roozbeh@uwindsor.ca</span></span></span>
</span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Mehrdad Saif </span></span></span><span id="id6" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">institutetext: </span>Department of Electrical and Computer Engineering, University of Windsor, 401 Sunset Avenue, Windsor, ON N9B 3P4, Canada, <span id="id6.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">email: </span>msaif@uwindsor.ca</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ehsan Hallaji
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Roozbeh Razavi-Far and Mehrdad Saif
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The advent of federated learning has facilitated large-scale data exchange amongst machine learning models while maintaining privacy. Despite its brief history, federated learning is rapidly evolving to make wider use more practical. One of the most significant advancements in this domain is the incorporation of transfer learning into federated learning, which overcomes fundamental constraints of primary federated learning, particularly in terms of security. This chapter performs a comprehensive survey on the intersection of federated and transfer learning from a security point of view. The main goal of this study is to uncover potential vulnerabilities and defense mechanisms that might compromise the privacy and performance of systems that use federated and transfer learning.</p>
</div>
<section id="Ch0.S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="Ch0.S1.p1" class="ltx_para">
<p id="Ch0.S1.p1.1" class="ltx_p">Machine learning has exploded in popularity as the information era has matured. As a sub-discipline of machine learning, deep learning (DL) is responsible for a number of achievements that have helped popularise the area. The hierarchical feature extraction within the DL models enables them to learn complex underlying patterns in the observed input space. This makes DL models suitable for processing various data types and facilitating different tasks such as prediction <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">HASSANI2022118861 </a></cite>, detection <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib93" title="" class="ltx_ref">8608001 </a></cite>, imputation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref">9370000 </a></cite>, and data reduction <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">HASSANI2021104150 </a></cite>.
Although the success of DL-based projects is contingent on several factors, one of the most important requirements is usually to have access to abundant training samples.</p>
</div>
<div id="Ch0.S1.p2" class="ltx_para">
<p id="Ch0.S1.p2.1" class="ltx_p">With the advancement of technologies such as internet of things and increasing number of intelligent devices, the diversity and volume of generated data is growing at an astonishing pace <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib88" title="" class="ltx_ref">9608954 </a></cite>. This abundant data stream is dispersed and diverse in character. When this data is evaluated as a whole, it may provide knowledge and discoveries that might possibly accelerate technological and scientific advancements. Nonetheless, the privacy hazards associated with data ownership are increasingly becoming a major problem. The conflict between user privacy and high-quality services is driving need for new technologies and research to enable knowledge extraction from data without jeopardising data-holding parties’ privacy.</p>
</div>
<div id="Ch0.S1.p3" class="ltx_para">
<p id="Ch0.S1.p3.1" class="ltx_p">Federated learning (FL) is perhaps the most recent approach presented to potentially resolve this issue. FL allows for the collaborative training of a DL model across a network of client devices. This multi-party collaboration is accomplished by communication with a central server and decentralisation of training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">pmlr-v54-mcmahan17a </a></cite>. In other words, the FL design requires that the training data be retained on the client, or local device, that generated or recorded it. FL’s data decentralisation addresses a significant portion of data and user privacy concerns, since model training at the network’s edge eliminates the need for direct data sharing. Nevertheless, in FL systems one should strike a balance between data privacy and model performance. The appropriate balance is determined by a number of criteria, including the model architecture, data type, and intended application. Beyond FL restrictions, ensuring the confidentiality and security of the FL infrastructure is critical for establishing trust amongst diverse clients of the federated network.</p>
</div>
<div id="Ch0.S1.p4" class="ltx_para">
<p id="Ch0.S1.p4.1" class="ltx_p">The conventional FL imposes a constraint by requiring customers’ training data to use similar attributes. However, most industries such as banking and healthcare do not comply with this assumption. In centralized machine learning, this was addressed by Transfer Learning (TL), which enables a model obtained from a specific domain to be used in other domains of with the same application <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib93" title="" class="ltx_ref">8608001 </a></cite>. Inspired by this, Federated TL (FTL) emerged as a way to overcome this constraint <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref">9076003 </a></cite>. FTL clients may differ in the employed attributes, which is more practical for industrial application. TL effectiveness is heavily reliant on inter-domain interactions. It is worthwhile to mention that organizations joined in a FTL network are often comparable in the service they provide and the data they use. As a result, including TL within the FL architecture can be quite advantageous.</p>
</div>
<div id="Ch0.S1.p5" class="ltx_para">
<p id="Ch0.S1.p5.1" class="ltx_p">FTL is at the crossroads of two distinct and rapidly expanding research areas of privacy-preserving and distributed machine learning. For this reason, it is crucial to investigate more on these two topics to make best use of FTL. Hence, this chapter studies different security and privacy aspects of FTL.</p>
</div>
<div id="Ch0.S1.p6" class="ltx_para">
<p id="Ch0.S1.p6.1" class="ltx_p">Information privacy and machine learning are distinct and rapidly expanding research areas that derive FTL, and, thus, going through their connections to FTL is necessary. Understanding the interplay between TL and FL, as well as identifying potential risks to FTL in real-world applications, is crucial. Knowing compatible diffense mechanisms with FTL is also vital for mitigating potential cyber-threats. Hence, in this chapter, we present a comprehensive survey on possible threats to FTL and the available defense mechanisms.</p>
</div>
<div id="Ch0.S1.p7" class="ltx_para">
<p id="Ch0.S1.p7.1" class="ltx_p">The rest of the chapter is organized as follows. The preliminaries of this survey are explained in Section 2. Section 3 reviews known attack scenarios on FL and TL w.r.t. performance and privacy. Section 4 presents tools and defense mechanisms that are undertook in the literature for mitigating threats to FL and TL. Section 5 explains the future directions in defending FTL. Finally, Chapter 6 concludes the conducted survey.</p>
</div>
</section>
<section id="Ch0.S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="Ch0.S2.p1" class="ltx_para">
<p id="Ch0.S2.p1.1" class="ltx_p">This chapter concisely reviews preliminaries of federated and transfer learning to facilitate the discussions in the following sections.</p>
</div>
<section id="Ch0.S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<div id="Ch0.S2.SS1.p1" class="ltx_para">
<p id="Ch0.S2.SS1.p1.1" class="ltx_p">FL paradigm allows for collaborative model training across several participants (i.e., also referred to as clients or devices). This multi-party collaboration is accomplished by communication with a central server and decentralization of training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">pmlr-v54-mcmahan17a </a></cite>. Data decentralization of FL mitigates a major part of user privacy issues. Moreover, the efficiency of FL reduces the communication overhead in the network. FL can be categorizes from different perspectives, as explained in the following.</p>
</div>
<section id="Ch0.S2.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Categories of Federated Learning</h4>

<div id="Ch0.S2.SS1.SSSx1.p1" class="ltx_para">
<p id="Ch0.S2.SS1.SSSx1.p1.1" class="ltx_p">FL variations generally fall under three categories depending on the portion of feature and sample space they share <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib114" title="" class="ltx_ref">10.1145/3298981 </a></cite>:</p>
<ol id="Ch0.S2.I1" class="ltx_enumerate">
<li id="Ch0.S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Ch0.S2.I1.i1.p1" class="ltx_para">
<p id="Ch0.S2.I1.i1.p1.1" class="ltx_p"><span id="Ch0.S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Horizontal Federated Learning:</span> Participants exchange data with comparable properties captured from various users <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">1316832 </a></cite>. For instance, clinical information of various patients is recorded using the same features across several hospitals. Therefore, similar deep learning structures can be trained on these datasets since they all process the same number and types of features.</p>
</div>
</li>
<li id="Ch0.S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Ch0.S2.I1.i2.p1" class="ltx_para">
<p id="Ch0.S2.I1.i2.p1.1" class="ltx_p"><span id="Ch0.S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Vertical Federated Learning:</span> The vertical variation is utilized in applications where participant datasets have considerable overlaps in the sample space but each has a separate set of attributes <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib104" title="" class="ltx_ref">10.1145/775047.775142 </a></cite>.</p>
</div>
</li>
<li id="Ch0.S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Ch0.S2.I1.i3.p1" class="ltx_para">
<p id="Ch0.S2.I1.i3.p1.1" class="ltx_p"><span id="Ch0.S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Federated Transfer Learning:</span> FTL facilitates knowledge transfer between participants when the overlap between sample and feature space is minimal <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref">9076003 </a></cite>. FTL is discussed in further detail later in this section.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="Ch0.S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Transfer Learning</h3>

<div id="Ch0.S2.SS2.p1" class="ltx_para">
<p id="Ch0.S2.SS2.p1.1" class="ltx_p">Most machine learning approaches work on the premise that the training and test data are in the same feature space. In industry, data may be hard to collect in certain applications, and, thus, there is a preference for using the available data shared by large companies and organizations. The challenge, however, is the difference between the data distributions despite the similarity of the applications. In other words, the ideal model needs to model from one domain with limited data resources, while abundant training data is available in another domain. For instance, consider a factory that trained a model to predict the market demand to adjust it production rate for different items. However, it may be time consuming to obtain enough samples for each produced item separately. Instead, TL can enable the model to be trained on the data of other organizations that produce similar products, albeit samples may not be recorded using the same features. The past decade has witnessed an increasing attraction towards research on TL, which resulted in proposing different variations TL under different names <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib78" title="" class="ltx_ref">5288526 </a></cite>.</p>
</div>
</section>
<section id="Ch0.S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Federated Transfer Learning</h3>

<div id="Ch0.S2.SS3.p1" class="ltx_para">
<p id="Ch0.S2.SS3.p1.1" class="ltx_p">Similar to TL, clients of FTL may not use the same attributes in the training data. This is mostly the case in organizations that are similar in nature but are not identical <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib92" title="" class="ltx_ref">DBLP:journals/corr/abs-2010-15561 </a></cite>. Due to such differences, these organizations share only a small portion of each other’s feature space. Therefore, under this condition, both samples and features are different in the dataset. Note that the considered condition in FTL is in contrast to the other variants of FL.</p>
</div>
<div id="Ch0.S2.SS3.p2" class="ltx_para">
<p id="Ch0.S2.SS3.p2.1" class="ltx_p">FTL takes a model that has been constructed on source data, and then aligns it to be employed in a target domain. This allows the model to be utilized for uncorrelated data points while exploiting the information gained from non-overlapping features in the source domain. As a result, FTL transmits information from the source domain’s non-overlapping attributes to new samples within the target domain.</p>
</div>
<div id="Ch0.S2.SS3.p3" class="ltx_para">
<p id="Ch0.S2.SS3.p3.1" class="ltx_p">Existing literature on FTL mainly studies the customization of FTL for certain applications <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref">9076082 </a>; <a href="#bib.bib113" title="" class="ltx_ref">9099064 </a></cite>. From a learning stand-point, only a limited number of works present distinct FTL protocols. A secure FTL framework is proposed in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib64" title="" class="ltx_ref">9076003 </a></cite> that uses secret sharing and homomorphic encrypton to protect privacy without sacrificing accuracy, which is a typical issue in privacy-preserving techniques. Other benefits of this method include the simplicity of homomorphic encryption and the fact that secret sharing ensures zero accuracy loss and quick processing time. On the other had, Homomorphic encryption, imposes significant computing overhead, and secret sharing necessitated offline operations prior to online computation. A following research <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib98" title="" class="ltx_ref">9006280 </a></cite> tackles the computation overhead of previous protocols and extends the FTL model beyond the semi-honset setting by taking malicious users into count as well. The authors use secret-sharing in the designed algorithm to enhance the security and efficiency of multi-party communications in FTL. An scalable heterogeneous FTL framework is also presented in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">9005992 </a></cite>, which uses secret sharing and homomorphic encryption.</p>
</div>
</section>
</section>
<section id="Ch0.S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Threats to Federated Learning</h2>

<figure id="Ch0.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Identification of sources of attacks on FL systems.</figcaption>
<table id="Ch0.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch0.T1.1.1.1" class="ltx_tr">
<th id="Ch0.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Attacks</th>
<th id="Ch0.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:15.0pt;padding-right:15.0pt;">Source of Attack</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch0.T1.1.2.1" class="ltx_tr">
<td id="Ch0.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">Data poisoning</td>
<td id="Ch0.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client</td>
</tr>
<tr id="Ch0.T1.1.3.2" class="ltx_tr">
<td id="Ch0.T1.1.3.2.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Model poisoning</td>
<td id="Ch0.T1.1.3.2.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client</td>
</tr>
<tr id="Ch0.T1.1.4.3" class="ltx_tr">
<td id="Ch0.T1.1.4.3.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Backdoor attack</td>
<td id="Ch0.T1.1.4.3.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client and malicious server</td>
</tr>
<tr id="Ch0.T1.1.5.4" class="ltx_tr">
<td id="Ch0.T1.1.5.4.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Evasion attack</td>
<td id="Ch0.T1.1.5.4.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client and model deployment</td>
</tr>
<tr id="Ch0.T1.1.6.5" class="ltx_tr">
<td id="Ch0.T1.1.6.5.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Non-robust aggregation</td>
<td id="Ch0.T1.1.6.5.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Aggregation algorithm</td>
</tr>
<tr id="Ch0.T1.1.7.6" class="ltx_tr">
<td id="Ch0.T1.1.7.6.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Training rule manipulation</td>
<td id="Ch0.T1.1.7.6.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client</td>
</tr>
<tr id="Ch0.T1.1.8.7" class="ltx_tr">
<td id="Ch0.T1.1.8.7.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Inference attacks</td>
<td id="Ch0.T1.1.8.7.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious server and communication</td>
</tr>
<tr id="Ch0.T1.1.9.8" class="ltx_tr">
<td id="Ch0.T1.1.9.8.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">GAN reconstruction</td>
<td id="Ch0.T1.1.9.8.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious server and communication</td>
</tr>
<tr id="Ch0.T1.1.10.9" class="ltx_tr">
<td id="Ch0.T1.1.10.9.1" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Free-riding attack</td>
<td id="Ch0.T1.1.10.9.2" class="ltx_td ltx_align_left" style="padding-left:15.0pt;padding-right:15.0pt;">Malicious client</td>
</tr>
<tr id="Ch0.T1.1.11.10" class="ltx_tr">
<td id="Ch0.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">Man-in-the-middle attack</td>
<td id="Ch0.T1.1.11.10.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:15.0pt;padding-right:15.0pt;">Communication</td>
</tr>
</tbody>
</table>
</figure>
<div id="Ch0.S3.p1" class="ltx_para">
<p id="Ch0.S3.p1.1" class="ltx_p">Threats to FL and TL often compromise the functionality or privacy of the system. Table <a href="#Ch0.T1" title="Table 1 ‣ 3 Threats to Federated Learning ‣ Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists the sources of threats for common attacks to FL. FL enables a distributed learning process without requiring data exchange, allowing members to freely join and exit federations. Recent research has shown, however, that resilience of FL against the mentioned threats in Table <a href="#Ch0.T1" title="Table 1 ‣ 3 Threats to Federated Learning ‣ Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> can be questionable.</p>
</div>
<div id="Ch0.S3.p2" class="ltx_para">
<p id="Ch0.S3.p2.1" class="ltx_p">Existing FL protocol designs are vulnerable to rough servers and adversarial parties. While both infer confidential information from participants’ updates, the former mainly tampers with the model training whereas the latter resorts to poisoning attacks to deviate the aggregation procedure.</p>
</div>
<div id="Ch0.S3.p3" class="ltx_para">
<p id="Ch0.S3.p3.1" class="ltx_p">During the training process, communicating model updates might divulge critical information and lead to deep leakage. This can consequently jeopardize the privacy of local data or lead to high-jacking the training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib118" title="" class="ltx_ref">NEURIPS2019_60a6c400 </a></cite>. The robustness of FL systems, on the other hand, can be degraded using poisoning attacks on the model to corrupt the model or training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib107" title="" class="ltx_ref">DBLP:journals/corr/abs-2007-05084 </a>; <a href="#bib.bib8" title="" class="ltx_ref">pmlr-v108-bagdasaryan20a </a>; <a href="#bib.bib13" title="" class="ltx_ref">pmlr-v97-bhagoji19a </a></cite>. In turn, these attacks lead to planting a backdoor into the global model or degrade its convergence.</p>
</div>
<section id="Ch0.S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Threat Models</h3>

<div id="Ch0.S3.SS1.p1" class="ltx_para">
<p id="Ch0.S3.SS1.p1.1" class="ltx_p">Attacks on FL can be launched in different fashions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">DBLP:journals/corr/abs-2012-06337 </a>; <a href="#bib.bib69" title="" class="ltx_ref">https://doi.org/10.48550/arxiv.2003.02133 </a></cite>. To have a better grasp of the nature of FL attacks, we will first go through the most frequent threat models in the following:</p>
</div>
<div id="Ch0.S3.SS1.p2" class="ltx_para">
<ul id="Ch0.S3.I1" class="ltx_itemize">
<li id="Ch0.S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i1.p1" class="ltx_para">
<p id="Ch0.S3.I1.i1.p1.1" class="ltx_p"><span id="Ch0.S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Outsider Adversaries</span> include attacks by eavesdroppers on the line of communication between clients and the FL server, as well as attacks by clients of the FL model once it is provided as a service.</p>
</div>
</li>
<li id="Ch0.S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i2.p1" class="ltx_para">
<p id="Ch0.S3.I1.i2.p1.1" class="ltx_p"><span id="Ch0.S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Insider Adversaries</span> involve attacks initiated from the server or the edge of the network. Byzantine <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">NIPS2017_f4b9ec30 </a></cite> and Sybil <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">DBLP:journals/corr/abs-1808-04866 </a></cite> attacks can be mentioned as two most important insider attacks.</p>
</div>
</li>
<li id="Ch0.S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i3.p1" class="ltx_para">
<p id="Ch0.S3.I1.i3.p1.1" class="ltx_p"><span id="Ch0.S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Semi-Honest Adversaries</span> are non-aggressive adversaries that attempt to discover the hidden states of other users while being honest to the FL protocol. Only the received information such as the global model’s parameters is visible to the attackers.</p>
</div>
</li>
<li id="Ch0.S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i4.p1" class="ltx_para">
<p id="Ch0.S3.I1.i4.p1.1" class="ltx_p"><span id="Ch0.S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Training Manipulation</span> is the process of learning, affecting, or distorting the FL model itself <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">pmlr-v20-biggio11 </a></cite>. The attacker can damage the integrity of the learning process by attacking the training data or the model during the training phase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">pmlr-v97-bhagoji19a </a></cite>.</p>
</div>
</li>
<li id="Ch0.S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i5.p1" class="ltx_para">
<p id="Ch0.S3.I1.i5.p1.1" class="ltx_p"><span id="Ch0.S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Inference Manipulation</span> mainly consists of evasion or inference attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">10.1145/1128817.1128824 </a></cite>. They usually deceive the model into making incorrect decisions or gather information regarding the model’s properties. These opponents’ efficacy is determined by the amount of knowledge given to the attacker, which classifies them into white-box and black-box variations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Ch0.S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Attacks on Performance</h3>

<div id="Ch0.S3.SS2.p1" class="ltx_para">
<p id="Ch0.S3.SS2.p1.1" class="ltx_p">Figure <a href="#Ch0.F1" title="Figure 1 ‣ 3.2 Attacks on Performance ‣ 3 Threats to Federated Learning ‣ Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the taxonomy of attacks on FL and TL. The common threats between FL and TL that can jeopardize FTL are also specified within the red area.</p>
</div>
<figure id="Ch0.F1" class="ltx_figure"><img src="/html/2207.02337/assets/x1.png" id="Ch0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taxonomy of the attacks on Federated and Transfer Learning. The common threats between FL and TL are specified in red area.</figcaption>
</figure>
<section id="Ch0.S3.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Data Poisoning</h4>

<div id="Ch0.S3.SS2.SSSx1.p1" class="ltx_para">
<p id="Ch0.S3.SS2.SSSx1.p1.1" class="ltx_p">Poisoning training data in FL often affect the integrity of the training data, compromising model performance by injecting a backdoor for particular triggers during inference and degrading the overall model accuracy. The most common types of data poisoning attacks are as follows:</p>
<ul id="Ch0.S3.I2" class="ltx_itemize">
<li id="Ch0.S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I2.i1.p1" class="ltx_para">
<p id="Ch0.S3.I2.i1.p1.1" class="ltx_p"><span id="Ch0.S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Denial-of-Service (DoS)</span> attacks generally attempt to minimize the target overall performance, impacting the recognition rate of all classes. In a FL system, label noise in the training data may be induced to create a poisoned model that cannot accurately predict any of the classes. This is also referred to as <span id="Ch0.S3.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">label flipping</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite> in the literature. When the parameters of this model are sent to the server, the performance of other devices diminishes.</p>
</div>
</li>
<li id="Ch0.S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I2.i2.p1" class="ltx_para">
<p id="Ch0.S3.I2.i2.p1.1" class="ltx_p"><span id="Ch0.S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Backdoor</span> attacks are designed to impose intentional and particular false predictions for specific data patterns. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite>. Backdoor attacks, unlike DoS attacks, only influence the model’s recognition ability for a specific group of samples or classes.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Ch0.S3.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Model Poisoning</h4>

<div id="Ch0.S3.SS2.SSSx2.p1" class="ltx_para">
<p id="Ch0.S3.SS2.SSSx2.p1.1" class="ltx_p">Poisoning a model refers to a wide range of techniques for tampering with the FL training procedure. It is worth mentioning that in some literature data poisoning is categorized as a type of model poisoning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">9308910 </a></cite>. However, here, we mainly target gradient and learning objective manipulation when referring to model poisoning.</p>
<ul id="Ch0.S3.I3" class="ltx_itemize">
<li id="Ch0.S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I3.i1.p1" class="ltx_para">
<p id="Ch0.S3.I3.i1.p1.1" class="ltx_p"><span id="Ch0.S3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Gradient Manipulation:</span> Local model gradients may be manipulated by adversaries to degrade overall performance of the central model, for example, by lowering detection accuracy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite>. For instance, this approach is used to inject hidden global model backdoors <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">pmlr-v108-bagdasaryan20a </a></cite>.</p>
</div>
</li>
<li id="Ch0.S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I3.i2.p1" class="ltx_para">
<p id="Ch0.S3.I3.i2.p1.1" class="ltx_p"><span id="Ch0.S3.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Training Objective Manipulation:</span> involve manipulating model training rules <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite>. Training rule manipulation, for example, is used to successfully carry out a covert poisoning operation by appending a deviating term to the loss function to penalize the difference of benign and malicious updates <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">pmlr-v97-bhagoji19a </a></cite>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="Ch0.S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Attacks on Privacy</h3>

<section id="Ch0.S3.SS3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Model Inversion Attacks</h4>

<div id="Ch0.S3.SS3.SSSx1.p1" class="ltx_para">
<p id="Ch0.S3.SS3.SSSx1.p1.1" class="ltx_p">It has been demonstrated that model inversion attacks can successfully define sensitive characteristics of the classes and instances covered by the model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a>; <a href="#bib.bib52" title="" class="ltx_ref">9308910 </a></cite>. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">10.1145/2810103.2813677 </a></cite> states that using these attacks in a white-box setting on decision trees enables reveal sensitive variables such as survey responses, which may be identified with no false positives. Another study demonstrates that a hacker may anticipate genetic data of a person simply using their demographic information <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite>.</p>
</div>
</section>
<section id="Ch0.S3.SS3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Membership Inference Attacks</h4>

<div id="Ch0.S3.SS3.SSSx2.p1" class="ltx_para">
<p id="Ch0.S3.SS3.SSSx2.p1.1" class="ltx_p">Membership inference aims at disclosing the membership of a particular sample to the training dataset or a certain class. Furthermore, this form of attack can work even if the objective is unrelated to the basic characteristics of the class <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib72" title="" class="ltx_ref">8835269 </a></cite>.</p>
</div>
</section>
<section id="Ch0.S3.SS3.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">GAN Reconstruction Attacks</h4>

<div id="Ch0.S3.SS3.SSSx3.p1" class="ltx_para">
<p id="Ch0.S3.SS3.SSSx3.p1.1" class="ltx_p">Model inversion is similar to GAN reconstruction; however, the latter is substantially more potent and have been demonstrated to create artificial samples that statistically resemble of the training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">10.1145/3133956.3134012 </a></cite>. Traditional model inversion approaches utterly fail when attacking more sophisticated DL architectures, whereas GAN reconstruction attacks may effectively create desirable outputs. It has been shown that even with the presence of differential privacy, GAN may be able to reach the objective. As a result, an adversary may be able to persuade benign clients to mistakenly commit gradient modifications that leak more confidential details than planned during collaborative learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">10.1145/3133956.3134012 </a></cite>.</p>
</div>
</section>
</section>
</section>
<section id="Ch0.S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Threats to Transfer Learning</h2>

<section id="Ch0.S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Backdoor Attacks</h3>

<div id="Ch0.S4.SS1.p1" class="ltx_para">
<p id="Ch0.S4.SS1.p1.4" class="ltx_p">Plenty of the pre-trained Teacher models (<math id="Ch0.S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p1.1.m1.1.1" xref="Ch0.S4.SS1.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p1.1.m1.1b"><ci id="Ch0.S4.SS1.p1.1.m1.1.1.cmml" xref="Ch0.S4.SS1.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p1.1.m1.1c">\mathcal{T}</annotation></semantics></math>) employed for TL are openly available, making them vulnerable to backdoor attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib108" title="" class="ltx_ref">9112322 </a>; <a href="#bib.bib116" title="" class="ltx_ref">10.1145/3319535.3354209 </a></cite>. In a white-box setup, the intruder has access to <math id="Ch0.S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p1.2.m2.1.1" xref="Ch0.S4.SS1.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p1.2.m2.1b"><ci id="Ch0.S4.SS1.p1.2.m2.1.1.cmml" xref="Ch0.S4.SS1.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p1.2.m2.1c">\mathcal{T}</annotation></semantics></math>, as is prevalent in modern applications. The intruder intends to cause a erroneous decision making for a Student model (<math id="Ch0.S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p1.3.m3.1.1" xref="Ch0.S4.SS1.p1.3.m3.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p1.3.m3.1b"><ci id="Ch0.S4.SS1.p1.3.m3.1.1.cmml" xref="Ch0.S4.SS1.p1.3.m3.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p1.3.m3.1c">\mathcal{S}</annotation></semantics></math>) that has been calibrated via TL using a publicly available pre-trained <math id="Ch0.S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p1.4.m4.1.1" xref="Ch0.S4.SS1.p1.4.m4.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p1.4.m4.1b"><ci id="Ch0.S4.SS1.p1.4.m4.1.1.cmml" xref="Ch0.S4.SS1.p1.4.m4.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p1.4.m4.1c">\mathcal{T}</annotation></semantics></math> model.</p>
</div>
<div id="Ch0.S4.SS1.p2" class="ltx_para">
<p id="Ch0.S4.SS1.p2.7" class="ltx_p">The attacker may breach the publicly accessible pre-trained <math id="Ch0.S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.1.m1.1.1" xref="Ch0.S4.SS1.p2.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.1.m1.1b"><ci id="Ch0.S4.SS1.p2.1.m1.1.1.cmml" xref="Ch0.S4.SS1.p2.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.1.m1.1c">\mathcal{T}</annotation></semantics></math> model prior to the <math id="Ch0.S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.2.m2.1.1" xref="Ch0.S4.SS1.p2.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.2.m2.1b"><ci id="Ch0.S4.SS1.p2.2.m2.1.1.cmml" xref="Ch0.S4.SS1.p2.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.2.m2.1c">\mathcal{S}</annotation></semantics></math> system deployment phase. Because the regulations of third-party platforms that store diverse <math id="Ch0.S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.3.m3.1.1" xref="Ch0.S4.SS1.p2.3.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.3.m3.1b"><ci id="Ch0.S4.SS1.p2.3.m3.1.1.cmml" xref="Ch0.S4.SS1.p2.3.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.3.m3.1c">\mathcal{T}</annotation></semantics></math> models are often inadequate, the platforms contain multiple variations of the same pre-trained neural networks. Since weights of a neural network are not self-explanatory, distinguishing damaging models from refined models is complicated, if not impossible. In this situation, we suppose that the intruder is familiar with the structure and parameters of the <math id="Ch0.S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.4.m4.1.1" xref="Ch0.S4.SS1.p2.4.m4.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.4.m4.1b"><ci id="Ch0.S4.SS1.p2.4.m4.1.1.cmml" xref="Ch0.S4.SS1.p2.4.m4.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.4.m4.1c">\mathcal{T}</annotation></semantics></math> and has black-box access to <math id="Ch0.S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.5.m5.1.1" xref="Ch0.S4.SS1.p2.5.m5.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.5.m5.1b"><ci id="Ch0.S4.SS1.p2.5.m5.1.1.cmml" xref="Ch0.S4.SS1.p2.5.m5.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.5.m5.1c">\mathcal{S}</annotation></semantics></math>, but is unaware of the specific <math id="Ch0.S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.6.m6.1.1" xref="Ch0.S4.SS1.p2.6.m6.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.6.m6.1b"><ci id="Ch0.S4.SS1.p2.6.m6.1.1.cmml" xref="Ch0.S4.SS1.p2.6.m6.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.6.m6.1c">\mathcal{T}</annotation></semantics></math> who trained this model and which layers were fixed for training <math id="Ch0.S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p2.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p2.7.m7.1.1" xref="Ch0.S4.SS1.p2.7.m7.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p2.7.m7.1b"><ci id="Ch0.S4.SS1.p2.7.m7.1.1.cmml" xref="Ch0.S4.SS1.p2.7.m7.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p2.7.m7.1c">\mathcal{S}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib106" title="" class="ltx_ref">10.5555/3277203.3277300 </a></cite>.</p>
</div>
<div id="Ch0.S4.SS1.p3" class="ltx_para">
<p id="Ch0.S4.SS1.p3.9" class="ltx_p">Adversaries might potentially get around the fine-tuning technique by leveraging openly accessible pre-trained <math id="Ch0.S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.1.m1.1.1" xref="Ch0.S4.SS1.p3.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.1.m1.1b"><ci id="Ch0.S4.SS1.p3.1.m1.1.1.cmml" xref="Ch0.S4.SS1.p3.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.1.m1.1c">\mathcal{T}</annotation></semantics></math> models to construct <math id="Ch0.S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.2.m2.1.1" xref="Ch0.S4.SS1.p3.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.2.m2.1b"><ci id="Ch0.S4.SS1.p3.2.m2.1.1.cmml" xref="Ch0.S4.SS1.p3.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.2.m2.1c">\mathcal{S}</annotation></semantics></math> models. The <math id="Ch0.S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.3.m3.1.1" xref="Ch0.S4.SS1.p3.3.m3.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.3.m3.1b"><ci id="Ch0.S4.SS1.p3.3.m3.1.1.cmml" xref="Ch0.S4.SS1.p3.3.m3.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.3.m3.1c">\mathcal{S}</annotation></semantics></math> models must be optimized using particular <math id="Ch0.S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.4.m4.1.1" xref="Ch0.S4.SS1.p3.4.m4.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.4.m4.1b"><ci id="Ch0.S4.SS1.p3.4.m4.1.1.cmml" xref="Ch0.S4.SS1.p3.4.m4.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.4.m4.1c">\mathcal{T}</annotation></semantics></math> models, in which a portion of the <math id="Ch0.S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.5.m5.1.1" xref="Ch0.S4.SS1.p3.5.m5.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.5.m5.1b"><ci id="Ch0.S4.SS1.p3.5.m5.1.1.cmml" xref="Ch0.S4.SS1.p3.5.m5.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.5.m5.1c">\mathcal{T}</annotation></semantics></math> structure must be incorporated and retrained frequently. In a white-box setting, we presume the intruder knows the certain <math id="Ch0.S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS1.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.6.m6.1.1" xref="Ch0.S4.SS1.p3.6.m6.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.6.m6.1b"><ci id="Ch0.S4.SS1.p3.6.m6.1.1.cmml" xref="Ch0.S4.SS1.p3.6.m6.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.6.m6.1c">\mathcal{T}</annotation></semantics></math> that trained <math id="Ch0.S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p3.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.7.m7.1.1" xref="Ch0.S4.SS1.p3.7.m7.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.7.m7.1b"><ci id="Ch0.S4.SS1.p3.7.m7.1.1.cmml" xref="Ch0.S4.SS1.p3.7.m7.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.7.m7.1c">\mathcal{S}</annotation></semantics></math> and which layers were unchanged throughout the <math id="Ch0.S4.SS1.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p3.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.8.m8.1.1" xref="Ch0.S4.SS1.p3.8.m8.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.8.m8.1b"><ci id="Ch0.S4.SS1.p3.8.m8.1.1.cmml" xref="Ch0.S4.SS1.p3.8.m8.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.8.m8.1c">\mathcal{S}</annotation></semantics></math> training. The adversary, in particular, has access to the architecture and weights of the <math id="Ch0.S4.SS1.p3.9.m9.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS1.p3.9.m9.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS1.p3.9.m9.1.1" xref="Ch0.S4.SS1.p3.9.m9.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS1.p3.9.m9.1b"><ci id="Ch0.S4.SS1.p3.9.m9.1.1.cmml" xref="Ch0.S4.SS1.p3.9.m9.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS1.p3.9.m9.1c">\mathcal{S}</annotation></semantics></math> model and may change them.</p>
</div>
</section>
<section id="Ch0.S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Adversarial Attacks</h3>

<div id="Ch0.S4.SS2.p1" class="ltx_para">
<p id="Ch0.S4.SS2.p1.1" class="ltx_p">In contrast to conventional adversarial attacks, which optimize false data to be mistaken for benign samples, the central notion of adversarial attacks against TL is to optimize a data matrix to imitate the intrinsic representation of the target data. Models transferred by re-learning the last linear layer have recently been shown to be sensitive to adversarial instances produced exclusively using a pre-trained model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib89" title="" class="ltx_ref">rezaei2020targetagnostic </a></cite>. It has been demonstrated that such an attack can fool models that have been transported with end-to-end fine-tuning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">DBLP:journals/corr/abs-2002-02998 </a></cite>. This discovery raises questions about the security of the extensively employed fine-tuning approach.</p>
</div>
</section>
<section id="Ch0.S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Inference Attacks</h3>

<div id="Ch0.S4.SS3.p1" class="ltx_para">
<p id="Ch0.S4.SS3.p1.1" class="ltx_p">An inference attack resorts to data analysis to gather unauthorized information about a subject or database. If an attacker can confidently estimate the true worth of a subject’s confidential information, it can be termed as leaked. The most frequent variants of this approach are membership inference and attribute inference.</p>
</div>
<section id="Ch0.S4.SS3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Membership Inference</h4>

<div id="Ch0.S4.SS3.SSSx1.p1" class="ltx_para">
<p id="Ch0.S4.SS3.SSSx1.p1.1" class="ltx_p">The goal of membership inference in machine learning is to establish whether a sample was employed to train the target model. Discovering the membership status of a particular user data might lead to serious information theft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref">zou2020privacy </a></cite>. For instance, revealing that a patient’s medical records were utilized to train a model linked with an illness might disclose that the patient has the condition.</p>
</div>
<div id="Ch0.S4.SS3.SSSx1.p2" class="ltx_para">
<p id="Ch0.S4.SS3.SSSx1.p2.4" class="ltx_p">In contrast with conventional machine learning, there are two attack surfaces for membership inference in TL setting, that is discovering the membership status of samples for both <math id="Ch0.S4.SS3.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS3.SSSx1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS3.SSSx1.p2.1.m1.1.1" xref="Ch0.S4.SS3.SSSx1.p2.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.SSSx1.p2.1.m1.1b"><ci id="Ch0.S4.SS3.SSSx1.p2.1.m1.1.1.cmml" xref="Ch0.S4.SS3.SSSx1.p2.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.SSSx1.p2.1.m1.1c">\mathcal{S}</annotation></semantics></math> and <math id="Ch0.S4.SS3.SSSx1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS3.SSSx1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS3.SSSx1.p2.2.m2.1.1" xref="Ch0.S4.SS3.SSSx1.p2.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.SSSx1.p2.2.m2.1b"><ci id="Ch0.S4.SS3.SSSx1.p2.2.m2.1.1.cmml" xref="Ch0.S4.SS3.SSSx1.p2.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.SSSx1.p2.2.m2.1c">\mathcal{T}</annotation></semantics></math> models. Furthermore, depending on the abilities of certain adversaries, access to either the <math id="Ch0.S4.SS3.SSSx1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.SS3.SSSx1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS3.SSSx1.p2.3.m3.1.1" xref="Ch0.S4.SS3.SSSx1.p2.3.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.SSSx1.p2.3.m3.1b"><ci id="Ch0.S4.SS3.SSSx1.p2.3.m3.1.1.cmml" xref="Ch0.S4.SS3.SSSx1.p2.3.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.SSSx1.p2.3.m3.1c">\mathcal{T}</annotation></semantics></math> or <math id="Ch0.S4.SS3.SSSx1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.SS3.SSSx1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.SS3.SSSx1.p2.4.m4.1.1" xref="Ch0.S4.SS3.SSSx1.p2.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.SS3.SSSx1.p2.4.m4.1b"><ci id="Ch0.S4.SS3.SSSx1.p2.4.m4.1.1.cmml" xref="Ch0.S4.SS3.SSSx1.p2.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.SS3.SSSx1.p2.4.m4.1c">\mathcal{S}</annotation></semantics></math> model may be possible. Given both attack surfaces and the extent of attackers’ access to the models, there are three possible attack scenarios:</p>
</div>
<div id="Ch0.S4.SS3.SSSx1.p3" class="ltx_para">
<ol id="Ch0.S4.I1" class="ltx_enumerate">
<li id="Ch0.S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Ch0.S4.I1.i1.p1" class="ltx_para">
<p id="Ch0.S4.I1.i1.p1.2" class="ltx_p">The attackers can observe <math id="Ch0.S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.I1.i1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i1.p1.1.m1.1.1" xref="Ch0.S4.I1.i1.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i1.p1.1.m1.1b"><ci id="Ch0.S4.I1.i1.p1.1.m1.1.1.cmml" xref="Ch0.S4.I1.i1.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i1.p1.1.m1.1c">\mathcal{T}</annotation></semantics></math> and aim at ascertaining the state of the <math id="Ch0.S4.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.I1.i1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i1.p1.2.m2.1.1" xref="Ch0.S4.I1.i1.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i1.p1.2.m2.1b"><ci id="Ch0.S4.I1.i1.p1.2.m2.1.1.cmml" xref="Ch0.S4.I1.i1.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i1.p1.2.m2.1c">\mathcal{T}</annotation></semantics></math> dataset’s membership. This approach is analogous to the traditional membership inference attack, in which the target model is trained from the ground up.</p>
</div>
</li>
<li id="Ch0.S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Ch0.S4.I1.i2.p1" class="ltx_para">
<p id="Ch0.S4.I1.i2.p1.2" class="ltx_p">The <math id="Ch0.S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.I1.i2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i2.p1.1.m1.1.1" xref="Ch0.S4.I1.i2.p1.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i2.p1.1.m1.1b"><ci id="Ch0.S4.I1.i2.p1.1.m1.1.1.cmml" xref="Ch0.S4.I1.i2.p1.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i2.p1.1.m1.1c">\mathcal{S}</annotation></semantics></math> model is visible to the attackers, and they attempt to ascertain the status of the <math id="Ch0.S4.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.I1.i2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i2.p1.2.m2.1.1" xref="Ch0.S4.I1.i2.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i2.p1.2.m2.1b"><ci id="Ch0.S4.I1.i2.p1.2.m2.1.1.cmml" xref="Ch0.S4.I1.i2.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i2.p1.2.m2.1c">\mathcal{T}</annotation></semantics></math> dataset’s membership The target model is not directly trained from the target dataset in this scenario.</p>
</div>
</li>
<li id="Ch0.S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Ch0.S4.I1.i3.p1" class="ltx_para">
<p id="Ch0.S4.I1.i3.p1.3" class="ltx_p">The attackers observes the <math id="Ch0.S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.I1.i3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i3.p1.1.m1.1.1" xref="Ch0.S4.I1.i3.p1.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i3.p1.1.m1.1b"><ci id="Ch0.S4.I1.i3.p1.1.m1.1.1.cmml" xref="Ch0.S4.I1.i3.p1.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i3.p1.1.m1.1c">\mathcal{S}</annotation></semantics></math> model and try to deduce the <math id="Ch0.S4.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S4.I1.i3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i3.p1.2.m2.1.1" xref="Ch0.S4.I1.i3.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i3.p1.2.m2.1b"><ci id="Ch0.S4.I1.i3.p1.2.m2.1.1.cmml" xref="Ch0.S4.I1.i3.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i3.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math> dataset’s membership status. In contrast to the first scenario, here the target model is transferred from the <math id="Ch0.S4.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S4.I1.i3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S4.I1.i3.p1.3.m3.1.1" xref="Ch0.S4.I1.i3.p1.3.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S4.I1.i3.p1.3.m3.1b"><ci id="Ch0.S4.I1.i3.p1.3.m3.1.1.cmml" xref="Ch0.S4.I1.i3.p1.3.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S4.I1.i3.p1.3.m3.1c">\mathcal{T}</annotation></semantics></math> model.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="Ch0.S4.SS3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Feature Inference</h4>

<div id="Ch0.S4.SS3.SSSx2.p1" class="ltx_para">
<p id="Ch0.S4.SS3.SSSx2.p1.1" class="ltx_p">An adversary with partial prior knowledge of a target’s record can devise feature inference to fill in the missing features by monitoring the model’s behaviour <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib117" title="" class="ltx_ref">yeom2018privacy </a>; <a href="#bib.bib33" title="" class="ltx_ref">10.5555/2671225.2671227 </a>; <a href="#bib.bib32" title="" class="ltx_ref">10.1145/2810103.2813677 </a></cite>. For instance, a description of attribute inference attack is given in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib117" title="" class="ltx_ref">yeom2018privacy </a></cite> and it has been demonstrated that by incorporating membership inference as a subroutine, this attack may deduce missing attribute values. Based on the missing attributes, a set of distinct feature vectors are generated and passed to the membership inference adversary as input. The output of this process is attribute values that correspond to the vector whose membership is confirmed via membership inference. Experimental validations for the effectiveness of an attribute inference of regression models are also available <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib117" title="" class="ltx_ref">yeom2018privacy </a></cite>.</p>
</div>
</section>
</section>
</section>
<section id="Ch0.S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Defense Mechanisms</h2>

<div id="Ch0.S5.p1" class="ltx_para">
<p id="Ch0.S5.p1.1" class="ltx_p">Various defense mechanisms are proposed to fortify FL against privacy and performance related threats. Figure <a href="#Ch0.F2" title="Figure 2 ‣ 5 Defense Mechanisms ‣ Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the taxonomy of the defense mechanisms in TL and FL, and the common approaches between the two that can be used to defend FTL.</p>
</div>
<figure id="Ch0.F2" class="ltx_figure"><img src="/html/2207.02337/assets/x2.png" id="Ch0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Taxonomy of the defense mechanisms for FTL. The common defense mechanisms between FL and TL are specified in red area.</figcaption>
</figure>
<section id="Ch0.S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Privacy Preserving</h3>

<div id="Ch0.S5.SS1.p1" class="ltx_para">
<p id="Ch0.S5.SS1.p1.1" class="ltx_p">Despite the wide diversity of previous efforts on safeguarding FL privacy, suggested methods typically fall into one of these categories: homomorphic encryption, secure multiparty computation, and differential privacy. The following paragraphs go through each of these groups.</p>
</div>
<section id="Ch0.S5.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Homomorphic Encryption</h4>

<div id="Ch0.S5.SS1.SSSx1.p1" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx1.p1.1" class="ltx_p">By processing on cyphertext, homomorphic encryption is commonly used to secure the learning process. Clients can use homomorphic encryption to perform arithmetic operations on encrypted data (i.e., cyphertext) without having to decode it. The most prevalent techniques in Homomorphic encryption are explained as in the following <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib76" title="" class="ltx_ref">OGBURN2013502 </a></cite>.</p>
</div>
<div id="Ch0.S5.SS1.SSSx1.p2" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx1.p2.1" class="ltx_p">Fully homomorphic encryption is capable of doing arbitrary calculations on the encrypted data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">10.1145/1536414.1536440 </a></cite>. This is while partially homomorphic encryption can only execute one operation (e.g., addition or multiplication), and substantially homomorphic encryption can do several operations <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">10.1007/978-3-642-32009-5_38 </a>; <a href="#bib.bib91" title="" class="ltx_ref">10.1145/359340.359342 </a>; <a href="#bib.bib77" title="" class="ltx_ref">10.1007/3-540-48910-X_16 </a></cite>. The latter, on the other hand, has a restricted amount of additions and multiplications. While completely homomorphic encryption offers greater flexibility, it is inefficient when compared to other forms of homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">10.1145/1536414.1536440 </a></cite>.</p>
</div>
<div id="Ch0.S5.SS1.SSSx1.p3" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx1.p3.1" class="ltx_p">Despite the benefits of homomorphic encryption, executing arithmetic on the encrypted integers increases the memory and processing time costs. For this reason, one of the main problems in homomorphic encryption is to find a proper balance between privacy and utility <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">10.1145/2857705.2857731 </a>; <a href="#bib.bib56" title="" class="ltx_ref">info:doi/10.2196/medinform.8805 </a></cite>. In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib82" title="" class="ltx_ref">8241854 </a></cite>, for instance, additive homomorphic encryption is used to secure distributed learning by securing model changes and maintaining gradient privacy. Another example is <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">DBLP:journals/corr/abs-1711-10677 </a></cite>, which uses an additive homomorphic architecture to defeat honest-but-curious adversaries using federated logistic regression on the encrypted vertical FL data. However, the overburdening of the system with additional computational and communication costs is a typical downside of such systems.</p>
</div>
</section>
<section id="Ch0.S5.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Secure Multiparty Computation</h4>

<div id="Ch0.S5.SS1.SSSx2.p1" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx2.p1.1" class="ltx_p">Secure Multiparty Computation (SMC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib115" title="" class="ltx_ref">4568388 </a></cite> is a sub-field of cryptography, in which multiple parties cooperate to estimate a function on their input, without compromising privacy between participants. As an example of SMC is proposed in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref">7958569 </a></cite>, which enables collaborative training without compromising privacy. Nevertheless, SMC is followed by considerable computational and communication burden, which may deter parties from collaborating. This dramatic rise in communication and processing costs makes SMC undesirable for large-scale FL.</p>
</div>
<div id="Ch0.S5.SS1.SSSx2.p2" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx2.p2.1" class="ltx_p">For safe aggregation of individual model updates, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">10.1145/3133956.3133982 </a></cite> suggested a protocol based on SMC that is secure, communication-efficient, and failure-resistant. Their technique makes the communicating information perceivable only when they are aggregated. Thus, their protocol is secured in honest-but-curious and malicious setups. In other words, none of the participants learns anything beyond the aggregate of the inputs of numerous honest users <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">10.1145/3133956.3133982 </a></cite>.</p>
</div>
<div id="Ch0.S5.SS1.SSSx2.p3" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx2.p3.1" class="ltx_p">Aside from the efficiency-related issues of SMC, another major problem for SMC-based systems is the necessity for all participants to coordinate at the same time during the training process. In practise, such multiparty interactions may not be ideal, especially in FL contexts where the client-server design is typical. Moreover, while the privacy of client data is preserved, malicious parties may still infer sensitive information from the final output
<cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref">7286780 </a>; <a href="#bib.bib90" title="" class="ltx_ref">10.1145/3196494.3196522 </a></cite>. As a result, SMC cannot guarantee information leakage protection, necessitating the incorporation of additional differential privacy mechanisms within the multiparty protocol to overcome these issues <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib85" title="" class="ltx_ref">10.1145/1807167.1807247 </a>; <a href="#bib.bib2" title="" class="ltx_ref">10.1007/978-3-642-24178-9_9 </a></cite>. In addition, all cryptography-based protocols preclude the audition of updates, during which a hacker can covertly inject backdoor features into the shared model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">pmlr-v97-bhagoji19a </a></cite>.</p>
</div>
</section>
<section id="Ch0.S5.SS1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Differential Privacy</h4>

<div id="Ch0.S5.SS1.SSSx3.p1" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx3.p1.1" class="ltx_p">The idea of Diffrential Privacy (DP) is to inject random noise into the generating updates so that the data interpretation becomes infeasible for malicious entities. DP is primarily used to safeguard DFL communications against privacy attacks (e.g., inference attacks); however, the literature also shows that DP is also beneficial against data poisoning, as these attacks are usually designed based on the communicated gradients <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref">10.5555/3367471.3367701 </a>; <a href="#bib.bib1" title="" class="ltx_ref">10.1145/2976749.2978318 </a>; <a href="#bib.bib38" title="" class="ltx_ref">DBLP:journals/corr/abs-1712-07557 </a></cite>.</p>
</div>
<div id="Ch0.S5.SS1.SSSx3.p2" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx3.p2.1" class="ltx_p">In contrast to homomorphic encryption and SMC whose main disadvantage was communication overhead, DP does not overburden the system in this sense. Instead, DP comes at the cost of deteriorating the model quality since the injected noise can potentially add up to the noise within the constructed model. Moreover, DP provides resistance to poisoning attempts due to its group privacy trait. As a result, as the number of attackers increases, this defense will reduce significantly.</p>
</div>
<div id="Ch0.S5.SS1.SSSx3.p3" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx3.p3.1" class="ltx_p">DP can be centralized, local, or distributed. In centralized DP, the noise addition is performed via a server, which makes it impractical in FDL. On the other hand, local <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">6736718 </a></cite> and distributed DP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">10.1145/2873069 </a>; <a href="#bib.bib25" title="" class="ltx_ref">dwork2006our </a></cite> both assume that the aggregator is not trusted, which perfectly complies with the FDL paradigm. In the local variant, participants inject noise to their estimated gradients before sharing them over the blockchain. However, research on local DP indicates its impotency to provide privacy guarantee on large-scale and heterogeneous models with numerous parameters <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib102" title="" class="ltx_ref">DBLP:journals/corr/abs-2009-05537 </a>; <a href="#bib.bib79" title="" class="ltx_ref">papernot2017semisupervised </a></cite>. In FDL, the injected noise should be calibrated to ensure successful DP. Despite the appealing security qualities of local DP, its practicality becomes questionable when dealing with immense number of users.</p>
</div>
<div id="Ch0.S5.SS1.SSSx3.p4" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx3.p4.5" class="ltx_p">It is also possible to integrate TL into DP. For instance, private aggregation of <math id="Ch0.S5.SS1.SSSx3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S5.SS1.SSSx3.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS1.SSSx3.p4.1.m1.1.1" xref="Ch0.S5.SS1.SSSx3.p4.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS1.SSSx3.p4.1.m1.1b"><ci id="Ch0.S5.SS1.SSSx3.p4.1.m1.1.1.cmml" xref="Ch0.S5.SS1.SSSx3.p4.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS1.SSSx3.p4.1.m1.1c">\mathcal{T}</annotation></semantics></math> ensembles <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">papernot2017semisupervised </a>; <a href="#bib.bib80" title="" class="ltx_ref">papernot2018scalable </a></cite> initially training an ensemble of <math id="Ch0.S5.SS1.SSSx3.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S5.SS1.SSSx3.p4.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS1.SSSx3.p4.2.m2.1.1" xref="Ch0.S5.SS1.SSSx3.p4.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS1.SSSx3.p4.2.m2.1b"><ci id="Ch0.S5.SS1.SSSx3.p4.2.m2.1.1.cmml" xref="Ch0.S5.SS1.SSSx3.p4.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS1.SSSx3.p4.2.m2.1c">\mathcal{T}</annotation></semantics></math>s on disjoint subsets of private data, then perturbs the ensemble’s information by introducing noise to the aggregated <math id="Ch0.S5.SS1.SSSx3.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S5.SS1.SSSx3.p4.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS1.SSSx3.p4.3.m3.1.1" xref="Ch0.S5.SS1.SSSx3.p4.3.m3.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS1.SSSx3.p4.3.m3.1b"><ci id="Ch0.S5.SS1.SSSx3.p4.3.m3.1.1.cmml" xref="Ch0.S5.SS1.SSSx3.p4.3.m3.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS1.SSSx3.p4.3.m3.1c">\mathcal{T}</annotation></semantics></math> votes before transferring the information to a <math id="Ch0.S5.SS1.SSSx3.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S5.SS1.SSSx3.p4.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS1.SSSx3.p4.4.m4.1.1" xref="Ch0.S5.SS1.SSSx3.p4.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS1.SSSx3.p4.4.m4.1b"><ci id="Ch0.S5.SS1.SSSx3.p4.4.m4.1.1.cmml" xref="Ch0.S5.SS1.SSSx3.p4.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS1.SSSx3.p4.4.m4.1c">\mathcal{S}</annotation></semantics></math>. The aggregated output of the ensemble then is used to train a <math id="Ch0.S5.SS1.SSSx3.p4.5.m5.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S5.SS1.SSSx3.p4.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS1.SSSx3.p4.5.m5.1.1" xref="Ch0.S5.SS1.SSSx3.p4.5.m5.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS1.SSSx3.p4.5.m5.1b"><ci id="Ch0.S5.SS1.SSSx3.p4.5.m5.1.1.cmml" xref="Ch0.S5.SS1.SSSx3.p4.5.m5.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS1.SSSx3.p4.5.m5.1c">\mathcal{S}</annotation></semantics></math> model, which learns to precisely replicate the ensemble. To meet the desired accuracy, this method requires a large number of clients, and each of them must have sufficient training records. On the other hand, most industrial applications deal with imbalanced data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">8892480 </a></cite>, and similarly, FL data is often imbalanced among parties that does not comply with this assumption.</p>
</div>
<div id="Ch0.S5.SS1.SSSx3.p5" class="ltx_para">
<p id="Ch0.S5.SS1.SSSx3.p5.1" class="ltx_p">It has been established that the usage of DP helps prevent inference attacks in TL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">10.1145/2976749.2978318 </a>; <a href="#bib.bib51" title="" class="ltx_ref">236254 </a></cite>, albeit at the cost of potential utility loss <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref">236254 </a>; <a href="#bib.bib7" title="" class="ltx_ref">10.1145/2976749.2978355 </a></cite>. By definition, DP seeks to conceal the presence or absence of a record in a dataset, which works against the objective of membership inference attacks. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">10.1145/2508859.2516686 </a></cite> draws attention to the fact that these two concepts seem to counteract each other and establishes a link between DP and membership inference attacks. This has been often carried out by minimizing the bias of the model towards any individual sample or feature by including adequate differential privacy noise. The existing connection between records and features is elaborated in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib117" title="" class="ltx_ref">yeom2018privacy </a></cite>.</p>
</div>
</section>
</section>
<section id="Ch0.S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Model Robustness</h3>

<div id="Ch0.S5.SS2.p1" class="ltx_para">
<p id="Ch0.S5.SS2.p1.1" class="ltx_p">Defenses are classified into two types: proactive and reactive. The former is an inexpensive method of anticipating attacks and associated consequences. The reactive defense operates by detecting an invasion and taking preventative steps. In the production environment, it is often deployed as a patch-up. FL presents multiple additional attack surfaces throughout training, resulting in complicated and unique countermeasures. In this part, we will look at some of the most common types of FL defensive tactics and investigate their usefulness and limits.</p>
</div>
<section id="Ch0.S5.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Anomaly Detection</h4>

<div id="Ch0.S5.SS2.SSSx1.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx1.p1.1" class="ltx_p">Anomaly detection methods actively identify and stops malicious updates from affecting the system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref">Hallaji2022 </a>; <a href="#bib.bib9" title="" class="ltx_ref">article </a></cite>. These methods may be also used in FL systems to identify potential threats <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">9411833 </a></cite>. One frequent technique for handling untargeted adversaries is to calculate a specific test error rate on updates and reject those disadvantageous or neutral to the global model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">article </a></cite>.</p>
</div>
<div id="Ch0.S5.SS2.SSSx1.p2" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib99" title="" class="ltx_ref">10.1145/2991079.2991125 </a></cite>, a protection mechanism is proposed that clusters participants based on their submitted suggestive attributes to identify malicious updates. It produces groups of benign and malicious users with each indicator attribute. Another detector monitors drifts in updates using a distance measure for different participants <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">NIPS2017_f4b9ec30 </a></cite>. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref">DBLP:journals/corr/abs-1910-09933 </a></cite> proposed producing low-dimensional model weight surrogates to recognise anomalous updates from participants. An outlier detection-based paradigm presented in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib50" title="" class="ltx_ref">8418594 </a></cite> selects a number of updates that work in favour of objective function among others. DL-based anomaly detection is often performed using autoencoders <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib84" title="" class="ltx_ref">app8122663 </a></cite>. These neural network models represent data in a latent space, in which anomalies can be discriminated. Examples of anomaly detection in FL are given in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">247652 </a>; <a href="#bib.bib61" title="" class="ltx_ref">li2020learning </a></cite>.</p>
</div>
<div id="Ch0.S5.SS2.SSSx1.p3" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx1.p3.1" class="ltx_p">Backdoor attacks in TL may also be mitigated via anomaly detection. As an example, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref">8119189 </a></cite> employs an anomaly detection technique to determine whether the input is a possible Trojan trigger. If the input is identified as an anomaly, it will not be passed to the neural network. This approach employs support vector machines and decision trees to find anomalies.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Robust Aggregation</h4>

<div id="Ch0.S5.SS2.SSSx2.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx2.p1.1" class="ltx_p">The security of FL aggregation techniques is of paramount importance. Extensive research endeavours has been dedicated to research on robust aggregation that can recognize and dismiss inaccurate or malicious updates during training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib83" title="" class="ltx_ref">pillutla2019robust </a>; <a href="#bib.bib41" title="" class="ltx_ref">DBLP:journals/corr/abs-2009-08294 </a></cite>. Furthermore, strong aggregation approaches must be able to withstand communications disturbances, client dropout, and incorrect model updates on top of hostile participants <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">9026922 </a></cite>. Existing constraints <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-04977 </a></cite> of aggregation methods for integration with FL lead to the emergence of more mature techniques such as adaptive aggregation, have been developed. This technique incorporates repeated median regression into an iteratively re-weighted least squares <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">DBLP:journals/corr/abs-1912-11464 </a></cite> and a resilient aggregation oracle <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib83" title="" class="ltx_ref">pillutla2019robust </a></cite>. This form of aggregation has been shown to be resistant to distortion rates distortion up to fifty percent of the users. To assess participants’ prospective contributions, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib66" title="" class="ltx_ref">10.1145/3432291.3432303 </a></cite> recommends employing a Gaussian distribution. They also provided layer-by-layer optimization procedures to ensure that the aggregation works effectively. Experiments reveal that this aggregation method surpasses the well-known FedAvg in terms of robustness and convergence. Aggregation methods can also help with the problem of FL client heterogeneity. FedProx was designed as a re-parametrization and generalisation of FedAvg <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib94" title="" class="ltx_ref">DBLP:journals/corr/abs-1812-06127 </a></cite>. In comparison to FedAvg, it exhibits substantially more consistent and accurate convergence behaviour in highly heterogeneous FL systems.</p>
</div>
<div id="Ch0.S5.SS2.SSSx2.p2" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx2.p2.1" class="ltx_p">Pruning also eliminates backdoors in TL by removing duplicate neurons that are no longer relevant for normal classification <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref">10.1007/978-3-030-00470-5_13 </a></cite>. However, it has been discovered that when applied to particular models, it significantly degrades the model performance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref">8835365 </a></cite>.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Pruning</h4>

<div id="Ch0.S5.SS2.SSSx3.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx3.p1.1" class="ltx_p">Pruning decreases the size of a deep learning model by removing neurons in order to reduce complexity, increase accuracy, and eliminate backdoors Clients in the FL environment are abundant, and they are frequently linked to the server via unreliable or costly connections. When it comes to training large-scale deep neural networks (DNN), engineers encounter a huge challenge due to the restricted processing power on some edge devices. Federated dropout <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">DBLP:journals/corr/abs-1812-07210 </a></cite> demonstrates that a good generalization can be achieved by allowing users to perform partial training on the global model. Both transmission and local processing costs are reduced by means of federated dropout. Discarding inactive nodes of a network also make it robust against backdoors <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref">10.1007/978-3-030-00470-5_13 </a></cite>. Passing benign and malicious behaviour into the same set of activations, one can combine pruning with fine-tuning. It has been shown that using this approach the backdoor task accuracy is reduced to zero in several circumstances.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Trusted Execution Environment</h4>

<div id="Ch0.S5.SS2.SSSx4.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx4.p1.1" class="ltx_p">Trusted Execution Environment (TEE) secures linked devices in FL, establishing digital trust <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">9411833 </a></cite>. By using an isolated and encrypted part of the main processor, it safeguards devices from inserting incorrect training results. TEE can be used in FL to mitigate algorithmic threats <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">CHEN202069 </a>; <a href="#bib.bib73" title="" class="ltx_ref">10.1145/3458864.3466628 </a></cite>. The validity of a participating device in a TEE Authentication should be checked by the connected service with which it is attempting to enroll. Furthermore, until the matching party provides a message, the status of code execution stays hidden. The execution route of the code cannot be changed until it takes explicit input or a validated interruption. The TEE is in charge of all data access privileges. Cryptographic technologies are used to secure TEE communications. Only the TEE secure environment stores, maintains, and uses private and public encryption keys. The TEE can show a remote client what code is presently being executed as well as the starting state. TEE can aid in resolving a key challenge for FL security since it is becoming progressively important in securing the central server and clients against hackers and preventing data theft.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Zero-Knowledge Proofs</h4>

<div id="Ch0.S5.SS2.SSSx5.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx5.p1.1" class="ltx_p">Zero-knowledge proofs allow one party to verify assertions made by another party without exchanging or exposing underlying data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib81" title="" class="ltx_ref">6547113 </a></cite>. In the mid-1980s, MIT researchers initially promoted the notion of zero-knowledge proofs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">doi:10.1137/0218012 </a></cite>. Zero-knowledge procedures are probabilistic evaluations, meaning they cannot guarantee something with 100 percent certainty that it will be discovered. Instead, they supply unconnected bits of information that might add up to suggest that an statement’s truth is overwhelmingly likely. Thus, zero-knowledge proofs offer a practical answer to the problem of private data verifiability. For instance, zero-knowledge proofs can be employed in FL to make sure the clients’ model used authentic feature for training and generating an update. Even though this approach has many appealing potentials for transforming secure update monitoring, we need to better understand how to use these approaches and discover problems in how the modules are constructed and deployed. Zero-knowledge proofs protocols mostly maintain their performance regardless of the volume of data.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Adversarial Training</h4>

<div id="Ch0.S5.SS2.SSSx6.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx6.p1.1" class="ltx_p">Adversarial training denotes a min-max optimization problem in which the adversarial samples and model parameters are updated alternately. Generally, adversarial samples are generated through maximizing a classification loss, and model parameters are attained via minimizing a loss w.r.t. the generated adversarial samples <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib94" title="" class="ltx_ref">DBLP:journals/corr/abs-1812-06127 </a>; <a href="#bib.bib87" title="" class="ltx_ref">book </a>; <a href="#bib.bib42" title="" class="ltx_ref">9609642 </a></cite>. This approach can provide an acceptable resilience against evasion attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib97" title="" class="ltx_ref">NEURIPS2019_7503cfac </a>; <a href="#bib.bib110" title="" class="ltx_ref">DBLP:journals/corr/abs-1812-03411 </a></cite>. While there are different approaches to carry out adversarial training, including the so-called generative adversarial networks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">Farajzadeh-Zanjani2022 </a>; <a href="#bib.bib28" title="" class="ltx_ref">9563211 </a>; <a href="#bib.bib30" title="" class="ltx_ref">9360878 </a>; <a href="#bib.bib29" title="" class="ltx_ref">FARAJZADEHZANJANI2021101 </a></cite>, non of them are flawless. To begin with, this approach was mainly designed for independent and identically distributed data. This is while FL data do not comply with this assumption, and, thus, further research is required to investigate the practicality of adversarial training in FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib49" title="" class="ltx_ref">DBLP:journals/corr/abs-1903-10484 </a></cite>. Furthermore, this approach can be very time-consuming. In addition, adversarial training often improves resilience for cases utilized during the training. Furthermore, it can possibly exhaust FL participants’ limited computational capabilities and leaving the trained model exposed to various forms of adversarial noise <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">pmlr-v97-engstrom19a </a>; <a href="#bib.bib103" title="" class="ltx_ref">NEURIPS2019_5d4ae76f </a></cite>.</p>
</div>
<div id="Ch0.S5.SS2.SSSx6.p2" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx6.p2.1" class="ltx_p">Adversarial training also aids in the prevention of TL inference attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref">zou2020privacy </a>; <a href="#bib.bib75" title="" class="ltx_ref">10.1145/3243734.3243855 </a></cite>. For example, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib75" title="" class="ltx_ref">10.1145/3243734.3243855 </a></cite> presents a technique for training models with membership privacy, which assures that a model’s predictions are indistinguishable on both training and unobserved samples of similar distributions. This technique formulates a min-max problem and develops an adversarial training procedure that minimizes the model’s prediction loss along with the attack maximum gain. This method, which ensures membership privacy, also functions as a powerful regularizer and aids in model generalization.</p>
</div>
<div id="Ch0.S5.SS2.SSSx6.p3" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx6.p3.1" class="ltx_p">Mitigating adversarial attacks is another use-case of adversarial training. As an example, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">aithal2021mitigating </a></cite> investigates the approach of introducing white noise to DL results to counter these assaults and emphasises on the noise-cost balance. The query count of the attacker is calculated analytically based on the noise standard deviation. Consequently, the degree of noise required to prevent attacks can be easily determined while maintaining the appropriate extent of security defined by query count and limiting performance deterioration.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Multi-Task Learning</h4>

<div id="Ch0.S5.SS2.SSSx7.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx7.p1.1" class="ltx_p">The statistical and system difficulties of FL such as efficiency and fault fault tolerance are addressed using Federated Multi-task Learning (FML) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib101" title="" class="ltx_ref">NIPS2017_6211080f </a>; <a href="#bib.bib62" title="" class="ltx_ref">pmlr-v139-li21h </a></cite>. The goal of FML is to learn models for numerous related activities at the same time. It can perfectly handle statistical problems since it can immediately infer associations among non-i.i.d. and imbalanced data. For instance, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib101" title="" class="ltx_ref">NIPS2017_6211080f </a></cite> designs a FML approach to accelerate convergence while managing devices that disconnect on a regular basis. This approach is also flexible against data heterogeneity.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Moving Target Defense</h4>

<div id="Ch0.S5.SS2.SSSx8.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx8.p1.1" class="ltx_p">Moving target defense <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">6578785 </a>; <a href="#bib.bib111" title="" class="ltx_ref">10.1145/2663474.2663486 </a>; <a href="#bib.bib57" title="" class="ltx_ref">article2 </a>; <a href="#bib.bib96" title="" class="ltx_ref">9047923 </a></cite> confuses malevolent adversaries by constantly re-configuring the system and make it harder for intruders to infer system states. This may be accomplished by randomly shifting the FL system’s components and nullify their knowledge of the system. This defense mechanism also creates complexity and expense for attackers and reduces the disclosure of vulnerabilities and the possibility of an attack. It also improves the system resilience, specially against sniffing attacks. This dynamic mechanisms disables intruders to make accurate estimations regarding the required resources for attacking the FL training process.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Client Trustworthiness Assessment</h4>

<div id="Ch0.S5.SS2.SSSx9.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx9.p1.1" class="ltx_p">Poisoning attacks in FL are mostly studied in a centralized context. Only a limited number of research endeavours, however, address these attacks in decentralized systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib109" title="" class="ltx_ref">Xie2020DBA: </a></cite>, where several adversarial parties follow the same objective and attempt to poison the training data. Although these attacks pose a greater risk in FL, their efficiency remains unknown compared to their centralized variants. This protection approach works by detecting authorized clients and drastically increasing the rate of failure for poisoning attacks, even when the attack is initiated in a distributed fashion.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx10" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Federated Distillation</h4>

<div id="Ch0.S5.SS2.SSSx10.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx10.p1.2" class="ltx_p">Exchanging model parameters becomes prohibitively expensive when communication resources are limited, especially for contemporary big DNNs. In this sense, federated distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib102" title="" class="ltx_ref">DBLP:journals/corr/abs-2009-05537 </a></cite> is an appealing FL option since it only transmits model outputs, which are often considerably less in size than the model sizes. Knowledge distillation is a fundamental algorithm in federated distillation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref">li2019fedmd </a></cite>. The goal of knowledge distillation is to perform TL from a large model (<math id="Ch0.S5.SS2.SSSx10.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="Ch0.S5.SS2.SSSx10.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS2.SSSx10.p1.1.m1.1.1" xref="Ch0.S5.SS2.SSSx10.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS2.SSSx10.p1.1.m1.1b"><ci id="Ch0.S5.SS2.SSSx10.p1.1.m1.1.1.cmml" xref="Ch0.S5.SS2.SSSx10.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS2.SSSx10.p1.1.m1.1c">\mathcal{T}</annotation></semantics></math>) to a compact model (<math id="Ch0.S5.SS2.SSSx10.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="Ch0.S5.SS2.SSSx10.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="Ch0.S5.SS2.SSSx10.p1.2.m2.1.1" xref="Ch0.S5.SS2.SSSx10.p1.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS2.SSSx10.p1.2.m2.1b"><ci id="Ch0.S5.SS2.SSSx10.p1.2.m2.1.1.cmml" xref="Ch0.S5.SS2.SSSx10.p1.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS2.SSSx10.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>). In FL, this idea translates into sharing the knowledge of a model rather than the parameters, which improve FL’s resilience while reducing communication and computing costs.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx11" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Regularization</h4>

<div id="Ch0.S5.SS2.SSSx11.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx11.p1.1" class="ltx_p">Classifiers make more confident predictions when confronted with data samples they have been trained on before. For this reasons, overfitting of a model can lead to a successful membership inference. Classifiers make more reliable predictions when confronted with records they have been trained on before. To tackle this issue, researchers have investigated the usage of regularization for preventing overfitting, which in turn eliminates membership inference. The conventional <math id="Ch0.S5.SS2.SSSx11.p1.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="Ch0.S5.SS2.SSSx11.p1.1.m1.1a"><msub id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.cmml"><mi id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.2" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.2.cmml">L</mi><mn id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.3" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch0.S5.SS2.SSSx11.p1.1.m1.1b"><apply id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.cmml" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.1.cmml" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1">subscript</csymbol><ci id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.2.cmml" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.3.cmml" xref="Ch0.S5.SS2.SSSx11.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch0.S5.SS2.SSSx11.p1.1.m1.1c">L_{2}</annotation></semantics></math> regularizer, as an example, is examined during the training process of a target classifier <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib100" title="" class="ltx_ref">7958568 </a></cite>.</p>
</div>
<div id="Ch0.S5.SS2.SSSx11.p2" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx11.p2.1" class="ltx_p">Dropout is another regularization strategy intended to counter membership inference attacks. It was employed in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib95" title="" class="ltx_ref">DBLP:journals/corr/abs-1806-01246 </a></cite> to counter membership inference attacks. In each training cycle, dropout dismisses a neuron with a particular probability.</p>
</div>
<div id="Ch0.S5.SS2.SSSx11.p3" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx11.p3.1" class="ltx_p">Model stacking is a conventional ensemble approach that combines the findings of multiple weak classifiers to form a strong model. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib95" title="" class="ltx_ref">DBLP:journals/corr/abs-1806-01246 </a></cite> investigated the use of this method to counter membership inference. The target classifier, in particular, is comprised of three models grouped into a tree structure (i.e., one model at top and two models at the bottom of the tree).</p>
</div>
<div id="Ch0.S5.SS2.SSSx11.p4" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx11.p4.1" class="ltx_p">The original data samples are fed to leafs of the tree, while the results obtained from the leafs are inputs to the top of the tree. The tree models are trained using separate sets of data samples, which decreases the likelihood that the target classifier will recall any particular point, which in turn reduces overfitting.</p>
</div>
</section>
<section id="Ch0.S5.SS2.SSSx12" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Perturbing Posteriors</h4>

<div id="Ch0.S5.SS2.SSSx12.p1" class="ltx_para">
<p id="Ch0.S5.SS2.SSSx12.p1.1" class="ltx_p">Rather than meddling with the target classifier’s training procedure, one can introduce noise to the classifiers’ outputs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref">10.1145/3319535.3363201 </a></cite>. This concept is referred to as perturbing posteriors. For instance, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref">10.1145/3319535.3363201 </a></cite>, designs a method to protect against membership inference launched in a black-box setting. This defensive approach operates in two stages and offers theoretical robustness guarantee. The first stage involves locating a generated noise vector that may be used to convert a vector of confidence scores into an adversarial example. This noise vector is added to the confidence scores with a probability in the next stage.</p>
</div>
</section>
</section>
</section>
<section id="Ch0.S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Research</h2>

<div id="Ch0.S6.p1" class="ltx_para">
<p id="Ch0.S6.p1.1" class="ltx_p">As mentioned previously, it is anticipated that FTL is most vulnarible against backdoor, membership inference, feature inference, and adversarial samples (refer back to Fig. <a href="#Ch0.F1" title="Figure 1 ‣ 3.2 Attacks on Performance ‣ 3 Threats to Federated Learning ‣ Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). In this section, we outline future development requirements that we believe will be promising for FTL in this sense.</p>
</div>
<section id="Ch0.S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Decentralized Federated Transfer Learning</h3>

<div id="Ch0.S6.SS1.p1" class="ltx_para">
<p id="Ch0.S6.SS1.p1.1" class="ltx_p">Decentralized FL is a new study topic in which the system has no singular central server. Decentralized FL may be more beneficial in business-based FL instances when third parties are not trusted by the clients. Each client might be selected as a server in a turn-based fashion. As of now, there are no decentralized FTL protocols in the literature. It would be fascinating to see if the same risks that exist in server-based FL also arise in decentralized FTL.</p>
</div>
</section>
<section id="Ch0.S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Flaws in Current Defense Mechanisms</h3>

<div id="Ch0.S6.SS2.p1" class="ltx_para">
<p id="Ch0.S6.SS2.p1.1" class="ltx_p">Because FL cannot review updates for privacy reasons, it is vulnerable to poisoning attacks, which is often used to counter adversarial attacks in ML, remains a questionable choice in FL since it was designed particularly for i.i.d. data and its effectiveness in non-i.i.d. scenarios is unknown. This can become problematic in the case of FTL furthermore. Besides, adversarial training is computationally expensive and may degrade efficiency, regardless of the type of FL.</p>
</div>
<div id="Ch0.S6.SS2.p2" class="ltx_para">
<p id="Ch0.S6.SS2.p2.1" class="ltx_p">Available privacy defenses for FTL are mostly based on homomorphic encryption and secret sharing. Nevertheless, since DP is used as a privacy-preserving method in both FL and TL, it may be also used for FTL. If future works extend DP to FTL, there are a number of points to consider. Firstly, DP cannot handle attribute inference. Secondly, client-level DP is designed for large-scale systems with numerous clients, and using it in smaller systems may affect it performance.</p>
</div>
</section>
<section id="Ch0.S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Optimizing Defense Mechanism Deployment</h3>

<div id="Ch0.S6.SS3.p1" class="ltx_para">
<p id="Ch0.S6.SS3.p1.1" class="ltx_p">The servers will require additional computational resources while implementing defense mechanisms to verify if any attacker is targeting the FTL system. Additionally, different forms of defense systems may have varying degrees of efficiency against different types of threats, as well as varying costs. It is crucial to look into how to optimize the deployment of defensive systems or the declaration of deterrent measures for FTL.</p>
</div>
</section>
<section id="Ch0.S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Achieving Simultaneous Objectives</h3>

<div id="Ch0.S6.SS4.p1" class="ltx_para">
<p id="Ch0.S6.SS4.p1.1" class="ltx_p">There are no extant research on FL or FTL that can achieve the following objectives at the same time <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">DBLP:journals/corr/abs-2012-06337 </a></cite>:</p>
<ol id="Ch0.S6.I1" class="ltx_enumerate">
<li id="Ch0.S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Ch0.S6.I1.i1.p1" class="ltx_para">
<p id="Ch0.S6.I1.i1.p1.1" class="ltx_p">Rapid model convergence.</p>
</div>
</li>
<li id="Ch0.S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Ch0.S6.I1.i2.p1" class="ltx_para">
<p id="Ch0.S6.I1.i2.p1.1" class="ltx_p">Descent generalization of model.</p>
</div>
</li>
<li id="Ch0.S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Ch0.S6.I1.i3.p1" class="ltx_para">
<p id="Ch0.S6.I1.i3.p1.1" class="ltx_p">Efficient communication.</p>
</div>
</li>
<li id="Ch0.S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="Ch0.S6.I1.i4.p1" class="ltx_para">
<p id="Ch0.S6.I1.i4.p1.1" class="ltx_p">Preserving privacy.</p>
</div>
</li>
<li id="Ch0.S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="Ch0.S6.I1.i5.p1" class="ltx_para">
<p id="Ch0.S6.I1.i5.p1.1" class="ltx_p">Resilience to targeted and untargeted attacks.</p>
</div>
</li>
<li id="Ch0.S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="Ch0.S6.I1.i6.p1" class="ltx_para">
<p id="Ch0.S6.I1.i6.p1.1" class="ltx_p">Fault tolerance.</p>
</div>
</li>
</ol>
<p id="Ch0.S6.SS4.p1.2" class="ltx_p">Past efforts sought to tackle several objectives simultaneously <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref">9130840 </a></cite>. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib112" title="" class="ltx_ref">xu2021reputation </a></cite> tackled cooperative fairness and privacy at the same time, and a architecture has been developed to solve mitigate these problems. To cut communication overhead and provide privacy perks, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">10.5555/3327757.3327856 </a></cite> integrated DP with model compression approaches. Another research <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">bernstein2019signsgd </a></cite> concentrates on enhancing convergence and preventing gradient leakage. Nevertheless, it is crucial to remember that privacy and robustness are incompatible by nature, as protecting against performance attacks typically necessitates full access to the training samples, which is irreconcilable with FTL’s privacy requirements. Even though encryption and DP-based approaches can guarantee verifiably privacy-preserving, they are vulnerable to poisoning techniques and may result in models with unfavourable privacy-performance trade-off. Finding a cohesive design that meets all of the aforementioned criteria is indeed undiscovered in the FTL domain.</p>
</div>
</section>
<section id="Ch0.S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Heterogeneity of Federated Transfer Learning</h3>

<div id="Ch0.S6.SS5.p1" class="ltx_para">
<p id="Ch0.S6.SS5.p1.1" class="ltx_p">The vast majority of privacy and robustness studies have been conducted on FL with homogenous designs. On the other hand, there is a common assumption of feature co-occurrence among most of the available work on heterogeneous FL. For FTL to be secure, the existing defense mechanisms should be compatible with fully heterogeneous feature space <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">9005992 </a></cite>.</p>
</div>
</section>
</section>
<section id="Ch0.S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="Ch0.S7.p1" class="ltx_para">
<p id="Ch0.S7.p1.1" class="ltx_p">FTL is one of the latest fields of machine learning, it is evolving at a rapid pace and will be a focal point of research in machine learning and privacy. As FL and TL evolve, so will the dangers to FTL’s privacy and security. It is critical to conduct a wide assessment of present FL and TL threats and countermeasures so that upcoming FTL designs consider the possible weaknesses in existing models. This survey provides a clear and straightforward review of the privacy and robustness attack and possible defense mechanisms that may be used in FTL. Designing a coherent FTL defensive mechanism that can withstand various attacks without decreasing model performance would demand multidisciplinary collaboration in the scientific community.</p>
</div>
</section>
<section id="Ch0.Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Ch0.Sx1.p1" class="ltx_para">
<p id="Ch0.Sx1.p1.1" class="ltx_p">This work is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under funding reference numbers CGSD3-569341-2022 and RGPIN-2021-02968.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.B., Mironov, I., Talwar, K.,
Zhang, L.: Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’16, p. 308–318 (2016)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Ács, G., Castelluccia, C.: I have a dream! (differentially private smart
metering).

</span>
<span class="ltx_bibblock">In: Information Hiding, pp. 118–132. Springer Berlin Heidelberg
(2011)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Agarwal, N., Suresh, A.T., Yu, F., Kumar, S., McMahan, H.B.: Cpsgd:
Communication-efficient and differentially-private distributed sgd.

</span>
<span class="ltx_bibblock">In: Proceedings of the 32nd International Conference on Neural
Information Processing Systems, p. 7575–7586 (2018)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Aithal, M.B., Li, X.: Mitigating black-box adversarial attacks via output noise
perturbation (2021).

</span>
<span class="ltx_bibblock">ArXiv:2109.15160

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
Ang, F., Chen, L., Zhao, N., Chen, Y., Wang, W., Yu, F.R.: Robust federated
learning with noisy communication.

</span>
<span class="ltx_bibblock">IEEE Transactions on Communications <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">68</span>(6), 3452–3464 (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
Aono, Y., Hayashi, T., Trieu Phong, L., Wang, L.: Scalable and secure logistic
regression via homomorphic encryption.

</span>
<span class="ltx_bibblock">In: Proceedings of the Sixth ACM Conference on Data and Application
Security and Privacy, p. 142–144 (2016)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Backes, M., Berrang, P., Humbert, M., Manoharan, P.: Membership privacy in
microrna-based studies.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, p. 319–330 (2016)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., Shmatikov, V.: How to backdoor
federated learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics, <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, vol. 108, pp. 2938–2948. PMLR (2020)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Barreno, M., Nelson, B., Joseph, A., Tygar, J.: The security of machine
learning.

</span>
<span class="ltx_bibblock">Machine Learning <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">81</span>, 121–148 (2010)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Barreno, M., Nelson, B., Sears, R., Joseph, A.D., Tygar, J.D.: Can machine
learning be secure?

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM Symposium on Information, Computer and
Communications Security, p. 16–25 (2006)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Benhamouda, F., Joye, M., Libert, B.: A new framework for privacy-preserving
aggregation of time-series data.

</span>
<span class="ltx_bibblock">ACM Trans. Inf. Syst. Secur. <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">18</span>(3) (2016)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
Bernstein, J., Zhao, J., Azizzadenesheli, K., Anandkumar, A.: signsgd with
majority vote is communication efficient and fault tolerant (2019).

</span>
<span class="ltx_bibblock">ArXiv:1810.05291

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
Bhagoji, A.N., Chakraborty, S., Mittal, P., Calo, S.: Analyzing federated
learning through an adversarial lens.

</span>
<span class="ltx_bibblock">In: Proceedings of the 36th International Conference on Machine
Learning, vol. 97, pp. 634–643 (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Biggio, B., Nelson, B., Laskov, P.: Support vector machines under adversarial
label noise.

</span>
<span class="ltx_bibblock">In: C.N. Hsu, W.S. Lee (eds.) Proceedings of the Asian Conference on
Machine Learning, <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, vol. 20,
pp. 97–112. PMLR (2011)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
Blanchard, P., El Mhamdi, E.M., Guerraoui, R., Stainer, J.: Machine learning
with adversaries: Byzantine tolerant gradient descent.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems, vol. 30 (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S.,
Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for
privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, CCS ’17, p. 1175–1191 (2017)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Bouacida, N., Mohapatra, P.: Vulnerabilities in federated learning.

</span>
<span class="ltx_bibblock">IEEE Access <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">9</span>, 63229–63249 (2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
Caldas, S., Konečný, J., McMahan, H.B., Talwalkar, A.: Expanding the
reach of federated learning by reducing client resource requirements (2018).

</span>
<span class="ltx_bibblock">ArXiv:1812.07210

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
Chen, Y., Luo, F., Li, T., Xiang, T., Liu, Z., Li, J.: A training-integrity
privacy-preserving federated learning scheme with trusted execution
environment.

</span>
<span class="ltx_bibblock">Information Sciences <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">522</span>, 69–79 (2020)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Chen, Y., Qin, X., Wang, J., Yu, C., Gao, W.: Fedhealth: A federated transfer
learning framework for wearable healthcare.

</span>
<span class="ltx_bibblock">IEEE Intelligent Systems <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">35</span>(4), 83–93 (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
Chin, T., Zhang, C., Marculescu, D.: Improving the adversarial robustness of
transfer learning via noisy feature distillation.

</span>
<span class="ltx_bibblock">CoRR (2020).

</span>
<span class="ltx_bibblock">ArXiv:2002.02998

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
Colbaugh, R., Glass, K.: Moving target defense for adaptive adversaries.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Intelligence and Security
Informatics, pp. 50–55 (2013)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
Damgård, I., Pastro, V., Smart, N., Zakarias, S.: Multiparty computation
from somewhat homomorphic encryption.

</span>
<span class="ltx_bibblock">In: Advances in Cryptology – CRYPTO, pp. 643–662 (2012)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
Duchi, J.C., Jordan, M.I., Wainwright, M.J.: Local privacy and statistical
minimax rates.

</span>
<span class="ltx_bibblock">In: 1st Annual Allerton Conference on Communication, Control, and
Computing (Allerton), pp. 1592–1592 (2013)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M.: Our data,
ourselves: Privacy via distributed noise generation.

</span>
<span class="ltx_bibblock">In: Advances in Cryptology (EUROCRYPT 2006), <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science</em>, vol. 4004, pp. 486–503. Springer Verlag (2006)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., Madry, A.: Exploring the
landscape of spatial robustness.

</span>
<span class="ltx_bibblock">In: K. Chaudhuri, R. Salakhutdinov (eds.) Proceedings of the 36th
International Conference on Machine Learning, <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine
Learning Research</em>, vol. 97, pp. 1802–1811. PMLR (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
Fang, M., Cao, X., Jia, J., Gong, N.: Local model poisoning attacks to
byzantine-robust federated learning.

</span>
<span class="ltx_bibblock">In: 29th USENIX Security Symposium (USENIX Security 20), pp.
1605–1622 (2020)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Farajzadeh-Zanjani, M., Hallaji, E., Razavi-Far, R., Saif, M.:
Generative-adversarial class-imbalance learning for classifying cyber-attacks
and faults - a cyber-physical power system.

</span>
<span class="ltx_bibblock">IEEE Transactions on Dependable and Secure Computing pp. 1–1 (2021).

</span>
<span class="ltx_bibblock">DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TDSC.2021.3118636</span>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
Farajzadeh-Zanjani, M., Hallaji, E., Razavi-Far, R., Saif, M.: Generative
adversarial dimensionality reduction for diagnosing faults and attacks in
cyber-physical systems.

</span>
<span class="ltx_bibblock">Neurocomputing <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">440</span>, 101–110 (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
Farajzadeh-Zanjani, M., Hallaji, E., Razavi-Far, R., Saif, M., Parvania, M.:
Adversarial semi-supervised learning for diagnosing faults and attacks in
power grids.

</span>
<span class="ltx_bibblock">IEEE Transactions on Smart Grid <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">12</span>(4), 3468–3478 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
Farajzadeh-Zanjani, M., Razavi-Far, R., Saif, M., Palade, V.: Generative
adversarial networks: A survey on training, variants, and applications.

</span>
<span class="ltx_bibblock">In: Generative Adversarial Learning: Architectures and Applications,
pp. 7–29. Springer International Publishing, Cham (2022)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Fredrikson, M., Jha, S., Ristenpart, T.: Model inversion attacks that exploit
confidence information and basic countermeasures.

</span>
<span class="ltx_bibblock">In: Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security, p. 1322–1333. Association for Computing Machinery
(2015)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Fredrikson, M., Lantz, E., Jha, S., Lin, S., Page, D., Ristenpart, T.: Privacy
in pharmacogenetics: An end-to-end case study of personalized warfarin
dosing.

</span>
<span class="ltx_bibblock">In: Proceedings of the 23rd USENIX Conference on Security Symposium,
p. 17–32 (2014)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Fu, S., Xie, C., Li, B., Chen, Q.: Attack-resistant federated learning with
residual-based reweighting.

</span>
<span class="ltx_bibblock">CoRR (2019).

</span>
<span class="ltx_bibblock">ArXiv:1912.11464

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
Fung, C., Yoon, C.J.M., Beschastnikh, I.: Mitigating sybils in federated
learning poisoning.

</span>
<span class="ltx_bibblock">CoRR (2018).

</span>
<span class="ltx_bibblock">ArXiv:1808.04866

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
Gao, D., Liu, Y., Huang, A., Ju, C., Yu, H., Yang, Q.: Privacy-preserving
heterogeneous federated transfer learning.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Big Data, pp. 2552–2559 (2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
Gentry, C.: Fully homomorphic encryption using ideal lattices.

</span>
<span class="ltx_bibblock">In: Proceedings of the Forty-First Annual ACM Symposium on Theory of
Computing, STOC ’09, p. 169–178. Association for Computing Machinery (2009)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Geyer, R.C., Klein, T., Nabi, M.: Differentially private federated learning:
A client level perspective <span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">abs/1712.07557</span> (2017).

</span>
<span class="ltx_bibblock">ArXiv:1712.07557

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
Goldwasser, S., Micali, S., Rackoff, C.: The knowledge complexity of
interactive proof systems.

</span>
<span class="ltx_bibblock">SIAM Journal on Computing <span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">18</span>(1), 186–208 (1989)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
Goryczka, S., Xiong, L.: A comprehensive comparison of multiparty secure
additions with differential privacy.

</span>
<span class="ltx_bibblock">IEEE Transactions on Dependable and Secure Computing <span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">14</span>(5),
463–477 (2017)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
Grama, M., Musat, M., Muñoz-González, L., Passerat-Palmbach,
J., Rueckert, D., Alansary, A.: Robust aggregation for adaptive privacy
preserving federated learning in healthcare (2020).

</span>
<span class="ltx_bibblock">ArXiv:2009.08294

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
Hallaji, E., Farajzadeh-Zanjani, M., Razavi-Far, R., Palade, V., Saif, M.:
Constrained generative adversarial learning for dimensionality reduction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering pp. 1–1 (2021).

</span>
<span class="ltx_bibblock">DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TKDE.2021.3126642</span>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
Hallaji, E., Razavi-Far, R., Saif, M.: DLIN: Deep ladder imputation network.

</span>
<span class="ltx_bibblock">IEEE Transactions on Cybernetics pp. 1–13 (2021).

</span>
<span class="ltx_bibblock">DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TCYB.2021.3054878</span>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(44)</span>
<span class="ltx_bibblock">
Hallaji, E., Razavi-Far, R., Saif, M.: Embedding time-series features into
generative adversarial networks for intrusion detection in internet of things
networks.

</span>
<span class="ltx_bibblock">In: Generative Adversarial Learning: Architectures and Applications,
pp. 169–183. Springer International Publishing, Cham (2022)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
Hardy, S., Henecka, W., Ivey-Law, H., Nock, R., Patrini, G., Smith, G.,
Thorne, B.: Private federated learning on vertically partitioned data via
entity resolution and additively homomorphic encryption (2017).

</span>
<span class="ltx_bibblock">ArXiv:1711.10677

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
Hassani, H., Hallaji, E., Razavi-Far, R., Saif, M.: Unsupervised concrete
feature selection based on mutual information for diagnosing faults and
cyber-attacks in power systems.

</span>
<span class="ltx_bibblock">Engineering Applications of Artificial Intelligence <span id="bib.bib46.1.1" class="ltx_text ltx_font_bold">100</span>,
104150 (2021)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
Hassani, H., Razavi-Far, R., Saif, M.: Real-time out-of-step prediction control
to prevent emerging blackouts in power systems: A reinforcement learning
approach.

</span>
<span class="ltx_bibblock">Applied Energy <span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">314</span>, 118861 (2022).

</span>
<span class="ltx_bibblock">DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.apenergy.2022.118861</span>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(48)</span>
<span class="ltx_bibblock">
Hitaj, B., Ateniese, G., Perez-Cruz, F.: Deep models under the gan: Information
leakage from collaborative deep learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, p. 603–618. Association for Computing Machinery
(2017)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(49)</span>
<span class="ltx_bibblock">
Jacobsen, J., Behrmann, J., Carlini, N., Tramèr, F., Papernot, N.:
Exploiting excessive invariance caused by norm-bounded adversarial robustness
(2019).

</span>
<span class="ltx_bibblock">ArXiv:1903.10484

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(50)</span>
<span class="ltx_bibblock">
Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., Li, B.:
Manipulating machine learning: Poisoning attacks and countermeasures for
regression learning.

</span>
<span class="ltx_bibblock">In: IEEE Symposium on Security and Privacy (SP), pp. 19–35 (2018)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(51)</span>
<span class="ltx_bibblock">
Jayaraman, B., Evans, D.: Evaluating differentially private machine learning in
practice.

</span>
<span class="ltx_bibblock">In: 28th USENIX Security Symposium, pp. 1895–1912 (2019)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(52)</span>
<span class="ltx_bibblock">
Jere, M.S., Farnan, T., Koushanfar, F.: A taxonomy of attacks on federated
learning.

</span>
<span class="ltx_bibblock">IEEE Security Privacy <span id="bib.bib52.1.1" class="ltx_text ltx_font_bold">19</span>(2), 20–28 (2021)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(53)</span>
<span class="ltx_bibblock">
Jia, J., Salem, A., Backes, M., Zhang, Y., Gong, N.Z.: Memguard: Defending
against black-box membership inference attacks via adversarial examples.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, p. 259–274 (2019)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)</span>
<span class="ltx_bibblock">
Kairouz, P., et al.: Advances and open problems in federated learning (2019).

</span>
<span class="ltx_bibblock">ArXiv:1912.04977

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(55)</span>
<span class="ltx_bibblock">
Kantarcioglu, M., Clifton, C.: Privacy-preserving distributed mining of
association rules on horizontally partitioned data.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering <span id="bib.bib55.1.1" class="ltx_text ltx_font_bold">16</span>(9),
1026–1037 (2004)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(56)</span>
<span class="ltx_bibblock">
Kim, M., Song, Y., Wang, S., Xia, Y., Jiang, X.: Secure logistic regression
based on homomorphic encryption: Design and evaluation.

</span>
<span class="ltx_bibblock">JMIR Med Inform <span id="bib.bib56.1.1" class="ltx_text ltx_font_bold">6</span>(2), e19 (2018)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(57)</span>
<span class="ltx_bibblock">
Lei, C., Zhang, H.Q., Jinglei, T., Zhang, Y.C., Liu, X.H.: Moving target
defense techniques: A survey.

</span>
<span class="ltx_bibblock">Security and Communication Networks <span id="bib.bib57.1.1" class="ltx_text ltx_font_bold">2018</span>, 1–25 (2018)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(58)</span>
<span class="ltx_bibblock">
Li, D., Wang, J.: Fedmd: Heterogenous federated learning via model distillation
(2019).

</span>
<span class="ltx_bibblock">ArXiv:1910.03581

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
Li, N., Qardaji, W., Su, D., Wu, Y., Yang, W.: Membership privacy: A unifying
framework for privacy definitions.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, p. 889–900. Association for Computing Machinery
(2013)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Li, S., Cheng, Y., Liu, Y., Wang, W., Chen, T.: Abnormal client behavior
detection in federated learning <span id="bib.bib60.1.1" class="ltx_text ltx_font_bold">abs/1910.09933</span> (2019).

</span>
<span class="ltx_bibblock">ArXiv:1910.09933

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
Li, S., Cheng, Y., Wang, W., Liu, Y., Chen, T.: Learning to detect malicious
clients for robust federated learning (2020).

</span>
<span class="ltx_bibblock">ArXiv:2002.00211

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
Li, T., Hu, S., Beirami, A., Smith, V.: Ditto: Fair and robust federated
learning through personalization.

</span>
<span class="ltx_bibblock">In: M. Meila, T. Zhang (eds.) Proceedings of the 38th International
Conference on Machine Learning, <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, vol. 139, pp. 6357–6368. PMLR (2021)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(63)</span>
<span class="ltx_bibblock">
Liu, K., Dolan-Gavitt, B., Garg, S.: Fine-pruning: Defending against
backdooring attacks on deep neural networks.

</span>
<span class="ltx_bibblock">In: M. Bailey, T. Holz, M. Stamatogiannakis, S. Ioannidis (eds.)
Research in Attacks, Intrusions, and Defenses, pp. 273–294. Springer
International Publishing, Cham (2018)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(64)</span>
<span class="ltx_bibblock">
Liu, Y., Kang, Y., Xing, C., Chen, T., Yang, Q.: A secure federated transfer
learning framework.

</span>
<span class="ltx_bibblock">IEEE Intelligent Systems <span id="bib.bib64.1.1" class="ltx_text ltx_font_bold">35</span>(4), 70–82 (2020)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
Liu, Y., Xie, Y., Srivastava, A.: Neural trojans.

</span>
<span class="ltx_bibblock">In: IEEE 35th International Conference on Computer Design, pp. 45–48
(2017)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
Lu, Y., Fan, L.: An efficient and robust aggregation algorithm for learning
federated cnn.

</span>
<span class="ltx_bibblock">In: Proceedings of the 3rd International Conference on Signal
Processing and Machine Learning, p. 1–7 (2020)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(67)</span>
<span class="ltx_bibblock">
Lyu, L., Li, Y., Nandakumar, K., Yu, J., Ma, X.: How to democratise and protect
ai: Fair and differentially private decentralised deep learning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Dependable and Secure Computing pp. 1–1 (2020)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(68)</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Ma, X., Sun, L., Zhao, J., Yang, Q., Yu, P.S.: Privacy and
robustness in federated learning: Attacks and defenses (2020).

</span>
<span class="ltx_bibblock">ArXiv:2012.06337

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(69)</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Yang, Q.: Threats to federated learning: A survey (2020).

</span>
<span class="ltx_bibblock">ArXiv:2003.02133

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(70)</span>
<span class="ltx_bibblock">
Ma, Y., Zhu, X., Hsu, J.: Data poisoning against differentially-private
learners: Attacks and defenses.

</span>
<span class="ltx_bibblock">In: Proceedings of the 28th International Joint Conference on
Artificial Intelligence, p. 4732–4738 (2019)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(71)</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., Arcas, B.A.y.:
Communication-Efficient Learning of Deep Networks from Decentralized Data.

</span>
<span class="ltx_bibblock">In: Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics, <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
vol. 54, pp. 1273–1282. PMLR (2017)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(72)</span>
<span class="ltx_bibblock">
Melis, L., Song, C., De Cristofaro, E., Shmatikov, V.: Exploiting unintended
feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In: EEE Symposium on Security and Privacy (SP), pp. 691–706 (2019)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(73)</span>
<span class="ltx_bibblock">
Mo, F., Haddadi, H., Katevas, K., Marin, E., Perino, D., Kourtellis, N.: Ppfl:
Privacy-preserving federated learning with trusted execution environments.

</span>
<span class="ltx_bibblock">In: Proceedings of the 19th Annual International Conference on Mobile
Systems, Applications, and Services, p. 94–108 (2021)

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(74)</span>
<span class="ltx_bibblock">
Mohassel, P., Zhang, Y.: Secureml: A system for scalable privacy-preserving
machine learning.

</span>
<span class="ltx_bibblock">In: IEEE Symposium on Security and Privacy, pp. 19–38 (2017)

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(75)</span>
<span class="ltx_bibblock">
Nasr, M., Shokri, R., Houmansadr, A.: Machine learning with membership privacy
using adversarial regularization.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, p. 634–646. Association for Computing Machinery
(2018)

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(76)</span>
<span class="ltx_bibblock">
Ogburn, M., Turner, C., Dahal, P.: Homomorphic encryption.

</span>
<span class="ltx_bibblock">Procedia Computer Science <span id="bib.bib76.1.1" class="ltx_text ltx_font_bold">20</span>, 502–509 (2013).

</span>
<span class="ltx_bibblock">Complex Adaptive Systems

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(77)</span>
<span class="ltx_bibblock">
Paillier, P.: Public-key cryptosystems based on composite degree residuosity
classes.

</span>
<span class="ltx_bibblock">In: J. Stern (ed.) Advances in Cryptology — EUROCRYPT ’99, pp.
223–238. Springer Berlin Heidelberg (1999)

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(78)</span>
<span class="ltx_bibblock">
Pan, S.J., Yang, Q.: A survey on transfer learning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering <span id="bib.bib78.1.1" class="ltx_text ltx_font_bold">22</span>(10),
1345–1359 (2010)

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(79)</span>
<span class="ltx_bibblock">
Papernot, N., Abadi, M., Úlfar Erlingsson, Goodfellow, I., Talwar, K.:
Semi-supervised knowledge transfer for deep learning from private training
data (2017).

</span>
<span class="ltx_bibblock">ArXiv:1610.05755

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(80)</span>
<span class="ltx_bibblock">
Papernot, N., Song, S., Mironov, I., Raghunathan, A., Talwar, K., Úlfar
Erlingsson: Scalable private learning with pate (2018).

</span>
<span class="ltx_bibblock">ArXiv:1802.08908

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(81)</span>
<span class="ltx_bibblock">
Parno, B., Howell, J., Gentry, C., Raykova, M.: Pinocchio: Nearly practical
verifiable computation.

</span>
<span class="ltx_bibblock">In: EEE Symposium on Security and Privacy, pp. 238–252 (2013)

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(82)</span>
<span class="ltx_bibblock">
Phong, L.T., Aono, Y., Hayashi, T., Wang, L., Moriai, S.: Privacy-preserving
deep learning via additively homomorphic encryption.

</span>
<span class="ltx_bibblock">IEEE Transactions on Information Forensics and Security
<span id="bib.bib82.1.1" class="ltx_text ltx_font_bold">13</span>(5), 1333–1345 (2018)

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(83)</span>
<span class="ltx_bibblock">
Pillutla, K., Kakade, S.M., Harchaoui, Z.: Robust aggregation for federated
learning (2019).

</span>
<span class="ltx_bibblock">ArXiv:1912.13445

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(84)</span>
<span class="ltx_bibblock">
Preuveneers, D., Rimmer, V., Tsingenopoulos, I., Spooren, J., Joosen, W.,
Ilie-Zudor, E.: Chained anomaly detection models for federated learning: An
intrusion detection case study.

</span>
<span class="ltx_bibblock">Applied Sciences <span id="bib.bib84.1.1" class="ltx_text ltx_font_bold">8</span>(12) (2018)

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(85)</span>
<span class="ltx_bibblock">
Rastogi, V., Nath, S.: Differentially private aggregation of distributed
time-series with transformation and encryption.

</span>
<span class="ltx_bibblock">In: Proceedings of the CM SIGMOD International Conference on
Management of Data, p. 735–746. Association for Computing Machinery (2010)

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(86)</span>
<span class="ltx_bibblock">
Razavi-Far, R., Farajzadeh-Zanajni, M., Wang, B., Saif, M., Chakrabarti, S.:
Imputation-based ensemble techniques for class imbalance learning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering <span id="bib.bib86.1.1" class="ltx_text ltx_font_bold">33</span>(5),
1988–2001 (2021)

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(87)</span>
<span class="ltx_bibblock">
Razavi-Far, R., Ruiz-Garcia, A., Palade, V., Schmidhuber, J. (eds.): Generative
Adversarial Learning: Architectures and Applications.

</span>
<span class="ltx_bibblock">Springer, Cham (2022)

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(88)</span>
<span class="ltx_bibblock">
Razavi-Far, R., Wan, D., Saif, M., Mozafari, N.: To tolerate or to impute
missing values in v2x communications data?

</span>
<span class="ltx_bibblock">IEEE Internet of Things Journal pp. 1–1 (2021).

</span>
<span class="ltx_bibblock">DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/JIOT.2021.3126749</span>

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(89)</span>
<span class="ltx_bibblock">
Rezaei, S., Liu, X.: A target-agnostic attack on deep models: Exploiting
security vulnerabilities of transfer learning (2020).

</span>
<span class="ltx_bibblock">ArXiv:1904.04334

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(90)</span>
<span class="ltx_bibblock">
Riazi, M.S., Weinert, C., Tkachenko, O., Songhori, E.M., Schneider, T.,
Koushanfar, F.: Chameleon: A hybrid secure computation framework for machine
learning applications.

</span>
<span class="ltx_bibblock">In: Proceedings of the Asia Conference on Computer and Communications
Security, ASIACCS ’18, p. 707–721 (2018)

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(91)</span>
<span class="ltx_bibblock">
Rivest, R.L., Shamir, A., Adleman, L.: A method for obtaining digital
signatures and public-key cryptosystems.

</span>
<span class="ltx_bibblock">Commun. ACM <span id="bib.bib91.1.1" class="ltx_text ltx_font_bold">21</span>(2), 120–126 (1978)

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(92)</span>
<span class="ltx_bibblock">
Saha, S., Ahmad, T.: Federated transfer learning: concept and applications
(2020).

</span>
<span class="ltx_bibblock">ArXiv:2010.15561

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(93)</span>
<span class="ltx_bibblock">
Saha, S., Bovolo, F., Bruzzone, L.: Unsupervised deep change vector analysis
for multiple-change detection in vhr images.

</span>
<span class="ltx_bibblock">IEEE Transactions on Geoscience and Remote Sensing <span id="bib.bib93.1.1" class="ltx_text ltx_font_bold">57</span>(6),
3677–3693 (2019)

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(94)</span>
<span class="ltx_bibblock">
Sahu, A.K., Li, T., Sanjabi, M., Zaheer, M., Talwalkar, A., Smith, V.: On the
convergence of federated optimization in heterogeneous networks (2018).

</span>
<span class="ltx_bibblock">ArXiv:1812.06127

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(95)</span>
<span class="ltx_bibblock">
Salem, A., Zhang, Y., Humbert, M., Fritz, M., Backes, M.: Ml-leaks: Model and
data independent membership inference attacks and defenses on machine
learning models (2018).

</span>
<span class="ltx_bibblock">ArXiv:1806.01246

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(96)</span>
<span class="ltx_bibblock">
Sengupta, S., Chowdhary, A., Sabur, A., Alshamrani, A., Huang, D., Kambhampati,
S.: A survey of moving target defenses for network security.

</span>
<span class="ltx_bibblock">IEEE Communications Surveys Tutorials <span id="bib.bib96.1.1" class="ltx_text ltx_font_bold">22</span>(3), 1909–1941
(2020)

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(97)</span>
<span class="ltx_bibblock">
Shafahi, A., Najibi, M., Ghiasi, M.A., Xu, Z., Dickerson, J., Studer, C.,
Davis, L.S., Taylor, G., Goldstein, T.: Adversarial training for free!

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems, vol. 32 (2019)

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(98)</span>
<span class="ltx_bibblock">
Sharma, S., Xing, C., Liu, Y., Kang, Y.: Secure and efficient federated
transfer learning.

</span>
<span class="ltx_bibblock">In: IEEE International Conference on Big Data (Big Data), pp.
2569–2576 (2019)

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(99)</span>
<span class="ltx_bibblock">
Shen, S., Tople, S., Saxena, P.: A¡span class=”smallcaps
smallercapital”¿uror¡/span¿: Defending against poisoning attacks in
collaborative deep learning systems.

</span>
<span class="ltx_bibblock">In: Proceedings of the 32nd Annual Conference on Computer Security
Applications, ACSAC ’16, p. 508–519 (2016)

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(100)</span>
<span class="ltx_bibblock">
Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks
against machine learning models.

</span>
<span class="ltx_bibblock">In: IEEE Symposium on Security and Privacy, pp. 3–18 (2017)

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(101)</span>
<span class="ltx_bibblock">
Smith, V., Chiang, C.K., Sanjabi, M., Talwalkar, A.S.: Federated multi-task
learning.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems, vol. 30 (2017)

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(102)</span>
<span class="ltx_bibblock">
Sun, L., Lyu, L.: Federated model distillation with noise-free differential
privacy (2020).

</span>
<span class="ltx_bibblock">ArXiv:2009.05537

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(103)</span>
<span class="ltx_bibblock">
Tramer, F., Boneh, D.: Adversarial training and robustness for multiple
perturbations.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems, vol. 32 (2019)

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(104)</span>
<span class="ltx_bibblock">
Vaidya, J., Clifton, C.: Privacy preserving association rule mining in
vertically partitioned data.

</span>
<span class="ltx_bibblock">In: Proceedings of the Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, p. 639–644 (2002)

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(105)</span>
<span class="ltx_bibblock">
Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., Zhao, B.Y.:
Neural cleanse: Identifying and mitigating backdoor attacks in neural
networks.

</span>
<span class="ltx_bibblock">In: IEEE Symposium on Security and Privacy, pp. 707–723 (2019)

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(106)</span>
<span class="ltx_bibblock">
Wang, B., Yao, Y., Viswanath, B., Zheng, H., Zhao, B.Y.: With great training
comes great vulnerability: Practical attacks against transfer learning.

</span>
<span class="ltx_bibblock">In: Proceedings of the 27th USENIX Conference on Security Symposium,
SEC’18, p. 1281–1297. USENIX Association (2018)

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(107)</span>
<span class="ltx_bibblock">
Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn, J.,
Lee, K., Papailiopoulos, D.S.: Attack of the tails: Yes, you really can
backdoor federated learning (2020).

</span>
<span class="ltx_bibblock">ArXiv:2007.05084

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(108)</span>
<span class="ltx_bibblock">
Wang, S., Nepal, S., Rudolph, C., Grobler, M., Chen, S., Chen, T.: Backdoor
attacks against transfer learning with pre-trained deep learning models.

</span>
<span class="ltx_bibblock">IEEE Transactions on Services Computing pp. 1–1 (2020)

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(109)</span>
<span class="ltx_bibblock">
Xie, C., Huang, K., Chen, P.Y., Li, B.: Dba: Distributed backdoor attacks
against federated learning.

</span>
<span class="ltx_bibblock">In: International Conference on Learning Representations (2020)

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(110)</span>
<span class="ltx_bibblock">
Xie, C., Wu, Y., van der Maaten, L., Yuille, A.L., He, K.: Feature denoising
for improving adversarial robustness (2018).

</span>
<span class="ltx_bibblock">ArXiv:1812.03411

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(111)</span>
<span class="ltx_bibblock">
Xu, J., Guo, P., Zhao, M., Erbacher, R.F., Zhu, M., Liu, P.: Comparing
different moving target defense techniques.

</span>
<span class="ltx_bibblock">In: Proceedings of the First ACM Workshop on Moving Target Defense,
MTD ’14, p. 97–107. Association for Computing Machinery (2014)

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(112)</span>
<span class="ltx_bibblock">
Xu, X., Lyu, L.: A reputation mechanism is all you need: Collaborative fairness
and adversarial robustness in federated learning (2021).

</span>
<span class="ltx_bibblock">ArXiv:2011.10464

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(113)</span>
<span class="ltx_bibblock">
Yang, H., He, H., Zhang, W., Cao, X.: Fedsteg: A federated transfer learning
framework for secure image steganalysis.

</span>
<span class="ltx_bibblock">IEEE Transactions on Network Science and Engineering <span id="bib.bib113.1.1" class="ltx_text ltx_font_bold">8</span>(2),
1084–1094 (2021)

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(114)</span>
<span class="ltx_bibblock">
Yang, Q., Liu, Y., Chen, T., Tong, Y.: Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology <span id="bib.bib114.1.1" class="ltx_text ltx_font_bold">10</span>(2)
(2019)

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(115)</span>
<span class="ltx_bibblock">
Yao, A.C.: Protocols for secure computations.

</span>
<span class="ltx_bibblock">In: 23rd Annual Symposium on Foundations of Computer Science, pp.
160–164 (1982)

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(116)</span>
<span class="ltx_bibblock">
Yao, Y., Li, H., Zheng, H., Zhao, B.Y.: Latent backdoor attacks on deep neural
networks.

</span>
<span class="ltx_bibblock">In: Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security, CCS ’19, p. 2041–2055. Association for Computing
Machinery, New York, NY, USA (2019)

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(117)</span>
<span class="ltx_bibblock">
Yeom, S., Giacomelli, I., Fredrikson, M., Jha, S.: Privacy risk in machine
learning: Analyzing the connection to overfitting (2018).

</span>
<span class="ltx_bibblock">ArXiv:1709.01604

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(118)</span>
<span class="ltx_bibblock">
Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients.

</span>
<span class="ltx_bibblock">In: Advances in Neural Information Processing Systems, vol. 32 (2019)

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(119)</span>
<span class="ltx_bibblock">
Zou, Y., Zhang, Z., Backes, M., Zhang, Y.: Privacy analysis of deep learning in
the wild: Membership inference attacks against transfer learning (2020).

</span>
<span class="ltx_bibblock">ArXiv:2009.04872

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.02336" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.02337" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.02337">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.02337" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.02338" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 12:52:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
