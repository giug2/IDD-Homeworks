<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>EMMeTT: Efficient Multimodal Machine Translation Training</title>
<!--Generated on Fri Sep 20 13:58:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
machine translation,  speech translation,  LLM,  foundation models,  multimodal,  GPT,  T5
" lang="en" name="keywords"/>
<base href="/html/2409.13523v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S1" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S2" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S3" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S4" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S5" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S6" title="In EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">EMMeTT: Efficient Multimodal Machine Translation Training</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Piotr ≈ªelasko<sup class="ltx_sup" id="id1.1.id1">1</sup>, Zhehuai Chen<sup class="ltx_sup" id="id2.2.id2">1</sup>, Mengru Wang, Daniel Galvez, Oleksii Hrinchuk, 
<br class="ltx_break"/>Shuoyang Ding, Ke Hu, Jagadeesh Balam, Vitaly Lavrukhin, Boris Ginsburg
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
NVIDIA 
<br class="ltx_break"/>{pzelasko,zhehuaic}@nvidia.com

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">A rising interest in the modality extension of foundation language models warrants discussion on the most effective, and efficient, multimodal training approach.
This work focuses on neural machine translation (NMT) and proposes a joint multimodal training regime of Speech-LLM to include automatic speech translation (AST).
We investigate two different foundation model architectures, decoder-only GPT and encoder-decoder T5, extended with Canary-1B‚Äôs speech encoder.
To handle joint multimodal training, we propose a novel training framework called EMMeTT.
EMMeTT improves training efficiency with the following: balanced sampling across languages, datasets, and modalities; efficient sequential data iteration; and a novel 2D bucketing scheme for multimodal data, complemented by a batch size optimizer (OOMptimizer).
We show that a multimodal training consistently helps with both architectures. Moreover, SALM-T5 trained with EMMeTT retains the original NMT capability while outperforming AST baselines on four-language subsets of FLORES and FLEURS. The resultant <span class="ltx_text ltx_font_italic" id="id3.id1.1">Multimodal Translation Model</span> produces strong text and speech translation results at the same time.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
machine translation, speech translation, LLM, foundation models, multimodal, GPT, T5

</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The ubiquitous availability and impressive capabilities of large language models (LLM) motivate our community to extend them to other modalities, and in the context of this work, audio modality in particular.
There have been several recent works proposing different approaches to training a Speech-LLM: Speech-LLaMA and variants¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib3" title="">3</a>]</cite> directly concatenate the speech prompts with text prompts and use the combined sequence as input to the LLM like a decoder-only GPT model. Flamingo and its extension¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib6" title="">6</a>]</cite> are another branch of works, where
cross-modal cross-attention is added to the pretrained GPT-based LLM with shared text query.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The central focus of this work is the study of multimodal training approach itself.
Firstly, the extension to audio modality typically causes catastrophic forgetting of the text domain capabilities. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib2" title="">2</a>]</cite> analyzes the task over-fitting problem in Speech-LLM and proposes an activation tuning stage to alleviate this problem.
The task over-fitting limits the usefulness of the original LLM exclusively to the tasks seen in audio domain training, which may be viewed as limiting of the model‚Äôs generalization, zero-shot, and in-context learning capabilities.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One possible solution to catastrophic forgetting is to train the model on data from multiple modalities jointly.
Hence, the second point of interest is how to perform joint multimodal training effectively and efficiently.
Since the scope of such problem is very broad, we focus on a well-defined domain of neural machine translation (NMT) and demonstrate a solution that allows a model to retain text NMT capabilities while acquiring automatic speech translation (AST) skills.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our main contribution is a joint multimodal training framework named EMMeTT that allowed us to train an NMT model with AST and NMT data jointly, resulting in a single model capable of performing both NMT and AST.
The resulting model matches the single-modal baseline NMT performance while outperforming AST baselines on English, French, German, and Spanish subset of FLEURS.
To enable this, we design data sampling strategies from the first principles to ensure a stationary data distribution across modalities, languages, and datasets throughout the training.
Furthermore, we stratify the sampling with dynamic bucketing and a novel 2D bucketing technique for maximal training efficiency, complemented by a batch size optimizer (OOMptimizer) algorithm that pushes GPU memory utilization to its limits across various sequence lengths, while preventing spurious out-of-memory (OOM) errors.
Finally, we demonstrate EMMeTT‚Äôs flexibility by training two different multimodal model architectures: BESTOW-GPT and SALM-T5 and producing strong results.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Speech and text translation.</span>
Recent works leveraging NMT data to complement AST systems can be broken down into two paradigms: pretrain-finetune and joint training.
Most works fall under pretrain-finetune category, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib7" title="">7</a>]</cite> jointly pretrains speech and text in an encoder-decoder model for ASR and AST with self-supervised and supervised losses.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib8" title="">8</a>]</cite> jointly learns a unified speech and text representation, which is later used to initialize an AST system.
Recent LLM-based systems usually connect a pretrained speech encoder with an NMT pretrained LLM. After finetuning with AST data, the systems can leverage speech and text pretrained knowledge from the encoder and decoder respectively and have shown strong performances, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib10" title="">10</a>]</cite>.
As for joint training, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib11" title="">11</a>]</cite> conducted NMT and AST joint finetuning on the pretrained encoder and decoder with a learned upsampling text encoder.
An ablation study in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib12" title="">12</a>]</cite> clearly shows the benefit of joint training in the last stage compared to speech-only finetune.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Foundation models for translation.</span> Large foundation models have been successfully adapted for NMT tasks across diverse language pairs. Raffel et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib13" title="">13</a>]</cite> introduced the T5 architecture, demonstrating its versatility in various text-to-text applications, including NMT. Brown et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib14" title="">14</a>]</cite> presented GPT-3, a decoder-only LLM capable of few-shot learning in translation tasks, showcasing the potential of general-purpose language models in NMT. Advancing the field further, Liu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib15" title="">15</a>]</cite> extended BART¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib16" title="">16</a>]</cite> to multilingual settings, showing its capacity to handle translation across multiple languages with minimal fine-tuning. As the scope of NMT expanded, researchers began exploring multimodal extensions of these models, integrating additional modalities such as speech. Prior works, such as Speech Augmented Language Models (SALM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib1" title="">1</a>]</cite> and BESTOW¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib6" title="">6</a>]</cite> demonstrated the potential for cross-modal learning, where text pretrained LLMs could be adapted to transfer knowledge acquired from massive text-based language resources to other modalities.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Models.</span>
We apply multimodal translation design on two types of Speech-LLM architectures, SALM-T5 and BESTOW-GPT.
SALM model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib1" title="">1</a>]</cite> is an open-source speech-language model proficient in speech recognition, word boosting, and translations. This model is designed to leverage a pre-trained and instruction-fine-tuned LLM by conditioning it on paired speech and text prompts to generate textual outputs for various speech tasks. We follow this architecture but replace the backbone LLM to a T5 architecture based machine translation pretrained model. The resultant architecture is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S3.F1" title="Figure 1 ‚Ä£ III Methods ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">1</span></a>(a) and denoted as SALM-T5.
BESTOW is a cross-attention variant of SALM proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib6" title="">6</a>]</cite>. Unlike SALM, which prepends speech prompts to text prompts as LLM inputs, BESTOW applies cross-attention between speech prompts and the original textual LLM inputs at every step before feeding them into the LLM. In this cross-attention mechanism, the query consists of the original LLM inputs, and the key and value are the speech prompts, similar to the LAS model design for end-to-end ASR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib17" title="">17</a>]</cite>. We follow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib6" title="">6</a>]</cite> and use TinyLlama as the LLM backbone. The resultant architecture is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S3.F1" title="Figure 1 ‚Ä£ III Methods ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">1</span></a>(b), denoted as BESTOW-GPT.
We introduce text translation task to the Speech-LLM training by bypassing the speech encoder and cross-attention transformer and only backpropagating the LLM backbone, as demonstrated in the figure.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="211" id="S3.F1.g1" src="extracted/5868500/figs/arch.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) SALM-T5 architecture. (b) BESTOW-GPT architecture. The proposed multimodal joint training is visualized on both architectures where green shade denotes speech translation training and blue dotted shade denotes text translation training. </figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Stationary data blending with a stochastic weighted multiplexer.</span>
The main objective for data blending is to ensure a stationary distribution of data sources in mini-batches throughout each training step. This ensures there are no local domain shifts throughout training, effectively stabilizing training and keeping model‚Äôs performance balanced the training domains, languages, and tasks.
With large training data random access sampling and blending during preprocessing are not viable; for the sake of brevity, we refer the reader to related discussion in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib18" title="">18</a>]</cite>.
Instead, we treat each individual dataset as a single stream read sequentially with some weight proportional to sampling likelihood.
To blend data, we interleave the input streams into a single output stream using a stochastic weighted multiplexer (MUX).
At each iteration step, MUX randomly selects an input stream to yield example from according to a multinomial distribution defined by the input stream weights (similar to ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib18" title="">18</a>]</cite>).
Note that with MUX, all input streams (datasets) are distributed uniformly throughout the training (i.e., stationary data source distribution).
We start with natural weights that are proportional to dataset size.
To allow upsampling and downsampling, we make each input stream infinite: once exhausted we iterate it again, re-shuffling underlying dataset shards for better randomization.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">Sampling stratification on sequence length.</span>
To maximize the training efficiency, we use dynamic bucketing from Lhotse¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib19" title="">19</a>]</cite>. It is a sampling strategy that groups utterances of similar sequence length in the same mini-batches, minimizing the necessary padding<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
We acknowledge that stratification by sequence length may degrade the stationarity of data source distribution discussed in an earlier section, as datasets may have different sequence length distributions, leading to non-uniform data source representation in some buckets.
</span></span></span>. For GPT-based models, source and target text is usually collapsed together into a single sequence with context and answer, making it possible to use regular bucketing effectively. However, with encoder-decoder text and multimodal models, source and target lengths need to be considered separately. We found that stratifying only on source length leads to about 40% padding on decoder‚Äôs sequence length dimension. To address that issue, we propose a 2D bucketing strategy: we allocate a training example to a bucket based on the input sequence length, and then choose a sub-bucket based on the output sequence length. The 2D bucket bins are estimated based on a 500k sample of the data so that each bucket (and sub-bucket) contains approximately the same total number of seconds or tokens. This helps sample from buckets more uniformly since longer sequence buckets have smaller batch sizes.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Batch size OOMptimizer.</span>
With bucketing, each bucket requires a different batch size (inversely proportional to the sequence length) to use hardware efficiently. With 2D bucketing and 10x10 buckets, batch size calibration becomes tedious and inaccurate. We propose an automatic search algorithm to determine an optimal batch size profile for model, data, and hardware combination. The algorithm is a variant of binary search: we start with an initial batch size setting and run a full training step of the model (forward pass, backward pass, and parameter update) with random inputs to get an accurate estimate of the GPU memory utilization during training. If an out of memory (OOM) error is raised, we mark the batch size as invalid and halve it. If the training step succeeds, we mark the batch size as valid and double it. Once we have determined at least one of each (valid and invalid) batch sizes, we explore the middle point settings recursively until convergence. We terminate the search when the found valid batch size is within 5% of invalid batch size.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Joint multimodal training: combining modalities with round robin and zip samplers.</span>
The previous paragraphs described the process of sampling a stratified mini-batch for a single modality.
We propose to sample a mini-batch for each modality separately, and combine them. We found two approaches to be effective: a randomized round robin sampler, which randomly chooses a singe modality at each training step; and a zip sampler, which combines two single-modal mini-batches into one, resulting in multimodal gradient accumulation. Our preliminary experiments demonstrated only minor differences, likely due to momentum-based optimizer enabling both modalities to participate in parameter updates for round robin approach. All multimodal experiments reported in this work use the round robin approach.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental setup</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">At a high level, we demonstrate the effectiveness of our methods as follows. First, we score the baseline NMT and AST models on standard benchmarks. Then, we extend the NMT baselines to audio modality either through audio-only or multimodal training, and check how well they score on both modalities after the procedure. We use 128 NVIDIA A100 80GB GPUs for each experiment.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Speech data</span>. We follow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib20" title="">20</a>]</cite> to include 31k hours of public data speech and 54k hours of extra in-house data for speech recognition (ASR), including English (67.4k hours), German (6.1k hours), Spanish (6.6k hours), and French (5.1k hours).
We obtain AST data by generating synthetic labels for ASR data using NMT models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib22" title="">22</a>]</cite> in all 4 languages.
Further, a 4.8k hours pseudo-labeled English <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" stretchy="false" xref="S4.p2.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">‚Üí</annotation></semantics></math> German translation dataset from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib23" title="">23</a>]</cite> was also used.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Text translation data</span>. The training set comprises public datasets<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>CC Aligned, CC Matrix, Paracrawl, WikiMatrix, EuroPat, Europarl, WMT CommonCrawl, UNPC, News Commentary, EESC, Rapid, EMEA, Covost, WikiTitles, and NLLB.</span></span></span> and synthetic data. For synthetic data generation, we performed back-translation with¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib22" title="">22</a>]</cite> for all language pairs using monolingual data from CulturaX¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib24" title="">24</a>]</cite>. To ensure accurate autoregressive modeling, we used synthetic translations as inputs and real text as targets.
For English-centric directions, the synthetic data make up 39% of the total. Synthetic datasets are fully utilized for directions where a large parallel corpus is unavailable. For preprocessing, we first performed language identification. We then filtered out data entries that either fell outside the length range of 1 to 256 tokens or where the target sentence was longer than 1.3 times the length of the source sentence. Next, we applied Bicleaner¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib25" title="">25</a>]</cite> to remove sentence pairs with a translation likelihood score below 0.5. These preprocessing steps resulted in a training set of 2.7 TB across 33 languages.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.3"><span class="ltx_text ltx_font_bold" id="S4.p4.3.1">Speech-LLM setup</span>.
For experiments we use PyTorch and NVIDIA NeMo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib26" title="">26</a>]</cite>. The speech encoders of SALM-T5 and BESTOW-GPT are initialized from the Canary-1B model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib20" title="">20</a>]</cite>. SALM-T5 initializes the LLM backbone from T5 NMT model below and directly feeds pretrained speech encoder outputs to LLM without modality adapter layers.
Following¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib6" title="">6</a>]</cite>, BESTOW-GPT initializes from TinyLlama-1.1B-chat model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib27" title="">27</a>]</cite> and uses a 2-layer cross-attention Transformer module.
We train all parameters in both models, which are both about 1.8 billion. We use distributed fused Adam¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib28" title="">28</a>]</cite> optimizer and cosine annealing, with learning rate <math alttext="1e-4" class="ltx_Math" display="inline" id="S4.p4.1.m1.1"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mrow id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml"><mn id="S4.p4.1.m1.1.1.2.2" xref="S4.p4.1.m1.1.1.2.2.cmml">1</mn><mo id="S4.p4.1.m1.1.1.2.1" xref="S4.p4.1.m1.1.1.2.1.cmml">‚Å¢</mo><mi id="S4.p4.1.m1.1.1.2.3" xref="S4.p4.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">‚àí</mo><mn id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><minus id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></minus><apply id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2"><times id="S4.p4.1.m1.1.1.2.1.cmml" xref="S4.p4.1.m1.1.1.2.1"></times><cn id="S4.p4.1.m1.1.1.2.2.cmml" type="integer" xref="S4.p4.1.m1.1.1.2.2">1</cn><ci id="S4.p4.1.m1.1.1.2.3.cmml" xref="S4.p4.1.m1.1.1.2.3">ùëí</ci></apply><cn id="S4.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.p4.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.1d">1 italic_e - 4</annotation></semantics></math> and weight decay of <math alttext="1e-3" class="ltx_Math" display="inline" id="S4.p4.2.m2.1"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mrow id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml"><mn id="S4.p4.2.m2.1.1.2.2" xref="S4.p4.2.m2.1.1.2.2.cmml">1</mn><mo id="S4.p4.2.m2.1.1.2.1" xref="S4.p4.2.m2.1.1.2.1.cmml">‚Å¢</mo><mi id="S4.p4.2.m2.1.1.2.3" xref="S4.p4.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">‚àí</mo><mn id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><minus id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1"></minus><apply id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2"><times id="S4.p4.2.m2.1.1.2.1.cmml" xref="S4.p4.2.m2.1.1.2.1"></times><cn id="S4.p4.2.m2.1.1.2.2.cmml" type="integer" xref="S4.p4.2.m2.1.1.2.2">1</cn><ci id="S4.p4.2.m2.1.1.2.3.cmml" xref="S4.p4.2.m2.1.1.2.3">ùëí</ci></apply><cn id="S4.p4.2.m2.1.1.3.cmml" type="integer" xref="S4.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">1e-3</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.1d">1 italic_e - 3</annotation></semantics></math>. Gradient clipping of <math alttext="1.0" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mn id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><cn id="S4.p4.3.m3.1.1.cmml" type="float" xref="S4.p4.3.m3.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">1.0</annotation></semantics></math> is applied. The data sampler uses 10x10 2D bucketing configuration, natural weights for data blending, and round robin modality combination with equal selection probabilities.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">NMT LLM setup</span>. For our T5 NMT baseline, we adopted a multilingual NVIDIA Riva Megatron NMT model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib29" title="">29</a>]</cite> trained for general purpose translation. The text translation model follows the T5 architecture with the following modifications. It comprises 12 encoder layers and 24 decoder layers with 20 attention heads. The hidden dimension is 1280, and the feedforward layer has a dimension of 5120. We use a SentencePiece tokenizer with a vocabulary size of 64k, shared across the source and target languages. RMSNorm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib30" title="">30</a>]</cite> is employed for normalization across layers. For optimization we leveraged fused Adam and an inverse Square Root Annealing learning rate schedule, starting with an initial learning rate of <math alttext="4e-4" class="ltx_Math" display="inline" id="S4.p5.1.m1.1"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mrow id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml"><mn id="S4.p5.1.m1.1.1.2.2" xref="S4.p5.1.m1.1.1.2.2.cmml">4</mn><mo id="S4.p5.1.m1.1.1.2.1" xref="S4.p5.1.m1.1.1.2.1.cmml">‚Å¢</mo><mi id="S4.p5.1.m1.1.1.2.3" xref="S4.p5.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">‚àí</mo><mn id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><minus id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1"></minus><apply id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2"><times id="S4.p5.1.m1.1.1.2.1.cmml" xref="S4.p5.1.m1.1.1.2.1"></times><cn id="S4.p5.1.m1.1.1.2.2.cmml" type="integer" xref="S4.p5.1.m1.1.1.2.2">4</cn><ci id="S4.p5.1.m1.1.1.2.3.cmml" xref="S4.p5.1.m1.1.1.2.3">ùëí</ci></apply><cn id="S4.p5.1.m1.1.1.3.cmml" type="integer" xref="S4.p5.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">4e-4</annotation><annotation encoding="application/x-llamapun" id="S4.p5.1.m1.1d">4 italic_e - 4</annotation></semantics></math>. Gradient clipping was applied at a threshold of 1.0 to stabilize training. Warmup steps were configured to 0.8% of 3.6 million maximum steps.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Evaluation</span>.
We evaluated AST performance on the public benchmarks FLEURS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib31" title="">31</a>]</cite>.
The text translation NMT model is evaluated on FLORES-101¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib32" title="">32</a>]</cite> benchmark.
We limit the evaluation to 6 pairs in 4 languages present in our AST data.
Decoding is performed using greedy-search, and we report BLEU score using SacreBLEU tool¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results</span>
</h2>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Speech translation SacreBLEU scores on a subset of FLEURS. The baselines are SeamlessM4T-v2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib34" title="">34</a>]</cite>, Canary-1B AST¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#bib.bib20" title="">20</a>]</cite>, and Canary-1B ASR cascaded with pretrained T5 NMT. BESTOW-GPT and SALM-T5 are TinyLlama and T5 models finetuned exclusively on speech. Multimodal indicates joint finetuning on speech and text. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.2" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T1.1.1.1">En <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">‚Üí</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.1.3">De</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.1.4">Es</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.1.5">Fr</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.1.6" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.6.1">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.2">
<td class="ltx_td ltx_align_left" id="S5.T1.2.2.2">De</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.2.3">Es</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.2.4">Fr</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S5.T1.2.2.1">
<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T1.2.2.1.m1.1"><semantics id="S5.T1.2.2.1.m1.1a"><mo id="S5.T1.2.2.1.m1.1.1" stretchy="false" xref="S5.T1.2.2.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b"><ci id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.1.m1.1d">‚Üí</annotation></semantics></math> En</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.2.3.1.1"><span class="ltx_text ltx_font_italic" id="S5.T1.2.3.1.1.1">Baselines</span></td>
<td class="ltx_td ltx_border_t" id="S5.T1.2.3.1.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T1.2.3.1.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.2.3.1.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T1.2.3.1.5"></td>
<td class="ltx_td ltx_border_t" id="S5.T1.2.3.1.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.2.3.1.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.2.3.1.8"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T1.2.4.2.1">SeamlessM4T-v2</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.4.2.2">33.2</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.4.2.3">23.7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.4.2.4">43.0</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.4.2.5">37.1</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.4.2.6">25.4</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.4.2.7">30.9</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.4.2.8">32.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T1.2.5.3.1">Canary-1B</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.5.3.2">32.1</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.5.3.3">22.7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.5.3.4">40.8</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.5.3.5">34.0</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.5.3.6">21.8</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.5.3.7">31.0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.5.3.8">30.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T1.2.6.4.1">¬†¬†<span class="ltx_text ltx_font_italic" id="S5.T1.2.6.4.1.1">‚ÄÉ+T5 cascade</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.6.4.2">32.6</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.6.4.3">23.9</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.6.4.4">42.6</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.6.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.2.6.4.5.1">39.4</span></td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.6.4.6">27.3</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.6.4.7"><span class="ltx_text ltx_font_bold" id="S5.T1.2.6.4.7.1">37.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.6.4.8">34.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.2.7.5.1">BESTOW-GPT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.7.5.2">32.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.7.5.3">23.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.7.5.4">41.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.7.5.5">35.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.7.5.6">23.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.7.5.7">35.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.7.5.8">32.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.8.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T1.2.8.6.1">¬†¬†<span class="ltx_text ltx_font_italic" id="S5.T1.2.8.6.1.1">‚ÄÉ+multimodal</span>
</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.8.6.2">33.2</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.8.6.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.8.6.3.1">24.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.8.6.4">42.7</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.8.6.5">37.9</td>
<td class="ltx_td ltx_align_left" id="S5.T1.2.8.6.6">24.9</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.8.6.7">36.8</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.2.8.6.8">33.3</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.9.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.2.9.7.1">SALM-T5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.9.7.2">34.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.9.7.3">24.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.9.7.4">43.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.9.7.5">38.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.9.7.6">25.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.9.7.7">37.0</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.2.9.7.8">33.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.10.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T1.2.10.8.1">¬†¬†<span class="ltx_text ltx_font_italic" id="S5.T1.2.10.8.1.1">‚ÄÉ+multimodal</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.2.10.8.2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.10.8.2.1">34.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.2.10.8.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.10.8.3.1">24.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T1.2.10.8.4"><span class="ltx_text ltx_font_bold" id="S5.T1.2.10.8.4.1">44.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.2.10.8.5">37.9</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T1.2.10.8.6"><span class="ltx_text ltx_font_bold" id="S5.T1.2.10.8.6.1">27.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T1.2.10.8.7">37.4</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T1.2.10.8.8"><span class="ltx_text ltx_font_bold" id="S5.T1.2.10.8.8.1">34.4</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Text translation SacreBLEU scores on a subset of FLORES. The T5 NMT baseline is trained exclusively on text, and extended to audio modality as SALM-T5. </figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.2" rowspan="2"><span class="ltx_text" id="S5.T2.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T2.1.1.1">En <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">‚Üí</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.3">De</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.4">Es</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.1.5">Fr</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.1.1.6" rowspan="2"><span class="ltx_text" id="S5.T2.1.1.6.1">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.2">
<td class="ltx_td ltx_align_left" id="S5.T2.2.2.2">De</td>
<td class="ltx_td ltx_align_left" id="S5.T2.2.2.3">Es</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.2.4">Fr</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S5.T2.2.2.1">
<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T2.2.2.1.m1.1"><semantics id="S5.T2.2.2.1.m1.1a"><mo id="S5.T2.2.2.1.m1.1.1" stretchy="false" xref="S5.T2.2.2.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.1.m1.1b"><ci id="S5.T2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.1.m1.1d">‚Üí</annotation></semantics></math> En</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.2.3.1.1">T5 NMT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.3.1.2">38.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.3.1.3">27.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.3.1.4">50.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.3.1.5">44.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.3.1.6">30.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.3.1.7">45.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T2.2.3.1.8">39.4</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T2.2.4.2.1">SALM-T5</td>
<td class="ltx_td ltx_align_left" id="S5.T2.2.4.2.2">0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.2.4.2.3">0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.4.2.4">0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.2.4.2.5">0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.2.4.2.6">0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.4.2.7">0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T2.2.4.2.8">0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T2.2.5.3.1">¬†¬†<span class="ltx_text ltx_font_italic" id="S5.T2.2.5.3.1.1">‚ÄÉ+multimodal</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.2.5.3.2">38.7</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.2.5.3.3">27.5</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.2.5.3.4">50.4</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.2.5.3.5">44.1</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T2.2.5.3.6">30.9</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.2.5.3.7">45.8</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T2.2.5.3.8">39.6</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Speech translation (Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S5.T1" title="TABLE I ‚Ä£ V Results ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">I</span></a>).</span> Both BESTOW-GPT and SALM-T5 are trained purely for speech translation as baselines. We expect that these models ‚Äùcatastrophically forget‚Äù their text translation skills while they learn speech translation. With speech-only training, SALM-T5 naturally achieves a better mean BLEU score of 33.9 compared to BESTOW-GPT‚Äôs 32.0 since the underlying foundation model was trained specifically for NMT.
We confront these results with multimodal training, where we observe 1.3 absolute BLEU score improvement for BESTOW-GPT and 0.5 absolute BLEU score improvement for SALM-T5. Larger improvement for BESTOW-GPT makes sense since TinyLlama model hasn‚Äôt had as much prior training on NMT data. In both cases, speech translation capability is improved by joint multimodal training, demonstrating the synergy between two modalities.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Text machine translation (Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S5.T2" title="TABLE II ‚Ä£ V Results ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">II</span></a>).</span> We compare the performance of the T5 NMT model in three scenarios: pretrained text-only model, after speech-only finetuning (SALM-T5) and after joint multimodal training (SALM-T5+multimodal). First, we notice that SALM-T5 completely lost its capability of translating text while acquiring AST skills (as seen in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S5.T1" title="TABLE I ‚Ä£ V Results ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">I</span></a>). For multimodal SALM-T5, the mean BLEU score across 6 language pairs is preserved (we don‚Äôt consider 0.2 as a meaningful improvement), while we observe up to 0.6 BLEU score variation in some pairs. Therefore, the first goal of multimodal training is achieved: the T5 model did not lose its original performance on text after extension to audio modality.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Multimodal training efficiency gains from 2D bucketing and OOMptimizer (opt). </figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.2" rowspan="2"><span class="ltx_text" id="S5.T3.1.1.1.2.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.3" rowspan="2"><span class="ltx_text" id="S5.T3.1.1.1.3.1">Runtime</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T3.1.1.1.4">Mean batch size</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.5" rowspan="2"><span class="ltx_text" id="S5.T3.1.1.1.5.1">Steps/sec</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S5.T3.1.2.2.1">Audio</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.2.2.2">Text</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.3.1.1">BESTOW-GPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.1.2">450k</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.1.3">7 days</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.3.1.4">13</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.1.5">100</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.1.6">1.15</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T3.1.4.2.1">¬†¬†<span class="ltx_text ltx_font_italic" id="S5.T3.1.4.2.1.1">‚ÄÉ+opt</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.2.2">100k</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.2.3">2.5 days</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.2.4">55</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.2.5">269</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.1.4.2.6">0.81</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T3.1.5.3.1">SALM-T5<span class="ltx_text ltx_font_italic" id="S5.T3.1.5.3.1.1">+opt</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.5.3.2">7.5k</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.5.3.3">5 hours</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S5.T3.1.5.3.4">30</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.5.3.5">251</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T3.1.5.3.6">0.7</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Efficient multimodal training with 2D bucketing and OOMptimizer.</span>
We compare the training efficiency of BESTOW-GPT without and with our optimizations in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13523v1#S5.T3" title="TABLE III ‚Ä£ V Results ‚Ä£ EMMeTT: Efficient Multimodal Machine Translation Training"><span class="ltx_text ltx_ref_tag">III</span></a>.
The baseline used 1D bucketing with a batch size heuristic on the total number of seconds (audio) or tokens (text) in a mini-batch, and a quadratic length penalty to offset the quadratic memory complexity of transformer.
The penalty and total length thresholds were tuned manually to observe close to 100% allocated GPU memory.
SALM-T5 model was trained directly with OOMptimizer to save resources and converged in 5 hours, which is much faster than BESTOW-GPT likely due to being pretrained specifically for NMT. We noticed that without OOMptimizer, the bucket batch sizes are constrained either by sequence length outliers (especially for drastically different input and output sequence length examples) or the longest sequence lengths in the dataset.
2D bucketing increased the batch sizes for most buckets by 1.5-2x for audio modality and 2-4x for text modality when training SALM-T5 model, since the large sequence length outliers have their own sub-buckets with a smaller batch size. With BESTOW-GPT, we applied 2D bucketing for audio modality with a modest 15-20% batch size increase.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced EMMeTT, an efficiency-driven mutlimodal training regime. We extended two foundation models: Megatron-NMT-1B based on T5 (encoder-decoder) and TinyLlama GPT (decoder-only) to support speech translation. Text translation capabilities were kept intact with MUX data blending and a round robin modality selection strategy. 2D bucketing and batch size optimization substantially accelerated the training in the presence of variable sequence lengths. Although not demonstrated in this work, we expect this framework to perform effectively with larger model sizes and model parallelism techniques such as tensor/sequence/pipeline parallel or fully-sharded data parallel. We aim to extend EMMeTT to general-purpose multimodal LLM training in our future works. Model training code, together with OOMptimizer, is open-source and available as a part of NVIDIA NeMo toolkit.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Z.¬†Chen, H.¬†Huang, A.¬†Andrusenko, O.¬†Hrinchuk, K.¬†C. Puvvada, J.¬†Li, S.¬†Ghosh, J.¬†Balam, and B.¬†Ginsburg, ‚ÄúSalm: Speech-augmented language model with in-context learning for speech recognition and translation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2024, pp. 13‚Äâ521‚Äì13‚Äâ525.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C.¬†Tang <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et¬†al.</em>, ‚ÄúSalmonn: Towards generic hearing abilities for large language models,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv preprint arXiv:2310.13289</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.¬†Bai <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et¬†al.</em>, ‚ÄúQwen technical report,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z.¬†Kong, A.¬†Goel, R.¬†Badlani, W.¬†Ping, R.¬†Valle, and B.¬†Catanzaro, ‚ÄúAudio flamingo: A novel audio language model with few-shot learning and dialogue abilities,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2402.01831</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.-B. Alayrac <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et¬†al.</em>, ‚ÄúFlamingo: a visual language model for few-shot learning,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Advances in Neural Information Processing Systems</em>, vol.¬†35, pp. 23‚Äâ716‚Äì23‚Äâ736, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Z.¬†Chen, H.¬†Huang, O.¬†Hrinchuk, K.¬†C. Puvvada, N.¬†R. Koluguri, P.¬†≈ªelasko, J.¬†Balam, and B.¬†Ginsburg, ‚ÄúBestow: Efficient and streamable speech language model with the best of two worlds in gpt and t5,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2406.19954</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.¬†Tang, H.¬†Gong, N.¬†Dong, C.¬†Wang, W.-N. Hsu, J.¬†Gu, A.¬†Baevski, X.¬†Li, A.¬†Mohamed, M.¬†Auli <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et¬†al.</em>, ‚ÄúUnified speech-text pre-training for speech translation and recognition,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">arXiv preprint arXiv:2204.05409</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R.¬†Zheng, J.¬†Chen, M.¬†Ma, and L.¬†Huang, ‚ÄúFused acoustic and text encoding for multimodal bilingual pretraining and speech translation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Machine Learning</em>.¬†¬†¬†PMLR, 2021, pp. 12‚Äâ736‚Äì12‚Äâ746.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z.¬†Huang, R.¬†Ye, T.¬†Ko, Q.¬†Dong, S.¬†Cheng, M.¬†Wang, and H.¬†Li, ‚ÄúSpeech translation with large language models: An industrial practice,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2312.13585</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X.¬†Xu, S.¬†Ouyang, B.¬†Yan, P.¬†Fernandes, W.¬†Chen, L.¬†Li, G.¬†Neubig, and S.¬†Watanabe, ‚ÄúCmu‚Äôs iwslt 2024 simultaneous speech translation system,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2408.07452</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, W.¬†Han, J.¬†Qin, Y.¬†Wang, A.¬†Bapna, Z.¬†Chen, N.¬†Chen, B.¬†Li, V.¬†Axelrod, G.¬†Wang <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">et¬†al.</em>, ‚ÄúGoogle USM: Scaling automatic speech recognition beyond 100 languages,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">arXiv preprint arXiv:2303.01037</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.¬†Bapna, C.¬†Cherry, Y.¬†Zhang, Y.¬†Jia, M.¬†Johnson, Y.¬†Cheng, S.¬†Khanuja, J.¬†Riesa, and A.¬†Conneau, ‚Äúmslam: Massively multilingual joint pre-training for speech and text,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2202.01374</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.¬†Raffel, N.¬†Shazeer, A.¬†Roberts, K.¬†Lee, S.¬†Narang, M.¬†Matena, Y.¬†Zhou, W.¬†Li, and P.¬†J. Liu, ‚ÄúExploring the limits of transfer learning with a unified text-to-text transformer,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Journal of Machine Learning Research</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.¬†B. Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†Kaplan, P.¬†Dhariwal, A.¬†Neelakantan, P.¬†Shyam, G.¬†Sastry, A.¬†Askell, S.¬†Agarwal, A.¬†Herbert-Voss, G.¬†Krueger, T.¬†Henighan, R.¬†Child, A.¬†Ramesh, D.¬†M. Ziegler, J.¬†Wu, C.¬†Winter, C.¬†Hesse, M.¬†Chen, E.¬†Sigler, M.¬†Litwin, S.¬†Gray, B.¬†Chess, J.¬†Clark, C.¬†Berner, S.¬†McCandlish, A.¬†Radford, I.¬†Sutskever, and D.¬†Amodei, ‚ÄúLanguage models are few-shot learners,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, vol. abs/2005.14165, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2005.14165</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y.¬†Liu, J.¬†Gu, N.¬†Goyal, X.¬†Li, S.¬†Edunov, M.¬†Ghazvininejad, M.¬†Lewis, and L.¬†Zettlemoyer, ‚ÄúMultilingual denoising pre-training for neural machine translation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Transactions of the Association for Computational Linguistics</em>.¬†¬†¬†MIT Press, 2020, pp. 726‚Äì742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M.¬†Lewis, Y.¬†Liu, N.¬†Goyal, M.¬†Ghazvininejad, A.¬†Mohamed, O.¬†Levy, V.¬†Stoyanov, and L.¬†Zettlemoyer, ‚ÄúBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>.¬†¬†¬†ACL, 2020, pp. 7871‚Äì7880.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W.¬†Chan, N.¬†Jaitly, Q.¬†Le, and O.¬†Vinyals, ‚ÄúListen, attend and spell: A neural network for large vocabulary conversational speech recognition,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.¬†¬†¬†IEEE, 2016, pp. 4960‚Äì4964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M.¬†Post, T.¬†Gowda, R.¬†Grundkiewicz, H.¬†Khayrallah, R.¬†Jain, and M.¬†Junczys-Dowmunt, ‚ÄúSOTASTREAM: A streaming approach to machine translation training,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)</em>, L.¬†Tan, D.¬†Milajevs, G.¬†Chauhan, J.¬†Gwinnup, and E.¬†Rippeth, Eds.¬†¬†¬†Singapore: Association for Computational Linguistics, Dec. 2023, pp. 110‚Äì119. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/2023.nlposs-1.13</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P.¬†≈ªelasko, D.¬†Povey, J.¬†Trmal, and S.¬†Khudanpur, ‚ÄúLhotse: a speech data representation library for the modern deep learning ecosystem,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of NeurIPS Workshop on Data-Centric AI</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K.¬†C. Puvvada, P.¬†≈ªelasko, H.¬†Huang, O.¬†Hrinchuk, N.¬†R. Koluguri, K.¬†Dhawan, S.¬†Majumdar, E.¬†Rastorgueva, Z.¬†Chen, V.¬†Lavrukhin, J.¬†Balam, and B.¬†Ginsburg, ‚ÄúLess is more: Accurate speech recognition &amp; translation without web-scale data,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of Interspeech</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
‚ÄúNVIDIA Megatron NMT en-to-any 500M.‚Äù [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_en_any_500m</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
‚ÄúNVIDIA Megatron NMT any-to-en 500M.‚Äù [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.¬†Mittal, O.¬†Hrinchuk, and O.¬†Kuchaiev, ‚ÄúLeveraging synthetic targets for machine translation,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2305.06155</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T.¬†Nguyen, C.¬†V. Nguyen, V.¬†D. Lai, H.¬†Man, N.¬†T. Ngo, F.¬†Dernoncourt, R.¬†A. Rossi, and T.¬†H. Nguyen, ‚ÄúCulturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages,‚Äù 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2309.09400</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G.¬†Ram√≠rez-S√°nchez, J.¬†Zaragoza-Bernabeu, M.¬†Ba√±√≥n, and S.¬†Ortiz-Rojas, ‚ÄúBifixer and bicleaner: two open-source tools to clean your parallel data.‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>.¬†¬†¬†European Association for Machine Translation, 2020, pp. 291‚Äì298.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
O.¬†Kuchaiev, J.¬†Li, H.¬†Nguyen, O.¬†Hrinchuk, R.¬†Leary, B.¬†Ginsburg, S.¬†Kriman, S.¬†Beliaev, V.¬†Lavrukhin, J.¬†Cook <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et¬†al.</em>, ‚ÄúNemo: a toolkit for building ai applications using neural modules,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">arXiv preprint arXiv:1909.09577</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
P.¬†Zhang, G.¬†Zeng, T.¬†Wang, and W.¬†Lu, ‚ÄúTinyllama: An open-source small language model,‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2401.02385</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma, ‚ÄúAdam: A method for stochastic optimization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of ICLR</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
‚ÄúNVIDIA Riva Megatron NMT any-to-any 1B.‚Äù [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/riva_megatronnmt_any_any_1b</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B.¬†Zhang and R.¬†Sennrich, ‚ÄúRoot mean square layer normalization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of NeurIPS</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A.¬†Conneau, M.¬†Ma, S.¬†Khanuja, Y.¬†Zhang, V.¬†Axelrod, S.¬†Dalmia, J.¬†Riesa, C.¬†Rivera, and A.¬†Bapna, ‚ÄúFleurs: Few-shot learning evaluation of universal representations of speech,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2022 IEEE Spoken Language Technology Workshop (SLT)</em>.¬†¬†¬†IEEE, 2023, pp. 798‚Äì805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
N.¬†Goyal, C.¬†Gao, V.¬†Chaudhary, P.-J. Chen, G.¬†Wenzek, D.¬†Ju, S.¬†Krishnan, M.¬†Ranzato, F.¬†Guzm√°n, and A.¬†Fan, ‚ÄúThe flores-101 evaluation benchmark for low-resource and multilingual machine translation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Transactions of the Association for Computational Linguistics</em>, vol.¬†10, pp. 522‚Äì538, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M.¬†Post, ‚ÄúA call for clarity in reporting bleu scores,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1804.08771</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
L.¬†Barrault, Y.-A. Chung, M.¬†C. Meglioli, D.¬†Dale, N.¬†Dong, M.¬†Duppenthaler, P.-A. Duquenne, B.¬†Ellis, H.¬†Elsahar, J.¬†Haaheim <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">et¬†al.</em>, ‚ÄúSeamless: Multilingual expressive and streaming speech translation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">arXiv preprint arXiv:2312.05187</em>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 13:58:45 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
