<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2101.03367] Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems</title><meta property="og:description" content="Next-generation autonomous and networked industrial systems (i.e.,
robots, vehicles, drones) have driven advances in ultra-reliable,
low latency communications (URLLC) and computing. These networked
multi-agent systems…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2101.03367">

<!--Generated on Fri Mar  1 13:36:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca
Barbieri
</span><span class="ltx_author_notes">©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.S. Savazzi and S. Kianoush are with the Institute of Electronics, Computer and Telecommunication Engineering (IEIIT) of Consiglio Nazionale delle Ricerche (CNR), Milano, Italy.M. Nicoli and L. Barbieri are with Politecnico di Milano DIG and DEIB department, Milano, Italy.M. Bennis is with the Centre for Wireless Communications, University of Oulu, Finland.The paper has been accepted for publication in the IEEE Communications Magazine. The work is supported by the CHIST-ERA European projects RadioSense (Wireless Big-Data Augmented Smart Industry) and CONNECT (Communication-aware Dynamic Edge Computing). It is also supported by the Project BASE5G (Broadband InterfAces and services for Smart Environments enabled by 5G technologies), funded by the Italian Lombardy Regional Government under the grant POR-FESR 2014-2020, ID 1155850. The current arXiv contains an additional Appendix that describes the MIMO radar dataset for the setup of Fig. 5. Sample dataset published on IEEE Data Port http://dx.doi.org/10.21227/0wmc-hq36.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Next-generation autonomous and networked industrial systems (i.e.,
robots, vehicles, drones) have driven advances in ultra-reliable,
low latency communications (URLLC) and computing. These networked
multi-agent systems require fast, communication-efficient and distributed
machine learning (ML) to provide mission critical control functionalities.
Distributed ML techniques, including federated learning (FL), represent
a mushrooming multidisciplinary research area weaving in sensing,
communication and learning. FL enables continual model training in
distributed wireless systems: rather than fusing raw data samples
at a centralized server, FL leverages a cooperative fusion approach
where networked agents, connected via URLLC, act as distributed learners
that periodically exchange their locally trained model parameters.
This article explores emerging opportunities of FL for the next-generation
networked industrial systems. Open problems are discussed, focusing
on cooperative driving in connected automated vehicles and collaborative
robotics in smart manufacturing.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid transformation of industrial systems, driven by the digitization
and 5G communication evolution, has led to extensive research initiatives
on manufacturing and automotive verticals. These include, for example, Industry 4.0
(I4.0), at European level, the European Factories of the Future Research
Association (EFFRA, effra.eu), the 5G Alliance for Connected Industries
and Automation (5g-acia.org) and the 5G Automotive Association (5gaa.org).
The envisioned smart industrial systems rely on networked <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">machines</em> with increasing level of intelligence and autonomy,
moving far beyond traditional low-cost, low-power sensors. According
to industrial requirements, such machines are required to support:</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">1) autonomous and adaptive decision making in dynamic situations with mobile operators/equipment, device-less human-machine interfaces and time-varying environments;</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">2) big-data-driven training of large-size machine learning (ML) models
for decision-making;</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">3) ultra-reliable and low-latency communications (URLLC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
for mission-critical device-to-device (D2D) operations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Networked and cooperative intelligent machines have recently opened new research opportunities that target the integration of distributed ML tools with sensing, communication and decision operations. Cross-fertilization
of these components is crucial to enable challenging collaborative
tasks in terms of safety, reliability, scalability and latency.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Among distributed ML techniques, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
has been emerging for model training in decentralized wireless systems. Model
parameters, namely weights and biases in deep neural network (DNN)
layers, are optimized collectively by cooperation of interconnected
devices, acting as distributed learners. In contrast to conventional
edge-cloud ML, FL does not require to send local training data to the server, which may be infeasible in mission critical settings with extremely low latency and data
privacy constraints.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The most popular FL implementation, namely federated averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>,
alternates between the computation of a <em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">local model</em> at each
device and a round of communication with the server for learning of
a <em id="S1.p7.1.2" class="ltx_emph ltx_font_italic">global model</em>. Local models are typically obtained by minimizing
a local loss function via Stochastic Gradient Descent (SGD) steps
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, using local training examples and target values.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Federated averaging is privacy-preserving by design, as it keeps the training
data on-device. However, it still leverages the server-client architecture,
which might not be robust to data poisoning attacks and scalability needs.
Overcoming this issue mandates moving towards fully decentralized FL
solutions relying <em id="S1.p8.1.1" class="ltx_emph ltx_font_italic">solely</em> on local processing and cooperation among end machines. As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the device sends its local ML model parameters to neighbors and receives in
return the corresponding updates. Next, it improves
its local parameters by fusing the received contributions. This procedure
continues until convergence.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The article addresses the opportunities of emerging distributed FL
tools specifically tailored for systems characterized by autonomous
industrial components (vehicles, robots). FL is first proposed as
an integral part of the sensing-decision-action loop. Next, novel decentralized FL tools and emerging research
challenges are highlighted. The potential of FL is further elaborated with
considerations primarily given to mission critical control operations in the field of cooperative automated vehicles and densely interconnected robots. Analysis with real data on a practical usage
scenario reveals FL as a promising tool underpinned by URLLC communications.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2101.03367/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="183" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Decentralized learning with federated datasets. Examples in mission critical control applications: collaborative industrial robotics and cooperative automated vehicles.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Sensing, decentralized learning and communication co-design</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Decentralized FL solutions have great potential for industrial 5G
and beyond 5G (B5G) verticals. In automated industrial processes, decentralized FL imbues intelligence directly into the end machines, which become smart cooperative agents. Fig.
<a href="#S2.F2" title="Figure 2 ‣ II Sensing, decentralized learning and communication co-design ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts a schematic of a cooperative and automated multi-agent
industrial system. It consists of connected machines performing collaborative
tasks, and integrating ML model training within the sensing-decision-action loop. The ML model outputs might be scenario-dependent predictions
of a physical process, or rather value functions to be used for policy
improvements (i.e., reinforcement learning). Outputs are fed to the
machine controller for local decisions or actuations. Training of
the ML model calls for highly efficient knowledge discovery operations based on the overall training data collected
by <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">all</em> the machines performing the same task (federated dataset),
rather than local data only. Recent advancements of FL constitute
a first but significant step towards <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">collaborative learning</em>
and, particularly, the understanding of how ML could be distributed
over networked devices, without centralized orchestration. Collaborative
learning underpins local decisions and allows the networked machines
to augment their ML model by sharing ego knowledge. As part of the
sensing-decision-action loop, emerging FL tools are expected to target
three fundamental challenges:</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">1) get over the restrictions of server-client architectures and movements of large, unstructured, raw datasets over D2D wireless
links, in favor of (typically) sparse ML model parameters exchange;</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">2) optimally balance opportunistic (ego) learning based on local training data,
and collaborative learning (leveraging neighbor’s experience), with
the goal of steering convergence;</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">3) learn and re-train continuously over URLLC links to adapt to changes
in the data distribution, environment, process or situation.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2101.03367/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="185" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Decentralized FL in the sensing-decision-action loop.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Decentralized FL: emerging trends</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">Decentralized FL alternates the mutual exchange of local ML models
<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{k}(t)" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.2" xref="S3.p1.1.m1.1.2.cmml"><msub id="S3.p1.1.m1.1.2.2" xref="S3.p1.1.m1.1.2.2.cmml"><mi id="S3.p1.1.m1.1.2.2.2" xref="S3.p1.1.m1.1.2.2.2.cmml">𝐖</mi><mi id="S3.p1.1.m1.1.2.2.3" xref="S3.p1.1.m1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.1.m1.1.2.1" xref="S3.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.p1.1.m1.1.2.3.2" xref="S3.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.p1.1.m1.1.2.3.2.1" xref="S3.p1.1.m1.1.2.cmml">(</mo><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S3.p1.1.m1.1.2.3.2.2" xref="S3.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.2.cmml" xref="S3.p1.1.m1.1.2"><times id="S3.p1.1.m1.1.2.1.cmml" xref="S3.p1.1.m1.1.2.1"></times><apply id="S3.p1.1.m1.1.2.2.cmml" xref="S3.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.2.2.1.cmml" xref="S3.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.p1.1.m1.1.2.2.2.cmml" xref="S3.p1.1.m1.1.2.2.2">𝐖</ci><ci id="S3.p1.1.m1.1.2.2.3.cmml" xref="S3.p1.1.m1.1.2.2.3">𝑘</ci></apply><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{W}_{k}(t)</annotation></semantics></math> with the on-device minimization of a local loss
function. The goal is to promote convergence to a global model <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{W}_{\infty}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">𝐖</mi><mi mathvariant="normal" id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">∞</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝐖</ci><infinity id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathbf{W}_{\infty}</annotation></semantics></math>
that minimizes a global loss, decomposed into the sum of local losses.
Fully distributed FL implementations run on top of D2D networks characterized
by arbitrary connectivity graphs. The collaborative learning is based on Decentralized Stochastic Gradient Descent (DSGD), which guarantees convergence under strong convexity assumptions of local loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and networks with doubly stochastic adjacency matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. DSGD alternates on-device SGD steps to obtain <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{W}_{k}(t)" display="inline"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1.2" xref="S3.p1.3.m3.1.2.cmml"><msub id="S3.p1.3.m3.1.2.2" xref="S3.p1.3.m3.1.2.2.cmml"><mi id="S3.p1.3.m3.1.2.2.2" xref="S3.p1.3.m3.1.2.2.2.cmml">𝐖</mi><mi id="S3.p1.3.m3.1.2.2.3" xref="S3.p1.3.m3.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.3.m3.1.2.1" xref="S3.p1.3.m3.1.2.1.cmml">​</mo><mrow id="S3.p1.3.m3.1.2.3.2" xref="S3.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S3.p1.3.m3.1.2.3.2.1" xref="S3.p1.3.m3.1.2.cmml">(</mo><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">t</mi><mo stretchy="false" id="S3.p1.3.m3.1.2.3.2.2" xref="S3.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.2.cmml" xref="S3.p1.3.m3.1.2"><times id="S3.p1.3.m3.1.2.1.cmml" xref="S3.p1.3.m3.1.2.1"></times><apply id="S3.p1.3.m3.1.2.2.cmml" xref="S3.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.2.2.1.cmml" xref="S3.p1.3.m3.1.2.2">subscript</csymbol><ci id="S3.p1.3.m3.1.2.2.2.cmml" xref="S3.p1.3.m3.1.2.2.2">𝐖</ci><ci id="S3.p1.3.m3.1.2.2.3.cmml" xref="S3.p1.3.m3.1.2.2.3">𝑘</ci></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathbf{W}_{k}(t)</annotation></semantics></math>,
with the mutual exchange of model parameters to steer convergence.
Mutual exchange is regulated by gossip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, consensus
or diffusion algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Consensus, diffusion and gradient negotiations</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In consensus based approaches, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, federated
nodes exchange local ML model parameters and update them sequentially
by distributed averaging. For the mutual exchange of models, nodes
might select random, time-varying or optimized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
partners. In gossipgrad <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the selection policy creates
a virtual network where each cooperating agent is connected to, typically,
two other nodes. Convergence time and loss are ruled by the specific
choice of model update operations. Some updating strategies favor
fast convergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> while others target model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Considering DNN model training, the neural network layers are also
typically learned separately and independently. Therefore, on-device
optimization and networking phases can run in parallel. Communication
overhead scales linearly with the model size and the number of cooperating
agents. However, non-independent-identically-distributed (non-IID)
federated datasets and large model size penalize the rate of convergence
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.9" class="ltx_p">Diffusion strategies incorporate a gradient negotiation phase,
described in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Consensus, diffusion and gradient negotiations ‣ III Decentralized FL: emerging trends ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where devices exchange the information
about how neighbor models should be adjusted considering local data.
Diffusion virtually expands the local training data-set, and boosts
the convergence compared with gossip. Gradient exchange strategies
are often regulated by request-reply negotiation stages between neighbors
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In the example of Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Consensus, diffusion and gradient negotiations ‣ III Decentralized FL: emerging trends ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the local
model <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">i</annotation></semantics></math>, <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{W}_{i}(t=1)" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><msub id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">𝐖</mi><mi id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"></times><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">𝐖</ci><ci id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">𝑖</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><eq id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"></eq><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{W}_{i}(t=1)</annotation></semantics></math>, is sent to the neighbor <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">k</annotation></semantics></math> at
time <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="t=1" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><eq id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></eq><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">t=1</annotation></semantics></math> to start a negotiation. The received model is used by
the neighbor <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">k</annotation></semantics></math> to compute a gradient vector <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\nabla\mathbf{W}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mo rspace="0.167em" id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">∇</mo><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">𝐖</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><ci id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1">∇</ci><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝐖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\nabla\mathbf{W}</annotation></semantics></math>
using the local loss. Both the gradient vector and the local model
are then fed-back to the device <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">i</annotation></semantics></math>, that started the negotiation
(<math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="t=2" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mrow id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.8.m8.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><eq id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1"></eq><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">t=2</annotation></semantics></math>). On-device optimization (<math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="t=3" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mrow id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.9.m9.1.1.1" xref="S3.SS1.p2.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><eq id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1"></eq><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">t=3</annotation></semantics></math>) finally adjusts the local
model by combining the gradients obtained from local loss with those
received from the neighbor (combine-and-adapt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>).
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> the negotiation resorts to a two-stage asynchronous
procedure: convergence speed improves at the cost of larger communication
overhead. Exploring this trade off is currently an open challenge.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In addition to the above tools, distributed ledger technologies can also
be applied to decentralized training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> by
validating clients model updates via a series of validation steps
(Proof-of-Work, or others). As a result, decentralized FL is transformed
into a market of expert model training nodes and validators. The development
of robust FL designs against data poisoning and adversarial manipulations
is still an open problem.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2101.03367/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="196" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Decentralized FL with diffusion: communication,
gradient negotiations (<math id="S3.F3.3.m1.2" class="ltx_Math" alttext="t=1,2" display="inline"><semantics id="S3.F3.3.m1.2b"><mrow id="S3.F3.3.m1.2.3" xref="S3.F3.3.m1.2.3.cmml"><mi id="S3.F3.3.m1.2.3.2" xref="S3.F3.3.m1.2.3.2.cmml">t</mi><mo id="S3.F3.3.m1.2.3.1" xref="S3.F3.3.m1.2.3.1.cmml">=</mo><mrow id="S3.F3.3.m1.2.3.3.2" xref="S3.F3.3.m1.2.3.3.1.cmml"><mn id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml">1</mn><mo id="S3.F3.3.m1.2.3.3.2.1" xref="S3.F3.3.m1.2.3.3.1.cmml">,</mo><mn id="S3.F3.3.m1.2.2" xref="S3.F3.3.m1.2.2.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.2c"><apply id="S3.F3.3.m1.2.3.cmml" xref="S3.F3.3.m1.2.3"><eq id="S3.F3.3.m1.2.3.1.cmml" xref="S3.F3.3.m1.2.3.1"></eq><ci id="S3.F3.3.m1.2.3.2.cmml" xref="S3.F3.3.m1.2.3.2">𝑡</ci><list id="S3.F3.3.m1.2.3.3.1.cmml" xref="S3.F3.3.m1.2.3.3.2"><cn type="integer" id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1">1</cn><cn type="integer" id="S3.F3.3.m1.2.2.cmml" xref="S3.F3.3.m1.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.2d">t=1,2</annotation></semantics></math>) and computing rounds (<math id="S3.F3.4.m2.1" class="ltx_Math" alttext="t=3" display="inline"><semantics id="S3.F3.4.m2.1b"><mrow id="S3.F3.4.m2.1.1" xref="S3.F3.4.m2.1.1.cmml"><mi id="S3.F3.4.m2.1.1.2" xref="S3.F3.4.m2.1.1.2.cmml">t</mi><mo id="S3.F3.4.m2.1.1.1" xref="S3.F3.4.m2.1.1.1.cmml">=</mo><mn id="S3.F3.4.m2.1.1.3" xref="S3.F3.4.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.4.m2.1c"><apply id="S3.F3.4.m2.1.1.cmml" xref="S3.F3.4.m2.1.1"><eq id="S3.F3.4.m2.1.1.1.cmml" xref="S3.F3.4.m2.1.1.1"></eq><ci id="S3.F3.4.m2.1.1.2.cmml" xref="S3.F3.4.m2.1.1.2">𝑡</ci><cn type="integer" id="S3.F3.4.m2.1.1.3.cmml" xref="S3.F3.4.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.m2.1d">t=3</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Improving communication efficiency</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Reliable and low-latency D2D communications serve as the backbone
for distributed FL computations. Transmission of model parameters
must be extremely reliable with packet error rate down to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{-8}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mo id="S3.SS2.p1.1.m1.1.1.3a" xref="S3.SS2.p1.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">10</cn><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><minus id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">10^{-8}</annotation></semantics></math>
to prevent frequent retransmissions penalizing the convergence rate
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Transmission time intervals (TTI) should be aligned with
the data dynamics and the computation times required to perform SGD
and model adaptation, targeting <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">5</annotation></semantics></math> ms and below <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The model size also affects communication design choices: DNN models
often used in industrial applications contain <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="&gt;15" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml"></mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><gt id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></gt><csymbol cd="latexml" id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">&gt;15</annotation></semantics></math>k parameters per
layer, usually extremely sparse.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Although vanilla FL assumed noiseless
or rate-limited communications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, recently proposed <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">digital</em>
and <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">analog</em> designs quantify the effects of intermittent communications,
time-varying fading and interference. Digital implementations of FL require each device to communicate with peers over half-duplex links via time scheduled wireless
access. Popular solutions to limit the model size, and thus the communication
overhead, are quantization, sparsification and distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Sparsifying operators select a subset of informative ML parameters,
often improving also the model generalization.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">More recently, analog FL and hybrid analog-digital implementations have been proposed for fast
and synchronous model averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Analog FL gets
over the restrictions of time scheduled access as it exploits the
superposition property of dense wireless transmissions when averaging
the neighbor models. Each device receives the superposition of the
ML models simultaneously transmitted by the neighborhood. When ML
parameters are sparse and sent as uncoded/uncompressed, Lasso type
recovery can be adopted for decoding and reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Besides low-level communications, optimization of resource allocation has been considered to find the best trade-off between on-device computation (SGD
updates) and wireless channel use for parameters exchange <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Optimized medium access control can also substantially reduce the
FL loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>: for example, scheduling should reward devices
having high quality data compared to those possessing few, or redundant
samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Emerging research challenges</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The roll-out of new decentralized communication paradigms in B5G is
expected to bridge the gap between deep learning and wireless networking
research, raising at the same time unprecedented challenges. For example,
current FL designs generally ignore the underlying dynamics of the
network graph, the presence of intermittent communication links or
weakly connected components <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Research in this direction
should focus on balancing local data collection, model adaptations
and cooperation from selected agents. Digital, analog or hybrid implementations
should be considered based on devices’ mobility, model size, computational
and bandwidth resources, as well as connectivity quality. Learning
an optimal policy for graph (and/or neighbor) selection while training
the ML model, namely learning <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">simultaneously</em> the network graph
and the model, is also a promising direction.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Automated systems are often characterized by heterogeneous devices
performing distinct but related tasks. Collaborative learning of multiple
functions, namely multitask learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, will have
a relevant impact on several industrial applications. Understanding
when cooperation, in the form of consensus, or any decentralized agreement
protocol, can better steer convergence compared with opportunistic or ego
behaviors is an open problem of wide interest. The analysis must also
take into consideration the cost of distributed computations, the federated data distribution and the network designs for URLLC.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Using the I4.0 vision and emerging B5G verticals as guidelines, in what follows opportunities of FL are highlighted for the needs of selected mission critical control applications, namely cooperative automated driving and collaborative robotics.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2101.03367/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="178" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distributed FL for cooperative perception by real-time
fusion of imaging data at different vehicles.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Distributed intelligence for cooperative automated driving</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Vehicular URLLC leverage B5G connectivity to enable flexible vehicle-to-everything
(V2X) interactions with road infrastructure (V2I) and other vehicles
(V2V). Distributed ML over V2X networks will play a central role in
cooperative intelligent transportation system (C-ITS), enabling level
4-5 cooperative automated driving functionalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Cooperative automated vehicles share maps of the driving environment
(Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C Emerging research challenges ‣ III Decentralized FL: emerging trends ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) using V2X URLLC to extend the range and resolution
of their ego imaging sensors (radar, camera, lidar). Beside improving detection
and localization of safety-related events, vehicles can also negotiate
the maneuvering and synchronize to a common mobility pattern, forming
tight autonomous-driving convoys and increasing traffic efficiency.
All the above scenarios are characterized by time-critical functions
that must be implemented on a closer-to-the-ground cloud, with part of the cooperative computational tasks performed locally, by pushing intelligence into the
vehicles rather than on the mobile edge cloud (MEC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">Decentralized FL techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are promising solutions for
these time-sensitive applications. They require the vehicles to transmit smaller models which can be
aggregated at the road side unit (RSU) or by vehicles via ultra-low latency
(<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">3</annotation></semantics></math> ms) and highly reliable (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.p2.2.m2.1a"><msup id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mn id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">10</mn><mrow id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml"><mo id="S4.p2.2.m2.1.1.3a" xref="S4.p2.2.m2.1.1.3.cmml">−</mo><mn id="S4.p2.2.m2.1.1.3.2" xref="S4.p2.2.m2.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">10</cn><apply id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3"><minus id="S4.p2.2.m2.1.1.3.1.cmml" xref="S4.p2.2.m2.1.1.3"></minus><cn type="integer" id="S4.p2.2.m2.1.1.3.2.cmml" xref="S4.p2.2.m2.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">10^{-5}</annotation></semantics></math>) V2X connectivity. The exchange of local ML model parameters, rather than raw data, is expected to decrease the learning time, allowing to quickly react to unexpected events and take safety-critical decisions.
Domain-specific FL designs must also account for intermittent communications, time-varying network graphs and non-IID training datasets changing quickly over time, thus evolving according to vehicle motions, with speeds up to 250 km/h (3GPP TR 22.886), and the surrounding environment. In such cases, a transitory phase is expected where vehicles with outdated, or partially trained models, will coexist with highly-automated ones and benefit from their cooperation. The federation with fully equipped vehicles will assist lower-level vehicles to get an augmented vision of the driving environment, even if equipped with less accurate sensors. Balancing between centralized and decentralized FL implementations for energy optimization is also a main issue. Besides
cooperative driving, to improve safety and reliability, the vehicles
need also to learn the network latency in a distributed manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>,
whereby decentralized FL (and its variants) are instrumental. Open
problems further relate to the tension between local and global models, and
the impact of large numbers of vehicles.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C Emerging research challenges ‣ III Decentralized FL: emerging trends ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> highlights a distributed ML-assisted cooperative
perception task where vehicles fuse their sensor data in a decentralized
implementation to get an extended vision of the environment. Fusing dynamics (i.e., global navigation satellite systems, GNSS, inertial measurement units, IMU) and imaging sensor data (i.e., lidar, camera, radar) from different vehicles improves
the location sensing accuracy making real-time responses feasible.
A main challenge is the association of unlabeled imaging measurements
at different vehicles to jointly sensed features for cooperative simultaneous localization and mapping (SLAM).
Large data
volumes and computational complexity are also critical challenges.
Decentralized FL is a promising candidate, as it is able to learn
a common model for data association and fusion from local raw data,
limiting the V2X exchange to model parameters.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2101.03367/assets/robotics_lay_v3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1047" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> (a) Decentralized FL setup in a cobot environment: frequency-modulated continuous wave (FMCW), multiple-input-multiple output (MIMO) radar locations (A,B,C) and ML model based classification of subject positions; (b) FL loss for gossip (blue) and diffusion (red) vs. time and over a D2D network of <math id="S4.F5.6.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S4.F5.6.m1.1b"><mn id="S4.F5.6.m1.1.1" xref="S4.F5.6.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S4.F5.6.m1.1c"><cn type="integer" id="S4.F5.6.m1.1.1.cmml" xref="S4.F5.6.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.6.m1.1d">15</annotation></semantics></math> robots. IID (no markers) and non-IID (diamond markers) federated datasets using the <math id="S4.F5.7.m2.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S4.F5.7.m2.1b"><mrow id="S4.F5.7.m2.1.1" xref="S4.F5.7.m2.1.1.cmml"><mn id="S4.F5.7.m2.1.1.2" xref="S4.F5.7.m2.1.1.2.cmml">8</mn><mo id="S4.F5.7.m2.1.1.1" xref="S4.F5.7.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.7.m2.1c"><apply id="S4.F5.7.m2.1.1.cmml" xref="S4.F5.7.m2.1.1"><csymbol cd="latexml" id="S4.F5.7.m2.1.1.1.cmml" xref="S4.F5.7.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.F5.7.m2.1.1.2.cmml" xref="S4.F5.7.m2.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.7.m2.1d">8\%</annotation></semantics></math> and <math id="S4.F5.8.m3.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S4.F5.8.m3.1b"><mrow id="S4.F5.8.m3.1.1" xref="S4.F5.8.m3.1.1.cmml"><mn id="S4.F5.8.m3.1.1.2" xref="S4.F5.8.m3.1.1.2.cmml">3</mn><mo id="S4.F5.8.m3.1.1.1" xref="S4.F5.8.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.8.m3.1c"><apply id="S4.F5.8.m3.1.1.cmml" xref="S4.F5.8.m3.1.1"><csymbol cd="latexml" id="S4.F5.8.m3.1.1.1.cmml" xref="S4.F5.8.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.F5.8.m3.1.1.2.cmml" xref="S4.F5.8.m3.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.m3.1d">3\%</annotation></semantics></math> of the full training dataset, respectively. D2D network with TTI=<math id="S4.F5.9.m4.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.F5.9.m4.1b"><mn id="S4.F5.9.m4.1.1" xref="S4.F5.9.m4.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.F5.9.m4.1c"><cn type="integer" id="S4.F5.9.m4.1.1.cmml" xref="S4.F5.9.m4.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.9.m4.1d">3</annotation></semantics></math>ms, BLER=<math id="S4.F5.10.m5.1" class="ltx_Math" alttext="10^{-9}" display="inline"><semantics id="S4.F5.10.m5.1b"><msup id="S4.F5.10.m5.1.1" xref="S4.F5.10.m5.1.1.cmml"><mn id="S4.F5.10.m5.1.1.2" xref="S4.F5.10.m5.1.1.2.cmml">10</mn><mrow id="S4.F5.10.m5.1.1.3" xref="S4.F5.10.m5.1.1.3.cmml"><mo id="S4.F5.10.m5.1.1.3b" xref="S4.F5.10.m5.1.1.3.cmml">−</mo><mn id="S4.F5.10.m5.1.1.3.2" xref="S4.F5.10.m5.1.1.3.2.cmml">9</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.F5.10.m5.1c"><apply id="S4.F5.10.m5.1.1.cmml" xref="S4.F5.10.m5.1.1"><csymbol cd="ambiguous" id="S4.F5.10.m5.1.1.1.cmml" xref="S4.F5.10.m5.1.1">superscript</csymbol><cn type="integer" id="S4.F5.10.m5.1.1.2.cmml" xref="S4.F5.10.m5.1.1.2">10</cn><apply id="S4.F5.10.m5.1.1.3.cmml" xref="S4.F5.10.m5.1.1.3"><minus id="S4.F5.10.m5.1.1.3.1.cmml" xref="S4.F5.10.m5.1.1.3"></minus><cn type="integer" id="S4.F5.10.m5.1.1.3.2.cmml" xref="S4.F5.10.m5.1.1.3.2">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.10.m5.1d">10^{-9}</annotation></semantics></math>. Dashed lines refer to the opportunistic learning case.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Decentralized FL for cobots</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The development of collaborative robotics in advanced manufacturing environments (cobots) can be interpreted as parallel to autonomous driving. Standardized in the ISO/TS 15066,
collaborative robot operations allow the machines (industrial manipulators, vehicles) installed in a shared workspace to move concurrently with human operators inside fenceless environments.
Connected and collaborative robots are transforming industrial workspaces, such as assembly lines in the example of Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a), and represent a challenging ground for the development of decentralized FL tools. First, robots operate in increasingly complex and time-varying environments. Network design, paired with distributed learning tools, must consider the problem of continual learning (and periodic re-training) over URLLC communication links to track variations of data dynamics caused by changes of the workflow process.
Second, robots might perform distinct tasks, although these can be
considered as strongly related, targeting a common manufacturing process (assembly or dis-assembly tasks) and efficient workspace sharing. In such cases, opportunistic (ego) and collaborative approaches should be carefully assessed. For example, FL can boost the model training for some tasks, such as vision functions, in common to many robots.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.6" class="ltx_p">To shed light on some of the above challenges, we resort to a typical mission critical control problem in Industry 4.0 workspaces. The goal is to plan the motions of a robotic manipulator based on the information about the positions of human operators sharing the workspace. Operator locations are classified using a ML model, described in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a), that computes the human-robot distance <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">d</annotation></semantics></math> and the direction of arrival (DOA) <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S5.p2.2.m2.1a"><mi id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><ci id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\theta</annotation></semantics></math> in real-time, using <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S5.p2.3.m3.1a"><mn id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><cn type="integer" id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">6</annotation></semantics></math> regions of interest as target classes. During the on-line workflow, the ML model can be trained/updated continuously by decentralized FL and consensus (cooperative learning loop). The sensing-decision-action loop is supervised by an industry standard programmable logic controller (PLC) that makes decisions about robot emergency stop, or trajectory replanning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, based on the information about subject positions. Decentralized FL uses D2D links, runs in parallel to the sensing-decision-action loop, and thus takes some load off of the PLC network, whose resources must be reserved for robot motion control. All the robots support vision functions implemented by <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.p2.4.m4.1a"><mn id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><cn type="integer" id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">3</annotation></semantics></math> radars working in the <math id="S5.p2.5.m5.1" class="ltx_Math" alttext="77-81" display="inline"><semantics id="S5.p2.5.m5.1a"><mrow id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mn id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">77</mn><mo id="S5.p2.5.m5.1.1.1" xref="S5.p2.5.m5.1.1.1.cmml">−</mo><mn id="S5.p2.5.m5.1.1.3" xref="S5.p2.5.m5.1.1.3.cmml">81</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><minus id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1.1"></minus><cn type="integer" id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">77</cn><cn type="integer" id="S5.p2.5.m5.1.1.3.cmml" xref="S5.p2.5.m5.1.1.3">81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">77-81</annotation></semantics></math>GHz band with a field-of-view of <math id="S5.p2.6.m6.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S5.p2.6.m6.1a"><mn id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><cn type="integer" id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">120</annotation></semantics></math>deg each. Radars produce the raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> that are fed into the ML model.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Results and discussions</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.5" class="ltx_p">The example of Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b) analyses the performance of decentralized FL approaches compared with ego, i.e., opportunistic, learning, considering the application case previously described. In particular, the robots might choose to combine in groups of <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S6.p1.1.m1.1a"><mn id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><cn type="integer" id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">15</annotation></semantics></math> agents and implement FL over a D2D network, or rather act opportunistically, thus learning from local data only, disabling the D2D radio interface. The examples also highlight the impact of federated data distribution on convergence time and validation loss. When federated data is IID partitioned, each device uses <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mn id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">8</mn><mo id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">8\%</annotation></semantics></math> of the full training dataset of 900 samples. In the more challenging non-IID scenario, only the <math id="S6.p1.3.m3.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S6.p1.3.m3.1a"><mrow id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mn id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">3</mn><mo id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">3\%</annotation></semantics></math> of the data is used: batches contain examples for only <math id="S6.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S6.p1.4.m4.1a"><mn id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><cn type="integer" id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">5</annotation></semantics></math> of the <math id="S6.p1.5.m5.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S6.p1.5.m5.1a"><mn id="S6.p1.5.m5.1.1" xref="S6.p1.5.m5.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S6.p1.5.m5.1b"><cn type="integer" id="S6.p1.5.m5.1.1.cmml" xref="S6.p1.5.m5.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.5.m5.1c">6</annotation></semantics></math> output target classes, chosen randomly.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.3" class="ltx_p">D2D communications are organized into consecutive frames and use time-division multiple access (TDMA) scheduling. Frames have payload <math id="S6.p2.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S6.p2.1.m1.1a"><mn id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><cn type="integer" id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">1</annotation></semantics></math> kB and TTI of <math id="S6.p2.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S6.p2.2.m2.1a"><mn id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><cn type="integer" id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">3</annotation></semantics></math> ms, in line with URLLC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Notice that a frame drop (BLER is <math id="S6.p2.3.m3.1" class="ltx_Math" alttext="10^{-9}" display="inline"><semantics id="S6.p2.3.m3.1a"><msup id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml"><mn id="S6.p2.3.m3.1.1.2" xref="S6.p2.3.m3.1.1.2.cmml">10</mn><mrow id="S6.p2.3.m3.1.1.3" xref="S6.p2.3.m3.1.1.3.cmml"><mo id="S6.p2.3.m3.1.1.3a" xref="S6.p2.3.m3.1.1.3.cmml">−</mo><mn id="S6.p2.3.m3.1.1.3.2" xref="S6.p2.3.m3.1.1.3.2.cmml">9</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><apply id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S6.p2.3.m3.1.1.1.cmml" xref="S6.p2.3.m3.1.1">superscript</csymbol><cn type="integer" id="S6.p2.3.m3.1.1.2.cmml" xref="S6.p2.3.m3.1.1.2">10</cn><apply id="S6.p2.3.m3.1.1.3.cmml" xref="S6.p2.3.m3.1.1.3"><minus id="S6.p2.3.m3.1.1.3.1.cmml" xref="S6.p2.3.m3.1.1.3"></minus><cn type="integer" id="S6.p2.3.m3.1.1.3.2.cmml" xref="S6.p2.3.m3.1.1.3.2">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">10^{-9}</annotation></semantics></math>) causes the loss of a layer update, and an increase of the convergence time. FL and networking have been simulated on a virtual environment but using real data from the plant. The environment allows to deploy networked agents acting as virtual robots and learning over a configurable federated dataset (IID and non-IID).</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.3" class="ltx_p">The simulations of Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b) quantify the average validation loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> experienced by the deployed agents versus time. This allows to assess the time required by the consensus rounds, as relevant for real-time implementation. Gossip and diffusion FL approaches, described previously, are evaluated over a k-regular D2D network consisting of robots with <math id="S6.p3.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S6.p3.1.m1.1a"><mn id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><cn type="integer" id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">2</annotation></semantics></math> neighbors each. Implementation of gossip is based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>: on each round, the agents randomly choose a single neighbor from the neighborhood, exchange the local ML models and adapt them by averaging, followed by SGD on local data. Diffusion FL implements the gradient negotiations described in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Consensus, diffusion and gradient negotiations ‣ III Decentralized FL: emerging trends ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>: on each round the agents now exchange the local ML models <span id="S6.p3.3.1" class="ltx_text ltx_font_italic">and</span> the gradients <math id="S6.p3.2.m2.1" class="ltx_Math" alttext="\nabla\mathbf{W}" display="inline"><semantics id="S6.p3.2.m2.1a"><mrow id="S6.p3.2.m2.1.1" xref="S6.p3.2.m2.1.1.cmml"><mo rspace="0.167em" id="S6.p3.2.m2.1.1.1" xref="S6.p3.2.m2.1.1.1.cmml">∇</mo><mi id="S6.p3.2.m2.1.1.2" xref="S6.p3.2.m2.1.1.2.cmml">𝐖</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p3.2.m2.1b"><apply id="S6.p3.2.m2.1.1.cmml" xref="S6.p3.2.m2.1.1"><ci id="S6.p3.2.m2.1.1.1.cmml" xref="S6.p3.2.m2.1.1.1">∇</ci><ci id="S6.p3.2.m2.1.1.2.cmml" xref="S6.p3.2.m2.1.1.2">𝐖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.2.m2.1c">\nabla\mathbf{W}</annotation></semantics></math> with a single neighbor, chosen again randomly. For both cases, the time span of an individual round consists of the transmission of the ML parameters in the assigned frames, the model adaptation and the SGD steps (<math id="S6.p3.3.m3.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S6.p3.3.m3.1a"><mn id="S6.p3.3.m3.1.1" xref="S6.p3.3.m3.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S6.p3.3.m3.1b"><cn type="integer" id="S6.p3.3.m3.1.1.cmml" xref="S6.p3.3.m3.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.3.m3.1c">30</annotation></semantics></math>ms per round). Before D2D transmission, the ML model parameters are sparsified <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to limit the communication overhead, measured in kB per round.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.5" class="ltx_p">Diffusion FL boosts the model training for robots possessing few data (<math id="S6.p4.1.m1.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S6.p4.1.m1.1a"><mrow id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml"><mn id="S6.p4.1.m1.1.1.2" xref="S6.p4.1.m1.1.1.2.cmml">3</mn><mo id="S6.p4.1.m1.1.1.1" xref="S6.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><apply id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1"><csymbol cd="latexml" id="S6.p4.1.m1.1.1.1.cmml" xref="S6.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.p4.1.m1.1.1.2.cmml" xref="S6.p4.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">3\%</annotation></semantics></math>) and insufficient examples (non-IID). Compared with ego learning, cooperation gives smaller loss after a training period of <math id="S6.p4.2.m2.1" class="ltx_Math" alttext="39" display="inline"><semantics id="S6.p4.2.m2.1a"><mn id="S6.p4.2.m2.1.1" xref="S6.p4.2.m2.1.1.cmml">39</mn><annotation-xml encoding="MathML-Content" id="S6.p4.2.m2.1b"><cn type="integer" id="S6.p4.2.m2.1.1.cmml" xref="S6.p4.2.m2.1.1">39</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.2.m2.1c">39</annotation></semantics></math> s, that is enough to track workflow changes. Diffusion needs considerable communication overhead (<math id="S6.p4.3.m3.1" class="ltx_Math" alttext="220" display="inline"><semantics id="S6.p4.3.m3.1a"><mn id="S6.p4.3.m3.1.1" xref="S6.p4.3.m3.1.1.cmml">220</mn><annotation-xml encoding="MathML-Content" id="S6.p4.3.m3.1b"><cn type="integer" id="S6.p4.3.m3.1.1.cmml" xref="S6.p4.3.m3.1.1">220</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.3.m3.1c">220</annotation></semantics></math> kB per round) due to gradients exchange. Convergence time thus increases with the network size. Notice that small but sudden increases of the loss are observed on some rounds when averaging the neighbor models. These effects are more visible when the local models are trained with non-IID data and can be mitigated by learning rate optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Gossip requires twice lower number of frames (<math id="S6.p4.4.m4.1" class="ltx_Math" alttext="92" display="inline"><semantics id="S6.p4.4.m4.1a"><mn id="S6.p4.4.m4.1.1" xref="S6.p4.4.m4.1.1.cmml">92</mn><annotation-xml encoding="MathML-Content" id="S6.p4.4.m4.1b"><cn type="integer" id="S6.p4.4.m4.1.1.cmml" xref="S6.p4.4.m4.1.1">92</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.4.m4.1c">92</annotation></semantics></math> kB per round) but it is less effective over non-IID data. However, it is still useful for refining models trained with IID data as it improves ego approaches after <math id="S6.p4.5.m5.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S6.p4.5.m5.1a"><mn id="S6.p4.5.m5.1.1" xref="S6.p4.5.m5.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S6.p4.5.m5.1b"><cn type="integer" id="S6.p4.5.m5.1.1.cmml" xref="S6.p4.5.m5.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.5.m5.1c">23</annotation></semantics></math> s. Opportunistic learning converges fast as it does not utilize D2D communications, but experiences a large loss: it is a viable solution only for agents possessing enough data and performing specialized tasks that do not require much re-training.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and future directions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this article we explored opportunities and applications of FL in
networked and automated industrial systems, underpinned by D2D wireless
communications. Open problems and challenges have been discussed,
focusing on manufacturing and automotive B5G verticals. Decentralized FL enables the cooperative learning of ML models. It can be seamlessly integrated into the application
dependent sensing-decision-action loop within each automated entity
to improve knowledge discovery operations. Learning and re-training
continuously to follow the changing dynamics of the environment have
a profound impact on the networking co-design, capitalizing on model
sparsity and superposition properties of the wireless links.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Future research is expected to target increasingly complex mobile
environments characterized by heterogeneous devices cooperating to
learn distinct, but related, functions. The choice between opportunistic and cooperative behaviors will largely depend on the URLLC design. Emerging FL
tools are promising in cooperative automated driving, leveraging V2X
interactions, and in collaborative robotics for distributed learning
in complex and dynamic workflows.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Appendix: Federated data and ML model</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.8" class="ltx_p">As shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the robots are equipped with <math id="Sx1.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="Sx1.p1.1.m1.1a"><mn id="Sx1.p1.1.m1.1.1" xref="Sx1.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.1.m1.1b"><cn type="integer" id="Sx1.p1.1.m1.1.1.cmml" xref="Sx1.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.1.m1.1c">3</annotation></semantics></math> Multiple-Input-Multiple-Output (MIMO) Frequency Modulated Continuous Wave (FMCW) radars, working in the <math id="Sx1.p1.2.m2.1" class="ltx_Math" alttext="77" display="inline"><semantics id="Sx1.p1.2.m2.1a"><mn id="Sx1.p1.2.m2.1.1" xref="Sx1.p1.2.m2.1.1.cmml">77</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.2.m2.1b"><cn type="integer" id="Sx1.p1.2.m2.1.1.cmml" xref="Sx1.p1.2.m2.1.1">77</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.2.m2.1c">77</annotation></semantics></math> GHz band. Radars implement a Time-Division (TD) MIMO system with <math id="Sx1.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="Sx1.p1.3.m3.1a"><mn id="Sx1.p1.3.m3.1.1" xref="Sx1.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.3.m3.1b"><cn type="integer" id="Sx1.p1.3.m3.1.1.cmml" xref="Sx1.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.3.m3.1c">2</annotation></semantics></math> transmit and <math id="Sx1.p1.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="Sx1.p1.4.m4.1a"><mn id="Sx1.p1.4.m4.1.1" xref="Sx1.p1.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.4.m4.1b"><cn type="integer" id="Sx1.p1.4.m4.1.1.cmml" xref="Sx1.p1.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.4.m4.1c">4</annotation></semantics></math> receive antennas each, and a field-of-view of <math id="Sx1.p1.5.m5.1" class="ltx_Math" alttext="120" display="inline"><semantics id="Sx1.p1.5.m5.1a"><mn id="Sx1.p1.5.m5.1.1" xref="Sx1.p1.5.m5.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.5.m5.1b"><cn type="integer" id="Sx1.p1.5.m5.1.1.cmml" xref="Sx1.p1.5.m5.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.5.m5.1c">120</annotation></semantics></math> deg. During the on-line workflow, the distance <math id="Sx1.p1.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx1.p1.6.m6.1a"><mi id="Sx1.p1.6.m6.1.1" xref="Sx1.p1.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.6.m6.1b"><ci id="Sx1.p1.6.m6.1.1.cmml" xref="Sx1.p1.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.6.m6.1c">d</annotation></semantics></math> and DOA <math id="Sx1.p1.7.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="Sx1.p1.7.m7.1a"><mi id="Sx1.p1.7.m7.1.1" xref="Sx1.p1.7.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.7.m7.1b"><ci id="Sx1.p1.7.m7.1.1.cmml" xref="Sx1.p1.7.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.7.m7.1c">\theta</annotation></semantics></math> information are classified by the agents using a trained ML model. The ML model is here trained to classify <math id="Sx1.p1.8.m8.1" class="ltx_Math" alttext="6" display="inline"><semantics id="Sx1.p1.8.m8.1a"><mn id="Sx1.p1.8.m8.1.1" xref="Sx1.p1.8.m8.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="Sx1.p1.8.m8.1b"><cn type="integer" id="Sx1.p1.8.m8.1.1.cmml" xref="Sx1.p1.8.m8.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.8.m8.1c">6</annotation></semantics></math> potential HR collaborative situations characterized by different HR distances and DOA ranges, corresponding to safe or unsafe conditions.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Based on the above setup, a simplified database for testing, is available in the repository:<a href="link" title="" class="ltx_ref ltx_href">http://dx.doi.org/10.21227/0wmc-hq36</a>. The database contains 4 main data structures, detailed as follows:</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">i) mmwave_data_test has dimension 900 x 256 x 63. Contains 900 FFT range-azimuth measurements of size 256 x 63: 256-point range samples corresponding to a max range of 11m (min range of 0.5m) and 63 angle bins, corresponding to AOA ranging from -75 to +75 degree. Data are used for testing (validation database). The corresponding labels are in label_test. Each label (from 0 to 5) corresponds to one of the 6 positions (from 1 to 6) of the operator as detailed in the Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.b.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">ii) mmwave_data_train has dimension 900 x 256 x 63. Contains 900 FFT range-azimuth measurements used for training. The corresponding labels are in label_train.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">iii) label_test with dimension 900 x 1, contains the true labels for test data (mmwave_data_test), namely classes (true labels) correspond to integers from 0 to 5.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">iv) label_train with dimension 900 x 1, contains the true labels for train data (mmwave_data_train), namely classes (true labels) correspond to integers from 0 to 5.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.4" class="ltx_p">The implemented ML model takes as input the raw range-azimuth data (after background subtraction) of size <math id="Sx1.p7.1.m1.1" class="ltx_Math" alttext="256\times 63" display="inline"><semantics id="Sx1.p7.1.m1.1a"><mrow id="Sx1.p7.1.m1.1.1" xref="Sx1.p7.1.m1.1.1.cmml"><mn id="Sx1.p7.1.m1.1.1.2" xref="Sx1.p7.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="Sx1.p7.1.m1.1.1.1" xref="Sx1.p7.1.m1.1.1.1.cmml">×</mo><mn id="Sx1.p7.1.m1.1.1.3" xref="Sx1.p7.1.m1.1.1.3.cmml">63</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p7.1.m1.1b"><apply id="Sx1.p7.1.m1.1.1.cmml" xref="Sx1.p7.1.m1.1.1"><times id="Sx1.p7.1.m1.1.1.1.cmml" xref="Sx1.p7.1.m1.1.1.1"></times><cn type="integer" id="Sx1.p7.1.m1.1.1.2.cmml" xref="Sx1.p7.1.m1.1.1.2">256</cn><cn type="integer" id="Sx1.p7.1.m1.1.1.3.cmml" xref="Sx1.p7.1.m1.1.1.3">63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.1.m1.1c">256\times 63</annotation></semantics></math> from the radars. As shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV Distributed intelligence for cooperative automated driving ‣ Opportunities of Federated Learning in Connected, Cooperative and Automated Industrial Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.a, it consists of <math id="Sx1.p7.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="Sx1.p7.2.m2.1a"><mn id="Sx1.p7.2.m2.1.1" xref="Sx1.p7.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="Sx1.p7.2.m2.1b"><cn type="integer" id="Sx1.p7.2.m2.1.1.cmml" xref="Sx1.p7.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.2.m2.1c">2</annotation></semantics></math> trainable neural network layers of <math id="Sx1.p7.3.m3.1" class="ltx_Math" alttext="25.000" display="inline"><semantics id="Sx1.p7.3.m3.1a"><mn id="Sx1.p7.3.m3.1.1" xref="Sx1.p7.3.m3.1.1.cmml">25.000</mn><annotation-xml encoding="MathML-Content" id="Sx1.p7.3.m3.1b"><cn type="float" id="Sx1.p7.3.m3.1.1.cmml" xref="Sx1.p7.3.m3.1.1">25.000</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.3.m3.1c">25.000</annotation></semantics></math> parameters. Decentralized FL uses gossip or diffusion methods (described previously) with SGD step size <math id="Sx1.p7.4.m4.1" class="ltx_Math" alttext="\mu_{t}=0.025" display="inline"><semantics id="Sx1.p7.4.m4.1a"><mrow id="Sx1.p7.4.m4.1.1" xref="Sx1.p7.4.m4.1.1.cmml"><msub id="Sx1.p7.4.m4.1.1.2" xref="Sx1.p7.4.m4.1.1.2.cmml"><mi id="Sx1.p7.4.m4.1.1.2.2" xref="Sx1.p7.4.m4.1.1.2.2.cmml">μ</mi><mi id="Sx1.p7.4.m4.1.1.2.3" xref="Sx1.p7.4.m4.1.1.2.3.cmml">t</mi></msub><mo id="Sx1.p7.4.m4.1.1.1" xref="Sx1.p7.4.m4.1.1.1.cmml">=</mo><mn id="Sx1.p7.4.m4.1.1.3" xref="Sx1.p7.4.m4.1.1.3.cmml">0.025</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p7.4.m4.1b"><apply id="Sx1.p7.4.m4.1.1.cmml" xref="Sx1.p7.4.m4.1.1"><eq id="Sx1.p7.4.m4.1.1.1.cmml" xref="Sx1.p7.4.m4.1.1.1"></eq><apply id="Sx1.p7.4.m4.1.1.2.cmml" xref="Sx1.p7.4.m4.1.1.2"><csymbol cd="ambiguous" id="Sx1.p7.4.m4.1.1.2.1.cmml" xref="Sx1.p7.4.m4.1.1.2">subscript</csymbol><ci id="Sx1.p7.4.m4.1.1.2.2.cmml" xref="Sx1.p7.4.m4.1.1.2.2">𝜇</ci><ci id="Sx1.p7.4.m4.1.1.2.3.cmml" xref="Sx1.p7.4.m4.1.1.2.3">𝑡</ci></apply><cn type="float" id="Sx1.p7.4.m4.1.1.3.cmml" xref="Sx1.p7.4.m4.1.1.3">0.025</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.4.m4.1c">\mu_{t}=0.025</annotation></semantics></math>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">M. Bennis, et al. “Ultrareliable and Low-Latency
Wireless Communication: Tail, Risk, and Scale,” Proc. of the IEEE,
vol. 106, no. 10, pp. 1834-1853, Oct. 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Konečný J., et al. “Federated optimization:
Distributed machine learning for on-device intelligence,” CoRR,
2016. [Online]. Available: http://arxiv.org/abs/ 1610.02527.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">P Kairouz, et al., “Advances and open problems
in federated learning,” [Online]. Available: https://arxiv.org/abs/1912.04977.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">M. M. Amiri et al., “Federated Learning Over
Wireless Fading Channels,” IEEE Trans. on Wireless Comm., vol. 19,
no. 5, pp. 3546-3557, May 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">J. A Daily, et al., “Gossipgrad: Scalable deep
learning using gossip communication based asynchronous gradient descent,”
[Online]. Available: https://arxiv.org/abs/1803.05880.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">S. Savazzi, et al. “Federated Learning with Cooperating
Devices: A Consensus Approach for Massive IoT Networks,” IEEE Internet
of Things Journal, vol. 7, no. 5, pp. 4641-4654, May 2020. [Online]. Available: https://tinyurl.com/y2kr7rw5.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">A. H. Sayed, et al., “Diffusion Strategies for
Adaptation and Learning over Networks: An Examination of Distributed
Strategies and Network Behavior,” IEEE Signal Proc. Mag., vol. 30,
no. 3, pp.155-171, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">A. Elgabli, et al., “GADMM: Fast and Communication
Efficient Framework for Distributed Machine Learning,” Journal of
Machine Learning Research, vol. 21 pp. 1-390, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">H. Xing, et al. “Decentralized Federated Learning
via SGD over Wireless D2D Networks,” Proc. IEEE SPAWC, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">S. Wang, et al., “Adaptive Federated Learning in
Resource Constrained Edge Computing Systems,” IEEE Journal on Sel.
Areas in Comm., vol. 37, no. 6, pp. 1205-1221, June 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">H. Kim, et al. “Blockchained On-Device Federated
Learning,” IEEE Communications Letters, June 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> M. Chen, et al. “Performance Optimization of Federated
Learning over Wireless Networks,” Proc. IEEE GLOBECOM, Waikoloa,
HI, USA, pp. 1-6, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">R. Nassif, et al. “Multitask Learning Over Graphs:
An Approach for Distributed, Streaming Machine Learning,” IEEE Signal
Proc. Mag., vol. 37, no. 3, pp. 14-25, May 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">M. Brambilla, et al., “Augmenting Vehicle Localization
by Cooperative Sensing of the Driving Environment: Insight on Data
Association in Urban Traffic Scenarios,” IEEE Trans. on Intelligent
Transportation Systems, vol. 21, no. 4, pp. 1646-1663, April 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">S. Kianoush, et al. “A Multisensory Edge-Cloud Platform
for Opportunistic Radio Sensing in Cobot Environments,” IEEE Internet of Things Journal, vol. 8, no. 2, pp. 1154-1168, January 2021.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Stefano Savazzi</span> 
is Researcher at the Consiglio Nazionale delle Ricerche (CNR) with
the Institute of Electronics, Computer and Telecommunication Engineering
(IEIIT). He was visiting researcher with the Uppsala University (2005) and the UCSD, San Diego (2009). He co-authored over 100 scientific publications. Research interests include signal processing, learning and networking design aspects for the Internet of Things and beyond 5G. He is the recipient of the Dimitris N. Chorafas Foundation Award.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Monica Nicoli</span> 
is an Associate Professor at Politecnico di Milano, with the Department of Management, Economics and Industrial Engineering. She was visiting researcher with ENI-Agip (Italy) in 1998-1999 and with Uppsala University (Sweden) in 2001. She co-authored over 100 scientific publications. Her research interests are in the area of statistical signal processing, with focus on wireless communications, localization and smart mobility. She is a recipient of the 1999 Marisa Bellisario Award, and a co-recipient of the 2014 IET Intelligent Transport Systems and the 2018 IEEE Statistical Signal Processing Workshop Best Paper Awards.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Mehdi Bennis</span> 
is an Associate Professor at the Centre for Wireless Communications,
University of Oulu, Finland, an Academy of Finland Research Fellow
and head of the intelligent connectivity and networks/systems group
(ICON). His main research interests are in radio resource management,
heterogeneous networks, game theory and machine learning in 5G networks
and beyond. He was the recipient of several prestigious awards
including the 2015 Fred W. Ellersick Prize from the IEEE Communications
Society, the 2016 Best Tutorial Prize from the IEEE Communications
Society, the 2017 EURASIP Best paper Award for the Journal of Wireless
Communications and Networks, the all-University of Oulu award for
research and the 2019 IEEE ComSoc Radio Communications Committee Early
Achievement Award.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Sanaz Kianoush</span> 
is a Postdoctoral researcher at the IEIIT institute of the CNR since November 2014. Visiting researcher at Aalto University in 2018, lecturer at Azad University Sary (Iran) in 2008.
Her research interests include statistical signal processing, machine
learning in communication systems, device-free radio localization.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab5" class="ltx_float biography">
<table id="tab5.1" class="ltx_tabular">
<tr id="tab5.1.1" class="ltx_tr">
<td id="tab5.1.1.1" class="ltx_td">
<span id="tab5.1.1.1.1" class="ltx_inline-block">
<span id="tab5.1.1.1.1.1" class="ltx_p"><span id="tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Luca Barbieri</span> 
is currently a Ph.D. candidate in Information Technology at DEIB department, Politecnico di Milano. His current research interests focus on machine learning and localization techniques for vehicular
and industrial networks.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2101.03364" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2101.03367" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2101.03367">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2101.03367" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2101.03368" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 13:36:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
