<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1910.13676] Multi Modal Semantic Segmentation using Synthetic Data</title><meta property="og:description" content="Semantic understanding of scenes in three-dimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true di…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi Modal Semantic Segmentation using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi Modal Semantic Segmentation using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1910.13676">

<!--Generated on Sun Mar 17 09:49:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
LiDARs,  point clouds,  multi modal semantic segmentation,  Convolutional neural networks,  Deep Learning,  Autonomous Driving,  synthetic data,  CARLA simulator
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi Modal Semantic Segmentation using Synthetic Data 
<br class="ltx_break">
<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span></span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Kartik Srivastava<sup id="id2.1.id1" class="ltx_sup">*1</sup>,
Akash Kumar Singh<sup id="id3.2.id2" class="ltx_sup">*2</sup>,
Guruprasad M. Hegde<sup id="id4.3.id3" class="ltx_sup">3</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
kartiksrivastava144@gmail.com,
ksakash@iitk.ac.in,
GuruprasadMahabaleshwar.Hegde@in.bosch.com

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Semantic understanding of scenes in three-dimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true distance of separation to objects which are crucial for taking mission critical decisions. As a first step, in this work we investigate the possibility of semantically classifying different parts of a given scene in 3D by learning the underlying geometric context in addition to the texture cues BUT in the absence of labelled real-world datasets. To this end we generate a large number of synthetic scenes, their pixel-wise labels and corresponding 3D representations using CARLA software framework. We then build a deep neural network that learns underlying category specific 3D representation and texture cues from color information of the rendered synthetic scenes. Further on we apply the learned model on different real world datasets to evaluate its performance. Our preliminary investigation of results show that the neural network is able to learn the geometric context from synthetic scenes and effectively apply this knowledge to classify each point of a 3D representation of a scene in real-world.</p>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1.1" class="ltx_sup">1</sup> <span id="footnote1.2" class="ltx_text ltx_font_italic">Department of Computer Science &amp; Information Systems, 
<br class="ltx_break">Birla Institute of Technology and Science, Pilani</span>. Hyderabad, India</span></span></span><span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1a.1" class="ltx_sup">2</sup> <span id="footnote1a.2" class="ltx_text ltx_font_italic">Department of Electrical Engineering, 
<br class="ltx_break">Indian Institute of Technology, Kanpur</span>. Kanpur, India.</span></span></span><span id="footnote1b" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1b.1" class="ltx_sup">3</sup> <span id="footnote1b.2" class="ltx_text ltx_font_italic">Robert Bosch Research and Technology Center</span>. Bangalore, India.</span></span></span><span id="footnote1c" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1c.1" class="ltx_sup">*</sup> Work done while employed as an intern at Robert Bosch Research and Technology Center, Bangalore.</span></span></span>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
LiDARs, point clouds, multi modal semantic segmentation, Convolutional neural networks, Deep Learning, Autonomous Driving, synthetic data, CARLA simulator

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Semantic segmentation and understanding of a scene is one of the fundamental requirements for any robotics applications. The task of semantic segmentation involves classifying each element of a scene to a set of predefined categories/classes. For instance, in a point wise representation of a 3D world, the task involves classifying each point to a specific category. The role of semantic segmentation and challenges thereby is further emphasized in mission critical tasks such as self-driving technologies. For e.g. during path planning stage it is important to understand pose, size and distance to an object and make a decision whether to apply brakes or negotiate with the obstacle.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/2d.jpg" id="S1.F1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="120" height="90" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/2_front_annotate.jpg" id="S1.F1.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="333" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1910.13676/assets/images/carla_training_data/statistics/01_007_0000600_13_class_3D_GT.jpg" id="S1.F1.g3" class="ltx_graphics ltx_figure_panel ltx_img_square" width="538" height="465" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1910.13676/assets/images/carla_training_data/01_007_0000600_13_class_3D.jpg" id="S1.F1.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="380" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.5.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text" style="font-size:90%;">a)Rendered Synthetic image, b)Multi modal image consisting of 3D and color information, c)Ground Truth semantic labels, d) Our multi-modal semantic segmentation results </span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Research community has made tremendous progress in developing semantic segmentation methods in the two dimensional (2D) image space using Deep Neural Networks (DNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However segmentation based on image information alone might not be sufficient for applications like self-driving since its difficult to obtain accurate 3D information such as pose, size, distance to an object to make critical decisions. In addition, color information could be misleading as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>a which are synthetically generated overcast/rainy scene. We can observe both road (denoted by A) and wall (denoted by B) have similar color and might confuse the segmentation algorithm while assigning a class to a pixel.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the above shortcomings of image based scene segmentation, self-driving technologies from companies like Waymo<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, Lyft<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> use multiple perception modules such as Light Detection And Ranging (LiDAR) devices in addition to cameras. LiDARs work based on active illumination and represent a scene in discrete set of points in 3D, also known as a Point Clouds (PC). For instance, in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>b the LiDAR information is fused with color information which helps to distinguish horizontal road and vertical wall based on their distinct orientation in 3D, resulting in better segmenting as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>d. The corresponding Ground Truth (GT) labels are shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>c where each class is represented by a distinct color (e.g. Road is represented by dark pink and Wall is represented by Gray). Based on the above factors it is important to develop and extend existing techniques to handle 3D representation of a scene such as PC inputs in addition to images. Currently there is far less work on how to adapt 2D image based segmentation techniques to 3D point clouds. One of the major bottlenecks being non-availability of labelled real world PC datasets.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Considering the above mentioned factors, in this work we investigate the usability of computer generated labelled synthetic datasets for semantic segmentation of PC. Along this direction our contributions include:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Developing a run-time and memory efficient implementation of state of art PC segmentation algorithm — Pointnet++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and is described in section <a href="#S3.SS4" title="III-D Runtime and memomry optimizations ‣ III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Validating the effectiveness of resulting segmentation algorithm to learn geometric cues from synthetic datasets and transferring the learnt knowledge to real world datasets as presented in section <a href="#S4" title="IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a></p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper we first briefly review some of the publicly available labelled 3D point cloud datasets and methods in 3D semantic segmentation in section <a href="#S2" title="II Related Work ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, followed by problem description and proposed solution and contributions in section <a href="#S3" title="III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We deliberate on experiments and results in section <a href="#S4" title="IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and finally conclude in section <a href="#S5" title="V Conclusion and Future Work ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Datasets for 3D semantic segmentation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There are a few available labelled datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, but they suffer from lack of sufficient representation of relevant classes for self driving applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> or consist of few labelled points<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The most recent Sem-kitti <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Synth-city <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> datasets are promising but suffer from unavailability of labelled data for rare events (e.g. collision between vehicles) which are crucial to evaluate the practicality of developed techniques in the context of self-driving.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To study the usability of synthetic data for semantic segmentation of real world point clouds, in this work we test our approaches predominantly on KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which is closer to our use case and will be described in section <a href="#S4" title="IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Segmentation of Point clouds</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Semantic segmentation of 3D point clouds has been one of the most well researched problem. Related techniques involve non-parametric approaches based on region growing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, segmenting based on graph partitioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, robust statistics to fit a pre-defined parametric model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Most of these methods perform well on constrained data sets with very limited variation in operating conditions. With the advent of large scale image data sets with high variations and the potential shown by DNN in semantically segmenting them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, it is natural that most of the recent point cloud segmentation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> are exploring similar ideas.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Most of these techniques initially transform a 3D PC to a suitable representation like Voxel grid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or to images using projection techniques<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and then apply well studied methods based on DNN for images. Since the processing is performed on a derived representation of raw point clouds which incurs loss of information, it might not be the most effective way to understand the benefits of synthetic data. For this reason we investigate Pointnet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> which directly process raw 3D point clouds and will be discussed in <a href="#S3" title="III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Problem statement and Solution</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As discussed in section <a href="#S2.SS2" title="II-B Segmentation of Point clouds ‣ II Related Work ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> it is well proven that fully supervised techniques based on DNN are ideal for semantic segmentation of images and some of the early work show potential by extending these techniques to segment 3D point clouds. However there are three major bottlenecks which hinders the progress of research in the field of 3D PC segmentation. 1) Sensor setup to acquire 3D PC that are suitable for self-driving applications is extremely expensive. 2) Complexity and thereby huge costs involved in manual labelling of PC and 3) which partly is a consequence of 1) and 2) is the scarcity of real-world labelled PC data in both normal and rare events. In the following section we propose our solution to address the listed problems.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To overcome the problem of scarcity of labelled data for normal and rare events we propose to leverage synthetic data. The synthetic data generator should represent a realistic operating environment of self-driving vehicle and should be able to provide labels for the underlying classes. It should also provide a programming interface to enable generation of rare events such as vehicle collisions. To fulfill the above conditions we leverage the CARLA simulator<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The detailed description of CARLA is provided in section <a href="#S3.SS1" title="III-A Synthetic data generation ‣ III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To reduce efforts in labelling of real-world 3D PC we propose a coarse to fine approach. The coarse labelling is provided by initially learning 3D representations from synthetic data using DNN that directly works on 3D points. The learnt model is then applied on real-world 3D PC to obtain coarse labels. The coarse labels could then be further refined or corrected manually. This could potentially save huge amounts of manual labor and consequently minimize cost of labelling. The details of DNN employed in this work is provided in section <a href="#S3.SS3" title="III-C Deep Neural Network for point cloud segmentation ‣ III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Synthetic data generation</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Sensor suite </h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">In this work we use CARLA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, an open source driving simulator for self-driving research to generate synthetic data. CARLA simulates commonly used sensors in autonomous driving such as cameras and LiDARs as shown in Fig.<a href="#S3.F2" title="Figure 2 ‣ III-A1 Sensor suite ‣ III-A Synthetic data generation ‣ III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1910.13676/assets/images/carla_training_data/multi-view_combined.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">A snapshot of sensors rendered in CARLA. The available sensor information are Color image, Semantic labels of 2D image, LiDAR and depth camera (from top-left in clockwise manner). Zoomed view of LiDAR is shown in the right image</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">We fuse the point clouds with color information and semantic labels to obtain labelled 3D PC. The details of generated dataset will be provided in section <a href="#S4.SS1" title="IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Scenario Generation using CARLA simulator</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Rare scenarios such as accidents and near misses do not appear in datasets in abundance in real world. To address this gap we generate scenarios in CARLA using the provided application programming interface and include them as part of our training and validation set. An example scenario of two cars crashing each other is shown in Fig. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:scenario_generation</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The simulator offers flexibility to configure different weather conditions, actor (e.g. pedestrians, vehicles) density and town layouts. We use these configuration parameters to create a diverse set of synthetic dataset. More specifically we performed the following procedure:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Define the actors in the scenario, along with their desired behaviour (run in a straight line or autopilot motion) and location of spawning in the CARLA world.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Define the ego-vehicle along with it's initial spawning point and sensors for collecting data.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Define the behaviour of the actors and when to trigger them with respect to the location of the ego-vehicle.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Create the CARLA server and launch the actors and ego-vehicles in the simulation.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Maneuver the ego-vehicle along the desired path and capture the scenario of how the actors respond to the ego-vehicle by dumping the data from the sensors.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F3" class="ltx_figure ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1910.13676/assets/images/scenarios/Car_crash/00053366.jpg" id="S1.F0.sf4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="152" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1910.13676/assets/images/scenarios/Car_crash/00053382.jpg" id="S1.F0.sf4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="152" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">Generated scenario of two cars crashing into each other</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Deep Neural Network for point cloud segmentation </span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this work we base our investigation on Open3D implementation of PointNet++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> which is one of the popular DNN architecture that takes point clouds as input and predicts class labels for each individual point.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">A typical LiDAR point cloud consists of a few million 3D points. Considering the fact that these data points needs to be represented as data type double, the required memory footprint to load the whole data-set at once, for the purpose of training a DNN becomes impractical and sluggish assuming a standard desktop RAM of 8GB. Even in case of batch-wise loading from a secondary memory like hard drive the process of reading every batch sequentially while training is too time consuming. To address this gap in default implementation, we developed an efficient way to generate and load batches of data as described below.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Runtime and memomry optimizations</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The main idea is to store very limited data on RAM which are accessed from a parallel process at the time of training for generating and feeding continuous batches of data. The pseudo code of our method is shown in Algorithm 1 below.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Training Data Stacking Algorithm</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold">Result:</span> <span id="alg1.l1.3" class="ltx_text ltx_font_italic">Queue</span> : queue to store batches for training

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">Initialize:</span>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text ltx_font_italic">Queue</span> : queue to store batches for training

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.2" class="ltx_text ltx_font_italic">QueueLimit</span> : limit on the size of the queue

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.2" class="ltx_text ltx_font_italic">Buffer</span> : temp buffer before filling the queue

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text ltx_font_italic">BufferLimit</span> : limit on the size of buffer

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text ltx_font_italic">Dataset</span> : class designed to manage the dataset

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text ltx_font_bold">FillQueue</span> :

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text ltx_font_bold">while</span> <span id="alg1.l9.3" class="ltx_text ltx_font_italic">True</span> <span id="alg1.l9.4" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>     <span id="alg1.l10.2" class="ltx_text ltx_font_bold">if</span> <span id="alg1.l10.3" class="ltx_text ltx_font_bold">len</span>(<span id="alg1.l10.4" class="ltx_text ltx_font_italic">Buffer</span>) &lt;  <span id="alg1.l10.5" class="ltx_text ltx_font_italic">BufferLimit</span> <span id="alg1.l10.6" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>         <span id="alg1.l11.2" class="ltx_text ltx_font_italic">Buffer</span><span id="alg1.l11.3" class="ltx_text ltx_font_bold">.</span> <span id="alg1.l11.4" class="ltx_text ltx_font_italic">Append</span>(<span id="alg1.l11.5" class="ltx_text ltx_font_bold">GetBatch</span>)
     
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>     <span id="alg1.l12.2" class="ltx_text ltx_font_bold">for</span> <span id="alg1.l12.3" class="ltx_text ltx_font_italic">P</span> <span id="alg1.l12.4" class="ltx_text ltx_font_bold">in</span> <span id="alg1.l12.5" class="ltx_text ltx_font_italic">Buffer</span> <span id="alg1.l12.6" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>         <span id="alg1.l13.2" class="ltx_text ltx_font_bold">if</span> <span id="alg1.l13.3" class="ltx_text ltx_font_bold">len</span>(<span id="alg1.l13.4" class="ltx_text ltx_font_italic">Queue</span>) <math id="alg1.l13.m1.1" class="ltx_Math" alttext="&gt;=" display="inline"><semantics id="alg1.l13.m1.1a"><mo id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">&gt;=</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><geq id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">&gt;=</annotation></semantics></math> <span id="alg1.l13.5" class="ltx_text ltx_font_italic">QueueLimit</span> <span id="alg1.l13.6" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>              <span id="alg1.l14.2" class="ltx_text ltx_font_bold">return</span>
         
</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span>         <span id="alg1.l15.2" class="ltx_text ltx_font_italic">Queue</span><span id="alg1.l15.3" class="ltx_text ltx_font_bold">.</span> <span id="alg1.l15.4" class="ltx_text ltx_font_italic">Put</span>(<span id="alg1.l15.5" class="ltx_text ltx_font_italic">P</span>)

</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>         <span id="alg1.l16.2" class="ltx_text ltx_font_italic">Buffer</span><span id="alg1.l16.3" class="ltx_text ltx_font_bold">.</span> <span id="alg1.l16.4" class="ltx_text ltx_font_italic">Remove</span>(<span id="alg1.l16.5" class="ltx_text ltx_font_italic">P</span>)
     
</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>     <span id="alg1.l17.2" class="ltx_text ltx_font_italic">time.sleep(0.01)</span>

</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span><span id="alg1.l18.2" class="ltx_text ltx_font_bold">GetBatch: </span>

</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span><span id="alg1.l19.2" class="ltx_text ltx_font_bold">return</span> <span id="alg1.l19.3" class="ltx_text ltx_font_italic">Dataset</span><span id="alg1.l19.4" class="ltx_text ltx_font_bold">.</span> <span id="alg1.l19.5" class="ltx_text ltx_font_italic">SampleBatch</span>

</div>
</div>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">FillQueue</span> method runs in parallel with the training script to continuously feed generated batches of data to the <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">Queue</span> which resides on RAM. We first initialize an array to contain batches of training data. This acts as a buffer for <span id="S3.SS4.p2.1.3" class="ltx_text ltx_font_italic">Queue</span>. Next, the data is appended to <span id="S3.SS4.p2.1.4" class="ltx_text ltx_font_italic">Buffer</span>. Subsequently the data in the <span id="S3.SS4.p2.1.5" class="ltx_text ltx_font_italic">Buffer</span> is pushed to the <span id="S3.SS4.p2.1.6" class="ltx_text ltx_font_italic">Queue</span> and corresponding elements are cleared from the <span id="S3.SS4.p2.1.7" class="ltx_text ltx_font_italic">Buffer</span>. BufferLimit and queue limit are two parameters that limit the size of data stored on RAM so that memory foot print can be controlled. <span id="S3.SS4.p2.1.8" class="ltx_text ltx_font_bold">GetBatch</span> is the method which generates batches of data from secondary memory using information on RAM such as filename, no. of points and location of data points which belongs to the <span id="S3.SS4.p2.1.9" class="ltx_text ltx_font_italic">Dataset</span> class.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">By limiting the amount of data residing on RAM and on-demand creation of batches of data in a parallel manner makes the training process run-time efficient.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We have used three datasets for our experiments - Semantic 3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, KITTI<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and in-house generated dataset using CARLA simulator. Each of these datasets are unique with respect to number of scenes captured, total number of 3D points, number of classes and type of sensor used while data acquisition. The above aspects are summarized in Table <a href="#S4.T1" title="TABLE I ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a></p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Datasets used for Experiments</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.4.1.1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Semantic</span></th>
<td id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.1.1.3.1" class="ltx_text ltx_font_bold ltx_font_italic">Size</span></td>
<td id="S4.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.1.1.4.1" class="ltx_text ltx_font_bold ltx_font_italic">Total Number</span></td>
<td id="S4.T1.4.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.1.1.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Sensor</span></td>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<th id="S4.T1.4.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S4.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.4.2.2.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Classes</span></th>
<td id="S4.T1.4.2.2.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.2.2.4.1" class="ltx_text ltx_font_bold ltx_font_italic">of Points</span></td>
<td id="S4.T1.4.2.2.5" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T1.4.3.3" class="ltx_tr">
<th id="S4.T1.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Semantic 3D</th>
<th id="S4.T1.4.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">8</th>
<td id="S4.T1.4.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30 scenes</td>
<td id="S4.T1.4.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4 billion</td>
<td id="S4.T1.4.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TLS</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">KITTI</th>
<th id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">19</th>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r">93 scenes</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r">1.6 million</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
</tr>
<tr id="S4.T1.4.5.5" class="ltx_tr">
<th id="S4.T1.4.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CARLA</th>
<th id="S4.T1.4.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">12</th>
<td id="S4.T1.4.5.5.3" class="ltx_td ltx_align_center ltx_border_r">4400 scenes</td>
<td id="S4.T1.4.5.5.4" class="ltx_td ltx_align_center ltx_border_r">1.6 billion</td>
<td id="S4.T1.4.5.5.5" class="ltx_td ltx_align_center ltx_border_r">Simulation</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S4.T1.4.6.1" class="ltx_tr">
<th id="S4.T1.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="5">TLS: Terrestrial Laser Scanner, LiDAR: Light Detection And Ranging</th>
</tr>
</tfoot>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1910.13676/assets/images/carla_training_data/statistics/Chart_percentage_points.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="315" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Classwise distribution of labelled points</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We used a standard split of 80%-20% for training and validating the Pointnet++ method on Semantic 3D and CARLA datasets. We used KITTI for testing the effectiveness of trained models using synthetic data generated by CARLA. We register the semantic GT with corresponding PC to form the labelled 3D representation prior to training and evaluation. An example of GT semantic labels are shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b and corresponding 3D labelled PC is shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>c.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For diverse data acquisition in CARLA we spawned ego vehicle mounted with single camera and LiDAR from different locations and drove it on autopilot to cruise through the provided urban and semi-urban town environments. The traffic density was varied between 40 to 80 vehicles and number of pedestrians varied between 10 and 30. In addition, we cycled through 14 different weather conditions. In total we generated a set of 4000 scenes (having over 1.6 billion points). The dataset also consists of 3D PC corresponding to generated rare scenarios as shown in section <a href="#S3.SS2" title="III-B Scenario Generation using CARLA simulator ‣ III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. Considering each time an ego vehicle is spawned at a different location, with varying vehicle and pedestrian density along with change in weather conditions and town settings we ensure that the generated dataset provides a diverse training and validation set.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Since each of the mentioned datasets have different number of semantic classes (Semantic 3D has 8 semantic classes, KITTI has 19 semantic classes and CARLA has 12 semantic classes) it is important to understand the classwise distribution to support quantitave evaluations. We show the classwise distribution of KITTI and CARLA in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> as both of them represent similar operating environment. There are five dominant classes in the two datasets namely Building, Road, Side-walk, Vegetation and Car. Since side-walk class is not present in Semantic-3D we restrict our evaluations to remaining 4 classes and mark the side-walk as unlabelled. The re-mapping of Semantic-3D and KITTI classes to CARLA is shown in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a></p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Mapping of semantic classes of different datasets to common 4 classes</span></figcaption>
<table id="S4.T2.1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Common 4</span></td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Semantic-3D<sup id="S4.T2.1.1.2.1.2.1.1" class="ltx_sup">*</sup></span></td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.3.1" class="ltx_text ltx_font_bold">CARLA<sup id="S4.T2.1.1.2.1.3.1.1" class="ltx_sup">*</sup></span></td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.4.1" class="ltx_text ltx_font_bold">KITTI<sup id="S4.T2.1.1.2.1.4.1.1" class="ltx_sup">*</sup></span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Building</span></td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Building</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Building</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Building</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Road</span></td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Man-made Terrain</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Road, Road-line</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Road</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.5.4.1.1" class="ltx_text ltx_font_bold">Car</span></td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Car</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Car</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Car, Motorcycle,</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">Bus, Bicycle</td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.1.1" class="ltx_text ltx_font_bold">Vegetation</span></td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">High-vegetation,</td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Vegetation</td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Vegetation</td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<td id="S4.T2.1.1.8.7.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">Low-vegetation</td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4">
<sup id="S4.T2.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>All remaining classes were mapped to unlabelled.</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation metrics and criteria</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To assess labeling performance, we used the standard Jaccard Index, commonly known as the PASCAL VOC intersection-over-union metric (IoU)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. To evaluate overall performance of our model, we used the standard mean intersection-over-union metric (<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="mIoU" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1b" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.5" xref="S4.SS2.p1.1.m1.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑚</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝐼</ci><ci id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">𝑜</ci><ci id="S4.SS2.p1.1.m1.1.1.5.cmml" xref="S4.SS2.p1.1.m1.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">mIoU</annotation></semantics></math>), which was calculated by taking the average of IoU values of individual classes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="IoU=\frac{TP}{TP+FP+FN}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mrow id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1a" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.4" xref="S4.E1.m1.1.1.2.4.cmml">U</mi></mrow><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mrow id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.1" xref="S4.E1.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mrow id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml"><mi id="S4.E1.m1.1.1.3.3.2.2" xref="S4.E1.m1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.2.1" xref="S4.E1.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.2.3" xref="S4.E1.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S4.E1.m1.1.1.3.3.1" xref="S4.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml">P</mi></mrow><mo id="S4.E1.m1.1.1.3.3.1a" xref="S4.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.3.3.4" xref="S4.E1.m1.1.1.3.3.4.cmml"><mi id="S4.E1.m1.1.1.3.3.4.2" xref="S4.E1.m1.1.1.3.3.4.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.4.1" xref="S4.E1.m1.1.1.3.3.4.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.4.3" xref="S4.E1.m1.1.1.3.3.4.3.cmml">N</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><times id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2.1"></times><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝐼</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">𝑜</ci><ci id="S4.E1.m1.1.1.2.4.cmml" xref="S4.E1.m1.1.1.2.4">𝑈</ci></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><divide id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3"></divide><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><times id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2.1"></times><ci id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2">𝑇</ci><ci id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><plus id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.1"></plus><apply id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2"><times id="S4.E1.m1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.1.1.3.3.2.1"></times><ci id="S4.E1.m1.1.1.3.3.2.2.cmml" xref="S4.E1.m1.1.1.3.3.2.2">𝑇</ci><ci id="S4.E1.m1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">𝐹</ci><ci id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3">𝑃</ci></apply><apply id="S4.E1.m1.1.1.3.3.4.cmml" xref="S4.E1.m1.1.1.3.3.4"><times id="S4.E1.m1.1.1.3.3.4.1.cmml" xref="S4.E1.m1.1.1.3.3.4.1"></times><ci id="S4.E1.m1.1.1.3.3.4.2.cmml" xref="S4.E1.m1.1.1.3.3.4.2">𝐹</ci><ci id="S4.E1.m1.1.1.3.3.4.3.cmml" xref="S4.E1.m1.1.1.3.3.4.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">IoU=\frac{TP}{TP+FP+FN}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="mIoU=\frac{\sum_{i=1}^{n}IoU_{i}}{n}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1b" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.2.5" xref="S4.E2.m1.1.1.2.5.cmml">U</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mrow id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><msubsup id="S4.E2.m1.1.1.3.2.1" xref="S4.E2.m1.1.1.3.2.1.cmml"><mo id="S4.E2.m1.1.1.3.2.1.2.2" xref="S4.E2.m1.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S4.E2.m1.1.1.3.2.1.2.3" xref="S4.E2.m1.1.1.3.2.1.2.3.cmml"><mi id="S4.E2.m1.1.1.3.2.1.2.3.2" xref="S4.E2.m1.1.1.3.2.1.2.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.3.2.1.2.3.1" xref="S4.E2.m1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.1.3.2.1.2.3.3" xref="S4.E2.m1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.1.3.2.1.3" xref="S4.E2.m1.1.1.3.2.1.3.cmml">n</mi></msubsup><mrow id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.3.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.2.1" xref="S4.E2.m1.1.1.3.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.3.2.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.2.1a" xref="S4.E2.m1.1.1.3.2.2.1.cmml">​</mo><msub id="S4.E2.m1.1.1.3.2.2.4" xref="S4.E2.m1.1.1.3.2.2.4.cmml"><mi id="S4.E2.m1.1.1.3.2.2.4.2" xref="S4.E2.m1.1.1.3.2.2.4.2.cmml">U</mi><mi id="S4.E2.m1.1.1.3.2.2.4.3" xref="S4.E2.m1.1.1.3.2.2.4.3.cmml">i</mi></msub></mrow></mrow><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝑚</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">𝐼</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">𝑜</ci><ci id="S4.E2.m1.1.1.2.5.cmml" xref="S4.E2.m1.1.1.2.5">𝑈</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><divide id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3"></divide><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><apply id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.1.1.cmml" xref="S4.E2.m1.1.1.3.2.1">superscript</csymbol><apply id="S4.E2.m1.1.1.3.2.1.2.cmml" xref="S4.E2.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.1.2.1.cmml" xref="S4.E2.m1.1.1.3.2.1">subscript</csymbol><sum id="S4.E2.m1.1.1.3.2.1.2.2.cmml" xref="S4.E2.m1.1.1.3.2.1.2.2"></sum><apply id="S4.E2.m1.1.1.3.2.1.2.3.cmml" xref="S4.E2.m1.1.1.3.2.1.2.3"><eq id="S4.E2.m1.1.1.3.2.1.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.1.2.3.1"></eq><ci id="S4.E2.m1.1.1.3.2.1.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E2.m1.1.1.3.2.1.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.3.2.1.3.cmml" xref="S4.E2.m1.1.1.3.2.1.3">𝑛</ci></apply><apply id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2"><times id="S4.E2.m1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2.2">𝐼</ci><ci id="S4.E2.m1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3">𝑜</ci><apply id="S4.E2.m1.1.1.3.2.2.4.cmml" xref="S4.E2.m1.1.1.3.2.2.4"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.2.4.1.cmml" xref="S4.E2.m1.1.1.3.2.2.4">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.2.4.2.cmml" xref="S4.E2.m1.1.1.3.2.2.4.2">𝑈</ci><ci id="S4.E2.m1.1.1.3.2.2.4.3.cmml" xref="S4.E2.m1.1.1.3.2.2.4.3">𝑖</ci></apply></apply></apply><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">mIoU=\frac{\sum_{i=1}^{n}IoU_{i}}{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.4" class="ltx_p">where where <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="TP" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><times id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></times><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">𝑇</ci><ci id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">TP</annotation></semantics></math>, <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="FP" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><times id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></times><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">𝐹</ci><ci id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">FP</annotation></semantics></math>, and <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="FN" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><times id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1"></times><ci id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">𝐹</ci><ci id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">FN</annotation></semantics></math> are the numbers of true positive, false positive, and false negative pixels, respectively and <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mi id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><ci id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">n</annotation></semantics></math> is the number of semantic classes of the dataset.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Training setup</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">As mentioned in section <a href="#S3" title="III Problem statement and Solution ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> we use our modified version of Pointnet++ for training and testing the effectiveness of synthetic data. We train the model on point clouds with and without color information denoted as RGB-D<sub id="S4.SS3.p1.4.1" class="ltx_sub"><span id="S4.SS3.p1.4.1.1" class="ltx_text ltx_font_italic">i</span>-*</sub> and D<sub id="S4.SS3.p1.4.2" class="ltx_sub"><span id="S4.SS3.p1.4.2.1" class="ltx_text ltx_font_italic">i</span>-*</sub> respectively. Here * represents the type of training dataset, <span id="S4.SS3.p1.4.3" class="ltx_text ltx_font_italic">i</span> represents the number of training classes (excluding unlabelled class), RGB stands for Red, Green, Blue channel of image and D stands for Depth information. We trained all models until saturation, and used Adam optimizer with learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="float" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.001</annotation></semantics></math>, momentum of <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="float" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">0.9</annotation></semantics></math> and learning rate decay of <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="float" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">0.7</annotation></semantics></math> . All models are trained on a sample size of <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="8192" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">8192</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn type="integer" id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">8192</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">8192</annotation></semantics></math> points per batch.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We trained and tested the Pointnet++ model on Semantic-3D dataset to establish a baseline on a real-world dataset. The results are captured in second row of Table <a href="#S4.T3" title="TABLE III ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. We attribute the high performance of the model mainly due to large population of labelled points for the 4 classes and the quality of static LiDAR scans which is less noisy. Further on we trained the models using the CARLA dataset and tested on Semantic-3D and KITTI datasets. Our aim is to understand how close we can match the baseline performance by training only on the synthetic dataset. The results are captured in rows three and four of Table <a href="#S4.T3" title="TABLE III ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">As expected the model performs well on Semantic-3D due to very high quality of TLS. Also the contribution from color information is minimal which is reflected in less difference in mIOU values of RGB-D and D-model. One of the possible reason cold be that majority of the classes have well defined geometry that is quite distinctive. On the KITTI dataset the performance of model trained on CARLA is lower and can be attributed to the relatively lower quality of LiDAR scan and resulting artifacts due to motion of the ego-vehicle.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Experimental comparison on 4 semantic classes</span></figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<td id="S4.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></td>
<td id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.2.1" class="ltx_text ltx_font_bold">Validation</span></td>
<td id="S4.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.3.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S4.T3.4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.4.1" class="ltx_text ltx_font_bold">mIoU*</span></td>
<td id="S4.T3.4.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.5.1" class="ltx_text ltx_font_bold">mIoU-D*</span></td>
</tr>
<tr id="S4.T3.4.2.2" class="ltx_tr">
<td id="S4.T3.4.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.T3.4.2.2.1.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.2.2.2.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T3.4.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.2.2.3.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T3.4.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.2.2.4.1" class="ltx_text ltx_font_bold">RGB-D</span></td>
<td id="S4.T3.4.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.2.2.5.1" class="ltx_text ltx_font_bold">[epochs]</span></td>
</tr>
<tr id="S4.T3.4.3.3" class="ltx_tr">
<td id="S4.T3.4.3.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T3.4.3.3.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.4.3.3.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.4.3.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.4.3.3.4.1" class="ltx_text ltx_font_bold">[epochs]</span></td>
<td id="S4.T3.4.3.3.5" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Semantic</td>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Semantic</td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Semantic</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">00.92</td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.82</td>
</tr>
<tr id="S4.T3.4.5.5" class="ltx_tr">
<td id="S4.T3.4.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">3D</td>
<td id="S4.T3.4.5.5.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T3.4.5.5.3" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T3.4.5.5.4" class="ltx_td ltx_align_center ltx_border_r">[100]</td>
<td id="S4.T3.4.5.5.5" class="ltx_td ltx_align_center ltx_border_r">[100]</td>
</tr>
<tr id="S4.T3.4.6.6" class="ltx_tr">
<td id="S4.T3.4.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T3.4.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T3.4.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Semantic</td>
<td id="S4.T3.4.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.836</td>
<td id="S4.T3.4.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.809</td>
</tr>
<tr id="S4.T3.4.7.7" class="ltx_tr">
<td id="S4.T3.4.7.7.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T3.4.7.7.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.4.7.7.3" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S4.T3.4.7.7.4" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
<td id="S4.T3.4.7.7.5" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
</tr>
<tr id="S4.T3.4.8.8" class="ltx_tr">
<td id="S4.T3.4.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T3.4.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T3.4.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">KITTI</td>
<td id="S4.T3.4.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7</td>
<td id="S4.T3.4.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.56</td>
</tr>
<tr id="S4.T3.4.9.9" class="ltx_tr">
<td id="S4.T3.4.9.9.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T3.4.9.9.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.4.9.9.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.4.9.9.4" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
<td id="S4.T3.4.9.9.5" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
</tr>
<tr id="S4.T3.4.10.10" class="ltx_tr">
<td id="S4.T3.4.10.10.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4">*Values computed on Test-set</td>
<td id="S4.T3.4.10.10.2" class="ltx_td ltx_border_t"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In Table <a href="#S4.T3" title="TABLE III ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> we see that the model trained using CARLA dataset comes close to the model trained on Semantic 3D dataset in only 20 epochs.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS1.4.1.1" class="ltx_text">IV-D</span>1 </span>Experiment A</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In this experiment we show the inference results of RGB-D<sub id="S4.SS4.SSS1.p1.1.1" class="ltx_sub">4-CARLA</sub> and D<sub id="S4.SS4.SSS1.p1.1.2" class="ltx_sub">4-CARLA</sub> on a scene from KITTI in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-D1 Experiment A ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We train both RGB-D<sub id="S4.SS4.SSS1.p1.1.3" class="ltx_sub">4-CARLA</sub> and D<sub id="S4.SS4.SSS1.p1.1.4" class="ltx_sub">4-CARLA</sub> models after remapping 12 classes of CARLA to 4 common classes as mentioned in column 3 and column 1 of Table <a href="#S4.T2" title="TABLE II ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> respectively. While testing on KITTI we remap the 19 semantic classes in KITTI GT images to 4 common classes as mentioned in column 4 and column 1 of Table <a href="#S4.T2" title="TABLE II ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> respectively. The resulting GT labels are then reprojected to the corresponding PC and is shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-D1 Experiment A ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>b. By visual inspection we can observe from Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-D1 Experiment A ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>c and Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-D1 Experiment A ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>d that RGB-D<sub id="S4.SS4.SSS1.p1.1.5" class="ltx_sub">4-CARLA</sub> has lesser mis-classifications at shorter distances and relatively higher mis-classification at far-away distance as compared to D<sub id="S4.SS4.SSS1.p1.1.6" class="ltx_sub">4-CARLA</sub>. For e.g. Vegetation (marked in green color) is labelled as Car (marked in blue). Since there are few points per object at far away distance, the contribution of mis-classification towards the mIOU is not significant. This is also reflected in fourth row of Table <a href="#S4.T3" title="TABLE III ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> with a higher mIOU score for RGB-D.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/000045_10_original_color.jpg" id="S4.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="120" height="36" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/000045_10_color_4_clas_GTs.png" id="S4.F5.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="398" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1910.13676/assets/images/carla_training_data/000045_10_no_color_4_class.png" id="S4.F5.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="329" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1910.13676/assets/images/carla_training_data/000045_10_color_4_class.png" id="S4.F5.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="309" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.5.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.8.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.9.2" class="ltx_text" style="font-size:90%;">a) Color image of a scene, b) Remapped GT of 19 classes of KITTI to 4 classes, c) Output of D<sub id="S4.F5.9.2.1" class="ltx_sub">4-CARLA</sub>, d) Output of RGB-D<sub id="S4.F5.9.2.2" class="ltx_sub">4-CARLA</sub> </span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS4.SSS2.4.1.1" class="ltx_text">IV-D</span>2 </span>Experiment B</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">To understand the importance of geometric cues we compare our method which uses both geometric and color cues with a 2-D semantic segmentation network that uses only color information. For this purpose we chose pre-trained model of ERF-Net<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and fine-tuned it using the GT semantic labelled images of KITTI after remapping the original 19 classes to 12 classes of CARLA, an example of such a remapping is shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b which is GT semantic labels for scene in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>a. We remapped Terrain, Sky, Rider, Truck, Bus, Motorcycle, Bicycle, and Train classes of KITTI to unlabelled and the remaining classes as listed in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Datasets ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> were unaltered. The GT for KITTI PC are obtained after re-projecting labels in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>b with corresponding PC and is shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>d. The resulting GT PCs are used to evaluate the performance of our multimodal semantic segmentation models. To have a fair comparison we also trained our version of Pointnet++ on CARLA dataset with 12 classes and evaluated the performance on 12 class GT of KITTI PC after remapping. For this we predict all 12 classes using the models trained only on CARLA dataset whereas consider only the 5 major classes of KITTI namely <span id="S4.SS4.SSS2.p1.1.1" class="ltx_text" style="color:#464646;">Building</span>, <span id="S4.SS4.SSS2.p1.1.2" class="ltx_text" style="color:#804080;">Road</span>, <span id="S4.SS4.SSS2.p1.1.3" class="ltx_text" style="color:#F423E8;">Side-walk</span>, <span id="S4.SS4.SSS2.p1.1.4" class="ltx_text" style="color:#6B8E23;">Vegetation</span> and <span id="S4.SS4.SSS2.p1.1.5" class="ltx_text" style="color:#0000FF;">Car</span> for computing mIoU as the remaining classes are very less in population. The results are listed in Table <a href="#S4.T4" title="TABLE IV ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Experimental comparison using All class model</span></figcaption>
<table id="S4.T4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.1" class="ltx_tr">
<td id="S4.T4.4.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></td>
<td id="S4.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.2.1" class="ltx_text ltx_font_bold">Validation</span></td>
<td id="S4.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.3.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S4.T4.4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.4.1" class="ltx_text ltx_font_bold">mIoU*</span></td>
<td id="S4.T4.4.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.5.1" class="ltx_text ltx_font_bold">mIoU-D*</span></td>
</tr>
<tr id="S4.T4.4.2.2" class="ltx_tr">
<td id="S4.T4.4.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.T4.4.2.2.1.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T4.4.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.2.2.2.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T4.4.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.2.2.3.1" class="ltx_text ltx_font_bold">Set</span></td>
<td id="S4.T4.4.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.2.2.4.1" class="ltx_text ltx_font_bold">RGB-D</span></td>
<td id="S4.T4.4.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.2.2.5.1" class="ltx_text ltx_font_bold">[epochs]</span></td>
</tr>
<tr id="S4.T4.4.3.3" class="ltx_tr">
<td id="S4.T4.4.3.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T4.4.3.3.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T4.4.3.3.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T4.4.3.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.4.3.3.4.1" class="ltx_text ltx_font_bold">[epochs]</span></td>
<td id="S4.T4.4.3.3.5" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T4.4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CARLA</td>
<td id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">KITTI</td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.6</td>
<td id="S4.T4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
</tr>
<tr id="S4.T4.4.5.5" class="ltx_tr">
<td id="S4.T4.4.5.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T4.4.5.5.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T4.4.5.5.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T4.4.5.5.4" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
<td id="S4.T4.4.5.5.5" class="ltx_td ltx_align_center ltx_border_r">[20]</td>
</tr>
<tr id="S4.T4.4.6.6" class="ltx_tr">
<td id="S4.T4.4.6.6.1" class="ltx_td ltx_align_left ltx_border_t" colspan="5">*On Test set classes - Building, Vegetation, Road, Sidewalk, Cars</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">It can be seen form Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>e and Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>f that both RGB-D<sub id="S4.SS4.SSS2.p2.1.1" class="ltx_sub">12-CARLA</sub> and D<sub id="S4.SS4.SSS2.p2.1.2" class="ltx_sub">12-CARLA</sub> are able to segment the side-walk with reasonable accuracy, whereas the inference from ERF-Net (using only the color image in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>a) as shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>c mis-classifies side-walk as Road. We speculate the presence of shadows and similar color of road surface and side walk as the main reason. Additionally when we induce a geometric cue from 3D PC, maybe the DNN is able to learn subtle characteristics such as relative difference in elevation (e.g.side-walk is above road), discontinuity in surface normal orientation at the border of two parallel planar surfaces (road and sidewalk) which leads to treat them as two different objects.</p>
</div>
<div id="S4.SS4.SSS2.p3" class="ltx_para">
<p id="S4.SS4.SSS2.p3.1" class="ltx_p">The <span id="S4.SS4.SSS2.p3.1.1" class="ltx_text" style="color:#BE9999;">Fence</span> on the extreme right is mis-classified as Vegetation by all models as shown in Fig.<a href="#S4.F6" title="Figure 6 ‣ IV-D2 Experiment B ‣ IV-D Results ‣ IV Experiments and Results ‣ Multi Modal Semantic Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, primarily due to very less representation in the training set. D<sub id="S4.SS4.SSS2.p3.1.2" class="ltx_sub">12-CARLA</sub> fails to identify the car at the far end at the middle of the street whereas the RGB-D<sub id="S4.SS4.SSS2.p3.1.3" class="ltx_sub">12-CARLA</sub> is able to better classify the far away car. This suggests both color and geometric information is necessary for better semantic segmentation of a 3D Point Cloud.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/000025_10_rgb.png" id="S4.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="120" height="37" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/000025_10_remapped_GT_12_class.jpg" id="S4.F6.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="181" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/000025_10_ERF_12_class.jpg" id="S4.F6.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="120" height="60" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1910.13676/assets/images/carla_training_data/000025_10_semseg_pc_12_class_GT.png" id="S4.F6.g4" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="561" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_pagination ltx_figure_panel ltx_role_start_2_columns"></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1910.13676/assets/images/carla_training_data/000025_10_semseg_pc_12_class_4_epoch_no_color.png" id="S4.F6.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="465" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.6.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1910.13676/assets/images/carla_training_data/000025_10_semseg_pc_12_class_4_epoch_color.png" id="S4.F6.g6" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="517" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.7.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_pagination ltx_figure_panel ltx_role_end_2_columns"></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.10.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.11.2" class="ltx_text" style="font-size:90%;">a) Color image of a scene, b) Remapped GT labels of KITTI to 12 class , c) Output of ERF-Net , d) Pointcloud representation of b), e) Output of D<sub id="S4.F6.11.2.1" class="ltx_sub">12-CARLA</sub>, f) Output of RGB-D<sub id="S4.F6.11.2.2" class="ltx_sub">12-CARLA</sub> </span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we have shown that it is possible to effectively make use of synthetic data to segment real world 3D Point clouds using multiple modalities namely images and point clouds. In this regard we generated synthetic dataset using CARLA and trained a DNN which is an in-house optimized version of Pointnet++. From our experiments we found that segmentation of static classes such as roads, side-walk, building and objects with rigid geometry such as Cars generalize well when tested on real-world datasets. We also showed that it is possible to obtain better results by training DNN using both color and 3D Point clouds as compared to using either one of them. The methods presented in this work emphasize the role of synthetic datasets especially in cases like segmentation of 3D PC where its difficult to find real-world labelled data and secondly involves huge costs for manual labelling. In such a case the DNN trained on synthetic data can be used to pre-label a large corpus of real-world dataset which potentially would lessen the labelling efforts and thereby significantly reduce labeling cost.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">As part of future work we would like to extend and validate our proposed solution on a much larger corpus of synthetic and real-world datasets.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> https://waymo.com/open

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> https://level5.lyft.com/dataset/

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> http://www.semantic3d.net

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> carla.org

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> https://www.cs.cmu.edu/ vmr/datasets/

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Huang, X.,et.al. <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">The apolloscape dataset for autonomous driving.</span> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 954-960), 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Griffiths, D. and Boehm, J., 2019. SynthCity: A large scale synthetic point cloud. arXiv preprint arXiv:1907.04758.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> M. Everingham, et.al. <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">The PASCAL visual object classes challenge: A retrospective.</span> IJCV, 111(1):98–136, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> https://www.cityscapes-dataset.com/benchmarks/

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> http://www.cvlibs.net/datasets/kitti/

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> http://semantic-kitti.org/

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> R. Unnikrishnan and M. Hebert, “Robust extraction of multiple structures from non-uniformly sampled data,” in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, 2003, pp. 1322-1329

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> R. L. Hoffman and A. K. Jain, “Segmentation and classification of range images,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 9, no. 5, pp. 608–620, Sep. 1987

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> Guruprasad M. Hegde, Cang Ye, ”A Recursive Planar Feature Extraction Method for 3D Range Data Segmentation,” IEEE International Conference on Systems, Man, and Cybernetics, 2011.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> V. Don and H. Maarten Uijt de, “Near real-time extraction of planar features from 3d flash-ladar video frames,” in Proc. SPIE, vol. 6977 of Optical Pattern Recognition, pp. 69770N-69770N-12, 2008.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> W. Tao, H. Jin and Y. Zhang “Color Image Segmentation Based on Mean Shift and Normalized Cuts,” IEEE Transactions on Systems, Man, and Cybernetics, part B, vol. 37, no. 5, pp. 1382-1389, 2007.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Jing Huang and Suya You. <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Point Cloud Labeling using 3D Convolutional Neural Network.</span> In Proc. of the Intl. Conf. on Pattern Recognition, 2016

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Charles R. Qi et.al. <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.</span> In Proc. of Conf. on Neural Information Processing Systems, 2017

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">BichenWu et.al. <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud.</span> Proc. of the IEEE Intl. Conf. on Robotics &amp; Automation, 2019

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">https://github.com/intel-isl/Open3D-PointNet2-Semantic3D

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Romera, et.al: <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Erfnet: Efﬁcient residual factorized convnet for real-time semantic segmentation.</span> IEEE Trans. on Intelligent Transportation Systems, 2018

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1910.13675" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1910.13676" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1910.13676">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1910.13676" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1910.13678" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 09:49:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
