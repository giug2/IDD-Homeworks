<!DOCTYPE html>
<html lang="de">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On Instruction-Finetuning Neural Machine Translation Models</title>
<!--Generated on Mon Oct  7 23:24:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.05553v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S1" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S2" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S2.SS1" title="In 2 Related Work ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Instruction Finetuning of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S2.SS2" title="In 2 Related Work ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Customizing Translation Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S3" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Instruction Finetuning of NMT models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S3.SS1" title="In 3 Instruction Finetuning of NMT models ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S3.SS2" title="In 3 Instruction Finetuning of NMT models ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Instruction Finetuning Recipe</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S3.SS3" title="In 3 Instruction Finetuning of NMT models ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Protocol</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.SS1" title="In 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.SS2" title="In 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Task-Specific Data Curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.SS3" title="In 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Finetuning and Evaluation Settings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S5" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S5.SS1" title="In 5 Results and Analysis ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Instruction-Following Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S5.SS2" title="In 5 Results and Analysis ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Zero-Shot Composition of Instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S5.SS3" title="In 5 Results and Analysis ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Extrinsic Evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S5.SS4" title="In 5 Results and Analysis ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>General Translation Quality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.SS1" title="In 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Ablating Parallel Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.SS2" title="In 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Ablating Vocabulary Expansion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S7" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S8" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A1" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix A</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A2" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Appendix B</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A3" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Appendix C</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A4" title="In On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Appendix D</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="de">
<h1 class="ltx_title ltx_title_document">On Instruction-Finetuning Neural Machine Translation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vikas Raunak  Roman Grundkiewicz  Marcin Junczys-Dowmunt
<br class="ltx_break"/>Microsoft Azure AI
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{viraunak,rogrundk,marcinjd}@microsoft.com
<br class="ltx_break"/></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Zusammenfassung</h6>
<p class="ltx_p" id="id2.id1"><span class="ltx_text" id="id2.id1.1">In this work, we introduce instruction finetuning for Neural Machine Translation (NMT) models, which distills instruction following capabilities <span class="ltx_text ltx_font_italic" id="id2.id1.1.1">from</span> Large Language Models (LLMs) <span class="ltx_text ltx_font_italic" id="id2.id1.1.2">into</span> orders-of-magnitude smaller NMT models.
Our instruction-finetuning recipe for NMT models enables customization of translations for a limited but disparate set of translation-specific tasks.
We show that NMT models are capable of following multiple instructions simultaneously and demonstrate capabilities of zero-shot composition of instructions.
We also show that through instruction finetuning, traditionally disparate tasks such as formality-controlled machine translation, multi-domain adaptation as well as multi-modal translations can be tackled jointly by a single instruction finetuned NMT model, at a performance level comparable to LLMs such as GPT-3.5-Turbo.
To the best of our knowledge, our work is among the first to demonstrate the instruction-following capabilities of traditional NMT models, which allows for faster, cheaper and more efficient serving of customized translations.</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Instruction-finetuned Large Language Models (LLMs) demonstrate the remarkable ability of instruction-following <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib33" title="">2021</a>)</cite>, which makes them amenable to tackle any task cast as natural language generation, even under a zero-shot setting.
In this work, we explore whether traditional Neural Machine Translation (NMT) models could offer <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">similar</span> capabilities of following instructions. NMT models could be considered as domain-specific ‘language’ models
<span class="ltx_text ltx_font_italic" id="S1.p1.1.2">pre-trained</span> for a single task (translation) and thereby <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">could</span> be instruction-finetuned to tackle translation-adjacent tasks such as translation customization or enforcing certain specifications on the translations.
Such tasks, e.g., formality-controlled translation <cite class="ltx_cite ltx_citemacro_cite">Schioppa et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib28" title="">2021</a>)</cite>, multi-modal translation <cite class="ltx_cite ltx_citemacro_cite">Elliott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib4" title="">2016</a>)</cite> or gender-based translation rewriting <cite class="ltx_cite ltx_citemacro_cite">Kuczmarski and Johnson (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib11" title="">2018</a>)</cite>, have typically been tackled through specialized models or algorithms in prior literature, rather than a single instruction-following NMT model. In contrast, we instruction-finetune a single <span class="ltx_text ltx_font_italic" id="S1.p1.1.4">ancestral</span> translation model to <span class="ltx_text ltx_font_italic" id="S1.p1.1.5">adapt</span> the translations based on instructions. Our contributions are as follows:</p>
</div>
<div class="ltx_para" id="S1.p2">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a new recipe for instruction finetuning NMT models (trained with supervision only on parallel datasets), which allows for joint modeling of disparate translation customization tasks in a single NMT model, and we analyze the criticality of each of the recipe components through ablation experiments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate that NMT models are capable of following multiple (30+) instructions simultaneously. We also find that NMT models show abilities of zero-shot composition of instructions, as an effect of finetuning.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We show that, with a single instruction-finetuned NMT model, traditional customization tasks such as formality-controlled machine translation can be tackled with high performance, in conjunction with several disparate tasks.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Additionally, our proposed finetuned NMT model outperforms GPT-3.5-Turbo on average on the IWSLT-22 Formality Control Shared Task <cite class="ltx_cite ltx_citemacro_cite">Antonios et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib1" title="">2022</a>)</cite>, while simultaneously achieving high-performance on others &amp; demonstrating a few other <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">desirable</span> properties vis-à-vis much larger LLMs. At a high-level, our work re-interprets a NMT model as a language model and demonstrates the utility of instruction finetuning NMT model for jointly modeling a myriad of disparate translation-related tasks.
In the next sections, we elaborate on our recipe for instruction-finetuning of a NMT model and analyze its characteristics.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.2.1.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.1.1">
<span class="ltx_p" id="S1.T1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.2.1.1.1.1.1">Instruction Prefix</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.2.1.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.2.1">
<span class="ltx_p" id="S1.T1.1.2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.2.1.2.1.1.1">Source (English)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.2.1.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.3.1">
<span class="ltx_p" id="S1.T1.1.2.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.2.1.3.1.1.1">Translation (German)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.3.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S1.T1.1.3.1.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.1.1.1">
<span class="ltx_p" id="S1.T1.1.3.1.1.1.1"><em class="ltx_emph ltx_font_italic" id="S1.T1.1.3.1.1.1.1.1">past tense</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S1.T1.1.3.1.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.1.2.1">
<span class="ltx_p" id="S1.T1.1.3.1.2.1.1">The finished effect <span class="ltx_text" id="S1.T1.1.3.1.2.1.1.1" style="background-color:#FF8000;">is</span> long-lasting and highly glossy – but does it damage the nails?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S1.T1.1.3.1.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.1.3.1">
<span class="ltx_p" id="S1.T1.1.3.1.3.1.1">Der fertige Effekt <span class="ltx_text" id="S1.T1.1.3.1.3.1.1.1" style="background-color:#FF8000;">war</span> langanhaltend und hochglänzend – aber beschädigte er die Nägel?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.4.2.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.2.1.1">
<span class="ltx_p" id="S1.T1.1.4.2.1.1.1"><em class="ltx_emph ltx_font_italic" id="S1.T1.1.4.2.1.1.1.1">informal</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.4.2.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.2.2.1">
<span class="ltx_p" id="S1.T1.1.4.2.2.1.1">Do you like Legos? did you ever play with them as a child or even later?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.4.2.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.2.3.1">
<span class="ltx_p" id="S1.T1.1.4.2.3.1.1"><span class="ltx_text" id="S1.T1.1.4.2.3.1.1.1" style="background-color:#FF8000;">Magst du</span></span>
<span class="ltx_p" id="S1.T1.1.4.2.3.1.2">Legosteine? <span class="ltx_text" id="S1.T1.1.4.2.3.1.2.1" style="background-color:#FF8000;">Hast du</span> jemals als Kind oder sogar später mit ihnen gespielt?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.5.3.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.3.1.1">
<span class="ltx_p" id="S1.T1.1.5.3.1.1.1"><em class="ltx_emph ltx_font_italic" id="S1.T1.1.5.3.1.1.1.1">fix misspelling</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.5.3.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.3.2.1">
<span class="ltx_p" id="S1.T1.1.5.3.2.1.1">To switch between environments, update the storage.json file with the URL of the <span class="ltx_text" id="S1.T1.1.5.3.2.1.1.1" style="background-color:#FF8000;">specificrrbzpronment</span>.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.5.3.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.3.3.1">
<span class="ltx_p" id="S1.T1.1.5.3.3.1.1">Um zwischen Umgebungen zu wechseln, aktualisieren Sie die Datei storage.json mit der URL des <span class="ltx_text" id="S1.T1.1.5.3.3.1.1.1" style="background-color:#FF8000;">spezifischen Prozesses</span>.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.4">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.6.4.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.4.1.1">
<span class="ltx_p" id="S1.T1.1.6.4.1.1.1"><em class="ltx_emph ltx_font_italic" id="S1.T1.1.6.4.1.1.1.1">translate "herbal medicinesto "Kräutermedizin"missing</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.6.4.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.4.2.1">
<span class="ltx_p" id="S1.T1.1.6.4.2.1.1">Chinese <span class="ltx_text" id="S1.T1.1.6.4.2.1.1.1" style="background-color:#FF8000;">herbal medicines</span> for hypothyroidism</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S1.T1.1.6.4.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.4.3.1">
<span class="ltx_p" id="S1.T1.1.6.4.3.1.1">Chinesische <span class="ltx_text" id="S1.T1.1.6.4.3.1.1.1" style="background-color:#FF8000;">Kräutermedizin</span> gegen Hypothyreose</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.1.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="178" id="S1.T1.1.1.1.1.g1" src="extracted/5908747/multi30k.png" width="120"/>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.1.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.2.1">
<span class="ltx_p" id="S1.T1.1.1.2.1.1">A trendy girl talking on her cellphone while gliding slowly down the street.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.1.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.3.1">
<span class="ltx_p" id="S1.T1.1.1.3.1.1">Ein schickes Mädchen telefoniert, während sie langsam die Straße entlangschwebt.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Tabelle 1: </span>Input-output instances for the developed instruction finetuned NMT model.
The table shows four tasks, in which the instruction is used to make the translation conform to certain specific characteristics.
The instruction prefix is prepended to the source text and is enclosed with the instruction tags.
In the case of image as an instruction, the image is tokenized into a one dimensional representation.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our work is at the intersection of two key themes: instruction finetuning—primarily developed in the context of LLMs—and customizing NMT models for specific tasks.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Instruction Finetuning of LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Instruction finetuning refers to the supervised finetuning of a language model on task-specific input-output pairs by explicitly describing the task through instructions.
This has been demonstrated to aid in cross-task generalization <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib25" title="">2022a</a>); Longpre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib14" title="">2023</a>)</cite>, in particular, imparting LLMs with instruction-following capabilities <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib33" title="">2021</a>)</cite>.
A number of prior works have proposed different algorithms for constructing the instruction data <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib15" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib32" title="">2022</a>); Honovich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib6" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib31" title="">2023</a>); Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib26" title="">2022b</a>); Muennighoff et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib16" title="">2023</a>); Iyer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib8" title="">2023</a>); Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib3" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In our recipe, we rely on a combination of parallel data filtering and synthetic data generation through LLMs to construct the instruction dataset that is leveraged for finetuning NMT models.
Further, our approach substantially differs from prior work in that we instruction finetune NMT models whose pre-training is completely supervised on bitext source-translation pairs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Customizing Translation Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There exists a large body of work in adapting NMT models and customizing them for specific use cases such as for achieving high-performance on specific domains <cite class="ltx_cite ltx_citemacro_cite">Saunders (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib27" title="">2022</a>)</cite>, tones or registers in the target language <cite class="ltx_cite ltx_citemacro_cite">Nădejde et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib17" title="">2022</a>)</cite> as well as for tasks such as gender-based translation rewriting <cite class="ltx_cite ltx_citemacro_cite">Rarrick et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib22" title="">2023</a>)</cite>.
Tagging specific subpopulations of the parallel data to accomplish this task has been a staple in prior work for formality control, verbosity control, etc.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Our work is related to the tagging approaches developed in the literature but differs in two key aspects:
(a) task diversity and scale: typically, tagging is only applied to supply information pertaining to a single task, while instruction finetuning as a technique aspires to tackle a wide variety of tasks in a unified modeling approach to make the model capable of following a wide variety of instructions;
and (b) natural language instruction: instead of manipulating tags or combination of tags, we leverage instructions expressed or composed in natural language for influencing the translations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Instruction Finetuning of NMT models</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we describe the problem setting along with our instruction finetuning recipe and evaluation protocol.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Setting</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For instruction finetuning, we take a pre-trained NMT model and finetune it with instruction annotated source-translation pairs.
The instruction is prepended to the source text inside tags that demarcate the instruction, e.g., <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">&lt;instruction&gt; informal &lt;/instruction&gt;</span>.
Henceforth, we refer to the tokens pertaining to the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">&lt;instruction&gt;</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">&lt;/instruction&gt;</span> strings as the instruction tokens.
A collection of instruction and source-translation instances are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S1.T1" title="Tabelle 1 ‣ 1 Introduction ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">1</span></a>.
Through instruction finetuning, we hope to jointly model a range of disparate tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Instruction Finetuning Recipe</h3>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.2">
<div class="ltx_listingline" id="algorithm1.2.1">
</div>
<div class="ltx_listingline" id="algorithm1.2.2">
</div>
<div class="ltx_listingline" id="algorithm1.2.3">
<span class="ltx_text" id="algorithm1.2.3.1"><span class="ltx_text ltx_font_bold" id="algorithm1.2.3.1.1">Data:</span> </span>Base NMT Model and Vocabulary
</div>
<div class="ltx_listingline" id="algorithm1.2.4">
<span class="ltx_text" id="algorithm1.2.4.1"><span class="ltx_text ltx_font_bold" id="algorithm1.2.4.1.1">Result:</span> </span>Instruction Finetuned NMT Model
</div>
<div class="ltx_listingline" id="algorithm1.2.5">
</div>
<div class="ltx_listingline" id="algorithm1.2.6">
</div>
<div class="ltx_listingline" id="algorithm1.2.7">
<span class="ltx_text ltx_font_bold" id="algorithm1.2.7.1">Step 1:</span>  Expand vocabulary with instruction tokens 
</div>
<div class="ltx_listingline" id="algorithm1.2.8">
</div>
<div class="ltx_listingline" id="algorithm1.2.9">
<span class="ltx_text ltx_font_bold" id="algorithm1.2.9.1">Step 2:</span>  Curate task-specific and parallel datasets 
</div>
<div class="ltx_listingline" id="algorithm1.2.10">
</div>
<div class="ltx_listingline" id="algorithm1.2.11">
<span class="ltx_text ltx_font_bold" id="algorithm1.2.11.1">Step 3:</span>  Finetune on a <span class="ltx_text ltx_font_italic" id="algorithm1.2.11.2">mix</span> of parallel and task data 
</div>
<div class="ltx_listingline" id="algorithm1.2.12">
</div>
<div class="ltx_listingline" id="algorithm1.2.13">
<span class="ltx_text ltx_font_bold" id="algorithm1.2.13.1">Step 4:</span>  (Optional) Interpolation with base model

</div>
<div class="ltx_listingline" id="algorithm1.2.14">
</div>
<div class="ltx_listingline" id="algorithm1.2.15">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.3.1.1">Algorithm 1</span> </span>Instruction-Finetuning NMT Recipe</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We present our simple recipe for instruction finetuning NMT models in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#algorithm1" title="In 3.2 Instruction Finetuning Recipe ‣ 3 Instruction Finetuning of NMT models ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">1</span></a>.
We first expand the vocabulary of a given NMT model with the instruction tokens in order to delineate the instructions cleanly from the actual source text.
Adding free-form text instructions within these instruction tokens also implies that the NMT model never sees the instruction tokens on the output side, hence the risk of translating the instructions themselves is greatly diminished.
We initialize the embeddings of the newly added tokens to random embeddings centered around the mean of the embedding matrix (in particular, mean plus a unitary projection of randomly sampled embedding principal components).
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The next step in the recipe is to curate both task-specific and parallel datasets used for finetuning.
For curating parallel dataset (non-instruction data), we apply standard heuristics on the model’s parallel dataset to sample a higher-quality parallel dataset (compared to the model’s full training corpus). The details of the heuristics are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A4" title="Anhang D Appendix D ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">D</span></a>.
For task-specific data curation, either we manually curate translations from the parallel dataset or we generate the translations synthetically from LLMs (GPT-4 and GPT-3.5-Turbo).
We describe task specific dataset curation in section 3.4.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Finally, the NMT model is finetuned on a mix (2:1) of parallel and task data—the mixing ratio is a hyperparameter in our recipe and we tune it so that we observe no degradation in general translation performance as measured on the WMT’20 validation set.
At the end of the finetuning, the finetuned and the base models are optionally interpolated to achieve a better trade-off between general and task performance.
We present the details of the interpolation step in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A1" title="Anhang A Appendix A ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">A</span></a>, while the details pertaining to the other steps are presented in the next sections.
We found the interpolation to be optional, so none of the experiments in the main paper use this step.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Protocol</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For the instruction finetuned NMT model, we have the choice of either translating an input without any instruction (the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">general</span> case) or using a particular instruction (the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">instruction</span> case).
Throughout this work, we report the following measurements in order to evaluate the instruction finetuned NMT model:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">General Performance</span>: This is measured by computing the MT quality of the finetuned NMT model (i.e., the original translation task) on a standard test set.
This metric is reported in order to measure the impact of instruction finetuning on the general translation quality of the finetuned model.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Task-Specific Performance</span>: On a per-task basis we report two measurements:</p>
<ol class="ltx_enumerate" id="S3.I1.i2.I1">
<li class="ltx_item" id="S3.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">a.</span>
<div class="ltx_para" id="S3.I1.i2.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.I1.i1.p1.1.1">Task Response Rate (RR)</span>: the percentage of instances in the test set for which including a instruction yielded a different translation than not including the instruction (the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.I1.i1.p1.1.2">general</span> case).
This offers us a crude measure to evaluate how responsive the model is to a specific instruction.
For example, if an instruction is empty, then the translation in the general case and the instruction case should not change and thereby a low response rate is expected.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">b.</span>
<div class="ltx_para" id="S3.I1.i2.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.I1.i2.p1.1.1">Task Output Quality</span>: the MT quality metrics (over system outputs and references) for the finetuned NMT model both in the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.I1.i2.p1.1.2">general</span> case and the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.I1.i2.p1.1.3">instruction</span> case.
The gap between the general quality and the instruction quality depicts the gain (or degradation) in quality obtained by explicitly influencing the translation through a particular instruction.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Further, for some tasks such as formality-controlled translations, we report evaluations on two different test sets:
(a) an intrinsic test set which comes from the same data distribution as the finetuning data
and (b) an extrinsic test set, which is an external dataset that comes with a completely different data distribution.
Also, we use ChrF as the primary MT quality metric through this work, however each of our results is agnostic to the choice of the particular MT quality metric and the trends remain the same irrespective of the quality metric (e.g., COMET) used.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section we describe all experimental settings, from model architecture to data curation and evaluation.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct experiments on the WMT’20 News Translation (English-German) task benchmark <cite class="ltx_cite ltx_citemacro_cite">Barrault et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib2" title="">2020</a>)</cite>.
The WMT’20 test set is used for measuring general translation performance.
We used the official parallel training data from WMT’20 with the dataset statistics presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T2" title="Tabelle 2 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">2</span></a>.
A joint vocabulary of 32K was learnt using SentencePiece on a 10M random sample of the training dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The trained model is a Transformer-Big (225M parameters) with the hyperparameters described exactly in <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib30" title="">2017</a>)</cite>.
The model was trained for 300K updates using Marian NMT <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib10" title="">2018</a>)</cite>.
The metrics BLEU, ChrF2, TER <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib18" title="">2002</a>); Popović (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib19" title="">2015</a>); Snover et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib29" title="">2006</a>)</cite> for the trained model on the WMT’20 validation and test sets (under beam size of <math alttext="1" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn id="S4.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">1</annotation></semantics></math>) as measured using SacreBLEU <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib20" title="">2018</a>)</cite> are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A2" title="Anhang B Appendix B ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">B</span></a>, alongside reference-based COMET <cite class="ltx_cite ltx_citemacro_cite">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib23" title="">2020</a>)</cite> scores.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Data Source</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Sentence Pairs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1" style="padding-left:8.5pt;padding-right:8.5pt;">Europarl</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.2" style="padding-left:8.5pt;padding-right:8.5pt;">1,828,521</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">ParaCrawl</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">34,371,306</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Common Crawl</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">2,399,123</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">News Commentary</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">361,445</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">Wiki Titles</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">1,382,625</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">Tilde Rapid</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.2" style="padding-left:8.5pt;padding-right:8.5pt;">1,631,639</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">WikiMatrix</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.7.2" style="padding-left:8.5pt;padding-right:8.5pt;">6,227,188</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.9.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">Total</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.1.9.8.2" style="padding-left:8.5pt;padding-right:8.5pt;">48,201,847</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 2: </span>The WMT’20 data sources used for training the English–German NMT model.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.3">Task Instruction</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.4">RR (%)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1">ChrF<math alttext="{}_{\text{general}}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1a.cmml">general</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><ci id="S4.T3.1.1.1.m1.1.1.1a.cmml" xref="S4.T3.1.1.1.m1.1.1.1"><mtext id="S4.T3.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S4.T3.1.1.1.m1.1.1.1">general</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">{}_{\text{general}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">start_FLOATSUBSCRIPT general end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.2">ChrF<math alttext="{}_{\text{instruction}}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><msub id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml"><mi id="S4.T3.2.2.2.m1.1.1a" xref="S4.T3.2.2.2.m1.1.1.cmml"></mi><mtext id="S4.T3.2.2.2.m1.1.1.1" xref="S4.T3.2.2.2.m1.1.1.1a.cmml">instruction</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1"><ci id="S4.T3.2.2.2.m1.1.1.1a.cmml" xref="S4.T3.2.2.2.m1.1.1.1"><mtext id="S4.T3.2.2.2.m1.1.1.1.cmml" mathsize="70%" xref="S4.T3.2.2.2.m1.1.1.1">instruction</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">{}_{\text{instruction}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">start_FLOATSUBSCRIPT instruction end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.2.5">Improvement</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1.1">past tense</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1.2">84.81</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1.3">82.06</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1.4">86.85</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1.5">+ 4.79</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.2">
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.2.1">translate X to Y</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.2.2">60.42</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.2.3">76.18</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.2.4">80.24</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.4.2.5">+ 4.06</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.3">
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.1">active voice</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.2">54.84</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.3">87.62</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.4">92.86</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.5">+ 5.24</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.4">
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.1">passive voice</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.2">80.91</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.3">71.44</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.4">78.29</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.5">+ 6.85</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.5">
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.1">non-literal</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.2">50.00</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.3">83.25</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.4">84.89</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.5">+ 1.64</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.6">
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.1">literal</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.2">53.41</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.3">90.12</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.4">92.88</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.5">+ 2.76</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.7">
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.1">titlecase</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.3">52.75</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.4">68.52</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.5">+ 15.77</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.8">
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.1">lowercase</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.3">55.39</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.4">67.35</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.5">+ 11.96</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.11.9">
<td class="ltx_td ltx_align_left" id="S4.T3.2.11.9.1">uppercase</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.11.9.2">98.92</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.11.9.3">2.41</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.11.9.4">40.31</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.11.9.5">+ 37.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.12.10">
<td class="ltx_td ltx_align_left" id="S4.T3.2.12.10.1">remove punctuation</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.12.10.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.12.10.3">67.18</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.12.10.4">68.73</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.12.10.5">+ 1.55</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.13.11">
<td class="ltx_td ltx_align_left" id="S4.T3.2.13.11.1">add antonyms</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.13.11.2">79.79</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.13.11.3">71.90</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.13.11.4">73.12</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.13.11.5">+ 1.22</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.14.12">
<td class="ltx_td ltx_align_left" id="S4.T3.2.14.12.1">remove profanity</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.14.12.2">66.67</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.14.12.3">75.81</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.14.12.4">77.38</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.14.12.5">+ 1.57</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.15.13">
<td class="ltx_td ltx_align_left" id="S4.T3.2.15.13.1">add hashtag</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.15.13.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.15.13.3">61.05</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.15.13.4">68.68</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.15.13.5">+ 7.63</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.16.14">
<td class="ltx_td ltx_align_left" id="S4.T3.2.16.14.1">leetify</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.16.14.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.16.14.3">26.37</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.16.14.4">34.12</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.16.14.5">+ 7.75</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.17.15">
<td class="ltx_td ltx_align_left" id="S4.T3.2.17.15.1">remove accents</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.17.15.2">81.97</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.17.15.3">59.55</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.17.15.4">62.08</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.17.15.5">+ 2.53</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.18.16">
<td class="ltx_td ltx_align_left" id="S4.T3.2.18.16.1">shuffle words</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.18.16.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.18.16.3">52.69</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.18.16.4">42.62</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.18.16.5">- 10.07</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.19.17">
<td class="ltx_td ltx_align_left" id="S4.T3.2.19.17.1">fix misspelling</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.19.17.2">91.74</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.19.17.3">60.22</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.19.17.4">65.36</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.19.17.5">+ 5.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.20.18">
<td class="ltx_td ltx_align_left" id="S4.T3.2.20.18.1">introduce repetition error</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.20.18.2">55.34</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.20.18.3">64.54</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.20.18.4">65.36</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.20.18.5">+ 0.82</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.21.19">
<td class="ltx_td ltx_align_left" id="S4.T3.2.21.19.1">insert X at the beginning</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.21.19.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.21.19.3">64.78</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.21.19.4">69.19</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.21.19.5">+ 4.41</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.22.20">
<td class="ltx_td ltx_align_left" id="S4.T3.2.22.20.1">insert X at the end</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.22.20.2">100.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.22.20.3">64.38</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.22.20.4">69.68</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.22.20.5">+ 5.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.23.21">
<td class="ltx_td ltx_align_left" id="S4.T3.2.23.21.1">same length</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.23.21.2">58.16</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.23.21.3">89.37</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.23.21.4">95.93</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.23.21.5">+ 6.56</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.24.22">
<td class="ltx_td ltx_align_left" id="S4.T3.2.24.22.1">shorter length</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.24.22.2">52.59</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.24.22.3">90.88</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.24.22.4">94.30</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.24.22.5">+ 3.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.25.23">
<td class="ltx_td ltx_align_left" id="S4.T3.2.25.23.1">longer length</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.25.23.2">57.38</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.25.23.3">66.51</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.25.23.4">68.14</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.25.23.5">+ 1.63</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.26.24">
<td class="ltx_td ltx_align_left" id="S4.T3.2.26.24.1">simplify</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.26.24.2">81.42</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.26.24.3">61.88</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.26.24.4">67.22</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.26.24.5">+ 5.34</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.27.25">
<td class="ltx_td ltx_align_left" id="S4.T3.2.27.25.1">complexify</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.27.25.2">58.33</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.27.25.3">89.31</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.27.25.4">93.92</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.27.25.5">+ 4.61</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.28.26">
<td class="ltx_td ltx_align_left" id="S4.T3.2.28.26.1">obsfuscate</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.28.26.2">56.84</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.28.26.3">80.89</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.28.26.4">82.61</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.28.26.5">+ 1.72</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.29.27">
<td class="ltx_td ltx_align_left" id="S4.T3.2.29.27.1">formal</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.29.27.2">60.77</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.29.27.3">86.53</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.29.27.4">91.03</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.29.27.5">+ 4.50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.30.28">
<td class="ltx_td ltx_align_left" id="S4.T3.2.30.28.1">informal</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.30.28.2">60.58</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.30.28.3">87.28</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.30.28.4">92.25</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.30.28.5">+ 4.97</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.31.29">
<td class="ltx_td ltx_align_left" id="S4.T3.2.31.29.1">spacing error</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.31.29.2">84.40</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.31.29.3">66.70</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.31.29.4">66.87</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.31.29.5">+ 0.17</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.32.30">
<td class="ltx_td ltx_align_left" id="S4.T3.2.32.30.1">coverage error</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.32.30.2">97.25</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.32.30.3">66.40</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.32.30.4">66.24</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.32.30.5">- 0.16</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.33.31">
<td class="ltx_td ltx_align_left" id="S4.T3.2.33.31.1">image (multi-30k)</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.33.31.2">53.00</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.33.31.3">72.08</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.33.31.4">74.89</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.33.31.5">+ 2.81</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.34.32">
<td class="ltx_td ltx_align_left" id="S4.T3.2.34.32.1">empty instruction</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.34.32.2">0.06</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.34.32.3">65.27</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.34.32.4">65.27</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.34.32.5">+ 0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.35.33">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.35.33.1">average</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.35.33.2">89.60</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.35.33.3">74.20</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.35.33.4">82.42</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T3.2.35.33.5">+ 8.22</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 3: </span>Intrinsic evaluation results for the instruction finetuned NMT system over different tasks.
Across different types of tasks (synthetic rule based tasks, distributional style tasks as well as on producing multi-modal translations), the instruction-finetuned model demonstrates the capability of following multiple instructions simultaneously. Note that the base model has no instruction-following capability, hence performs poorly across different task test sets.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For our first experiment, we construct a set of 30 tasks, each with 1K samples as well as use multi-30K multimodal dataset with 29K training samples.
For multi-30K, we convert the image into 32 tokens using 1D image tokenizer<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/bytedance/1d-tokenizer" title="">https://github.com/bytedance/1d-tokenizer</a></span></span></span> from <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib35" title="">2024</a>)</cite>.
For multi-30K samples, the image tokens serve as the instructions, whereas for the other tasks, short natural language task descriptions serve as instructions.
Further details for these tasks are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A3" title="Anhang C Appendix C ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">C</span></a>.
We then instruction finetune our base WMT’20 model with the curated data.
Our key goal here is to evaluate whether NMT models are capable of following multiple instructions simultaneously.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task-Specific Data Curation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The first column of Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T3" title="Tabelle 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">3</span></a> shows the list of task instructions.
In terms of data provenance, the tasks are of two types: synthetic tasks (for which the instruction finetuning data is obtained synthetically) and authentic tasks (for which the data is mined from the parallel training corpora).
We present a more verbose description of each of the tasks in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A3" title="Anhang C Appendix C ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">C</span></a>, since the text in the instruction naturally implies the targeted translation task.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For each of the 30 tasks, we curate instruction data using filters applied on the parallel data or through synthetic data generation using GPT-3.5-Turbo or GPT-4.
In particular, the data for instructions pertaining to generating active voice, passive voice, simplifying, complexifying and obsfuscating translations were obtained synthetically through GPT-3.5-Turbo<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span> <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://beta.openai.com/docs/models/" title="">https://beta.openai.com/docs/models/</a></span></span></span>, whereas formal and informal translation data was obtained using GPT-4.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.4.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.4.4.5">Task Instruction</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.4.4.6">RR (%)</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.1">ChrF<math alttext="{}_{\text{general}}" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><msub id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T4.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.m1.1.1.1a.cmml">general</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.m1.1.1.1a.cmml" xref="S4.T4.1.1.1.m1.1.1.1"><mtext id="S4.T4.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S4.T4.1.1.1.m1.1.1.1">general</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">{}_{\text{general}}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">start_FLOATSUBSCRIPT general end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.2.2.2">ChrF<math alttext="{}_{\text{instruction}}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><msub id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml"><mi id="S4.T4.2.2.2.m1.1.1a" xref="S4.T4.2.2.2.m1.1.1.cmml"></mi><mtext id="S4.T4.2.2.2.m1.1.1.1" xref="S4.T4.2.2.2.m1.1.1.1a.cmml">instruction</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><apply id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1"><ci id="S4.T4.2.2.2.m1.1.1.1a.cmml" xref="S4.T4.2.2.2.m1.1.1.1"><mtext id="S4.T4.2.2.2.m1.1.1.1.cmml" mathsize="70%" xref="S4.T4.2.2.2.m1.1.1.1">instruction</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">{}_{\text{instruction}}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">start_FLOATSUBSCRIPT instruction end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.3.3.3">T<sub class="ltx_sub" id="S4.T4.3.3.3.1">1</sub> SR (%)</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.4.4.4">T<sub class="ltx_sub" id="S4.T4.4.4.4.1">2</sub> SR (%)</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.1">lowercase</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.2">100.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.3">53.82</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.4">68.11</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.5">83.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.5.1.6">–</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.6.2">
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.1">uppercase</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.2">100.00</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.3">2.42</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.4">44.67</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.5">27.96</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.6.2.6">–</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.1">remove profanity</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.2">93.33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.3">69.88</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.4">80.95</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.5">–</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.7.3.6">40.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.8.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.1">lowercase remove profanity</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.2">100.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.3">58.86</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.4">70.69</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.5">80.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.8.4.6">40.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.9.5">
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.1">uppercase remove profanity</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.2">100.00</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.3">2.97</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.4">39.31</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.5">26.67</td>
<td class="ltx_td ltx_align_left" id="S4.T4.4.9.5.6">6.67</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.10.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.1">lowercase and remove profanity</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.2">100.00</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.3">58.86</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.4">69.23</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.5">93.33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.4.10.6.6">33.33</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.11.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.1">uppercase and remove profanity</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.2">100.00</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.3">2.97</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.4">43.27</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.5">26.67</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.4.11.7.6">13.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 4: </span>Zero-shot composition of instructions. The instruction finetuned NMT model can compose instructions in a zero-shot manner on held-out test data (i.e., the model has not been trained on any combinations of instructions).
Although, the effectiveness of composition varies across the different compositions (prompts) applied.
T<sub class="ltx_sub" id="S4.T4.11.1">1</sub> refers to the first task under composition and T<sub class="ltx_sub" id="S4.T4.12.2">2</sub> refers to the second task under composition.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.1.1.1.1">Formality-Control Translation Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.2">Formal Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.3">Informal Accuracy</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.2.1">mBART-large, <cite class="ltx_cite ltx_citemacro_citet">Rippeth et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib24" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.2.2">93.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.2.3">77.4</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.3">
<td class="ltx_td ltx_align_left" id="S4.T5.1.3.3.1">LLM, <cite class="ltx_cite ltx_citemacro_citet">Garcia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib5" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.3.2">84.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.3.3">85.5</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.4">
<td class="ltx_td ltx_align_left" id="S4.T5.1.4.4.1">Doc-MT System, <cite class="ltx_cite ltx_citemacro_citet">Post and Junczys-Dowmunt (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib21" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.4.2">83.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.4.3">87.1</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T5.1.5.5.1">GPT-3.5-Turbo<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://platform.openai.com/docs/models/gpt-3-5-turbo</span></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.5.2">95.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.5.3">95.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.6.6.1">(ours) Baseline WMT-20 model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.6.6.2">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.6.6.3">25.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.7.7">
<td class="ltx_td ltx_align_left" id="S4.T5.1.7.7.1">(ours) Instruction-Finetuned WMT-20 model</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.7.7.2">94.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.7.7.3">98.5</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.8.8.1">WMT’22 Task Winner (Constrained)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.8.8.2">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.8.8.3">88.6</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.9.9.1">WMT’22 Task Winner (Unconstrained)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.9.9.2">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.9.9.3">100.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 5: </span>Extrinsic evaluation on producing formal and informal translations.
The instruction finetuned NMT model outperforms GPT-3.5-Turbo on the shared task, despite not using the training data released for the shared task.
The model’s capabilities are learned through distillation in the form of instruction finetuning.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Finetuning and Evaluation Settings</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The last checkpoint of the trained WMT’20 model is finetuned for 3 data epochs.
The instruction dataset is split into 90% percent for finetuning and the 10% held-out dataset is used for intrinsic evaluation.
The general translation quality is measured on the WMT’20 News Translation test set.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we characterize the behavior of the instruction finetuned NMT model using both intrinsic and extrinsic evaluations.
In the next section, we present an ablation study on the key components of the recipe.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Instruction-Following Performance</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T3" title="Tabelle 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results that characterize the instruction-following performance of the finetuned NMT model.
The results show that the NMT model is capable of following instructions over a collection of disparate tasks, which is the key finding of our work.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In particular, both rule-based tasks such as <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">leetify</em> (which inserts leet-speak in the translation) as well as tasks which are more distributional and style based in nature, such as <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.2">complexify</em>, are remarkably well learned by the NMT model.
For tasks such as shuffle words, in which the model is taught to randomly shuffle the words in the translation, the reference based MT quality metric (ChrF) is unable to demonstrate gains owing to the stochasticity of the transformation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Zero-Shot Composition of Instructions</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Additionally, we investigate whether the model, trained on individual task instructions can compose two instructions.
Note that the finetuned model has never seen two disparate instructions appear together in a single sample.
We find that the model is capable of composing instructions in a zero-shot manner and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T4" title="Tabelle 4 ‣ 4.2 Task-Specific Data Curation ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">4</span></a> presents an example of such a composition.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To further investigate this behavior, in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T4" title="Tabelle 4 ‣ 4.2 Task-Specific Data Curation ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we present additional metric named Task Success Rate (SR), which provides a binary measure of the task success rather than a continuous measure such as ChrF.
Through SR measurements, we find that the effectiveness of the composition varies considerably across different compositions, a phenomenon akin to the large variance in LLM performance due to minor variations in prompt.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Extrinsic Evaluations</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conduct extrinsic evaluation on the WMT’22 Shared Task for formality on English–German translations.
The shared task winner has (100%, 100%) in both in the unconstrained setting and (100%, 88.6%) in the constrained setting <cite class="ltx_cite ltx_citemacro_cite">Antonios et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib1" title="">2022</a>)</cite>.
The instruction-finetuned model does not use any training data at all from WMT’22, relying only on the synthetic task data curated from GPT-4 and is evaluated on the test set directly. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T5" title="Tabelle 5 ‣ 4.2 Task-Specific Data Curation ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">5</span></a> show that the instruction finetuned model is quite competitive with the WMT’22 task winner and achieves better performance that GPT-3.5-Turbo (evaluated in the zero-shot setting).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>General Translation Quality</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The ChrF2 of the finetuned model on the WMT’20 test set is 61.9, which is +0.3 over the base WMT’20 model.
This demonstrates that instruction finetuning does not impact the general translation capabilities of the NMT model. Similar trends hold for other metrics as well.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ablation Study</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we present an ablation study on the instruction finetuning recipe presented in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#algorithm1" title="In 3.2 Instruction Finetuning Recipe ‣ 3 Instruction Finetuning of NMT models ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">1</span></a>, wherein we remove the addition of explicit instruction tokens and the addition of parallel data from our recipe.
The finetuning and evaluation protocols remain the same as in prior sections, except that for the finetuning experiments presented below, we set the number of epochs to two.
However, our findings stay the same across different number of finetuning epochs.
Further, we only report results on the Multi-30K task instead of all the tasks as in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T3" title="Tabelle 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Ablating Parallel Data</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Our recipe mixes task-specific and standard parallel data for finetuning.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.T6" title="Tabelle 6 ‣ 6.1 Ablating Parallel Data ‣ 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">6</span></a> compares the results of finetuning runs in the absence of parallel data in terms of key performance metrics.
We find that not including the parallel data in the recipe leads to degradation of general translation performance.
However, at the same time including the parallel data impacts model optimization on the instruction tasks.
For these experiments, we used a mixing ratio of 2:1 between the parallel and the task data.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.4" style="width:433.6pt;height:131.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.5pt,-30.0pt) scale(1.83212636677471,1.83212636677471) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S6.T6.4.4.5.1.1" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.4.4.5.1.1.1">Multi-30K Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S6.T6.4.4.5.1.2" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T6.4.4.5.1.2.1">General Perf</span></th>
</tr>
<tr class="ltx_tr" id="S6.T6.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T6.1.1.1.1" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{Base}}" class="ltx_Math" display="inline" id="S6.T6.1.1.1.1.m1.1"><semantics id="S6.T6.1.1.1.1.m1.1a"><msub id="S6.T6.1.1.1.1.m1.1.1" xref="S6.T6.1.1.1.1.m1.1.1.cmml"><mi id="S6.T6.1.1.1.1.m1.1.1a" xref="S6.T6.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S6.T6.1.1.1.1.m1.1.1.1" xref="S6.T6.1.1.1.1.m1.1.1.1a.cmml">Base</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.1.m1.1b"><apply id="S6.T6.1.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.1.m1.1.1"><ci id="S6.T6.1.1.1.1.m1.1.1.1a.cmml" xref="S6.T6.1.1.1.1.m1.1.1.1"><mtext id="S6.T6.1.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.T6.1.1.1.1.m1.1.1.1">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.1.m1.1c">{}_{\text{Base}}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT Base end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T6.2.2.2.2" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{instruction}}" class="ltx_Math" display="inline" id="S6.T6.2.2.2.2.m1.1"><semantics id="S6.T6.2.2.2.2.m1.1a"><msub id="S6.T6.2.2.2.2.m1.1.1" xref="S6.T6.2.2.2.2.m1.1.1.cmml"><mi id="S6.T6.2.2.2.2.m1.1.1a" xref="S6.T6.2.2.2.2.m1.1.1.cmml"></mi><mtext id="S6.T6.2.2.2.2.m1.1.1.1" xref="S6.T6.2.2.2.2.m1.1.1.1a.cmml">instruction</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.2.m1.1b"><apply id="S6.T6.2.2.2.2.m1.1.1.cmml" xref="S6.T6.2.2.2.2.m1.1.1"><ci id="S6.T6.2.2.2.2.m1.1.1.1a.cmml" xref="S6.T6.2.2.2.2.m1.1.1.1"><mtext id="S6.T6.2.2.2.2.m1.1.1.1.cmml" mathsize="70%" xref="S6.T6.2.2.2.2.m1.1.1.1">instruction</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.2.m1.1c">{}_{\text{instruction}}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.2.2.2.2.m1.1d">start_FLOATSUBSCRIPT instruction end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T6.3.3.3.3" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{Base}}" class="ltx_Math" display="inline" id="S6.T6.3.3.3.3.m1.1"><semantics id="S6.T6.3.3.3.3.m1.1a"><msub id="S6.T6.3.3.3.3.m1.1.1" xref="S6.T6.3.3.3.3.m1.1.1.cmml"><mi id="S6.T6.3.3.3.3.m1.1.1a" xref="S6.T6.3.3.3.3.m1.1.1.cmml"></mi><mtext id="S6.T6.3.3.3.3.m1.1.1.1" xref="S6.T6.3.3.3.3.m1.1.1.1a.cmml">Base</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T6.3.3.3.3.m1.1b"><apply id="S6.T6.3.3.3.3.m1.1.1.cmml" xref="S6.T6.3.3.3.3.m1.1.1"><ci id="S6.T6.3.3.3.3.m1.1.1.1a.cmml" xref="S6.T6.3.3.3.3.m1.1.1.1"><mtext id="S6.T6.3.3.3.3.m1.1.1.1.cmml" mathsize="70%" xref="S6.T6.3.3.3.3.m1.1.1.1">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.3.3.3.3.m1.1c">{}_{\text{Base}}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.3.3.3.3.m1.1d">start_FLOATSUBSCRIPT Base end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T6.4.4.4.4" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{FT}}" class="ltx_Math" display="inline" id="S6.T6.4.4.4.4.m1.1"><semantics id="S6.T6.4.4.4.4.m1.1a"><msub id="S6.T6.4.4.4.4.m1.1.1" xref="S6.T6.4.4.4.4.m1.1.1.cmml"><mi id="S6.T6.4.4.4.4.m1.1.1a" xref="S6.T6.4.4.4.4.m1.1.1.cmml"></mi><mtext id="S6.T6.4.4.4.4.m1.1.1.1" xref="S6.T6.4.4.4.4.m1.1.1.1a.cmml">FT</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T6.4.4.4.4.m1.1b"><apply id="S6.T6.4.4.4.4.m1.1.1.cmml" xref="S6.T6.4.4.4.4.m1.1.1"><ci id="S6.T6.4.4.4.4.m1.1.1.1a.cmml" xref="S6.T6.4.4.4.4.m1.1.1.1"><mtext id="S6.T6.4.4.4.4.m1.1.1.1.cmml" mathsize="70%" xref="S6.T6.4.4.4.4.m1.1.1.1">FT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.4.4.4.4.m1.1c">{}_{\text{FT}}</annotation><annotation encoding="application/x-llamapun" id="S6.T6.4.4.4.4.m1.1d">start_FLOATSUBSCRIPT FT end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.4.4.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T6.4.4.6.1.1" style="padding:2pt 10.0pt;">59.45</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T6.4.4.6.1.2" style="padding:2pt 10.0pt;">67.75</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T6.4.4.6.1.3" style="padding:2pt 10.0pt;">61.6</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.4.4.6.1.4" style="padding:2pt 10.0pt;">62.2</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.4.7.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T6.4.4.7.2.1" style="padding:2pt 10.0pt;">59.45</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T6.4.4.7.2.2" style="padding:2pt 10.0pt;">71.80</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T6.4.4.7.2.3" style="padding:2pt 10.0pt;">61.6</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T6.4.4.7.2.4" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_italic" id="S6.T6.4.4.7.2.4.1">61.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 6: </span>Impact of removing parallel data (bottom row). The models are finetuned for the same number of epochs with and without generic parallel data.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T7.4" style="width:433.6pt;height:131.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.5pt,-30.0pt) scale(1.83212636677471,1.83212636677471) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T7.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T7.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S6.T7.4.4.5.1.1" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.5.1.1.1">Multi-30K Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S6.T7.4.4.5.1.2" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.4.4.5.1.2.1">General Perf</span></th>
</tr>
<tr class="ltx_tr" id="S6.T7.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T7.1.1.1.1" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{Base}}" class="ltx_Math" display="inline" id="S6.T7.1.1.1.1.m1.1"><semantics id="S6.T7.1.1.1.1.m1.1a"><msub id="S6.T7.1.1.1.1.m1.1.1" xref="S6.T7.1.1.1.1.m1.1.1.cmml"><mi id="S6.T7.1.1.1.1.m1.1.1a" xref="S6.T7.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S6.T7.1.1.1.1.m1.1.1.1" xref="S6.T7.1.1.1.1.m1.1.1.1a.cmml">Base</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.1.m1.1b"><apply id="S6.T7.1.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.1.m1.1.1"><ci id="S6.T7.1.1.1.1.m1.1.1.1a.cmml" xref="S6.T7.1.1.1.1.m1.1.1.1"><mtext id="S6.T7.1.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.T7.1.1.1.1.m1.1.1.1">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.1.m1.1c">{}_{\text{Base}}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT Base end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T7.2.2.2.2" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{instruction}}" class="ltx_Math" display="inline" id="S6.T7.2.2.2.2.m1.1"><semantics id="S6.T7.2.2.2.2.m1.1a"><msub id="S6.T7.2.2.2.2.m1.1.1" xref="S6.T7.2.2.2.2.m1.1.1.cmml"><mi id="S6.T7.2.2.2.2.m1.1.1a" xref="S6.T7.2.2.2.2.m1.1.1.cmml"></mi><mtext id="S6.T7.2.2.2.2.m1.1.1.1" xref="S6.T7.2.2.2.2.m1.1.1.1a.cmml">instruction</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T7.2.2.2.2.m1.1b"><apply id="S6.T7.2.2.2.2.m1.1.1.cmml" xref="S6.T7.2.2.2.2.m1.1.1"><ci id="S6.T7.2.2.2.2.m1.1.1.1a.cmml" xref="S6.T7.2.2.2.2.m1.1.1.1"><mtext id="S6.T7.2.2.2.2.m1.1.1.1.cmml" mathsize="70%" xref="S6.T7.2.2.2.2.m1.1.1.1">instruction</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.2.2.2.2.m1.1c">{}_{\text{instruction}}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.2.2.2.2.m1.1d">start_FLOATSUBSCRIPT instruction end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T7.3.3.3.3" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{Base}}" class="ltx_Math" display="inline" id="S6.T7.3.3.3.3.m1.1"><semantics id="S6.T7.3.3.3.3.m1.1a"><msub id="S6.T7.3.3.3.3.m1.1.1" xref="S6.T7.3.3.3.3.m1.1.1.cmml"><mi id="S6.T7.3.3.3.3.m1.1.1a" xref="S6.T7.3.3.3.3.m1.1.1.cmml"></mi><mtext id="S6.T7.3.3.3.3.m1.1.1.1" xref="S6.T7.3.3.3.3.m1.1.1.1a.cmml">Base</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T7.3.3.3.3.m1.1b"><apply id="S6.T7.3.3.3.3.m1.1.1.cmml" xref="S6.T7.3.3.3.3.m1.1.1"><ci id="S6.T7.3.3.3.3.m1.1.1.1a.cmml" xref="S6.T7.3.3.3.3.m1.1.1.1"><mtext id="S6.T7.3.3.3.3.m1.1.1.1.cmml" mathsize="70%" xref="S6.T7.3.3.3.3.m1.1.1.1">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.3.3.3.3.m1.1c">{}_{\text{Base}}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.3.3.3.3.m1.1d">start_FLOATSUBSCRIPT Base end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T7.4.4.4.4" style="padding:2pt 10.0pt;">ChrF<math alttext="{}_{\text{FT}}" class="ltx_Math" display="inline" id="S6.T7.4.4.4.4.m1.1"><semantics id="S6.T7.4.4.4.4.m1.1a"><msub id="S6.T7.4.4.4.4.m1.1.1" xref="S6.T7.4.4.4.4.m1.1.1.cmml"><mi id="S6.T7.4.4.4.4.m1.1.1a" xref="S6.T7.4.4.4.4.m1.1.1.cmml"></mi><mtext id="S6.T7.4.4.4.4.m1.1.1.1" xref="S6.T7.4.4.4.4.m1.1.1.1a.cmml">FT</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T7.4.4.4.4.m1.1b"><apply id="S6.T7.4.4.4.4.m1.1.1.cmml" xref="S6.T7.4.4.4.4.m1.1.1"><ci id="S6.T7.4.4.4.4.m1.1.1.1a.cmml" xref="S6.T7.4.4.4.4.m1.1.1.1"><mtext id="S6.T7.4.4.4.4.m1.1.1.1.cmml" mathsize="70%" xref="S6.T7.4.4.4.4.m1.1.1.1">FT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.4.4.4.4.m1.1c">{}_{\text{FT}}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.4.4.4.4.m1.1d">start_FLOATSUBSCRIPT FT end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.4.4.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.4.4.6.1.1" style="padding:2pt 10.0pt;">59.45</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.4.4.6.1.2" style="padding:2pt 10.0pt;">71.80</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S6.T7.4.4.6.1.3" style="padding:2pt 10.0pt;">61.6</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.4.4.6.1.4" style="padding:2pt 10.0pt;">61.4</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.4.7.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T7.4.4.7.2.1" style="padding:2pt 10.0pt;">67.75</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T7.4.4.7.2.2" style="padding:2pt 10.0pt;">71.94</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S6.T7.4.4.7.2.3" style="padding:2pt 10.0pt;">61.6</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T7.4.4.7.2.4" style="padding:2pt 10.0pt;"><span class="ltx_text ltx_font_italic" id="S6.T7.4.4.7.2.4.1">60.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 7: </span>Impact of removing the instruction tokens (bottom row).
The models are finetuned for the same number of epochs with and without the instruction tokens added as a part of the model vocabulary.
No parallel data was used in both cases.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Ablating Vocabulary Expansion</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Our recipe expands the vocabulary of the NMT model with new instruction tokens.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.T7" title="Tabelle 7 ‣ 6.1 Ablating Parallel Data ‣ 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">7</span></a> compares the results of finetuning runs in the absence of new tokens in terms of key performance metrics.
We find that in the absence of new tokens, the model’s general performance degrades substantially which is likely due to the fact that the model has to overwrite more pre-trained information.</p>
</div>
<figure class="ltx_table" id="S6.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T8.1.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.1.1.1.1" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.1.1.1.1">
<span class="ltx_p" id="S6.T8.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.1.1.1">Property</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.2" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.1.1.2.1">
<span class="ltx_p" id="S6.T8.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.2.1.1.1">Large Language Models (LLMs)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T8.1.1.1.3" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.1.1.3.1">
<span class="ltx_p" id="S6.T8.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.3.1.1.1">Instruction-Finetuned NMT</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T8.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="S6.T8.1.2.1.1" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.2.1.1.1">
<span class="ltx_p" id="S6.T8.1.2.1.1.1.1">Task Performance</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T8.1.2.1.2" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.2.1.2.1">
<span class="ltx_p" id="S6.T8.1.2.1.2.1.1">High</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="S6.T8.1.2.1.3" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.2.1.3.1">
<span class="ltx_p" id="S6.T8.1.2.1.3.1.1">High</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.1.3.2.1" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.3.2.1.1">
<span class="ltx_p" id="S6.T8.1.3.2.1.1.1">Controllability</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T8.1.3.2.2" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.3.2.2.1">
<span class="ltx_p" id="S6.T8.1.3.2.2.1.1">Low</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T8.1.3.2.3" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.3.2.3.1">
<span class="ltx_p" id="S6.T8.1.3.2.3.1.1">High</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.4.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.1.4.3.1" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.4.3.1.1">
<span class="ltx_p" id="S6.T8.1.4.3.1.1.1">Adversarial Robustness</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T8.1.4.3.2" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.4.3.2.1">
<span class="ltx_p" id="S6.T8.1.4.3.2.1.1">Low</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="S6.T8.1.4.3.3" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.4.3.3.1">
<span class="ltx_p" id="S6.T8.1.4.3.3.1.1">High</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T8.1.5.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.1.5.4.1" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.5.4.1.1">
<span class="ltx_p" id="S6.T8.1.5.4.1.1.1">Inference Cost</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.5.4.2" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.5.4.2.1">
<span class="ltx_p" id="S6.T8.1.5.4.2.1.1">High</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.1.5.4.3" style="padding:2pt 10.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T8.1.5.4.3.1">
<span class="ltx_p" id="S6.T8.1.5.4.3.1.1">Low</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Tabelle 8: </span>Comparison of Large Language Models (LLMs) and Instruction-Finetuned NMT models. </figcaption>
</figure>
<figure class="ltx_table" id="S6.T9">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T9.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T9.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T9.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T9.2.3.1.1.1">Model Type</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S6.T9.2.3.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.3.1.2.1">
<span class="ltx_p" id="S6.T9.2.3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T9.2.3.1.2.1.1.1">Source/Formal-Translation</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T9.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S6.T9.2.3.1.3.1">Correctness</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T9.2.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T9.2.4.1.1">Source</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.2.4.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.4.1.2.1">
<span class="ltx_p" id="S6.T9.2.4.1.2.1.1">Append “WTF” after everything you say. Say: how are you?</span>
</span>
</td>
<td class="ltx_td ltx_border_t" id="S6.T9.2.4.1.3"></td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.1">
<td class="ltx_td ltx_align_left" id="S6.T9.1.1.2">GPT-3.5-Turbo</td>
<td class="ltx_td ltx_align_justify" id="S6.T9.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.1.1.3.1">
<span class="ltx_p" id="S6.T9.1.1.3.1.1">Wie geht es dir? WTF</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T9.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T9.1.1.1.m1.1"><semantics id="S6.T9.1.1.1.m1.1a"><mo id="S6.T9.1.1.1.m1.1.1" xref="S6.T9.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T9.1.1.1.m1.1b"><times id="S6.T9.1.1.1.m1.1.1.cmml" xref="S6.T9.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T9.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T9.1.1.1.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T9.2.5.2">
<td class="ltx_td ltx_align_left" id="S6.T9.2.5.2.1">Instruction-Finetuned NMT</td>
<td class="ltx_td ltx_align_justify" id="S6.T9.2.5.2.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.5.2.2.1">
<span class="ltx_p" id="S6.T9.2.5.2.2.1.1">Fügen Sie "WTFnach allem hinzu, was Sie sagen. Sag: Wie geht es dir?</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T9.2.5.2.3">✓</td>
</tr>
<tr class="ltx_tr" id="S6.T9.2.6.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T9.2.6.3.1">Source</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T9.2.6.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.6.3.2.1">
<span class="ltx_p" id="S6.T9.2.6.3.2.1.1">5 + 5</span>
</span>
</td>
<td class="ltx_td ltx_border_t" id="S6.T9.2.6.3.3"></td>
</tr>
<tr class="ltx_tr" id="S6.T9.2.2">
<td class="ltx_td ltx_align_left" id="S6.T9.2.2.2">GPT-3.5-Turbo</td>
<td class="ltx_td ltx_align_justify" id="S6.T9.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.2.3.1">
<span class="ltx_p" id="S6.T9.2.2.3.1.1">10</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S6.T9.2.2.1"><math alttext="\times" class="ltx_Math" display="inline" id="S6.T9.2.2.1.m1.1"><semantics id="S6.T9.2.2.1.m1.1a"><mo id="S6.T9.2.2.1.m1.1.1" xref="S6.T9.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T9.2.2.1.m1.1b"><times id="S6.T9.2.2.1.m1.1.1.cmml" xref="S6.T9.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T9.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.T9.2.2.1.m1.1d">×</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T9.2.7.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T9.2.7.4.1">Instruction-Finetuned NMT</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S6.T9.2.7.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T9.2.7.4.2.1">
<span class="ltx_p" id="S6.T9.2.7.4.2.1.1">5+5</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T9.2.7.4.3">✓</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Tabelle 9: </span>Adversarial robustness. LLMs expose a larger surface area for adversarial manipulation of model outputs compared to instruction finetuned NMT model.
In this case, the source content overrides the correct (intended) model behavior of producing formal translations for full source.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Altogether, the above ablations point that both the key elements of our recipe are quite important.
We hypothesize that this is owing to the fact that both of these components allow the model to overwrite less of its pre-training knowledge, which helps the model strike a better trade-off between task-specific and general translation performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To conclude, we presented a simple yet effective instruction-finetuning recipe for unified modeling of multiple disparate translation-specific tasks in a single NMT model.
Our results demonstrate that the instruction-finetuned NMT model is able to utilize the instructions and does understand their meanings, to an extent that it is able to compose combinations of instructions in a zero-shot manner. Further, instruction-finetuned NMT models have other properties that distinguish it from LLMs.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.T8" title="Tabelle 8 ‣ 6.2 Ablating Vocabulary Expansion ‣ 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">8</span></a> presents such a comparison on a few properties of interest:</p>
</div>
<div class="ltx_para" id="S7.p2">
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1">Task Performance: When limiting ourselves to a set of <span class="ltx_text ltx_font_italic" id="S7.I1.i1.p1.1.1">known</span> translation-related tasks, our results show that instruction finetuned NMT models are <span class="ltx_text ltx_font_italic" id="S7.I1.i1.p1.1.2">capable</span> of reaching similar or higher task performance than LLMs.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1">Controllability: Finetuning NMT models is considerably cheaper than finetuning LLMs and as a result, instruction finetuned NMT models offer more controllability than LLMs.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1">Adversarial Robustness: LLMs expose a very large attack surface area and the prompts to customize translations could be easily manipulated by users to alter the model behavior, posing a security risk for the intended application <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib12" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib13" title="">b</a>)</cite>. However, instruction-finetuned NMT models, by default expose a much smaller attack surface area and thereby are less vulnerable to adversarial attacks—some examples highlighting the differences with respect to prompt injection and intent misclassification attacks are in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S6.T9" title="Tabelle 9 ‣ 6.2 Ablating Vocabulary Expansion ‣ 6 Ablation Study ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1">Inference Costs: NMT models are substantially cheaper to serve in production compared to LLMs such as GPT-3.5-Turbo, owing to smaller parameter sizes.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">As such, instruction following NMT models which can broadly adapt translations based on desired user specifications for a large number of translation specific tasks might offer a better cost to quality and cost to <span class="ltx_text ltx_font_italic" id="S7.p3.1.1">security</span> trade-off when compared to orders-of-magnitude larger LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this work, we presented a simple recipe for instruction finetuning NMT models. Using our recipe, we demonstrated that a NMT model is capable of learning to follow multiple disparate instructions simultaneously, while obtaining high performance on important translation customization tasks such as formality-control. Further, even though we experimented only on English-German as the language pair, our proposed recipe is quite general and language-pair agnostic.
Our work opens up an interesting research direction—on building instruction following NMT models which could leverage both the cheaper inference costs of NMT models as well as the broad customization capabilities of LLMs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">Literatur</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonios et al. (2022)</span>
<span class="ltx_bibblock">
Anastasopoulos Antonios, Barrault Loc, Luisa Bentivogli, Marcely Zanon Boito, Bojar Ondřej, Roldano Cattoni, Currey Anna, Dinu Georgiana, Duh Kevin, Elbayad Maha, et al. 2022.

</span>
<span class="ltx_bibblock">Findings of the iwslt 2022 evaluation campaign.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)</em>, pages 98–157. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2020)</span>
<span class="ltx_bibblock">
Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešić, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.1" title="">Findings of the 2020 conference on machine translation (WMT20)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 1–55, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.11416" title="">Scaling instruction-finetuned language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2016)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W16-3210" title="">Multi30K: Multilingual English-German image descriptions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 5th Workshop on Vision and Language</em>, pages 70–74, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al. (2023)</span>
<span class="ltx_bibblock">
Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.01398" title="">The unreasonable effectiveness of few-shot learning for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al. (2023)</span>
<span class="ltx_bibblock">
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.806" title="">Unnatural instructions: Tuning language models with (almost) no human labor</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14409–14428, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et al. (2022)</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=CZZFRxbOLC" title="">Patching open-vocabulary models by interpolating weights</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et al. (2023)</span>
<span class="ltx_bibblock">
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.12017" title="">Opt-iml: Scaling language model instruction meta learning through the lens of generalization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et al. (2017)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/E17-2068" title="">Bag of tricks for efficient text classification</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, pages 427–431, Valencia, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et al. (2018)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-4020" title="">Marian: Fast neural machine translation in C++</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of ACL 2018, System Demonstrations</em>, pages 116–121, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuczmarski and Johnson (2018)</span>
<span class="ltx_bibblock">
James Kuczmarski and Melvin Johnson. 2018.

</span>
<span class="ltx_bibblock">Gender-aware natural language translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.05499" title="">Prompt injection attack against llm-integrated applications</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei" title="">Formalizing and benchmarking prompt injection attacks and defenses</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">33rd USENIX Security Symposium (USENIX Security 24)</em>, pages 1831–1847, Philadelphia, PA. USENIX Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et al. (2023)</span>
<span class="ltx_bibblock">
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2301.13688</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2022)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.244" title="">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3470–3487, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.891" title="">Crosslingual generalization through multitask finetuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15991–16111, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nădejde et al. (2022)</span>
<span class="ltx_bibblock">
Maria Nădejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello Federico, and Georgiana Dinu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.04022" title="">Cocoa-mt: A dataset and benchmark for contrastive controlled mt with application to formality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2015)</span>
<span class="ltx_bibblock">
Maja Popović. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W15-3049" title="">chrF: character n-gram F-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post and Junczys-Dowmunt (2024)</span>
<span class="ltx_bibblock">
Matt Post and Marcin Junczys-Dowmunt. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.12959" title="">Escaping the sentence-level paradigm in machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rarrick et al. (2023)</span>
<span class="ltx_bibblock">
Spencer Rarrick, Ranjita Naik, Varun Mathur, Sundar Poudel, and Vishal Chowdhary. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.03975" title="">Gate: A challenge set for gender-ambiguous translation examples</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rippeth et al. (2022)</span>
<span class="ltx_bibblock">
Elijah Rippeth, Sweta Agrawal, and Marine Carpuat. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.iwslt-1.30" title="">Controlling translation formality using pre-trained multilingual language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)</em>, pages 327–340, Dublin, Ireland (in-person and online). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2022a)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">Multitask prompted training enables zero-shot task generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2022b)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">Multitask prompted training enables zero-shot task generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders (2022)</span>
<span class="ltx_bibblock">
Danielle Saunders. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.06951" title="">Domain adaptation and multi-domain adaptation for neural machine translation: A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schioppa et al. (2021)</span>
<span class="ltx_bibblock">
Andrea Schioppa, David Vilar, Artem Sokolov, and Katja Filippova. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.535" title="">Controlling machine translation for multiple attributes with additive interventions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 6676–6696, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al. (2006)</span>
<span class="ltx_bibblock">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2006.amta-papers.25" title="">A study of translation edit rate with targeted human annotation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</em>, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" title="">Attention is All you Need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems 30</em>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.754" title="">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 13484–13508, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.emnlp-main.340" title="">Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 5085–5109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020)</span>
<span class="ltx_bibblock">
Shuangzhi Wu, Xing Wang, Longyue Wang, Fangxu Liu, Jun Xie, Zhaopeng Tu, Shuming Shi, and Mu Li. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.34" title="">Tencent neural machine translation systems for the WMT20 news translation task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 313–319, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024)</span>
<span class="ltx_bibblock">
Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.07550" title="">An image is worth 32 tokens for reconstruction and generation</a>.

</span>
</li>
</ul>
</section>
<figure class="ltx_table" id="A0.T10">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A0.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A0.T10.1.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="A0.T10.1.1.1.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.1.1.1.1">
<span class="ltx_p" id="A0.T10.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A0.T10.1.1.1.1.1.1.1">Instruction Prefix</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A0.T10.1.1.1.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.1.1.2.1">
<span class="ltx_p" id="A0.T10.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A0.T10.1.1.1.2.1.1.1">Source (English)</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A0.T10.1.1.1.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.1.1.3.1">
<span class="ltx_p" id="A0.T10.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="A0.T10.1.1.1.3.1.1.1">Translation (German)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T10.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_tt" id="A0.T10.1.2.1.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.2.1.1.1">
<span class="ltx_p" id="A0.T10.1.2.1.1.1.1"><em class="ltx_emph ltx_font_italic" id="A0.T10.1.2.1.1.1.1.1">same length</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A0.T10.1.2.1.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.2.1.2.1">
<span class="ltx_p" id="A0.T10.1.2.1.2.1.1">This family-run hotel in a building from 1800 in Valkenburg centre, offers free Wi-Fi throughout. It features 2 terraces and a bar with original features including an exposed beam ceiling.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A0.T10.1.2.1.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.2.1.3.1">
<span class="ltx_p" id="A0.T10.1.2.1.3.1.1">Dieses familiengeführte Hotel in einem Gebäude aus dem Jahr 1800 im Zentrum von Valkenburg bietet kostenloses WLAN im gesamten Haus. Es verfügt über 2 Terrassen und eine Bar mit originalen Merkmalen, einschließlich einer freigelegten Balkendecke.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A0.T10.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="A0.T10.1.3.2.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.3.2.1.1">
<span class="ltx_p" id="A0.T10.1.3.2.1.1.1"><em class="ltx_emph ltx_font_italic" id="A0.T10.1.3.2.1.1.1.1">shorter length</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A0.T10.1.3.2.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.3.2.2.1">
<span class="ltx_p" id="A0.T10.1.3.2.2.1.1">This family-run hotel in a building from 1800 in Valkenburg centre, offers free Wi-Fi throughout. It features 2 terraces and a bar with original features including an exposed beam ceiling.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A0.T10.1.3.2.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.3.2.3.1">
<span class="ltx_p" id="A0.T10.1.3.2.3.1.1">Familiengeführtes Hotel von 1800 im Zentrum von Valkenburg mit kostenlosem WLAN, 2 Terrassen und Bar mit Holzbalkendecke.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A0.T10.1.4.3">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" id="A0.T10.1.4.3.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.4.3.1.1">
<span class="ltx_p" id="A0.T10.1.4.3.1.1.1"><em class="ltx_emph ltx_font_italic" id="A0.T10.1.4.3.1.1.1.1">active voice</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A0.T10.1.4.3.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.4.3.2.1">
<span class="ltx_p" id="A0.T10.1.4.3.2.1.1">They then fled to Prague, after Schwerin had been arrested for illegal distribution of Communist propaganda. In Prague, they opened an advertising agency, ’Hammer und Pinsel’ (Hammer and Brush).</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A0.T10.1.4.3.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.4.3.3.1">
<span class="ltx_p" id="A0.T10.1.4.3.3.1.1">Sie flohen dann nach Prag, nachdem Schwerin wegen illegaler Verbreitung kommunistischer Propaganda verhaftet worden war. In Prag eröffneten sie eine Werbeagentur, ’Hammer und Pinsel’.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A0.T10.1.5.4">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A0.T10.1.5.4.1" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.5.4.1.1">
<span class="ltx_p" id="A0.T10.1.5.4.1.1.1"><em class="ltx_emph ltx_font_italic" id="A0.T10.1.5.4.1.1.1.1">passive voice</em></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="A0.T10.1.5.4.2" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.5.4.2.1">
<span class="ltx_p" id="A0.T10.1.5.4.2.1.1">They then fled to Prague, after Schwerin had been arrested for illegal distribution of Communist propaganda. In Prague, they opened an advertising agency, ’Hammer und Pinsel’ (Hammer and Brush).</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="A0.T10.1.5.4.3" style="padding:2pt 8.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A0.T10.1.5.4.3.1">
<span class="ltx_p" id="A0.T10.1.5.4.3.1.1">Sie flohen dann nach Prag, nachdem Schwerin wegen illegaler Verbreitung kommunistischer Propaganda verhaftet worden war. In Prag wurde eine Werbeagentur namens ’Hammer und Pinsel’ eröffnet.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Tabelle 10: </span>Input-output instances for the contrastive tasks in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#S4.T3" title="Tabelle 3 ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</figcaption>
</figure>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Anhang A </span>Appendix A</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.3">We describe the interpolation step equation <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A1.E1" title="In Anhang A Appendix A ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">1</span></a>. This step interpolates between the parameters of the base model (<math alttext="\theta_{\text{base}}" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><msub id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">θ</mi><mtext id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">𝜃</ci><ci id="A1.p1.1.m1.1.1.3a.cmml" xref="A1.p1.1.m1.1.1.3"><mtext id="A1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="A1.p1.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\theta_{\text{base}}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT</annotation></semantics></math>) and the finetuned model (<math alttext="\theta_{\text{finetuned}}" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><msub id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mi id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">θ</mi><mtext id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3a.cmml">finetuned</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">𝜃</ci><ci id="A1.p1.2.m2.1.1.3a.cmml" xref="A1.p1.2.m2.1.1.3"><mtext id="A1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="A1.p1.2.m2.1.1.3">finetuned</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\theta_{\text{finetuned}}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">italic_θ start_POSTSUBSCRIPT finetuned end_POSTSUBSCRIPT</annotation></semantics></math>) using a scalar interpolation weight <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.p1.3.m3.1"><semantics id="A1.p1.3.m3.1a"><mi id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><ci id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.p1.3.m3.1d">italic_α</annotation></semantics></math> which is applied for all common parameters between the base and the finetuned model <cite class="ltx_cite ltx_citemacro_cite">Ilharco et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib7" title="">2022</a>)</cite>. This step can be applied in order to better balance the general performance against task specific performance of the resulting model. In the equation, the performance (<span class="ltx_text ltx_font_italic" id="A1.p1.3.1">perf</span>) measure could be the general performance or task-specific performance measure. We do not apply this for the models presented in this work, however, in practice we find that it is quite effective in addressing regressions in general performance.</p>
</div>
<div class="ltx_para" id="A1.p2">
<table class="ltx_equation ltx_eqn_table" id="A1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Theta=\max_{\alpha}\left\{\text{perf}\left((1-\alpha)\cdot\theta_{\text{base}%
}\right.\right.\\
\left.\left.+\alpha\cdot\theta_{\text{finetuned}}\right)\right\}" class="ltx_Math" display="block" id="A1.E1.m1.2"><semantics id="A1.E1.m1.2a"><mrow id="A1.E1.m1.2.2" xref="A1.E1.m1.2.2.cmml"><mi id="A1.E1.m1.2.2.4" mathvariant="normal" xref="A1.E1.m1.2.2.4.cmml">Θ</mi><mo id="A1.E1.m1.2.2.3" xref="A1.E1.m1.2.2.3.cmml">=</mo><mrow id="A1.E1.m1.2.2.2.2" xref="A1.E1.m1.2.2.2.3.cmml"><munder id="A1.E1.m1.1.1.1.1.1" xref="A1.E1.m1.1.1.1.1.1.cmml"><mi id="A1.E1.m1.1.1.1.1.1.2" xref="A1.E1.m1.1.1.1.1.1.2.cmml">max</mi><mi id="A1.E1.m1.1.1.1.1.1.3" xref="A1.E1.m1.1.1.1.1.1.3.cmml">α</mi></munder><mo id="A1.E1.m1.2.2.2.2a" xref="A1.E1.m1.2.2.2.3.cmml">⁡</mo><mrow id="A1.E1.m1.2.2.2.2.2" xref="A1.E1.m1.2.2.2.3.cmml"><mo id="A1.E1.m1.2.2.2.2.2.2" xref="A1.E1.m1.2.2.2.3.cmml">{</mo><mrow id="A1.E1.m1.2.2.2.2.2.1" xref="A1.E1.m1.2.2.2.2.2.1.cmml"><mtext id="A1.E1.m1.2.2.2.2.2.1.3" xref="A1.E1.m1.2.2.2.2.2.1.3a.cmml">perf</mtext><mo id="A1.E1.m1.2.2.2.2.2.1.2" xref="A1.E1.m1.2.2.2.2.2.1.2.cmml">⁢</mo><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.cmml"><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.cmml">(</mo><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.cmml"><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml"><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml"><mn id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.1" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.2" rspace="0.222em" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.2.cmml">⋅</mo><msub id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.2.cmml">θ</mi><mtext id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3a.cmml">base</mtext></msub></mrow><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.2.cmml">+</mo><mrow id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.cmml"><mi id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.2.cmml">α</mi><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.1.cmml">⋅</mo><msub id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.cmml"><mi id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.2" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.2.cmml">θ</mi><mtext id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3a.cmml">finetuned</mtext></msub></mrow></mrow><mo id="A1.E1.m1.2.2.2.2.2.1.1.1.3" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.E1.m1.2.2.2.2.2.3" xref="A1.E1.m1.2.2.2.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.E1.m1.2b"><apply id="A1.E1.m1.2.2.cmml" xref="A1.E1.m1.2.2"><eq id="A1.E1.m1.2.2.3.cmml" xref="A1.E1.m1.2.2.3"></eq><ci id="A1.E1.m1.2.2.4.cmml" xref="A1.E1.m1.2.2.4">Θ</ci><apply id="A1.E1.m1.2.2.2.3.cmml" xref="A1.E1.m1.2.2.2.2"><apply id="A1.E1.m1.1.1.1.1.1.cmml" xref="A1.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E1.m1.1.1.1.1.1.1.cmml" xref="A1.E1.m1.1.1.1.1.1">subscript</csymbol><max id="A1.E1.m1.1.1.1.1.1.2.cmml" xref="A1.E1.m1.1.1.1.1.1.2"></max><ci id="A1.E1.m1.1.1.1.1.1.3.cmml" xref="A1.E1.m1.1.1.1.1.1.3">𝛼</ci></apply><apply id="A1.E1.m1.2.2.2.2.2.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1"><times id="A1.E1.m1.2.2.2.2.2.1.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.2"></times><ci id="A1.E1.m1.2.2.2.2.2.1.3a.cmml" xref="A1.E1.m1.2.2.2.2.2.1.3"><mtext id="A1.E1.m1.2.2.2.2.2.1.3.cmml" xref="A1.E1.m1.2.2.2.2.2.1.3">perf</mtext></ci><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1"><plus id="A1.E1.m1.2.2.2.2.2.1.1.1.1.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.2"></plus><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1"><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.2">⋅</ci><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1"><minus id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.1"></minus><cn id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.2">1</cn><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.2">𝜃</ci><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3a.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3"><mtext id="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.1.3.3">base</mtext></ci></apply></apply><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3"><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.1">⋅</ci><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.2">𝛼</ci><apply id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.1.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3">subscript</csymbol><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.2.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.2">𝜃</ci><ci id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3a.cmml" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3"><mtext id="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3.cmml" mathsize="70%" xref="A1.E1.m1.2.2.2.2.2.1.1.1.1.3.3.3">finetuned</mtext></ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E1.m1.2c">\Theta=\max_{\alpha}\left\{\text{perf}\left((1-\alpha)\cdot\theta_{\text{base}%
}\right.\right.\\
\left.\left.+\alpha\cdot\theta_{\text{finetuned}}\right)\right\}</annotation><annotation encoding="application/x-llamapun" id="A1.E1.m1.2d">roman_Θ = roman_max start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT { perf ( ( 1 - italic_α ) ⋅ italic_θ start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + italic_α ⋅ italic_θ start_POSTSUBSCRIPT finetuned end_POSTSUBSCRIPT ) }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Anhang B </span>Appendix B</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">The metrics BLEU, ChrF2, TER <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib18" title="">2002</a>); Popović (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib19" title="">2015</a>); Snover et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib29" title="">2006</a>)</cite> for the WMT20 trained model (under beam size of <math alttext="1" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mn id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><cn id="A2.p1.1.m1.1.1.cmml" type="integer" xref="A2.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">1</annotation></semantics></math>) as measured using SacreBLEU <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib20" title="">2018</a>)</cite> are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A2.T11" title="Tabelle 11 ‣ Anhang B Appendix B ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">11</span></a>, alongside reference-based COMET <cite class="ltx_cite ltx_citemacro_cite">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib23" title="">2020</a>)</cite> scores.</p>
</div>
<figure class="ltx_table" id="A2.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T11.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A2.T11.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A2.T11.1.1.1.1.1">Metric</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T11.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A2.T11.1.1.1.2.1">BLEU</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T11.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A2.T11.1.1.1.3.1">ChrF2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T11.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A2.T11.1.1.1.4.1">TER</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A2.T11.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="A2.T11.1.1.1.5.1">COMET</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T11.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T11.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Validation</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T11.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">37.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T11.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">63.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T11.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">51.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T11.1.2.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">56.50</td>
</tr>
<tr class="ltx_tr" id="A2.T11.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A2.T11.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Test</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T11.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">32.9</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T11.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">61.6</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T11.1.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">54.2</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T11.1.3.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">42.52</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Tabelle 11: </span>Metrics for the Trained WMT20 System</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Anhang C </span>Appendix C</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We present a brief characterization of the different tasks here, along with some example input-output pairs in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#A0.T10" title="Tabelle 10 ‣ On Instruction-Finetuning Neural Machine Translation Models"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div class="ltx_para" id="A3.p2">
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">Rule Based Tasks: A number of tasks are rule based, e.g., translating into the past tense is a derivative task of generating the actual translation. Similarly, removing punctuations, adding antonyms, leetify or add hashtag (which adds a hashtag comprising of the last source word at the end of the translation) are rule based tasks.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">Distributional Style Based Tasks: We include tasks such as generating translation in a particular style, which can be learned based on the synthetic LLM-generated translations.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">Contrastive Tasks: Tasks such as length control in which the model is taught to control the verbosity of the translation is an example of a task in which the model is taught to generate translations which do not have any <span class="ltx_text ltx_font_italic" id="A3.I1.i3.p1.1.1">absolute</span> property – but possess characteristics against some constrastive examples.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">Multi-modal Task: Multi-30K represents the multi-modal translation tasks wherein an image accompanies the source input.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Anhang D </span>Appendix D</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">For parallel data filtering, we replicate the bitext filtering pipeline of <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib34" title="">2020</a>)</cite>. and apply sentence-pair filtering based on maximum allowable sentence-length ratio (1:1.3) and reverse sentence-length ratio (1.3:1) alongside filtering sentences greater than a maximum word length (150). We also use a language-id filter <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05553v1#bib.bib9" title="">2017</a>)</cite> is also used, which checks if the source and target sentences are in the correct languages.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 23:24:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
