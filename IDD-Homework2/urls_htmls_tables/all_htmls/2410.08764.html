<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Measuring the Groundedness of Legal Question-Answering Systems</title>
<!--Generated on Fri Oct 11 12:20:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.08764v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S1" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S2" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S2.SS1" title="In 2 Related Work ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Grounding of Generated Responses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S2.SS2" title="In 2 Related Work ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Hallucination Detection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S3" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Grounding Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S4" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Dataset Creation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S4.SS1" title="In 4 Dataset Creation ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S4.SS2" title="In 4 Dataset Creation ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Synthetic Adaptation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S4.SS3" title="In 4 Dataset Creation ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Data Splits</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Benchmarking Methodologies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS1" title="In 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Similarity-based Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS1.SSS0.Px1" title="In 5.1 Similarity-based Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Semantic Similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS1.SSS0.Px2" title="In 5.1 Similarity-based Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Quoted Information Precision</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS2" title="In 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Natural Language Inference</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS2.SSS0.Px1" title="In 5.2 Natural Language Inference ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">FactKB</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS2.SSS0.Px2" title="In 5.2 Natural Language Inference ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Hallucination Evaluation Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS3" title="In 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Prompting Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS3.SSS0.Px1" title="In 5.3 Prompting Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Direct Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS3.SSS0.Px2" title="In 5.3 Prompting Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Amazon RefChecker</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS3.SSS0.Px3" title="In 5.3 Prompting Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">SelfCheckGPT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS3.SSS0.Px4" title="In 5.3 Prompting Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">DeepEval: Claims Extraction and Verification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS4" title="In 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Fine-Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experimental Set-Up</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.SS0.SSS0.Px1" title="In 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.SS0.SSS0.Px2" title="In 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Performance Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.SS0.SSS0.Px3" title="In 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Computational Resources</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S7" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Groundedness Classification Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S8" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Error Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S8.SS0.SSS0.Px1" title="In 8 Error Analysis ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title">Misclassification Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S9" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1" title="In Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Response Error Types</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.SS1" title="In Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Dev Set Error Types</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.SS2" title="In Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Description and Examples</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Measuring the Groundedness of Legal Question-Answering Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Dietrich Trautmann, <span class="ltx_text ltx_font_bold" id="id2.1.id1">Natalia Ostapuk</span>, <span class="ltx_text ltx_font_bold" id="id3.2.id2">Quentin Grail</span>, <span class="ltx_text ltx_font_bold" id="id4.3.id3">Adrian Alan Pol</span>, <span class="ltx_text ltx_font_bold" id="id5.4.id4">Guglielmo Bonifazi</span>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.5.id5">Shang Gao</span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id7.1.id1">Martin Gajek
<br class="ltx_break"/></span>Thomson Reuters Labs, Zug, Switzerland
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.2.id2">{first.last}@tr.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">In high-stakes domains like legal question-answering, the accuracy and trustworthiness of generative AI systems are of paramount importance.
This work presents a comprehensive benchmark of various methods to assess the groundedness of AI-generated responses, aiming to significantly enhance their reliability.
Our experiments include similarity-based metrics and natural language inference models to evaluate whether responses are well-founded in the given contexts.
We also explore different prompting strategies for large language models to improve the detection of ungrounded responses.
We validated the effectiveness of these methods using a newly created grounding classification corpus, designed specifically for legal queries and corresponding responses from retrieval-augmented prompting, focusing on their alignment with source material.
Our results indicate potential in groundedness classification of generated responses, with the best method achieving a macro-F1 score of <math alttext="0.8" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn id="id1.1.m1.1.1.cmml" type="float" xref="id1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">0.8</annotation></semantics></math>.
Additionally, we evaluated the methods in terms of their latency to determine their suitability for real-world applications, as this step typically follows the generation process.
This capability is essential for processes that may trigger additional manual verification or automated response regeneration.
In summary, this study demonstrates the potential of various detection methods to improve the trustworthiness of generative AI in legal settings.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="692" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example query and corresponding LLM responses with <span class="ltx_text ltx_font_bold" id="S1.F1.3.1" style="color:#0000FF;">grounded</span> and <span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="color:#FF0000;">erroneous</span> spans (Procedural Errors). The retrieved context used for grounding the responses was omitted due to its length. The remaining sentences in both responses are identical and grounded, but not highlighted to emphasize the differences.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Generative AI systems are increasingly employed in high-stakes domains such as legal question-answering, where accuracy and trust are paramount <cite class="ltx_cite ltx_citemacro_cite">Monroy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib15" title="">2009</a>); Vold and Conrad (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib22" title="">2021</a>); Khazaeli et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib8" title="">2021</a>); Martinez-Gil (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib13" title="">2023</a>)</cite>.
A significant challenge in these applications is the detection of outputs that are not grounded in the input data (context), which can compromise user trust and diminish the application‚Äôs value <cite class="ltx_cite ltx_citemacro_cite">Maynez et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib14" title="">2020</a>); Rawte et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib17" title="">2023</a>)</cite>.
This work addresses this challenge by conducting a comprehensive benchmarking to assess the groundedness of AI-generated legal responses, thereby enhancing their reliability.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Our methodology investigates diverse approaches to classify responses based on their foundation in the provided source material (cf. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">1</span></a>).
We utilize:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">Similarity-based techniques</em>, employing various text similarity metrics to quantify the alignment between the generated text and the input data at the sentence-level.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">Natural language inference</em> models to determine if the generated response sentences are entailed by or contradict the sentences in the source material.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Diverse <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">prompting strategies</em> for large language models (LLMs) to detect ungrounded responses. <cite class="ltx_cite ltx_citemacro_cite">Bubeck et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib1" title="">2023</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We evaluate these approaches on a new corpus of legal queries and responses, annotated for their degree of groundedness.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Experimental results demonstrate the effectiveness of many methods in the detection of potentially ungrounded answers.
We also discuss the trade-offs between task performance and computational efficiency, highlighting the capabilities of particular approaches to operate with minimal added latency in real-world applications.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Furthermore, we investigated the types of errors present in the responses, categorizing them into six distinct classes: <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">Factual Inaccuracies</em>, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">Contextual Misinterpretations</em>, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">Procedural Errors</em>, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.4">Reasoning Errors</em>, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.5">Misattributions</em>, and <em class="ltx_emph ltx_font_italic" id="S1.p5.1.6">Terminological Errors</em>.
Our analysis reveals that factual inaccuracies are the most prevalent type of errors.
Importantly, we found that the misclassification rates in the overall groundedness assessment task are not uniform across these error categories, providing valuable insights for targeted improvements in AI-generated legal responses.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our findings underscore the potential of automated groundedness assessment tools to improve the reliability and utility of generative AI in legal settings, ensuring that the generated responses are consistently accurate and trustworthy.
The error analysis further contributes to a nuanced understanding of the challenges in this domain, paving the way for more refined and effective AI systems in legal applications.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Grounding of Generated Responses</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Grounding and factual consistency in language model outputs, especially for summarization and question-answering tasks, have been a focal point of recent research. <cite class="ltx_cite ltx_citemacro_citet">Kry≈õci≈Ñski et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib9" title="">2020</a>)</cite> introduced a weakly-supervised, model-based approach to verify factual consistency between source documents and generated summaries. This method uniquely combines consistency checks with the extraction of supporting and contradictory spans.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Building on this, <cite class="ltx_cite ltx_citemacro_citet">Maynez et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib14" title="">2020</a>)</cite> performed an extensive human evaluation of neural abstractive summarization systems. Their results showed a significant amount of ungrounded content in model-generated summaries and found that textual entailment measures correlate more strongly with faithfulness than standard metrics. This finding closely relates to our interest in assessing the groundedness of AI-generated legal responses.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The Chain-of-Knowledge (CoK) framework <cite class="ltx_cite ltx_citemacro_cite">Li et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib10" title="">2023</a>)</cite> marks a major advance in reducing hallucinations. By dynamically incorporating grounding information from various sources, CoK enhances factual accuracy in knowledge-intensive tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">In essence, grounding of LLM-generated responses aims to ensure that outputs are factually consistent with input data, thereby enhancing reliability and reducing ungrounded LLM-generated content.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Hallucination Detection</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Advancements in hallucination detection have been pivotal in developing more reliable and grounded LLMs, particularly for question-answering (QA) systems.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">HaluEval-Wild</em> benchmark <cite class="ltx_cite ltx_citemacro_cite">Zhu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib25" title="">2024</a>)</cite> offers a novel approach to evaluating LLM hallucinations in real-world settings. By categorizing challenging user queries into five distinct types, this tool provides essential insights for enhancing LLM reliability in scenarios that mirror real-world interactions, which is crucial for QA systems.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib23" title="">2024</a>)</cite> contribute with <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.1">MIGRES</em>, a method that uses LLMs‚Äô ability to identify missing information for targeted knowledge retrieval and extraction. This approach promises to improve the groundedness of responses by ensuring comprehensive information gathering.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">In long-form question answering, <cite class="ltx_cite ltx_citemacro_citet">Rosenthal et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib18" title="">2024</a>)</cite> introduced <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.1">ClapNQ</em>, a benchmark designed for retrieval-augmented generation (RAG) systems. Its emphasis on concise, cohesive answers grounded in source passages makes it particularly relevant for evaluating QA systems that require detailed, well-supported responses.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">An empirical evaluation of AI-driven legal research tools <cite class="ltx_cite ltx_citemacro_cite">Magesh et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib11" title="">2024</a>)</cite> challenges claims of "hallucination-free" systems, underscoring the necessity for rigorous evaluation in assessing the groundedness of legal QA systems.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Additionally, <cite class="ltx_cite ltx_citemacro_citet">Hong et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib4" title="">2024</a>)</cite> have launched the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p6.1.1">Hallucinations Leaderboard</em>, an open initiative for measuring and comparing hallucinations across various LLMs and tasks. This resource offers a valuable opportunity for benchmarking the groundedness of QA systems against a diverse range of models and applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Grounding Definition</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Grounding in legal question-answering systems refers to the extent to which an AI-generated response is firmly rooted in, supported by, and directly attributable to the provided legal source material. It ensures the model‚Äôs output aligns with and accurately represents the information in the input data, avoiding fabrication, extraneous details, or misleading content. A well-grounded response should adhere closely to the facts, legal principles, and reasoning presented in the source material, without introducing unsupported claims or misrepresenting the legal context <cite class="ltx_cite ltx_citemacro_cite">Chandu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib2" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Several key aspects ensure the reliability of AI-generated legal responses. Factual alignment and relevance are crucial, ensuring the content reflects the source documents and addresses the legal query accurately. Source attribution allows tracing information back to specific input texts, while legal interpretation fidelity ensures conclusions are substantiated by the provided materials. This involves not only accurately conveying factual information but also maintaining the integrity of legal procedures, correctly interpreting the context, and using appropriate legal terminology. The generated responses must adhere to the given context, avoiding unsupported claims or extrapolations, and preserving the nuances and complexities of legal language and concepts <cite class="ltx_cite ltx_citemacro_cite">Magesh et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib11" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The assessment of grounding in legal AI responses involves a comprehensive evaluation of how faithfully the generated content aligns with the retrieved legal context. This evaluation considers various aspects of the response, including its factual accuracy, the appropriateness of legal interpretations, the coherence of legal reasoning, and the proper use of legal terminology. Grounding is vital in legal applications to maintain the integrity of legal advice, ensure compliance with laws and precedents, and prevent misinformation. By ensuring strong grounding, legal question-answering systems can provide more reliable, trustworthy, and legally sound responses, which is crucial in the high-stakes environment of legal practice and decision-making.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Split</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">#Queries</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">#Responses</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">#Response Sentences</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.1.1">Training</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.1.2">400</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.1.3">1080</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.1.4">5671</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.3.2.1">Development</th>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.2.2">58</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.2.3">162</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.2.4">797</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.4.3.1">Testing</th>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.3.2">115</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.3.3">316</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.3.4">1516</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.T1.1.5.4.1">Total</th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S3.T1.1.5.4.2">573</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S3.T1.1.5.4.3">1558</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S3.T1.1.5.4.4">7984</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data Set Statistics</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Creation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we will describe and list all the steps involved in creating the <em class="ltx_emph ltx_font_italic" id="S4.p1.1.1">Groundedness Classification</em> dataset used in our benchmarking.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Source</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The dataset originates from proprietary data in the <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">Casetext</em> Legal Research Skill<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://casetext.com/cocounsel/" title="">https://casetext.com/cocounsel/</a></span></span></span>.
We limited the data selection to the internal users only, primarily consisting of diverse sales demonstrations as well as domain experts and engineering-related testing sessions.
All queries, however, are realistic representations of everyday research in the legal domain.
Additionally, we performed a deduplication process on the input queries.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The dataset comprises input queries (e.g., questions about particular legal use cases) accompanied by LLM-generated responses and retrieved context data.
During development, legal professionals verified these responses to ensure they were grounded in the context provided to the LLM (as part of the prompt).
The context data is derived from a retrieval system with access to the <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">Casetext</em> database for legal research, which includes case law, statutes, regulations, and legal texts authored by internal legal experts and lawyers.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The ground truth responses (LLM-based answers) were generated using custom instructions in a prompt to <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.1">GPT-4</em> in the current production environment.
At this stage of the dataset creation process, we had compiled a selection of legal user queries, gold responses, and their corresponding contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Synthetic Adaptation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The next step in our dataset creation process involved generating evoked ungrounded responses to evaluate both grounded and ungrounded outputs.
We instructed <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">GPT-4o</em> to make subtle and unintrusive variations to the original grounded responses, preserving most of the meaning while introducing minor deviations from the provided context.
In the prompt, we included the original query and context alongside the gold response and these instructions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">These adapted responses, which we consider partially ungrounded<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Only some sentences ended up with slight modifications, while most were kept as the original sentences.</span></span></span>, complement our final dataset.
The inclusion of both grounded and ungrounded responses allows for a more comprehensive evaluation of response quality and adherence to provided context.
An example of this subtle deviation from the source material in the generated response was depicted in the leading example in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data Splits</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We divided the dataset into training, development, and test sets using a ratio of <math alttext="70:10:20" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">70</mn><mo id="S4.SS3.p1.1.m1.1.1.3" lspace="0.278em" rspace="0.278em" xref="S4.SS3.p1.1.m1.1.1.3.cmml">:</mo><mn id="S4.SS3.p1.1.m1.1.1.4" xref="S4.SS3.p1.1.m1.1.1.4.cmml">10</mn><mo id="S4.SS3.p1.1.m1.1.1.5" lspace="0.278em" rspace="0.278em" xref="S4.SS3.p1.1.m1.1.1.5.cmml">:</mo><mn id="S4.SS3.p1.1.m1.1.1.6" xref="S4.SS3.p1.1.m1.1.1.6.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><and id="S4.SS3.p1.1.m1.1.1a.cmml" xref="S4.SS3.p1.1.m1.1.1"></and><apply id="S4.SS3.p1.1.m1.1.1b.cmml" xref="S4.SS3.p1.1.m1.1.1"><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">:</ci><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.2">70</cn><cn id="S4.SS3.p1.1.m1.1.1.4.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.4">10</cn></apply><apply id="S4.SS3.p1.1.m1.1.1c.cmml" xref="S4.SS3.p1.1.m1.1.1"><ci id="S4.SS3.p1.1.m1.1.1.5.cmml" xref="S4.SS3.p1.1.m1.1.1.5">:</ci><share href="https://arxiv.org/html/2410.08764v1#S4.SS3.p1.1.m1.1.1.4.cmml" id="S4.SS3.p1.1.m1.1.1d.cmml" xref="S4.SS3.p1.1.m1.1.1"></share><cn id="S4.SS3.p1.1.m1.1.1.6.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.6">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">70:10:20</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">70 : 10 : 20</annotation></semantics></math>, respectively.
This split ensures a representative distribution across all subsets while maintaining a sufficiently large test set for robust evaluation.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The resulting counts for each split are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S3.T1" title="Table 1 ‚Ä£ 3 Grounding Definition ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.
It is noteworthy that the number of responses is not exactly twice the number of queries.
This discrepancy arises from our dataset creation process, where we retained multiple significant variations of generated responses for certain queries to enhance the diversity and coverage of our dataset.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">To maintain the integrity of our evaluation, we ensured that all responses corresponding to a particular query were assigned to the same split.
This approach prevents potential leakage between the training and evaluation sets, thereby providing a more accurate assessment of model performance on unseen data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Benchmarking Methodologies</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section overviews the diverse methodologies employed in our benchmark study for quantifying response grounding, systematically evaluating approaches that assess adherence of generated responses to provided context.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Similarity-based Approaches</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Similarity-based approaches compare each response sentence against all context sentences, allowing for detailed grounding assessment.
We aggregate these sentence-level estimations for the final response-level prediction.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Semantic Similarity</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">We embedded sentences using the <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS0.Px1.p1.1.1">nlpaueb/legal-bert-base-uncased</em> model with the Sentence-Transformers library.
Matching pairs were identified using cosine similarity, with an optimized threshold determined on the development set for final grounding prediction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Quoted Information Precision</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.2">Adapting the <em class="ltx_emph ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.2.1">QuIP-score</em> <cite class="ltx_cite ltx_citemacro_cite">Weller et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib24" title="">2024</a>)</cite>, we examined character <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">italic_n</annotation></semantics></math>-gram overlap between LLM responses and context sentences.
We optimized both the <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S5.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S5.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.2.m2.1d">italic_n</annotation></semantics></math>-gram size (21 in our setup) and similarity threshold on the development set for grounding determination in the final evaluation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Natural Language Inference</h3>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">FactKB</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">Evaluating factual consistency in natural language generation is crucial, especially for complex domains.
We employed FactKB<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hf.co/bunsenfeng/FactKB" title="">https://hf.co/bunsenfeng/FactKB</a></span></span></span>, an approach leveraging pre-training with facts from external knowledge bases, to address challenges in entity and relation errors <cite class="ltx_cite ltx_citemacro_cite">Feng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib3" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p2.1"><em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px1.p2.1.1">FactKB</em> has shown state-of-the-art performance in factual consistency evaluation across various domains.
We used it to compute factuality scores of generated response sentences against source context sentences.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p3.1">Our grounding determination process involved identifying the highest-scoring source sentence for each target sentence based on <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px1.p3.1.1">FactKB</em> scores, then applying an optimized threshold to classify grounding sufficiency.
This threshold, determined using our development set, balanced precision and recall in grounding classification, adapting <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px1.p3.1.2">FactKB</em> to our specific task of response grounding quantification.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Hallucination Evaluation Models</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">The Hallucination Evaluation Model (HEM), developed by Vectara <cite class="ltx_cite ltx_citemacro_cite">Hughes et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib6" title="">2023</a>)</cite>, is designed to detect hallucinations in LLM-generated responses.
HEM is available in two versions: <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.1">V1</em>, a fine-tuned model based on <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.2">cross-encoder/nli-deberta-v3-base</em>, and <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.3">V2</em>, an improved version using <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.4">flan-t5-base</em>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p2.1">Built on research in factual consistency for summarization, HEM classifies whether a summary is factually consistent with its source.
The model was fine-tuned on diverse documents to ensure robustness across content types and is publicly available on Hugging Face under the Apache 2 license.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p3.1">HEM evaluates LLM responses by comparing them to source documents, classifying summaries as consistent or inconsistent.
For our study, we implemented a fine-grained approach, scoring individual sentences against corresponding contexts.
This granular analysis provides a nuanced assessment of hallucinations at the sentence level, offering deeper insights into model performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Prompting Approaches</h3>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Direct Prompting</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">One straight-forward approach for groundedness classification via prompting is asking either the same or another LLM whether a particular response for a query is grounded in a context or not <cite class="ltx_cite ltx_citemacro_cite">Trautmann et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib21" title="">2022</a>)</cite>.
Therefore, we utilized several LLMs with a custom prompt and collected the binary classification as the prompt-based baselines.
We used the specialized open access model <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.1.1">Lynx-v1.1</em> <cite class="ltx_cite ltx_citemacro_cite">Ravi et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib16" title="">2024</a>)</cite> and the general purpose public LLMs <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.1.2">GPT-4o</em> and <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px1.p1.1.3">Claude Sonnet 3.5</em>.
All three LLMs were evaluated with the same prompt from <cite class="ltx_cite ltx_citemacro_citet">Ravi et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib16" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p2.1">In principle, this approach has similarities with <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px1.p2.1.1">Reflexion</em> by <cite class="ltx_cite ltx_citemacro_citet">Shinn et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib19" title="">2024</a>)</cite>, where a <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px1.p2.1.2">Self-Reflection</em> LLM should reflect on a previous answer and if necessary to update its prediction.
The authors showed that this was helpful, especially for more complex tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Amazon RefChecker</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">RefChecker <cite class="ltx_cite ltx_citemacro_cite">Hu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib5" title="">2024</a>)</cite> introduces a framework for hallucination detection using knowledge triplets to capture fine-grained assertions.
The process involves three steps: claim extraction, hallucination checking, and aggregation.
This decoupled process is also known as prompt chaining <cite class="ltx_cite ltx_citemacro_cite">Trautmann (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib20" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p2.1">An LLM identifies knowledge triplets from the response to the original query.
Zero-shot checkers then predict hallucination labels for each triplet (entailment, contradiction, or neutral).
Finally, these labels are integrated to compute an overall hallucination score for the response.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p3">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p3.1">RefChecker‚Äôs computational demands are notable: for n triplets extracted, the LLM is prompted with the entire original context n times, significantly impacting processing time and resource consumption.
This approach balances granular analysis with computational intensity, offering a detailed but resource-intensive method for hallucination detection.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">SelfCheckGPT</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.1">We adapt the approach of <cite class="ltx_cite ltx_citemacro_citet">Manakul et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib12" title="">2023</a>)</cite>, which assesses hallucination likelihood in LLM-generated sentences by evaluating their consistency with multiple answers from the same query.
SelfCheckGPT assumes that grounded sentences should be consistent with other sampled answers.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p2.1">The method generates new responses using the initial prompt with increased temperature.
It then calculates a hallucination score for each sentence as the average of contradiction probabilities with these new samples.
The response-level score is the maximum of sentence-level scores, with the threshold optimized on the training set.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p3">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p3.1">We enhance this approach with a novel context-based evaluation (<em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px3.p3.1.1">ContextNLI</em>) using the <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px3.p3.1.2">potsawee/deberta-v3-large-mnli</em> model.
This compares each answer sentence against context sentences, identifying the minimum contradiction score as the hallucination probability.
The maximum score across all sentences represents the answer‚Äôs overall hallucination likelihood.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p4">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p4.1">We implement two variants of this approach: <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px3.p4.1.1">Multi-Gen</em>, which follows the original consistency checks, and our novel <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px3.p4.1.2">ContextNLI</em>, which incorporates the context-based evaluation, thus providing complementary methods for assessing the groundedness of LLM-generated content.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">DeepEval: Claims Extraction and Verification</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px4.p1.1">We adapt the <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS0.Px4.p1.1.1">Faithfulness</span> metrics from <cite class="ltx_cite ltx_citemacro_citet">Ip (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib7" title="">2023</a>)</cite> to detect contradictions between source documents and generated answers. This approach divides the task into two subtasks: claims extraction and claim verification (prompt chains, <cite class="ltx_cite ltx_citemacro_citet">Trautmann (<a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#bib.bib20" title="">2023</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px4.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px4.p2.1">First, we use an LLM to extract claims independently from both source documents and generated answers using a custom prompt. Then, a second LLM call with another custom prompt identifies claims from the generated answer not factually supported by the source document claims. If any generated claim contradicts a source claim, we consider the answer inaccurate.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS0.Px4.p3">
<p class="ltx_p" id="S5.SS3.SSS0.Px4.p3.1">This method requires three LLM calls in total: two for claims extraction and one for comparison. We utilize <em class="ltx_emph ltx_font_italic" id="S5.SS3.SSS0.Px4.p3.1.1">Claude Sonnet 3.5</em> for all these calls, balancing task complexity reduction with comprehensive analysis.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Fine-Tuning</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In addition to our primary methods, we fine-tuned a Cross-Encoder classifier (<em class="ltx_emph ltx_font_italic" id="S5.SS4.p1.1.1">DeBERTa v3</em> as the base model) specifically tailored to our dataset.
To ensure the integrity of our evaluation, we meticulously prepared a specialized training and evaluation corpus based on the initial data splits, thereby avoiding any potential contamination between sets.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Our fine-tuning approach focused on the nuanced differences between grounded and ungrounded responses.
For each pair of such responses, we isolated the sentences that differed between them.
This selective process allowed us to concentrate on the most informative elements for distinguishing between grounded and ungrounded content.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.2">To establish ground truth for the grounded responses, we employed a semantic similarity measure (as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS1" title="5.1 Similarity-based Approaches ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">5.1</span></a>).
For each sentence in the grounded response, we identified the most semantically similar sentence from the context and assigned it the corresponding cosine similarity score.
These scores typically ranged from <math alttext="0.8" class="ltx_Math" display="inline" id="S5.SS4.p3.1.m1.1"><semantics id="S5.SS4.p3.1.m1.1a"><mn id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><cn id="S5.SS4.p3.1.m1.1.1.cmml" type="float" xref="S5.SS4.p3.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.1.m1.1d">0.8</annotation></semantics></math> to <math alttext="0.99" class="ltx_Math" display="inline" id="S5.SS4.p3.2.m2.1"><semantics id="S5.SS4.p3.2.m2.1a"><mn id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">0.99</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><cn id="S5.SS4.p3.2.m2.1.1.cmml" type="float" xref="S5.SS4.p3.2.m2.1.1">0.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">0.99</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.2.m2.1d">0.99</annotation></semantics></math>, indicating high levels of semantic alignment.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">Conversely, for the ungrounded responses, we paired each sentence with the same context sentence used for its grounded counterpart.
However, we assigned these pairs a score of <math alttext="1" class="ltx_Math" display="inline" id="S5.SS4.p4.1.m1.1"><semantics id="S5.SS4.p4.1.m1.1a"><mn id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><cn id="S5.SS4.p4.1.m1.1.1.cmml" type="integer" xref="S5.SS4.p4.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p4.1.m1.1d">1</annotation></semantics></math> minus the cosine similarity, effectively inverting the grounding measure.
This approach provided a balanced representation of both grounded and ungrounded examples in our training data.</p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<p class="ltx_p" id="S5.SS4.p5.2">Through this methodology, we compiled a balanced dataset comprising <math alttext="558" class="ltx_Math" display="inline" id="S5.SS4.p5.1.m1.1"><semantics id="S5.SS4.p5.1.m1.1a"><mn id="S5.SS4.p5.1.m1.1.1" xref="S5.SS4.p5.1.m1.1.1.cmml">558</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.1.m1.1b"><cn id="S5.SS4.p5.1.m1.1.1.cmml" type="integer" xref="S5.SS4.p5.1.m1.1.1">558</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.1.m1.1c">558</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.1.m1.1d">558</annotation></semantics></math> samples for training and <math alttext="75" class="ltx_Math" display="inline" id="S5.SS4.p5.2.m2.1"><semantics id="S5.SS4.p5.2.m2.1a"><mn id="S5.SS4.p5.2.m2.1.1" xref="S5.SS4.p5.2.m2.1.1.cmml">75</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.2.m2.1b"><cn id="S5.SS4.p5.2.m2.1.1.cmml" type="integer" xref="S5.SS4.p5.2.m2.1.1">75</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.2.m2.1c">75</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.2.m2.1d">75</annotation></semantics></math> for development.
This carefully curated dataset served as the foundation for our fine-tuning process, enabling the Cross-Encoder to learn the subtle distinctions between grounded and ungrounded content within our specific corpus.</p>
</div>
<div class="ltx_para" id="S5.SS4.p6">
<p class="ltx_p" id="S5.SS4.p6.1">The outcomes of our fine-tuning efforts (after hyper-parameter optimization), are comprehensively presented (macro averaged) in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.T2" title="Table 2 ‚Ä£ 5.4 Fine-Tuning ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Model Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">M-Prec</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">M-Rec</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1">M-F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.5.1">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.2.1.1">deberta-v3-base</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">0.459</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.3">0.466</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.4">0.450</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.5">0.493</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.3.2.1">deberta-v3-large</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.3.2.2">0.736</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.3.2.3">0.739</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.3.2.4">0.733</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.3.2.5">0.733</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>DEV set metrics for DeBERTa models</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p7">
<p class="ltx_p" id="S5.SS4.p7.1">Following the fine-tuning stage, we integrated this grounding classification (GC) model into our benchmark, employing a methodology analogous to that used for the NLI approaches described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S5.SS2" title="5.2 Natural Language Inference ‚Ä£ 5 Benchmarking Methodologies ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental Set-Up</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our benchmarking study aimed to evaluate various methods for classifying LLM responses as grounded or ungrounded relative to a given context and query.</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Methodology</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Despite the varied granularity of approaches (response-level vs. sentence-level), we standardized outputs to binary classifications for consistent comparison.
We developed each method on the training set, optimized parameters on the development set, and conducted final evaluations on the test set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Performance Metrics</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">We assessed classification accuracy (including macro-averaged f1, precision, and recall) and computational efficiency through latency measurements.
Latency was computed as the average processing time across all samples in the development set.
These metrics provide insights into each approach‚Äôs practical applicability.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Computational Resources</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">Local approaches utilized Amazon EC2 G5 Instances (8xlarge)<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/ec2/instance-types/g5/" title="">https://aws.amazon.com/ec2/instance-types/g5/</a></span></span></span>. Prompting-based methods were executed via Azure OpenAI Services<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" title="">https://azure.microsoft.com/en-us/products/ai-services/openai-service</a></span></span></span>, AWS Bedrock (Anthropic‚Äôs Claude)<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/bedrock/claude/" title="">https://aws.amazon.com/bedrock/claude/</a></span></span></span>, and Anthropic‚Äôs API directly, ensuring diverse and robust evaluation environments.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:411.9pt;height:221.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.3pt,33.4pt) scale(0.767862848176389,0.767862848176389) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S6.T3.1.1.1.1.1.1">#</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.1.1.2" rowspan="2"><span class="ltx_text" id="S6.T3.1.1.1.1.2.1">Model Name</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4" id="S6.T3.1.1.1.1.3">Development Set</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S6.T3.1.1.1.1.4">Test Set</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.1">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.2">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.3">Macro-F1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.1.1.2.2.4">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.5">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.6">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.7">Macro-F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.2.8">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.3.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.3.3.1">1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.3.3.2">COS_SIM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.3">0.525</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.4">0.520</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.5">0.494</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.1.1.3.3.6">0.520</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.7">0.497</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.8">0.497</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.9">0.493</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.3.3.10">0.497</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.4.4.1">2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.4.4.2">QUIP</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.3">0.648</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.4">0.533</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.5">0.421</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.4.4.6">0.533</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.7">0.560</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.8">0.509</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.9">0.379</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.4.10">0.509</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.5.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.5.5.1">3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.5.5.2">HEM V1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.3">0.640</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.4">0.640</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.5">0.640</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.1.1.5.5.6">0.640</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.7">0.598</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.8">0.595</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.9">0.592</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.5.5.10">0.595</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.6.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.6.6.1">4</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.6.6.2">HEM V2</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.3">0.580</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.4">0.580</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.5">0.580</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.6.6.6">0.580</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.7">0.564</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.8">0.563</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.9">0.562</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.6.6.10">0.563</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.7.7">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.7.7.1">5</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.7.7.2">FACT_KB</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.3">0.527</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.4">0.527</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.5">0.526</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.7.7.6">0.527</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.7">0.510</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.8">0.510</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.9">0.508</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.7.7.10">0.510</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.8.8">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.8.8.1">6</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.8.8.2">GC-large</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.3">0.694</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.4">0.667</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.5">0.655</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.8.8.6">0.667</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.7">0.628</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.8">0.620</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.9">0.615</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.8.8.10">0.620</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.9.9">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.9.9.1">7</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.9.9.2">LYNX v1.1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.3">0.764</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.4">0.460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.5">0.571</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.1.1.9.9.6">0.460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.7">0.792</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.8">0.503</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.9">0.597</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.9.9.10">0.503</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.10.10">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.10.10.1">8</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.10.10.2">Sonnet 3.5</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.3">0.728</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.4">0.727</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.5">0.726</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.10.10.6">0.727</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.7">0.724</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.8">0.715</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.9">0.712</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.10.10.10">0.715</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.11.11">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.11.11.1">9</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.11.11.2">GPT-4o</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.3.1">0.783</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.4.1">0.773</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.5.1">0.771</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.11.11.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.6.1">0.773</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.7"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.11.11.7.1">0.802</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.8.1">0.763</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.9.1">0.755</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.11.11.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.11.11.10.1">0.763</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.12.12">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.12.12.1">10</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.1.1.12.12.2">RefChecker (Haiku)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.3">0.511</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.4">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.5">0.450</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.1.1.12.12.6">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.7">0.514</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.8">0.507</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.9">0.435</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.12.12.10">0.508</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.13.13">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.13.13.1">11</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.13.13.2">RefChecker (Sonnet 3)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.3">0.500</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.4">0.500</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.5">0.366</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.13.13.6">0.500</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.7">0.500</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.8">0.500</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.9">0.386</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.13.13.10">0.500</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.14.14">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.14.14.1">12</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.14.14.2">DeepEval Claims Verify</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.3.1">0.801</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.4.1">0.800</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.5"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.5.1">0.800</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.14.14.6"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.6.1">0.800</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T3.1.1.14.14.7.1">0.779</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.8"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.8.1">0.774</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.9"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.9.1">0.774</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.14.14.10"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.14.14.10.1">0.775</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.15.15">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.15.15.1">13</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.1.1.15.15.2">SCGPT (Multi-Gen)</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.3">0.627</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.4">0.627</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.5">0.627</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.15.15.6">0.627</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.7">0.679</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.8">0.667</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.9">0.661</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.1.15.15.10">0.667</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.16.16">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S6.T3.1.1.16.16.1">14</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S6.T3.1.1.16.16.2">SCGPT (ContextNLI)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.3">0.620</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.4">0.620</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.5">0.620</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T3.1.1.16.16.6">0.620</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.7">0.610</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.8">0.604</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.9">0.600</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.1.1.16.16.10">0.604</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison of different models on Development and Test sets</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="432" id="S6.F2.g1" src="x2.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Development set results for our benchmark. We report the F1-scores (y-axis) for each method and the corresponding latency (x-axis) in seconds per response. Approach names denoted with <em class="ltx_emph ltx_font_italic" id="S6.F2.3.1">*</em> were run on an AWS <em class="ltx_emph ltx_font_italic" id="S6.F2.4.2">ml.8xlarge</em> instance.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="S6.F3.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Counts of unique error types in the development set. Some responses contained up to three different error types. The frequency axis is in log-scale.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.1" style="width:281.9pt;height:114pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.9pt,6.0pt) scale(0.904653828569553,0.904653828569553) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.1.1">Error Type</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.2.1">Misclassified</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.3.1">Total</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S6.T4.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.4.1">Percentage</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.1.1.2.1.1">Terminological Errors</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T4.1.1.2.1.2">2</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T4.1.1.2.1.3">3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.1.2.1.4">66.7%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T4.1.1.3.2.1">Factual Inaccuracies</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.3.2.2">12</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.3.2.3">59</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.1.3.2.4">20.3%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T4.1.1.4.3.1">Procedural Errors</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.4.3.2">1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.4.3.3">5</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.1.4.3.4">20.0%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T4.1.1.5.4.1">Reasoning Errors</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.5.4.2">2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.5.4.3">15</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.1.5.4.4">13.3%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T4.1.1.6.5.1">Contextual Misinterpretations</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.6.5.2">1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T4.1.1.6.5.3">9</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.1.6.5.4">11.1%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S6.T4.1.1.7.6.1">Misattributions</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S6.T4.1.1.7.6.2">0</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S6.T4.1.1.7.6.3">1</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S6.T4.1.1.7.6.4">0.0%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Development set misclassification of the best performing model by error types.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Groundedness Classification Results</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our benchmark evaluation of groundedness classification approaches revealed insightful performance trade-offs, as shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.T3" title="Table 3 ‚Ä£ Computational Resources ‚Ä£ 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">3</span></a>.
The metrics include classification precision, recall, F1-score, and accuracy, providing a comprehensive view of each method‚Äôs applicability.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.2">The multi-stage prompt chaining approach, <em class="ltx_emph ltx_font_italic" id="S7.p2.2.1">DeepEval Claims Verify</em>, achieved top classification metrics, but with high latency (<math alttext="26.1" class="ltx_Math" display="inline" id="S7.p2.1.m1.1"><semantics id="S7.p2.1.m1.1a"><mn id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">26.1</mn><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><cn id="S7.p2.1.m1.1.1.cmml" type="float" xref="S7.p2.1.m1.1.1">26.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">26.1</annotation><annotation encoding="application/x-llamapun" id="S7.p2.1.m1.1d">26.1</annotation></semantics></math> seconds per request).
In contrast, <em class="ltx_emph ltx_font_italic" id="S7.p2.2.2">direct prompting</em> with <em class="ltx_emph ltx_font_italic" id="S7.p2.2.3">GPT-4o</em> achieved the second-highest scores with significantly lower latency (<math alttext="2.2" class="ltx_Math" display="inline" id="S7.p2.2.m2.1"><semantics id="S7.p2.2.m2.1a"><mn id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">2.2</mn><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><cn id="S7.p2.2.m2.1.1.cmml" type="float" xref="S7.p2.2.m2.1.1">2.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">2.2</annotation><annotation encoding="application/x-llamapun" id="S7.p2.2.m2.1d">2.2</annotation></semantics></math> seconds), as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.F2" title="Figure 2 ‚Ä£ Computational Resources ‚Ä£ 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">A clear speed-performance trade-off emerged across methods.
Similarity-based approaches (<em class="ltx_emph ltx_font_italic" id="S7.p3.1.1">COS_SIM</em> and <em class="ltx_emph ltx_font_italic" id="S7.p3.1.2">QUIP</em>) were fastest but struggled with ungrounded response identification.
NLI methods showed improved performance at the cost of increased latency.
Within NLI, <em class="ltx_emph ltx_font_italic" id="S7.p3.1.3">HEM V1</em> outperformed <em class="ltx_emph ltx_font_italic" id="S7.p3.1.4">HEM V2</em>, and fine-tuning on our corpus further improving results.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Unexpectedly, complex prompt chaining approaches like <em class="ltx_emph ltx_font_italic" id="S7.p4.1.1">RefChecker</em> and <em class="ltx_emph ltx_font_italic" id="S7.p4.1.2">SelfCheckGPT</em> underperformed, highlighting challenges in developing universally effective methods across diverse contexts.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">These findings emphasize the importance of balancing task performance and computational efficiency when selecting a groundedness classification approach, with optimal choices depending on specific application requirements and resource constraints.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Error Analysis</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We conducted a detailed investigation into the types of response errors present in our benchmark dataset to gain deeper insights into ungrounded content.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Through examination of error spans in the training set, we identified six distinct error types.
The models were instructed to select from our predefined error types (Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.T5" title="Table 5 ‚Ä£ A.2 Description and Examples ‚Ä£ Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">5</span></a>, App. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.SS2" title="A.2 Description and Examples ‚Ä£ Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">A.2</span></a>).</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Focusing on the development set, our analysis revealed interesting patterns.
The LLMs achieved exact agreement on the hallucination type in <math alttext="29\%" class="ltx_Math" display="inline" id="S8.p3.1.m1.1"><semantics id="S8.p3.1.m1.1a"><mrow id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml"><mn id="S8.p3.1.m1.1.1.2" xref="S8.p3.1.m1.1.1.2.cmml">29</mn><mo id="S8.p3.1.m1.1.1.1" xref="S8.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><apply id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1"><csymbol cd="latexml" id="S8.p3.1.m1.1.1.1.cmml" xref="S8.p3.1.m1.1.1.1">percent</csymbol><cn id="S8.p3.1.m1.1.1.2.cmml" type="integer" xref="S8.p3.1.m1.1.1.2">29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">29\%</annotation><annotation encoding="application/x-llamapun" id="S8.p3.1.m1.1d">29 %</annotation></semantics></math> of cases, with at least one overlapping error type for each response.
<em class="ltx_emph ltx_font_italic" id="S8.p3.1.1">GPT-4o</em> typically predicted a single error type, while <em class="ltx_emph ltx_font_italic" id="S8.p3.1.2">Claude-3.5-Sonnet</em> often suggested multiple types per response.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">We aggregated predictions where both LLMs agreed.
The distribution of unique error types is visualized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.F3" title="Figure 3 ‚Ä£ Computational Resources ‚Ä£ 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, with per-response occurrences in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.F4" title="Figure 4 ‚Ä£ A.1 Dev Set Error Types ‚Ä£ Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">4</span></a> (App. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.SS1" title="A.1 Dev Set Error Types ‚Ä£ Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">A.1</span></a>).
<em class="ltx_emph ltx_font_italic" id="S8.p4.1.1">Factual Inaccuracies</em> were most common, followed by <em class="ltx_emph ltx_font_italic" id="S8.p4.1.2">Reasoning Errors</em>.
All initially defined error types were represented, validating our classification scheme.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">This analysis provides valuable insights into response error types and ungrounded content in language model outputs, crucial for developing targeted strategies to improve response generation.</p>
</div>
<section class="ltx_paragraph" id="S8.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Misclassification Analysis</h4>
<div class="ltx_para" id="S8.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS0.SSS0.Px1.p1.3">We conducted a misclassification analysis on our best-performing model, <em class="ltx_emph ltx_font_italic" id="S8.SS0.SSS0.Px1.p1.3.1">DeepEval Claims Verify</em>, to gain deeper insights into its performance across different error types.
As summarized in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#S6.T4" title="Table 4 ‚Ä£ Computational Resources ‚Ä£ 6 Experimental Set-Up ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">4</span></a>, <em class="ltx_emph ltx_font_italic" id="S8.SS0.SSS0.Px1.p1.3.2">Terminological Errors</em> showed the highest misclassification rate (<math alttext="67\%" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S8.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S8.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">67</mn><mo id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn id="S8.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px1.p1.1.m1.1.1.2">67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.1.m1.1c">67\%</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.1.m1.1d">67 %</annotation></semantics></math>), despite their low frequency, followed by <em class="ltx_emph ltx_font_italic" id="S8.SS0.SSS0.Px1.p1.3.3">Factual Inaccuracies</em> (<math alttext="20\%" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S8.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S8.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">20</mn><mo id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn id="S8.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px1.p1.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.2.m2.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.2.m2.1d">20 %</annotation></semantics></math>) and <em class="ltx_emph ltx_font_italic" id="S8.SS0.SSS0.Px1.p1.3.4">Procedural Errors</em> (<math alttext="20\%" class="ltx_Math" display="inline" id="S8.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S8.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S8.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S8.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">20</mn><mo id="S8.SS0.SSS0.Px1.p1.3.m3.1.1.1" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S8.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S8.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn id="S8.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="integer" xref="S8.SS0.SSS0.Px1.p1.3.m3.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.3.m3.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S8.SS0.SSS0.Px1.p1.3.m3.1d">20 %</annotation></semantics></math>).
These findings reveal the varying challenges posed by different error categories and highlight areas for potential improvement in groundedness classification models, particularly in handling less common but difficult-to-classify error types.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.2">Our comprehensive benchmark study on groundedness classification of legal question-answering systems has revealed significant insights into performance and efficiency trade-offs.
The multi-stage prompt chaining approach, <em class="ltx_emph ltx_font_italic" id="S9.p1.2.1">DeepEval Claims Verify</em>, emerged as the top performer with an F1 score of <math alttext="0.80" class="ltx_Math" display="inline" id="S9.p1.1.m1.1"><semantics id="S9.p1.1.m1.1a"><mn id="S9.p1.1.m1.1.1" xref="S9.p1.1.m1.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S9.p1.1.m1.1b"><cn id="S9.p1.1.m1.1.1.cmml" type="float" xref="S9.p1.1.m1.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.p1.1.m1.1c">0.80</annotation><annotation encoding="application/x-llamapun" id="S9.p1.1.m1.1d">0.80</annotation></semantics></math>, closely followed by direct prompting using <em class="ltx_emph ltx_font_italic" id="S9.p1.2.2">GPT-4o</em> at <math alttext="0.77" class="ltx_Math" display="inline" id="S9.p1.2.m2.1"><semantics id="S9.p1.2.m2.1a"><mn id="S9.p1.2.m2.1.1" xref="S9.p1.2.m2.1.1.cmml">0.77</mn><annotation-xml encoding="MathML-Content" id="S9.p1.2.m2.1b"><cn id="S9.p1.2.m2.1.1.cmml" type="float" xref="S9.p1.2.m2.1.1">0.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.p1.2.m2.1c">0.77</annotation><annotation encoding="application/x-llamapun" id="S9.p1.2.m2.1d">0.77</annotation></semantics></math>, which demonstrated lower latency.
These results highlight the potential of advanced prompting techniques in achieving high accuracy.</p>
</div>
<div class="ltx_para" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">Similarity-based and natural language inference methods, while less accurate, offered fast processing times.
Our response error type classification identified <em class="ltx_emph ltx_font_italic" id="S9.p2.1.1">Factual Inaccuracies</em> and <em class="ltx_emph ltx_font_italic" id="S9.p2.1.2">Reasoning Errors</em> as the most prevalent types of ungrounded content, providing direction for future improvements.</p>
</div>
<div class="ltx_para" id="S9.p3">
<p class="ltx_p" id="S9.p3.1">The study underscores the critical balance between task performance, computational efficiency, and ease of implementation when selecting groundedness classification methods.
With top-performing methods achieving F1 scores of <math alttext="0.80" class="ltx_Math" display="inline" id="S9.p3.1.m1.1"><semantics id="S9.p3.1.m1.1a"><mn id="S9.p3.1.m1.1.1" xref="S9.p3.1.m1.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S9.p3.1.m1.1b"><cn id="S9.p3.1.m1.1.1.cmml" type="float" xref="S9.p3.1.m1.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.p3.1.m1.1c">0.80</annotation><annotation encoding="application/x-llamapun" id="S9.p3.1.m1.1d">0.80</annotation></semantics></math>, this benchmark represents a significant advancement in the reliable assessment of AI-generated content across diverse applications.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While our study offers valuable insights into the performance of various groundedness classification approaches, it is essential to acknowledge several limitations inherent in our experimental setup and the methods we evaluated.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Firstly, our dataset, though carefully curated, is limited in size and domain scope.
The responses were generated using specific language models and may not fully represent the diverse range of hallucinations or ungrounded content that could occur across different models or domains.
This limitation potentially affects the generalizability of our findings to broader contexts or more specialized applications.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Secondly, the binary classification of responses as either grounded or ungrounded may oversimplify the nuanced nature of language model outputs.
In reality, responses often contain a mix of grounded and ungrounded elements, and a more granular assessment might provide deeper insights into model behavior.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Our evaluation metrics, while standard in the field, may not capture all aspects of response quality or usefulness.
For instance, a response that is technically grounded but irrelevant or poorly structured might still receive a high rating within our current framework.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">The computational resources required for some of the more complex approaches, particularly those involving multiple API calls or large language models, pose scalability challenges.
This limitation may restrict the practical applicability of these methods in real-time or resource-constrained environments.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">Additionally, our error type classification, while informative, relies on the agreement between two specific language models.
This approach may introduce biases or limitations based on the particular characteristics of these models.</p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">Lastly, the rapid pace of development in language model technology means that our findings may quickly become outdated as new models and techniques emerge.
The performance gaps we observed between different approaches may shift with the introduction of more advanced models or refined methodologies.</p>
</div>
<div class="ltx_para" id="Sx1.p8">
<p class="ltx_p" id="Sx1.p8.1">Future work should address these limitations by expanding the dataset to include a broader range of domains and increasing its size.
Developing more nuanced classification frameworks that can capture the complexity of language model outputs would also be beneficial.
Furthermore, exploring scalable methods that can be applied in real-time or resource-constrained environments, as well as continuously updating the evaluation framework to reflect the latest advancements in language model technology, will be crucial for the ongoing relevance of this research.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This study on groundedness classification methods aims to improve the reliability and trustworthiness of AI-generated content, which has significant ethical implications.
By developing more accurate methods to detect ungrounded or hallucinated information, we contribute to the broader goal of mitigating the spread of misinformation and enhancing the integrity of AI-assisted communication.
Our work aligns with the principles of beneficence and non-maleficence, as it seeks to maximize the benefits of language models while minimizing potential harms associated with inaccurate or misleading information.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">We acknowledge that the development and deployment of these classification methods may have broader societal impacts.
We emphasize the importance of transparent and responsible use of these methods, respecting principles of fairness and privacy.
Furthermore, we encourage ongoing dialogue and collaboration within the NLP community to address the ethical challenges associated with AI-generated content and its evaluation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et¬†al. (2023)</span>
<span class="ltx_bibblock">
S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin¬†Tat Lee, Yuanzhi Li, Scott Lundberg, et¬†al. 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4. arxiv.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.12712</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Khyathi¬†Raghavi Chandu, Yonatan Bisk, and Alan¬†W Black. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-acl.375" title="">Grounding ‚Äògrounding‚Äô in NLP</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 4283‚Äì4305, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et¬†al. (2023)</span>
<span class="ltx_bibblock">
Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov. 2023.

</span>
<span class="ltx_bibblock">Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 933‚Äì952.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et¬†al. (2024)</span>
<span class="ltx_bibblock">
Giwon Hong, Aryo¬†Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu¬†Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, and Pasquale Minervini. 2024.

</span>
<span class="ltx_bibblock">The hallucinations leaderboard‚Äìan open effort to measure hallucinations in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2404.05904</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et¬†al. (2024)</span>
<span class="ltx_bibblock">
Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, and Zheng Zhang. 2024.

</span>
<span class="ltx_bibblock">Refchecker: Reference-based fine-grained hallucination checker and benchmark for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2405.14486</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hughes et¬†al. (2023)</span>
<span class="ltx_bibblock">
Simon Hughes, Minseok Bae, and Miaoran Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/vectara/hallucination-leaderboard" title="">Vectara Hallucination Leaderboard</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ip (2023)</span>
<span class="ltx_bibblock">
Jeffrey Ip. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/confident-ai/deepeval" title="">Deepeval: A tool for deep learning model evaluation</a>.

</span>
<span class="ltx_bibblock">GitHub repository.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khazaeli et¬†al. (2021)</span>
<span class="ltx_bibblock">
Soha Khazaeli, Janardhana Punuru, Chad Morris, Sanjay Sharma, Bert Staub, Michael Cole, Sunny Chiu-Webster, and Dhruv Sakalley. 2021.

</span>
<span class="ltx_bibblock">A free format legal question answering system.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Natural Legal Language Processing Workshop 2021</em>, pages 107‚Äì113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kry≈õci≈Ñski et¬†al. (2020)</span>
<span class="ltx_bibblock">
Wojciech Kry≈õci≈Ñski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020.

</span>
<span class="ltx_bibblock">Evaluating the factual consistency of abstractive text summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 9332‚Äì9346.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. (2023)</span>
<span class="ltx_bibblock">
Xingxuan Li, Ruochen Zhao, Yew¬†Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magesh et¬†al. (2024)</span>
<span class="ltx_bibblock">
Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher¬†D Manning, and Daniel¬†E Ho. 2024.

</span>
<span class="ltx_bibblock">Hallucination-free? assessing the reliability of leading ai legal research tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2405.20362</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manakul et¬†al. (2023)</span>
<span class="ltx_bibblock">
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.

</span>
<span class="ltx_bibblock">Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 9004‚Äì9017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez-Gil (2023)</span>
<span class="ltx_bibblock">
Jorge Martinez-Gil. 2023.

</span>
<span class="ltx_bibblock">A survey on legal question‚Äìanswering systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Computer Science Review</em>, 48:100552.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynez et¬†al. (2020)</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.173" title="">On faithfulness and factuality in abstractive summarization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 1906‚Äì1919, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monroy et¬†al. (2009)</span>
<span class="ltx_bibblock">
Alfredo Monroy, Hiram Calvo, and Alexander Gelbukh. 2009.

</span>
<span class="ltx_bibblock">Nlp for shallow question answering of legal documents using graphs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Conference on Intelligent Text Processing and Computational Linguistics</em>, pages 498‚Äì508. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravi et¬†al. (2024)</span>
<span class="ltx_bibblock">
Selvan¬†Sunitha Ravi, Bartosz Mielczarek, Anand Kannappan, Douwe Kiela, and Rebecca Qian. 2024.

</span>
<span class="ltx_bibblock">Lynx: An open source hallucination evaluation model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2407.08488</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawte et¬†al. (2023)</span>
<span class="ltx_bibblock">
Vipula Rawte, Amit Sheth, and Amitava Das. 2023.

</span>
<span class="ltx_bibblock">A survey of hallucination in large foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2309.05922</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal et¬†al. (2024)</span>
<span class="ltx_bibblock">
Sara Rosenthal, Avirup Sil, Radu Florian, and Salim Roukos. 2024.

</span>
<span class="ltx_bibblock">Clapnq: Cohesive long-form answers from passages in natural questions for rag systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2404.02103</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et¬†al. (2024)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trautmann (2023)</span>
<span class="ltx_bibblock">
Dietrich Trautmann. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.04138" title="">Large language model prompt chaining for long legal document classification</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2308.04138</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trautmann et¬†al. (2022)</span>
<span class="ltx_bibblock">
Dietrich Trautmann, Alina Petrova, and Frank Schilder. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.02199" title="">Legal prompt engineering for multilingual legal judgement prediction</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2212.02199</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vold and Conrad (2021)</span>
<span class="ltx_bibblock">
Andrew Vold and Jack¬†G Conrad. 2021.

</span>
<span class="ltx_bibblock">Using transformers to improve answer retrieval for legal questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law</em>, pages 245‚Äì249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2024)</span>
<span class="ltx_bibblock">
Keheng Wang, Feiyu Duan, Peiguang Li, Sirui Wang, and Xunliang Cai. 2024.

</span>
<span class="ltx_bibblock">Llms know what they need: Leveraging a missing information guided framework to empower retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2404.14043</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller et¬†al. (2024)</span>
<span class="ltx_bibblock">
Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van¬†Durme. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.eacl-long.140" title="">‚Äúaccording to . . . ‚Äù: Prompting language models improves quoting from pre-training data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2288‚Äì2301, St. Julian‚Äôs, Malta. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. (2024)</span>
<span class="ltx_bibblock">
Zhiying Zhu, Zhiqing Sun, and Yiming Yang. 2024.

</span>
<span class="ltx_bibblock">Halueval-wild: Evaluating hallucinations of language models in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2403.04307</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Response Error Types</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dev Set Error Types</h3>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="A1.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Counts of response error types in the development set. The frequency axis is in log-scale.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Description and Examples</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">See the table <a class="ltx_ref" href="https://arxiv.org/html/2410.08764v1#A1.T5" title="Table 5 ‚Ä£ A.2 Description and Examples ‚Ä£ Appendix A Response Error Types ‚Ä£ Measuring the Groundedness of Legal Question-Answering Systems"><span class="ltx_text ltx_ref_tag">5</span></a> for our six response error types with their descriptions and examples.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.1.1.1.1">
<span class="ltx_p" id="A1.T5.1.1.1.1.1.1" style="width:86.7pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1.1.1">Error Type</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.1.1.2.1">
<span class="ltx_p" id="A1.T5.1.1.1.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1.1.1">Short Description</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.1.1.3.1">
<span class="ltx_p" id="A1.T5.1.1.1.3.1.1" style="width:173.4pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.3.1.1.1">Examples</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.2.1.1.1">
<span class="ltx_p" id="A1.T5.1.2.1.1.1.1" style="width:86.7pt;">Factual Inaccuracies</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.2.1.2.1">
<span class="ltx_p" id="A1.T5.1.2.1.2.1.1" style="width:130.1pt;">Misrepresentation of established facts, dates, or details</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.2.1.3.1">
<span class="ltx_p" id="A1.T5.1.2.1.3.1.1" style="width:173.4pt;">1. Brown v. Board of Education was decided in 1964. 
<br class="ltx_break"/>2. The First Amendment protects only written speech.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.3.2.1.1">
<span class="ltx_p" id="A1.T5.1.3.2.1.1.1" style="width:86.7pt;">Contextual Misinterpretations</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.3.2.2.1">
<span class="ltx_p" id="A1.T5.1.3.2.2.1.1" style="width:130.1pt;">Misapplication of legal principles or inappropriate analogies</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.3.2.3.1">
<span class="ltx_p" id="A1.T5.1.3.2.3.1.1" style="width:173.4pt;">1. Applying Miranda rights to a civil tax dispute. 
<br class="ltx_break"/>2. Using Roe v. Wade precedent in a Second Amendment case.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.4.3.1.1">
<span class="ltx_p" id="A1.T5.1.4.3.1.1.1" style="width:86.7pt;">Procedural Errors</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.4.3.2.1">
<span class="ltx_p" id="A1.T5.1.4.3.2.1.1" style="width:130.1pt;">Mistakes in describing legal procedures or processes</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.4.3.3.1">
<span class="ltx_p" id="A1.T5.1.4.3.3.1.1" style="width:173.4pt;">1. A case goes directly from district court to the Supreme Court, skipping the appellate court. 
<br class="ltx_break"/>2. Claiming that jury selection occurs after opening statements in a trial.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.5.4.1.1">
<span class="ltx_p" id="A1.T5.1.5.4.1.1.1" style="width:86.7pt;">Reasoning Errors</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.5.4.2.1">
<span class="ltx_p" id="A1.T5.1.5.4.2.1.1" style="width:130.1pt;">Flawed arguments or unsupported legal conclusions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.5.4.3.1">
<span class="ltx_p" id="A1.T5.1.5.4.3.1.1" style="width:173.4pt;">1. Since the Fourth Amendment protects against unreasonable searches, all warrantless searches are unconstitutional. 
<br class="ltx_break"/>2. Because the Supreme Court ruled on abortion in Roe v. Wade, states cannot pass any abortion laws.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.6.5.1.1">
<span class="ltx_p" id="A1.T5.1.6.5.1.1.1" style="width:86.7pt;">Misattributions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.6.5.2.1">
<span class="ltx_p" id="A1.T5.1.6.5.2.1.1" style="width:130.1pt;">Incorrect assignment of opinions, quotes, or actions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A1.T5.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.6.5.3.1">
<span class="ltx_p" id="A1.T5.1.6.5.3.1.1" style="width:173.4pt;">1. Justice Scalia wrote the majority opinion in Obergefell v. Hodges. 
<br class="ltx_break"/>2. The phrase "separate but equal" originated from Brown v. Board of Education.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.7.6.1.1">
<span class="ltx_p" id="A1.T5.1.7.6.1.1.1" style="width:86.7pt;">Terminological Errors</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.7.6.2.1">
<span class="ltx_p" id="A1.T5.1.7.6.2.1.1" style="width:130.1pt;">Misuse or misinterpretation of legal terms or concepts</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T5.1.7.6.3.1">
<span class="ltx_p" id="A1.T5.1.7.6.3.1.1" style="width:173.4pt;">1. "Habeas corpus" refers to the right to a speedy trial. 
<br class="ltx_break"/>2. "Strict scrutiny" means that a law is automatically unconstitutional.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Response error types with a description and examples</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 12:20:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
