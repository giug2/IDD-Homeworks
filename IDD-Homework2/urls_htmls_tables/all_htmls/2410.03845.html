<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD</title>
<!--Generated on Fri Oct  4 18:15:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
OpenROAD,  conversational AI,  chatbot,  retrieval-augmented generation,  electronic design automation,  EDA,  LLM,  RAG,  ASIC design,  chip design
" lang="en" name="keywords"/>
<base href="/html/2410.03845v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S1" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S2" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Dataset Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S2.SS1" title="In II Dataset Generation ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Data Sources</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S2.SS2" title="In II Dataset Generation ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Issues and Discussion Analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">RAG Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.SS1" title="In III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Dataset Ingestion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.SS2" title="In III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Hybrid Retriever Function</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.SS3" title="In III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Domain Specific Retriever Tools</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.SS4" title="In III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Context-Aware Response Generation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S4" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Hosted Application</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S5" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation and Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S6" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S7" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Future Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S8" title="In ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Aviral Kaintura1,
Palaniappan R2,
Shui Song Luar3,
Indira Iyer Almeida3
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1National Forensic Sciences University Delhi Campus, New Delhi, India
<br class="ltx_break"/>2 BITS Pilani Hyderabad Campus, Hyderabad, Telangana, India
<br class="ltx_break"/>3Precision Innovations Inc.
</span>
<span class="ltx_contact ltx_role_affiliation">1102ctbmti2122019@nfsu.ac.in,
2f20212915@hyderabad.bits-pilani.ac.in,
3jluar@precisioninno.com,
3iiyer@precisioninno.com
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Open-source Electronic Design Automation (EDA) tools are rapidly transforming chip design by addressing key barriers of commercial EDA tools such as complexity, costs, and access. Recent advancements in Large Language Models (LLMs) have further enhanced efficiency in chip design by providing user assistance across a range of tasks like setup, decision-making, and flow automation. This paper introduces ORAssistant, a conversational assistant for OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to improve the user experience for the OpenROAD flow, from RTL-GDSII by providing context-specific responses to common user queries, including installation, command usage, flow setup, and execution, in prose format. Currently, ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and KLayout. The data model is built from publicly available documentation and GitHub resources. The proposed architecture is scalable, supporting extensions to other open-source tools, operating modes, and LLM models. We use Google Gemini as the base LLM model to build and test ORAssistant. Early evaluation results of the RAG-based model show notable improvements in performance and accuracy compared to non-fine-tuned LLMs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
OpenROAD, conversational AI, chatbot, retrieval-augmented generation, electronic design automation, EDA, LLM, RAG, ASIC design, chip design

</div>
<section class="ltx_section ltx_indent_first" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Since 2018, open-source EDA tools have rapidly democratized hardware design and driven innovation through research and collaboration. Free from licensing constraints, they foster a thriving ecosystem of chip design and education<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib3" title="">3</a>]</cite>. Recently, ML and GenAI-based chip design methodologies have been applied to open-source tools, yielding significant benefits in productivity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib4" title="">4</a>]</cite>. The open infrastructure of these tools simplifies model training, integration and enable the use of shared resources, such as documentation, scripts, and datasets for a host of deployment options<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">ORAssistant began as a Google Summer of Code (GSoC) 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib6" title="">6</a>]</cite> project<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Palaniappan R and Aviral Kaintura contributed equally to this project under the mentorship of Indira Iyer and Jack Luar.</span></span></span> with an objective of assisting OpenROAD users in completing basic tasks successfully ‚Äî from setup to flow execution. We focused on addressing frequently occurring problems in areas such as installation, design setup and flow, command usage, and debugging. Traditional user resources, such as documentation and tutorials, tend to become outdated quickly and fail to incorporate practical knowledge gained from collaborative user experiences. Our goal was to build a chatbot that harnesses the dynamic nature of open-source tools and resources to address basic OpenROAD tasks efficiently. We developed a publicly available chatbot<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib7" title="">7</a>]</cite> that supports continuous improvement and scalability across the tool chain. The complete source code for ORAssistant is available on GitHub <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">ORAssistant‚Äôs RAG architecture can be used atop any publicly available LLM. The RAG system enhances the base LLM‚Äôs output by ensuring that knowledge is retrieved from trusted data sources, generating reliable responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib9" title="">9</a>]</cite>. Our key contributions are:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Development of ORAssistant</span>: A conversational AI agent that assists users in a simple question and answer format with basic conversational abilities.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">ORAssistant RAG Dataset</span>: A curated dataset derived from open-source tool documentation and <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.2">GitHub</span> data from the OpenROAD and OpenROAD-flow-scripts repositories.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">ORAssistant Evaluation Framework</span>: Development of a comprehensive evaluation methodology that utilizes both the publicly available EDA Corpus dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib10" title="">10</a>]</cite> and a self-curated question-answer (QA) dataset. This framework enables robust assessment of LLM performance in the OpenROAD domain, facilitating comparisons between ORAssistant and non-fine-tuned LLMs.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Dataset Generation</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The effectiveness of any RAG-based system depends on the quality of the underlying data sources, as they form the basis for model accuracy, information retrieval, and response generation. Thus, curating a properly annotated dataset is crucial.</p>
</div>
<section class="ltx_subsection ltx_indent_first" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Data Sources</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">ORAssistant primarily assists users of OpenROAD and OpenROAD-flow-scripts while also offering basic support across the RTL-to-GDSII tool chain, from synthesis to layout verification. Its knowledge base includes public documentation, tool manuals, and custom-annotated <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.1">GitHub</span> discussions, as shown below:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">OpenROAD Documentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib11" title="">11</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">OpenROAD-flow-scripts Documentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib12" title="">12</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">OpenROAD Man pages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib11" title="">11</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">OpenSTA Documentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib13" title="">13</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Yosys Documentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib14" title="">14</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i6.p1">
<p class="ltx_p" id="S2.I1.i6.p1.1">KLayout Documentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib15" title="">15</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i7.p1">
<p class="ltx_p" id="S2.I1.i7.p1.1">Research Papers, Tutorials on OpenROAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib16" title="">16</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i8.p1">
<p class="ltx_p" id="S2.I1.i8.p1.1">Reformatted and labeled <span class="ltx_text ltx_font_smallcaps" id="S2.I1.i8.p1.1.1">GitHub</span> discussions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib17" title="">17</a>]</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">These diverse sources capture detailed information about the tools and their usage. To keep the dataset updated, an automated build script is used to extract and version data from the OpenROAD repository and external sources, ensuring that ORAssistant is equipped with the most relevant and up-to-date information. Additionally, live hyperlinks for each data source are stored, enabling citations during response generation.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Issues and Discussion Analysis</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Along with public documentation sources, selectively curated conversations from support forums such as <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.1">GitHub</span> issues and discussions have been incorporated to identify key problem areas. Using the <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.2">GitHub</span> <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.3">GraphQL API</span>, conversations from the issues and discussions pages of the OpenROAD and OpenROAD-flow-scripts repositories, are extracted and stored in <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.4">JSONL</span> datasets. Since these scraped conversations lack proper annotation, an LLM-based categorization approach has been used to generate three tags for each conversation: category, subcategory, and referenced tools. This characterization enables the correct routing of the tool-based RAG system to domain-specific retriever tools.
Detailed analysis indicated that GitHub Issues were predominantly bug reports with limited relevance. GitHub Discussions provided a wider variety of user queries and their corresponding solutions, making it a more valuable data source for ORAssistant.
</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The current <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.1">JSONL</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib18" title="">18</a>]</cite> consists of a total of 736 issues and 344 discussions.
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S2.T1" title="TABLE I ‚Ä£ II-B Issues and Discussion Analysis ‚Ä£ II Dataset Generation ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the distribution of issues and discussions by category.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Distribution of GitHub Issues and Discussions by Category</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Issues (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Discussions (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">Bug</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2">45.90</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3">4.65</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.3.2.1">Feature request</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.2.2">18.60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.3.2.3">8.72</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.4.3.1">Runtime</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.3.2">13.60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.3.3">28.50</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.5.4.1">Build</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.4.2">9.92</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.4.3">8.14</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.6.5.1">Query</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.6.5.2">7.34</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.6.5.3">40.70</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.7.6.1">Installation</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.7.6.2">2.85</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.7.6.3">3.49</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S2.T1.1.8.7.1">Documentation</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.8.7.2">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.8.7.3">1.16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S2.T1.1.9.8.1">Configuration</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.9.8.2">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.1.9.8.3">4.65</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">RAG Architecture</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">ORAssistant‚Äôs tool-based RAG architecture and data ingestion pipeline can be integrated with any LLM, supporting both local and cloud-based deployments. This allows organizations to balance performance requirements, computational resources, and data privacy concerns.</p>
</div>
<section class="ltx_subsection ltx_indent_first" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Dataset Ingestion</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The pipeline processes documents in multiple formats (Markdown, PDF, HTML), and divides them into smaller, manageable document chunks based on format-specific delimiters. These chunks are then fed to a SBERT-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib19" title="">19</a>]</cite> embedding model, which encodes the textual information into dense vector representations. The resulting vectors are then stored in a Facebook AI Similarity Search (FAISS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib20" title="">20</a>]</cite> vector database for downstream use. While this approach efficiently captures the semantic meaning in each chunk, it is unsuitable for retrieving documents based on exact keywords. To perform exact term matching, we use the classical Best Match 25 (BM25) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib21" title="">21</a>]</cite> indexing technique, which allows for keyword-based retrieval. The knowledge base is thus represented as a weighted sum of vectors and keyword indices, allowing for both semantic and exact-term matching.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<p class="ltx_p ltx_align_center" id="S3.F1.1"><span class="ltx_text" id="S3.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S3.F1.1.1.g1" src="extracted/5902635/retrievers.png" width="334"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Hybrid Retriever Function</figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Hybrid Retriever Function</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.F1" title="Figure 1 ‚Ä£ III-A Dataset Ingestion ‚Ä£ III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">1</span></a>, the retriever function searches its input knowledge base to identify document chunks most relevant to a given query. It employs multiple vector search techniques, including similarity search and maximal marginal relevance (MMR) search. In similarity search, chunks with the highest cosine similarity to the input are selected, while MMR search introduces diversity by minimizing redundancy in the retrieved chunks. Additionally, a classical text-based search is performed on the BM25 index to retrieve documents containing the exact keywords specified in the query. A re-ranking model then processes these search results, adjusting the document ranking to provide a top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> selection of the most relevant documents, ensuring both precision and diversity in the final output.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S3.F2.g1" src="extracted/5902635/arch.png" width="295"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>ORAssistant‚Äôs tool-based RAG Architecture</figcaption>
</figure>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Domain Specific Retriever Tools</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">ORAssistant‚Äôs knowledge base encompasses a wide range of information from various applications in the OpenROAD flow. Subsets of this knowledge base relevant to specific applications are provided to the hybrid retriever function to form domain specific retriever tools, as listed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.F2" title="Figure 2 ‚Ä£ III-B Hybrid Retriever Function ‚Ä£ III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">2</span></a>. A custom prompt guides the base LLM in selecting the appropriate retriever tools for each query. For instance, the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">OR Commands</span> tool retrieves information specific to the OpenROAD framework‚Äôs commands, while the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">Installation</span> tool focuses on documentation related to installation procedures. In contrast to using a single retriever function on a pooled knowledge base, the modular approach significantly reduces the chances of incorrect document retrieval. Additionally, the architecture allows for future integration with other open-source tools and flow runners within the OpenROAD ecosystem.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Context-Aware Response Generation</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">As the conversation progresses, ORAssistant stores question-answer pairs locally to maintain context. When the user submits a new query, the system first processes the stored conversation history to ensure context continuity. The incoming query is then rephrased, incorporating information from previous exchanges. This gives ORAssistant the capability to answer follow up queries, and thereby maintain long, context-aware conversations.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The tool-based RAG architecture ensures that responses are context-correct by leveraging both the conversation history and domain-specific knowledge sources. These sources enable the system to provide responses with precise citations and hyperlinks for each query. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S3.F2" title="Figure 2 ‚Ä£ III-B Hybrid Retriever Function ‚Ä£ III RAG Architecture ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the tool-based RAG system operating in two distinct stages:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Initial LLM Call</span>: ORAssistant processes the conversation history to contextualize and rephrase the user‚Äôs current query. Based on the refined query, the most appropriate tools are selected and documents are subsequently retrieved.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Secondary LLM Call</span>: ORAssistant generates a response for the rephrased query using the documents retrieved by the selected tools.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Hosted Application</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">ORAssistant can be accessed on a <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Next.js<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S4.p1.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib22" title="">22</a><span class="ltx_text ltx_font_upright" id="S4.p1.1.1.2.2">]</span></cite></span> based web front-end<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib7" title="">7</a>]</cite>. The hosted version uses Gemini 1.5 Flash <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib23" title="">23</a>]</cite> as its base LLM alongside the <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">text-embedding-004<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S4.p1.1.2.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib24" title="">24</a><span class="ltx_text ltx_font_upright" id="S4.p1.1.2.2.2">]</span></cite></span> model for generating vector embeddings. The web application supports the creation of multiple conversation threads, each retaining its own history. This allows users to switch between threads while keeping each discussion focused and relevant to specific topics. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S4.F3" title="Figure 3 ‚Ä£ IV Hosted Application ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">3</span></a> shows an ORAssistant-generated, composite response for a user query about floorplan creation options in OpenROAD-flow-scripts.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="354" id="S4.F3.g1" src="extracted/5902635/example.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of ORAssistant generated response.</figcaption>
</figure>
</section>
<section class="ltx_section ltx_indent_first" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation and Results</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The evaluation of ORAssistant is crucial to understanding its performance and limitations. The process aims to quantify the system‚Äôs ability to successfully retrieve information from correct sources and provide precise responses. The evaluation process identifies areas where the model struggles, allowing for continuous improvement. Additionally, it helps identify gaps in the original documentation and knowledge sources. These insights can guide future improvements through a bidirectional feedback loop.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">To assess the effectiveness of our approach, we compared ORAssistant‚Äôs performance against base pre-trained LLMs like Gemini 1.5 Flash <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib23" title="">23</a>]</cite> and GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib25" title="">25</a>]</cite>. We utilize an approach based on GPTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib26" title="">26</a>]</cite>, where a separate LLM is used as an automated evaluator. This <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">LLM Judge</span> is given the original question, a ground truth answer, and the model-generated response. Using a carefully designed system prompt, the <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">LLM Judge</span> then assesses the quality, coherence, and accuracy of the generated response by comparing it to the ground truth. The evaluation generates the following metrics:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Classification metrics</span>: As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S5.T2" title="TABLE II ‚Ä£ V Evaluation and Results ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">II</span></a>, the judge compares the ground truth and LLM‚Äôs response, classifying them into one of four predefined categories (TP, TN, FP, FN). Metrics such as Accuracy, Precision, Recall, and F1 score are then computed using these classifications.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">LLMScore</span>: The judge assigns a score in the range of <math alttext="[0,1]" class="ltx_Math" display="inline" id="S5.I1.i2.p1.1.m1.2"><semantics id="S5.I1.i2.p1.1.m1.2a"><mrow id="S5.I1.i2.p1.1.m1.2.3.2" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml"><mo id="S5.I1.i2.p1.1.m1.2.3.2.1" stretchy="false" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">0</mn><mo id="S5.I1.i2.p1.1.m1.2.3.2.2" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i2.p1.1.m1.2.2" xref="S5.I1.i2.p1.1.m1.2.2.cmml">1</mn><mo id="S5.I1.i2.p1.1.m1.2.3.2.3" stretchy="false" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.2b"><interval closure="closed" id="S5.I1.i2.p1.1.m1.2.3.1.cmml" xref="S5.I1.i2.p1.1.m1.2.3.2"><cn id="S5.I1.i2.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i2.p1.1.m1.1.1">0</cn><cn id="S5.I1.i2.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i2.p1.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>, based on the quality and accuracy of the LLM‚Äôs response in relation to the ground truth.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Evaluation Metrics for Model Answers</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T2.1" style="width:433.6pt;height:363.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(93.7pt,-78.5pt) scale(1.76061671384325,1.76061671384325) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.1.1.1.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1">Q:</span> What does CTS stand for? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.2">A:</span> CTS stands for Clock Tree Synthesis. It is a stage‚Ä¶ 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.3">Eval:</span> True Positive (TP) (Detailed, accurate, and relevant.)</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.2.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.2.1.1.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1.1.1.1.1">Q:</span> What is the latest movie released? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1.1.1.1.2">A:</span> I can‚Äôt provide information on movies‚Ä¶ 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1.1.1.1.3">Eval:</span> True Negative (TN) (Correctly identified out of scope.)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.3.2.1.1">
<span class="ltx_p" id="S5.T2.1.1.3.2.1.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.2.1.1.1.1">Q:</span> What does CTS stand for? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.2.1.1.1.2">A:</span> CTS stands for Central Time Scheduling‚Ä¶ 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.2.1.1.1.3">Eval:</span> False Positive (FP) (Incorrect and irrelevant.)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.4.3.1.1">
<span class="ltx_p" id="S5.T2.1.1.4.3.1.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.1.1.1.1">Q:</span> What does CTS stand for? 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.1.1.1.2">A:</span> I cannot provide an answer‚Ä¶ 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.1.1.1.3">Eval:</span> False Negative (FN) (Failed to answer when expected.)</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">For our evaluation, we used two QA datasets for evaluation: a custom curated HumanEval dataset with 50 OpenROAD-related QA pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib27" title="">27</a>]</cite>, and 100 QA pairs from the publicly available EDA Corpus dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">To ensure a fair comparison, we conducted five independent runs for each question across these models: ORAssistant (with Gemini 1.5 Flash), base GPT-4o, and base Gemini 1.5 Flash. Multiple runs help account for the variability in LLM outputs, reducing outliers and improving statistical reliability. Evaluation metrics computed using Gemini 1.5 Pro<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib28" title="">28</a>]</cite> as the judge LLM have been averaged for each dataset and presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S5.T3" title="TABLE III ‚Ä£ V Evaluation and Results ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">III</span></a>. As depicted in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#S5.T3" title="TABLE III ‚Ä£ V Evaluation and Results ‚Ä£ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD"><span class="ltx_text ltx_ref_tag">III</span></a>, ORAssistant significantly outperforms the base pre-trained LLMs across both the EDA Corpus and HumanEval datasets. ORAssistant achieves notably high precision and recall scores, indicating very few false positives and false negatives in its responses. In contrast, both GPT-4o and Gemini 1.5 Flash exhibit subpar performance. Although GPT-4o achieves high recall scores on both datasets, it is offset by a very low precision score. This suggests that the model often hallucinates and generates false positive responses, without acknowledging its lack of knowledge. Across both datasets, ORAssistant attains a considerably high LLMScore, when compared to the base pre-trained LLMs. In terms of response times, ORAssistant averages 2.6 seconds across the testing datasets, while base GPT-4o records 4.7 seconds and base Gemini 1.5 Flash averages 2.3 seconds.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Evaluation Results on EDA Corpus (100 Questions) and Human Eval (50 Questions) Datasets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.1" style="width:216.8pt;height:106pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-75.6pt,37.0pt) scale(0.589134493237761,0.589134493237761) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S5.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1">EDA Corpus Dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.1">Architecture</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.2">Acc. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.3">Prec. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.4">Rec. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.5">F1 (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.2.1.6">LLMScore (%)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.1">ORAssistant</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.2.2.1">90.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.2.3.1">94.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.4">95.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.2.5.1">95.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.2.6.1">77.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.1">GPT-4o</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.2">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.3">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.3.4.1">100.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.5">65.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.4.3.6">52.6</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.1">Gemini 1.5 Flash</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.2">38.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.3">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.4">75.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.5">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.5.4.6">35.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="6" id="S5.T3.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.6.5.1.1">Human Eval Dataset</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.1">Architecture</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.2">Acc. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.3">Prec. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.4">Rec. (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.5">F1 (%)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.7.6.6">LLMScore (%)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.1">ORAssistant</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.7.2.1">87.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.7.3.1">92.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.4">94.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.7.5.1">93.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.7.6.1">79.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.1">GPT-4o</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.2">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.3">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.9.8.4.1">100.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.5">63.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.9.8.6">48.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.1">Gemini 1.5 Flash</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.2">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.3">35.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.4">80.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.5">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.1.10.9.6">28.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.p5">
<br class="ltx_break"/>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Since ORAssistant uses Gemini 1.5 Flash, the contrast between its scores and the base Gemini 1.5 Flash scores highlights how the tool-based RAG architecture guides the LLM towards better performance. While base pre-trained models struggle with relevance due to general training data and outdated knowledge, ORAssistant enhances accuracy by leveraging up-to-date data sources. Moreover, ORAssistant avoids hallucinations by grounding its responses in reliable, deterministic, and contextually relevant data sources.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">An alternate approach for an OpenROAD Assistant<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib29" title="">29</a>]</cite> as a chatbot and script generator, uses a fine-tuned LLM. Other tools like the Hybrid RAG based Ask-EDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib30" title="">30</a>]</cite> and the domain-adaptive ChatNeMo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib31" title="">31</a>]</cite> use proprietary data. ChatEDA
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib32" title="">32</a>]</cite> employs fine-tuning for basic task planning in physical design using OpenROAD and other EDA tools. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03845v1#bib.bib33" title="">33</a>]</cite>, the authors utilize a RAG based system tailored for OpenROAD and EDA tools, with custom fine-tuned embeddings and reranker models.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">RAG offers greater flexibility through real-time adaptation to tool and data source changes. Our tool-based architecture is scalable and aligns well with OpenROAD‚Äôs modular flow.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Future Work</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our tool-based architecture enables support for interfacing with OpenROAD‚Äôs Python-based APIs for custom applications. Combining a fine-tuned model with our RAG architecture will provide dual advantages of real-time adaptability for documentation and enhanced accuracy for tasks like design exploration and script generation. To further enhance the performance of the retriever functions, embeddings and reranker models can be fine-tuned on ORAssistant‚Äôs knowledge base. Adding human-in-the-loop feedback is another way to continuously improve both the knowledge base and generated responses. We also plan to interface ORAssistant with OpenROAD command-line interface (CLI) and graphical-user-interface (GUI).</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we present ORAssistant, a RAG-based assistant built using a scalable, tool-based architecture for the OpenROAD flow. This chatbot assists users by providing contextual and reliable answers for common user queries using native and publicly available data sources. Initial results show that the RAG model outperforms base pre-trained LLMs from metrics derived from human evaluations and automated LLM-based methods. ORAssistant showcases the potential of GenAI-based tools in chip design to enhance user experience, enabling users to learn faster and gain deeper insights across all levels of expertise. This work, supported by the OpenROAD project team, GSoC, and OSRE, is the result of collaborative and open-source community-driven EDA advocacy, and contribution.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.¬†B. Kahng, ‚ÄúOpen-source EDA: If we build it, who will come?‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2020 IFIP/IEEE 28th International Conference on Very Large Scale Integration (VLSI-SOC)</em>.¬†¬†¬†IEEE, 2020, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Efabless. ChipIgnite. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://efabless.com/chipignite" title="">https://efabless.com/chipignite</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.¬†Venn, ‚ÄúTiny Tapeout: A shared silicon tape out platform accessible to everyone,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Solid-State Circuits Magazine</em>, vol.¬†16, no.¬†2, pp. 20‚Äì29, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Efabless. (2023) Using Generative AI for ASIC Design. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=-R7limdUyts" title="">https://www.youtube.com/watch?v=-R7limdUyts</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
AWS. (2023) Open source chip design on AWS. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/blogs/industries/open-source-chip-design-on-aws/" title="">https://aws.amazon.com/blogs/industries/open-source-chip-design-on-aws/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
U.¬†S. C. O. S.¬†P. Office. (2024) Open Source Research Experience (OSRE 2024). Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ucsc-ospo.github.io/osre24" title="">https://ucsc-ospo.github.io/osre24</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
The-OpenROAD-Project. (2024) ORAssistant front end. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://orassistant.netlify.app/" title="">https://orassistant.netlify.app/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
The OpenROAD Project, ‚ÄúORAssistant,‚Äù 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/The-OpenROAD-Project/ORAssistant" title="">https://github.com/The-OpenROAD-Project/ORAssistant</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P.¬†Lewis, E.¬†Perez, A.¬†Piktus, F.¬†Petroni, V.¬†Karpukhin, N.¬†Goyal, H.¬†K√ºttler, M.¬†Lewis, W.-t. Yih, T.¬†Rockt√§schel <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">et¬†al.</em>, ‚ÄúRetrieval-augmented generation for knowledge-intensive NLP tasks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">Advances in Neural Information Processing Systems</em>, vol.¬†33, pp. 9459‚Äì9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B.-Y. Wu, U.¬†Sharma, S.¬†R.¬†D. Kankipati, A.¬†Yadav, B.¬†K. George, S.¬†R. Guntupalli, A.¬†Rovinski, and V.¬†A. Chhabria, ‚ÄúEDA Corpus: A large language model dataset for enhanced interaction with OpenROAD,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2405.06676</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
The-OpenROAD-Project. (2024) OpenROAD Documentation. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openroad.readthedocs.io" title="">https://openroad.readthedocs.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
‚Äî‚Äî. (2024) OpenROAD-flow-scripts documentation. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openroad-flow-scripts.readthedocs.io" title="">https://openroad-flow-scripts.readthedocs.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
‚Äî‚Äî. (2024) OpenSTA documentation. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/The-OpenROAD-Project/OpenSTA/tree/master" title="">https://github.com/The-OpenROAD-Project/OpenSTA/tree/master</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
YosysHQ. (2024) YosysHQ documentation library. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://yosyshq.readthedocs.io" title="">https://yosyshq.readthedocs.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K.¬†Project. (2024) KLayout documentation. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.klayout.de/doc.html" title="">https://www.klayout.de/doc.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
The-OpenROAD-Project. (2024) The-OpenROAD-Project. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://theopenroadproject.org/" title="">https://theopenroadproject.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
‚Äî‚Äî. (2024) OpenROAD repository. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/The-OpenROAD-Project/OpenROAD" title="">https://github.com/The-OpenROAD-Project/OpenROAD</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
‚Äî‚Äî. (2024) ORQA RAG dataset huggingface repository. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/The-OpenROAD-Project/ORQA_RAG_datasets" title="">https://huggingface.co/datasets/The-OpenROAD-Project/ORQA_RAG_datasets</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
N.¬†Reimers and I.¬†Gurevych, ‚ÄúSentence-BERT: Sentence embeddings using Siamese BERT-networks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>.¬†¬†¬†Association for Computational Linguistics, 11 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.¬†Douze, A.¬†Guzhva, C.¬†Deng, J.¬†Johnson, G.¬†Szilvasy, P.-E. Mazar√©, M.¬†Lomeli, L.¬†Hosseini, and H.¬†J√©gou, ‚ÄúThe FAISS library,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2401.08281</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E.¬†A. Stuart, ‚ÄúMatching methods for causal inference: A review and a look forward,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Statistical science: a review journal of the Institute of Mathematical Statistics</em>, vol.¬†25, no.¬†1, p.¬†1, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Vercel. (2024) Next.js - the react framework. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nextjs.org/" title="">https://nextjs.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
G.¬†DeepMind. (2024) Gemini flash. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://deepmind.google/technologies/gemini/flash/" title="">https://deepmind.google/technologies/gemini/flash/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Google Cloud. (2024) Text embeddings API. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api" title="">https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
OpenAI. (2024) Hello gpt-4o. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J.¬†Fu, S.-K. Ng, Z.¬†Jiang, and P.¬†Liu, ‚ÄúGPTScore: Evaluate as you desire,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2302.04166</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
The-OpenROAD-Project. (2024) ORAssistant RAG dataset. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/The-OpenROAD-Project/ORAssistant_HumanEval_QA_Pairs" title="">https://huggingface.co/datasets/The-OpenROAD-Project/ORAssistant_HumanEval_QA_Pairs</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Google DeepMind. (2024) Gemini pro. Accessed: 2024-09-22. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://deepmind.google/technologies/gemini/pro/" title="">https://deepmind.google/technologies/gemini/pro/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
U.¬†Sharma, B.-Y. Wu, S.¬†R.¬†D. Kankipati, V.¬†A. Chhabria, and A.¬†Rovinski, ‚ÄúOpenROAD-Assistant: An open-source large language model for physical design tasks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD</em>, 2024, pp. 1‚Äì7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L.¬†Shi, M.¬†Kazda, B.¬†Sears, N.¬†Shropshire, and R.¬†Puri, ‚ÄúAsk-EDA: A design assistant empowered by LLM, hybrid RAG and abbreviation de-hallucination,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2406.06575</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.¬†Liu, T.-D. Ene, R.¬†Kirby, C.¬†Cheng, N.¬†Pinckney, R.¬†Liang, J.¬†Alben, H.¬†Anand, S.¬†Banerjee, I.¬†Bayraktaroglu <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et¬†al.</em>, ‚ÄúChipNeMo: Domain-adapted LLMs for chip design,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">arXiv preprint arXiv:2311.00176</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H.¬†Wu, Z.¬†He, X.¬†Zhang, X.¬†Yao, S.¬†Zheng, H.¬†Zheng, and B.¬†Yu, ‚ÄúChatEDA: A large language model powered autonomous agent for EDA,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y.¬†Pu, Z.¬†He, T.¬†Qiu, H.¬†Wu, and B.¬†Yu, ‚ÄúCustomized retrieval augmented generation and benchmarking for EDA tool documentation QA,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2407.15353</em>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 18:15:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
