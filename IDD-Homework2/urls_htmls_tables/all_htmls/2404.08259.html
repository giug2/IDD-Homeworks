<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024</title>
<!--Generated on Wed Apr 10 19:19:35 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.08259v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S1" title="1 Introduction ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2" title="2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS1" title="2.1 Low-Resource Languages ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Low-Resource Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS2" title="2.2 Machine Translation ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS2.SSS0.Px1" title="Nearest Neighbor Machine Translation ‣ 2.2 Machine Translation ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Nearest Neighbor Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS2.SSS0.Px2" title="Transfer Learning ‣ 2.2 Machine Translation ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Transfer Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS2.SSS0.Px3" title="Pre-trained Language Models ‣ 2.2 Machine Translation ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Pre-trained Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS3" title="2.3 Refined Solutions ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Refined Solutions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS3.SSS0.Px1" title="Data Filtering and Normalization ‣ 2.3 Refined Solutions ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Data Filtering and Normalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS3.SSS0.Px2" title="Multilinguality ‣ 2.3 Refined Solutions ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Multilinguality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S2.SS3.SSS0.Px3" title="Language Similarity ‣ 2.3 Refined Solutions ‣ 2 Related Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Language Similarity</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S3" title="3 Methodology ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S3.SS1" title="3.1 Data Acquisition ‣ 3 Methodology ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S3.SS2" title="3.2 Framework ‣ 3 Methodology ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S4" title="4 Implementation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S4.SS0.SSS0.Px1" title="Data Preparation ‣ 4 Implementation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Data Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S4.SS0.SSS0.Px2" title="Cross Validation ‣ 4 Implementation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Cross Validation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S4.SS0.SSS0.Px3" title="System Implementation ‣ 4 Implementation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">System Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S4.SS0.SSS0.Px4" title="Statistical Significance ‣ 4 Implementation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title">Statistical Significance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5" title="5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS1" title="5.1 Metrics ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS2" title="5.2 System 1: Baseline ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>System 1: Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS3" title="5.3 System 2: Back-translation ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>System 2: Back-translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS4" title="5.4 System 3: Transfer Learning ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>System 3: Transfer Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS5" title="5.5 Statistical Analysis ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Statistical Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.SS6" title="5.6 Qualitative Analysis ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Qualitative Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S6" title="6 Conclusion ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S7" title="7 Limitations ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S8" title="8 Future Work ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S9" title="9 Ethical Considerations ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S10" title="10 Acknowledgment ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Acknowledgment</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2404.08259v1 [cs.CL] 12 Apr 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case Study<span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Preprint accepted at SIGUL 2024</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Wan-Hua Her 
<br class="ltx_break"/>Information Science
<br class="ltx_break"/>University of Regensburg
<br class="ltx_break"/>Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">wan-hua.her@stud.uni-regensburg.de</span>
<br class="ltx_break"/>&amp;Udo Kruschwitz 
<br class="ltx_break"/>Information Science
<br class="ltx_break"/>University of Regensburg
<br class="ltx_break"/>Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.2.id2">udo.kruschwitz@ur.de</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using combined metrics: BLEU, chrF and TER. Statistical significance results with Bonferroni correction show surprisingly high baseline systems, and that Back-translation leads to significant improvement. Furthermore, we present a qualitative analysis of translation errors and system limitations.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.1.1">K</em><span class="ltx_text ltx_font_bold" id="p1.1.2">eywords</span> Neural Machine Translation, Low-resource Languages, Back-translation, Bavarian, German</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neural Machine Translation (NMT) has progressed so far to reach human-level performance on some languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib1" title="">1</a>]</cite> and has become one of the most prominent approaches within the research area of Machine Translation (MT). Its easy-to-adapt architecture has achieved impressive performance and high accuracy. Promising methods that fall under NMT include Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib3" title="">3</a>]</cite>, pre-trained language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib5" title="">5</a>]</cite>, and multilingual models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib9" title="">9</a>]</cite> etc.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, existing NMT resources focus overwhelmingly on high-resource languages, which dominate a great portion of contents on the Internet and Social Media. Low-resource languages are often spoken by minorities with minimal online presence and insufficient amount of resources to achieve comparable NMT results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib11" title="">11</a>]</cite>, but they might even have a very large population of speakers and still be under-resourced (such as Hindi, Bengali and Urdu). Growing interest in low-resource MT is evident through the annually held Conference on Machine Translation (WMT). In 2021, WMT featured tasks to promote MT in low-resource scenarios by exploring similarity and multilinguality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib12" title="">12</a>]</cite>. Among all tasks, the objective of the Very Low Resource Supervised Machine Translation task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib13" title="">13</a>]</cite> focused on Transfer Learning between German and Upper Sorbian. The task examined effects of utilizing similar languages and results show that combining Transfer Learning and data augmentation can successfully exploit language similarity during training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We introduce our experiment to develop bidirectional state-of-the-art NMT systems for German and Bavarian, a classic high-resource to/from low-resource language pair. Inspired by WMT21, our experiment explores the generalizability of Back-translation and Transfer Learning from the highest-ranking approach from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib14" title="">14</a>]</cite>. Our approach covers the following: First, a simple Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib15" title="">15</a>]</cite> is trained as the baseline. Secondly, we use the base model for Back-translation and take the extended corpus to train our second model. Lastly, we experiment with Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib3" title="">3</a>]</cite> by introducing German-French as the parent model. For evaluation we opt for a combination of three metrics: BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib16" title="">16</a>]</cite>, chrF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib17" title="">17</a>]</cite> and TER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib18" title="">18</a>]</cite>. Recent studies have argued that using BLEU as a single metric neglects the complexity of different linguistic characteristics. Using combined metrics and having various penalization standards may be able to capture translation errors more diversely <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">By choosing the language pair Bavarian / German we offer one exemplar for a low-resource language (combined with a high-resource one) that can serve as a reference point for further experimental work applied to other low-resource MT. This will ultimately help addressing the imbalance that still prevails between a handful of well-resourced languages and the many others that are not.
This paper makes the following contributions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We offer a systematic evaluation of state-of-the-art NMT approaches for a language pair involving a low-resource language that has attracted little attention so far. This investigation explores both translation from as well as into the low-resource language. We focus on a Transformer baseline against Back-translation and a Transfer Learning approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">To foster reproducibility and replicabilty (which is in the very spirit of SIGUL, LREC and COLING) we make all code available via a GitHub project repository<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/whher/nmt-de-bar" title="">https://github.com/whher/nmt-de-bar</a></span></span></span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Low-Resource Languages</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The challenges of low-resource languages can be very diverse, hence difficult to define in simple words.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">For a start, even though large web-crawled data such as OPUS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib21" title="">21</a>]</cite> has resulted in automatically generated parallel corpora for many minor languages, the quality of the data has been reported to be noisy. Examples include the Bantu (Niger-Congo) languages, where parallel data exists, but often too inconsistent to generate desirable MT performance and reproducible benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib22" title="">22</a>]</cite>. Misalignments and mistranslations have also been reported while working with multilingual Indian languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib23" title="">23</a>]</cite>. The rise of Unsupervised NMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib26" title="">26</a>]</cite> alleviates the need for large amounts of labeled training data. Nonetheless, researchers have noted however strong the supervision during training is, there is an overall dependence on parallel data to support evaluation systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib28" title="">28</a>]</cite>. We therefore see the problem of these less-studied languages as a problem caused by both the <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">quantity</span> and the <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.2">quality</span> of the resources. Without linguistically-trained speakers, parallel data is often curated in an unsupervised fashion and therefore noisy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Furthermore, there are endangered languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib29" title="">29</a>]</cite>, for example, the language Bribri is an extremely low-resource indigenous language which is currently being displaced by English and Spanish <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib11" title="">11</a>]</cite>. Aside from suffering digital inequalities and having insufficient written data, it was more challenging to create standardized representations of Bribri, since lexemes and rules vary from communities of speakers. Another similar study which focused on Alemannic dialects also highlights that dialects do not have uniform spelling rules, and that spelling reflect different regional pronunciations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib30" title="">30</a>]</cite>. This raises a great challenge for MT to decide which variation should be given precedence. These under-resourced languages raise a string of challenges due to long years of absence of standardization, and that digital revitalization is not merely a question of gathering data and training models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">To optimize text processing and its size during training, the most common way is to create a joint vocabulary through Byte Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib31" title="">31</a>]</cite>. BPE is a highly effective subword segmentation algorithm. It iteratively merges frequent words and creates new subword units from infrequent words. A drawback of this approach is that the model learns patterns of smaller unit composition only by recognizing the infrequent words. To counter this, BPE dropout was introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib32" title="">32</a>]</cite> to stochastically corrupt the segmentation procedure within BPE.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Machine Translation</h3>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Nearest Neighbor Machine Translation</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Non- and semi-parametric methods have been successfully applied to MT tasks in recent years. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib33" title="">33</a>]</cite> demonstrate a powerful combination of neural networks and non-parametric retrieval mechanisms to improve translation. <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.1.1">k</span>NN-MT follows the retrieval principle and proposes a more efficient non-parametric translation method, which augments the decoder of a pre-trained NMT model with a nearest neighbor retrieval mechanism, allowing direct access to data store of cached examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib34" title="">34</a>]</cite>. This approach scales the decoder to an arbitrary amount of examples at test time, particularly strengthening decoder’s translation capability. However, the big drawback is high computational cost and low decoding speed due to word-by-word generation. Chunk-based <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.1.2">k</span>NN-MT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib35" title="">35</a>]</cite> solves this problem by processing translation in chunks of words instead of passing single tokens through the data store.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Transfer Learning</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">in MT is often done by training a high-resource language pair and using this parent model to initialize parameters in a child model with low-resource languages. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib3" title="">3</a>]</cite> achieved translation improvements for Hansa, Turkish and Uzbek into English by using French-English as a parent model. Experiments from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib36" title="">36</a>]</cite> showed improvements using Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib15" title="">15</a>]</cite> to train low-resource languages such as Estonian and Slovak. Their results pointed out key factors for a successful transfer include the size of the parent corpus and sharing the target or source language. For instance, Estonian-English as a child gained up to 2.44 BLEU with Finnish-English as a parent. </p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p2.1">In Dual Transfer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib2" title="">2</a>]</cite>, two parent models are used to initialize one child. Monolingual and parallel parent data were trained separately so that inner layers and embeddings can be transferred separately. Another recent study extends conventional transfer learning by additionally transferring probability distributions from parent to child. The Consistency-based Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib37" title="">37</a>]</cite> argues that parent prediction distribution is highly informative and can be useful to guide child translation. Their experiment showed that using German-English as a parent can achieve BLEU improvement up to 6.2 for Indonesian-English. Furthermore, the study from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib6" title="">6</a>]</cite> investigated a technique to incrementally add new language pairs to a multilingual MT model based on knowledge transfer, without posing the original model at risk for catastrophic forgetting.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pre-trained Language Models</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">(PLMs) can be fine-tuned on low-resource languages. For instance, MT quality between Spanish and Quecha was shown to improve by leveraging Spanish-English and Spanish-Finnish PLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib4" title="">4</a>]</cite>, with the latter yielding better results. Furthermore, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib38" title="">38</a>]</cite> combined a BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib39" title="">39</a>]</cite> encoder with a vanilla NMT decoder. Evaluation on low-resource languages like English-Vietnamese show that their two-stage training improves performance significantly compared to simple fine-tuning. XLM extends the features of BERT by using Cross-Lingual Masked Language Modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib40" title="">40</a>]</cite>. It has not only been reported to be beneficial for general unsupervised learning, but also for low-resource supervised MT such as English-Romanian. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib41" title="">41</a>]</cite> acknowledged the success of PLMs and presented their granulated study of fine-tuning, which showed that cross-attention layers are crucial to continue training downstream tasks and that they are powerful when adapting to new languages.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Refined Solutions</h3>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data Filtering and Normalization</h4>
<div class="ltx_para ltx_noindent" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">Translation data for low-resource languages are very difficult to come by and the primary source are often from the Web, making the data noisy and of poor quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib42" title="">42</a>]</cite>. Extra analysis and text normalization are often required to prevent overfitting. For instance, inaccurate translations, noisy data and a large amount of text-overlap was found in the parallel data for African languages collected from large crowd-sourced platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib22" title="">22</a>]</cite>. Comparative results showed that an English-Zulu model trained with noisy data leads to unreliable results and a reduction of 7 BLEU. Research from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib28" title="">28</a>]</cite> corroborated this and provided guidelines for removing low-quality translations. They presented translation filtering by way of n-gram models trained on monolingual data and sentence-level char-BLEU score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib43" title="">43</a>]</cite> below 15 or over 90. Another novel filtering approach was proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib42" title="">42</a>]</cite>, where cosine similarity is determined based on available parallel (good quality) data, which is then used as the threshold to filter out pseudo-parallel (noisy) sentences.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multilinguality</h4>
<div class="ltx_para ltx_noindent" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">Previous findings have pointed out that one-to-many models with middle-sized parallel corpora have achieved better results than one-to-one models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib44" title="">44</a>]</cite>. The multilingual model consisting of seven Asian languages developed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib9" title="">9</a>]</cite> using the Asian Language Treebank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib45" title="">45</a>]</cite> is a great example. The presence of multiple in-domain aligned languages was argued to have contributed to better learn joint representations, hence leading to intra-language improvements. However, low-resource languages often face the risk of being overfitted in multilingual setups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib46" title="">46</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib7" title="">7</a>]</cite> investigated the extent of multilinguality for low-resource languages. Their corpus consists of Bible texts in 1,108 languages, all aligned by verse. Results show that BLEU increase/decrease with respect to the number of training languages is not uniform across languages. Although the 5-language models outperform bilingual baseline models for Turkish and Xhosa, accuracy decrease can be found in Tagalog. The negative correlation between number of languages and translation quality is found to start at 10 languages, and maximal degeneration is observed at 100 languages, where addition of languages does not affect translation fluency anymore. This complication and pattern of degeneration can be explained by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib47" title="">47</a>]</cite>, where text repetition harms the likelihood function during decoding. Furthermore, the errors in sequence modeling are more obvious for multilingual corpora, indicating that increased number of languages leads to increased destructive interference.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Language Similarity</h4>
<div class="ltx_para ltx_noindent" id="S2.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p1.1">Leveraging similarities between low-resource languages has been a growing interest in the MT community and is evident through the Similar Language Translation task (SLT) and Very Low Resource Supervised Machine Translation task at WMT21 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib48" title="">48</a>]</cite>. Regardless of level of closeness and degree of mutual structures, similarity between languages has shown to have positive interactions with MT quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib49" title="">49</a>]</cite>. The goal of using language relatedness is similar to leveraging multilinguality. The major difference is they often do not use English as the pivot language, but translate between closely-related languages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p2.1">In the Very Low Resource Supervised Machine Translation task at WMT21 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib13" title="">13</a>]</cite> between German and Upper Sorbian, the participants were encouraged to make use of Czech and Polish datasets (languages closely related to Sorbian). Results pointed out the importance of including related languages, and that carefully applying tricks can compensate for using smaller datasets substantially. For example, NoahNMT’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib50" title="">50</a>]</cite> approach entails a Dual Transfer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib2" title="">2</a>]</cite> model that was initialized using German and Czech monolingual data as a parent model. The NRC-CNRC team’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib14" title="">14</a>]</cite> high-performance was attributed to the combination of minor tricks such as Back-translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib51" title="">51</a>]</cite>, monolingual data selection by way of consine similarity, Moore-Lewis filtering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib52" title="">52</a>]</cite> and BPE dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p3.1">The technique Back-translation is further backed up by the study from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib30" title="">30</a>]</cite>. They investigated the effect on Alemannic dialect translation and experienced significant improvement, suggesting that Back-translation is a highly promising method for low-resource languages.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Motivated by the current findings, we present our experiment to develop bidirectional state-of-the-art NMT systems between German and Bavarian (ISO codes are de and bar respectively) - a language pair consisting of high- and low-resource languages.
While Bavarian and Upper Sorbian are very different languages, they are both spoken by communities which are geographically located within or near Germany. We expect that applying the NMT methods that were found to be effective as part of WMT21 might result in similar findings for our setting.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We formulate the following three research questions (applied to the exemplar language pair Bavarian / German):</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">RQ1</span>: Does translating between similar languages achieve generally higher BLEU scores?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">RQ2</span>: How well does Back-translation perform for (bidirectional) German-Bavarian?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">RQ3</span>: Does cross-lingual transfer lead to improved results for German-Bavarian? More specifically, does the child model profit from related parent languages (i.e. German-French)?</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Acquisition</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The Tatoeba Challenge<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Helsinki-nlp/tatoeba-challenge" title="">https://github.com/Helsinki-nlp/tatoeba-challenge</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib53" title="">53</a>]</cite> is one of the most active projects advocating low-resource MT. It maintains a leader board to compare submitted MT system performance from the community. To our knowledge, we are the first to conduct MT for German-Bavarian systems. We discovered parallel and monolingual sources on OPUS<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu/" title="">https://opus.nlpl.eu/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib21" title="">21</a>]</cite>, which we used for our experiments. More information about data sources can be found in our repository.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Framework</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Inspired by the WMT21 Very Low Resource Supervised Machine Translation task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib13" title="">13</a>]</cite>, our experiment revisits solutions that have been proven to work effectively with low-resource languages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">First, a simple Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib15" title="">15</a>]</cite> model using preprocessed parallel data is trained as the baseline model.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Secondly, Back-translation is used to generate silver-paired parallel data to increase corpus size.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Lastly, we experiment with Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib3" title="">3</a>]</cite> by introducing German-French as the parent model.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For evaluation, we opt for an ensemble of automated MT metrics consisting of BLEU, chrF and TER for our systems. This is backed up by recent argumentation from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib19" title="">19</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib20" title="">20</a>]</cite>, which states that multiple metrics instead of a single metric can diversify the evaluation based on different linguistic characteristics. This approach is a growing trend and has also been adopted by WMT21. Moreover, the study from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib30" title="">30</a>]</cite> pointed out BLEU is insufficient in word matching due to ununified orthography.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Implementation</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data Preparation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">In total we found 99.7K parallel sentences between Bavarian and German on OPUS (details can be found in our repository). After extensive preprocessing, the corpus size was reduced to 42K. To conduct data augmentation for the second system, we downloaded an extra 258K of German and 295K Bavarian monolingual text, mainly from Wikipedia and Wikinews. For German-French, we collected a total size of 184K of parallel data from Tatoeba and WikiMedia, which was reduced to 165K after preprocessing. We argue that the amount of in-domain data could contribute positively to Transfer Learning. Text preprocessing removes special symbols and noisy annotation, as proposed in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">In addition to conventional text preprocessing, we took two further measures to de-noise the data. The additional measures entail check and remove misaligned texts by way of cosine similarity between source and target languages and smart sentence truncation. Based on the knowledge that Bavarian and German share common script and that many morphemes are alike, cosine similarity is a great way to support misalignment removal. We assume that a low cosine correlation indicates a low relevance in context between source and target. Following exploratory experiments, we set the correlation threshold at 0.48 and treat anything that falls below 0.48 as misalignment and remove this. We leave a systematic investigation into this aspect as future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p3.1">Our consideration for smart truncation comes from the long-tailed distribution of sentence lengths (outliers span up to 8000). Having long sentences in the corpus therefore poses potential threat that could damage MT performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib54" title="">54</a>]</cite>. However, if all longer sequences were simply removed, we might lose a significant amount of precious parallel data. Therefore, we implemented smart truncation to deal with longer sequences in the parallel corpus. The truncation is set at the sequence length of 90. </p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Cross Validation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">In low-resource MT training, it is important to implement Cross Validation (CV) to ensure robust predictive performance and address problems like overfitting. In this case, where the training corpus is small, CV can provide insights on the variability. We opt for 5-fold CV to compare training results. After text preprocessing, the cleaned text are randomly shuffled and split into 5 chunks. The subsets are then concatenated respectively before training. For our baseline systems, 4 of 5 iterations have the subset size of 33813 for training and 8453 for test. The last iteration has the size of 33812 and 8454 respectively.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">System Implementation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">of all three systems is carried out as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S3.SS2" title="3.2 Framework ‣ 3 Methodology ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_tag">3.2</span></a>. We utilized the MT development toolkit Sockeye <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib55" title="">55</a>]</cite> for BPE encoding, model training and evaluation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Statistical Significance</h4>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">For statistical significance analysis, our experimental setup needs to take the multiple comparison problem into account. When testing multiple hypotheses simultaneously, the increased number of statistical inferences leads to increased probability of inexact inferences and Type I errors, making the conventional <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px4.p1.1.1">p</span> threshold of 0.05 less reliable. This is a well-known problem, e.g. in the Genome- and Public Health-related research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib57" title="">57</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p2.5">Methods that counteract multiple testing generally adjust <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p2.1.m1.1"><semantics id="S4.SS0.SSS0.Px4.p2.1.m1.1a"><mi id="S4.SS0.SSS0.Px4.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.1.m1.1b"><ci id="S4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p2.1.m1.1d">italic_α</annotation></semantics></math> so that the chance of observing inaccurate significant result is reduced. The Bonferroni correction is the simplest (and fairly conservative) approach to cut off the <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p2.2.m2.1"><semantics id="S4.SS0.SSS0.Px4.p2.2.m2.1a"><mi id="S4.SS0.SSS0.Px4.p2.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.2.m2.1b"><ci id="S4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p2.2.m2.1d">italic_α</annotation></semantics></math> value. Bonferroni corrects the <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p2.3.m3.1"><semantics id="S4.SS0.SSS0.Px4.p2.3.m3.1a"><mi id="S4.SS0.SSS0.Px4.p2.3.m3.1.1" xref="S4.SS0.SSS0.Px4.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.3.m3.1b"><ci id="S4.SS0.SSS0.Px4.p2.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p2.3.m3.1d">italic_α</annotation></semantics></math> by considering the set of <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px4.p2.5.1">n</span> comparisons, causing the <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p2.4.m4.1"><semantics id="S4.SS0.SSS0.Px4.p2.4.m4.1a"><mi id="S4.SS0.SSS0.Px4.p2.4.m4.1.1" xref="S4.SS0.SSS0.Px4.p2.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.4.m4.1b"><ci id="S4.SS0.SSS0.Px4.p2.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.4.m4.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p2.4.m4.1d">italic_α</annotation></semantics></math> threshold to become <math alttext="\alpha/\textit{n}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p2.5.m5.1"><semantics id="S4.SS0.SSS0.Px4.p2.5.m5.1a"><mrow id="S4.SS0.SSS0.Px4.p2.5.m5.1.1" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.cmml"><mi id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.2" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.2.cmml">α</mi><mo id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.1" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.1.cmml">/</mo><mtext id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3a.cmml">𝑛</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.5.m5.1b"><apply id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1"><divide id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.1"></divide><ci id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.2">𝛼</ci><ci id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3a.cmml" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3"><mtext id="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p2.5.m5.1.1.3">𝑛</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.5.m5.1c">\alpha/\textit{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px4.p2.5.m5.1d">italic_α / n</annotation></semantics></math>. With the Bonferroni correction, the <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px4.p2.5.2">p</span>-value is set to 0.017 as opposed to 0.05.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Metrics</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Despite the popularity of BLEU, recent studies from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib19" title="">19</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib58" title="">58</a>]</cite> questioned the phenomenon of using BLEU as a single metric, especially in low-resource scenarios, where language structures and scripts are complex and different from many high-resource languages. For example, the meta evaluation on Indian languages by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib59" title="">59</a>]</cite> reported higher human judgement correlation using COMET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib60" title="">60</a>]</cite> as opposed to BLEU. The limitation of BLEU also lies in the strong dependence on reference translation, whose quality can be highly unstable, especially when data is noisy. Issues such as translationese and poor reference diversity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib20" title="">20</a>]</cite> might also jeopardize the entire evaluation. We therefore include chrF and TER for a more diverse evaluation. ChrF is language-independent and has been reported to better capture complex morpho-syntactic structures in MT evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib17" title="">17</a>]</cite>. TER (Translation Error Rate) quantifies the amount of edit operations it takes to change the system output to match the reference translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib18" title="">18</a>]</cite>. This intuitive technique avoids knowledge-intensive calculations and focuses on matching hypothesis with reference. The main advantage of TER as opposed to BLEU is the lower penalty for phrasal shifts. TER has also been reported to correlate highly with human judgement and has been implemented in recent WMT tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib61" title="">61</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>System 1: Baseline</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Despite the lack of sufficient amount of parallel data, baseline models in both translation directions exceed 60 BLEU (see Table <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.T1" title="Table 1 ‣ 5.4 System 3: Transfer Learning ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_tag">1</span></a>). For bar-de baseline, BLEU scores have an average of 66, chrF has an average of 78 and TER 33. We want to point out little variation between the folds - indicating that the results are robust. However, we observe relatively lower scores on the opposite direction, namely an average of 61 BLEU, 74 chrF and 36 TER. Variation are also small for the de-bar base systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>System 2: Back-translation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Back-translation (BT) was applied to the best performing baseline folds with monolingual data. Significant improvements can be observed in all three metrics for bar-de, whereas de-bar systems show subtle increase. In contrast to baseline systems, we observe a systematic increase of standard deviation. Where SD was between 0.3 and 0.6 for base systems, 0.7 to 2.2 SD was found in back-translated systems.
</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>System 3: Transfer Learning</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In contrast to surprisingly high baselines, both parent models perform similarly moderate, the fr-de model scored 29 BLEU, 52 chrF and 65 TER, whereas the de-fr parent reached 30 BLEU, 53 chrF and 65 TER. Given the fact that the German-French corpus size is significantly bigger than the German-Bavarian corpus, we had expected better performance of the parent models. However, our results are comparable with available German-French models on Hugging Face, for instance the one from Helsinki-NLP<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Helsinki-NLP/opus-mt-fr-de" title="">https://huggingface.co/Helsinki-NLP/opus-mt-fr-de</a></span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Despite the parents’ BLEU scores are only a half of our baseline models, Transfer Learning improves children’s performance considerably. For bar-de, the best system has 54 BLEU, 71 chrF and 42 TER, which is an increase of 25 BLEU and 19 chrF and decrease of 23 TER. For de-bar, the best model scored 51 BLEU, 65 chrF and 43 TER, which has a performance leap of 21 BLEU, 12 chrF and 22 TER from parent. We note that Transfer Learning improved translation capacity from parent to child with an enhancement of more than 20 BLEU. This corroborates with the recent studies on the use of Transfer Learning for low-resource languages. However, these improvement cannot compare with the very high baseline systems and their back-translated extensions.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">BLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1">chrF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.5.1">TER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.1.2.1.1" rowspan="3"><span class="ltx_text" id="S5.T1.1.2.1.1.1">bar-de</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.1.2.1.2">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.1.3">66.0</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.1.4">78.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.2.1.5">32.7</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.3.2.1">Back-translated</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2">73.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.3">82.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.4">25.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.4.3.1">Transferred</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.2">53.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.3">70.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.4.3.4">41.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.1.5.4.1" rowspan="3"><span class="ltx_text" id="S5.T1.1.5.4.1.1">de-bar</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.5.4.2">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.5.4.3">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.5.4.4">74.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.5.4.5">36.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.6.5.1">Back-translated</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.2">63.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.3">76.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.6.5.4">31.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.1.7.6.1">Transferred</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.6.2">48.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.6.3">63.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.7.6.4">44.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of best performing models from each system</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Statistical Analysis</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Two-tailed pairwise t-tests were conducted on all pairs with Bonferroni correction (<span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">p</span> threshold is 0.017). Test statistics are shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.T2" title="Table 2 ‣ 5.5 Statistical Analysis ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.T3" title="Table 3 ‣ 5.5 Statistical Analysis ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_tag">3</span></a>. For bar-de models, the BLEU results from baseline (M = 65.7, SD = 0.2) and BT (M = 70.5, SD = 2) indicate that Back-translation leads to significant improvement, <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">t</span> = -4.89, <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.3">p</span> = 0.0036. BT also performs significantly better than transferred systems (M = 52.8, SD = 0.7), <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.4">t</span> = 17.25, <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.5">p</span> &lt; 0.0. Further statistics from the metrics chrF and TER corroborate these findings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">For de-bar models, the tendency is similar. ChrF results show a positive enhancement from baseline (M = 74.1, SD = 0.4) to BT (M = 75.5, SD = 0.7), <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.1">t</span> = -3.84, <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.2">p</span> = 0.149. The improvement of BT over transferred systems (M = 64.2, SD = 0.6) is significant as well. TER statistics also verify these findings. Interestingly, while chrF and TER successfully rejects the null hypothesis between baseline and BT performance, BLEU does the opposite. We argue that the results are nevertheless significant based on chrF and TER, and consider this disagreement between metrics as an occurrence derived from linguistically-different perspectives and computations.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.1">Group 1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.1">Group 2</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.1.5.1">t</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.1.6.1">p</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T2.1.1.7.1">p<span class="ltx_text ltx_font_upright" id="S5.T2.1.1.7.1.1"> (corr.)</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1">Reject <math alttext="\text{H}_{0}" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><msub id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml"><mtext id="S5.T2.1.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.1.m1.1.1.2a.cmml">𝐇</mtext><mn id="S5.T2.1.1.1.1.m1.1.1.3" mathvariant="normal" xref="S5.T2.1.1.1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T2.1.1.1.1.m1.1.1.2a.cmml" xref="S5.T2.1.1.1.1.m1.1.1.2"><mtext id="S5.T2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.1.m1.1.1.2">𝐇</mtext></ci><cn id="S5.T2.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T2.1.1.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\text{H}_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.1" rowspan="3"><span class="ltx_text" id="S5.T2.1.2.1.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.4">-4.89</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.5">0.0012</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1.6">0.0036</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.7">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.3">37.86</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.3.2.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.1">BT</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.3">17.25</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.4.3.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.1" rowspan="3"><span class="ltx_text" id="S5.T2.1.5.4.1.1">chrF</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.4">-5.83</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.5">0.0004</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.5.4.6">0.0012</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.5.4.7">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.3">20.65</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.6.5.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.5.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.1">BT</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.3">19.82</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.7.6.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.7.6.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.1.8.7.1" rowspan="3"><span class="ltx_text" id="S5.T2.1.8.7.1.1">TER</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.7.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.7.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.7.4">6.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.7.5">0.0003</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.7.6">0.0009</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.8.7.7">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.3">-19.29</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.9.8.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.8.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.10.9.1">BT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.10.9.2">Transfer</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.10.9.3">-16.2</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.10.9.4">0.0</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.10.9.5">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.10.9.6">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of t-test with Bonferroni correction for bar-de systems.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.1">Group 1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.1">Group 2</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.1.5.1">t</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.1.6.1">p</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.T3.1.1.7.1">p<span class="ltx_text ltx_font_upright" id="S5.T3.1.1.7.1.1"> (corr.)</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1">Reject <math alttext="\text{H}_{0}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml"><mtext id="S5.T3.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.m1.1.1.2a.cmml">𝐇</mtext><mn id="S5.T3.1.1.1.1.m1.1.1.3" mathvariant="normal" xref="S5.T3.1.1.1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T3.1.1.1.1.m1.1.1.2a.cmml" xref="S5.T3.1.1.1.1.m1.1.1.2"><mtext id="S5.T3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.m1.1.1.2">𝐇</mtext></ci><cn id="S5.T3.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T3.1.1.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\text{H}_{0}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.1" rowspan="3"><span class="ltx_text" id="S5.T3.1.2.1.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.4">-2.85</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.5">0.0214</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1.6">0.0641</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.7">False</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.3">29.58</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.1">BT</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.3">22.04</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.3.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.1" rowspan="3"><span class="ltx_text" id="S5.T3.1.5.4.1.1">chrF</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.4">-3.84</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.5">0.005</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.5.4.6">0.0149</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.5.4.7">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.3">30.12</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.5.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.6.5.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.1">BT</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.3">26.28</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.6.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.7.6.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T3.1.8.7.1" rowspan="3"><span class="ltx_text" id="S5.T3.1.8.7.1.1">TER</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.8.7.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.8.7.3">BT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.8.7.4">5.02</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.8.7.5">0.001</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.8.7.6">0.0031</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.8.7.7">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.8">
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.8.1">Baseline</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.8.2">Transfer</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.8.3">-23.74</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.8.4">0.0</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.9.8.5">0.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.9.8.6">True</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.10.9.1">BT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.10.9.2">Transfer</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.10.9.3">-15.91</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.10.9.4">0.0</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.10.9.5">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.10.9.6">True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of t-test with Bonferroni correction for de-bar systems.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Qualitative Analysis</h3>
<div class="ltx_para ltx_noindent" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">We argue that the surprisingly high baseline results come from the similarity of the source and target languages. This corresponds to findings from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib49" title="">49</a>]</cite> that language relatedness contributes positively to MT quality. The analysis of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib23" title="">23</a>]</cite>’s multilingual NMT on Indo-Aryan languages lists linguistic characteristics such as word-order construction, degree of inflection, amount of similar word root, meaning and conjunct verbs as the key drivers for improving training. Our experiments corroborate these argumentation, thus answering <span class="ltx_text ltx_font_bold" id="S5.SS6.p1.1.1">RQ1</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">The significant improvement from Back-translation, which can be seen with all metrics, aligns well with previous findings. Especially in the submitted systems for WMT21 Very Low Resource Supervised MT between Upper Sorbian and German by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib14" title="">14</a>]</cite>, Back-translation boosted the training corpus size and contributed to performance increase. However, we are aware of its limits. For instance, the augmented text includes many errors, which were inherited from the baseline systems. This issue of <span class="ltx_text ltx_font_italic" id="S5.SS6.p2.1.1">Translationese</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib62" title="">62</a>]</cite> is widely discussed, especially in the context of using silver-paired data for MT. In our case, we have opted for a smaller amount of augmented data, with the aim to reduce Translationese as much as possible while still allowing model improvement. We therefore answer <span class="ltx_text ltx_font_bold" id="S5.SS6.p2.1.2">RQ2</span> that Back-translation contributes positively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">Regarding <span class="ltx_text ltx_font_bold" id="S5.SS6.p3.1.1">RQ3</span>, we point out that while Transfer Learning did improve performance from parent to child, its final performance was not sufficient to exceed the other two systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p4">
<p class="ltx_p" id="S5.SS6.p4.1">We note that our results are similar to the ones from the German - Upper Sorbian translation task from WMT21. Our baseline and back-translated models have an accuracy range between 60 to 73 BLEU and 74 to 82 chrF, comparable with the final scores from the German - Upper Sorbian task. However, it is interesting to note that their chrF scores are substantially higher than ours (by 10), while our BLEU scores are similar. This brings us back to the notion that all metrics work linguistically different and these variations reflect through different languages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS6.p5">
<p class="ltx_p" id="S5.SS6.p5.1">Furthermore, a common finding can be observed between our experimental results and the WMT21 experiments we comapre against, namely the result discrepancy between high-to-low and low-to-high directions. In our study, de-bar is ca. 10 BLEU and 10 chrF behind bar-de. Similarly but not as extreme, Upper Sorbian - German also performs better than its high-to-low counter direction. This performance gap on the same corpus but different translation directions raises attention, with possible reasons due to the multiple orthographic standards and sub-dialects in our case.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">German Input</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Bavarian Output</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S5.T4.1.2.1.1" rowspan="2"><span class="ltx_text" id="S5.T4.1.2.1.1.1">sie hat heute abend im restaurant fisch bestellt.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.2.1.2">Base</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.2.1.3">se hod heit abend im restaurant fisch bestöid.</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.1.3.2.1">BT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.1.3.2.2">se hod heid obend im restaurant fisch bestejd.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Examples of German to Bavarian translation.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS6.p6">
<p class="ltx_p" id="S5.SS6.p6.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#S5.T4" title="Table 4 ‣ 5.6 Qualitative Analysis ‣ 5 Evaluation ‣ Investigating Neural Machine Translation for Low-Resource Languages: Using Bavarian as a Case StudyPreprint accepted at SIGUL 2024"><span class="ltx_text ltx_ref_tag">4</span></a> depicts two translation examples. We translate the German phrase “Sie hat heute Abend im Restaurant Fisch bestellt" (English meaning “she ordered fish in the restaurant tonight.") into Bavarian using all of our systems. We observe that while Base and BT outputs look similar, their differences could come from various sub-dialects in the corpus. For instance, the term “heute" was translated into “heit" and “heid", with only the last consonant different. However, in the Germanic linguistics, these consonants “t" and “d" differ themselves in voice. The linguistic notion of <span class="ltx_text ltx_font_italic" id="S5.SS6.p6.1.1">Fortis and Lenis<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright" href="https://en.wikipedia.org/wiki/Fortis_and_lenis" title="">https://en.wikipedia.org/wiki/Fortis_and_lenis</a></span></span></span></span> differentiates oral pressure that is given to these consonants. Thus, we suspect these differences come from various dialects.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we presented experimental work in Neural Machine Translation with the aim to push forward our understanding of how to best address the gap between a handful of well-resourced languages and the long tail of languages for which no sufficient resources are available.
More specifically, we focused on methods and case studies that have shown promising results for languages with limited resources. We conceptualized the problems of noisy data and data shortage by way of recent studies. We revisited creative solutions designed to combat these challenges such as Back-translation, multilingual training and language relatedness.
Our own low-resource implementation utilized data augmentation and cross-lingual transfer on German and Bavarian. We report our steps to preprocess the corpus and carry out training for three bidirectional systems. 5-fold cross validation was carried out on each system to compare robustness. We opted for a combined metric system using BLEU, chrF and TER to evaluate translation from different perspectives. For multiple hypothesis testing, pairwise t-tests with Bonferroni correction were conducted to test for statistical significance.
Results show that translation between similar languages performs generally better and that augmented data contribute positively. However, even though cross-lingual transfer showed huge improvement from parent to child, it was not able to exceed baseline and back-translated models. We recognize that Transfer Learning is an effective approach for low-resource languages, but note that in our study language similarity played a more important role.
To support reproducibility and replicability all code is made available via GitHub.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The Bavarian orthography has been a known problem for decades, as it is mostly a spoken language and has not been properly standardized. For example, the word ’Bavarian’ alone can be written in two ways: Boarisch or Bairisch. The investigation by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib63" title="">63</a>]</cite> illustrates that there are multiple Bavarian orthographic conventions. From a computational perspective, the issue is “deciding which representation should be given precedence", as stated in the Bribri case study by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib11" title="">11</a>]</cite>. Overcoming dialectal variations is also a problem of politics that can carry on for years. In light of the findings by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.08259v1#bib.bib64" title="">64</a>]</cite>, we would add that the automated translation of Bavarian should - like other under-sourced languages - be carefully planned with ethical considerations, and that purely using web-scraped data to deploy translation systems might neglect the concerns of speakers.
Another challenge lies in multiple sub-dialects. This phenomenon can be observed in our corpus, which is mined from the Bavarian Wikipedia, where articles are written in different regional dialects. We argue that these sub-dialects in the parallel corpus lead to translation confusion, resulting in translation outputs which consist of mixed accents. Nevertheless, should there be a more refined and organized corpus of a particular sub-dialect, our systems can serve as baselines for fine-tuning.
Another, more general limitation is the fact that throughout our work we conducted purely technical evaluations. The strength of such an experimental setup is that it can be reproduced and offers objective results. However, it is clearly necessary to involve native speakers to gain more insights into the quality of any translation process. We mitigated against the problem by choosing not just a single evaluation metric (such as BLEU), but no matter how many different metrics are chosen they are no substitute for user studies.
</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Future Work</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Following our findings and the limitations stated above, we propose further research directions to inspire future work: First, the curation of a more refined and organized parallel corpus for modern German-Bavarian to help establish a high quality benchmark for training and evaluation. An example to achieve this is through recruiting native speakers in both Bavarian and German who have an adequate amount of linguistic knowledge. This annotation could include not only translation of parallel sentences, but also the sub-dialects or Bavarian regional variations the speakers associate themselves with. This human-annotated dataset could furthermore be split into two parts, one for training and another for evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Additionally, identification of dialects would be an approach to counter translation confusion and mixed accents. This could help unify and isolate non-standardized languages or dialects. As mentioned in the previous section, a great way to start modelling sub-dialect detection is to automatically analyze the Wikipedia articles with their corresponding sub-dialects. This would greatly reduce the training corpus size, but additional measures to increase the corpus size could be taken, such as acquiring diverse datasets (i.e. open-source subtitles of Bavarian TV-programs or historical documents). More generally, we see our work as a reference benchmark for future work – be it to explore the same language pair further or other work into the general problem of low-resource language translation efforts.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Ethical Considerations</h2>
<div class="ltx_para ltx_noindent" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Ethical concerns arise whenever natural language is being sampled and used to train machine learning systems. For this experimental work we used existing test collections and other freely accessible data. All the experiments are conducted within the ethical framework imposed on us by our institution. In this context we did not identify a specific ethical issue.</p>
</div>
<div class="ltx_para ltx_noindent" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">However, it is clear that once any automated translation system is on its way to be deployed that care must be taken to (a) train it on <span class="ltx_text ltx_font_italic" id="S9.p2.1.1">representative</span> samples, (b) mitigate against common biases, and (c) make sure no personal information is included in the training data. If trained on social media data there is also a risk that toxic content might surface. Care must be taken to take these issues seriously (rather than treating this as a box-ticking exercise), but we would argue that there are no ethical concerns arising from this work that have not already been identified previously.</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Acknowledgment</h2>
<div class="ltx_para ltx_noindent" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">We would like to thank the anonymous reviewers for their constructive feedback.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio
Ranzato.

</span>
<span class="ltx_bibblock">Phrase-Based &amp; Neural Unsupervised Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing</span>, pages 5039–5049, Brussels, Belgium,
2018. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Meng Zhang, Liangyou Li, and Qun Liu.

</span>
<span class="ltx_bibblock">Two Parents, One Child: Dual Transfer for Low-Resource
Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021</span>, pages 2726–2738, Online, 2021.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight.

</span>
<span class="ltx_bibblock">Transfer Learning for Low-Resource Neural Machine
Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2016 Conference on Empirical Methods
in Natural Language Processing</span>, pages 1568–1575, Austin, Texas, 2016.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Nouman Ahmed, Natalia Flechas Manrique, and Antonije Petrović.

</span>
<span class="ltx_bibblock">Enhancing Spanish-Quechua machine translation with pre-trained
models and diverse data sources: LCT-EHU at AmericasNLP shared task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Workshop on Natural Language Processing
for Indigenous Languages of the Americas (AmericasNLP)</span>, pages 156–162,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Stephane Clinchant, Kweon Woo Jung, and Vassilina Nikoulina.

</span>
<span class="ltx_bibblock">On the use of BERT for neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 3rd Workshop on Neural Generation and
Translation</span>, pages 108–117, Hong Kong, November 2019. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Kaiyu Huang, Peng Li, Jin Ma, Ting Yao, and Yang Liu.

</span>
<span class="ltx_bibblock">Knowledge transfer in incremental learning for multilingual neural
machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 15286–15304,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Aaron Mueller, Garrett Nicolai, Arya D McCarthy, Dylan Lewis, Winston Wu, and
David Yarowsky.

</span>
<span class="ltx_bibblock">An Analysis of Massively Multilingual Neural Machine
Translation for Low-Resource Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 12th Conference on Language
Resources and Evaluation</span>, pages 3710–2718. European Language Resources
Association (ELRA), 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pages 3874–3884,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Raj Dabre, Atsushi Fujita, and Chenhui Chu.

</span>
<span class="ltx_bibblock">Exploiting Multilingualism through Multistage Fine-Tuning for
Low-Resource Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages
1410–1416, Hong Kong, China, 2019. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj
Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman.

</span>
<span class="ltx_bibblock">Small data, big impact: Leveraging minimal data for effective machine
translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 2740–2756,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Isaac Feldman and Rolando Coto-Solano.

</span>
<span class="ltx_bibblock">Neural Machine Translation Models with Back-Translation for
the Extremely Low-Resource Indigenous Language Bribri.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 28th International Conference on
Computational Linguistics</span>, pages 3965–3976, Barcelona, Spain (Online),
2020. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondřej Bojar,
Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina
España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette
Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield,
Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai,
Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie,
Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki
Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi,
Valentin Vydrin, and Marcos Zampieri.

</span>
<span class="ltx_bibblock">Findings of the 2021 Conference on Machine Translation
(WMT21).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Sixth Conference on Machine
Translation</span>, pages 1–88, Online, November 2021. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jindřich Libovický and Alexander Fraser.

</span>
<span class="ltx_bibblock">Findings of the WMT 2021 Shared Tasks in Unsupervised MT
and Very Low Resource Supervised MT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Sixth Conference on Machine
Translation</span>, pages 726–732, Online, November 2021. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rebecca Knowles and Samuel Larkin.

</span>
<span class="ltx_bibblock">NRC-CNRC systems for Upper Sorbian-German and Lower
Sorbian-German machine translation 2021.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Sixth Conference on Machine Translation</span>,
pages 999–1008, Online, November 2021. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">CoRR</span>, abs/1706.03762, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics</span>, pages 311–318, Philadelphia, Pennsylvania, USA,
July 2002. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Maja Popović.

</span>
<span class="ltx_bibblock">chrF: character n-gram F-score for automatic MT evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of the Tenth Workshop on Statistical Machine
Translation</span>, pages 392–395, Lisbon, Portugal, September 2015. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul.

</span>
<span class="ltx_bibblock">A study of translation edit rate with targeted human annotation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 7th Conference of the Association for
Machine Translation in the Americas: Technical Papers</span>, pages 223–231,
Cambridge, Massachusetts, USA, August 8-12 2006. Association for Machine
Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt,
Hitokazu Matsushita, and Arul Menezes.

</span>
<span class="ltx_bibblock">To ship or not to ship: An extensive evaluation of automatic metrics
for machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Sixth Conference on Machine Translation</span>,
pages 478–494, Online, November 2021. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Markus Freitag, David Grangier, and Isaac Caswell.

</span>
<span class="ltx_bibblock">BLEU might be guilty but references are not innocent.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">CoRR</span>, abs/2004.06063, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jörg Tiedemann.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in OPUS.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC’12)</span>, pages 2214–2218, Istanbul,
Turkey, May 2012. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Machel Reid, Junjie Hu, Graham Neubig, and Yutaka Matsuo.

</span>
<span class="ltx_bibblock">AfroMT: Pretraining Strategies and Reproducible Benchmarks
for Translation of 8 African Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing</span>, pages 1306–1320, Online and Punta
Cana, Dominican Republic, 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Vikrant Goyal, Sourav Kumar, and Dipti Misra Sharma.

</span>
<span class="ltx_bibblock">Efficient Neural Machine Translation for Low-Resource
Languages via Exploiting Related Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics: Student Research
Workshop</span>, pages 162–168, Online, 2020. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser.

</span>
<span class="ltx_bibblock">Improving the Lexical Ability of Pretrained Language Models
for Unsupervised Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</span>, pages 173–180, Online, 2021. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Mikel Artetxe, Gorka Labaka, and Eneko Agirre.

</span>
<span class="ltx_bibblock">A robust self-learning method for fully unsupervised cross-lingual
mappings of word embeddings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long
Papers)</span>, pages 789–798, Melbourne, Australia, 2018. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.

</span>
<span class="ltx_bibblock">Unsupervised machine translation using monolingual corpora only.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Emily M. Bender.

</span>
<span class="ltx_bibblock">The #BenderRule: On Naming the Languages We Study and Why It
Matters, 2019.

</span>
<span class="ltx_bibblock">https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample,
Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato.

</span>
<span class="ltx_bibblock">The FLORES Evaluation Datasets for Low-Resource Machine
Translation: Nepali–English and Sinhala–English.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages
6097–6110, Hong Kong, China, 2019. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Christopher Cieri, Mike Maxwell, Stephanie Strassel, and Jennifer Tracey.

</span>
<span class="ltx_bibblock">Selection Criteria for Low Resource Language Programs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC’16)</span>, pages 4543–4549,
Portorož, Slovenia, May 2016. European Language Resources Association
(ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Louisa Lambrecht, Felix Schneider, and Alexander Waibel.

</span>
<span class="ltx_bibblock">Machine translation from Standard German to alemannic dialects.

</span>
<span class="ltx_bibblock">In Maite Melero, Sakriani Sakti, and Claudia Soria, editors, <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 1st Annual Meeting of the ELRA/ISCA Special Interest Group
on Under-Resourced Languages</span>, pages 129–136, Marseille, France, June 2022.
European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 1715–1725, Berlin,
Germany, August 2016. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita.

</span>
<span class="ltx_bibblock">BPE-dropout: Simple and effective subword regularization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 1882–1892, Online, July 2020. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li.

</span>
<span class="ltx_bibblock">Search engine guided non-parametric neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">CoRR</span>, abs/1705.07267, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.

</span>
<span class="ltx_bibblock">Nearest neighbor machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Pedro Henrique Martins, Zita Marinho, and André F. T. Martins.

</span>
<span class="ltx_bibblock">Chunk-based nearest neighbor machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span>, pages 4228–4245, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Tom Kocmi and Ondřej Bojar.

</span>
<span class="ltx_bibblock">Trivial Transfer Learning for Low-Resource Neural Machine
Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the Third Conference on Machine
Translation: Research Papers</span>, pages 244–252, Belgium, Brussels, 2018.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhaocong Li, Xuebo Liu, Derek F. Wong, Lidia S. Chao, and Min Zhang.

</span>
<span class="ltx_bibblock">ConsistTL: Modeling consistency in transfer learning for
low-resource neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span>, pages 8383–8394, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Kenji Imamura and Eiichiro Sumita.

</span>
<span class="ltx_bibblock">Recycling a pre-trained BERT encoder for neural machine
translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 3rd Workshop on Neural Generation and
Translation</span>, pages 23–31, Hong Kong, November 2019. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers)</span>,
pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Guillaume Lample and Alexis Conneau.

</span>
<span class="ltx_bibblock">Cross-lingual language model pretraining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">CoRR</span>, abs/1901.07291, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Mozhdeh Gheini, Xiang Ren, and Jonathan May.

</span>
<span class="ltx_bibblock">Cross-Attention is All You Need: Adapting Pretrained
Transformers for Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing</span>, pages 1754–1765, Online and Punta
Cana, Dominican Republic, 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Akshay Batheja and Pushpak Bhattacharyya.

</span>
<span class="ltx_bibblock">Improving machine translation with phrase pair injection and corpus
filtering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span>, pages 5395–5400, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Etienne Denoual and Yves Lepage.

</span>
<span class="ltx_bibblock">BLEU in characters: Towards automatic MT evaluation in languages
without word delimiters.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Companion Volume to the Proceedings of Conference including
Posters/Demos and tutorial abstracts</span>, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang.

</span>
<span class="ltx_bibblock">Multi-task learning for multiple language translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pages 1723–1732,
Beijing, China, July 2015. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch, and Eiichiro Sumita.

</span>
<span class="ltx_bibblock">Introducing the Asian language treebank (ALT).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC’16)</span>, pages 1574–1578, Portorož,
Slovenia, May 2016. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Maha Elbayad, Anna Sun, and Shruti Bhosale.

</span>
<span class="ltx_bibblock">Fixing MoE over-fitting on low-resource languages in multilingual
machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</span>, pages 14237–14253, Toronto, Canada, July 2023. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">CoRR</span>, abs/1904.09751, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Loic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R.
Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus
Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow,
Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins,
Makoto Morishita, and Christof Monz, editors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of the Sixth Conference on Machine Translation</span>.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics, Online, November 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ife Adebara, El Moatez Billah Nagoudi, and Muhammad Abdul Mageed.

</span>
<span class="ltx_bibblock">Translating similar languages: Role of mutual intelligibility in
multilingual transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Proceedings of the Fifth Conference on Machine Translation</span>,
pages 381–386, Online, November 2020. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Meng Zhang, Minghao Wu, Pengfei Li, Liangyou Li, and Qun Liu.

</span>
<span class="ltx_bibblock">NoahNMT at WMT 2021: Dual transfer for very low resource
supervised machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Proceedings of the Sixth Conference on Machine Translation</span>,
pages 1009–1013, Online, November 2021. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">CoRR</span>, abs/1511.06709, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Robert C. Moore and William Lewis.

</span>
<span class="ltx_bibblock">Intelligent selection of language model training data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the ACL 2010 Conference Short Papers</span>, pages
220–224, Uppsala, Sweden, July 2010. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Jörg Tiedemann.

</span>
<span class="ltx_bibblock">The Tatoeba Translation Challenge – Realistic data sets
for low resource and multilingual MT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Proceedings of the Fifth Conference on Machine Translation</span>,
pages 1174–1182, Online, November 2020. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Philipp Koehn and Rebecca Knowles.

</span>
<span class="ltx_bibblock">Six Challenges for Neural Machine Translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proceedings of the First Workshop on Neural Machine
Translation</span>, pages 28–39, Vancouver, 2017. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Tobias Domhan, Michael Denkowski, David Vilar, Xing Niu, Felix Hieber, and
Kenneth Heafield.

</span>
<span class="ltx_bibblock">The sockeye 2 neural machine translation toolkit at AMTA 2020.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 14th Conference of the Association for
Machine Translation in the Americas (Volume 1: Research Track)</span>, pages
110–115, Virtual, October 2020. Association for Machine Translation in the
Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M Aickin and H Gensler.

</span>
<span class="ltx_bibblock">Adjusting for multiple testing when reporting research results: The
bonferroni vs holm methods, May 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
William S Noble.

</span>
<span class="ltx_bibblock">How does multiple testing correction work?, Dec 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and
Wolfgang Macherey.

</span>
<span class="ltx_bibblock">Experts, Errors, and Context: A Large-Scale Study of Human
Evaluation for Machine Translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Transactions of the Association for Computational Linguistics</span>,
9:1460–1474, 12 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush
Kumar, Mitesh M. Khapra, and Raj Dabre.

</span>
<span class="ltx_bibblock">IndicMT eval: A dataset to meta-evaluate machine translation
metrics for Indian languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 14210–14228,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.

</span>
<span class="ltx_bibblock">COMET: A neural framework for MT evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pages 2685–2702, Online, November
2020. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondřej Bojar.

</span>
<span class="ltx_bibblock">Results of the WMT20 metrics shared task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Proceedings of the Fifth Conference on Machine Translation</span>,
pages 688–725, Online, November 2020. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Yvette Graham, Barry Haddow, and Philipp Koehn.

</span>
<span class="ltx_bibblock">Statistical power and translationese in machine translation
evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pages 72–81, Online, November 2020.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Ludwig Zehetner.

</span>
<span class="ltx_bibblock">Zur schreibung des bairischen.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Schmankerl</span>, 37:31–32, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Manuel Mager, Elisabeth Mager, Katharina Kann, and Ngoc Thang Vu.

</span>
<span class="ltx_bibblock">Ethical considerations for machine translation of indigenous
languages: Giving a voice to the speakers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 4871–4897,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Apr 10 19:19:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
