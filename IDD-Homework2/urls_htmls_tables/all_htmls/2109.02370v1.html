<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.02370] Improved RAMEN: Towards Domain Generalization for Visual Question Answering</title><meta property="og:description" content="Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.
Established as a multi-disciplinary field in machine learning, both computer vision and natural…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improved RAMEN: Towards Domain Generalization for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improved RAMEN: Towards Domain Generalization for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.02370">

<!--Generated on Tue Mar 19 09:57:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
visual question answering,  computer vision,  natural language processing,  attention,  generalisation,  RAMEN,  early fusion,  late fusion,  transformer
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improved RAMEN: Towards Domain Generalization for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bhanuka Manesha Samarasekara Vitharana Gamage
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">School of Information Technology</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Monash University
<br class="ltx_break"></span>Bandar Sunway, Malaysia 
<br class="ltx_break">bsam0002@student.monash.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lim Chern Hong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">School of Information Technology</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">Monash University
<br class="ltx_break"></span>Bandar Sunway, Malaysia
<br class="ltx_break">lim.chernhong@monash.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.
Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance.
However, there is a gap between the SOTA results and real world applications.
This is due to the lack of model generalisation.
The RAMEN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets.
This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization.
Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module.
Improvements of up to five VQA datasets from the experiments conducted are evident.
Following the results, this study analyses the effects of both the improvements on the domain generalization problem.
The code is available on GitHub though the following link <a target="_blank" href="https://github.com/bhanukaManesha/ramen" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/bhanukaManesha/ramen</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
visual question answering, computer vision, natural language processing, attention, generalisation, RAMEN, early fusion, late fusion, transformer

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a multi-disciplinary problem in machine learning that exists at the intersection of the computer vision, natural language processing and knowledge representation fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Recently, the task of VQA has been classified as an AI-complete task due to the complexity of it.
This problem requires the semantic understanding of each of the three fields as well as the relationship between each one of them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
One of the main issues in VQA is that the state-of-the-art (SOTA) results on the datasets do not translate on to real-world applications.
This has directed the VQA field towards generalization.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The datasets in the field of VQA can be separated into two main categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The first type focuses on answering questions by understanding the objects on natural real world images and the other focuses on using synthetic images to test reasoning questions.
The problem with this categorization is that the algorithms tend to focus on one or the other and not generalize on both.
This known as the domain generalization problem, because the VQA models generalize on both types of dataset either through training from scratch or fine tuning to the domains and not overfitting on one type.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> addressed this issue by introducing a framework for domain generalization.
This framework allows to train models of both domains with similar visual and textual features to evaluate their generalization ability.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">They also introduced the RAMEN model architecture which was able to outperform all the other models compared in the study in terms of domain generalization.
However, this model uses a simple architecture with a potential for improvement and exploration.
Therefore, this study proposes improvements to the architecture of the RAMEN model while analyzing the effect of these changes to the overall problem of domain generalization.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The main contributions of this study includes the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Improvements to the domain generalization performance of the RAMEN model architecture by proposing modifications to the fusion and aggregation modules.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A broad comparison of the vector based fusion operations for early and late fusion pertaining to domain generalization.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Implementation and analysis of a transformer based aggregation module to map the relationships between bi-modal embeddings of the regional proposals in the RAMEN model.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of the paper is organized as follows:
Section <a href="#S2" title="II Related Work ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides more context to the domain generalization problem in VQA, the RAMEN model and the transformer architecture in VQA.
The proposed improvements to the RAMEN model is detailed in Section <a href="#S3" title="III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, which is followed by the experiment strategy used in Section <a href="#S4" title="IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
A comprehensive analysis of the results is conducted in Section <a href="#S5" title="V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> which is then summarized in Section <a href="#S6" title="VI Conclusion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2109.02370/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="453" height="485" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">High-level architecture of the RAMEN model</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Summary of VQA datasets used in this study. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<td id="S2.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.1.1.1" class="ltx_p"><span id="S2.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</td>
<td id="S2.T1.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.2.1.1" class="ltx_p"><span id="S2.T1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Image Type</span></span>
</span>
</td>
<td id="S2.T1.4.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.3.1.1" class="ltx_p"><span id="S2.T1.4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Question Type</span></span>
</span>
</td>
<td id="S2.T1.4.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.4.1.1" class="ltx_p"><span id="S2.T1.4.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Images</span></span>
</span>
</td>
<td id="S2.T1.4.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.5.1.1" class="ltx_p"><span id="S2.T1.4.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Q&amp;A Pairs</span></span>
</span>
</td>
<td id="S2.T1.4.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.6.1.1" class="ltx_p"><span id="S2.T1.4.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Download links</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.2.2" class="ltx_tr">
<td id="S2.T1.4.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.1.1.1" class="ltx_p">VQAv1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.2.1.1" class="ltx_p">Natural</span>
</span>
</td>
<td id="S2.T1.4.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.3.1.1" class="ltx_p">Human</span>
</span>
</td>
<td id="S2.T1.4.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.4.1.1" class="ltx_p">204K</span>
</span>
</td>
<td id="S2.T1.4.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.5.1.1" class="ltx_p">614K</span>
</span>
</td>
<td id="S2.T1.4.2.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.6.1.1" class="ltx_p"><a target="_blank" href="https://visualqa.org/vqa_v1_download.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/vqa_v1_download.html</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.3.3" class="ltx_tr">
<td id="S2.T1.4.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.1.1.1" class="ltx_p">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.2.1.1" class="ltx_p">Natural</span>
</span>
</td>
<td id="S2.T1.4.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.3.1.1" class="ltx_p">Human</span>
</span>
</td>
<td id="S2.T1.4.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.4.1.1" class="ltx_p">204K</span>
</span>
</td>
<td id="S2.T1.4.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.5.1.1" class="ltx_p">1.1M</span>
</span>
</td>
<td id="S2.T1.4.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.6.1.1" class="ltx_p"><a target="_blank" href="https://visualqa.org/download.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/download.html</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4.4" class="ltx_tr">
<td id="S2.T1.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.1.1.1" class="ltx_p">TDIUC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.2.1.1" class="ltx_p">Natural</span>
</span>
</td>
<td id="S2.T1.4.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.3.1.1" class="ltx_p">Both</span>
</span>
</td>
<td id="S2.T1.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.4.1.1" class="ltx_p">167K</span>
</span>
</td>
<td id="S2.T1.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.5.1.1" class="ltx_p">1.6M</span>
</span>
</td>
<td id="S2.T1.4.4.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.6.1.1" class="ltx_p"><a target="_blank" href="https://kushalkafle.com/projects/tdiuc.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://kushalkafle.com/projects/tdiuc.html</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.5.5" class="ltx_tr">
<td id="S2.T1.4.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.1.1.1" class="ltx_p">C-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.2.1.1" class="ltx_p">Natural</span>
</span>
</td>
<td id="S2.T1.4.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.3.1.1" class="ltx_p">Human</span>
</span>
</td>
<td id="S2.T1.4.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.4.1.1" class="ltx_p">123K</span>
</span>
</td>
<td id="S2.T1.4.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.5.1.1" class="ltx_p">369K</span>
</span>
</td>
<td id="S2.T1.4.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.6.1.1" class="ltx_p"><a target="_blank" href="https://computing.ece.vt.edu/~aish/cvqa/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://computing.ece.vt.edu/~aish/cvqa/</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.6.6" class="ltx_tr">
<td id="S2.T1.4.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.1.1.1" class="ltx_p">VQACPv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.2.1.1" class="ltx_p">Natural</span>
</span>
</td>
<td id="S2.T1.4.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.3.1.1" class="ltx_p">Human</span>
</span>
</td>
<td id="S2.T1.4.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.4.1.1" class="ltx_p">219K</span>
</span>
</td>
<td id="S2.T1.4.6.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.5.1.1" class="ltx_p">603K</span>
</span>
</td>
<td id="S2.T1.4.6.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.6.6.6.1.1" class="ltx_p"><a target="_blank" href="https://computing.ece.vt.edu/~aish/vqacp/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://computing.ece.vt.edu/~aish/vqacp/</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.7.7" class="ltx_tr">
<td id="S2.T1.4.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.1.1.1" class="ltx_p">CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.2.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.3.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.7.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.4.1.1" class="ltx_p">100K</span>
</span>
</td>
<td id="S2.T1.4.7.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.5.1.1" class="ltx_p">999K</span>
</span>
</td>
<td id="S2.T1.4.7.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.7.7.6.1.1" class="ltx_p"><a target="_blank" href="https://cs.stanford.edu/people/jcjohns/clevr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cs.stanford.edu/people/jcjohns/clevr/</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.8.8" class="ltx_tr">
<td id="S2.T1.4.8.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.1.1.1" class="ltx_p">CLEVR-Humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.8.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.2.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.8.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.3.1.1" class="ltx_p">Human</span>
</span>
</td>
<td id="S2.T1.4.8.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.4.1.1" class="ltx_p">32K</span>
</span>
</td>
<td id="S2.T1.4.8.8.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.5.1.1" class="ltx_p">32K</span>
</span>
</td>
<td id="S2.T1.4.8.8.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.8.8.6.1.1" class="ltx_p"><a target="_blank" href="https://cs.stanford.edu/people/jcjohns/iep/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cs.stanford.edu/people/jcjohns/iep/</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.9.9" class="ltx_tr">
<td id="S2.T1.4.9.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.1.1.1" class="ltx_p">CLEVR-CoGenT-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.9.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.2.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.9.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.3.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.9.9.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.4.1.1" class="ltx_p">100K</span>
</span>
</td>
<td id="S2.T1.4.9.9.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.5.1.1" class="ltx_p">999K</span>
</span>
</td>
<td id="S2.T1.4.9.9.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.9.9.6.1.1" class="ltx_p"><a target="_blank" href="https://cs.stanford.edu/people/jcjohns/clevr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cs.stanford.edu/people/jcjohns/clevr/</a></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.10.10" class="ltx_tr">
<td id="S2.T1.4.10.10.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:76.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.1.1.1" class="ltx_p">CLEVR-CoGenT-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S2.T1.4.10.10.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.2.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.10.10.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.3.1.1" class="ltx_p">Synthetic</span>
</span>
</td>
<td id="S2.T1.4.10.10.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.4.1.1" class="ltx_p">30K</span>
</span>
</td>
<td id="S2.T1.4.10.10.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.5.1.1" class="ltx_p">299K</span>
</span>
</td>
<td id="S2.T1.4.10.10.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:142.3pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.4.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.10.10.6.1.1" class="ltx_p"><a target="_blank" href="https://cs.stanford.edu/people/jcjohns/clevr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cs.stanford.edu/people/jcjohns/clevr/</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section first summarizes the main VQA datasets used in this study, followed by the RAMEN model.
Next, the background of the transformer architecture in VQA is explored.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">VQA Datasets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The dataset is the most important part of the VQA pipeline as it determines what the model learns.
If the dataset contains inherent biases, the model will learn these and the performance of the model will be affected.
Many datasets were introduced, with each dataset focusing on either solving biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or solving a specific type of question domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">This study focuses on the datasets used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, where the datasets were divided into two groups; VQA datasets for natural image understanding and VQA datasets to test reasoning.
Table <a href="#S2.T1" title="TABLE I ‣ II Related Work ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the VQA datasets used by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to test for generalization.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.5.1.1" class="ltx_text">II-A</span>1 </span><span id="S2.SS1.SSS1.6.2" class="ltx_text ltx_font_bold">VQA Datasets for Natural Image Understanding</span>
</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">These datasets aims to provide answers by identifying objects in the image.
This can be through colour, count or other visual cues.
All the datasets in this group uses the MSCOCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as the base image dataset except for TDIUC which adds extra images.</p>
</div>
<section id="S2.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQAv1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</h5>

<div id="S2.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px1.p1.1" class="ltx_p">One of the most widely known datasets with the current SOTA accuracy of 75.26%.
This dataset mainly focuses on detection questions such as <em id="S2.SS1.SSS1.Px1.p1.1.1" class="ltx_emph ltx_font_italic">Is there food on the table</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and lacks reasoning questions such as <em id="S2.SS1.SSS1.Px1.p1.1.2" class="ltx_emph ltx_font_italic">What is behind the computer in the corner of the table?</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
However, it consists of inherent question-answer biases where questions such as <em id="S2.SS1.SSS1.Px1.p1.1.3" class="ltx_emph ltx_font_italic">Where is the giraffe standing on?</em> always contains the answer <em id="S2.SS1.SSS1.Px1.p1.1.4" class="ltx_emph ltx_font_italic">grass</em>.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</h5>

<div id="S2.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px2.p1.1" class="ltx_p">The successor to VQAv1, was able to reduce the question-answer biases by introducing complementary questions.
However, even though this allowed the VQAv2 dataset to be more balanced, the bias of having more detection questions is still prevalent in this dataset; which makes the models trained on VQAv2 datasets inherently weaker when answering questions with reasoning.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">TDIUC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</h5>

<div id="S2.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px3.p1.1" class="ltx_p">This dataset was created with the primary aim of evaluating the performance of models on 12 distinct types of VQA tasks.
Color attributes, positional reasoning and object presence are some of the types of tasks.
A new metric called Mean-per-type was also introduced as shown in Equation <a href="#S4.E7" title="In Mean-per-type ‣ IV-C Evaluation metrics ‣ IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> in Section <a href="#S4.E7" title="In Mean-per-type ‣ IV-C Evaluation metrics ‣ IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
Therefore, it is evident that a model needs to perform well across all the question types to get a good performance score.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">C-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</h5>

<div id="S2.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px4.p1.1" class="ltx_p">Aims to re-split the VQAv1 dataset to introduce novel combinations for the question-answer pairs when testing.
During testing the models will come across new combinations of question-answer pairs.
Therefore, the models need to be able to generalize on the task and not the question and answer.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQACPv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</h5>

<div id="S2.SS1.SSS1.Px5.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px5.p1.1" class="ltx_p">Overcomes the question and language bias by splitting the VQAv1 and VQAv2 dataset.
A completely different answer distribution is present in the test split compared to the training split.
This allows the models to test their ability to generalize by not over-fitting on the training set.</p>
</div>
</section>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.5.1.1" class="ltx_text">II-A</span>2 </span><span id="S2.SS1.SSS2.6.2" class="ltx_text ltx_font_bold">VQA Datasets to Test Reasoning</span>
</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">These datasets aim to test the ability of models to answer reasoning based questions by using synthetic images.
These synthetic computer generated images allow this dataset to generate complex reasoning questions automatically.
All the datasets in this group use the images from the CLEVR dataset, with each dataset having different question-answer pairs.</p>
</div>
<section id="S2.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</h5>

<div id="S2.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS2.Px1.p1.1" class="ltx_p">The main goal of this dataset is to test the reasoning capability of models on geometric shapes.
Similar to TDIUC, this dataset is classified into five categories.</p>
</div>
</section>
<section id="S2.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CLEVR-Humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</h5>

<div id="S2.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.Px2.p1.1" class="ltx_p">The main downside of CLEVR dataset is that the questions are computer generated, thus being very structured.
The CLEVR-Humans dataset addresses this issue by using free form human generated question-answer pairs.
It still uses the same images from the CLEVR dataset.</p>
</div>
</section>
<section id="S2.SS1.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CLEVR-CoGenT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</h5>

<div id="S2.SS1.SSS2.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS2.Px3.p1.1" class="ltx_p">This dataset was introduced with the CLEVR dataset having two splits with mutually exclusive color and shapes, namely, CLEVR-CoGenTA and CLEVR-CoGenTB.
This dataset aims to study the model’s ability to recognize novel combinations of attributes such as color and shapes at test time.
For example, CLEVR-CoGenTA contains red colour cylinders in the training set, in contrast, CLEVR-CoGenTB does not contain red colour cylinders.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">RAMEN</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The VQA pipeline consists of five main components; VQA dataset, Image representation, Question representation, Multi-modal representation and Answer classification.
Many studies have been done in the field of VQA, with each focusing on improving different sections of the VQA pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> proposed a framework to compare the performance of VQA algorithms across different domains.
They standardize the image representation and the question representation across the VQA datasets of multiple domains.
With this, they were able to compare the performance of multiple algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> across domains and assess the generalization ability of the model architectures.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">They also proposed a model named RAMEN with a conceptually simple architecture that was able to generalize across multiple domains.
Figure <a href="#S2.F1" title="Figure 1 ‣ II Related Work ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the high-level architecture of the RAMEN model, where the five main components of the VQA pipeline can be identified in this model.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image Representation</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">The image representation module focuses on extracting features from the image and converting them into visual features.
Various methods exists that focuses on extracting features from images using different techniques such as VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
The RAMEN model uses a Faster-RCNN based technique where the image is passed through the bottom-up-top-down network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which uses attention on the object level to return visual features as a set of regions.
These regions corresponds to the main object regions in the image which are used to answer the questions.
For VQAv1, VQAv2, CVQA, VQACPv2 and TDIUC datasets the bottom-up attention module returns 36 regions and for the CLEVR family of datasets it return 15 regions.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Question Representation</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">The question representation module converts the question into a vector representation.
This vector representation encodes all the words while maintaining the flow and positional information of the question.
Studies have proposed multiple ways to extract these features by using CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> networks.
The RAMEN model uses a GRU based approach by first splitting the questions into multiple word tokens.
Each token is instantiated with the GLOVE embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Then the embeddings are passed through a GRU based RNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to obtain the question representation.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multi Modal representation</h5>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">Once the image and question representations are passed into this module, the two vectors are fused together using concatenation.
This step is also known as early fusion in the model architecture.
Next, the RAMEN model uses a Multi Layer Perceptorn (MLP) to create a bimodal embedding.
This allows the model to learn the relationship between the image and question representation.
Then the bimodal embedding is concatenated with the question representation, with the Late Fusion module.
The fused vector is then passed to the aggregation module where it is passed through a bi-directional GRU network.
This step captures the relationships between the bimodal embeddings.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Classification</h5>

<div id="S2.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px4.p1.1" class="ltx_p">In the final module, the output of the multi-modal representation is pass through a series of linear layers to perform the pre-classification step.
This is then followed by a single linear layer for classification.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Transformer</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The introduction of the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> has been a pivotal moment in the NLP community.
The main use case of the transformer network is for machine translation.
The main advantage of using transformer over traditional RNN networks is that the sequences are processed as a whole compared to one by one.
Moreover, the transformer uses multi-head attention and positional encoding to obtain more information about the relationships between the features.
This allows the transformer architecture to be parallelizable compared to sequential RNN networks.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Many studies have been done in the field of VQA that incorporated transformers into the architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Most of them focus on encoding the question using the transformer architecture.
The Bidirectional Encoder Representations from Transformers (BERT) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, which is derived from the transformer architecture is commonly used to encode the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
However, limited research has been done where the transformer architecture is used to capture the relationship between visual and question features.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> showed that the transformer architecture is able to capture intra-modality and cross-modality relationships on the VQA and GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> datasets.
Therefore, this study aims to investigate the effect of using transformer as an encoder in VQA.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Additive Fusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="462" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Multiplicative Fusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x4.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="45" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Concat Fusion</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x5.png" id="S3.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="36" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Question Fusion</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Fusion Strategies for Early and Late fusion</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The key focus of the improvements are on the multi modal representation section of the RAMEN model.
Based on the ablation study done by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the early fusion module has a significant effect on the performance of the model.
The aggregation technique also has an effect on the performance of the model, whereas, the late fusion module has the minimum effect on the performance of the model.
Therefore, the experiments are done on the Early Fusion, Late Fusion and the Aggregation modules of the model as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ II Related Work ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Fusion Strategies</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> performed a survey on the fusion strategies in ImageVQA and VideoVQA studies.
They classified the fusion strategies into three main types; Vector operations, Neural Networks (NN) and Bilinear pooling.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The RAMEN model uses a mix of vector operations and neural networks to perform the multi-modal fusion.
The early and late sub-modules uses simple concatenation of the features and the shared projection and aggregation sub-modules uses neural networks as the strategy.
First in the early fusion module, the regional visual features are fused using concatenation with the question embedding to obtain the early fused embedding.
This is then passed through the neural network based shared projection and the output bi-modal embedding is obtained.
In the late fusion module, the bi-modal embedding is again fused using concatenation with the question embedding to obtain the late fused embedding.
After the fusion operation, both early and late fusion embeddings are passed through a Batch Normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
Finally, the vector is passed through the aggregation module, which is a Recurrent Neural Network based fusion strategy to obtain the fused vector for classification.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In the survey, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> also categorized the vector operations into three main sections; concatenation, addition and multiplication.
In this study, these three main vector operations will be experimented on the RAMEN model to observe the performance effect on the NN.
Figure <a href="#S3.F2" title="Figure 2 ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overview of the fusion strategies tested in this study.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span><span id="S3.SS1.SSS1.6.2" class="ltx_text ltx_font_bold">Concat Fusion</span>
</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">This is the baseline strategy used by the RAMEN model.
A question embedding size of 1024 and visual feature size of 2048 is used to obtain a final embedding size of 3072.
In this approach the output embedding passes all the information from both embeddings to the NN to identify the relationships.
No information is lost in this approach and all the feature points are given similar weights.
In order to perform vector operations, the question embedding is repeated to match the size of the visual features and bi-modal embedding.
To do this, the question embedding is repeated 36 times for the VQA family of datasets and 15 times for the CLEVR family of datasets.
This is done for all the fusion strategies experimented in this study.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.3" class="ltx_p">Equation <a href="#S3.E1" title="In III-A1 Concat Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is used to obtain the final embedding (<math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><msub id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">c_{i}</annotation></semantics></math>), where <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><msub id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p2.2.m2.1.1.2" xref="S3.SS1.SSS1.p2.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS1.p2.2.m2.1.1.3" xref="S3.SS1.SSS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><apply id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.2">𝑞</ci><ci id="S3.SS1.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">q_{i}</annotation></semantics></math> is the question embedding and <math id="S3.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><msub id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1.2" xref="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS1.p2.3.m3.1.1.3" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><apply id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.2">𝑣</ci><ci id="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">v_{i}</annotation></semantics></math> is the regional visual features or the bi-modal embedding.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="c_{i}=BatchNorm([q_{i},v_{i}])" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">c</mi><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.5" xref="S3.E1.m1.1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2b" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.6" xref="S3.E1.m1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2c" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.7" xref="S3.E1.m1.1.1.1.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2d" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.8" xref="S3.E1.m1.1.1.1.8.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2e" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.9" xref="S3.E1.m1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2f" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.10" xref="S3.E1.m1.1.1.1.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2g" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.11" xref="S3.E1.m1.1.1.1.11.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2h" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">[</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.2.4" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml">v</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.5" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑐</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝐵</ci><ci id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4">𝑎</ci><ci id="S3.E1.m1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.5">𝑡</ci><ci id="S3.E1.m1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.6">𝑐</ci><ci id="S3.E1.m1.1.1.1.7.cmml" xref="S3.E1.m1.1.1.1.7">ℎ</ci><ci id="S3.E1.m1.1.1.1.8.cmml" xref="S3.E1.m1.1.1.1.8">𝑁</ci><ci id="S3.E1.m1.1.1.1.9.cmml" xref="S3.E1.m1.1.1.1.9">𝑜</ci><ci id="S3.E1.m1.1.1.1.10.cmml" xref="S3.E1.m1.1.1.1.10">𝑟</ci><ci id="S3.E1.m1.1.1.1.11.cmml" xref="S3.E1.m1.1.1.1.11">𝑚</ci><interval closure="closed" id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2">𝑣</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">c_{i}=BatchNorm([q_{i},v_{i}])</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span><span id="S3.SS1.SSS2.6.2" class="ltx_text ltx_font_bold">Additive Fusion</span>
</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.3" class="ltx_p">For the additive fusion, the question embedding is matched to the same size as the visual features.
Therefore, the embedding size is changed from 1024 to 2048 to obtain the final embedding size of 2048.
This approach emphasizes on the different feature points which allows the model to update the question embeddings to focus on them.
This approach has information loss due to the addition operation, however, it is compensated by the increase in the question embedding size.
Equation <a href="#S3.E2" title="In III-A2 Additive Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is used to obtain the final embedding (<math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">c_{i}</annotation></semantics></math>).
<math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><msub id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">q_{i}</annotation></semantics></math> and <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><msub id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS2.p1.3.m3.1.1.2" xref="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS2.p1.3.m3.1.1.3" xref="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><apply id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.2">𝑣</ci><ci id="S3.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">v_{i}</annotation></semantics></math> are as described as in Equation <a href="#S3.E1" title="In III-A1 Concat Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="c_{i}=BatchNorm(v_{i}\oplus q_{i})" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">c</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.4" xref="S3.E2.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2a" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.5" xref="S3.E2.m1.1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2b" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.6" xref="S3.E2.m1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2c" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.7" xref="S3.E2.m1.1.1.1.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2d" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.8" xref="S3.E2.m1.1.1.1.8.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2e" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.9" xref="S3.E2.m1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2f" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.10" xref="S3.E2.m1.1.1.1.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2g" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.11" xref="S3.E2.m1.1.1.1.11.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2h" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">⊕</mo><msub id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">q</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝐵</ci><ci id="S3.E2.m1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.4">𝑎</ci><ci id="S3.E2.m1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.5">𝑡</ci><ci id="S3.E2.m1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.6">𝑐</ci><ci id="S3.E2.m1.1.1.1.7.cmml" xref="S3.E2.m1.1.1.1.7">ℎ</ci><ci id="S3.E2.m1.1.1.1.8.cmml" xref="S3.E2.m1.1.1.1.8">𝑁</ci><ci id="S3.E2.m1.1.1.1.9.cmml" xref="S3.E2.m1.1.1.1.9">𝑜</ci><ci id="S3.E2.m1.1.1.1.10.cmml" xref="S3.E2.m1.1.1.1.10">𝑟</ci><ci id="S3.E2.m1.1.1.1.11.cmml" xref="S3.E2.m1.1.1.1.11">𝑚</ci><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">𝑞</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">c_{i}=BatchNorm(v_{i}\oplus q_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span><span id="S3.SS1.SSS3.6.2" class="ltx_text ltx_font_bold">Multiplicative Fusion</span>
</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.3" class="ltx_p">Similar to additive, the question embedding size of 2048 is used for this strategy.
In this approach, the emphasis on different feature points is greater than the additive fusion.
Equation <a href="#S3.E3" title="In III-A3 Multiplicative Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is used to obtain the final embedding (<math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><msub id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.1.1.2" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS3.p1.1.m1.1.1.3" xref="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><apply id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">c_{i}</annotation></semantics></math>).
<math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><msub id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p1.2.m2.1.1.2" xref="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS3.p1.2.m2.1.1.3" xref="S3.SS1.SSS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><apply id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">q_{i}</annotation></semantics></math> and <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><msub id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p1.3.m3.1.1.2" xref="S3.SS1.SSS3.p1.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS3.p1.3.m3.1.1.3" xref="S3.SS1.SSS3.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><apply id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1.2">𝑣</ci><ci id="S3.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">v_{i}</annotation></semantics></math> are as described as in Equation <a href="#S3.E1" title="In III-A1 Concat Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="c_{i}=BatchNorm(q_{i}\odot v_{i})" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">c</mi><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2a" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.5" xref="S3.E3.m1.1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2b" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.6" xref="S3.E3.m1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2c" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.7" xref="S3.E3.m1.1.1.1.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2d" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.8" xref="S3.E3.m1.1.1.1.8.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2e" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.9" xref="S3.E3.m1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2f" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.10" xref="S3.E3.m1.1.1.1.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2g" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.1.1.1.11" xref="S3.E3.m1.1.1.1.11.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2h" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml">q</mi><mi id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">⊙</mo><msub id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">v</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑐</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝐵</ci><ci id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4">𝑎</ci><ci id="S3.E3.m1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.5">𝑡</ci><ci id="S3.E3.m1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.6">𝑐</ci><ci id="S3.E3.m1.1.1.1.7.cmml" xref="S3.E3.m1.1.1.1.7">ℎ</ci><ci id="S3.E3.m1.1.1.1.8.cmml" xref="S3.E3.m1.1.1.1.8">𝑁</ci><ci id="S3.E3.m1.1.1.1.9.cmml" xref="S3.E3.m1.1.1.1.9">𝑜</ci><ci id="S3.E3.m1.1.1.1.10.cmml" xref="S3.E3.m1.1.1.1.10">𝑟</ci><ci id="S3.E3.m1.1.1.1.11.cmml" xref="S3.E3.m1.1.1.1.11">𝑚</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">direct-product</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2">𝑞</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">𝑣</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">c_{i}=BatchNorm(q_{i}\odot v_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS4.5.1.1" class="ltx_text">III-A</span>4 </span><span id="S3.SS1.SSS4.6.2" class="ltx_text ltx_font_bold">Question Fusion</span>
</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.3" class="ltx_p">Question fusion uses a dual concatenation strategy.
The question embedding is concatenated before and after the visual features.
An embedding size of 1024 is used for the question embedding, with the final embedding size of 4096.
The main emphasis in this strategy is to provide more feature points for the question embedding.
Datasets such as the CLEVR family that are used to test reasoning contains longer questions compared to other datasets.
Therefore, limiting the question embedding to a single vector of size 1024 or 2048 can effect the emphasis of the question on the model.
Equation <a href="#S3.E4" title="In III-A4 Question Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is used to obtain the final embedding (<math id="S3.SS1.SSS4.p1.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS1.SSS4.p1.1.m1.1a"><msub id="S3.SS1.SSS4.p1.1.m1.1.1" xref="S3.SS1.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS4.p1.1.m1.1.1.2" xref="S3.SS1.SSS4.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS4.p1.1.m1.1.1.3" xref="S3.SS1.SSS4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.1.m1.1b"><apply id="S3.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.1.m1.1c">c_{i}</annotation></semantics></math>).
<math id="S3.SS1.SSS4.p1.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S3.SS1.SSS4.p1.2.m2.1a"><msub id="S3.SS1.SSS4.p1.2.m2.1.1" xref="S3.SS1.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS4.p1.2.m2.1.1.2" xref="S3.SS1.SSS4.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS4.p1.2.m2.1.1.3" xref="S3.SS1.SSS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.2.m2.1b"><apply id="S3.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.SS1.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.2.m2.1c">q_{i}</annotation></semantics></math> and <math id="S3.SS1.SSS4.p1.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS1.SSS4.p1.3.m3.1a"><msub id="S3.SS1.SSS4.p1.3.m3.1.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.1.1.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS4.p1.3.m3.1.1.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.3.m3.1b"><apply id="S3.SS1.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS4.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS4.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.2">𝑣</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.3.m3.1c">v_{i}</annotation></semantics></math> are as described as in Equation <a href="#S3.E1" title="In III-A1 Concat Fusion ‣ III-A Fusion Strategies ‣ III Methodology ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="c_{i}=BatchNorm([q_{i},v_{i},q_{i}])" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">c</mi><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.4" xref="S3.E4.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2a" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.5" xref="S3.E4.m1.1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2b" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.6" xref="S3.E4.m1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2c" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.7" xref="S3.E4.m1.1.1.1.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2d" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.8" xref="S3.E4.m1.1.1.1.8.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2e" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.9" xref="S3.E4.m1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2f" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.10" xref="S3.E4.m1.1.1.1.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2g" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mi id="S3.E4.m1.1.1.1.11" xref="S3.E4.m1.1.1.1.11.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2h" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.3.4" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml">[</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.3.5" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml">v</mi><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.1.1.3.6" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.3.2.cmml">q</mi><mi id="S3.E4.m1.1.1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.3.7" xref="S3.E4.m1.1.1.1.1.1.1.4.cmml">]</mo></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑐</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">𝐵</ci><ci id="S3.E4.m1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.4">𝑎</ci><ci id="S3.E4.m1.1.1.1.5.cmml" xref="S3.E4.m1.1.1.1.5">𝑡</ci><ci id="S3.E4.m1.1.1.1.6.cmml" xref="S3.E4.m1.1.1.1.6">𝑐</ci><ci id="S3.E4.m1.1.1.1.7.cmml" xref="S3.E4.m1.1.1.1.7">ℎ</ci><ci id="S3.E4.m1.1.1.1.8.cmml" xref="S3.E4.m1.1.1.1.8">𝑁</ci><ci id="S3.E4.m1.1.1.1.9.cmml" xref="S3.E4.m1.1.1.1.9">𝑜</ci><ci id="S3.E4.m1.1.1.1.10.cmml" xref="S3.E4.m1.1.1.1.10">𝑟</ci><ci id="S3.E4.m1.1.1.1.11.cmml" xref="S3.E4.m1.1.1.1.11">𝑚</ci><list id="S3.E4.m1.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2">𝑣</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3.2">𝑞</ci><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">c_{i}=BatchNorm([q_{i},v_{i},q_{i}])</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Aggregation Strategies</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This module is used to calculate the relationship between the question and bi-modal embeddings.
The bi-modal embeddings contains the relationship between the question and each regional visual feature.
Thereby this module aims to identify the relationships between the visual regions.
Higher performance on this module will lead to better results on questions that require multi object or localized information to answer.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.5.1.1" class="ltx_text">III-B</span>1 </span><span id="S3.SS2.SSS1.6.2" class="ltx_text ltx_font_bold">bi-GRU network</span>
</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The baseline aggregation strategy in the RAMEN model uses a bidirectional GRU based RNN to calculate the feature vector.
The main downside to this approach is that the model goes through each region sequentially in both directions.
Therefore, to obtain a relationship between two regions of the image, the model needs to pass through the other regions which can lead to information loss.
This approach is best used when all the regions are equally important for the question.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span><span id="S3.SS2.SSS2.6.2" class="ltx_text ltx_font_bold">Transformer network</span>
</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The transformer architecture is stronger in identifying the relationship between the multiple regions/vectors, because the network processes all the regions at once and not sequentially.
This is the reason why the transformer model performs well on the machine translation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
This allows it to capture relationships among regions better than RNNs.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">However, the positional encoder in the traditional transformer network masks out half of the regions.
This is to ensure the model is not able to see the next word in machine translation.
For the RAMEN model, this is not an issue.
So the mask is removed and the transformer is able to view all the regions.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">The output of the original transformer model is a set of decoders for the translated sentence.
But in this case, the main aim is to obtain a representation to be passed to the classification module.
Therefore, the decoder is replaced with a fully connected NN that returns a vector representation instead of the transformer decoder module.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">One of the main downside of the transformer network is the slow convergence.
Typically the transformer network might take upto sixty hours to fully convergence on translation tasks.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Dataset specification</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In the baseline paper, the accuracy of the CLEVR-CoGenTB dataset was obtained on a sub split of the test set.
But in this study, the accuracy is obtained on the complete test set.
Similarly, the original paper fine-tuned the model trained on the CLEVR-dataset with the CLEVR-Humans dataset to obtain the accuracy.
However, this study, trains the CLEVR-Humans dataset from scratch.
All other training and testing splits of the datasets are identical to the baseline paper.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Model specification</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Due to the changes in the datasets, the baseline accuracies are all re-calculated to ensure consistency.
All the model hyper-parameters are maintained as mentioned in the baseline paper.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The model with the transformer as the aggregation strategy is named as the TransformerNet and for baseline model with the bi-GRU network the name RAMEN model is used.
With each model, the four different fusion strategies are experimented for both the early and late fusion modules.
Therefore, in total the nine datasets are trained on eight versions of the models.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Evaluation metrics</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Three types of evaluations metrics are used in this study to compare the results between the datasets.
These are the same metrics used in the baseline study.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">10-choose-3</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Equation <a href="#S4.E5" title="In 10-choose-3 ‣ IV-C Evaluation metrics ‣ IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the evaluation metric used by VQAv1, VQAv2, CVQA and VQACPv2.
These datasets provide multiple answers for each question from multiple human annotators.
Thus using this metric reduces the inter-human variability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p2" class="ltx_para">
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.3" class="ltx_Math" alttext="Acc(answer)=min\{\frac{\text{\# of annotators provided answer}}{3},1\}" display="block"><semantics id="S4.E5.m1.3a"><mrow id="S4.E5.m1.3.3" xref="S4.E5.m1.3.3.cmml"><mrow id="S4.E5.m1.3.3.1" xref="S4.E5.m1.3.3.1.cmml"><mi id="S4.E5.m1.3.3.1.3" xref="S4.E5.m1.3.3.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.2" xref="S4.E5.m1.3.3.1.2.cmml">​</mo><mi id="S4.E5.m1.3.3.1.4" xref="S4.E5.m1.3.3.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.2a" xref="S4.E5.m1.3.3.1.2.cmml">​</mo><mi id="S4.E5.m1.3.3.1.5" xref="S4.E5.m1.3.3.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.2b" xref="S4.E5.m1.3.3.1.2.cmml">​</mo><mrow id="S4.E5.m1.3.3.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.3.3.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.cmml"><mi id="S4.E5.m1.3.3.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="S4.E5.m1.3.3.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1a" xref="S4.E5.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="S4.E5.m1.3.3.1.1.1.1.4" xref="S4.E5.m1.3.3.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1b" xref="S4.E5.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="S4.E5.m1.3.3.1.1.1.1.5" xref="S4.E5.m1.3.3.1.1.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1c" xref="S4.E5.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="S4.E5.m1.3.3.1.1.1.1.6" xref="S4.E5.m1.3.3.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1d" xref="S4.E5.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="S4.E5.m1.3.3.1.1.1.1.7" xref="S4.E5.m1.3.3.1.1.1.1.7.cmml">r</mi></mrow><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.3.3.2" xref="S4.E5.m1.3.3.2.cmml">=</mo><mrow id="S4.E5.m1.3.3.3" xref="S4.E5.m1.3.3.3.cmml"><mi id="S4.E5.m1.3.3.3.2" xref="S4.E5.m1.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.3.1" xref="S4.E5.m1.3.3.3.1.cmml">​</mo><mi id="S4.E5.m1.3.3.3.3" xref="S4.E5.m1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.3.1a" xref="S4.E5.m1.3.3.3.1.cmml">​</mo><mi id="S4.E5.m1.3.3.3.4" xref="S4.E5.m1.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.3.1b" xref="S4.E5.m1.3.3.3.1.cmml">​</mo><mrow id="S4.E5.m1.3.3.3.5.2" xref="S4.E5.m1.3.3.3.5.1.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.3.5.2.1" xref="S4.E5.m1.3.3.3.5.1.cmml">{</mo><mfrac id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mtext id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2a.cmml"># of annotators provided answer</mtext><mn id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml">3</mn></mfrac><mo id="S4.E5.m1.3.3.3.5.2.2" xref="S4.E5.m1.3.3.3.5.1.cmml">,</mo><mn id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml">1</mn><mo stretchy="false" id="S4.E5.m1.3.3.3.5.2.3" xref="S4.E5.m1.3.3.3.5.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.3b"><apply id="S4.E5.m1.3.3.cmml" xref="S4.E5.m1.3.3"><eq id="S4.E5.m1.3.3.2.cmml" xref="S4.E5.m1.3.3.2"></eq><apply id="S4.E5.m1.3.3.1.cmml" xref="S4.E5.m1.3.3.1"><times id="S4.E5.m1.3.3.1.2.cmml" xref="S4.E5.m1.3.3.1.2"></times><ci id="S4.E5.m1.3.3.1.3.cmml" xref="S4.E5.m1.3.3.1.3">𝐴</ci><ci id="S4.E5.m1.3.3.1.4.cmml" xref="S4.E5.m1.3.3.1.4">𝑐</ci><ci id="S4.E5.m1.3.3.1.5.cmml" xref="S4.E5.m1.3.3.1.5">𝑐</ci><apply id="S4.E5.m1.3.3.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1"><times id="S4.E5.m1.3.3.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1"></times><ci id="S4.E5.m1.3.3.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.2">𝑎</ci><ci id="S4.E5.m1.3.3.1.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.3">𝑛</ci><ci id="S4.E5.m1.3.3.1.1.1.1.4.cmml" xref="S4.E5.m1.3.3.1.1.1.1.4">𝑠</ci><ci id="S4.E5.m1.3.3.1.1.1.1.5.cmml" xref="S4.E5.m1.3.3.1.1.1.1.5">𝑤</ci><ci id="S4.E5.m1.3.3.1.1.1.1.6.cmml" xref="S4.E5.m1.3.3.1.1.1.1.6">𝑒</ci><ci id="S4.E5.m1.3.3.1.1.1.1.7.cmml" xref="S4.E5.m1.3.3.1.1.1.1.7">𝑟</ci></apply></apply><apply id="S4.E5.m1.3.3.3.cmml" xref="S4.E5.m1.3.3.3"><times id="S4.E5.m1.3.3.3.1.cmml" xref="S4.E5.m1.3.3.3.1"></times><ci id="S4.E5.m1.3.3.3.2.cmml" xref="S4.E5.m1.3.3.3.2">𝑚</ci><ci id="S4.E5.m1.3.3.3.3.cmml" xref="S4.E5.m1.3.3.3.3">𝑖</ci><ci id="S4.E5.m1.3.3.3.4.cmml" xref="S4.E5.m1.3.3.3.4">𝑛</ci><set id="S4.E5.m1.3.3.3.5.1.cmml" xref="S4.E5.m1.3.3.3.5.2"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><divide id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1"></divide><ci id="S4.E5.m1.1.1.2a.cmml" xref="S4.E5.m1.1.1.2"><mtext id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2"># of annotators provided answer</mtext></ci><cn type="integer" id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3">3</cn></apply><cn type="integer" id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.3c">Acc(answer)=min\{\frac{\text{\# of annotators provided answer}}{3},1\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simple Accuracy</h5>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">CLEVR, CLEVR-Humans, CLEVR-CoGenT-A and CLEVR-CoGenT-B uses the simple accuracy shown in Equation <a href="#S4.E6" title="In Simple Accuracy ‣ IV-C Evaluation metrics ‣ IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> as the evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.1" class="ltx_Math" alttext="Acc(answer)=\frac{\text{\# correct answer}}{\text{\# questions}}" display="block"><semantics id="S4.E6.m1.1a"><mrow id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml"><mi id="S4.E6.m1.1.1.1.3" xref="S4.E6.m1.1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.2" xref="S4.E6.m1.1.1.1.2.cmml">​</mo><mi id="S4.E6.m1.1.1.1.4" xref="S4.E6.m1.1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.2a" xref="S4.E6.m1.1.1.1.2.cmml">​</mo><mi id="S4.E6.m1.1.1.1.5" xref="S4.E6.m1.1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.2b" xref="S4.E6.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E6.m1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.E6.m1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.1a" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.E6.m1.1.1.1.1.1.1.4" xref="S4.E6.m1.1.1.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.1b" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.E6.m1.1.1.1.1.1.1.5" xref="S4.E6.m1.1.1.1.1.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.1c" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.E6.m1.1.1.1.1.1.1.6" xref="S4.E6.m1.1.1.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.1d" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.E6.m1.1.1.1.1.1.1.7" xref="S4.E6.m1.1.1.1.1.1.1.7.cmml">r</mi></mrow><mo stretchy="false" id="S4.E6.m1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.1.1.2" xref="S4.E6.m1.1.1.2.cmml">=</mo><mfrac id="S4.E6.m1.1.1.3" xref="S4.E6.m1.1.1.3.cmml"><mtext id="S4.E6.m1.1.1.3.2" xref="S4.E6.m1.1.1.3.2a.cmml"># correct answer</mtext><mtext id="S4.E6.m1.1.1.3.3" xref="S4.E6.m1.1.1.3.3a.cmml"># questions</mtext></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.1b"><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"><eq id="S4.E6.m1.1.1.2.cmml" xref="S4.E6.m1.1.1.2"></eq><apply id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1"><times id="S4.E6.m1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.2"></times><ci id="S4.E6.m1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.3">𝐴</ci><ci id="S4.E6.m1.1.1.1.4.cmml" xref="S4.E6.m1.1.1.1.4">𝑐</ci><ci id="S4.E6.m1.1.1.1.5.cmml" xref="S4.E6.m1.1.1.1.5">𝑐</ci><apply id="S4.E6.m1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1"><times id="S4.E6.m1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1"></times><ci id="S4.E6.m1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2">𝑎</ci><ci id="S4.E6.m1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.3">𝑛</ci><ci id="S4.E6.m1.1.1.1.1.1.1.4.cmml" xref="S4.E6.m1.1.1.1.1.1.1.4">𝑠</ci><ci id="S4.E6.m1.1.1.1.1.1.1.5.cmml" xref="S4.E6.m1.1.1.1.1.1.1.5">𝑤</ci><ci id="S4.E6.m1.1.1.1.1.1.1.6.cmml" xref="S4.E6.m1.1.1.1.1.1.1.6">𝑒</ci><ci id="S4.E6.m1.1.1.1.1.1.1.7.cmml" xref="S4.E6.m1.1.1.1.1.1.1.7">𝑟</ci></apply></apply><apply id="S4.E6.m1.1.1.3.cmml" xref="S4.E6.m1.1.1.3"><divide id="S4.E6.m1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.3"></divide><ci id="S4.E6.m1.1.1.3.2a.cmml" xref="S4.E6.m1.1.1.3.2"><mtext id="S4.E6.m1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.3.2"># correct answer</mtext></ci><ci id="S4.E6.m1.1.1.3.3a.cmml" xref="S4.E6.m1.1.1.3.3"><mtext id="S4.E6.m1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.3.3"># questions</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.1c">Acc(answer)=\frac{\text{\# correct answer}}{\text{\# questions}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mean-per-type</h5>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">The TDIUC dataset uses the mean-per-type evaluation metric as shown in Equation <a href="#S4.E7" title="In Mean-per-type ‣ IV-C Evaluation metrics ‣ IV Experiment ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
This ensures that the model is able to perform well on each category, even though the number of test instances of each category are different.</p>
</div>
<div id="S4.SS3.SSS0.Px3.p2" class="ltx_para">
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.2" class="ltx_Math" alttext="Acc(answer)=\frac{\sum{\{\frac{\text{\# correct answer per type}}{\text{\# of questions per type}}}\}}{\text{\# of types}}" display="block"><semantics id="S4.E7.m1.2a"><mrow id="S4.E7.m1.2.2" xref="S4.E7.m1.2.2.cmml"><mrow id="S4.E7.m1.2.2.1" xref="S4.E7.m1.2.2.1.cmml"><mi id="S4.E7.m1.2.2.1.3" xref="S4.E7.m1.2.2.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.2" xref="S4.E7.m1.2.2.1.2.cmml">​</mo><mi id="S4.E7.m1.2.2.1.4" xref="S4.E7.m1.2.2.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.2a" xref="S4.E7.m1.2.2.1.2.cmml">​</mo><mi id="S4.E7.m1.2.2.1.5" xref="S4.E7.m1.2.2.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.2b" xref="S4.E7.m1.2.2.1.2.cmml">​</mo><mrow id="S4.E7.m1.2.2.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.2.2.1.1.1.2" xref="S4.E7.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.2.2.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.cmml"><mi id="S4.E7.m1.2.2.1.1.1.1.2" xref="S4.E7.m1.2.2.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.1.1" xref="S4.E7.m1.2.2.1.1.1.1.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.1.1a" xref="S4.E7.m1.2.2.1.1.1.1.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.1.1.4" xref="S4.E7.m1.2.2.1.1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.1.1b" xref="S4.E7.m1.2.2.1.1.1.1.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.1.1.5" xref="S4.E7.m1.2.2.1.1.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.1.1c" xref="S4.E7.m1.2.2.1.1.1.1.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.1.1.6" xref="S4.E7.m1.2.2.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.1.1.1.1.1d" xref="S4.E7.m1.2.2.1.1.1.1.1.cmml">​</mo><mi id="S4.E7.m1.2.2.1.1.1.1.7" xref="S4.E7.m1.2.2.1.1.1.1.7.cmml">r</mi></mrow><mo stretchy="false" id="S4.E7.m1.2.2.1.1.1.3" xref="S4.E7.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.2.2.2" xref="S4.E7.m1.2.2.2.cmml">=</mo><mfrac id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.cmml"><mo rspace="0em" id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.2.cmml">∑</mo><mrow id="S4.E7.m1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.E7.m1.1.1.1.3.2.1" xref="S4.E7.m1.1.1.1.3.1.cmml">{</mo><mfrac id="S4.E7.m1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><mtext id="S4.E7.m1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.2a.cmml"># correct answer per type</mtext><mtext id="S4.E7.m1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.3a.cmml"># of questions per type</mtext></mfrac><mo stretchy="false" id="S4.E7.m1.1.1.1.3.2.2" xref="S4.E7.m1.1.1.1.3.1.cmml">}</mo></mrow></mrow><mtext id="S4.E7.m1.1.1.3" xref="S4.E7.m1.1.1.3a.cmml"># of types</mtext></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.2b"><apply id="S4.E7.m1.2.2.cmml" xref="S4.E7.m1.2.2"><eq id="S4.E7.m1.2.2.2.cmml" xref="S4.E7.m1.2.2.2"></eq><apply id="S4.E7.m1.2.2.1.cmml" xref="S4.E7.m1.2.2.1"><times id="S4.E7.m1.2.2.1.2.cmml" xref="S4.E7.m1.2.2.1.2"></times><ci id="S4.E7.m1.2.2.1.3.cmml" xref="S4.E7.m1.2.2.1.3">𝐴</ci><ci id="S4.E7.m1.2.2.1.4.cmml" xref="S4.E7.m1.2.2.1.4">𝑐</ci><ci id="S4.E7.m1.2.2.1.5.cmml" xref="S4.E7.m1.2.2.1.5">𝑐</ci><apply id="S4.E7.m1.2.2.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1"><times id="S4.E7.m1.2.2.1.1.1.1.1.cmml" xref="S4.E7.m1.2.2.1.1.1.1.1"></times><ci id="S4.E7.m1.2.2.1.1.1.1.2.cmml" xref="S4.E7.m1.2.2.1.1.1.1.2">𝑎</ci><ci id="S4.E7.m1.2.2.1.1.1.1.3.cmml" xref="S4.E7.m1.2.2.1.1.1.1.3">𝑛</ci><ci id="S4.E7.m1.2.2.1.1.1.1.4.cmml" xref="S4.E7.m1.2.2.1.1.1.1.4">𝑠</ci><ci id="S4.E7.m1.2.2.1.1.1.1.5.cmml" xref="S4.E7.m1.2.2.1.1.1.1.5">𝑤</ci><ci id="S4.E7.m1.2.2.1.1.1.1.6.cmml" xref="S4.E7.m1.2.2.1.1.1.1.6">𝑒</ci><ci id="S4.E7.m1.2.2.1.1.1.1.7.cmml" xref="S4.E7.m1.2.2.1.1.1.1.7">𝑟</ci></apply></apply><apply id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"><divide id="S4.E7.m1.1.1.2.cmml" xref="S4.E7.m1.1.1"></divide><apply id="S4.E7.m1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><sum id="S4.E7.m1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.2"></sum><set id="S4.E7.m1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.3.2"><apply id="S4.E7.m1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1"><divide id="S4.E7.m1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1"></divide><ci id="S4.E7.m1.1.1.1.1.2a.cmml" xref="S4.E7.m1.1.1.1.1.2"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.2"># correct answer per type</mtext></ci><ci id="S4.E7.m1.1.1.1.1.3a.cmml" xref="S4.E7.m1.1.1.1.1.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.3"># of questions per type</mtext></ci></apply></set></apply><ci id="S4.E7.m1.1.1.3a.cmml" xref="S4.E7.m1.1.1.3"><mtext id="S4.E7.m1.1.1.3.cmml" xref="S4.E7.m1.1.1.3"># of types</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.2c">Acc(answer)=\frac{\sum{\{\frac{\text{\# correct answer per type}}{\text{\# of questions per type}}}\}}{\text{\# of types}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Training specifications</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">All the experiments were done on a PC running <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">Ubuntu 18.04.1 LTS</em> with an <em id="S4.SS4.p1.1.2" class="ltx_emph ltx_font_italic">Intel® Xeon(R) W-2145 CPU @ 3.70GHz</em> with 16 logical cores and 64GB RAM.
A single <em id="S4.SS4.p1.1.3" class="ltx_emph ltx_font_italic">Quadro P5000</em> GPU was used to perform the NN training with a 7200RPM <em id="S4.SS4.p1.1.4" class="ltx_emph ltx_font_italic">Seagate</em> hard drive to store the data.
The gradual learning rate warm up is used similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The mini-batch size of 256 is used for all the experiments.
The models are trained until 25 epochs with some exceptions in the TransformerNet experiments; mainly due to the slower convergence rate.
As shown in Appendix <a href="#A1" title="Appendix A Total training time ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, an average training time of 46 minutes per epoch was elapsed for all experiments.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results &amp; Discussion</span>
</h2>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">Results from all eight model with the nine VQA datasets.</span></figcaption>
<table id="S5.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.4.1.1" class="ltx_tr">
<td id="S5.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S5.T2.4.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T2.4.1.1.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="4"><span id="S5.T2.4.1.1.2.1" class="ltx_text ltx_font_bold">Ramen</span></td>
<td id="S5.T2.4.1.1.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="4"><span id="S5.T2.4.1.1.3.1" class="ltx_text ltx_font_bold">TransformerNet</span></td>
</tr>
<tr id="S5.T2.4.2.2" class="ltx_tr">
<td id="S5.T2.4.2.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S5.T2.4.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.4.2.2.1.1.1" class="ltx_tr">
<td id="S5.T2.4.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
</tr>
<tr id="S5.T2.4.2.2.1.1.2" class="ltx_tr">
<td id="S5.T2.4.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T2.4.2.2.1.1.2.1.1" class="ltx_text ltx_font_bold">Concat</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.4.2.2.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.2.1" class="ltx_text ltx_font_bold">Additive</span></td>
<td id="S5.T2.4.2.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.3.1" class="ltx_text ltx_font_bold">Multiplicative</span></td>
<td id="S5.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.4.1" class="ltx_text ltx_font_bold">Question</span></td>
<td id="S5.T2.4.2.2.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.5.1" class="ltx_text ltx_font_bold">Concat</span></td>
<td id="S5.T2.4.2.2.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.6.1" class="ltx_text ltx_font_bold">Additive</span></td>
<td id="S5.T2.4.2.2.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.7.1" class="ltx_text ltx_font_bold">Multiplicative</span></td>
<td id="S5.T2.4.2.2.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.2.2.8.1" class="ltx_text ltx_font_bold">Question</span></td>
</tr>
<tr id="S5.T2.4.3.3" class="ltx_tr">
<td id="S5.T2.4.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.3.3.1.1" class="ltx_text ltx_font_bold">VQAv1</span></td>
<td id="S5.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.30</td>
<td id="S5.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.21</td>
<td id="S5.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.3.3.4.1" class="ltx_text ltx_font_bold">65.54</span></td>
<td id="S5.T2.4.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">64.76</td>
<td id="S5.T2.4.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">63.32</td>
<td id="S5.T2.4.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.88</td>
<td id="S5.T2.4.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.91</td>
<td id="S5.T2.4.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.08</td>
</tr>
<tr id="S5.T2.4.4.4" class="ltx_tr">
<td id="S5.T2.4.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.4.4.1.1" class="ltx_text ltx_font_bold">VQAv2</span></td>
<td id="S5.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.16</td>
<td id="S5.T2.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.64</td>
<td id="S5.T2.4.4.4.4" class="ltx_td ltx_align_center" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.4.4.4.1" class="ltx_text ltx_font_bold">65.28</span></td>
<td id="S5.T2.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">65.07</td>
<td id="S5.T2.4.4.4.6" class="ltx_td ltx_align_center" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">64.06</td>
<td id="S5.T2.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.79</td>
<td id="S5.T2.4.4.4.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.32</td>
<td id="S5.T2.4.4.4.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.14</td>
</tr>
<tr id="S5.T2.4.5.5" class="ltx_tr">
<td id="S5.T2.4.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.5.5.1.1" class="ltx_text ltx_font_bold">VQACPv2</span></td>
<td id="S5.T2.4.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.5.5.2.1" class="ltx_text ltx_font_bold">37.61</span></td>
<td id="S5.T2.4.5.5.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.73</td>
<td id="S5.T2.4.5.5.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.28</td>
<td id="S5.T2.4.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">37.03</td>
<td id="S5.T2.4.5.5.6" class="ltx_td ltx_align_center" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">37.47</td>
<td id="S5.T2.4.5.5.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.60</td>
<td id="S5.T2.4.5.5.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.60</td>
<td id="S5.T2.4.5.5.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.89</td>
</tr>
<tr id="S5.T2.4.6.6" class="ltx_tr">
<td id="S5.T2.4.6.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.6.6.1.1" class="ltx_text ltx_font_bold">CVQA</span></td>
<td id="S5.T2.4.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">56.98</td>
<td id="S5.T2.4.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.82</td>
<td id="S5.T2.4.6.6.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.58</td>
<td id="S5.T2.4.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">56.81</td>
<td id="S5.T2.4.6.6.6" class="ltx_td ltx_align_center" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.6.6.6.1" class="ltx_text ltx_font_bold">57.74</span></td>
<td id="S5.T2.4.6.6.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.49</td>
<td id="S5.T2.4.6.6.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.20</td>
<td id="S5.T2.4.6.6.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.47</td>
</tr>
<tr id="S5.T2.4.7.7" class="ltx_tr">
<td id="S5.T2.4.7.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.7.7.1.1" class="ltx_text ltx_font_bold">TDIUC</span></td>
<td id="S5.T2.4.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.7.7.2.1" class="ltx_text ltx_font_bold">66.48</span></td>
<td id="S5.T2.4.7.7.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.90</td>
<td id="S5.T2.4.7.7.4" class="ltx_td ltx_align_center" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">65.69</td>
<td id="S5.T2.4.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.81</td>
<td id="S5.T2.4.7.7.6" class="ltx_td ltx_align_center" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">65.47</td>
<td id="S5.T2.4.7.7.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.03</td>
<td id="S5.T2.4.7.7.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.34</td>
<td id="S5.T2.4.7.7.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.90</td>
</tr>
<tr id="S5.T2.4.8.8" class="ltx_tr">
<td id="S5.T2.4.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.8.8.1.1" class="ltx_text ltx_font_bold">CLEVR</span></td>
<td id="S5.T2.4.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">96.52</td>
<td id="S5.T2.4.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">96.26</td>
<td id="S5.T2.4.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.8.8.4.1" class="ltx_text ltx_font_bold">96.72</span></td>
<td id="S5.T2.4.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.06</td>
<td id="S5.T2.4.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">95.79</td>
<td id="S5.T2.4.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.52</td>
<td id="S5.T2.4.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.31</td>
<td id="S5.T2.4.8.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.07</td>
</tr>
<tr id="S5.T2.4.9.9" class="ltx_tr">
<td id="S5.T2.4.9.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.9.9.1.1" class="ltx_text ltx_font_bold">CLEVR-Humans</span></td>
<td id="S5.T2.4.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.57</td>
<td id="S5.T2.4.9.9.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.21</td>
<td id="S5.T2.4.9.9.4" class="ltx_td ltx_align_center" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">46.46</td>
<td id="S5.T2.4.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.9.9.5.1" class="ltx_text ltx_font_bold">48.63</span></td>
<td id="S5.T2.4.9.9.6" class="ltx_td ltx_align_center" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">46.49</td>
<td id="S5.T2.4.9.9.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.45</td>
<td id="S5.T2.4.9.9.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.07</td>
<td id="S5.T2.4.9.9.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.99</td>
</tr>
<tr id="S5.T2.4.10.10" class="ltx_tr">
<td id="S5.T2.4.10.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.10.10.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTA</span></td>
<td id="S5.T2.4.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.59</td>
<td id="S5.T2.4.10.10.3" class="ltx_td ltx_align_center" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">96.84</td>
<td id="S5.T2.4.10.10.4" class="ltx_td ltx_align_center" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">96.63</td>
<td id="S5.T2.4.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.10.10.5.1" class="ltx_text ltx_font_bold">96.90</span></td>
<td id="S5.T2.4.10.10.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.43</td>
<td id="S5.T2.4.10.10.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.26</td>
<td id="S5.T2.4.10.10.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.07</td>
<td id="S5.T2.4.10.10.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.24</td>
</tr>
<tr id="S5.T2.4.11.11" class="ltx_tr">
<td id="S5.T2.4.11.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.11.11.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTB</span></td>
<td id="S5.T2.4.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.27</td>
<td id="S5.T2.4.11.11.3" class="ltx_td ltx_align_center" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">89.42</td>
<td id="S5.T2.4.11.11.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">86.22</td>
<td id="S5.T2.4.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">88.74</td>
<td id="S5.T2.4.11.11.6" class="ltx_td ltx_align_center" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.11.11.6.1" class="ltx_text ltx_font_bold">89.68</span></td>
<td id="S5.T2.4.11.11.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.80</td>
<td id="S5.T2.4.11.11.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.18</td>
<td id="S5.T2.4.11.11.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.19</td>
</tr>
<tr id="S5.T2.4.12.12" class="ltx_tr">
<td id="S5.T2.4.12.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.12.12.1.1" class="ltx_text ltx_font_bold">Mean</span></td>
<td id="S5.T2.4.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.05</td>
<td id="S5.T2.4.12.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.45</td>
<td id="S5.T2.4.12.12.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#D4E3FE;padding-top:1.5pt;padding-bottom:1.5pt;">68.38</td>
<td id="S5.T2.4.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#74A7FE;padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T2.4.12.12.5.1" class="ltx_text ltx_font_bold">68.76</span></td>
<td id="S5.T2.4.12.12.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#A8C6FE;padding-top:1.5pt;padding-bottom:1.5pt;">68.49</td>
<td id="S5.T2.4.12.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">51.53</td>
<td id="S5.T2.4.12.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.89</td>
<td id="S5.T2.4.12.12.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.66</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x6.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">The difference in mean compared to the overall number of improved datasets.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.02370/assets/x7.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Improvement of the datasets for each model compared to the baseline.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Summaries of the results.</span></figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section focuses on the key observations from the experiments and the effect of changing the aggregation and fusion strategies.
Table <a href="#S5.T2" title="TABLE II ‣ V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show the scores obtained by each experiment.
The top three models are highlighted with darker colours indicating better performance.
Appendix <a href="#A2" title="Appendix B Training results ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> contains the full results table with the training scores and number of epochs.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">When comparing the baseline results (<em id="S5.p2.1.1" class="ltx_emph ltx_font_italic">Ramen-Concat</em>) with the results from the RAMEN study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, it is evident that there exist a minor difference in the scores.
The training was done with the same hyper-parameters as stated in the baseline paper even though there exists a deficiency in the scores.
However, since all the experiments are done with the fixed set of hyper-parameters, the scores in this study are consistent.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Overall observations</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">First, considering the model with the highest mean score across all the datasets, the <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Ramen-Question</em> model has a score of 68.76.
With a percentage difference of about 1%, it is evident that the improvement of using different fusion and aggregation strategies is minor.
However, many other patterns in the results can be observed which can help improve the performance of future models.
It is also noted that <em id="S5.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Ramen-Multiplicative</em> and <em id="S5.SS1.p1.1.3" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> was also able to improve the performance by about 0.5% and 0.65% respectively.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Next, it is evident that TransformerNet model did not perform at all on most of the datasets, with differences in accuracies of more than 25% on the CLEVR datasets.
This issue is addressed in Section <a href="#S5.SS4" title="V-D Aggregation Strategies ‣ V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span></span></a>.
However, the <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> model performed well on most of the datasets.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">When considering the model with the highest number of top scores on the nine datasets, the <em id="S5.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Ramen-Multiplicative</em> model has achieved the highest score on three of the main datasets.
Therefore, it is evident that this model can perform well on both natural and synthetic types of VQA datasets.
However, the model is not able to generalize to question and attribute biases well.
This is identified from the CVQA and VQACPv2 datasets, due to the lower performance on them.
When comparing the performance of the model between CLEVR-CoGenTA and CLEVR-CoGenTB datasets, it is found to be evident that the model is seeing a dip in performance.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Next, comparing the performance of the model based on the scores in the top-3 rank of the experiments, it is evident that <em id="S5.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Ramen-Question</em> has the overall best performance.
In seven out of the nine dataset, this model was able to achieve the top three results.
This indicates the ability of the model to generalize across multiple datasets.
Also the <em id="S5.SS1.p4.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> model was able to achieve top three in seven out of nine datasets even though it only have the highest scores for the CVQA dataset.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">With reference to Figure <a href="#S5.F3.sf1" title="In Figure 3 ‣ V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, the <em id="S5.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Ramen-Additive</em> model was able to improve on three datasets.
However, due to the lower scores on the other datasets, especially CLEVR-Humans, the model is not able to achieve a positive mean.
This indicates that the model is not able to perform well on free-form questions.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">In the following section the performance of the models on specific datasets are analysed.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Dataset observations</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Figure <a href="#S5.F3.sf2" title="In Figure 3 ‣ V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> demonstrates the datasets impact on the improvement of the models.
It only showcase the experiments with at least one improvement, hence <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Additive</em>, <em id="S5.SS2.p1.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Multiplicative</em> and <em id="S5.SS2.p1.1.3" class="ltx_emph ltx_font_italic">TransformerNet-Question</em> are ignored.
All of the remaining models were able to improve on the VQAv2 dataset.
This suggests that all the fusion strategies and the transformer aggregation strategy can improve the performance of localizing and detecting items.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">However, only the Multiplicative and Question fusion were able to improve the VQAv1 dataset.
Since, VQAv1 has inherent bias in the questions, this implies that the these two fusion strategies are more prone to over-fitting on the VQA based datasets.
This is also made clear by the poor performance on the CVQA and VQACPv2 datasets.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">However, the most performance gain is by the <em id="S5.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Ramen-Question</em> on the CLEVR-Humans dataset.
It points to an indication that the pre and post concatenation of the question embedding has an improvement on the free form questions.
This is also true for the models performance on VQAv1 and VQAv2 datasets.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Compared to the baseline, the <em id="S5.SS2.p4.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> is the only model to have an improvement in the score for CVQA.
This highlights that the transformer aggregation module provides the ability for the module to generalize.
This is further emphasized since the CLEVR-CoGenTB and VQAv2 datasets also show improvement in the scores.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Fusion Strategies</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Overall it is clear that the different fusion strategies favor various datasets due to the unique characteristics in them.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS1.5.1.1" class="ltx_text">V-C</span>1 </span>Concat Fusion</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">The baseline concatenation fusion approach was able to get the highest score for VQACPv2 and TDIUC dataset using the RAMEN model and CLEVR-CoGenTB using the TransformerNet model.
The VQACPv2 dataset aims to test the answer bias in the models.
Therefore, concatenation based fusion is able to generalize well in terms of answer biases as both <em id="S5.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">Ramen-concat</em> and <em id="S5.SS3.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> was able to achieve high scores.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">Next, the TDIUC MPT metric measures the performance of the model on multiple question types.
Considering that both the <em id="S5.SS3.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">Ramen-Concat</em> and <em id="S5.SS3.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> have high scores on the TDIUC dataset, it is clear that concatenation based fusion allows for much more question type based generalization.</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p">The <em id="S5.SS3.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> model is trained on CLEVR-CoGenTA and tested on CLEVR-CoGenTB.
Therefore, the model will not learn any details about the complementary attributes in the dataset.
This indicates that the model is able to generalization well onto unseen combinations of attributes.
However, the relationship between concatenation based fusion and attribute based generalization cannot be established.
This is due to the lower score in the <em id="S5.SS3.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">Ramen-Concat</em> model, which implies that the performance gain is due to the transformer aggregation strategy.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS2.5.1.1" class="ltx_text">V-C</span>2 </span>Additive Fusion</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">The additive fusion strategy was not able to get the highest score for any of the datasets.
However, the <em id="S5.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">Ramen-Additive</em> model was able to improve on the CLEVR-CoGenTA and CLEVR-CoGenB datasets, which point towards the model’s ability to generalize to new concept compositions.
The main issue with the additive fusion strategy is the information loss and the lower emphasis on the vector operation.</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS3.5.1.1" class="ltx_text">V-C</span>3 </span>Multiplicative Fusion</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">The multiplicative fusion strategy achieved the highest score for VQAv1, VQAv2 and CLEVR datasets.
As mentioned in Section <a href="#S5.SS1" title="V-A Overall observations ‣ V Results &amp; Discussion ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>, the model suffers with generalization.
However, the emphasis on the vector operation is higher compared to additive fusion, therefore the most important details are passed on through the fusion module.</p>
</div>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS4.5.1.1" class="ltx_text">V-C</span>4 </span>Question Fusion</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p id="S5.SS3.SSS4.p1.1" class="ltx_p">The question fusion obtained the highest score for CLEVR-Humans and CLEVR-CoGenTA.
It also achieved high scores for all the CLEVR datasets.
This is an indication that for reasoning type datasets with higher significance on the question, the double concatenation of the question has an effect.</p>
</div>
<div id="S5.SS3.SSS4.p2" class="ltx_para">
<p id="S5.SS3.SSS4.p2.1" class="ltx_p">The ability for the model to generalize on new concept compositions can be observed due to the performance on CLEVR-CoGenTA and CLEVR-CoGenTB.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Aggregation Strategies</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The transformer module as the aggregation strategy does not perform well.
With only decent performance using the concat fusion strategy, it may not be suitable to be part of the RAMEN model.
Many issues where faced when training the transformer module such as slow convergence and longer training time.
However, the slow convergence of the transformer module remains a significant drawback.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">This is evident when considering the number of epochs used to train <em id="S5.SS4.p2.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> vs <em id="S5.SS4.p2.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Question</em> on the VQAv2 dataset.
<em id="S5.SS4.p2.1.3" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em> was trained for 50 epochs where the highest score was obtained at epoch 46, whereas <em id="S5.SS4.p2.1.4" class="ltx_emph ltx_font_italic">TransformerNet-additive</em> was only trained for 25 epochs.
Appendix <a href="#A2" title="Appendix B Training results ‣ Improved RAMEN: Towards Domain Generalization for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> reports all the training details used for each dataset.
Therefore, training the TransformerNet models for longer can provide better scores.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Additionally, due to the longer training times and the time constraint, hyper-parameter tunning was not an option.
With an average time of 58 minutes per epoch for the <em id="S5.SS4.p3.1.1" class="ltx_emph ltx_font_italic">TransformerNet-Question</em> model, the training would take more than 48 hours on a single GPU.
However, as observed by <em id="S5.SS4.p3.1.2" class="ltx_emph ltx_font_italic">TransformerNet-Concat</em>, in an ideal scenario the model is able to convergence.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The proposed improvements of this study resulted in minor gains in performance of about 1%.
However, in-terms of domain generalization the <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">Ramen-Multiplicative</em>, <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">Ramen-Question</em> and <em id="S6.p1.1.3" class="ltx_emph ltx_font_italic">TransformerNetwork-Concat</em> models were able to achieve improvements in five out of the nine datasets.
Also, <em id="S6.p1.1.4" class="ltx_emph ltx_font_italic">Ramen-Question</em> and <em id="S6.p1.1.5" class="ltx_emph ltx_font_italic">TransformerNetwork-Concat</em> models were able achieve the top three scores in seven out of the nine datasets.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Analyzing the fusion strategies, provided insights to the different characteristics that effect domain generalization.
For example, Question fusion performed well on reasoning questions due to the increase in the number of question embedding data points, which resulted in an increase in the amount of information passed into the aggregation module.
This knowledge can be used to improve the performance of model and domain generalization.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">When studying the effects of the Transformer module as the aggregation strategy, it is clear that selecting the correct hyper-parameters and providing the necessary amount of training time to converge are two main requirements when training the transformer module.
This is one of the main limitations of this study.
The time constraint and high computational cost of the experiments led to some of the experiments not converging to the higher scores.
Therefore, since VQA datasets tend to be larger in size, more powerful hardware is needed to perform a broader hyper-parameter search to optimize the models performance.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Focusing only on the vector operation based fusion strategies was also another limitation of the study.
Bilinear Pooling techniques for fusion has proven to be effective when it comes to identifying relationships.
However, the added computational cost of pooling on top of the transformer module will result in poor performance if training is not done till convergence.
Nonetheless, this is a probable path that can be explored in the future.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Another limitation of the study is that the RAMEN architecture itself may be causing the bottle neck when trying to improve the generalization.
Since both improvements were done to sub-modules of the multi-modal section of the RAMEN model, inherent limitations may exist in the architecture.
Using the knowledge gained from the analysis of the fusion and aggregation module, a new architecture can be developed to well suite domain generalization.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">In conclusion, this study paved the path to understanding the characteristics required for domain generalization as well as improving the performance of the RAMEN model.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgment</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">A special thanks to Robik Shrestha from the Rochester Institute of Technology who was one of the main researchers of the RAMEN study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for the guidance and support in providing the links to the datasets and setting up the baseline for this study.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. Shrestha, K. Kafle, and C. Kanan, “Answer them all! toward universal
visual question answering models,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
10 472–10 481.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: Visual question answering,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision</em>, vol. 2015 Inter, pp.
2425–2433, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. Zhang, R. Cao, and S. Wu, “Information fusion in visual question answering:
A survey,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, vol. 52, pp. 268–280, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, A. Agrawal, D. Summers-Stay, D. Batra, and D. Parikh,
“Making the V in VQA Matter: Elevating the Role of Image Understanding in
Visual Question Answering,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer
Vision</em>, vol. 127, no. 4, pp. 398–414, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Kafle and C. Kanan, “Visual question answering: Datasets, algorithms, and
future challenges,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, vol.
163, pp. 3–20, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Agrawal, A. Kembhavi, D. Batra, and D. Parikh, “C-VQA: A Compositional
Split of the Visual Question Answering (VQA) v1.0 Dataset,” pp. 1–10,
2017. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1704.08243" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.08243</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, “Don’t just assume; look
and answer: Overcoming priors for visual question answering,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2018, pp. 4971–4980.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Johnson, L. Fei-Fei, B. Hariharan, C. L. Zitnick, L. Van Der Maaten, and
R. Girshick, “CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings - 30th IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2017</em>, vol. 2017-Janua, pp.
1988–1997, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Johnson, J. Hoffman, L. Fei-fei, C. L. Zitnick, and R. Girshick,
“Inferring and Executing Programs for Visual Reasoning,”
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, pp. 2989–2998, 2017. [Online]. Available:
<a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Johnson_Inferring_and_Executing_ICCV_2017_paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://openaccess.thecvf.com/content_ICCV_2017/papers/Johnson_Inferring_and_Executing_ICCV_2017_paper.pdf</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz, “A multi-world approach to question answering
about real-world scenes based on uncertain input,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems 27</em>, Z. Ghahramani, M. Welling,
C. Cortes, N. Lawrence, and K. Weinberger, Eds.   Curran Associates, Inc., 2014, pp. 1682–1690.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei, “Visual
genome: Connecting language and vision using crowdsourced dense image
annotations,” 2016. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/1602.07332" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1602.07332</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, “Learning visual
question answering by bootstrapping hard attention,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science (including subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics)</em>, vol. 11210 LNCS, pp.
3–20, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. A. Hasan, Y. Ling, O. Farri, J. Liu, H. Müller, and M. Lungren,
“Overview of ImageCLEF 2018 medical domain Visual Question Answering
task,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CEUR Workshop Proceedings</em>, vol. 2125, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Shah, A. Mishra, N. Yadati, and P. P. Talukdar, “KVQA: Knowledge-Aware
Visual Question Answering,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on
Artificial Intelligence</em>, vol. 33, pp. 8876–8884, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Kafle, B. Price, S. Cohen, and C. Kanan, “Supplementary Materials for DVQA
: Understanding Data Visualizations via Question Answering,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, pp. 5648–5656, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona,
D. Ramanan, C. L. Zitnick, and P. Dollár, “Microsoft coco: Common objects
in context,” 2014.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Xu and K. Saenko, “Ask, Attend and Answer: Exploring Question-Guided
Spatial Attention for Visual Question Answering,” ser. Lecture Notes in
Computer Science, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds.   Cham: Springer International Publishing, 2016,
vol. 9911, pp. 451–466. [Online]. Available:
<a target="_blank" href="http://link.springer.com/10.1007/978-3-319-46448-0http://link.springer.com/10.1007/978-3-319-46478-7http://link.springer.com/10.1007/978-3-319-46478-7_28" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://link.springer.com/10.1007/978-3-319-46448-0http://link.springer.com/10.1007/978-3-319-46478-7http://link.springer.com/10.1007/978-3-319-46478-7_28</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, 2016, pp. 289–297.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
I. Ilievski, S. Yan, and J. Feng, “A Focused Dynamic Attention Model for
Visual Question Answering,” 2016. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1604.01485" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1604.01485</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Noh and B. Han, “Training Recurrent Answering Units with Joint Loss
Minimization for VQA,” 2016. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1606.03647" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1606.03647</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks
for image question answering,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition</em>, vol.
2016-Decem, no. 1, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Nam, J.-W. Ha, and J. Kim, “Dual Attention Networks for Multimodal
Reasoning and Matching,” pp. 299–307, nov 2016. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1611.00471" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1611.00471</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
“Multimodal compact bilinear pooling for visual question answering and
visual grounding,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01847</em>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
“Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
Answering,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition</em>, pp. 6077–6086, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Jiang, V. Natarajan, X. Chen, M. Rohrbach, D. Batra, and D. Parikh,
“Pythia v0.1: the Winning Entry to the VQA Challenge 2018,” pp. 4–6,
2018. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1807.09956" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1807.09956</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. H. Kim, J. Jun, and B. T. Zhang, “Bilinear attention networks,”
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 2018-Decem,
no. NeurIPS, pp. 1564–1574, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
B. Liu, Z. Huang, Z. Zeng, Z. Chen, and J. Fu, “Learning Rich Image Region
Representation for Visual Question Answering,” pp. 14–16, 2019. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1910.13077" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1910.13077</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, “Deep modular co-attention
networks for visual question answering,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Computer Society Conference on Computer Vision and Pattern Recognition</em>, vol.
2019-June, pp. 6274–6283, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
W. Norcliffe-brown and S. Parisot, “Learning Conditioned Graph Structures for
Interpretable Visual Question Answering arXiv : 1806 . 07243v6 [ cs . CV ] 1
Nov 2018,” no. Nips, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. A. Hudson and C. D. Manning, “Compositional attention networks for machine
reasoning,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">6th International Conference on Learning Representations,
ICLR 2018 - Conference Track Proceedings</em>, pp. 1–20, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia, “Abc-cnn: An
attention based convolutional neural network for visual question answering,”
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.05960</em>, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual and
textual question answering,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International conference on machine
learning</em>, 2016, pp. 2397–2406.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M. Ren, R. Kiros, and R. Zemel, “Exploring models and data for image question
answering,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
2015, pp. 2953–2961.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “Mutan: Multimodal tucker
fusion for visual question answering,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2017, pp. 2612–2620.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z. Su, C. Zhu, Y. Dong, D. Cai, Y. Chen, and J. Li, “Learning visual knowledge
memory networks for visual question answering,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition</em>, 2018, pp.
7736–7745.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. Singh, V. Ying, and A. Nutkiewicz, “Attention on attention: Architectures
for visual question answering (vqa),” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1803.07724</em>, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Song, P. Zeng, L. Gao, and H. T. Shen, “From pixels to objects: Cubic
visual attention for visual question answering.” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, 2018, pp.
906–912.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
P. Lu, H. Li, W. Zhang, J. Wang, and X. Wang, “Co-attending free-form regions
and detections with multi-modal multiplicative feature embedding for visual
question answering,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.06794</em>, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
L. Ma, Z. Lu, and H. Li, “Learning to answer questions from image using
convolutional neural network,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1506.00333</em>, 2015.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word
representation,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP)</em>, 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated
recurrent neural networks on sequence modeling,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1412.3555</em>, 2014.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2017, pp.
5998–6008.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
G. Li, L. Zhu, P. Liu, and Y. Yang, “Entangled transformer for image
captioning,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</em>, 2019, pp. 8928–8937.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C. Sur, “Self-segregating and coordinated-segregating transformer for focused
deep multi-modular network for visual question answering,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2006.14264</em>, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Kant, D. Batra, P. Anderson, A. Schwing, D. Parikh, J. Lu, and H. Agrawal,
“Spatially aware multimodal transformers for textvqa,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2007.12146</em>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert:
Pre-training of generic visual-linguistic representations,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1908.08530</em>, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
from transformers,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. U. Khan, A. Mazaheri, N. d. V. Lobo, and M. Shah, “Mmft-bert: Multimodal
fusion transformer with bert encodings for visual question answering,”
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.14095</em>, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
D. A. Hudson and C. D. Manning, “Gqa: A new dataset for real-world visual
reasoning and compositional question answering,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019, pp.
6700–6709.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1502.03167</em>, 2015.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Total training time</h2>

<figure id="A1.tab1" class="ltx_table">
<table id="A1.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.tab1.1.1.1" class="ltx_tr">
<th id="A1.tab1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="3"><span id="A1.tab1.1.1.1.1.1" class="ltx_text"><span id="A1.tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset </span></span></th>
<th id="A1.tab1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="8"><span id="A1.tab1.1.1.1.2.1" class="ltx_text ltx_font_bold">Training Time per Epoch (minutes)</span></th>
</tr>
<tr id="A1.tab1.1.2.2" class="ltx_tr">
<th id="A1.tab1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="4"><span id="A1.tab1.1.2.2.1.1" class="ltx_text ltx_font_bold">RAMEN</span></th>
<th id="A1.tab1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="4"><span id="A1.tab1.1.2.2.2.1" class="ltx_text ltx_font_bold">TransformerNet</span></th>
</tr>
<tr id="A1.tab1.1.3.3" class="ltx_tr">
<td id="A1.tab1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="A1.tab1.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.tab1.1.3.3.1.1.1" class="ltx_tr">
<td id="A1.tab1.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.3.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Baseline</span></td>
</tr>
<tr id="A1.tab1.1.3.3.1.1.2" class="ltx_tr">
<td id="A1.tab1.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A1.tab1.1.3.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Concat</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="A1.tab1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Additive</td>
<td id="A1.tab1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Multiplicative</td>
<td id="A1.tab1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Question Fusion</td>
<td id="A1.tab1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Concatenation</td>
<td id="A1.tab1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Additive</td>
<td id="A1.tab1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Multiplicative</td>
<td id="A1.tab1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Question Fusion</td>
</tr>
<tr id="A1.tab1.1.4.4" class="ltx_tr">
<td id="A1.tab1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.4.4.1.1" class="ltx_text ltx_font_bold">VQAv1</span></td>
<td id="A1.tab1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">17.00</td>
<td id="A1.tab1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.51</td>
<td id="A1.tab1.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.67</td>
<td id="A1.tab1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.52</td>
<td id="A1.tab1.1.4.4.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">32.75</td>
<td id="A1.tab1.1.4.4.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">29.04</td>
<td id="A1.tab1.1.4.4.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">29.10</td>
<td id="A1.tab1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.12</td>
</tr>
<tr id="A1.tab1.1.5.5" class="ltx_tr">
<td id="A1.tab1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.5.5.1.1" class="ltx_text ltx_font_bold">VQAv2</span></td>
<td id="A1.tab1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.15</td>
<td id="A1.tab1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.80</td>
<td id="A1.tab1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.94</td>
<td id="A1.tab1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">76.59</td>
<td id="A1.tab1.1.5.5.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.45</td>
<td id="A1.tab1.1.5.5.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">77.88</td>
<td id="A1.tab1.1.5.5.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">78.30</td>
<td id="A1.tab1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">110.40</td>
</tr>
<tr id="A1.tab1.1.6.6" class="ltx_tr">
<td id="A1.tab1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.6.6.1.1" class="ltx_text ltx_font_bold">VQACP2</span></td>
<td id="A1.tab1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">16.29</td>
<td id="A1.tab1.1.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.27</td>
<td id="A1.tab1.1.6.6.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">32.74</td>
<td id="A1.tab1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.08</td>
<td id="A1.tab1.1.6.6.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">25.00</td>
<td id="A1.tab1.1.6.6.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.39</td>
<td id="A1.tab1.1.6.6.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.67</td>
<td id="A1.tab1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">77.67</td>
</tr>
<tr id="A1.tab1.1.7.7" class="ltx_tr">
<td id="A1.tab1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.7.7.1.1" class="ltx_text ltx_font_bold">CVQA</span></td>
<td id="A1.tab1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">11.09</td>
<td id="A1.tab1.1.7.7.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">14.87</td>
<td id="A1.tab1.1.7.7.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">15.04</td>
<td id="A1.tab1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.31</td>
<td id="A1.tab1.1.7.7.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">21.67</td>
<td id="A1.tab1.1.7.7.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.94</td>
<td id="A1.tab1.1.7.7.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.11</td>
<td id="A1.tab1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.85</td>
</tr>
<tr id="A1.tab1.1.8.8" class="ltx_tr">
<td id="A1.tab1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.8.8.1.1" class="ltx_text ltx_font_bold">TDIUC</span></td>
<td id="A1.tab1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.79</td>
<td id="A1.tab1.1.8.8.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">85.32</td>
<td id="A1.tab1.1.8.8.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">86.13</td>
<td id="A1.tab1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">129.87</td>
<td id="A1.tab1.1.8.8.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">91.65</td>
<td id="A1.tab1.1.8.8.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.73</td>
<td id="A1.tab1.1.8.8.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">90.00</td>
<td id="A1.tab1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">183.57</td>
</tr>
<tr id="A1.tab1.1.9.9" class="ltx_tr">
<td id="A1.tab1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.9.9.1.1" class="ltx_text ltx_font_bold">CLEVR</span></td>
<td id="A1.tab1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.51</td>
<td id="A1.tab1.1.9.9.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.31</td>
<td id="A1.tab1.1.9.9.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">51.49</td>
<td id="A1.tab1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">47.26</td>
<td id="A1.tab1.1.9.9.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.18</td>
<td id="A1.tab1.1.9.9.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.28</td>
<td id="A1.tab1.1.9.9.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.82</td>
<td id="A1.tab1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.57</td>
</tr>
<tr id="A1.tab1.1.10.10" class="ltx_tr">
<td id="A1.tab1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.10.10.1.1" class="ltx_text ltx_font_bold">CLEVR-Human</span></td>
<td id="A1.tab1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.80</td>
<td id="A1.tab1.1.10.10.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.01</td>
<td id="A1.tab1.1.10.10.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.39</td>
<td id="A1.tab1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.28</td>
<td id="A1.tab1.1.10.10.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.82</td>
<td id="A1.tab1.1.10.10.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.52</td>
<td id="A1.tab1.1.10.10.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">92.38</td>
<td id="A1.tab1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.19</td>
</tr>
<tr id="A1.tab1.1.11.11" class="ltx_tr">
<td id="A1.tab1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.11.11.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTA</span></td>
<td id="A1.tab1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">16.40</td>
<td id="A1.tab1.1.11.11.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">47.47</td>
<td id="A1.tab1.1.11.11.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.28</td>
<td id="A1.tab1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.95</td>
<td id="A1.tab1.1.11.11.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">28.11</td>
<td id="A1.tab1.1.11.11.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.52</td>
<td id="A1.tab1.1.11.11.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.74</td>
<td id="A1.tab1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.49</td>
</tr>
<tr id="A1.tab1.1.12.12" class="ltx_tr">
<td id="A1.tab1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A1.tab1.1.12.12.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTB</span></td>
<td id="A1.tab1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="A1.tab1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
</tr>
<tr id="A1.tab1.1.13.13" class="ltx_tr">
<td id="A1.tab1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average</td>
<td id="A1.tab1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20.75</td>
<td id="A1.tab1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.82</td>
<td id="A1.tab1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.33</td>
<td id="A1.tab1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.98</td>
<td id="A1.tab1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">35.83</td>
<td id="A1.tab1.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.54</td>
<td id="A1.tab1.1.13.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.26</td>
<td id="A1.tab1.1.13.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">74.48</td>
</tr>
<tr id="A1.tab1.1.14.14" class="ltx_tr">
<td id="A1.tab1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Total Average Time</td>
<td id="A1.tab1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="8"><span id="A1.tab1.1.14.14.2.1" class="ltx_text ltx_font_bold">46.00</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A2" class="ltx_appendix ltx_pruned_first">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Training results</h2>

<div id="A2.p2" class="ltx_para ltx_noindent">
<div id="A2.p2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:144.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.3pt,45.0pt) scale(0.61571,0.61571) ;">
<p id="A2.p2.1.1" class="ltx_p"><span id="A2.p2.1.1.1" class="ltx_text">
<span id="A2.p2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:704.3pt;height:235pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A2.p2.1.1.1.1.1" class="ltx_p"><span id="A2.p2.1.1.1.1.1.1" class="ltx_text">
<span id="A2.p2.1.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="A2.p2.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text"><span id="A2.p2.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold"> Dataset</span></span></span>
<span id="A2.p2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Ramen Concat</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">TransformerNet Concat</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Ramen Additive</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">TransformerNet Additive</span></span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">Test Score</span></span></span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p2.1.1.1.1.1.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">VQAv1</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.44</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.3</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8388.85</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.32</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">84.65</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.21</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.33</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</span>
<span id="A2.p2.1.1.1.1.1.1.1.4.4.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.88</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">VQAv2</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">84.95</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.16</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">46</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">82.56</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">46</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.06</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">82.43</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.64</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.99</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p2.1.1.1.1.1.1.1.5.5.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.79</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">VQACP2</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">80.81</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.61</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">80.92</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.47</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">86.53</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.73</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">93</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.93</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">93</span>
<span id="A2.p2.1.1.1.1.1.1.1.6.6.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.6</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">CVQA</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.36</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.98</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">73.61</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.74</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.23</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.82</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">79.66</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="A2.p2.1.1.1.1.1.1.1.7.7.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.49</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">TDIUC</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">9</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">66.48</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.47</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.9</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">7</span>
<span id="A2.p2.1.1.1.1.1.1.1.8.8.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.03</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">CLEVR</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.75</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.52</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.94</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">95.79</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.63</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.26</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.5</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p2.1.1.1.1.1.1.1.9.9.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.52</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">CLEVR-Human</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">31</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">31</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">44.57</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">77</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.2</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">77</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.49</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">10</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">87.67</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">10</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.21</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">83</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.96</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">83</span>
<span id="A2.p2.1.1.1.1.1.1.1.10.10.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.45</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.11.11.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTA</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.76</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.59</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">83</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">98.86</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">83</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.43</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">24</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.82</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">24</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.84</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">32</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.48</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">32</span>
<span id="A2.p2.1.1.1.1.1.1.1.11.11.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.26</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p2.1.1.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTB</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.27</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">83</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.68</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.42</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">32</span>
<span id="A2.p2.1.1.1.1.1.1.1.12.12.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.8</span></span>
<span id="A2.p2.1.1.1.1.1.1.1.13.13" class="ltx_tr">
<span id="A2.p2.1.1.1.1.1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average</span>
<span id="A2.p2.1.1.1.1.1.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.05</span>
<span id="A2.p2.1.1.1.1.1.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.49</span>
<span id="A2.p2.1.1.1.1.1.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.45</span>
<span id="A2.p2.1.1.1.1.1.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;">51.53</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
<div id="A2.p4" class="ltx_para ltx_noindent">
<div id="A2.p4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:146.6pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-130.6pt,44.0pt) scale(0.62405,0.62405) ;">
<p id="A2.p4.1.1" class="ltx_p"><span id="A2.p4.1.1.1" class="ltx_text">
<span id="A2.p4.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:694.8pt;height:235pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A2.p4.1.1.1.1.1" class="ltx_p"><span id="A2.p4.1.1.1.1.1.1" class="ltx_text">
<span id="A2.p4.1.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="A2.p4.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Ramen Question Fusion</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">TransformerNet Question Fusion</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Ramen Multiplicative</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">TransformerNet Multiplicative</span></span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Test Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Training Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">Test Score</span></span></span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Epoch</span>
<span id="A2.p4.1.1.1.1.1.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Score</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">VQAv1</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">76.9</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.76</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.61</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.08</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">70.32</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.54</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.83</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.4.4.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.91</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">VQAv2</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">79.53</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.07</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.31</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.14</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.98</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.28</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.1</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.5.5.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.32</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">VQACP2</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">30</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">87.09</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">30</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.03</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">26</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.75</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">26</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">26.89</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">71.76</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.28</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">93</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.93</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">93</span>
<span id="A2.p4.1.1.1.1.1.1.1.6.6.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">27.6</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">CVQA</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">75.38</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.81</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.16</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.47</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">90.41</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.58</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">17</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.2</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">17</span>
<span id="A2.p4.1.1.1.1.1.1.1.7.7.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.2</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">TDIUC</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">11</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.81</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">11</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.9</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.69</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</span>
<span id="A2.p4.1.1.1.1.1.1.1.8.8.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.34</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">CLEVR</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">97.83</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.06</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.84</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.07</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.86</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.72</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.24</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.9.9.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.31</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">CLEVR-Human</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">17</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.76</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">17</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.63</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.36</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.99</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">100</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">46.46</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.64</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.10.10.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.07</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.11.11.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTA</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.68</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.9</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">23</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.35</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">23</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.24</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">99.81</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">96.63</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.42</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</span>
<span id="A2.p4.1.1.1.1.1.1.1.11.11.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.07</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="A2.p4.1.1.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">CLEVR-CoGenTB</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.74</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">23</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.19</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">86.22</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">16</span>
<span id="A2.p4.1.1.1.1.1.1.1.12.12.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.18</span></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13" class="ltx_tr">
<span id="A2.p4.1.1.1.1.1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average</span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.76</span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.6" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.7" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.8" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.66</span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.10" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.11" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.12" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.38</span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.14" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.15" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.16" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></span>
<span id="A2.p4.1.1.1.1.1.1.1.13.13.17" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.89</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.02369" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.02370" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.02370">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.02370" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.02371" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 09:57:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
