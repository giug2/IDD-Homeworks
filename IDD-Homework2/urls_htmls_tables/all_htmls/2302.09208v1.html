<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.09208] Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering</title><meta property="og:description" content="In this paper, a bridge member damage cause estimation framework is proposed by calculating the image position using Structure from Motion (SfM) and acquiring its information via Visual Question Answering (VQA). For th‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.09208">

<!--Generated on Fri Mar  1 00:53:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Bridge Maintenance,  Structure from Motion,  Visual Question Answering">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Graduate School of Frontier Sciences, The University of Tokyo</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Graduate School of Engineering, The University of Tokyo
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Graduate School of Science and Engineering, Saitama University
</span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Graduate School of Information Sciences, Tohoku University
</span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>RIKEN Center for AIP 
<br class="ltx_break"><span id="id5.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>yamane.tatsuro.20@dois.k.u-tokyo.ac.jp</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">
Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tatsuro Yamane
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pang-jo Chun
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ji Dang
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Takayuki Okatani
</span><span class="ltx_author_notes">4455</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this paper, a bridge member damage cause estimation framework is proposed by calculating the image position using Structure from Motion (SfM) and acquiring its information via Visual Question Answering (VQA). For this, a VQA model was developed that uses bridge images for dataset creation and outputs the damage or member name and its existence based on the images and questions. In the developed model, the correct answer rate for questions requiring the member‚Äôs name and the damage‚Äôs name were 67.4% and 68.9%, respectively. The correct answer rate for questions requiring a yes/no answer was 99.1%. Based on the developed model, a damage cause estimation method was proposed. In the proposed method, the damage causes are narrowed down by inputting new questions to the VQA model, which are determined based on the surrounding images obtained via SfM and the results of the VQA model. Subsequently, the proposed method was then applied to an actual bridge and shown to be capable of determining damage and estimating its cause. The proposed method could be used to prevent damage causes from being overlooked, and practitioners could determine inspection focus areas, which could contribute to the improvement of maintenance techniques. In the future, it is expected to contribute to infrastructure diagnosis automation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Bridge Maintenance, Structure from Motion, Visual Question Answering
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Aging infrastructure such as bridges and tunnels can cause serious accidents if left unattended. According to the ASCE‚Äôs 2021 Infrastructure Report Card¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, there are more than 617,000 bridges in the United States, 42% of which are more than 50 years old, and 7.5%, or 46,154 bridges, are structurally deficient or in ‚Äòpoor‚Äô condition. Owning to this pressing issue of aging infrastructure, there has been increased research interest in proper infrastructure safety assessment¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In addition, regular condition assessment and monitoring are important to properly assess infrastructure safety¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. However, currently, infrastructure condition assessment is performed visually, which requires extensive effort and cost¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Various studies have been conducted to address this problem and realize efficient maintenance management. In recent years, research on damage detection from images using machine learning techniques has been conducted. Cha et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> proposed a crack detection method from images using convolutional neural networks (CNNs), a type of deep learning algorithm. Yang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> conducted a study on pixel-level crack detection using fully convolutional networks. Chun et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> developed a pixel-level crack detection method for concrete walls using a Light Gradient-Boosting Machine. Wang &amp; Hu¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, Zhang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and Chun et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposed a deep-learning-based crack detection method for pavements. Xu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> proposed a damage detection framework based on restricted Boltzmann machines to detect cracks in box girders. Besides crack and fissure detection studies, various studies have been conducted on other types of damage detection. Liang¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> conducted pixel-level concrete delamination detection research. Fondevik et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Rahman et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed a pixel-level corrosion damage detection method from images.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Research is also actively being conducted to record the detected damage in a three-dimensional (3D) bridge model. Liu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed a method to record cracks detected from images in a 3D pier model. In addition, Yamane et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> proposed a damage location recording method using an object detection model in a 3D bridge model. Other studies have been conducted on methods to record damage and non-damage areas detected from images in a 3D bridge model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">As mentioned, many studies have been conducted on damage detection from images. Damage detection from images using the aforementioned methods is expected to reduce labour during inspections and contribute to a more efficient infrastructure maintenance. These studies contribute to damage detection automation; however, little attention is paid to how to utilize the information obtained from such images. Hence, it would be of great help to infrastructure maintenance engineers if machines could not only detect damage from images, but also estimate its cause. As an example of estimating the cause of damage, one can imagine a case where corrosion is detected, and it is estimated that it is caused by a leak from a nearby drainage pipe. As mentioned above, if the cause of damage can be estimated, it is believed that not only automated inspection, but also automated diagnosis and labour savings will be possible. Furthermore, damage cause estimation automation is useful in and of itself. For example, for inexperienced engineers, this may lead to careful confirmation of the damage cause presented to the machine, effectively preventing overlooking it. Moreover, based on suggestions from the machine, it would also contribute to improving the maintenance skills of novice engineers, as they could learn where to focus in each case.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For damage cause estimation from images, it is necessary to obtain information not only on the damage but also on related members. If a simple image classification model is used to obtain information, it is possible that the combined information in the image may be mistaken. For example, in the case of an image of the main girder and slab of a steel girder bridge, if the main girder is corroded and the slab is cracked, the model that determines the member‚Äôs name may judge that it is the main girder, while the model that determines the damage name may judge that it is cracked. In this case, the individual classification model would be correct; however, it would not correctly determine which damage is occurring in which member.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">An approach to obtain output that takes into account the relationship between member and damage names has been studied using an image captioning model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In that study, a model is developed to output sentences regarding the damage and the member in which the damage occurs from bridge images, making it possible to obtain information that includes the relationships between words. However, there are many possible patterns in the text that can be output based on an image. Thus, for example, if multiple damages or members are present in an image simultaneously, there is a possibility that something other than the desired information will be output.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To address this problem and obtain information from images as needed, it is necessary to use a model that can input images and questions into the model and produce output in accordance with the input content. This task is called Visual Question Answering (VQA) and has been actively studied in the field of computer vision. Meanwhile, to estimate the cause of the damage in the image, it is also necessary to acquire the surrounding conditions. However, the damage and the information related to its cause are not necessarily captured in a single image. For example, it is quite possible that an image of a particular corrosion case does not show a leaky drainage pipe that could be the cause of the damage. Therefore, when estimating the cause, it is necessary to analyse not only the image of interest but also multiple images in the surrounding area and make a decision based on the results.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Accordingly, we propose a damage cause estimation method by calculating the camera coordinates using Structure from Motion (SfM), a technique to calculate the coordinates at which images were taken, and extracting information from the images via VQA using multiple images taken around the damage based on the calculated camera coordinates. The remainder of this paper is organized as follows. First, the acquisition method for images around the image of interest using SfM is explained. Then, the training process of the VQA model using bridge images is explained. Afterward, a damage cause estimation method based on the output of the developed VQA model is explained. Field test results are presented next after applying the proposed method to an actual bridge structure. Finally, the conclusions of this study are summarized.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology framework of this paper</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The proposed method consists of four parts: acquisition of images surrounding the image of interest (Sec.¬†<a href="#S3" title="3 Acquisition of images surrounding the image of interest ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), acquisition of damage information from bridge images (Sec.¬†<a href="#S4" title="4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), damage cause estimation based on the acquired damage information (Sec.¬†<a href="#S5" title="5 Damage cause estimation based on the acquired damage information ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), and field testing (Sec.¬†<a href="#S6" title="6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The framework is presented in Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Methodology framework of this paper ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2302.09208/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="182" height="198" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text" style="font-size:90%;">Proposed framework methodology and field test
</span></figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Specifically, in the acquisition of images surrounding the image of interest, we explain how to obtain the area of the bridge captured by each image based on the SfM-estimated camera position coordinates. In the acquisition of damage information from bridge images, we explain how to create a dataset for VQA using bridge images, develop a VQA model based on the created dataset, and verify the model accuracy. For damage cause estimation based on the acquired damage information, we explain how to estimate the cause of damage using the developed VQA model. Finally, we describe the field test results and discuss the application of the proposed method to an actual bridge. Each of these topics is described in detail in the following sections.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Acquisition of images surrounding the image of interest</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Three-dimensional scene and surface reconstruction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In damage cause estimation, it is desirable not to use a single image alone; surrounding images are also needed. For this, the coordinates of the camera that captured each image must be calculated to determine the area of the bridge from which each image was taken. SfM is a method that can calculate the shooting coordinates of each image based on multiple images. This is a technique that detects common feature points between images from each image and calculates the camera position based on epipolar geometry. SfM is a well-known method for image-based 3D scene and surface reconstruction, a key challenge in the field of computer vision¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The dense point cloud data of an object can be constructed based on the positions of each camera determined via SfM and stereo matching results using multiple images. The dense point cloud data can also be used to construct a polygon mesh model with a triangular mesh.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In this study, SfM is used to calculate the shooting coordinates of each image and to determine the images surrounding the image of interest. SfM is a commonly used technology, and there are various commercially available software packages for SfM-based 3D modelling. Nevertheless, the proposed framework is applicable regardless of the type of SfM software.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Shooting area and surrounding images</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In selecting images surrounding the image of interest, it is not appropriate to simply use images whose camera coordinates are close to those of the camera that took the image of interest. This is because if the cameras are facing in different directions, it is possible that they are capturing images of areas that are far from the damage. Even if the camera coordinates are far, it is possible that the camera is capturing a close area from a different direction. Therefore, in this study, we calculate the intersection of the bridge mesh model created based on the SfM results and the camera direction vectors; this intersection is used as the coordinates of the point where the image was taken. Based on these coordinates, the surrounding images are selected.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The mesh model created based on the SfM results consists of a set of triangular meshes. Therefore, the mesh model intersection points can be calculated by computing the triangular meshes that intersect the direction vectors of the camera whose coordinates have already been identified, as shown in Fig.¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.2 Shooting area and surrounding images ‚Ä£ 3 Acquisition of images surrounding the image of interest ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Note that the camera direction vector and the triangular mesh may intersect multiple times. Therefore, the intersection point closest to the camera coordinates is used as the shooting point coordinates.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2302.09208/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Intersection of the camera direction vector and the triangular mesh
</span></figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Algorithms for determining the intersection between a triangular mesh and a ray represent a simple geometric problem and are not discussed here. For example, the M√∂ller Trumbore intersection algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> can be used for intersection determination.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">To determine the surrounding images, we assume a ball centred on the intersection coordinates between the camera that captured the image of interest (Camera A) and the model, as shown in Fig.¬†<a href="#S3.F3" title="Figure 3 ‚Ä£ 3.2 Shooting area and surrounding images ‚Ä£ 3 Acquisition of images surrounding the image of interest ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Cameras (Cameras B) whose intersection coordinates are inside the ball are treated as the images surrounding the image of interest. The other cameras (Cameras C) are considered to have little relevance to the image of interest and are not used in the subsequent analysis. In this study, for simplicity, the radius of this ball is assumed to be 1 m for determining the surrounding images. Although further discussion is needed on an appropriate setting for this value, the focus of this study is on the methodology itself, and we chose a value that is easy to calculate.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2302.09208/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">Image of interest and its surrounding images
</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Acquisition of damage information from bridge images</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Visual Question Answering</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For damage cause estimation from images, it is necessary to obtain information on both damage and member as needed. VQA is a machine learning task that, given an image and a question related to the image, outputs an appropriate answer. The questions are input as sentences, and the answers are expected to be a wide range of content, such as words, yes/no, and numbers. The first dataset for the VQA task is the DAtaset for QUestion Answering on Real-world images (DAQUAR)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, which is a dataset limited to indoor scenes with a total of 1449 images. Various other datasets have also been published, including Visual7W¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and Visual Madlibs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. In addition, datasets for VQA generally have multiple questions for a single image; Visual Question Answering (VQA)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is a widely used dataset for VQA, and an improved version of this dataset, Visual Question Answering v2.0 (VQA v2.0)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> with 265016 total images, is also available. Note that these datasets are constructed for general objects; therefore, it is difficult to utilize them directly in civil engineering fields such as bridge diagnostics.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As VQA is a task that has recently attracted attention with the development of deep leaning technology, deep-learning-based models have been mainly proposed. Most deep-learning-based models for VQA use CNNs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for processing images, recurrent neural networks (RNNs)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> for processing time series data, and Word2Vec¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> for word embeddings. In practice, not simple RNNs but their derivatives long short-term memory (LSTM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and gated recurrent units (GRUs)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are often used.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">There are several models that do not use RNNs. Zhou et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed a VQA model called iBOWIMG, which uses trained GoogLeNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> for image feature extraction and inputs questions as simple one-hot vectors to the model for feature extraction for each word. Ma et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposed a VQA model composed entirely of CNNs. Their proposed model consists of an Image CNN for processing images, a Sentence CNN for processing vectorized questions, and a multimodal convolution layer for combining the output from both CNNs.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Ren et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> proposed a VQA model composed of a CNN and LSTM. In their model, the CNN-processed features on the image are first input to the LSTM, and then the words that make up the question are sequentially input to the LSTM. Malinowski et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> also proposed a VQA model consisting of a CNN and LSTM. Their model generates outputs by inputting CNN-processed features on images at the same time as words are sequentially inputted into the LSTM. Noh et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposed DPPnet, a model consisting of a CNN and GRUs. This model outputs responses by integrating the obtained features by sequentially inputting each word into the GRUs and the features obtained from the image by the CNN into a dynamic parameter layer.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">When using neural networks to handle sentences, simple RNN-based methods are limited in that accuracy deteriorates as the sentence length increases. Attention mechanism¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> has been proposed to focus on important parts by probabilistically weighting the input data. Attention mechanism has been extensively used in tasks such as sentence generation from images and machine translation. In recent years, attention mechanism has been widely used in image processing to improve accuracy by focusing on important parts of the input image. Many studies have been conducted in VQA to improve accuracy by focusing on important parts of input images and questions using the attention mechanism.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Shih et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> presented a VQA model using attention mechanism. In their model, features from the input sentence are selectively combined with features from the image in a region selection layer to determine which parts of the image to focus on. Zhu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> proposed a VQA model using attention mechanism and LSTM. In their model, after a word is input to the LSTM, attention to the image is iteratively computed for each word. This attention region is determined by both the current word input to the model and the previous attention-weighted image. Lu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> proposed a co-attention model that performs attention not only to the image but also to the question text. In this model, an image representation is used to determine attention for the question text, and the question text representation is used to determine attention for the image.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">As the co-attention model can consider the importance of each word compared to image-only attention, several co-attention-based VQA models have been proposed¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. However, as attention is determined independently for each modality (image and question), simple co-attention models do not consider the dense interaction between each word in the question and each image region. For example, attention to each word in a question is determined from the entire input image and may focus on areas that are not inherently important. Therefore, a dense co-attention model has been proposed to establish the full interaction between each word in a question and each image region¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Compared to the co-attention model with coarse interaction, the dense co-attention model is more accurate. Another method that more accurately considers the interaction of each word with each image region is called bottom-up attention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Bottom-up attention uses a pre-trained object detection model to detect each object in the image and uses the results to calculate attention, which can consider the dense interaction of each word with each image region. Yu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> proposed the bottom-up-attention-based Deep Modular Co-Attention Network (MCAN). MCAN consists of multiple modular co-attention (MCA) layers consisting of self- and guided-attention units. The self- and guided-attention units are constructed using the multi-head attention proposed by Vaswani et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Models with bottom-up attention are known to be significantly more accurate than non-bottom-up attention models.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">For example, as shown in the MCAN flowchart in Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Visual Question Answering ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, if the question is ‚ÄòWhat kind of damage has occurred to the bearing?‚Äô it is expected that the damage can be more accurately determined by focusing on the bearing, rather than on a location in the image that is unrelated to the content of the question. Therefore, in this study, a VQA model for bridge images was constructed using MCAN. It is worth noting that, as new VQA models are proposed, if a more accurate model is available, it can be easily adopted in the proposed framework.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2302.09208/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="98" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Deep modular co-attention network (MCAN) flowchart¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training models using bridge images</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Training dataset</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Various datasets for VQA are publicly available; however, these datasets are for general objects. Accordingly, even if a model is trained using such datasets, it is not possible to perform specialized VQA such as assessing bridge damage. Therefore, in this study, a new dataset for VQA was created using actual bridge images and a model was developed.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">An example of the created dataset is shown in Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2.1 Training dataset ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. For each image, questions were created asking what member is present, what damage is present, what damage occurs on what member, and so on.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2302.09208/assets/x5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="186" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">Example of the created dataset
</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Most of the images in the created dataset were extracted from actual bridge inspection reports at the Kanto Regional Development Bureau of the Ministry of Land, Infrastructure, Transport, and Tourism of Japan. However, bridge inspection reports are mainly composed of images taken of damaged areas; therefore, very few of these images do not show damage. By contrast, when unmanned aerial vehicles (UAVs) are used to capture images of actual bridges, it is expected that many undamaged images will be acquired. Therefore, in this study, in addition to the images extracted from bridge inspection reports, images extracted from videos of bridges taken by UAVs were added to the dataset. The number of images used to create the dataset was 442,035, consisting of 421,956 bridge inspection report images and 20,079 UAV-taken images.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p">When creating a VQA dataset, a human is usually free to create questions based on images, and the human creates appropriate answers based on the questions and images. However, when creating a VQA dataset from bridge images, specialized knowledge is required, and the cost of creating a VQA dataset is high if the same method for general datasets is used. Therefore, in this study, for images extracted from bridge inspection reports, questions and answers were mechanically generated based on the names of members and damages described in the inspection reports along with the images. For the UAV-extracted images, the names of the members in the images and the presence or absence of damage in the members were determined manually, and the questions and answers were generated mechanically based on the results.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p">Some damages in the inspection report were described as a single damage (e.g., sinking/displacement/slanting of bridge abutments). Therefore, such damages were treated as a single damage in the created dataset as well. In general VQA datasets, multiple questions are prepared for each image. Therefore, in this study, we created a dataset with multiple questions for each image. The total number of questions created was 3,875,010, with an average of approximately 8.8 questions per image. The dataset consists of questions that can be answered by name of the member and damage, and ‚Äòyes/no‚Äô as shown in Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2.1 Training dataset ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. There are 38 types of members and 22 types of damages. Fig.¬†<a href="#S4.F6" title="Figure 6 ‚Ä£ 4.2.1 Training dataset ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the occurrence of each answer in the dataset.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2302.09208/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="249" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">Breakdown of the answers in the dataset
</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Model training</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.4" class="ltx_p">In this study, MCAN was trained on the created training data. The created dataset was divided into 310,259 images for the training data, 88,114 images for the validation data, and 43,662 images for the test data. Images of different bridges were used for each dataset. Adam¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> was used as the model optimization method; the Adam hyperparameters were <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><msub id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">Œ≤</mi><mn id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">ùõΩ</ci><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\beta_{1}</annotation></semantics></math> = 0.9 and <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><msub id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">Œ≤</mi><mn id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">ùõΩ</ci><cn type="integer" id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">\beta_{2}</annotation></semantics></math> = 0.98, following Yu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. The learning rate was set to min(2.5<math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\textit{te}^{-5}" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><msup id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.p1.3.m3.1.1.2a.cmml">te</mtext><mrow id="S4.SS2.SSS2.p1.3.m3.1.1.3" xref="S4.SS2.SSS2.p1.3.m3.1.1.3.cmml"><mo id="S4.SS2.SSS2.p1.3.m3.1.1.3a" xref="S4.SS2.SSS2.p1.3.m3.1.1.3.cmml">‚àí</mo><mn id="S4.SS2.SSS2.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS2.p1.3.m3.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><apply id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.3.m3.1.1.2a.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.2">te</mtext></ci><apply id="S4.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.3"><minus id="S4.SS2.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">\textit{te}^{-5}</annotation></semantics></math>,1<math id="S4.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\textit{e}^{-4}" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><msup id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.4.m4.1.1.2" xref="S4.SS2.SSS2.p1.4.m4.1.1.2a.cmml">e</mtext><mrow id="S4.SS2.SSS2.p1.4.m4.1.1.3" xref="S4.SS2.SSS2.p1.4.m4.1.1.3.cmml"><mo id="S4.SS2.SSS2.p1.4.m4.1.1.3a" xref="S4.SS2.SSS2.p1.4.m4.1.1.3.cmml">‚àí</mo><mn id="S4.SS2.SSS2.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS2.p1.4.m4.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><apply id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.4.m4.1.1.2a.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.2">e</mtext></ci><apply id="S4.SS2.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.3"><minus id="S4.SS2.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">\textit{e}^{-4}</annotation></semantics></math>), where <span id="S4.SS2.SSS2.p1.4.1" class="ltx_text ltx_font_italic">t</span> is the current number of epochs starting from 1. This results in a small learning rate at the beginning of learning, which decays by 1/5 every two epochs after 10 epochs. The batch size at each epoch was 32, and training was performed up to 13 epochs.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">In this study, the input images are entered as features obtained via bottom-up attention. These features are obtained as intermediate features extracted from a Faster R-CNN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> model (the backbone network is ResNet-101¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>) pre-trained on the Visual Genome dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Note that the Visual Genome dataset does not include specialized data such as specific bridge member or damage names, but is a dataset for general objects. Therefore, when detecting objects in the image using the Faster R-CNN, the objects are detected as objects similar in shape to each member or damage.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.2.2 Model training ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows an object detection example from an image from the created dataset using the Faster R-CNN trained on the Visual Genome dataset. Fig.¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.2.2 Model training ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows an image of a bridge abutment with water leaking, free lime, and cracks. The detection results show that ‚Äòwall‚Äô, ‚Äòrain‚Äô, and ‚Äògraffiti‚Äô were detected. As shown in the figure, the Faster R-CNN trained on the Visual Genome dataset does not directly correctly recognize objects in its object detection task. However, similar detection results in our dataset indicate the presence of similar members or damage. For example, if another image with water leaking or free lime is input, the same ‚Äòrain‚Äô or ‚Äògraffiti‚Äô detection results are expected. Therefore, by using the features obtained here as input for MCAN, VQA based on the features in each image region can be performed without creating a separate object detection dataset for bridge inspection.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.1.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2302.09208/assets/x7.png" id="S4.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="151" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.1.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F7.1.1.2.2" class="ltx_text" style="font-size:90%;">Input image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;"><img src="/html/2302.09208/assets/x8.png" id="S4.F7.2.2.g1" class="ltx_graphics ltx_img_landscape" width="151" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F7.2.2.2.2" class="ltx_text" style="font-size:90%;">Detection result</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.4.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.5.2" class="ltx_text" style="font-size:90%;">Example of detection results using the Faster R-CNN
</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Results and accuracy</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">An example of the test data output results from the trained model is shown in Fig.¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.2.3 Results and accuracy ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The top image in Fig.¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.2.3 Results and accuracy ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows a main girder with corrosion and degradation of the anticorrosive material. In cases where this image and the question ‚ÄòIs there corrosion in the image?‚Äô were input into the model, the model correctly returned ‚Äòyes‚Äô. In addition, the question ‚ÄòIs the drainage pipe in the image?‚Äô returned the correct result of ‚Äòno‚Äô. Furthermore, the question ‚ÄòIs there degradation of the anticorrosive on the main girder?‚Äô also returned the correct result of ‚Äòyes‚Äô. Finally, the question ‚ÄòWhat is the member that has degradation of the anticorrosive?‚Äô also returned the correct result, i.e., ‚Äòmain girder‚Äô.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">The middle image in Fig.¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.2.3 Results and accuracy ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows a girder bridge captured by a UAV. This bridge has utility attachments on the exterior girders, and the question ‚ÄòIs the utility attachment in the image?‚Äô was inputted, and ‚Äòyes‚Äô was output correctly. In addition, the question ‚ÄòIs there damage on the utility attachment?‚Äô also returned the correct result of ‚Äòno‚Äô. Furthermore, the question ‚ÄòIs the wheel guard in the image?‚Äô also returned the correct result, ‚Äòyes‚Äô. Finally, the question ‚ÄòIs there damage on the wheel guard?‚Äô also returned the correct result, ‚Äòyes‚Äô.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">The lower image in Fig.¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.2.3 Results and accuracy ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows a corroded drainage pipe. In cases where this image and the question ‚ÄòIs the drainage pipe in the image?‚Äô were input into the model, the model correctly returned ‚Äòyes‚Äô. In addition, the question ‚ÄòIs there corrosion in the image?‚Äô also returned the correct result of ‚Äòyes‚Äô. Furthermore, the question ‚ÄòIs there corrosion on the drainage pipe?‚Äô also returned the correct result, i.e., ‚Äòyes‚Äô. However, the question ‚ÄòWhat kind of damage has occurred to the drainage pipe?‚Äô was returned as ‚Äòfracture‚Äô, which is incorrect. The drainage pipe in this image is not fractured and the tip is corroded, but fractured steel usually corrodes at the fracture point as well. Therefore, it is difficult to determine that the pipe has not fractured from this image alone. Nevertheless, the model answered most of the questions correctly.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2302.09208/assets/x9.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="363" height="330" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.4.2" class="ltx_text" style="font-size:90%;">Example of output results from the trained model
</span></figcaption>
</figure>
<div id="S4.SS2.SSS3.p4" class="ltx_para">
<p id="S4.SS2.SSS3.p4.1" class="ltx_p">The accuracy of the output results on the test data was evaluated using the trained VQA model. Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.2.3 Results and accuracy ‚Ä£ 4.2 Training models using bridge images ‚Ä£ 4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the percentage of correct answers for each output category on the test data. The correct answer rate for questions that require the member name as the answer was 67.4%, and that for questions that require the damage name as the answer was 68.9%. The question that required a yes/no answer had the highest percentage of correct answers, at 99.1%. The overall correct answer rate for the test data was 81.2%.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Percentage of correct output answers.
</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">category</th>
<td id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">members</td>
<td id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">damages</td>
<td id="S4.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">YES/NO</td>
<td id="S4.T1.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">All</td>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<th id="S4.T1.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Number of questions</th>
<td id="S4.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">84392</td>
<td id="S4.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">84392</td>
<td id="S4.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t">122926</td>
<td id="S4.T1.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t">291710</td>
</tr>
<tr id="S4.T1.4.3.3" class="ltx_tr">
<th id="S4.T1.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Number of correct answers</th>
<td id="S4.T1.4.3.3.2" class="ltx_td ltx_align_center">56897</td>
<td id="S4.T1.4.3.3.3" class="ltx_td ltx_align_center">58152</td>
<td id="S4.T1.4.3.3.4" class="ltx_td ltx_align_center">121775</td>
<td id="S4.T1.4.3.3.5" class="ltx_td ltx_align_center">236824</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Correct answer rate(%)</th>
<td id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">67.4</td>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">68.9</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">99.1</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">81.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.SSS3.p5" class="ltx_para">
<p id="S4.SS2.SSS3.p5.1" class="ltx_p">One of the reasons for the higher accuracy of the yes/no questions is that the dataset created contains 38 types of members and 22 types of damages, resulting in more output options compared to yes/no questions. Furthermore, the low correct answer rate for questions that require the member name or damage as the answer is due to the fact that a single answer was created for each question; therefore, when multiple damages and members are included in a single image, they are considered to be incorrect.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Damage cause estimation based on the acquired damage information</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, the damage cause estimation method based on image location calculated via SfM and the VQA results is described. In this study, the estimation method is proposed for corrosion as an example of damage for which the cause is to be estimated. When corrosion occurs in bridge members, the main cause is moisture supply from the surrounding area. There are many possible reasons and origins for this moisture. Typical reasons include the following:</p>
</div>
<div id="S5.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Leaking from cracking on the slab;</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Leaking from cracking on the wheel guard;</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Leaking from damaged expansion joint;</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">Leaking from damaged drainage pipe.</p>
</div>
</li>
</ul>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">To identify the cause of damage as described above, it is first necessary to determine whether or not these members exist in the vicinity of the corroded area. If a member is present, it is then necessary to check whether an event has occurred in that member that could have caused the damage. Based on this, the damage cause estimation process is as shown in Fig.¬†<a href="#S5.F9" title="Figure 9 ‚Ä£ 5 Damage cause estimation based on the acquired damage information ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2302.09208/assets/x10.png" id="S5.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="363" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text" style="font-size:90%;">Damage cause estimation process
</span></figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">As shown in Fig.¬†<a href="#S5.F9" title="Figure 9 ‚Ä£ 5 Damage cause estimation based on the acquired damage information ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the first step is to analyse the image of interest (Camera A) (Step 1), where the type of damage in the image is determined. Then, based on the output damage name, a judgment is made regarding which member the damage is occurring in. Subsequently, based on the determined damage and member names, an analysis is performed to determine the damage cause that can be assumed in advance (Step 2). In this case, the surrounding images calculated via SfM are also used for analysis, as analysing the image of interest alone may miss information necessary for damage cause estimation. First, for the image to be analysed, it is determined if there are any members related to the assumed damage cause. If so, the system checks if there is an event that may have caused the damage to the member. In this way, the number of possible damage causes is analysed for each image. The above analysis is performed for all surrounding images, including the image of interest.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The number of images in which related members are determined to exist as a result of the analysis in Step 2 is <span id="S5.p5.1.1" class="ltx_text ltx_font_italic">N</span>. Let <span id="S5.p5.1.2" class="ltx_text ltx_font_italic">M</span> be the number of times that an event that could be the cause of damage is determined to occur. Then, the probability of each damage cause is expressed as <span id="S5.p5.1.3" class="ltx_text ltx_font_italic">M</span>/<span id="S5.p5.1.4" class="ltx_text ltx_font_italic">N</span>. If <span id="S5.p5.1.5" class="ltx_text ltx_font_italic">M</span>/<span id="S5.p5.1.6" class="ltx_text ltx_font_italic">N</span> is high, it is considered that the cause of damage is more likely to be the cause of the damage determined in Step 1.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">In this way, damage causes can be estimated based on the SfM results and VQA. However, the cause of damage that can be estimated by this method is limited to cases where the damage is caused by the conditions surrounding the damage. Therefore, damage causes that are difficult to determine by analysing the surrounding images alone, such as salt damage, cannot be estimated. Such damage causes must be estimated in combination with other approaches, such as using a database of distances from shorelines.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Field testing</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we verify whether the proposed framework can be used for damage cause estimation using actual bridge images. The bridge used in this study is the steel girder bridge shown in Fig.¬†<a href="#S6.F10" title="Figure 10 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, which is managed by the Ministry of Land, Infrastructure, Transport, and Tourism of Japan. The span length and total width of the bridge are 25.4 and 9.8 m, respectively. The bridge was constructed in 1970 in the mountainous area of Yamanashi Prefecture, Japan. It has several damaged sections; particularly, there is severe corrosion on the superstructure.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2302.09208/assets/x11.png" id="S6.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="181" height="124" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S6.F10.4.2" class="ltx_text" style="font-size:90%;">Steel girder bridge used for the field test in this study
</span></figcaption>
</figure>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In this study, images of this bridge were taken using Skydio 2, a small UAV developed by Skydio, as shown in Fig.¬†<a href="#S6.F11" title="Figure 11 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. The focal length of the camera mounted on the UAV is approximately 4 mm, with a 35-mm-equivalent focal length of approximately 21 mm and a pixel count of approximately 12.3 megapixels (4056 √ó 3040 px). Then, Agisoft‚Äôs Metashape was used for SfM processing. The number of images used for SfM was 541. The polygon mesh model created based on the SfM results is shown in Fig.¬†<a href="#S6.F12" title="Figure 12 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure id="S6.F11" class="ltx_figure"><img src="/html/2302.09208/assets/x12.png" id="S6.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S6.F11.4.2" class="ltx_text" style="font-size:90%;">Bridge images taken using a small unmanned aerial vehicle
</span></figcaption>
</figure>
<figure id="S6.F12" class="ltx_figure"><img src="/html/2302.09208/assets/x13.png" id="S6.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="181" height="202" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S6.F12.4.2" class="ltx_text" style="font-size:90%;">Polygon mesh model created based on the SfM results
</span></figcaption>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The coordinate positions of each camera and the intersection coordinates obtained from each camera are shown in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. The calculated coordinates are projected onto a two-dimensional plan view of the bridge. The red dots in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> indicate the coordinate positions of each camera. The orange, green, and blue dots are the intersection coordinates calculated by intersection determination with the mesh model. The orange dots are the intersection coordinates of the image of interest used to estimate the cause of damage. The green points are the intersection coordinates that exist inside a sphere with a radius of 1 m centered on the intersection coordinates of the image of interest, and the number of images is 63. The blue dots are the intersection coordinates of the out-of-range images and are not used for estimating the cause of damage. In this study, the VQA model constructed in Sec.¬†<a href="#S4" title="4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> was used to estimate the cause of damage based on the images indicated by the orange and green dots in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">The coordinates of each camera and the intersection coordinates obtained from each camera are shown in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. The calculated coordinates are projected onto a two-dimensional plane view of the bridge. The red dots in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> indicate the coordinates of each camera. The orange, green, and blue dots are the intersection coordinates calculated via intersection determination with the mesh model. The orange dots are the intersection coordinates of the image of interest used for damage cause estimation. The green dots are the intersection coordinates that exist inside a ball with a 1-m radius centred on the intersection coordinates of the image of interest (the number of images is 63). The blue dots are the intersection coordinates of the out-of-range images and are not used for damage cause estimation. In this study, the VQA model constructed in Sec.¬†<a href="#S4" title="4 Acquisition of damage information from bridge images ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> was used for damage cause estimation based on the images indicated by the orange and green dots in Fig.¬†<a href="#S6.F13" title="Figure 13 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure id="S6.F13" class="ltx_figure"><img src="/html/2302.09208/assets/x14.png" id="S6.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="181" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S6.F13.4.2" class="ltx_text" style="font-size:90%;">Camera and intersection coordinates
</span></figcaption>
</figure>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">First, VQA was used to estimate the type of damage and the member in which it occurs in the image of interest. The analysis results are shown in Fig.¬†<a href="#S6.F14" title="Figure 14 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. First, a question was asked regarding what type of damage was occurring in the image of interest. In this case, the output result was ‚Äòcorrosion‚Äô. Based on the output results obtained, a question was then asked regarding which member was corroded. In this case, the result was a ‚Äòcross beam‚Äô.</p>
</div>
<figure id="S6.F14" class="ltx_figure"><img src="/html/2302.09208/assets/x15.png" id="S6.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="181" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S6.F14.4.2" class="ltx_text" style="font-size:90%;">Output results for the image of interest
</span></figcaption>
</figure>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Then, VQA was used to analyse the image of interest and the surrounding images to estimate the cause of damage. In this study, four causes of corrosion damage were assumed and estimated: 1. leaking from cracking on the slab, 2. leaking from the expansion joint, 3. leaking from the drainage pipe, and 4. leaking from cracking on the wheel guard.</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p">In determining water leaking through cracking on the slab, ‚Äòslab‚Äô was set as a member related to the damage cause. The analysis was conducted assuming ‚Äòcracking‚Äô and ‚Äòleaking‚Äô as possible damage-causing events when a ‚Äòslab‚Äô exists. In determining leaking from the expansion joint, ‚Äòabutment‚Äô was set instead of expansion joint as the member related to the damage cause because the UAV could not approach the expansion joint in the range where the images were taken. This is valid because it is presumed that if there is leaking from the expansion joint, leaking can also be confirmed in the abutment directly below it. The analysis was conducted assuming ‚Äòleaking‚Äô as a possible damage-causing event when an ‚Äòabutment‚Äô exists. This allows the indirect determination of leaking from the expansion joint, despite the expansion joint not being photographed directly. In determining water leaking through the drainage pipe, ‚Äòdrainage pipe‚Äô was set as a member related to the damage cause. The analysis was conducted assuming ‚Äòcorrosion‚Äô, ‚Äòfissure‚Äô, ‚Äòfracture‚Äô, and ‚Äòleaking‚Äô as possible damage-causing events when a ‚Äòdrainage pipe‚Äô exists. Finally, in determining water leaking through cracking on the wheel guard, ‚Äòwheel guard‚Äô was set as a member related to the damage cause. The analysis was conducted assuming ‚Äòleaking‚Äô as a possible damage-causing event when a ‚Äòwheel guard‚Äô exists.</p>
</div>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">The analysis of these four damage causes was performed on 64 images, including the image of interest. An example of the analysis results is shown in Fig.¬†<a href="#S6.F15" title="Figure 15 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. The left image in Fig.¬†<a href="#S6.F15" title="Figure 15 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows an example of checking for the presence of a slab in the image and then checking for cracking on the slab. The right image in Fig.¬†<a href="#S6.F15" title="Figure 15 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows an example of checking for the presence of an abutment in the image and then checking for the presence of leaking marks on the abutment.</p>
</div>
<figure id="S6.F15" class="ltx_figure"><img src="/html/2302.09208/assets/x16.png" id="S6.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="362" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S6.F15.4.2" class="ltx_text" style="font-size:90%;">Example of output results with surrounding images
</span></figcaption>
</figure>
<div id="S6.p9" class="ltx_para">
<p id="S6.p9.1" class="ltx_p">Table <a href="#S6.T2" title="Table 2 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the damage cause estimation results. The number of members <span id="S6.p9.1.1" class="ltx_text ltx_font_italic">N</span> in Table <a href="#S6.T2" title="Table 2 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicates the number of images that were judged to contain related members that could cause corrosion. The number of damages <span id="S6.p9.1.2" class="ltx_text ltx_font_italic">M</span> indicates the number of images that are estimated to show damage that could cause corrosion among the images that are determined to show relevant members. As shown in Table <a href="#S6.T2" title="Table 2 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, out of the 64 images analysed, ‚Äòslab‚Äô was determined to be present in 61 images, with ‚Äòcracking‚Äô or ‚Äòleaking‚Äô being detected on the slab in 58 of these images. ‚ÄòAbutment‚Äô was determined to be present in 22 images, with ‚Äòleaking‚Äô being detected on the abutment in 18 of these images. The drainage pipe and wheel guard were not detected in any of the 64 images analysed in this study. Therefore, these were not considered to be the cause of damage in this case. In fact, the 64 images did not show any drainage pipes or wheel guards. Based on these results, corrosion is most likely caused by cracks on the slab or leakage from the expansion joint.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.3.2" class="ltx_text" style="font-size:90%;">Damage cause estimation results.
</span></figcaption>
<table id="S6.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.4.1.1" class="ltx_tr">
<th id="S6.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">Damage</th>
<td id="S6.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">Corrosion</td>
</tr>
<tr id="S6.T2.4.2.2" class="ltx_tr">
<th id="S6.T2.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Member</th>
<td id="S6.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Cross beam</td>
</tr>
<tr id="S6.T2.4.3.3" class="ltx_tr">
<th id="S6.T2.4.3.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S6.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_border_t">Leaking from</td>
<td id="S6.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_border_t">Leaking from the</td>
<td id="S6.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_border_t">Leaking from the</td>
<td id="S6.T2.4.3.3.5" class="ltx_td ltx_align_center ltx_border_t">Leaking from</td>
</tr>
<tr id="S6.T2.4.4.4" class="ltx_tr">
<th id="S6.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Damage cause</th>
<td id="S6.T2.4.4.4.2" class="ltx_td ltx_align_center">cracking on</td>
<td id="S6.T2.4.4.4.3" class="ltx_td ltx_align_center">expansion joint</td>
<td id="S6.T2.4.4.4.4" class="ltx_td ltx_align_center">drainage pipe</td>
<td id="S6.T2.4.4.4.5" class="ltx_td ltx_align_center">cracking on</td>
</tr>
<tr id="S6.T2.4.5.5" class="ltx_tr">
<th id="S6.T2.4.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S6.T2.4.5.5.2" class="ltx_td ltx_align_center">the slab</td>
<td id="S6.T2.4.5.5.3" class="ltx_td"></td>
<td id="S6.T2.4.5.5.4" class="ltx_td"></td>
<td id="S6.T2.4.5.5.5" class="ltx_td ltx_align_center">the wheel guard</td>
</tr>
<tr id="S6.T2.4.6.6" class="ltx_tr">
<th id="S6.T2.4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<span id="S6.T2.4.6.6.1.1" class="ltx_text ltx_font_italic">N</span>:</th>
<td id="S6.T2.4.6.6.2" class="ltx_td ltx_border_t"></td>
<td id="S6.T2.4.6.6.3" class="ltx_td ltx_border_t"></td>
<td id="S6.T2.4.6.6.4" class="ltx_td ltx_border_t"></td>
<td id="S6.T2.4.6.6.5" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S6.T2.4.7.7" class="ltx_tr">
<th id="S6.T2.4.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Number</th>
<td id="S6.T2.4.7.7.2" class="ltx_td ltx_align_center">61</td>
<td id="S6.T2.4.7.7.3" class="ltx_td ltx_align_center">22</td>
<td id="S6.T2.4.7.7.4" class="ltx_td ltx_align_center">0</td>
<td id="S6.T2.4.7.7.5" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S6.T2.4.8.8" class="ltx_tr">
<th id="S6.T2.4.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">of members</th>
<td id="S6.T2.4.8.8.2" class="ltx_td"></td>
<td id="S6.T2.4.8.8.3" class="ltx_td"></td>
<td id="S6.T2.4.8.8.4" class="ltx_td"></td>
<td id="S6.T2.4.8.8.5" class="ltx_td"></td>
</tr>
<tr id="S6.T2.4.9.9" class="ltx_tr">
<th id="S6.T2.4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S6.T2.4.9.9.1.1" class="ltx_text ltx_font_italic">M</span>:</th>
<td id="S6.T2.4.9.9.2" class="ltx_td"></td>
<td id="S6.T2.4.9.9.3" class="ltx_td"></td>
<td id="S6.T2.4.9.9.4" class="ltx_td"></td>
<td id="S6.T2.4.9.9.5" class="ltx_td"></td>
</tr>
<tr id="S6.T2.4.10.10" class="ltx_tr">
<th id="S6.T2.4.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Number</th>
<td id="S6.T2.4.10.10.2" class="ltx_td ltx_align_center">58</td>
<td id="S6.T2.4.10.10.3" class="ltx_td ltx_align_center">18</td>
<td id="S6.T2.4.10.10.4" class="ltx_td ltx_align_center">0</td>
<td id="S6.T2.4.10.10.5" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S6.T2.4.11.11" class="ltx_tr">
<th id="S6.T2.4.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">of damages</th>
<td id="S6.T2.4.11.11.2" class="ltx_td"></td>
<td id="S6.T2.4.11.11.3" class="ltx_td"></td>
<td id="S6.T2.4.11.11.4" class="ltx_td"></td>
<td id="S6.T2.4.11.11.5" class="ltx_td"></td>
</tr>
<tr id="S6.T2.4.12.12" class="ltx_tr">
<th id="S6.T2.4.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<span id="S6.T2.4.12.12.1.1" class="ltx_text ltx_font_italic">M</span>/<span id="S6.T2.4.12.12.1.2" class="ltx_text ltx_font_italic">N</span>
</th>
<td id="S6.T2.4.12.12.2" class="ltx_td ltx_align_center ltx_border_bb">0.95</td>
<td id="S6.T2.4.12.12.3" class="ltx_td ltx_align_center ltx_border_bb">0.82</td>
<td id="S6.T2.4.12.12.4" class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td id="S6.T2.4.12.12.5" class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p10" class="ltx_para">
<p id="S6.p10.1" class="ltx_p">Furthermore, it is confirmed that accurate damage cause estimation is possible from actual bridge images. It is worth noting that the image of interest shown in Fig.¬†<a href="#S6.F14" title="Figure 14 ‚Ä£ 6 Field testing ‚Ä£ Bridge Damage Cause Estimation Using Multiple Images Based on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> does not show leaking on the abutment. Therefore, by using the proposed method to analyse multiple images for damage cause estimation, it is possible to obtain results that cannot be obtained by analysing the image of interest alone. Moreover, as the VQA model accuracy is not perfect, diagnosis based on a single image alone may yield erroneous results. However, as demonstrated here, diagnosis based on the analysis results of multiple images can provide a more accurate diagnosis. In addition, when practicing engineers use the proposed method, it is expected to contribute to preventing overlooking the cause of damage by checking the related members that are estimated to have a high damage cause probability by referring to the analysis results.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, a damage cause estimation method is proposed by calculating camera coordinates via SfM and extracting information from the images via VQA using multiple images taken around the damage region based on the calculated camera coordinates.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In damage cause estimation, it is desirable to utilize not only a single image but also images of the surrounding conditions. Therefore, we first described a method for acquiring the image of interest and its surrounding images using SfM. Next, a VQA model was constructed using MCAN with a dataset specific to bridge members and damage information. The correct answer rate for questions requiring the member‚Äôs name and the damage‚Äôs name were 67.4% and 68.9%, respectively. The correct answer rate for questions requiring a yes/no answer was 99.1%. The overall correct answer rate for the test data was 81.2%.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Furthermore, a damage cause estimation method is proposed using the SfM results and VQA model. Based on the VQA model output results and the pre-assumed damage causes, we proposed an approach to narrow the damage causes by inputting new questions into the VQA model.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">The proposed method was used to determine the damage and estimate its cause on a real bridge. In this study, corrosion in bridge members was taken as the damage to be determined and estimate its causes. As a result, it was concluded that corrosion was most likely caused by cracks on the slab or leakage from the expansion joint.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">When the proposed method is used by practicing engineers, it is expected to contribute to preventing overlooking the cause of damage by focusing on the related members that are estimated to have a high damage cause probability. In addition, for inexperienced engineers, it could also contribute to improve their maintenance skills, as they can learn where to focus in case of damage.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">To improve the damage cause estimation accuracy, we could consider the severity of the damage, the relationship between the upper and lower positions of the damage, and weight the damage according to the distance between them. Further VQA model accuracy improvement is also important to improve the overall damage cause estimation accuracy.</p>
</div>
<section id="S7.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.0.1 </span>Acknowledgments</h4>

<div id="S7.SS0.SSS1.p1" class="ltx_para">
<p id="S7.SS0.SSS1.p1.1" class="ltx_p">We would like to thank the Kofu River and National Highway Office, the Kanto Regional Development Bureau, and the Ministry of Land, Infrastructure, Transport, and Tourism of Japan for allowing us to take measurements of the bridges under their management. We would like to express our sincere gratitude to these organizations for making this research possible. The authors report there are no competing interests to declare. This research is supported by JST Moonshot Research and Development under Grant JPMJMS2032, and JSPS KAKENHI Grant Numbers 21H01417, 22J14364.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Agdas, D., Rice, J.A., Martinez, J.R., Lasa, I.R.: Comparison of visual
inspection and structural-health monitoring as bridge condition assessment
methods. Journal of Performance of Constructed Facilities <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">30</span>(3),
04015049 (2016)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Amezquita-Sanchez, J.P., Adeli, H.: Feature extraction and classification
techniques for health monitoring of structures. Scientia Iranica
<span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">22</span>(6), 1931‚Äì1940 (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 6077‚Äì6086 (2018)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: Vqa: Visual question answering. In: 2015 IEEE International Conference on
Computer Vision (ICCV). pp. 2425‚Äì2433 (2015)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
ASCE: ASCE‚Äôs 2021 infrastructure report card. American Society of Civil
Engineers (ASCE) (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473 (2014)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Cha, Y.J., Choi, W., B√ºy√ºk√∂zt√ºrk, O.: Deep learning-based crack
damage detection using convolutional neural networks. Computer-Aided Civil
and Infrastructure Engineering <span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">32</span>(5), 361‚Äì378 (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Cho, K., Van¬†Merri√´nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
Schwenk, H., Bengio, Y.: Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078 (2014)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chun, P.J., Izumi, S., Yamane, T.: Automatic detection method of cracks from
concrete surface imagery using two-step light gradient boosting machine.
Computer-Aided Civil and Infrastructure Engineering <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">36</span>(1), 61‚Äì72
(2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chun, P.J., Yamane, T., Maemura, Y.: A deep learning-based image captioning
method to automatically generate comprehensive explanations of bridge damage.
Computer-Aided Civil and Infrastructure Engineering <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">37</span>(11),
1387‚Äì1401 (2022)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chun, P.J., Yamane, T., Tsuzuki, Y.: Automatic detection of cracks in asphalt
pavement using deep learning to overcome weaknesses in images and gis
visualization. Applied Sciences <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">11</span>(3), ¬†892 (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fondevik, S.K., Stahl, A., Transeth, A.A., Knudsen, O.√ò.: Image segmentation
of corrosion damages in industrial inspections. In: 2020 IEEE 32nd
International Conference on Tools with Artificial Intelligence (ICTAI). pp.
787‚Äì792 (2020)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gattulli, V., Chiaramonte, L.: Condition assessment by visual inspection for a
bridge management system. Computer-Aided Civil and Infrastructure Engineering
<span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">20</span>(2), 95‚Äì107 (2005)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in
vqa matter: Elevating the role of image understanding in visual question
answering. In: 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 6325‚Äì6334 (2017)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hartley, R., Zisserman, A.: Multiple View Geometry in Computer Vision.
Cambridge University Press (2003)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 770‚Äì778 (2016)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation
<span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">9</span>(8), 1735‚Äì1780 (1997)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 (2014)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M.S., Fei-Fei, L.: Visual
genome: Connecting language and vision using crowdsourced dense image
annotations. International Journal of Computer Vision <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">123</span>(1),
32‚Äì73 (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep
convolutional neural networks. In: Advances in Neural Information Processing
Systems. vol.¬†25 (2012)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Liang, X.: Image-based post-disaster inspection of reinforced concrete bridge
systems using deep learning with bayesian optimization. Computer-Aided Civil
and Infrastructure Engineering <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">34</span>(5), 415‚Äì430 (2019)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Liu, Y.F., Nie, X., Fan, J.S., Liu, X.G.: Image-based crack assessment of
bridge piers using unmanned aerial vehicles and three-dimensional scene
reconstruction. Computer-Aided Civil and Infrastructure Engineering
<span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">35</span>(5), 511‚Äì529 (2020)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image
co-attention for visual question answering. Advances in neural information
processing systems <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">29</span> (2016)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ma, L., Lu, Z., Li, H.: Learning to answer questions from image using
convolutional neural network. In: Proceedings of the AAAI Conference on
Artificial Intelligence (2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Malinowski, M., Fritz, M.: A multi-world approach to question answering about
real-world scenes based on uncertain input. Advances in Neural Information
Processing Systems <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">27</span> (2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A deep learning
approach to visual question answering. International Journal of Computer
Vision <span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">125</span>, 110‚Äì135 (2017)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed
representations of words and phrases and their compositionality. Advances in
Neural Information Processing Systems <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">26</span> (2013)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M√∂ller, T., Trumbore, B.: Fast, minimum storage ray-triangle intersection.
Journal of Graphics Tools <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">2</span>(1), 21‚Äì28 (1997)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Nam, H., Ha, J.W., Kim, J.: Dual attention networks for multimodal reasoning
and matching. In: 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 2156‚Äì2164 (2017)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Nguyen, D.K., Okatani, T.: Improved fusion of visual and language
representations by dense symmetric co-attention for visual question
answering. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 6087‚Äì6096 (2018)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Noh, H., Seo, P.H., Han, B.: Image question answering using convolutional
neural network with dynamic parameter prediction. In: 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 30‚Äì38 (2016)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ou, J., Li, H.: Structural health monitoring in mainland china: Review and
future trends. Structural Health Monitoring <span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">9</span>(3), 219‚Äì231 (2010)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Qarib, H., Adeli, H.: Recent advances in health monitoring of civil structures.
Scientia Iranica <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">21</span>(6), 1733‚Äì1742 (2014)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Rahman, A., Wu, Z.Y., Kalfarisi, R.: Semantic deep learning integrated with rgb
feature-based rule optimization for facility surface corrosion detection and
evaluation. Journal of Computing in Civil Engineering <span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">35</span>(6),
04021018 (2021)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., Zemel, R.: Exploring models and data for image question
answering. Advances in Neural Information Processing Systems <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">28</span>
(2015)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. Advances in neural information
processing systems <span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">28</span> (2015)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by
back-propagating errors. Nature <span id="bib.bib37.1.1" class="ltx_text ltx_font_bold">323</span>(6088), 533‚Äì536 (1986)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Shih, K.J., Singh, S., Hoiem, D.: Where to look: Focus regions for visual
question answering. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 4613‚Äì4621 (2016)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: 2015 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). pp.¬†1‚Äì9 (2015)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Vaghefi, K., Oats, R.C., Harris, D.K., Ahlborn, T.T.M., Brooks, C.N., Endsley,
K.A., Roussi, C., Shuchman, R., Burns, J.W., Dobson, R.: Evaluation of
commercially available remote sensors for highway bridge condition
assessment. Journal of Bridge Engineering <span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">17</span>(6), 886‚Äì895 (2012)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, ≈Å., Polosukhin, I.: Attention is all you need. Advances in Neural
Information Processing Systems <span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wang, X., Hu, Z.: Grid-based pavement crack analysis using deep learning. In:
2017 4th International Conference on Transportation Information and Safety
(ICTIS). pp. 917‚Äì924 (2017)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Xu, Y., Li, S., Zhang, D., Jin, Y., Zhang, F., Li, N., Li, H.: Identification
framework for cracks on a steel structure surface by a restricted boltzmann
machines algorithm based on consumer-grade camera images. Structural Control
and Health Monitoring <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">25</span>(2), e2075 (2018)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Yamane, T., Chun, P.J., Dang, J., Honda, R.: Recording of bridge damage areas
by 3D integration of multiple images and reduction of the variability in
detected results. Computer-Aided Civil and Infrastructure Engineering (2023)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Yamane, T., Chun, P.J., Honda, R.: Detecting and localising damage based on
image recognition and structure from motion, and reflecting it in a 3D
bridge model. Structure and Infrastructure Engineering (2022)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yang, X., Li, H., Yu, Y., Luo, X., Huang, T., Yang, X.: Automatic pixel-level
crack detection and measurement using fully convolutional network.
Computer-Aided Civil and Infrastructure Engineering <span id="bib.bib46.1.1" class="ltx_text ltx_font_bold">33</span>(12),
1090‚Äì1109 (2018)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yu, L., Park, E., Berg, A.C., Berg, T.L.: Visual madlibs: Fill in the blank
description generation and question answering. In: 2015 IEEE International
Conference on Computer Vision (ICCV). pp. 2461‚Äì2469 (2015)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Cui, Y., Tao, D., Tian, Q.: Deep modular co-attention networks
for visual question answering. In: 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 6281‚Äì6290 (2019)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Yu, Z., Yu, J., Xiang, C., Fan, J., Tao, D.: Beyond bilinear: Generalized
multimodal factorized high-order pooling for visual question answering. IEEE
Transactions on Neural Networks and Learning Systems <span id="bib.bib49.1.1" class="ltx_text ltx_font_bold">29</span>(12),
5947‚Äì5959 (2018)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zhang, A., Wang, K.C., Li, B., Yang, E., Dai, X., Peng, Y., Fei, Y., Liu, Y.,
Li, J.Q., Chen, C.: Automated pixel-level pavement crack detection on 3D
asphalt surfaces using a deep-learning network. Computer-Aided Civil and
Infrastructure Engineering <span id="bib.bib50.1.1" class="ltx_text ltx_font_bold">32</span>(10), 805‚Äì819 (2017)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhou, B., Tian, Y., Sukhbaatar, S., Szlam, A., Fergus, R.: Simple baseline for
visual question answering. arXiv preprint arXiv:1512.02167 (2015)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question
answering in images. In: 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 4995‚Äì5004 (2016)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.09207" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.09208" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.09208">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.09208" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.09209" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 00:53:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
