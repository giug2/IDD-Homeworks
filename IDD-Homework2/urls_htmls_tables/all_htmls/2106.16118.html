<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.16118] SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo</title><meta property="og:description" content="Robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions. Even with large-scale real-world data collectioâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.16118">

<!--Generated on Wed Mar 13 12:49:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document"> SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Thomas Kollar<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">âˆ—,1</span></sup>
Michael Laskey<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">âˆ—,1</span></sup>
Kevin Stone<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">âˆ—,1</span></sup>
Brijen Thananjeyan<sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">âˆ—,1,2</span></sup>
Mark Tjersland<sup id="id10.10.id5" class="ltx_sup"><span id="id10.10.id5.1" class="ltx_text ltx_font_italic">âˆ—,1</span></sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p"><span id="id11.id1.1" class="ltx_text">Robot manipulation of unknown objects in unstructured environments is a challenging problem due to the variety of shapes, materials, arrangements and lighting conditions. Even with large-scale real-world data collection, robust perception and manipulation of transparent and reflective objects across various lighting conditions remains challenging. To address these challenges we propose an approach to performing sim-to-real transfer of robotic perception. The underlying model, SimNet, is trained as a single multi-headed neural network using simulated stereo data as input and simulated object segmentation masks, 3D oriented bounding boxes (OBBs), object keypoints and disparity as output. A key component of SimNet is the incorporation of a learned stereo sub-network that predicts disparity. SimNet is evaluated on 2D car detection, unknown object detection and deformable object keypoint detection and significantly outperforms a baseline that uses a structured light RGB-D sensor. By inferring grasp positions using the OBB and keypoint predictions, SimNet can be used to perform end-to-end manipulation of unknown objects in both â€easyâ€ and â€hardâ€ scenarios using our fleet of Toyota HSR robots in four home environments. In unknown object grasping experiments, the predictions from the baseline RGB-D network and SimNet enable successful grasps of most of the â€œeasyâ€ objects. However, the RGB-D baseline only grasps 35% of the â€œhardâ€ (e.g., transparent) objects, while SimNet grasps 95%, suggesting that SimNet can enable robust manipulation of unknown objects, including transparent objects, in unknown environments. Additional visualizations and materials are located at <a target="_blank" href="https://tinyurl.com/simnet-corl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://tinyurl.com/simnet-corl</a>.</span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<blockquote id="p1.1" class="ltx_quote">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Keywords:</span> Sim-to-Real, Computer Vision, Manipulation</p>
</blockquote>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">To successfully deploy robots into diverse, unstructured environments such as the home, robots must have robust and general behaviors that that can adapt to variations in lighting, furniture, and objects. As such, robots must be able to to perceive and manipulate objects that they have not seen before. This is made more challenging by the fact that many everyday objects are made of reflective or transparent materials and are located in places that have harsh lighting (e.g., FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, right side).</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To perform well in these settings, large-scale real-world data collection is often performedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which is costly, time-consuming, has limited flexibility and sometimes contains missing data (as is the case with RGB-D sensing of transparent objects). An alternative is to use simulation to automatically label a large dataset of imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Although physics-based and photorealistic simulations can enable end-to-end learning methods, rendering can take significant computational resources and artists must generate high-quality artefactsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">This paper proposes an approach for sim-to-real transfer of robot perception and manipulation that does not require large-scale real-world datasets to be collected and does not require photorealism during simulation. The approach has three primary components. First, a non-photorealistic simulator that randomizes over lighting and textures is used to automatically produce large-scale domain-randomized data that includes stereo imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, 3D oriented bounding boxes, object keypoints, and segmentation masks (FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The second component and a core contribution SimNet (FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is a lightweight multi-headed neural network that is trained exclusively on simulated data and takes as input stereo pairs and predicts segmentation masks, 3D oriented bounding boxes (OBBs), keypoints and disparity as output. Key to our approach is the ability to predict disparity images using a differentiable cost volume to match features in a stereo imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This network is end-to-end trained using a depth reconstruction auxiliary loss, which encourages the network to focus on disparity predictions and geometric information that are task-relevant. Unlike other approaches, which use active depth sensingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the incorporation of the learned differentiable stereo network into SimNet enables perception in the presence of non-Lambertian surfaces such as reflective or transparent materials and in challenging lighting conditions such as direct sunlight. SimNet can run at 20Hz on a Titan Xp GPU. The third component is a heuristic method for predicting grasp positions from the OBB and keypoint predictions. These grasp positions are combined with a classical planner to execute manipulation behaviorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2106.16118/assets/figures/Simnet_NEW-min.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Simulation images:</span> Simulated data was generated for three domains: cars, small objects and t-shirts using a non-photorealistic simulator with domain-randomization. Dataset generation is parallelized across machines and can be generated in an hour for $60 (USD) cloud compute cost. Labels for OBBs, semantic segmentation masks, depth images, and keypoints can automatically be generated. The data is used to train SimNet. By learning to focus on geometry, sim2real transfer can be performed using only very low-quality scenes.
</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">SimNet is evaluated on three real-world domains: 2D car detection, unknown object grasping and deformable object keypoint prediction for t-shirt folding. SimNet is trained on a large-scale synthetic dataset generated for each domain and evaluated on real-world validation sets that include optically-challenging objects, distractor objects, furniture and direct sunlight. The baseline network predicts the same outputs, but uses RGB-D input from a structured light sensor. All three domains showed mAP gains with SimNet over the baseline. SimNet is also evaluated in an unknown object grasping experiment with both â€œeasyâ€ and â€œhardâ€ objects using a fleet of Toyota HSR-C robots in four different homes. The â€œhardâ€ objects are optically challenging and are reflective or transparent. Both the RGB-D network and SimNet are able to effectively generate grasps for the easy objects. However, the robot is only able grasp 35% of the â€œhardâ€ objects with the RGB-D network, while it is able to grasp 95% with SimNet. These results demonstrate that SimNet is able to grasp a wide variety of unknown objects, including transparent objects, in unstructured environments. SimNet is also evaluated on a t-shirt folding experiment across several homes and shirts.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">This work makes five contributions: <em id="S1.p5.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">(i)</em> an efficient neural network, SimNet, that leverages learned stereo matching to enable robust multi-headed predictions of keypoints, oriented bounding boxes (OBBs), semantic segmentation masks, and depth images using only low-quality synthetic data, <em id="S1.p5.1.2" class="ltx_emph ltx_font_bold ltx_font_italic">(ii)</em> the first network to enable single-shot prediction of 3D oriented bounding boxes of unknown objects <em id="S1.p5.1.3" class="ltx_emph ltx_font_bold ltx_font_italic">(iii)</em> stereo vision experiments on simulated and real home scenes with annotations for oriented bounding boxes, segmentation masks, depth images, and keypoints, <em id="S1.p5.1.4" class="ltx_emph ltx_font_bold ltx_font_italic">(iv)</em> experiments on the KITTI stereo vision benchmark dataset that suggest that the network transfers significantly better than standard domain randomization and naive stereo concatenation, and <em id="S1.p5.1.5" class="ltx_emph ltx_font_bold ltx_font_italic">(v)</em> experiments in four homes that suggest that the network robustly transfers to real-world environments compared to active depth sensing and can be used to construct robot manipulation policies for grasping and t-shirt folding. The model training code and validation datasets will be publicly released.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Our related work is divided into perception for manipulation, sim-to-real and learned stereo.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2106.16118/assets/figures/Panoptic_NEW-min.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="499" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.2.1" class="ltx_text ltx_font_bold">SimNet:</span>
In SimNet, each stereo RGB image is fed into a feature extractor before being fed into SCVN, which performs approximate stereo matching. The output of the SCVN is a low resolution disparity image fed in with features from the left image to a ResNet-FPN backbone and output prediction heads. The output heads predict room-level segmentation, OBBs, keypoints, and full-resolution disparity images.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Parameterized Representations for Manipulation</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In contrast to end-to-end robot policy learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>,
a popular approach for generating behaviors is to parameterize motions by the outputs of a perception networkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. This decouples perception from planning and control, and enables perception systems to be trained in simulation without the need for accurate physical simulation.
One representation commonly used for grasping of rigid objects are oriented bounding boxes (OBBs)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. These are useful for grasping common household objects, by aligning grasps along the principal componentsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Our work contributes a method to directly predict OBBs from a neural network.
A representation that is used for deformable manipulation are keypoints or learned correspondences, which have seen significant success in tasks such as deformable manipulationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and graspingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ganapathi etÂ al. [<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Sundaresan etÂ al. [<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> predict correspondences from domain-randomized depth or monocular RGB images and demonstrate impressive transfer to real fabrics and ropes in constrained lab environments. In contrast, we demonstrate robust transfer on diverse home scenes, and compare to the methods in these papers as baselines.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sim-to-Real Transfer</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Transferring perception models
trained on simulated data to reality is an active area of research. One technique to transfer from simulation to reality is to train directly on a geometric representation of the world such as a point cloud or depth imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Prior work has used depth-based transfer techniques in a variety of scenarios, such as grasping unknown objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, tying a ropeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and segmenting object instances in cluttered scenesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Depth-based transfer is commonly performed with an active sensor using structured light. The applicability of these sensors is limited to regimes where their reflected infrared pattern is consistent enough that it can be pattern matched for 3-D reconstructionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, home settings contain non-Lambertian objects such as glassware and a large amount of natural light, which can create large sensing errors. Existing work studies combining depth sensors with RGB information to fill in missing depth information for clear objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> using photorealistic simulation and real data. In contrast toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we exclusively use inexpensive RGB sensors and low quality simulated data for training. Our approach is comparable to <cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. [<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which separately leverages depth and synthetic RGB inputs to compensate for the limitations of depth sensing alone. We compare to several variants ofÂ <cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. [<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as a baseline approach, and find that using stereo images outperforms using RGB and depth on optically challenging objects.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">An alternative to depth-based transfer is domain randomization with RGB images (DR)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Typically, DR randomizes over the lighting and textures in the environment by utilizing a low-quality, non-photorealistic renderer and has produced successful results in car detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, 6-DOF pose estimation of known objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and camera calibrationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
By injecting large randomization into texture and lighting, the network is forced to use the geometry of the scene to solve the taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. A limitation of this approach is that geometric reasoning on a monocular image requires leveraging global shape priors, lighting effects, and scene understandingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In this work, we propose a transfer technique that uses learned stereo matching with domain randomization to extract geometric features using only local non-semantic information and demonstrate robustness on the KITTI benchmark.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Learned Stereo Matching</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The goal of stereo matching is to try to explicitly compute the pixel displacement offset between objects from the left and right images in a stereo pairÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. By computing a per-pixel displacement offset, or disparity, a high-resolution point cloud can be generated of the world. Given that it is challenging to estimate similarity features of image patches, recent work has explored using deep learning techniques to advance stereo techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
In contrast to the learned stereo community that targets achieving perfect depth-sensing when training with real dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we are interested in enabling transfer from simulation for â€œhigh-levelâ€ vision tasks. We specifically propose a network for manipulation, SimNet, that regresses coarse depth with an efficient stereo matching network architecture based onÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to learn features that are robust enough to predict oriented bounding boxes, segmentation masks, and keypoints on real objects for manipulation.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>SimNet: Enabling Predictions for Manipulation From Synthetic Stereo</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2106.16118/assets/figures/Fleet_Manipulation_NEW-min.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="497" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">Manipulation overview</span>: A stereo pair is fed as input to SimNet, which produces OBBs. Grasp positions are produced from OBBs and are used by a classical planner to grasp the objectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. This grasping technique has been deployed on a fleet of home robots to perform manipulation in optically challenging scenarios.
</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we will discuss our network architecture that leverages approximate stereo matching techniques fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to predict segmentation masks, OBBs, and keypoints on unseen objects for robot manipulation. Our key insight is that by training approximate stereo matching algorithms on pure synthetic data, we can learn robust â€œlow-levelâ€ features like disparity than enable sim2real transfer of â€œhigh-levelâ€ vision tasks. We first discuss how we learn robust low-level features and then feed that into high-level perception algorithms. Finally, we discuss how our low-cost synthetic data is generated. For training details see supplemental Sec.Â <a href="#A2.SS2" title="B.2 Training Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>; the overall network architecture is in Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stereo Cost Volume Networks (SCVN) For Robust Low-Level Features</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.19" class="ltx_p">We will now describe the stereo matching neural network module used in SimNetâ€™s backbone, which enables transfer from simulation. In this section, we use <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">âŠ™</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\odot</annotation></semantics></math> to denote Hadamard products, and use <math id="S3.SS1.p1.2.m2.5" class="ltx_Math" alttext="I_{[i,j:k,:]}" display="inline"><semantics id="S3.SS1.p1.2.m2.5a"><msub id="S3.SS1.p1.2.m2.5.6" xref="S3.SS1.p1.2.m2.5.6.cmml"><mi id="S3.SS1.p1.2.m2.5.6.2" xref="S3.SS1.p1.2.m2.5.6.2.cmml">I</mi><mrow id="S3.SS1.p1.2.m2.5.5.5.5" xref="S3.SS1.p1.2.m2.5.5.5.6.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.5.5.5.5.2" xref="S3.SS1.p1.2.m2.5.5.5.6.1.cmml">[</mo><mrow id="S3.SS1.p1.2.m2.5.5.5.5.1" xref="S3.SS1.p1.2.m2.5.5.5.5.1.cmml"><mrow id="S3.SS1.p1.2.m2.5.5.5.5.1.2.2" xref="S3.SS1.p1.2.m2.5.5.5.5.1.2.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.2.m2.5.5.5.5.1.2.2.1" xref="S3.SS1.p1.2.m2.5.5.5.5.1.2.1.cmml">,</mo><mi id="S3.SS1.p1.2.m2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.cmml">j</mi></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p1.2.m2.5.5.5.5.1.1" xref="S3.SS1.p1.2.m2.5.5.5.5.1.1.cmml">:</mo><mrow id="S3.SS1.p1.2.m2.5.5.5.5.1.3.2" xref="S3.SS1.p1.2.m2.5.5.5.5.1.3.1.cmml"><mi id="S3.SS1.p1.2.m2.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.cmml">k</mi><mo id="S3.SS1.p1.2.m2.5.5.5.5.1.3.2.1" xref="S3.SS1.p1.2.m2.5.5.5.5.1.3.1.cmml">,</mo><mo rspace="0em" id="S3.SS1.p1.2.m2.4.4.4.4" xref="S3.SS1.p1.2.m2.4.4.4.4.cmml">:</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.p1.2.m2.5.5.5.5.3" xref="S3.SS1.p1.2.m2.5.5.5.6.1.cmml">]</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.5b"><apply id="S3.SS1.p1.2.m2.5.6.cmml" xref="S3.SS1.p1.2.m2.5.6"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.5.6.1.cmml" xref="S3.SS1.p1.2.m2.5.6">subscript</csymbol><ci id="S3.SS1.p1.2.m2.5.6.2.cmml" xref="S3.SS1.p1.2.m2.5.6.2">ğ¼</ci><apply id="S3.SS1.p1.2.m2.5.5.5.6.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.5.5.5.6.1.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5.2">delimited-[]</csymbol><apply id="S3.SS1.p1.2.m2.5.5.5.5.1.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5.1"><ci id="S3.SS1.p1.2.m2.5.5.5.5.1.1.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5.1.1">:</ci><list id="S3.SS1.p1.2.m2.5.5.5.5.1.2.1.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5.1.2.2"><ci id="S3.SS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1">ğ‘–</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2">ğ‘—</ci></list><list id="S3.SS1.p1.2.m2.5.5.5.5.1.3.1.cmml" xref="S3.SS1.p1.2.m2.5.5.5.5.1.3.2"><ci id="S3.SS1.p1.2.m2.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3">ğ‘˜</ci><ci id="S3.SS1.p1.2.m2.4.4.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4.4.4">:</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.5c">I_{[i,j:k,:]}</annotation></semantics></math> to select all elements with index <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">i</annotation></semantics></math> in the first dimension of tensor <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">I</annotation></semantics></math>, index in <math id="S3.SS1.p1.5.m5.2" class="ltx_Math" alttext="\{j,\ldots k-1\}" display="inline"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.2.1" xref="S3.SS1.p1.5.m5.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.2" xref="S3.SS1.p1.5.m5.2.2.2.cmml">{</mo><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">j</mi><mo id="S3.SS1.p1.5.m5.2.2.1.3" xref="S3.SS1.p1.5.m5.2.2.2.cmml">,</mo><mrow id="S3.SS1.p1.5.m5.2.2.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.cmml"><mrow id="S3.SS1.p1.5.m5.2.2.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.1.2.cmml"><mi mathvariant="normal" id="S3.SS1.p1.5.m5.2.2.1.1.2.2" xref="S3.SS1.p1.5.m5.2.2.1.1.2.2.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.2.2.1.1.2.1" xref="S3.SS1.p1.5.m5.2.2.1.1.2.1.cmml">â€‹</mo><mi id="S3.SS1.p1.5.m5.2.2.1.1.2.3" xref="S3.SS1.p1.5.m5.2.2.1.1.2.3.cmml">k</mi></mrow><mo id="S3.SS1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p1.5.m5.2.2.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p1.5.m5.2.2.1.4" xref="S3.SS1.p1.5.m5.2.2.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><set id="S3.SS1.p1.5.m5.2.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘—</ci><apply id="S3.SS1.p1.5.m5.2.2.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1"><minus id="S3.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1"></minus><apply id="S3.SS1.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.2"><times id="S3.SS1.p1.5.m5.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.2.1"></times><ci id="S3.SS1.p1.5.m5.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.2.2">â€¦</ci><ci id="S3.SS1.p1.5.m5.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.2.3">ğ‘˜</ci></apply><cn type="integer" id="S3.SS1.p1.5.m5.2.2.1.1.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.3">1</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">\{j,\ldots k-1\}</annotation></semantics></math> in the second dimension of <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">I</annotation></semantics></math>, and any index in the third dimension onwards.
Let <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="I_{\rm l}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">I_{\rm l}</annotation></semantics></math> and <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="I_{\rm r}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">I_{\rm r}</annotation></semantics></math> denote the left and right RGB images from the stereo pair. Each image has dimension <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="3\times H_{0}\times W_{0}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mn id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mi id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">H</mi><mn id="S3.SS1.p1.9.m9.1.1.3.3" xref="S3.SS1.p1.9.m9.1.1.3.3.cmml">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.9.m9.1.1.1a" xref="S3.SS1.p1.9.m9.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p1.9.m9.1.1.4" xref="S3.SS1.p1.9.m9.1.1.4.cmml"><mi id="S3.SS1.p1.9.m9.1.1.4.2" xref="S3.SS1.p1.9.m9.1.1.4.2.cmml">W</mi><mn id="S3.SS1.p1.9.m9.1.1.4.3" xref="S3.SS1.p1.9.m9.1.1.4.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><times id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1"></times><cn type="integer" id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">3</cn><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">ğ»</ci><cn type="integer" id="S3.SS1.p1.9.m9.1.1.3.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3.3">0</cn></apply><apply id="S3.SS1.p1.9.m9.1.1.4.cmml" xref="S3.SS1.p1.9.m9.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.4.1.cmml" xref="S3.SS1.p1.9.m9.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.4.2.cmml" xref="S3.SS1.p1.9.m9.1.1.4.2">ğ‘Š</ci><cn type="integer" id="S3.SS1.p1.9.m9.1.1.4.3.cmml" xref="S3.SS1.p1.9.m9.1.1.4.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">3\times H_{0}\times W_{0}</annotation></semantics></math>.
The left and right images are fed into neural networks <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="\Phi_{\rm l}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">Î¦</mi><mi mathvariant="normal" id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">Î¦</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\Phi_{\rm l}</annotation></semantics></math> and <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="\Phi_{\rm r}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">Î¦</mi><mi mathvariant="normal" id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">Î¦</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\Phi_{\rm r}</annotation></semantics></math>
that featurize each image respectively and output feature volumes <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><msub id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\phi_{\rm l}</annotation></semantics></math> and <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="\phi_{\rm r}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><msub id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\phi_{\rm r}</annotation></semantics></math>. Both <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><msub id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml"><mi id="S3.SS1.p1.14.m14.1.1.2" xref="S3.SS1.p1.14.m14.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p1.14.m14.1.1.3" xref="S3.SS1.p1.14.m14.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m14.1.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p1.14.m14.1.1.2.cmml" xref="S3.SS1.p1.14.m14.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p1.14.m14.1.1.3.cmml" xref="S3.SS1.p1.14.m14.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">\phi_{\rm l}</annotation></semantics></math> and <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="\phi_{\rm r}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><msub id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">\phi_{\rm r}</annotation></semantics></math> have dimension <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="C_{\phi}\times H_{\phi}\times W_{\phi}" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><mrow id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml"><msub id="S3.SS1.p1.16.m16.1.1.2" xref="S3.SS1.p1.16.m16.1.1.2.cmml"><mi id="S3.SS1.p1.16.m16.1.1.2.2" xref="S3.SS1.p1.16.m16.1.1.2.2.cmml">C</mi><mi id="S3.SS1.p1.16.m16.1.1.2.3" xref="S3.SS1.p1.16.m16.1.1.2.3.cmml">Ï•</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.16.m16.1.1.1" xref="S3.SS1.p1.16.m16.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p1.16.m16.1.1.3" xref="S3.SS1.p1.16.m16.1.1.3.cmml"><mi id="S3.SS1.p1.16.m16.1.1.3.2" xref="S3.SS1.p1.16.m16.1.1.3.2.cmml">H</mi><mi id="S3.SS1.p1.16.m16.1.1.3.3" xref="S3.SS1.p1.16.m16.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.16.m16.1.1.1a" xref="S3.SS1.p1.16.m16.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p1.16.m16.1.1.4" xref="S3.SS1.p1.16.m16.1.1.4.cmml"><mi id="S3.SS1.p1.16.m16.1.1.4.2" xref="S3.SS1.p1.16.m16.1.1.4.2.cmml">W</mi><mi id="S3.SS1.p1.16.m16.1.1.4.3" xref="S3.SS1.p1.16.m16.1.1.4.3.cmml">Ï•</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><apply id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1"><times id="S3.SS1.p1.16.m16.1.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1.1"></times><apply id="S3.SS1.p1.16.m16.1.1.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.2.1.cmml" xref="S3.SS1.p1.16.m16.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.2.2.cmml" xref="S3.SS1.p1.16.m16.1.1.2.2">ğ¶</ci><ci id="S3.SS1.p1.16.m16.1.1.2.3.cmml" xref="S3.SS1.p1.16.m16.1.1.2.3">italic-Ï•</ci></apply><apply id="S3.SS1.p1.16.m16.1.1.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.3.1.cmml" xref="S3.SS1.p1.16.m16.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.3.2.cmml" xref="S3.SS1.p1.16.m16.1.1.3.2">ğ»</ci><ci id="S3.SS1.p1.16.m16.1.1.3.3.cmml" xref="S3.SS1.p1.16.m16.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.p1.16.m16.1.1.4.cmml" xref="S3.SS1.p1.16.m16.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.16.m16.1.1.4.1.cmml" xref="S3.SS1.p1.16.m16.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.16.m16.1.1.4.2.cmml" xref="S3.SS1.p1.16.m16.1.1.4.2">ğ‘Š</ci><ci id="S3.SS1.p1.16.m16.1.1.4.3.cmml" xref="S3.SS1.p1.16.m16.1.1.4.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">C_{\phi}\times H_{\phi}\times W_{\phi}</annotation></semantics></math>, where <math id="S3.SS1.p1.17.m17.1" class="ltx_Math" alttext="C_{\phi}" display="inline"><semantics id="S3.SS1.p1.17.m17.1a"><msub id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml"><mi id="S3.SS1.p1.17.m17.1.1.2" xref="S3.SS1.p1.17.m17.1.1.2.cmml">C</mi><mi id="S3.SS1.p1.17.m17.1.1.3" xref="S3.SS1.p1.17.m17.1.1.3.cmml">Ï•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><apply id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.17.m17.1.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p1.17.m17.1.1.2.cmml" xref="S3.SS1.p1.17.m17.1.1.2">ğ¶</ci><ci id="S3.SS1.p1.17.m17.1.1.3.cmml" xref="S3.SS1.p1.17.m17.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">C_{\phi}</annotation></semantics></math> is the number of channels in each feature volume, and <math id="S3.SS1.p1.18.m18.1" class="ltx_Math" alttext="H_{\phi}" display="inline"><semantics id="S3.SS1.p1.18.m18.1a"><msub id="S3.SS1.p1.18.m18.1.1" xref="S3.SS1.p1.18.m18.1.1.cmml"><mi id="S3.SS1.p1.18.m18.1.1.2" xref="S3.SS1.p1.18.m18.1.1.2.cmml">H</mi><mi id="S3.SS1.p1.18.m18.1.1.3" xref="S3.SS1.p1.18.m18.1.1.3.cmml">Ï•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m18.1b"><apply id="S3.SS1.p1.18.m18.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.18.m18.1.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1">subscript</csymbol><ci id="S3.SS1.p1.18.m18.1.1.2.cmml" xref="S3.SS1.p1.18.m18.1.1.2">ğ»</ci><ci id="S3.SS1.p1.18.m18.1.1.3.cmml" xref="S3.SS1.p1.18.m18.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m18.1c">H_{\phi}</annotation></semantics></math> and <math id="S3.SS1.p1.19.m19.1" class="ltx_Math" alttext="W_{\phi}" display="inline"><semantics id="S3.SS1.p1.19.m19.1a"><msub id="S3.SS1.p1.19.m19.1.1" xref="S3.SS1.p1.19.m19.1.1.cmml"><mi id="S3.SS1.p1.19.m19.1.1.2" xref="S3.SS1.p1.19.m19.1.1.2.cmml">W</mi><mi id="S3.SS1.p1.19.m19.1.1.3" xref="S3.SS1.p1.19.m19.1.1.3.cmml">Ï•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m19.1b"><apply id="S3.SS1.p1.19.m19.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1">subscript</csymbol><ci id="S3.SS1.p1.19.m19.1.1.2.cmml" xref="S3.SS1.p1.19.m19.1.1.2">ğ‘Š</ci><ci id="S3.SS1.p1.19.m19.1.1.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m19.1c">W_{\phi}</annotation></semantics></math> are their height and width, respectively. We used a lightweight Dilated ResNet-FPNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> as our feature extractor, to enable large receptive fields with a minimal amount of convolutional layers.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.11" class="ltx_p">The extracted features <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\phi_{\rm l}</annotation></semantics></math> and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\phi_{\rm r}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\phi_{\rm r}</annotation></semantics></math> are fed into a stereo cost volume network <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="f_{\rm cost}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">cost</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğ‘“</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">cost</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">f_{\rm cost}</annotation></semantics></math> that consists of an approximate stereo matching module that searches horizontally in the feature volumes for correspondences within an allowed disparity range. In classical stereo vision literature, correspondences across left and right images can be found by searching along a horizontal line across the images for a match, and the disparity is the difference in the <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">x</annotation></semantics></math> coordinates in the match, which is high for closer points in 3D space and low for farther points. The architecture for <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="f_{\rm cost}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">cost</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ‘“</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">cost</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">f_{\rm cost}</annotation></semantics></math> is heavily inspired by the techniques inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and approximately performs this search.
The first phase of the network <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="f^{(0)}_{\rm cost}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msubsup id="S3.SS1.p2.6.m6.1.2" xref="S3.SS1.p2.6.m6.1.2.cmml"><mi id="S3.SS1.p2.6.m6.1.2.2.2" xref="S3.SS1.p2.6.m6.1.2.2.2.cmml">f</mi><mi id="S3.SS1.p2.6.m6.1.2.3" xref="S3.SS1.p2.6.m6.1.2.3.cmml">cost</mi><mrow id="S3.SS1.p2.6.m6.1.1.1.3" xref="S3.SS1.p2.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.6.m6.1.1.1.3.1" xref="S3.SS1.p2.6.m6.1.2.cmml">(</mo><mn id="S3.SS1.p2.6.m6.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.p2.6.m6.1.1.1.3.2" xref="S3.SS1.p2.6.m6.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.2.cmml" xref="S3.SS1.p2.6.m6.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.2.1.cmml" xref="S3.SS1.p2.6.m6.1.2">subscript</csymbol><apply id="S3.SS1.p2.6.m6.1.2.2.cmml" xref="S3.SS1.p2.6.m6.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.2.2.1.cmml" xref="S3.SS1.p2.6.m6.1.2">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.2.2.2.cmml" xref="S3.SS1.p2.6.m6.1.2.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1">0</cn></apply><ci id="S3.SS1.p2.6.m6.1.2.3.cmml" xref="S3.SS1.p2.6.m6.1.2.3">cost</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">f^{(0)}_{\rm cost}</annotation></semantics></math> computes pixelwise dot products between horizontally shifted versions of the feature volumes. The output of this phase has dimension <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="C_{\rm c}\times H_{\phi}\times W_{\phi}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><msub id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.2.2.cmml">C</mi><mi mathvariant="normal" id="S3.SS1.p2.7.m7.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">H</mi><mi id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.1a" xref="S3.SS1.p2.7.m7.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p2.7.m7.1.1.4" xref="S3.SS1.p2.7.m7.1.1.4.cmml"><mi id="S3.SS1.p2.7.m7.1.1.4.2" xref="S3.SS1.p2.7.m7.1.1.4.2.cmml">W</mi><mi id="S3.SS1.p2.7.m7.1.1.4.3" xref="S3.SS1.p2.7.m7.1.1.4.3.cmml">Ï•</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><times id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></times><apply id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.2">ğ¶</ci><ci id="S3.SS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3">c</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">ğ»</ci><ci id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.4.cmml" xref="S3.SS1.p2.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.4.1.cmml" xref="S3.SS1.p2.7.m7.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.4.2.cmml" xref="S3.SS1.p2.7.m7.1.1.4.2">ğ‘Š</ci><ci id="S3.SS1.p2.7.m7.1.1.4.3.cmml" xref="S3.SS1.p2.7.m7.1.1.4.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">C_{\rm c}\times H_{\phi}\times W_{\phi}</annotation></semantics></math>. <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="2*(C_{\rm c}-1)" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mrow id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mn id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">âˆ—</mo><mrow id="S3.SS1.p2.8.m8.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.8.m8.1.1.1.1.2" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.8.m8.1.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml"><msub id="S3.SS1.p2.8.m8.1.1.1.1.1.2" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.8.m8.1.1.1.1.1.2.2" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.2.cmml">C</mi><mi mathvariant="normal" id="S3.SS1.p2.8.m8.1.1.1.1.1.2.3" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS1.p2.8.m8.1.1.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.8.m8.1.1.1.1.1.3" xref="S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p2.8.m8.1.1.1.1.3" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><times id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2"></times><cn type="integer" id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">2</cn><apply id="S3.SS1.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1"><minus id="S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.1"></minus><apply id="S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.2">ğ¶</ci><ci id="S3.SS1.p2.8.m8.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.3">c</ci></apply><cn type="integer" id="S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">2*(C_{\rm c}-1)</annotation></semantics></math> represents the maximum disparity considered by the network, and the minimum disparity considered is <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mn id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><cn type="integer" id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">0</cn></annotation-xml></semantics></math>. The <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mi id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">i</annotation></semantics></math>-th <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="H_{\rm c}\times W_{\rm c}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><mrow id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><msub id="S3.SS1.p2.11.m11.1.1.2" xref="S3.SS1.p2.11.m11.1.1.2.cmml"><mi id="S3.SS1.p2.11.m11.1.1.2.2" xref="S3.SS1.p2.11.m11.1.1.2.2.cmml">H</mi><mi mathvariant="normal" id="S3.SS1.p2.11.m11.1.1.2.3" xref="S3.SS1.p2.11.m11.1.1.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.11.m11.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p2.11.m11.1.1.3" xref="S3.SS1.p2.11.m11.1.1.3.cmml"><mi id="S3.SS1.p2.11.m11.1.1.3.2" xref="S3.SS1.p2.11.m11.1.1.3.2.cmml">W</mi><mi mathvariant="normal" id="S3.SS1.p2.11.m11.1.1.3.3" xref="S3.SS1.p2.11.m11.1.1.3.3.cmml">c</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><times id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1"></times><apply id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.2.1.cmml" xref="S3.SS1.p2.11.m11.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.2.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2.2">ğ»</ci><ci id="S3.SS1.p2.11.m11.1.1.2.3.cmml" xref="S3.SS1.p2.11.m11.1.1.2.3">c</ci></apply><apply id="S3.SS1.p2.11.m11.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.3.1.cmml" xref="S3.SS1.p2.11.m11.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.3.2.cmml" xref="S3.SS1.p2.11.m11.1.1.3.2">ğ‘Š</ci><ci id="S3.SS1.p2.11.m11.1.1.3.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3.3">c</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">H_{\rm c}\times W_{\rm c}</annotation></semantics></math> slice of the output is computed as:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<table id="A4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.6" class="ltx_math_unparsed" alttext="\displaystyle\vspace{-3ex}f^{(0)}_{\rm cost}(\phi_{\rm l},\phi_{\rm r})_{[i,:,i:]}" display="inline"><semantics id="S3.Ex1.m1.6a"><mrow id="S3.Ex1.m1.6.6"><msubsup id="S3.Ex1.m1.6.6.4"><mi id="S3.Ex1.m1.6.6.4.2.2">f</mi><mi id="S3.Ex1.m1.6.6.4.3">cost</mi><mrow id="S3.Ex1.m1.1.1.1.3"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.3.1">(</mo><mn id="S3.Ex1.m1.1.1.1.1">0</mn><mo stretchy="false" id="S3.Ex1.m1.1.1.1.3.2">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.6.6.3">â€‹</mo><msub id="S3.Ex1.m1.6.6.2"><mrow id="S3.Ex1.m1.6.6.2.2.2"><mo stretchy="false" id="S3.Ex1.m1.6.6.2.2.2.3">(</mo><msub id="S3.Ex1.m1.5.5.1.1.1.1"><mi id="S3.Ex1.m1.5.5.1.1.1.1.2">Ï•</mi><mi mathvariant="normal" id="S3.Ex1.m1.5.5.1.1.1.1.3">l</mi></msub><mo id="S3.Ex1.m1.6.6.2.2.2.4">,</mo><msub id="S3.Ex1.m1.6.6.2.2.2.2"><mi id="S3.Ex1.m1.6.6.2.2.2.2.2">Ï•</mi><mi mathvariant="normal" id="S3.Ex1.m1.6.6.2.2.2.2.3">r</mi></msub><mo stretchy="false" id="S3.Ex1.m1.6.6.2.2.2.5">)</mo></mrow><mrow id="S3.Ex1.m1.4.4.3"><mo stretchy="false" id="S3.Ex1.m1.4.4.3.4">[</mo><mi id="S3.Ex1.m1.2.2.1.1">i</mi><mo id="S3.Ex1.m1.4.4.3.5">,</mo><mo rspace="0em" id="S3.Ex1.m1.3.3.2.2">:</mo><mo id="S3.Ex1.m1.4.4.3.6">,</mo><mi id="S3.Ex1.m1.4.4.3.3">i</mi><mo lspace="0.278em" rspace="0em" id="S3.Ex1.m1.4.4.3.7">:</mo><mo stretchy="false" id="S3.Ex1.m1.4.4.3.8">]</mo></mrow></msub></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m1.6b">\displaystyle\vspace{-3ex}f^{(0)}_{\rm cost}(\phi_{\rm l},\phi_{\rm r})_{[i,:,i:]}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.13" class="ltx_math_unparsed" alttext="\displaystyle=\sum_{j=0}^{C-1}\left(\phi_{\rm l,[i,:,i:]}\odot\phi_{\rm r,[i,:,:W-i]}\right)_{[j]};\hskip 14.45377ptf^{(0)}_{\rm cost}(\phi_{\rm l},\phi_{\rm r})_{[i,:,:i]}=0\vspace{-3ex}" display="inline"><semantics id="S3.Ex1.m2.13a"><mrow id="S3.Ex1.m2.13.13.2"><mrow id="S3.Ex1.m2.12.12.1.1"><mi id="S3.Ex1.m2.12.12.1.1.3"></mi><mo id="S3.Ex1.m2.12.12.1.1.2">=</mo><mrow id="S3.Ex1.m2.12.12.1.1.1"><mstyle displaystyle="true" id="S3.Ex1.m2.12.12.1.1.1.2"><munderover id="S3.Ex1.m2.12.12.1.1.1.2a"><mo movablelimits="false" id="S3.Ex1.m2.12.12.1.1.1.2.2.2">âˆ‘</mo><mrow id="S3.Ex1.m2.12.12.1.1.1.2.2.3"><mi id="S3.Ex1.m2.12.12.1.1.1.2.2.3.2">j</mi><mo id="S3.Ex1.m2.12.12.1.1.1.2.2.3.1">=</mo><mn id="S3.Ex1.m2.12.12.1.1.1.2.2.3.3">0</mn></mrow><mrow id="S3.Ex1.m2.12.12.1.1.1.2.3"><mi id="S3.Ex1.m2.12.12.1.1.1.2.3.2">C</mi><mo id="S3.Ex1.m2.12.12.1.1.1.2.3.1">âˆ’</mo><mn id="S3.Ex1.m2.12.12.1.1.1.2.3.3">1</mn></mrow></munderover></mstyle><msub id="S3.Ex1.m2.12.12.1.1.1.1"><mrow id="S3.Ex1.m2.12.12.1.1.1.1.1.1"><mo id="S3.Ex1.m2.12.12.1.1.1.1.1.1.2">(</mo><mrow id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1"><msub id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1.2"><mi id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1.2.2">Ï•</mi><mrow id="S3.Ex1.m2.4.4.4"><mi mathvariant="normal" id="S3.Ex1.m2.4.4.4.4">l</mi><mo id="S3.Ex1.m2.4.4.4.5">,</mo><mrow id="S3.Ex1.m2.4.4.4.6"><mo stretchy="false" id="S3.Ex1.m2.4.4.4.6.1">[</mo><mi mathvariant="normal" id="S3.Ex1.m2.1.1.1.1">i</mi><mo id="S3.Ex1.m2.4.4.4.6.2">,</mo><mo rspace="0em" id="S3.Ex1.m2.2.2.2.2">:</mo><mo id="S3.Ex1.m2.4.4.4.6.3">,</mo><mi mathvariant="normal" id="S3.Ex1.m2.3.3.3.3">i</mi><mo lspace="0.278em" rspace="0em" id="S3.Ex1.m2.4.4.4.6.4">:</mo><mo stretchy="false" id="S3.Ex1.m2.4.4.4.6.5">]</mo></mrow></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1.1">âŠ™</mo><msub id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1.3"><mi id="S3.Ex1.m2.12.12.1.1.1.1.1.1.1.3.2">Ï•</mi><mrow id="S3.Ex1.m2.7.7.3"><mi mathvariant="normal" id="S3.Ex1.m2.7.7.3.3">r</mi><mo id="S3.Ex1.m2.7.7.3.4">,</mo><mrow id="S3.Ex1.m2.7.7.3.5"><mo stretchy="false" id="S3.Ex1.m2.7.7.3.5.1">[</mo><mi mathvariant="normal" id="S3.Ex1.m2.5.5.1.1">i</mi><mo id="S3.Ex1.m2.7.7.3.5.2">,</mo><mo rspace="0em" id="S3.Ex1.m2.6.6.2.2">:</mo><mo id="S3.Ex1.m2.7.7.3.5.3">,</mo><mo rspace="0.278em" id="S3.Ex1.m2.7.7.3.5.4">:</mo><mi mathvariant="normal" id="S3.Ex1.m2.7.7.3.5.5">W</mi><mo id="S3.Ex1.m2.7.7.3.5.6">âˆ’</mo><mi mathvariant="normal" id="S3.Ex1.m2.7.7.3.5.7">i</mi><mo stretchy="false" id="S3.Ex1.m2.7.7.3.5.8">]</mo></mrow></mrow></msub></mrow><mo id="S3.Ex1.m2.12.12.1.1.1.1.1.1.3">)</mo></mrow><mrow id="S3.Ex1.m2.8.8.1.3"><mo stretchy="false" id="S3.Ex1.m2.8.8.1.3.1">[</mo><mi id="S3.Ex1.m2.8.8.1.1">j</mi><mo stretchy="false" id="S3.Ex1.m2.8.8.1.3.2">]</mo></mrow></msub></mrow></mrow><mo rspace="1.617em" id="S3.Ex1.m2.13.13.2.3">;</mo><mrow id="S3.Ex1.m2.13.13.2.2"><mrow id="S3.Ex1.m2.13.13.2.2.2"><msubsup id="S3.Ex1.m2.13.13.2.2.2.4"><mi id="S3.Ex1.m2.13.13.2.2.2.4.2.2">f</mi><mi id="S3.Ex1.m2.13.13.2.2.2.4.3">cost</mi><mrow id="S3.Ex1.m2.9.9.1.3"><mo stretchy="false" id="S3.Ex1.m2.9.9.1.3.1">(</mo><mn id="S3.Ex1.m2.9.9.1.1">0</mn><mo stretchy="false" id="S3.Ex1.m2.9.9.1.3.2">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.13.13.2.2.2.3">â€‹</mo><msub id="S3.Ex1.m2.13.13.2.2.2.2"><mrow id="S3.Ex1.m2.13.13.2.2.2.2.2.2"><mo stretchy="false" id="S3.Ex1.m2.13.13.2.2.2.2.2.2.3">(</mo><msub id="S3.Ex1.m2.13.13.2.2.1.1.1.1.1"><mi id="S3.Ex1.m2.13.13.2.2.1.1.1.1.1.2">Ï•</mi><mi mathvariant="normal" id="S3.Ex1.m2.13.13.2.2.1.1.1.1.1.3">l</mi></msub><mo id="S3.Ex1.m2.13.13.2.2.2.2.2.2.4">,</mo><msub id="S3.Ex1.m2.13.13.2.2.2.2.2.2.2"><mi id="S3.Ex1.m2.13.13.2.2.2.2.2.2.2.2">Ï•</mi><mi mathvariant="normal" id="S3.Ex1.m2.13.13.2.2.2.2.2.2.2.3">r</mi></msub><mo stretchy="false" id="S3.Ex1.m2.13.13.2.2.2.2.2.2.5">)</mo></mrow><mrow id="S3.Ex1.m2.11.11.2"><mo stretchy="false" id="S3.Ex1.m2.11.11.2.3">[</mo><mi id="S3.Ex1.m2.10.10.1.1">i</mi><mo id="S3.Ex1.m2.11.11.2.4">,</mo><mo rspace="0em" id="S3.Ex1.m2.11.11.2.2">:</mo><mo id="S3.Ex1.m2.11.11.2.5">,</mo><mo rspace="0.278em" id="S3.Ex1.m2.11.11.2.6">:</mo><mi id="S3.Ex1.m2.11.11.2.7">i</mi><mo stretchy="false" id="S3.Ex1.m2.11.11.2.8">]</mo></mrow></msub></mrow><mo id="S3.Ex1.m2.13.13.2.2.3">=</mo><mn id="S3.Ex1.m2.13.13.2.2.4">0</mn></mrow></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m2.13b">\displaystyle=\sum_{j=0}^{C-1}\left(\phi_{\rm l,[i,:,i:]}\odot\phi_{\rm r,[i,:,:W-i]}\right)_{[j]};\hskip 14.45377ptf^{(0)}_{\rm cost}(\phi_{\rm l},\phi_{\rm r})_{[i,:,:i]}=0\vspace{-3ex}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.10" class="ltx_p">The first case takes the rightmost <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="H_{\rm c}-i" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><msub id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2.2" xref="S3.SS1.p4.1.m1.1.1.2.2.cmml">H</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1.2.3" xref="S3.SS1.p4.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">âˆ’</mo><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><minus id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></minus><apply id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.2.1.cmml" xref="S3.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2.2">ğ»</ci><ci id="S3.SS1.p4.1.m1.1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.1.2.3">c</ci></apply><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">H_{\rm c}-i</annotation></semantics></math> columns of the left feature volume <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\phi_{\rm l}</annotation></semantics></math> and computes a pixelwise dot product with the leftmost <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="H_{\rm c}-i" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><msub id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2.2" xref="S3.SS1.p4.3.m3.1.1.2.2.cmml">H</mi><mi mathvariant="normal" id="S3.SS1.p4.3.m3.1.1.2.3" xref="S3.SS1.p4.3.m3.1.1.2.3.cmml">c</mi></msub><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">âˆ’</mo><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><minus id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></minus><apply id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2.2">ğ»</ci><ci id="S3.SS1.p4.3.m3.1.1.2.3.cmml" xref="S3.SS1.p4.3.m3.1.1.2.3">c</ci></apply><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">H_{\rm c}-i</annotation></semantics></math> columns of <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\phi_{\rm r}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\phi_{\rm r}</annotation></semantics></math>. This operation horizontally searches for matches across the two feature volumes at a disparity of <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="2i" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mrow id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mn id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p4.5.m5.1.1.1" xref="S3.SS1.p4.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><times id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1.1"></times><cn type="integer" id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">2</cn><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">2i</annotation></semantics></math>. The next phase of the network <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="f^{(1)}_{\rm cost}" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><msubsup id="S3.SS1.p4.6.m6.1.2" xref="S3.SS1.p4.6.m6.1.2.cmml"><mi id="S3.SS1.p4.6.m6.1.2.2.2" xref="S3.SS1.p4.6.m6.1.2.2.2.cmml">f</mi><mi id="S3.SS1.p4.6.m6.1.2.3" xref="S3.SS1.p4.6.m6.1.2.3.cmml">cost</mi><mrow id="S3.SS1.p4.6.m6.1.1.1.3" xref="S3.SS1.p4.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.6.m6.1.1.1.3.1" xref="S3.SS1.p4.6.m6.1.2.cmml">(</mo><mn id="S3.SS1.p4.6.m6.1.1.1.1" xref="S3.SS1.p4.6.m6.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S3.SS1.p4.6.m6.1.1.1.3.2" xref="S3.SS1.p4.6.m6.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><apply id="S3.SS1.p4.6.m6.1.2.cmml" xref="S3.SS1.p4.6.m6.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.2.1.cmml" xref="S3.SS1.p4.6.m6.1.2">subscript</csymbol><apply id="S3.SS1.p4.6.m6.1.2.2.cmml" xref="S3.SS1.p4.6.m6.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.2.2.1.cmml" xref="S3.SS1.p4.6.m6.1.2">superscript</csymbol><ci id="S3.SS1.p4.6.m6.1.2.2.2.cmml" xref="S3.SS1.p4.6.m6.1.2.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p4.6.m6.1.1.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1.1.1">1</cn></apply><ci id="S3.SS1.p4.6.m6.1.2.3.cmml" xref="S3.SS1.p4.6.m6.1.2.3">cost</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">f^{(1)}_{\rm cost}</annotation></semantics></math> feeds the resulting volume into a sequence of ResNet blocks, which outputs a volume of dimension <math id="S3.SS1.p4.7.m7.1" class="ltx_Math" alttext="C_{\rm c}\times H_{\phi}\times W_{\phi}" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><mrow id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml"><msub id="S3.SS1.p4.7.m7.1.1.2" xref="S3.SS1.p4.7.m7.1.1.2.cmml"><mi id="S3.SS1.p4.7.m7.1.1.2.2" xref="S3.SS1.p4.7.m7.1.1.2.2.cmml">C</mi><mi mathvariant="normal" id="S3.SS1.p4.7.m7.1.1.2.3" xref="S3.SS1.p4.7.m7.1.1.2.3.cmml">c</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.7.m7.1.1.1" xref="S3.SS1.p4.7.m7.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.7.m7.1.1.3" xref="S3.SS1.p4.7.m7.1.1.3.cmml"><mi id="S3.SS1.p4.7.m7.1.1.3.2" xref="S3.SS1.p4.7.m7.1.1.3.2.cmml">H</mi><mi id="S3.SS1.p4.7.m7.1.1.3.3" xref="S3.SS1.p4.7.m7.1.1.3.3.cmml">Ï•</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.7.m7.1.1.1a" xref="S3.SS1.p4.7.m7.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.7.m7.1.1.4" xref="S3.SS1.p4.7.m7.1.1.4.cmml"><mi id="S3.SS1.p4.7.m7.1.1.4.2" xref="S3.SS1.p4.7.m7.1.1.4.2.cmml">W</mi><mi id="S3.SS1.p4.7.m7.1.1.4.3" xref="S3.SS1.p4.7.m7.1.1.4.3.cmml">Ï•</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><apply id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1"><times id="S3.SS1.p4.7.m7.1.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1.1"></times><apply id="S3.SS1.p4.7.m7.1.1.2.cmml" xref="S3.SS1.p4.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.2.1.cmml" xref="S3.SS1.p4.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.2.2.cmml" xref="S3.SS1.p4.7.m7.1.1.2.2">ğ¶</ci><ci id="S3.SS1.p4.7.m7.1.1.2.3.cmml" xref="S3.SS1.p4.7.m7.1.1.2.3">c</ci></apply><apply id="S3.SS1.p4.7.m7.1.1.3.cmml" xref="S3.SS1.p4.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.3.1.cmml" xref="S3.SS1.p4.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.3.2.cmml" xref="S3.SS1.p4.7.m7.1.1.3.2">ğ»</ci><ci id="S3.SS1.p4.7.m7.1.1.3.3.cmml" xref="S3.SS1.p4.7.m7.1.1.3.3">italic-Ï•</ci></apply><apply id="S3.SS1.p4.7.m7.1.1.4.cmml" xref="S3.SS1.p4.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p4.7.m7.1.1.4.1.cmml" xref="S3.SS1.p4.7.m7.1.1.4">subscript</csymbol><ci id="S3.SS1.p4.7.m7.1.1.4.2.cmml" xref="S3.SS1.p4.7.m7.1.1.4.2">ğ‘Š</ci><ci id="S3.SS1.p4.7.m7.1.1.4.3.cmml" xref="S3.SS1.p4.7.m7.1.1.4.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">C_{\rm c}\times H_{\phi}\times W_{\phi}</annotation></semantics></math> before performing a soft argmin along the first axis of the volume. The soft argmin operation approximately finds the disparity for each pixel by locating its best match. The final volume is an estimate of a low-resolution disparity image <math id="S3.SS1.p4.8.m8.2" class="ltx_Math" alttext="\hat{I}_{\rm d,low}" display="inline"><semantics id="S3.SS1.p4.8.m8.2a"><msub id="S3.SS1.p4.8.m8.2.3" xref="S3.SS1.p4.8.m8.2.3.cmml"><mover accent="true" id="S3.SS1.p4.8.m8.2.3.2" xref="S3.SS1.p4.8.m8.2.3.2.cmml"><mi id="S3.SS1.p4.8.m8.2.3.2.2" xref="S3.SS1.p4.8.m8.2.3.2.2.cmml">I</mi><mo id="S3.SS1.p4.8.m8.2.3.2.1" xref="S3.SS1.p4.8.m8.2.3.2.1.cmml">^</mo></mover><mrow id="S3.SS1.p4.8.m8.2.2.2.4" xref="S3.SS1.p4.8.m8.2.2.2.3.cmml"><mi mathvariant="normal" id="S3.SS1.p4.8.m8.1.1.1.1" xref="S3.SS1.p4.8.m8.1.1.1.1.cmml">d</mi><mo id="S3.SS1.p4.8.m8.2.2.2.4.1" xref="S3.SS1.p4.8.m8.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p4.8.m8.2.2.2.2" xref="S3.SS1.p4.8.m8.2.2.2.2.cmml">low</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.2b"><apply id="S3.SS1.p4.8.m8.2.3.cmml" xref="S3.SS1.p4.8.m8.2.3"><csymbol cd="ambiguous" id="S3.SS1.p4.8.m8.2.3.1.cmml" xref="S3.SS1.p4.8.m8.2.3">subscript</csymbol><apply id="S3.SS1.p4.8.m8.2.3.2.cmml" xref="S3.SS1.p4.8.m8.2.3.2"><ci id="S3.SS1.p4.8.m8.2.3.2.1.cmml" xref="S3.SS1.p4.8.m8.2.3.2.1">^</ci><ci id="S3.SS1.p4.8.m8.2.3.2.2.cmml" xref="S3.SS1.p4.8.m8.2.3.2.2">ğ¼</ci></apply><list id="S3.SS1.p4.8.m8.2.2.2.3.cmml" xref="S3.SS1.p4.8.m8.2.2.2.4"><ci id="S3.SS1.p4.8.m8.1.1.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1.1.1">d</ci><ci id="S3.SS1.p4.8.m8.2.2.2.2.cmml" xref="S3.SS1.p4.8.m8.2.2.2.2">low</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.2c">\hat{I}_{\rm d,low}</annotation></semantics></math> with shape <math id="S3.SS1.p4.9.m9.1" class="ltx_Math" alttext="H_{\phi}\times W_{\phi}" display="inline"><semantics id="S3.SS1.p4.9.m9.1a"><mrow id="S3.SS1.p4.9.m9.1.1" xref="S3.SS1.p4.9.m9.1.1.cmml"><msub id="S3.SS1.p4.9.m9.1.1.2" xref="S3.SS1.p4.9.m9.1.1.2.cmml"><mi id="S3.SS1.p4.9.m9.1.1.2.2" xref="S3.SS1.p4.9.m9.1.1.2.2.cmml">H</mi><mi id="S3.SS1.p4.9.m9.1.1.2.3" xref="S3.SS1.p4.9.m9.1.1.2.3.cmml">Ï•</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.9.m9.1.1.1" xref="S3.SS1.p4.9.m9.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.9.m9.1.1.3" xref="S3.SS1.p4.9.m9.1.1.3.cmml"><mi id="S3.SS1.p4.9.m9.1.1.3.2" xref="S3.SS1.p4.9.m9.1.1.3.2.cmml">W</mi><mi id="S3.SS1.p4.9.m9.1.1.3.3" xref="S3.SS1.p4.9.m9.1.1.3.3.cmml">Ï•</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m9.1b"><apply id="S3.SS1.p4.9.m9.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1"><times id="S3.SS1.p4.9.m9.1.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1.1"></times><apply id="S3.SS1.p4.9.m9.1.1.2.cmml" xref="S3.SS1.p4.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m9.1.1.2.1.cmml" xref="S3.SS1.p4.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.9.m9.1.1.2.2.cmml" xref="S3.SS1.p4.9.m9.1.1.2.2">ğ»</ci><ci id="S3.SS1.p4.9.m9.1.1.2.3.cmml" xref="S3.SS1.p4.9.m9.1.1.2.3">italic-Ï•</ci></apply><apply id="S3.SS1.p4.9.m9.1.1.3.cmml" xref="S3.SS1.p4.9.m9.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.9.m9.1.1.3.1.cmml" xref="S3.SS1.p4.9.m9.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.9.m9.1.1.3.2.cmml" xref="S3.SS1.p4.9.m9.1.1.3.2">ğ‘Š</ci><ci id="S3.SS1.p4.9.m9.1.1.3.3.cmml" xref="S3.SS1.p4.9.m9.1.1.3.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m9.1c">H_{\phi}\times W_{\phi}</annotation></semantics></math>. We denote <math id="S3.SS1.p4.10.m10.2" class="ltx_Math" alttext="f_{\rm cost}=f^{(1)}_{\rm cost}\circ f^{(0)}_{\rm cost}" display="inline"><semantics id="S3.SS1.p4.10.m10.2a"><mrow id="S3.SS1.p4.10.m10.2.3" xref="S3.SS1.p4.10.m10.2.3.cmml"><msub id="S3.SS1.p4.10.m10.2.3.2" xref="S3.SS1.p4.10.m10.2.3.2.cmml"><mi id="S3.SS1.p4.10.m10.2.3.2.2" xref="S3.SS1.p4.10.m10.2.3.2.2.cmml">f</mi><mi id="S3.SS1.p4.10.m10.2.3.2.3" xref="S3.SS1.p4.10.m10.2.3.2.3.cmml">cost</mi></msub><mo id="S3.SS1.p4.10.m10.2.3.1" xref="S3.SS1.p4.10.m10.2.3.1.cmml">=</mo><mrow id="S3.SS1.p4.10.m10.2.3.3" xref="S3.SS1.p4.10.m10.2.3.3.cmml"><msubsup id="S3.SS1.p4.10.m10.2.3.3.2" xref="S3.SS1.p4.10.m10.2.3.3.2.cmml"><mi id="S3.SS1.p4.10.m10.2.3.3.2.2.2" xref="S3.SS1.p4.10.m10.2.3.3.2.2.2.cmml">f</mi><mi id="S3.SS1.p4.10.m10.2.3.3.2.3" xref="S3.SS1.p4.10.m10.2.3.3.2.3.cmml">cost</mi><mrow id="S3.SS1.p4.10.m10.1.1.1.3" xref="S3.SS1.p4.10.m10.2.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.p4.10.m10.1.1.1.3.1" xref="S3.SS1.p4.10.m10.2.3.3.2.cmml">(</mo><mn id="S3.SS1.p4.10.m10.1.1.1.1" xref="S3.SS1.p4.10.m10.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S3.SS1.p4.10.m10.1.1.1.3.2" xref="S3.SS1.p4.10.m10.2.3.3.2.cmml">)</mo></mrow></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.10.m10.2.3.3.1" xref="S3.SS1.p4.10.m10.2.3.3.1.cmml">âˆ˜</mo><msubsup id="S3.SS1.p4.10.m10.2.3.3.3" xref="S3.SS1.p4.10.m10.2.3.3.3.cmml"><mi id="S3.SS1.p4.10.m10.2.3.3.3.2.2" xref="S3.SS1.p4.10.m10.2.3.3.3.2.2.cmml">f</mi><mi id="S3.SS1.p4.10.m10.2.3.3.3.3" xref="S3.SS1.p4.10.m10.2.3.3.3.3.cmml">cost</mi><mrow id="S3.SS1.p4.10.m10.2.2.1.3" xref="S3.SS1.p4.10.m10.2.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p4.10.m10.2.2.1.3.1" xref="S3.SS1.p4.10.m10.2.3.3.3.cmml">(</mo><mn id="S3.SS1.p4.10.m10.2.2.1.1" xref="S3.SS1.p4.10.m10.2.2.1.1.cmml">0</mn><mo stretchy="false" id="S3.SS1.p4.10.m10.2.2.1.3.2" xref="S3.SS1.p4.10.m10.2.3.3.3.cmml">)</mo></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m10.2b"><apply id="S3.SS1.p4.10.m10.2.3.cmml" xref="S3.SS1.p4.10.m10.2.3"><eq id="S3.SS1.p4.10.m10.2.3.1.cmml" xref="S3.SS1.p4.10.m10.2.3.1"></eq><apply id="S3.SS1.p4.10.m10.2.3.2.cmml" xref="S3.SS1.p4.10.m10.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.2.3.2.1.cmml" xref="S3.SS1.p4.10.m10.2.3.2">subscript</csymbol><ci id="S3.SS1.p4.10.m10.2.3.2.2.cmml" xref="S3.SS1.p4.10.m10.2.3.2.2">ğ‘“</ci><ci id="S3.SS1.p4.10.m10.2.3.2.3.cmml" xref="S3.SS1.p4.10.m10.2.3.2.3">cost</ci></apply><apply id="S3.SS1.p4.10.m10.2.3.3.cmml" xref="S3.SS1.p4.10.m10.2.3.3"><compose id="S3.SS1.p4.10.m10.2.3.3.1.cmml" xref="S3.SS1.p4.10.m10.2.3.3.1"></compose><apply id="S3.SS1.p4.10.m10.2.3.3.2.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.2.3.3.2.1.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2">subscript</csymbol><apply id="S3.SS1.p4.10.m10.2.3.3.2.2.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.2.3.3.2.2.1.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2">superscript</csymbol><ci id="S3.SS1.p4.10.m10.2.3.3.2.2.2.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p4.10.m10.1.1.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1.1.1">1</cn></apply><ci id="S3.SS1.p4.10.m10.2.3.3.2.3.cmml" xref="S3.SS1.p4.10.m10.2.3.3.2.3">cost</ci></apply><apply id="S3.SS1.p4.10.m10.2.3.3.3.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.2.3.3.3.1.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3">subscript</csymbol><apply id="S3.SS1.p4.10.m10.2.3.3.3.2.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.10.m10.2.3.3.3.2.1.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3">superscript</csymbol><ci id="S3.SS1.p4.10.m10.2.3.3.3.2.2.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p4.10.m10.2.2.1.1.cmml" xref="S3.SS1.p4.10.m10.2.2.1.1">0</cn></apply><ci id="S3.SS1.p4.10.m10.2.3.3.3.3.cmml" xref="S3.SS1.p4.10.m10.2.3.3.3.3">cost</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m10.2c">f_{\rm cost}=f^{(1)}_{\rm cost}\circ f^{(0)}_{\rm cost}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.9" class="ltx_p"><span id="S3.SS1.p5.9.1" class="ltx_text ltx_font_bold">Disparity Auxiliary Loss</span>
In addition to the losses for the high-level perception heads, we additionally train the weights of <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\Phi_{l}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><msub id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">Î¦</ci><ci id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\Phi_{l}</annotation></semantics></math>, <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="\Phi_{r}" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><msub id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">Î¦</mi><mi id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">Î¦</ci><ci id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\Phi_{r}</annotation></semantics></math>, and <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="f_{\rm cost}" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><msub id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">f</mi><mi id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">cost</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">ğ‘“</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">cost</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">f_{\rm cost}</annotation></semantics></math> by minimizing an auxiliary depth reconstruction loss function. In particular, the loss function takes in a target disparity image <math id="S3.SS1.p5.4.m4.2" class="ltx_Math" alttext="I_{\rm targ,d}" display="inline"><semantics id="S3.SS1.p5.4.m4.2a"><msub id="S3.SS1.p5.4.m4.2.3" xref="S3.SS1.p5.4.m4.2.3.cmml"><mi id="S3.SS1.p5.4.m4.2.3.2" xref="S3.SS1.p5.4.m4.2.3.2.cmml">I</mi><mrow id="S3.SS1.p5.4.m4.2.2.2.4" xref="S3.SS1.p5.4.m4.2.2.2.3.cmml"><mi id="S3.SS1.p5.4.m4.1.1.1.1" xref="S3.SS1.p5.4.m4.1.1.1.1.cmml">targ</mi><mo id="S3.SS1.p5.4.m4.2.2.2.4.1" xref="S3.SS1.p5.4.m4.2.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p5.4.m4.2.2.2.2" xref="S3.SS1.p5.4.m4.2.2.2.2.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.2b"><apply id="S3.SS1.p5.4.m4.2.3.cmml" xref="S3.SS1.p5.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.2.3.1.cmml" xref="S3.SS1.p5.4.m4.2.3">subscript</csymbol><ci id="S3.SS1.p5.4.m4.2.3.2.cmml" xref="S3.SS1.p5.4.m4.2.3.2">ğ¼</ci><list id="S3.SS1.p5.4.m4.2.2.2.3.cmml" xref="S3.SS1.p5.4.m4.2.2.2.4"><ci id="S3.SS1.p5.4.m4.1.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1.1.1">targ</ci><ci id="S3.SS1.p5.4.m4.2.2.2.2.cmml" xref="S3.SS1.p5.4.m4.2.2.2.2">d</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.2c">I_{\rm targ,d}</annotation></semantics></math> of dimension <math id="S3.SS1.p5.5.m5.1" class="ltx_Math" alttext="H_{0}\times W_{0}" display="inline"><semantics id="S3.SS1.p5.5.m5.1a"><mrow id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml"><msub id="S3.SS1.p5.5.m5.1.1.2" xref="S3.SS1.p5.5.m5.1.1.2.cmml"><mi id="S3.SS1.p5.5.m5.1.1.2.2" xref="S3.SS1.p5.5.m5.1.1.2.2.cmml">H</mi><mn id="S3.SS1.p5.5.m5.1.1.2.3" xref="S3.SS1.p5.5.m5.1.1.2.3.cmml">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.5.m5.1.1.1" xref="S3.SS1.p5.5.m5.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p5.5.m5.1.1.3" xref="S3.SS1.p5.5.m5.1.1.3.cmml"><mi id="S3.SS1.p5.5.m5.1.1.3.2" xref="S3.SS1.p5.5.m5.1.1.3.2.cmml">W</mi><mn id="S3.SS1.p5.5.m5.1.1.3.3" xref="S3.SS1.p5.5.m5.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><apply id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1"><times id="S3.SS1.p5.5.m5.1.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1.1"></times><apply id="S3.SS1.p5.5.m5.1.1.2.cmml" xref="S3.SS1.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p5.5.m5.1.1.2.1.cmml" xref="S3.SS1.p5.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p5.5.m5.1.1.2.2.cmml" xref="S3.SS1.p5.5.m5.1.1.2.2">ğ»</ci><cn type="integer" id="S3.SS1.p5.5.m5.1.1.2.3.cmml" xref="S3.SS1.p5.5.m5.1.1.2.3">0</cn></apply><apply id="S3.SS1.p5.5.m5.1.1.3.cmml" xref="S3.SS1.p5.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.5.m5.1.1.3.1.cmml" xref="S3.SS1.p5.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS1.p5.5.m5.1.1.3.2.cmml" xref="S3.SS1.p5.5.m5.1.1.3.2">ğ‘Š</ci><cn type="integer" id="S3.SS1.p5.5.m5.1.1.3.3.cmml" xref="S3.SS1.p5.5.m5.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">H_{0}\times W_{0}</annotation></semantics></math>, downsamples it by a factor of <math id="S3.SS1.p5.6.m6.1" class="ltx_Math" alttext="H_{0}/H_{\phi}" display="inline"><semantics id="S3.SS1.p5.6.m6.1a"><mrow id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml"><msub id="S3.SS1.p5.6.m6.1.1.2" xref="S3.SS1.p5.6.m6.1.1.2.cmml"><mi id="S3.SS1.p5.6.m6.1.1.2.2" xref="S3.SS1.p5.6.m6.1.1.2.2.cmml">H</mi><mn id="S3.SS1.p5.6.m6.1.1.2.3" xref="S3.SS1.p5.6.m6.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS1.p5.6.m6.1.1.1" xref="S3.SS1.p5.6.m6.1.1.1.cmml">/</mo><msub id="S3.SS1.p5.6.m6.1.1.3" xref="S3.SS1.p5.6.m6.1.1.3.cmml"><mi id="S3.SS1.p5.6.m6.1.1.3.2" xref="S3.SS1.p5.6.m6.1.1.3.2.cmml">H</mi><mi id="S3.SS1.p5.6.m6.1.1.3.3" xref="S3.SS1.p5.6.m6.1.1.3.3.cmml">Ï•</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><apply id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1"><divide id="S3.SS1.p5.6.m6.1.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1.1"></divide><apply id="S3.SS1.p5.6.m6.1.1.2.cmml" xref="S3.SS1.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p5.6.m6.1.1.2.1.cmml" xref="S3.SS1.p5.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p5.6.m6.1.1.2.2.cmml" xref="S3.SS1.p5.6.m6.1.1.2.2">ğ»</ci><cn type="integer" id="S3.SS1.p5.6.m6.1.1.2.3.cmml" xref="S3.SS1.p5.6.m6.1.1.2.3">0</cn></apply><apply id="S3.SS1.p5.6.m6.1.1.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.6.m6.1.1.3.1.cmml" xref="S3.SS1.p5.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p5.6.m6.1.1.3.2.cmml" xref="S3.SS1.p5.6.m6.1.1.3.2">ğ»</ci><ci id="S3.SS1.p5.6.m6.1.1.3.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">H_{0}/H_{\phi}</annotation></semantics></math> and then computes the Huber lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> <math id="S3.SS1.p5.7.m7.2" class="ltx_Math" alttext="\ell_{\rm d,small}" display="inline"><semantics id="S3.SS1.p5.7.m7.2a"><msub id="S3.SS1.p5.7.m7.2.3" xref="S3.SS1.p5.7.m7.2.3.cmml"><mi mathvariant="normal" id="S3.SS1.p5.7.m7.2.3.2" xref="S3.SS1.p5.7.m7.2.3.2.cmml">â„“</mi><mrow id="S3.SS1.p5.7.m7.2.2.2.4" xref="S3.SS1.p5.7.m7.2.2.2.3.cmml"><mi mathvariant="normal" id="S3.SS1.p5.7.m7.1.1.1.1" xref="S3.SS1.p5.7.m7.1.1.1.1.cmml">d</mi><mo id="S3.SS1.p5.7.m7.2.2.2.4.1" xref="S3.SS1.p5.7.m7.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p5.7.m7.2.2.2.2" xref="S3.SS1.p5.7.m7.2.2.2.2.cmml">small</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.7.m7.2b"><apply id="S3.SS1.p5.7.m7.2.3.cmml" xref="S3.SS1.p5.7.m7.2.3"><csymbol cd="ambiguous" id="S3.SS1.p5.7.m7.2.3.1.cmml" xref="S3.SS1.p5.7.m7.2.3">subscript</csymbol><ci id="S3.SS1.p5.7.m7.2.3.2.cmml" xref="S3.SS1.p5.7.m7.2.3.2">â„“</ci><list id="S3.SS1.p5.7.m7.2.2.2.3.cmml" xref="S3.SS1.p5.7.m7.2.2.2.4"><ci id="S3.SS1.p5.7.m7.1.1.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1.1.1">d</ci><ci id="S3.SS1.p5.7.m7.2.2.2.2.cmml" xref="S3.SS1.p5.7.m7.2.2.2.2">small</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.7.m7.2c">\ell_{\rm d,small}</annotation></semantics></math> of it with the low-resolution depth prediction <math id="S3.SS1.p5.8.m8.2" class="ltx_Math" alttext="f_{\rm cost}(\phi_{l},\phi_{r})" display="inline"><semantics id="S3.SS1.p5.8.m8.2a"><mrow id="S3.SS1.p5.8.m8.2.2" xref="S3.SS1.p5.8.m8.2.2.cmml"><msub id="S3.SS1.p5.8.m8.2.2.4" xref="S3.SS1.p5.8.m8.2.2.4.cmml"><mi id="S3.SS1.p5.8.m8.2.2.4.2" xref="S3.SS1.p5.8.m8.2.2.4.2.cmml">f</mi><mi id="S3.SS1.p5.8.m8.2.2.4.3" xref="S3.SS1.p5.8.m8.2.2.4.3.cmml">cost</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.8.m8.2.2.3" xref="S3.SS1.p5.8.m8.2.2.3.cmml">â€‹</mo><mrow id="S3.SS1.p5.8.m8.2.2.2.2" xref="S3.SS1.p5.8.m8.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p5.8.m8.2.2.2.2.3" xref="S3.SS1.p5.8.m8.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p5.8.m8.1.1.1.1.1" xref="S3.SS1.p5.8.m8.1.1.1.1.1.cmml"><mi id="S3.SS1.p5.8.m8.1.1.1.1.1.2" xref="S3.SS1.p5.8.m8.1.1.1.1.1.2.cmml">Ï•</mi><mi id="S3.SS1.p5.8.m8.1.1.1.1.1.3" xref="S3.SS1.p5.8.m8.1.1.1.1.1.3.cmml">l</mi></msub><mo id="S3.SS1.p5.8.m8.2.2.2.2.4" xref="S3.SS1.p5.8.m8.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p5.8.m8.2.2.2.2.2" xref="S3.SS1.p5.8.m8.2.2.2.2.2.cmml"><mi id="S3.SS1.p5.8.m8.2.2.2.2.2.2" xref="S3.SS1.p5.8.m8.2.2.2.2.2.2.cmml">Ï•</mi><mi id="S3.SS1.p5.8.m8.2.2.2.2.2.3" xref="S3.SS1.p5.8.m8.2.2.2.2.2.3.cmml">r</mi></msub><mo stretchy="false" id="S3.SS1.p5.8.m8.2.2.2.2.5" xref="S3.SS1.p5.8.m8.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.8.m8.2b"><apply id="S3.SS1.p5.8.m8.2.2.cmml" xref="S3.SS1.p5.8.m8.2.2"><times id="S3.SS1.p5.8.m8.2.2.3.cmml" xref="S3.SS1.p5.8.m8.2.2.3"></times><apply id="S3.SS1.p5.8.m8.2.2.4.cmml" xref="S3.SS1.p5.8.m8.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.2.2.4.1.cmml" xref="S3.SS1.p5.8.m8.2.2.4">subscript</csymbol><ci id="S3.SS1.p5.8.m8.2.2.4.2.cmml" xref="S3.SS1.p5.8.m8.2.2.4.2">ğ‘“</ci><ci id="S3.SS1.p5.8.m8.2.2.4.3.cmml" xref="S3.SS1.p5.8.m8.2.2.4.3">cost</ci></apply><interval closure="open" id="S3.SS1.p5.8.m8.2.2.2.3.cmml" xref="S3.SS1.p5.8.m8.2.2.2.2"><apply id="S3.SS1.p5.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p5.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS1.p5.8.m8.1.1.1.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p5.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS1.p5.8.m8.1.1.1.1.1.3">ğ‘™</ci></apply><apply id="S3.SS1.p5.8.m8.2.2.2.2.2.cmml" xref="S3.SS1.p5.8.m8.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.2.2.2.2.2.1.cmml" xref="S3.SS1.p5.8.m8.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p5.8.m8.2.2.2.2.2.2.cmml" xref="S3.SS1.p5.8.m8.2.2.2.2.2.2">italic-Ï•</ci><ci id="S3.SS1.p5.8.m8.2.2.2.2.2.3.cmml" xref="S3.SS1.p5.8.m8.2.2.2.2.2.3">ğ‘Ÿ</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.8.m8.2c">f_{\rm cost}(\phi_{l},\phi_{r})</annotation></semantics></math>. That is, the network weights are trained to minimize <math id="S3.SS1.p5.9.m9.6" class="ltx_Math" alttext="\ell_{\rm d,small}(f_{\rm cost}(\phi_{l},\phi_{r}),\texttt{downsample}(I_{\rm targ,d},H_{0}/H_{\phi}))" display="inline"><semantics id="S3.SS1.p5.9.m9.6a"><mrow id="S3.SS1.p5.9.m9.6.6" xref="S3.SS1.p5.9.m9.6.6.cmml"><msub id="S3.SS1.p5.9.m9.6.6.4" xref="S3.SS1.p5.9.m9.6.6.4.cmml"><mi mathvariant="normal" id="S3.SS1.p5.9.m9.6.6.4.2" xref="S3.SS1.p5.9.m9.6.6.4.2.cmml">â„“</mi><mrow id="S3.SS1.p5.9.m9.2.2.2.4" xref="S3.SS1.p5.9.m9.2.2.2.3.cmml"><mi mathvariant="normal" id="S3.SS1.p5.9.m9.1.1.1.1" xref="S3.SS1.p5.9.m9.1.1.1.1.cmml">d</mi><mo id="S3.SS1.p5.9.m9.2.2.2.4.1" xref="S3.SS1.p5.9.m9.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p5.9.m9.2.2.2.2" xref="S3.SS1.p5.9.m9.2.2.2.2.cmml">small</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.9.m9.6.6.3" xref="S3.SS1.p5.9.m9.6.6.3.cmml">â€‹</mo><mrow id="S3.SS1.p5.9.m9.6.6.2.2" xref="S3.SS1.p5.9.m9.6.6.2.3.cmml"><mo stretchy="false" id="S3.SS1.p5.9.m9.6.6.2.2.3" xref="S3.SS1.p5.9.m9.6.6.2.3.cmml">(</mo><mrow id="S3.SS1.p5.9.m9.5.5.1.1.1" xref="S3.SS1.p5.9.m9.5.5.1.1.1.cmml"><msub id="S3.SS1.p5.9.m9.5.5.1.1.1.4" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4.cmml"><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.4.2" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4.2.cmml">f</mi><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.4.3" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4.3.cmml">cost</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.9.m9.5.5.1.1.1.3" xref="S3.SS1.p5.9.m9.5.5.1.1.1.3.cmml">â€‹</mo><mrow id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.3" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.3.cmml">(</mo><msub id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.2" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.2.cmml">Ï•</mi><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.3" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.3.cmml">l</mi></msub><mo id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.4" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.3.cmml">,</mo><msub id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.cmml"><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.2" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.2.cmml">Ï•</mi><mi id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.3" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.3.cmml">r</mi></msub><mo stretchy="false" id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.5" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p5.9.m9.6.6.2.2.4" xref="S3.SS1.p5.9.m9.6.6.2.3.cmml">,</mo><mrow id="S3.SS1.p5.9.m9.6.6.2.2.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p5.9.m9.6.6.2.2.2.4" xref="S3.SS1.p5.9.m9.6.6.2.2.2.4a.cmml">downsample</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p5.9.m9.6.6.2.2.2.3" xref="S3.SS1.p5.9.m9.6.6.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.3" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1" xref="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.2.cmml">I</mi><mrow id="S3.SS1.p5.9.m9.4.4.2.4" xref="S3.SS1.p5.9.m9.4.4.2.3.cmml"><mi id="S3.SS1.p5.9.m9.3.3.1.1" xref="S3.SS1.p5.9.m9.3.3.1.1.cmml">targ</mi><mo id="S3.SS1.p5.9.m9.4.4.2.4.1" xref="S3.SS1.p5.9.m9.4.4.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p5.9.m9.4.4.2.2" xref="S3.SS1.p5.9.m9.4.4.2.2.cmml">d</mi></mrow></msub><mo id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.4" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.cmml"><msub id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.2.cmml">H</mi><mn id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.3" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.1" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.1.cmml">/</mo><msub id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.cmml"><mi id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.2" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.2.cmml">H</mi><mi id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.3" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.3.cmml">Ï•</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.5" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.p5.9.m9.6.6.2.2.5" xref="S3.SS1.p5.9.m9.6.6.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.9.m9.6b"><apply id="S3.SS1.p5.9.m9.6.6.cmml" xref="S3.SS1.p5.9.m9.6.6"><times id="S3.SS1.p5.9.m9.6.6.3.cmml" xref="S3.SS1.p5.9.m9.6.6.3"></times><apply id="S3.SS1.p5.9.m9.6.6.4.cmml" xref="S3.SS1.p5.9.m9.6.6.4"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.6.6.4.1.cmml" xref="S3.SS1.p5.9.m9.6.6.4">subscript</csymbol><ci id="S3.SS1.p5.9.m9.6.6.4.2.cmml" xref="S3.SS1.p5.9.m9.6.6.4.2">â„“</ci><list id="S3.SS1.p5.9.m9.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.2.2.2.4"><ci id="S3.SS1.p5.9.m9.1.1.1.1.cmml" xref="S3.SS1.p5.9.m9.1.1.1.1">d</ci><ci id="S3.SS1.p5.9.m9.2.2.2.2.cmml" xref="S3.SS1.p5.9.m9.2.2.2.2">small</ci></list></apply><interval closure="open" id="S3.SS1.p5.9.m9.6.6.2.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2"><apply id="S3.SS1.p5.9.m9.5.5.1.1.1.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1"><times id="S3.SS1.p5.9.m9.5.5.1.1.1.3.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.3"></times><apply id="S3.SS1.p5.9.m9.5.5.1.1.1.4.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.5.5.1.1.1.4.1.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4">subscript</csymbol><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.4.2.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4.2">ğ‘“</ci><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.4.3.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.4.3">cost</ci></apply><interval closure="open" id="S3.SS1.p5.9.m9.5.5.1.1.1.2.3.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2"><apply id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.1.1.1.3">ğ‘™</ci></apply><apply id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.1.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.2.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.2">italic-Ï•</ci><ci id="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.5.5.1.1.1.2.2.2.3">ğ‘Ÿ</ci></apply></interval></apply><apply id="S3.SS1.p5.9.m9.6.6.2.2.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2"><times id="S3.SS1.p5.9.m9.6.6.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.3"></times><ci id="S3.SS1.p5.9.m9.6.6.2.2.2.4a.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.4"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p5.9.m9.6.6.2.2.2.4.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.4">downsample</mtext></ci><interval closure="open" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2"><apply id="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.1.1.1.2">ğ¼</ci><list id="S3.SS1.p5.9.m9.4.4.2.3.cmml" xref="S3.SS1.p5.9.m9.4.4.2.4"><ci id="S3.SS1.p5.9.m9.3.3.1.1.cmml" xref="S3.SS1.p5.9.m9.3.3.1.1">targ</ci><ci id="S3.SS1.p5.9.m9.4.4.2.2.cmml" xref="S3.SS1.p5.9.m9.4.4.2.2">d</ci></list></apply><apply id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2"><divide id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.1"></divide><apply id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.2">ğ»</ci><cn type="integer" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.2.3">0</cn></apply><apply id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.2.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.2">ğ»</ci><ci id="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.3.cmml" xref="S3.SS1.p5.9.m9.6.6.2.2.2.2.2.2.3.3">italic-Ï•</ci></apply></apply></interval></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.9.m9.6c">\ell_{\rm d,small}(f_{\rm cost}(\phi_{l},\phi_{r}),\texttt{downsample}(I_{\rm targ,d},H_{0}/H_{\phi}))</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Extracting High-Level Predictions for Manipulation Tasks</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.3" class="ltx_p">Given a SCVN to extract geometric features from stereo images, we need to learn on top of it high-level predictions relevant to manipulation.
To design a backbone for robust simulation-trained manipulation, we feed the output of a SVCN, <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="\hat{I}_{\rm d,low}" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><msub id="S3.SS2.p1.1.m1.2.3" xref="S3.SS2.p1.1.m1.2.3.cmml"><mover accent="true" id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.2.cmml"><mi id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.2.2.cmml">I</mi><mo id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.2.1.cmml">^</mo></mover><mrow id="S3.SS2.p1.1.m1.2.2.2.4" xref="S3.SS2.p1.1.m1.2.2.2.3.cmml"><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.cmml">d</mi><mo id="S3.SS2.p1.1.m1.2.2.2.4.1" xref="S3.SS2.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2.2.2" xref="S3.SS2.p1.1.m1.2.2.2.2.cmml">low</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><apply id="S3.SS2.p1.1.m1.2.3.cmml" xref="S3.SS2.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3">subscript</csymbol><apply id="S3.SS2.p1.1.m1.2.3.2.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.2.3.2.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2.1">^</ci><ci id="S3.SS2.p1.1.m1.2.3.2.2.cmml" xref="S3.SS2.p1.1.m1.2.3.2.2">ğ¼</ci></apply><list id="S3.SS2.p1.1.m1.2.2.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.2.4"><ci id="S3.SS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1">d</ci><ci id="S3.SS2.p1.1.m1.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2">low</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">\hat{I}_{\rm d,low}</annotation></semantics></math> , into a Resnet18-FPNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> feature backbone <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="f_{\rm backbone}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">backbone</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘“</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">backbone</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">f_{\rm backbone}</annotation></semantics></math>. Additionally, we leverage early stage features from the left RGB image, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="I_{\rm l}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ¼</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">I_{\rm l}</annotation></semantics></math>, to enable high resolution texture information to be considered at inference time, similar to Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The features are extracted from the ResNet stem and concatenate with the output of the SCVN and fed into the backbone. The output of the backbone is fed into each of the prediction heads.
In this section, we describe each of the prediction heads of SimNet (Fig.Â <a href="#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and the losses used to train it. Each prediction head high uses the up-scaling branch defined inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, which aggregates different resolutions across the feature extractor.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Room Level Segmentation:</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Given a mobile robot that is trying to manipulate objects on a table, it is useful to be able to know where the table is in the room and the objects on the table. We can express this level of scene understanding as segmentation problem consisting of predicting three categories: surfaces, objects and background. Cross-entropy loss <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="l_{\rm seg}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">seg</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">seg</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">l_{\rm seg}</annotation></semantics></math> is used for training, as in prior workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Oriented Bounding Boxes:</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p1.6" class="ltx_p">Detection of an OBB requires determining individual object instances as well as estimating translation, <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="t\in\mathbb{R}^{3}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">t</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">â„</mi><mn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><in id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">ğ‘¡</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">â„</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">t\in\mathbb{R}^{3}</annotation></semantics></math>, scale <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="S\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">S</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.cmml"><mn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><in id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1"></in><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">ğ‘†</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2">â„</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3"><times id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">S\in\mathbb{R}^{3\times 3}</annotation></semantics></math>, and rotation, <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="R\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">R</mi><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml"><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><in id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1"></in><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">ğ‘…</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2">â„</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">R\in\mathbb{R}^{3\times 3}</annotation></semantics></math>, of the encompassing OBB. We can recover these parameters by using four different output heads. First to recover object instances, we regress a <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="W_{0}\times H_{0}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml"><msub id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">W</mi><mn id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.3.cmml">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml">Ã—</mo><msub id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.cmml">H</mi><mn id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1"><times id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1"></times><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.2.3">0</cn></apply><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.2">ğ»</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">W_{0}\times H_{0}</annotation></semantics></math> image,which is the resolution of the input left image, where for each object in the image a Gaussian heatmap is predicted. Instances can then be derived using peak detection as in Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. We use an <math id="S3.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">L_{1}</annotation></semantics></math> loss on this output head and denote the loss as <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="l_{\rm inst}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">inst</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3">inst</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">l_{\rm inst}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p2.6" class="ltx_p">Given instances of object, we can now regress the remaining 9-DOF pose parameters. To recover scale and translation, we first regress a <math id="S3.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="W_{0}/8\times H_{0}/8\times 16" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.cmml"><msub id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.2.cmml">W</mi><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.3.cmml">8</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.1.cmml">Ã—</mo><msub id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.2.cmml">H</mi><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.3.cmml">8</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1"><times id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1"></times><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2"><divide id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.1"></divide><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2"><times id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.1"></times><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2"><divide id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.1"></divide><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.2.3">0</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.2.3">8</cn></apply><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.2">ğ»</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.2.3.3">0</cn></apply></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.3">8</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">W_{0}/8\times H_{0}/8\times 16</annotation></semantics></math> output head where each element contains pixel-wise offset from detected peak to the 8 box vertices projected on to the image. We can then recover the scale and translation of the box up to a scale ambiguity using EPnP similar to Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. In contrast to prior work in pose estimation, we aligned the predicted box based on principal axes sized in a fixed reference frame. To recover absolute scale and translation, we additionally regress the distance from the camera <math id="S3.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="z\in\mathbb{R}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">z</mi><mo id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml">âˆˆ</mo><mi id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">â„</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1"><in id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.1"></in><ci id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.2">ğ‘§</ci><ci id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.2.m2.1c">z\in\mathbb{R}</annotation></semantics></math> of the box centroid as a <math id="S3.SS2.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="W_{0}/8\times H_{0}/8" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.cmml"><msub id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.2.cmml">W</mi><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.3.cmml">8</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.1.cmml">Ã—</mo><msub id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.2" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.2.cmml">H</mi><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1"><divide id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.1"></divide><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2"><times id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.1"></times><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2"><divide id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.1"></divide><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.2.3">0</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.2.3">8</cn></apply><apply id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.2">ğ»</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.2.3.3">0</cn></apply></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.3.m3.1c">W_{0}/8\times H_{0}/8</annotation></semantics></math> tensor. The two losses on these tensors are an <math id="S3.SS2.SSS0.Px2.p2.4.m4.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.4.m4.1a"><msub id="S3.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.4.m4.1b"><apply id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.4.m4.1c">L_{1}</annotation></semantics></math> loss and are denoted <math id="S3.SS2.SSS0.Px2.p2.5.m5.1" class="ltx_Math" alttext="l_{\rm vrtx}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.5.m5.1a"><msub id="S3.SS2.SSS0.Px2.p2.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3.cmml">vrtx</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.5.m5.1.1.3">vrtx</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.5.m5.1c">l_{\rm vrtx}</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="l_{\rm cent}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p2.6.m6.1a"><msub id="S3.SS2.SSS0.Px2.p2.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3.cmml">cent</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.6.m6.1.1.3">cent</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.6.m6.1c">l_{\rm cent}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p3.7" class="ltx_p">Finally the rotation, <math id="S3.SS2.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.1.m1.1c">R</annotation></semantics></math> of the OBB can be recovered via directly predicting the covariance matrix, <math id="S3.SS2.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="\Sigma\in\mathbb{R}^{3\times 3}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.2.cmml">Î£</mi><mo id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.cmml"><mn id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.1" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.3" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1"><in id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.1"></in><ci id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.2">Î£</ci><apply id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.2">â„</ci><apply id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3"><times id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.2">3</cn><cn type="integer" id="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p3.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.2.m2.1c">\Sigma\in\mathbb{R}^{3\times 3}</annotation></semantics></math> of the ground truth 3D point cloud of the target object, which can be easily generated in simulation. We directly regressed an output tensor of <math id="S3.SS2.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="W_{0}/8\times H_{0}/8\times 6" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p3.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.cmml"><msub id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.2.cmml">W</mi><mn id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.1" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.3.cmml">8</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.1" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.1.cmml">Ã—</mo><msub id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.2" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.2.cmml">H</mi><mn id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.1" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.1.cmml">/</mo><mn id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.3.cmml">8</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1"><times id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.1"></times><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2"><divide id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.1"></divide><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2"><times id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.1"></times><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2"><divide id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.1"></divide><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.2.3">0</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.2.3">8</cn></apply><apply id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.2">ğ»</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.2.3.3">0</cn></apply></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.2.3">8</cn></apply><cn type="integer" id="S3.SS2.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.3.m3.1c">W_{0}/8\times H_{0}/8\times 6</annotation></semantics></math>, which for each pixel contains both the diagonal and symmetric off diagonal elements of the target covariance matrix. We can then recover rotation based on the <span id="S3.SS2.SSS0.Px2.p3.7.1" class="ltx_text ltx_font_bold">SVD</span> of <math id="S3.SS2.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="\Sigma" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.4.m4.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px2.p3.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p3.4.m4.1.1.cmml">Î£</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.4.m4.1.1">Î£</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.4.m4.1c">\Sigma</annotation></semantics></math>. We additionally use an <math id="S3.SS2.SSS0.Px2.p3.5.m5.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.5.m5.1a"><msub id="S3.SS2.SSS0.Px2.p3.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.SSS0.Px2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.5.m5.1c">L_{1}</annotation></semantics></math> loss on this output head and denote the loss <math id="S3.SS2.SSS0.Px2.p3.6.m6.1" class="ltx_Math" alttext="l_{\rm rot}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.6.m6.1a"><msub id="S3.SS2.SSS0.Px2.p3.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1.3.cmml">rot</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p3.6.m6.1.1.3">rot</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.6.m6.1c">l_{\rm rot}</annotation></semantics></math>. Note for the 9-DOF pose loses, we only enforce them where the Gaussian heatmaps are greater than <math id="S3.SS2.SSS0.Px2.p3.7.m7.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S3.SS2.SSS0.Px2.p3.7.m7.1a"><mn id="S3.SS2.SSS0.Px2.p3.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p3.7.m7.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p3.7.m7.1b"><cn type="float" id="S3.SS2.SSS0.Px2.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p3.7.m7.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p3.7.m7.1c">0.3</annotation></semantics></math>, to prevent ambiguity in empty space.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Keypoints:</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Keypoints and learned correspondences are a common representation for robot manipulation, especially in deformable manipulationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. SimNet has an output head that predicts keypoints of various classes, which can be fed into planners for manipulation. For instance, one of the keypoint classes we predict in Sec.Â <a href="#S4.SS1" title="4.1 Manipulation Experiments â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> are t-shirt sleeves, which is used for t-shirt folding. The output head predicts heatmaps for each keypoint class, and is trained to match target heatmaps with Gaussian distributions placed at each ground-truth keypoint location using a pixelwise cross-entropy loss <math id="S3.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="l_{\rm kp}" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">l</mi><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">kp</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2">ğ‘™</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3">kp</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">l_{\rm kp}</annotation></semantics></math>. To extract keypoints from the predicted heatmaps, we use non-maximum suppression to perform peak detection as inÂ <cite class="ltx_cite ltx_citemacro_citet">Hou etÂ al. [<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Full Resolution Disparity:</h5>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">The final output head is a full resolution disparity image, which can be converted into a 3D point cloud for collision avoidance. Since our SCVN, produces a disparity image at quarter resolution, we can combine the feature extractor backbone and the left stereo image to produce a full resolution depth image. We use the same branch architecture as the previous heads to aggregate information across different scales. During training we use the same loss as the SCVN, but enforced at full resolution. This output is trained using a Huber loss function.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Efficient Synthetic Dataset Generation</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Given the complexity of all the output predictions defined in the previous section, it would be impractical to label a sufficient amount of real-data to generalize across scenes. Thus, we are interested in using synthetic data to provide ground truth annotations on a wide variety of scenarios. To force the network to learn geometric features, we randomize over lighting and textures as recommended inÂ <cite class="ltx_cite ltx_citemacro_citet">Mayer etÂ al. [<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. In contrast toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we use OpenGL shaders with PyRenderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> instead of physically based renderingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> approaches. Low-quality rendering greatly speeds up computation, and allows for data-set generation on the order of an hour. We generate three datasets: cars, graspable objects, and t-shirts (Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and discuss specific dataset details in supplemental Sec.Â <a href="#A4" title="Appendix D Dataset Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.
We intend to release our datasets as well as real-world validation data as part of this paper.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2106.16118/assets/figures/Panoptic_Preds_NEW-min.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="313" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.2.1" class="ltx_text ltx_font_bold">Graspable Object Predictions:</span> SimNet is evaluated on an oriented bounding box regression task with objects of varying sizes and shapes placed on flat surfaces. Top: Is the left RGB image from different homes. Middle: Is our model trained with RGB-D sim2real transfer similar to Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The top right corner is the output of the Asus Xtion depth sensor. Bottom: Is SimNet. In the top right, we show the low-res disparity estimate predicted by the learned stereo net. SimNet consistently enables better sim2real transfer of the predictions for optically challenging scenarios.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">SimNet is evaluated on a set of real-world computer vision and robotics tasks to see if training on synthetic stereo data will transfer robustly to diverse, real images in unstructured environments and enable manipulation of optically challenging objects. SimNet is compared against baseline approaches using monocular, naive stereo concatenation and active RGB-D sensing (supplemental Sec.Â <a href="#A2.SS1" title="B.1 Baseline Implementation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Manipulation Experiments</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">The learned perception models and manipulation policies are evaluated on two optically challenging tasks: (1) grasping objects on tabletops and (2) t-shirt folding.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Experimental Setup</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">All physical experiments are conducted on tabletops found across four different, real homes. Manipulation is performed with the Toyota HSR robot, which has a four DOF arm, mobile base, and pair of parallel jaw grippersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. It has a mounted stereo pair from a Zed 2 camera and a Asus Xtion Pro RGB-D sensor. Each home has different background objects, furniture, graspable objects, and lighting conditions, which evaluates each networkâ€™s ability to robustly generalize to diverse scenarios.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2106.16118/assets/figures/shirtpredictions-min.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.2.1" class="ltx_text ltx_font_bold">T-shirt Keypoint Predictions:</span> SimNet is evaluated on keypoint regression for shirts in various stages of folding. Three classes of keypoints are predicted: sleeves, neck, and bottom corners. RGB-D performs poorly due to strong natural lighting and minimal depth variation. SimNet accurately predicts keypoints on the shirts despite these challenges.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Ablation of Baselines on Validation Data</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We compare to the following baselines: <span id="S4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">mono</span>, which uses the left stereo image, <span id="S4.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_bold">depth</span>, which only uses depth inputs, and <span id="S4.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_bold">RGB-D</span>, which uses both RGB and depth inputs. To make the RGB-D baseline competitive, we swept over four different algorithms to fuse color and depth information on a held out set of <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn type="integer" id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">500</annotation></semantics></math> non-optically challenging scenes (i.e. matte objects with minimal natural light). We measured 3D mAP@0.25, a common metric in pose predictionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, of annotated bounding boxes of each object on the tabletop of interest. We report the best results of each method in Table <a href="#S4.T1" title="Table 1 â€£ 4.1.2 Ablation of Baselines on Validation Data â€£ 4.1 Manipulation Experiments â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Additional sensing ablations can be found in Sec.Â <a href="#A3" title="Appendix C Perception Ablation Studies â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mono</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Depth</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB-D</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimNet</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">3D mAP</td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.164</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.831</td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.855</td>
<td id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">0.921</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.4.1" class="ltx_text ltx_font_bold">SimNet Perception Results:</span> We present an ablation of different sensing modalities on the 3D OBB prediction task. We evaluate 3D mAP on a dataset of real, human-annotated images of optically <span id="S4.T1.5.2" class="ltx_text ltx_font_bold">easy objects.</span> We find that monocular sensing performs poorly on this task, likely due to the task being 3D in nature, while depth and RGB-D perform much better. SimNet outperforms these methods on the real images. Baseline implementation details can be found in Sec.Â <a href="#A2.SS1" title="B.1 Baseline Implementation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. We present full ablation results in supplemental Sec.Â <a href="#A3" title="Appendix C Perception Ablation Studies â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Unknown Object Grasping</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In this experiment, the robotâ€™s task is to grasp objects on a tabletop comprised of two classes of household objects in each home: <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">optically easy</span>, which consists of opaque, non-reflective objects, and <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">optically hard</span>, which contains optically challenging, transparent objects (Fig.Â <a href="#A1.F7" title="Figure 7 â€£ A.2 Setup 2: HSR Stereo Pair â€£ Appendix A Stereo Data Collection Rig â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). For each trial, we select an object uniformly at random from the dataset and randomly place it on the tabletop with other distractor objects. The task is to grasp the foremost object in the scene, by using a heuristic grasp planner that takes OBBs as input. Specifically, given an OBB, the robot aligns the gripper with the largest principal axis. In the event, of similar sized principal axes like a ball, the robot favors grasping the object on the side closest to the robot. A grasp is successful if the robot is able to raise the object off the table and remove it from the scene.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">For each of the four homes we test five easy objects and five hard objects and compare SimNet against the best RGB-D baseline found in Sec. <a href="#S4.SS1.SSS2" title="4.1.2 Ablation of Baselines on Validation Data â€£ 4.1 Manipulation Experiments â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>.
We report quantitative results in Table <a href="#S4.T2" title="Table 2 â€£ 4.2 Unknown Object Grasping â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and qualitative results of the predictions in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3.3 Efficient Synthetic Dataset Generation â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. SimNet outperforms RGB-D, 92.5% vs. 62.5% in grasp success.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method (Object Class)</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Home 1</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Home 2</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Home 3</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Home 4</th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">RGB-D (O. Easy)</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4/5</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4/5</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.5.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.6.1" class="ltx_text ltx_font_bold">18/20</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SimNet (O. Easy)</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.2.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">4/5</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.4.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">4/5</td>
<td id="S4.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.3.2.6.1" class="ltx_text ltx_font_bold">18/20</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RGB-D (O. Hard)</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0/5</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">1/5</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">1/5</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.4.3.5.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">7/20</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">SimNet (O. Hard)</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.5.4.2.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.5.4.3.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.5.4.4.1" class="ltx_text ltx_font_bold">5/5</span></td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4/5</td>
<td id="S4.T2.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.5.4.6.1" class="ltx_text ltx_font_bold">19/20</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.3.1" class="ltx_text ltx_font_bold">SimNet Grasping Results:</span> Grasp success scores between the best RGB-D method and SimNet across homes. On optically easy objects (i.e. matte and non-reflective) RGBD-D and SimNet perform similar on average. However, when presented with challenging objects such as glassware, SimNet outperforms RGB-D.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Grasp Point Prediction in T-shirt Folding</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">The robot is also evaluated on a t-shirt folding task, where it must execute a sequence of four folds on unseen, real t-shirts. This task is challenging to perform using depth sensing, because the depth resolution of most commercial depth sensors cannot capture the subtle variations in depth due to the thickness of a t-shirt. Keypoints are a popular representation for manipulating deformable objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. We parameterize a robot shirt folding policy using keypoint predictions for the shirtâ€™s neck, sleeves, and bottom corners (Sec.Â <a href="#A2.SS4.SSS2" title="B.4.2 T-Shirt Folding â€£ B.4 Manipulation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.4.2</span></a>). To compute quantitative results on sim2real transfer, we collect a validation dataset of 32 real images from 12 t-shirts in 3 homes of stages of t-shirt folding (Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1.1 Experimental Setup â€£ 4.1 Manipulation Experiments â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) and report keypoint prediction mAP (TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Grasp Point Prediction in T-shirt Folding â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We additionally present videos of folding using the robot on the project website for qualitative evaluation. We find that SimNet significantly outperforms RGB-D and depth, and slightly outperforms mono. A large part of our dataset was collected in a room with direct natural light, which increase noise in the active depth sensor, due to large amount of infrared light.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mono</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Depth</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RGB-D</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimNet</td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<td id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.893</td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.282</td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.631</td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.2.2.5.1" class="ltx_text ltx_font_bold">0.917</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.3.1" class="ltx_text ltx_font_bold">T-Shirt Folding Results:</span> A comparison of keypoint mAP shows that active depth-based models transfer poorly on this task, due to interference from natural lighting and low depth profile of the shirts. The monocular network transfers well to the real shirt images, and SimNet slightly outperforms it. Unlike the models that use active depth sensing, SimNet is robust to lighting variation and the low depth variation of the shirts.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2106.16118/assets/figures/kittipredictions2.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="78" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.2.1" class="ltx_text ltx_font_bold">KITTI bounding box predictions:</span> We evaluate SimNet on the 2D bounding box detection task in the KITTI benchmark. Top: Is the prediction of monocular-DR. Bottom: The predictions of SimNet with the estimated disparity from the SVCN in the top right corner. By performing approximate stereo matching SimNet achieves high-prediction accuracy via improved transfer from simulation.
</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>KITTI 2D Car Detection</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">In this experiment, we evaluate whether SimNet is effective sim-to-real for other vision tasks outside of home robotics such as 2D car detection on the KITTI benchmarkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. We compare the stereo cost volume network trained on simulated images to networks using monocular inputs (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Mono</span>), stacking left and right image inputs (<span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_bold">Stacked</span>), and using real data with a standard validation/training split used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> (<span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_bold">SimNet-real</span>). To predict 2D bounding boxes, we modify SimNet to use the single-shot box detection output head in CenterNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In the table below we report mAP@0.5 for the aggregate of moderate and easy classes car classes.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p">Interestingly, SimNet can significantly outperform monocular images and naive stereo concatenation, which suggests explicit stereo matching leads to more robust transfer. Furthermore, the gap between real and sim data is only 3.5%, which suggests relatively robust transfer. However, qualitatively SimNet fails to detect cars at distance, since the ability to reason about geometry decays with distance from the camera. For visualizations of the predictions see supplemental Fig.Â <a href="#S4.F6" title="Figure 6 â€£ 4.3 Grasp Point Prediction in T-shirt Folding â€£ 4 Experiments â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Thus, SimNet is best used for applications with limited range from the camera, such as robot manipulation.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mono</td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Stacked</td>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">SimNet</td>
<td id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SimNet-real</td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<td id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.565</td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.710</td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><span id="S4.T4.1.2.2.4.1" class="ltx_text ltx_font_bold">0.826</span></td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.861</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.3.1" class="ltx_text ltx_font_bold">KITTI Results:</span> Results of different sim-to-real techniques for 2D car detection on the KITTI Benchmark. We find that the simply stacking the left and right images in the stacked input network significantly outperforms the monocular network. SimNet, however, transfers much better than naive stereo concatenation and is close to performance using real data.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We present SimNet, am efficient, multi-headed prediction network that leverages approximate stereo matching to transfer from simulation to reality. The network is trained on entirely simulated data and robustly transfers to real images of unknown optically-challenging objects such as glassware, even in direct sunlight. We show that these predictions are sufficient for robot manipulation such as t-shirt folding and grasping. In future work, we plan to use SimNet to automate household chores.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We like to thank Jeremy Ma for designing our stereo data collection rig used to collect indoor scenes. We also thank Krishna Shankar and Max Bajracharya for their invaluable insights into both classical and learned stereo techniques. Finally, we like to thank the Machine Learning Research team at TRI for their feedback and guidance.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. [2018]</span>
<span class="ltx_bibblock">
A.Â Gupta, A.Â Murali, D.Â Gandhi, and L.Â Pinto.

</span>
<span class="ltx_bibblock">Robot learning in homes: Improving generalization and reducing
dataset bias.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.07049</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sajjan etÂ al. [2020]</span>
<span class="ltx_bibblock">
S.Â Sajjan, M.Â Moore, M.Â Pan, G.Â Nagaraja, J.Â Lee, A.Â Zeng, and S.Â Song.

</span>
<span class="ltx_bibblock">Clear grasp: 3d shape estimation of transparent objects for
manipulation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pages 3634â€“3642. IEEE, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalashnikov etÂ al. [2018]</span>
<span class="ltx_bibblock">
D.Â Kalashnikov, A.Â Irpan, P.Â Pastor, J.Â Ibarz, A.Â Herzog, E.Â Jang, D.Â Quillen,
E.Â Holly, M.Â Kalakrishnan, V.Â Vanhoucke, etÂ al.

</span>
<span class="ltx_bibblock">Qt-opt: Scalable deep reinforcement learning for vision-based robotic
manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.10293</em>, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahler etÂ al. [2019]</span>
<span class="ltx_bibblock">
J.Â Mahler, M.Â Matl, V.Â Satish, M.Â Danielczuk, B.Â DeRose, S.Â McKinley, and
K.Â Goldberg.

</span>
<span class="ltx_bibblock">Learning ambidextrous robot grasping policies.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Science Robotics</em>, 4(26), 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundaresan etÂ al. [2020]</span>
<span class="ltx_bibblock">
P.Â Sundaresan, J.Â Grannen, B.Â Thananjeyan, A.Â Balakrishna, M.Â Laskey, K.Â Stone,
J.Â E. Gonzalez, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Learning rope manipulation policies using dense object descriptors
trained on synthetic depth data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pages 9411â€“9418. IEEE, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. [2020]</span>
<span class="ltx_bibblock">
C.Â Xie, Y.Â Xiang, A.Â Mousavian, and D.Â Fox.

</span>
<span class="ltx_bibblock">The best of both modes: Separately leveraging rgb and depth for
unseen object instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Conference on robot learning</em>, pages 1369â€“1378. PMLR, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobin etÂ al. [2017]</span>
<span class="ltx_bibblock">
J.Â Tobin, R.Â Fong, A.Â Ray, J.Â Schneider, W.Â Zaremba, and P.Â Abbeel.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference on intelligent robots
and systems (IROS)</em>, pages 23â€“30. IEEE, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danielczuk etÂ al. [2019]</span>
<span class="ltx_bibblock">
M.Â Danielczuk, M.Â Matl, S.Â Gupta, A.Â Li, A.Â Lee, J.Â Mahler, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Segmenting unknown 3d objects from real depth images using mask r-cnn
trained on synthetic data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Robotics and Automation
(ICRA)</em>, pages 7283â€“7290. IEEE, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akkaya etÂ al. [2019]</span>
<span class="ltx_bibblock">
I.Â Akkaya, M.Â Andrychowicz, M.Â Chociej, M.Â Litwin, B.Â McGrew, A.Â Petron,
A.Â Paino, M.Â Plappert, G.Â Powell, R.Â Ribas, etÂ al.

</span>
<span class="ltx_bibblock">Solving rubikâ€™s cube with a robot hand.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.07113</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganapathi etÂ al. [2020]</span>
<span class="ltx_bibblock">
A.Â Ganapathi, P.Â Sundaresan, B.Â Thananjeyan, A.Â Balakrishna, D.Â Seita,
J.Â Grannen, M.Â Hwang, R.Â Hoque, J.Â E. Gonzalez, N.Â Jamali, etÂ al.

</span>
<span class="ltx_bibblock">Learning dense visual correspondences in simulation to smooth and
fold real fabrics.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.12698</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoque etÂ al. [2020]</span>
<span class="ltx_bibblock">
R.Â Hoque, D.Â Seita, A.Â Balakrishna, A.Â Ganapathi, A.Â K. Tanwani, N.Â Jamali,
K.Â Yamane, S.Â Iba, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Visuospatial foresight for multi-step, multi-task fabric
manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.09044</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seita etÂ al. [2019]</span>
<span class="ltx_bibblock">
D.Â Seita, A.Â Ganapathi, R.Â Hoque, M.Â Hwang, E.Â Cen, A.Â K. Tanwani,
A.Â Balakrishna, B.Â Thananjeyan, J.Â Ichnowski, N.Â Jamali, etÂ al.

</span>
<span class="ltx_bibblock">Deep imitation learning of sequential fabric smoothing from an
algorithmic supervisor.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.04854</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts and Paczan [2020]</span>
<span class="ltx_bibblock">
M.Â Roberts and N.Â Paczan.

</span>
<span class="ltx_bibblock">Hypersim: A photorealistic synthetic dataset for holistic indoor
scene understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.02523</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadeghi and Levine [2016]</span>
<span class="ltx_bibblock">
F.Â Sadeghi and S.Â Levine.

</span>
<span class="ltx_bibblock">Cad2rl: Real single-image flight without a single real image.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.04201</em>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer etÂ al. [2016]</span>
<span class="ltx_bibblock">
N.Â Mayer, E.Â Ilg, P.Â Hausser, P.Â Fischer, D.Â Cremers, A.Â Dosovitskiy, and
T.Â Brox.

</span>
<span class="ltx_bibblock">A large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4040â€“4048, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kendall etÂ al. [2017]</span>
<span class="ltx_bibblock">
A.Â Kendall, H.Â Martirosyan, S.Â Dasgupta, P.Â Henry, R.Â Kennedy, A.Â Bachrach, and
A.Â Bry.

</span>
<span class="ltx_bibblock">End-to-end learning of geometry and context for deep stereo
regression.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, pages 66â€“75, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grannen etÂ al. [2020]</span>
<span class="ltx_bibblock">
J.Â Grannen, P.Â Sundaresan, B.Â Thananjeyan, J.Â Ichnowski, A.Â Balakrishna,
M.Â Hwang, V.Â Viswanath, M.Â Laskey, J.Â E. Gonzalez, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Untangling dense knots by learning task-relevant keypoints.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.04999</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seita etÂ al. [2018]</span>
<span class="ltx_bibblock">
D.Â Seita, N.Â Jamali, M.Â Laskey, A.Â K. Tanwani, R.Â Berenstein, P.Â Baskaran,
S.Â Iba, J.Â Canny, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Deep transfer learning of pick points on fabric for robot bed-making.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09810</em>, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundaresan etÂ al. [2019]</span>
<span class="ltx_bibblock">
P.Â Sundaresan, B.Â Thananjeyan, J.Â Chiu, D.Â Fer, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Automated extraction of surgical needles from tissue phantoms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 15th International Conference on Automation
Science and Engineering (CASE)</em>, pages 170â€“177. IEEE, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haarnoja etÂ al. [2018]</span>
<span class="ltx_bibblock">
T.Â Haarnoja, A.Â Zhou, K.Â Hartikainen, G.Â Tucker, S.Â Ha, J.Â Tan, V.Â Kumar,
H.Â Zhu, A.Â Gupta, P.Â Abbeel, etÂ al.

</span>
<span class="ltx_bibblock">Soft actor-critic algorithms and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.05905</em>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thananjeyan etÂ al. [2021]</span>
<span class="ltx_bibblock">
B.Â Thananjeyan, A.Â Balakrishna, S.Â Nair, M.Â Luo, K.Â Srinivasan, M.Â Hwang, J.Â E.
Gonzalez, J.Â Ibarz, C.Â Finn, and K.Â Goldberg.

</span>
<span class="ltx_bibblock">Recovery rl: Safe reinforcement learning with learned recovery zones.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, 6(3):4915â€“4922, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levine etÂ al. [2016]</span>
<span class="ltx_bibblock">
S.Â Levine, C.Â Finn, T.Â Darrell, and P.Â Abbeel.

</span>
<span class="ltx_bibblock">End-to-end training of deep visuomotor policies.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 17(1):1334â€“1373, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balasubramanian etÂ al. [2012]</span>
<span class="ltx_bibblock">
R.Â Balasubramanian, L.Â Xu, P.Â D. Brook, J.Â R. Smith, and Y.Â Matsuoka.

</span>
<span class="ltx_bibblock">Physical human interactive guidance: Identifying grasping principles
from human-planned grasps.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>, 28(4):899â€“910, 2012.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maitin-Shepard etÂ al. [2010]</span>
<span class="ltx_bibblock">
J.Â Maitin-Shepard, M.Â Cusumano-Towner, J.Â Lei, and P.Â Abbeel.

</span>
<span class="ltx_bibblock">Cloth grasp point detection based on multiple-view geometric cues
with application to robotic towel folding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2010 IEEE International Conference on Robotics and
Automation</em>, pages 2308â€“2315. IEEE, 2010.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manuelli etÂ al. [2019]</span>
<span class="ltx_bibblock">
L.Â Manuelli, W.Â Gao, P.Â Florence, and R.Â Tedrake.

</span>
<span class="ltx_bibblock">kpam: Keypoint affordances for category-level robotic manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.06684</em>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Florence etÂ al. [2018]</span>
<span class="ltx_bibblock">
P.Â R. Florence, L.Â Manuelli, and R.Â Tedrake.

</span>
<span class="ltx_bibblock">Dense object nets: Learning dense visual object descriptors by and
for robotic manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.08756</em>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grunnet-Jepsen etÂ al. [2018]</span>
<span class="ltx_bibblock">
A.Â Grunnet-Jepsen, J.Â N. Sweetser, P.Â Winer, A.Â Takagi, and J.Â Woodfill.

</span>
<span class="ltx_bibblock">Projectors for intelÂ® realsenseâ„¢ depth cameras
d4xx.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Intel Support, Interl Corporation: Santa Clara, CA, USA</em>, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay etÂ al. [2018]</span>
<span class="ltx_bibblock">
J.Â Tremblay, A.Â Prakash, D.Â Acuna, M.Â Brophy, V.Â Jampani, C.Â Anil, T.Â To,
E.Â Cameracci, S.Â Boochoon, and S.Â Birchfield.

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</em>, pages 969â€“977, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alghonaim and Johns [2020]</span>
<span class="ltx_bibblock">
R.Â Alghonaim and E.Â Johns.

</span>
<span class="ltx_bibblock">Benchmarking domain randomisation for visual sim-to-real transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.07112</em>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay etÂ al. [2018]</span>
<span class="ltx_bibblock">
J.Â Tremblay, T.Â To, B.Â Sundaralingam, Y.Â Xiang, D.Â Fox, and S.Â Birchfield.

</span>
<span class="ltx_bibblock">Deep object pose estimation for semantic robotic grasping of
household objects.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.10790</em>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. [2020]</span>
<span class="ltx_bibblock">
T.Â E. Lee, J.Â Tremblay, T.Â To, J.Â Cheng, T.Â Mosier, O.Â Kroemer, D.Â Fox, and
S.Â Birchfield.

</span>
<span class="ltx_bibblock">Camera-to-robot pose estimation from a single image.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Robotics and
Automation (ICRA)</em>, pages 9426â€“9432. IEEE, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard [2012]</span>
<span class="ltx_bibblock">
I.Â P. Howard.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Perceiving in depth, volume 1: basic mechanisms</em>.

</span>
<span class="ltx_bibblock">Oxford University Press, 2012.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forsyth and Ponce [2012]</span>
<span class="ltx_bibblock">
D.Â A. Forsyth and J.Â Ponce.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Computer vision: a modern approach</em>.

</span>
<span class="ltx_bibblock">Pearson,, 2012.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al. [2016]</span>
<span class="ltx_bibblock">
W.Â Luo, A.Â G. Schwing, and R.Â Urtasun.

</span>
<span class="ltx_bibblock">Efficient deep learning for stereo matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 5695â€“5703, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilg etÂ al. [2017]</span>
<span class="ltx_bibblock">
E.Â Ilg, N.Â Mayer, T.Â Saikia, M.Â Keuper, A.Â Dosovitskiy, and T.Â Brox.

</span>
<span class="ltx_bibblock">Flownet 2.0: Evolution of optical flow estimation with deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2462â€“2470, 2017.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer etÂ al. [2018]</span>
<span class="ltx_bibblock">
N.Â Mayer, E.Â Ilg, P.Â Fischer, C.Â Hazirbas, D.Â Cremers, A.Â Dosovitskiy, and
T.Â Brox.

</span>
<span class="ltx_bibblock">What makes good synthetic training data for learning disparity and
optical flow estimation?

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 126(9):942â€“960, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zbontar and LeCun [2015]</span>
<span class="ltx_bibblock">
J.Â Zbontar and Y.Â LeCun.

</span>
<span class="ltx_bibblock">Computing the stereo matching cost with a convolutional neural
network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 1592â€“1599, 2015.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. [2017]</span>
<span class="ltx_bibblock">
F.Â Yu, V.Â Koltun, and T.Â Funkhouser.

</span>
<span class="ltx_bibblock">Dilated residual networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 472â€“480, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huber [1992]</span>
<span class="ltx_bibblock">
P.Â J. Huber.

</span>
<span class="ltx_bibblock">Robust estimation of a location parameter.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Breakthroughs in statistics</em>, pages 492â€“518. Springer,
1992.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov etÂ al. [2019]</span>
<span class="ltx_bibblock">
A.Â Kirillov, R.Â Girshick, K.Â He, and P.Â DollÃ¡r.

</span>
<span class="ltx_bibblock">Panoptic feature pyramid networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6399â€“6408, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al. [2020]</span>
<span class="ltx_bibblock">
T.Â Hou, A.Â Ahmadyan, L.Â Zhang, J.Â Wei, and M.Â Grundmann.

</span>
<span class="ltx_bibblock">Mobilepose: Real-time pose estimation for unseen objects with weak
shape supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.03522</em>, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan etÂ al. [2019]</span>
<span class="ltx_bibblock">
K.Â Duan, S.Â Bai, L.Â Xie, H.Â Qi, Q.Â Huang, and Q.Â Tian.

</span>
<span class="ltx_bibblock">Centernet: Keypoint triplets for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 6569â€“6578, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matl [2013]</span>
<span class="ltx_bibblock">
M.Â Matl.

</span>
<span class="ltx_bibblock">Pyrender.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mmatl/pyrender" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mmatl/pyrender</a>, 2013.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pharr etÂ al. [2016]</span>
<span class="ltx_bibblock">
M.Â Pharr, W.Â Jakob, and G.Â Humphreys.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Physically based rendering: From theory to implementation</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamamoto etÂ al. [2019]</span>
<span class="ltx_bibblock">
T.Â Yamamoto, K.Â Terada, A.Â Ochiai, F.Â Saito, Y.Â Asahara, and K.Â Murase.

</span>
<span class="ltx_bibblock">Development of human support robot as the research platform of a
domestic mobile manipulator.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ROBOMECH journal</em>, 6(1):1â€“15, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. [2019]</span>
<span class="ltx_bibblock">
H.Â Wang, S.Â Sridhar, J.Â Huang, J.Â Valentin, S.Â Song, and L.Â J. Guibas.

</span>
<span class="ltx_bibblock">Normalized object coordinate space for category-level 6d object pose
and size estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 2642â€“2651, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiger etÂ al. [2012]</span>
<span class="ltx_bibblock">
A.Â Geiger, P.Â Lenz, and R.Â Urtasun.

</span>
<span class="ltx_bibblock">Are we ready for autonomous driving? the kitti vision benchmark
suite.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, 2012.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiger etÂ al. [2013]</span>
<span class="ltx_bibblock">
A.Â Geiger, P.Â Lenz, C.Â Stiller, and R.Â Urtasun.

</span>
<span class="ltx_bibblock">Vision meets robotics: The kitti dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">International Journal of Robotics Research (IJRR)</em>, 2013.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy [2015]</span>
<span class="ltx_bibblock">
S.Â Ioffe and C.Â Szegedy.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
448â€“456. PMLR, 2015.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2017]</span>
<span class="ltx_bibblock">
L.Â Li, K.Â Jamieson, G.Â DeSalvo, A.Â Rostamizadeh, and A.Â Talwalkar.

</span>
<span class="ltx_bibblock">Hyperband: A novel bandit-based approach to hyperparameter
optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 18(1):6765â€“6816, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang etÂ al. [2015]</span>
<span class="ltx_bibblock">
A.Â X. Chang, T.Â Funkhouser, L.Â Guibas, P.Â Hanrahan, Q.Â Huang, Z.Â Li,
S.Â Savarese, M.Â Savva, S.Â Song, H.Â Su, etÂ al.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.03012</em>, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Stereo Data Collection Rig</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">We now describe the two physical rigs used to collected stereo images for validation. In each rig, there is also an RGB-D sensor that is also used to collect data to evaluate the depth-based baselines.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Setup 1: Basler Stereo Pair</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.2" class="ltx_p">To collect data in the homes, we created a a stereo pair with two Basler 60uc cameras. Our cameraâ€™s native resolution is <math id="A1.SS1.p1.1.m1.1" class="ltx_Math" alttext="2560\times 2048" display="inline"><semantics id="A1.SS1.p1.1.m1.1a"><mrow id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml"><mn id="A1.SS1.p1.1.m1.1.1.2" xref="A1.SS1.p1.1.m1.1.1.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS1.p1.1.m1.1.1.1" xref="A1.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.SS1.p1.1.m1.1.1.3" xref="A1.SS1.p1.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><apply id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1"><times id="A1.SS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="A1.SS1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.p1.1.m1.1.1.2">2560</cn><cn type="integer" id="A1.SS1.p1.1.m1.1.1.3.cmml" xref="A1.SS1.p1.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">2560\times 2048</annotation></semantics></math> pixels with a wide field of view fish-eye lens. However, we perform inference on a 4x down-sampled version. To obtain a stereo pair, all images are rectified to a pinhole camera model. The baseline of our stereo camera is <math id="A1.SS1.p1.2.m2.1" class="ltx_Math" alttext="10cm" display="inline"><semantics id="A1.SS1.p1.2.m2.1a"><mrow id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml"><mn id="A1.SS1.p1.2.m2.1.1.2" xref="A1.SS1.p1.2.m2.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="A1.SS1.p1.2.m2.1.1.1" xref="A1.SS1.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="A1.SS1.p1.2.m2.1.1.3" xref="A1.SS1.p1.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.SS1.p1.2.m2.1.1.1a" xref="A1.SS1.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="A1.SS1.p1.2.m2.1.1.4" xref="A1.SS1.p1.2.m2.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><apply id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"><times id="A1.SS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="A1.SS1.p1.2.m2.1.1.2.cmml" xref="A1.SS1.p1.2.m2.1.1.2">10</cn><ci id="A1.SS1.p1.2.m2.1.1.3.cmml" xref="A1.SS1.p1.2.m2.1.1.3">ğ‘</ci><ci id="A1.SS1.p1.2.m2.1.1.4.cmml" xref="A1.SS1.p1.2.m2.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">10cm</annotation></semantics></math>, which was selected to match the average distance between human eyes.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para ltx_noindent">
<p id="A1.SS1.p2.1" class="ltx_p">To obtain depth data for our baseline technique, we mount a Microsoft Kinect Azure in wide field of view mode. The Kinect is mounted to a fixed steel base on top of stereo camera pair. We calibrate the Kinect to the left stereo camera using standard checkerboard calibrationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Camera calibration allows us to project the Kinectâ€™s point cloud into the left camera. We down sample the depth data using a <math id="A1.SS1.p2.1.m1.1" class="ltx_Math" alttext="2\times 2" display="inline"><semantics id="A1.SS1.p2.1.m1.1a"><mrow id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml"><mn id="A1.SS1.p2.1.m1.1.1.2" xref="A1.SS1.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS1.p2.1.m1.1.1.1" xref="A1.SS1.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.SS1.p2.1.m1.1.1.3" xref="A1.SS1.p2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><apply id="A1.SS1.p2.1.m1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1"><times id="A1.SS1.p2.1.m1.1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="A1.SS1.p2.1.m1.1.1.2.cmml" xref="A1.SS1.p2.1.m1.1.1.2">2</cn><cn type="integer" id="A1.SS1.p2.1.m1.1.1.3.cmml" xref="A1.SS1.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">2\times 2</annotation></semantics></math> median filter to match the resolution inference is performed at. Example images from our data collection rig can be seen on Fig. <a href="#A1.F8" title="Figure 8 â€£ A.2 Setup 2: HSR Stereo Pair â€£ Appendix A Stereo Data Collection Rig â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Setup 2: HSR Stereo Pair</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.4" class="ltx_p">On the physical experiments run on the Toyota HSR and for the t-shirt experiments, we the stereo pair from a mounted Zed2 camera, which has resolution <math id="A1.SS2.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1024" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mn id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS2.p1.1.m1.1.1.1" xref="A1.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><times id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="A1.SS2.p1.1.m1.1.1.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="A1.SS2.p1.1.m1.1.1.3.cmml" xref="A1.SS2.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">1920\times 1024</annotation></semantics></math> pixels. We down-sample images by a factor of 2, so the images used for training and prediction have resolution <math id="A1.SS2.p1.2.m2.1" class="ltx_Math" alttext="960\times 512" display="inline"><semantics id="A1.SS2.p1.2.m2.1a"><mrow id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">960</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS2.p1.2.m2.1.1.1" xref="A1.SS2.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><times id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="A1.SS2.p1.2.m2.1.1.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2">960</cn><cn type="integer" id="A1.SS2.p1.2.m2.1.1.3.cmml" xref="A1.SS2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">960\times 512</annotation></semantics></math> pixels. The Zed2 camera is a time synchronized stereo pair camera with a baseline of <math id="A1.SS2.p1.3.m3.1" class="ltx_Math" alttext="12cm" display="inline"><semantics id="A1.SS2.p1.3.m3.1a"><mrow id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml"><mn id="A1.SS2.p1.3.m3.1.1.2" xref="A1.SS2.p1.3.m3.1.1.2.cmml">12</mn><mo lspace="0em" rspace="0em" id="A1.SS2.p1.3.m3.1.1.1" xref="A1.SS2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="A1.SS2.p1.3.m3.1.1.3" xref="A1.SS2.p1.3.m3.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.SS2.p1.3.m3.1.1.1a" xref="A1.SS2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="A1.SS2.p1.3.m3.1.1.4" xref="A1.SS2.p1.3.m3.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.1b"><apply id="A1.SS2.p1.3.m3.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1"><times id="A1.SS2.p1.3.m3.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="A1.SS2.p1.3.m3.1.1.2.cmml" xref="A1.SS2.p1.3.m3.1.1.2">12</cn><ci id="A1.SS2.p1.3.m3.1.1.3.cmml" xref="A1.SS2.p1.3.m3.1.1.3">ğ‘</ci><ci id="A1.SS2.p1.3.m3.1.1.4.cmml" xref="A1.SS2.p1.3.m3.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.1c">12cm</annotation></semantics></math>. The depth and RGB-D experiments on the HSR are run using a mounted ASUS Xtion, which is placed <math id="A1.SS2.p1.4.m4.1" class="ltx_Math" alttext="5cm" display="inline"><semantics id="A1.SS2.p1.4.m4.1a"><mrow id="A1.SS2.p1.4.m4.1.1" xref="A1.SS2.p1.4.m4.1.1.cmml"><mn id="A1.SS2.p1.4.m4.1.1.2" xref="A1.SS2.p1.4.m4.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A1.SS2.p1.4.m4.1.1.1" xref="A1.SS2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="A1.SS2.p1.4.m4.1.1.3" xref="A1.SS2.p1.4.m4.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.SS2.p1.4.m4.1.1.1a" xref="A1.SS2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="A1.SS2.p1.4.m4.1.1.4" xref="A1.SS2.p1.4.m4.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.4.m4.1b"><apply id="A1.SS2.p1.4.m4.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1"><times id="A1.SS2.p1.4.m4.1.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="A1.SS2.p1.4.m4.1.1.2.cmml" xref="A1.SS2.p1.4.m4.1.1.2">5</cn><ci id="A1.SS2.p1.4.m4.1.1.3.cmml" xref="A1.SS2.p1.4.m4.1.1.3">ğ‘</ci><ci id="A1.SS2.p1.4.m4.1.1.4.cmml" xref="A1.SS2.p1.4.m4.1.1.4">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.4.m4.1c">5cm</annotation></semantics></math> below the Zed2.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2106.16118/assets/figures/grasping_fig-min1.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A1.F7.2.1" class="ltx_text ltx_font_bold">Grasping Objects and Experimental Setup:</span> We split objects in each house into two categories: easy and hard. Hard objects are typically transparent, reflective, or translucent, making them hard to perceive with active depth sensors. Each house has 5 objects in each class. We also present the experimental setup in each house for these experiments. Each house has different lighting conditions, tables, and background objects. During grasping experiments, two objects or more are placed on the table, and the robot grasps and extracts the object located in front.</figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2106.16118/assets/figures/panoptic_home.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="A1.F8.2.1" class="ltx_text ltx_font_bold">SimNet Predictions on Indoor Scenes:</span> We present visualizations of SimNetâ€™s predictions on the dataset used in the sensing ablation studies. This dataset consists of optically easy scenarios , but can have many objects in the each scene. We find that the OBB and scene level segmentation predictions from SimNet on this dataset are high quality compared to baselines (SectionÂ <a href="#A3" title="Appendix C Perception Ablation Studies â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Baseline Implementation Details</h3>

<div id="A2.SS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS1.p1.1" class="ltx_p">In this section, we will describe implementation details for the baselines used.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para ltx_noindent">
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Monocular (Mono):</span> This network trains only on the left image from the stereo pair. The left image is featurized as in the fully stereo network to form <math id="A2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="A2.I1.i1.p1.1.m1.1a"><msub id="A2.I1.i1.p1.1.m1.1.1" xref="A2.I1.i1.p1.1.m1.1.1.cmml"><mi id="A2.I1.i1.p1.1.m1.1.1.2" xref="A2.I1.i1.p1.1.m1.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="A2.I1.i1.p1.1.m1.1.1.3" xref="A2.I1.i1.p1.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="A2.I1.i1.p1.1.m1.1b"><apply id="A2.I1.i1.p1.1.m1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.I1.i1.p1.1.m1.1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="A2.I1.i1.p1.1.m1.1.1.2.cmml" xref="A2.I1.i1.p1.1.m1.1.1.2">italic-Ï•</ci><ci id="A2.I1.i1.p1.1.m1.1.1.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i1.p1.1.m1.1c">\phi_{\rm l}</annotation></semantics></math> and then fed into the backbone.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Monocular, Auxiliary Loss (Mono-Aux):</span> This network trains only on the left image from the stereo pair. In contrast to Mono, this network takes one of the channels of <math id="A2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\phi_{\rm l}" display="inline"><semantics id="A2.I1.i2.p1.1.m1.1a"><msub id="A2.I1.i2.p1.1.m1.1.1" xref="A2.I1.i2.p1.1.m1.1.1.cmml"><mi id="A2.I1.i2.p1.1.m1.1.1.2" xref="A2.I1.i2.p1.1.m1.1.1.2.cmml">Ï•</mi><mi mathvariant="normal" id="A2.I1.i2.p1.1.m1.1.1.3" xref="A2.I1.i2.p1.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="A2.I1.i2.p1.1.m1.1b"><apply id="A2.I1.i2.p1.1.m1.1.1.cmml" xref="A2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.I1.i2.p1.1.m1.1.1.1.cmml" xref="A2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.I1.i2.p1.1.m1.1.1.2.cmml" xref="A2.I1.i2.p1.1.m1.1.1.2">italic-Ï•</ci><ci id="A2.I1.i2.p1.1.m1.1.1.3.cmml" xref="A2.I1.i2.p1.1.m1.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i2.p1.1.m1.1c">\phi_{\rm l}</annotation></semantics></math> and applies the disparity reconstruction auxiliary loss, described in SectionÂ <a href="#S3.SS1" title="3.1 Stereo Cost Volume Networks (SCVN) For Robust Low-Level Features â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. This forces the monocular network to reason about geometry in the scene.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Depth:</span> This network only trains on depth images, and is identical to the Mono network, except for the number of input channels. Because depth images from real sensors contain many artifacts, we process each synthetic depth image by adding noise and random ellipse dropouts, as in Â <cite class="ltx_cite ltx_citemacro_citet">Mahler etÂ al. [<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. This process is illustrated in FigureÂ <a href="#A2.F9" title="Figure 9 â€£ B.1 Baseline Implementation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Depth, Auxiliary Loss (Depth-Aux):</span> This network only trains on depth images and is identical to the Mono-Disp network, except for the number of input channels (1). We use the auxiliary loss from SectionÂ <a href="#S3.SS1" title="3.1 Stereo Cost Volume Networks (SCVN) For Robust Low-Level Features â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> on the last channel of the predicted features to predict a low-resolution depth image (not disparity image), before feeding the feature volume to the ResNet50-FPN backbone. This forces the network to try to clean up artifacts in the real or noisy depth images.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">RGB-D:</span> This network separately featurizes the RGB and depth images using seperate ResNet stems. It then concatenates computed feature channels and feeds them into the ResNet50-FPN backbone.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p"><span id="A2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">RGB-D, Auxiliary Loss (RGB-D-Aux):</span> This network separately featurizes the RGB and depth images, concatenates the features and feed them into the ResNet50-FPN backbone. However, on one channel of the depth features, we apply the depth reconstruction auxiliary loss function (SectionÂ <a href="#S3.SS1" title="3.1 Stereo Cost Volume Networks (SCVN) For Robust Low-Level Features â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p"><span id="A2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">RGB-D, Stacked Input (RGB-D-Stack):</span> This network concatenates the RGB and D channels of the input into a single <math id="A2.I1.i7.p1.1.m1.1" class="ltx_Math" alttext="H\times W\times 4" display="inline"><semantics id="A2.I1.i7.p1.1.m1.1a"><mrow id="A2.I1.i7.p1.1.m1.1.1" xref="A2.I1.i7.p1.1.m1.1.1.cmml"><mi id="A2.I1.i7.p1.1.m1.1.1.2" xref="A2.I1.i7.p1.1.m1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="A2.I1.i7.p1.1.m1.1.1.1" xref="A2.I1.i7.p1.1.m1.1.1.1.cmml">Ã—</mo><mi id="A2.I1.i7.p1.1.m1.1.1.3" xref="A2.I1.i7.p1.1.m1.1.1.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="A2.I1.i7.p1.1.m1.1.1.1a" xref="A2.I1.i7.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="A2.I1.i7.p1.1.m1.1.1.4" xref="A2.I1.i7.p1.1.m1.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.I1.i7.p1.1.m1.1b"><apply id="A2.I1.i7.p1.1.m1.1.1.cmml" xref="A2.I1.i7.p1.1.m1.1.1"><times id="A2.I1.i7.p1.1.m1.1.1.1.cmml" xref="A2.I1.i7.p1.1.m1.1.1.1"></times><ci id="A2.I1.i7.p1.1.m1.1.1.2.cmml" xref="A2.I1.i7.p1.1.m1.1.1.2">ğ»</ci><ci id="A2.I1.i7.p1.1.m1.1.1.3.cmml" xref="A2.I1.i7.p1.1.m1.1.1.3">ğ‘Š</ci><cn type="integer" id="A2.I1.i7.p1.1.m1.1.1.4.cmml" xref="A2.I1.i7.p1.1.m1.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i7.p1.1.m1.1c">H\times W\times 4</annotation></semantics></math> array and feeds it into the network as a single input image.</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A2.I1.i8.p1" class="ltx_para ltx_noindent">
<p id="A2.I1.i8.p1.1" class="ltx_p"><span id="A2.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">RGB-D, Sequential Input (RGB-D-Seq):</span> This baseline is heavily inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and first trains a depth network as described above. The segmentation and disparity prediction outputs are then fed as input to a mono network in addition to the left image of the stereo pair. The idea with the network architecture is to use RGB information to simply refine the output of the depth prediction network. The depth network is frozen after is trained, and is not updated when the refinement network is trained.</p>
</div>
</li>
</ol>
</div>
<figure id="A2.F9" class="ltx_figure"><img src="/html/2106.16118/assets/figures/depth_noise-min.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="330" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A2.F9.2.1" class="ltx_text ltx_font_bold">Depth Image Noise Injection:</span> To simulate the artifacts present in real depth images (right column), we take synthetically generated depth images (middle column), and add process noise and random dropouts, particularly around edges, similar toÂ <cite class="ltx_cite ltx_citemacro_citet">Mahler etÂ al. [<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</figcaption>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Training Details</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.3" class="ltx_p">All networks are trained on a Telsa V100 GPU for 400,000 gradient steps with a batch size of 18. Models are trained using the Adam optimizer with learning rate <math id="A2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha=5e-4" display="inline"><semantics id="A2.SS2.p1.1.m1.1a"><mrow id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml"><mi id="A2.SS2.p1.1.m1.1.1.2" xref="A2.SS2.p1.1.m1.1.1.2.cmml">Î±</mi><mo id="A2.SS2.p1.1.m1.1.1.1" xref="A2.SS2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="A2.SS2.p1.1.m1.1.1.3" xref="A2.SS2.p1.1.m1.1.1.3.cmml"><mrow id="A2.SS2.p1.1.m1.1.1.3.2" xref="A2.SS2.p1.1.m1.1.1.3.2.cmml"><mn id="A2.SS2.p1.1.m1.1.1.3.2.2" xref="A2.SS2.p1.1.m1.1.1.3.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A2.SS2.p1.1.m1.1.1.3.2.1" xref="A2.SS2.p1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="A2.SS2.p1.1.m1.1.1.3.2.3" xref="A2.SS2.p1.1.m1.1.1.3.2.3.cmml">e</mi></mrow><mo id="A2.SS2.p1.1.m1.1.1.3.1" xref="A2.SS2.p1.1.m1.1.1.3.1.cmml">âˆ’</mo><mn id="A2.SS2.p1.1.m1.1.1.3.3" xref="A2.SS2.p1.1.m1.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><apply id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1"><eq id="A2.SS2.p1.1.m1.1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1.1"></eq><ci id="A2.SS2.p1.1.m1.1.1.2.cmml" xref="A2.SS2.p1.1.m1.1.1.2">ğ›¼</ci><apply id="A2.SS2.p1.1.m1.1.1.3.cmml" xref="A2.SS2.p1.1.m1.1.1.3"><minus id="A2.SS2.p1.1.m1.1.1.3.1.cmml" xref="A2.SS2.p1.1.m1.1.1.3.1"></minus><apply id="A2.SS2.p1.1.m1.1.1.3.2.cmml" xref="A2.SS2.p1.1.m1.1.1.3.2"><times id="A2.SS2.p1.1.m1.1.1.3.2.1.cmml" xref="A2.SS2.p1.1.m1.1.1.3.2.1"></times><cn type="integer" id="A2.SS2.p1.1.m1.1.1.3.2.2.cmml" xref="A2.SS2.p1.1.m1.1.1.3.2.2">5</cn><ci id="A2.SS2.p1.1.m1.1.1.3.2.3.cmml" xref="A2.SS2.p1.1.m1.1.1.3.2.3">ğ‘’</ci></apply><cn type="integer" id="A2.SS2.p1.1.m1.1.1.3.3.cmml" xref="A2.SS2.p1.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">\alpha=5e-4</annotation></semantics></math> and moment estimate decay rates <math id="A2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="A2.SS2.p1.2.m2.1a"><mrow id="A2.SS2.p1.2.m2.1.1" xref="A2.SS2.p1.2.m2.1.1.cmml"><msub id="A2.SS2.p1.2.m2.1.1.2" xref="A2.SS2.p1.2.m2.1.1.2.cmml"><mi id="A2.SS2.p1.2.m2.1.1.2.2" xref="A2.SS2.p1.2.m2.1.1.2.2.cmml">Î²</mi><mn id="A2.SS2.p1.2.m2.1.1.2.3" xref="A2.SS2.p1.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="A2.SS2.p1.2.m2.1.1.1" xref="A2.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="A2.SS2.p1.2.m2.1.1.3" xref="A2.SS2.p1.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.2.m2.1b"><apply id="A2.SS2.p1.2.m2.1.1.cmml" xref="A2.SS2.p1.2.m2.1.1"><eq id="A2.SS2.p1.2.m2.1.1.1.cmml" xref="A2.SS2.p1.2.m2.1.1.1"></eq><apply id="A2.SS2.p1.2.m2.1.1.2.cmml" xref="A2.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS2.p1.2.m2.1.1.2.1.cmml" xref="A2.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS2.p1.2.m2.1.1.2.2.cmml" xref="A2.SS2.p1.2.m2.1.1.2.2">ğ›½</ci><cn type="integer" id="A2.SS2.p1.2.m2.1.1.2.3.cmml" xref="A2.SS2.p1.2.m2.1.1.2.3">1</cn></apply><cn type="float" id="A2.SS2.p1.2.m2.1.1.3.cmml" xref="A2.SS2.p1.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.2.m2.1c">\beta_{1}=0.9</annotation></semantics></math> and <math id="A2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\beta_{2}=0.99" display="inline"><semantics id="A2.SS2.p1.3.m3.1a"><mrow id="A2.SS2.p1.3.m3.1.1" xref="A2.SS2.p1.3.m3.1.1.cmml"><msub id="A2.SS2.p1.3.m3.1.1.2" xref="A2.SS2.p1.3.m3.1.1.2.cmml"><mi id="A2.SS2.p1.3.m3.1.1.2.2" xref="A2.SS2.p1.3.m3.1.1.2.2.cmml">Î²</mi><mn id="A2.SS2.p1.3.m3.1.1.2.3" xref="A2.SS2.p1.3.m3.1.1.2.3.cmml">2</mn></msub><mo id="A2.SS2.p1.3.m3.1.1.1" xref="A2.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="A2.SS2.p1.3.m3.1.1.3" xref="A2.SS2.p1.3.m3.1.1.3.cmml">0.99</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.3.m3.1b"><apply id="A2.SS2.p1.3.m3.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1"><eq id="A2.SS2.p1.3.m3.1.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1.1"></eq><apply id="A2.SS2.p1.3.m3.1.1.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS2.p1.3.m3.1.1.2.1.cmml" xref="A2.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS2.p1.3.m3.1.1.2.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2.2">ğ›½</ci><cn type="integer" id="A2.SS2.p1.3.m3.1.1.2.3.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3">2</cn></apply><cn type="float" id="A2.SS2.p1.3.m3.1.1.3.cmml" xref="A2.SS2.p1.3.m3.1.1.3">0.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.3.m3.1c">\beta_{2}=0.99</annotation></semantics></math>.
The network is trained end-to-end using Batch NormalizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> between each layer on the datasets described in Sec. <a href="#S3.SS3" title="3.3 Efficient Synthetic Dataset Generation â€£ 3 SimNet: Enabling Predictions for Manipulation From Synthetic Stereo â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. We trained the network from scratch with random weights.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS2.p2.2" class="ltx_p">For an input training stereo image <math id="A2.SS2.p2.1.m1.2" class="ltx_Math" alttext="(I_{\rm l},I_{\rm r})" display="inline"><semantics id="A2.SS2.p2.1.m1.2a"><mrow id="A2.SS2.p2.1.m1.2.2.2" xref="A2.SS2.p2.1.m1.2.2.3.cmml"><mo stretchy="false" id="A2.SS2.p2.1.m1.2.2.2.3" xref="A2.SS2.p2.1.m1.2.2.3.cmml">(</mo><msub id="A2.SS2.p2.1.m1.1.1.1.1" xref="A2.SS2.p2.1.m1.1.1.1.1.cmml"><mi id="A2.SS2.p2.1.m1.1.1.1.1.2" xref="A2.SS2.p2.1.m1.1.1.1.1.2.cmml">I</mi><mi mathvariant="normal" id="A2.SS2.p2.1.m1.1.1.1.1.3" xref="A2.SS2.p2.1.m1.1.1.1.1.3.cmml">l</mi></msub><mo id="A2.SS2.p2.1.m1.2.2.2.4" xref="A2.SS2.p2.1.m1.2.2.3.cmml">,</mo><msub id="A2.SS2.p2.1.m1.2.2.2.2" xref="A2.SS2.p2.1.m1.2.2.2.2.cmml"><mi id="A2.SS2.p2.1.m1.2.2.2.2.2" xref="A2.SS2.p2.1.m1.2.2.2.2.2.cmml">I</mi><mi mathvariant="normal" id="A2.SS2.p2.1.m1.2.2.2.2.3" xref="A2.SS2.p2.1.m1.2.2.2.2.3.cmml">r</mi></msub><mo stretchy="false" id="A2.SS2.p2.1.m1.2.2.2.5" xref="A2.SS2.p2.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.1.m1.2b"><interval closure="open" id="A2.SS2.p2.1.m1.2.2.3.cmml" xref="A2.SS2.p2.1.m1.2.2.2"><apply id="A2.SS2.p2.1.m1.1.1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="A2.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="A2.SS2.p2.1.m1.1.1.1.1.2">ğ¼</ci><ci id="A2.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="A2.SS2.p2.1.m1.1.1.1.1.3">l</ci></apply><apply id="A2.SS2.p2.1.m1.2.2.2.2.cmml" xref="A2.SS2.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A2.SS2.p2.1.m1.2.2.2.2.1.cmml" xref="A2.SS2.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="A2.SS2.p2.1.m1.2.2.2.2.2.cmml" xref="A2.SS2.p2.1.m1.2.2.2.2.2">ğ¼</ci><ci id="A2.SS2.p2.1.m1.2.2.2.2.3.cmml" xref="A2.SS2.p2.1.m1.2.2.2.2.3">r</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.1.m1.2c">(I_{\rm l},I_{\rm r})</annotation></semantics></math> and corresponding labels, the network is trained by minimizing <math id="A2.SS2.p2.2.m2.2" class="ltx_Math" alttext="\lambda_{\rm seg}\ell_{\rm seg}+\lambda_{\rm kp}\ell_{\rm kp}+\lambda_{\rm d}\ell_{\rm d}+\lambda_{\rm d}\ell_{\rm d,small}+\lambda_{\rm cov}\ell_{\rm cov}+\lambda_{\rm inst}\ell_{\rm inst}+\lambda_{\rm vrtx}\ell_{\rm vrtx}+\lambda_{\rm cent}\ell_{\rm cent}" display="inline"><semantics id="A2.SS2.p2.2.m2.2a"><mrow id="A2.SS2.p2.2.m2.2.3" xref="A2.SS2.p2.2.m2.2.3.cmml"><mrow id="A2.SS2.p2.2.m2.2.3.2" xref="A2.SS2.p2.2.m2.2.3.2.cmml"><msub id="A2.SS2.p2.2.m2.2.3.2.2" xref="A2.SS2.p2.2.m2.2.3.2.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.2.2.2" xref="A2.SS2.p2.2.m2.2.3.2.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.2.2.3" xref="A2.SS2.p2.2.m2.2.3.2.2.3.cmml">seg</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.2.1" xref="A2.SS2.p2.2.m2.2.3.2.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.2.3" xref="A2.SS2.p2.2.m2.2.3.2.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.2.3.2" xref="A2.SS2.p2.2.m2.2.3.2.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.2.3.3" xref="A2.SS2.p2.2.m2.2.3.2.3.3.cmml">seg</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.3" xref="A2.SS2.p2.2.m2.2.3.3.cmml"><msub id="A2.SS2.p2.2.m2.2.3.3.2" xref="A2.SS2.p2.2.m2.2.3.3.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.3.2.2" xref="A2.SS2.p2.2.m2.2.3.3.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.3.2.3" xref="A2.SS2.p2.2.m2.2.3.3.2.3.cmml">kp</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.3.1" xref="A2.SS2.p2.2.m2.2.3.3.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.3.3" xref="A2.SS2.p2.2.m2.2.3.3.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.3.3.2" xref="A2.SS2.p2.2.m2.2.3.3.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.3.3.3" xref="A2.SS2.p2.2.m2.2.3.3.3.3.cmml">kp</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1a" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.4" xref="A2.SS2.p2.2.m2.2.3.4.cmml"><msub id="A2.SS2.p2.2.m2.2.3.4.2" xref="A2.SS2.p2.2.m2.2.3.4.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.4.2.2" xref="A2.SS2.p2.2.m2.2.3.4.2.2.cmml">Î»</mi><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.4.2.3" xref="A2.SS2.p2.2.m2.2.3.4.2.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.4.1" xref="A2.SS2.p2.2.m2.2.3.4.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.4.3" xref="A2.SS2.p2.2.m2.2.3.4.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.4.3.2" xref="A2.SS2.p2.2.m2.2.3.4.3.2.cmml">â„“</mi><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.4.3.3" xref="A2.SS2.p2.2.m2.2.3.4.3.3.cmml">d</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1b" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.5" xref="A2.SS2.p2.2.m2.2.3.5.cmml"><msub id="A2.SS2.p2.2.m2.2.3.5.2" xref="A2.SS2.p2.2.m2.2.3.5.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.5.2.2" xref="A2.SS2.p2.2.m2.2.3.5.2.2.cmml">Î»</mi><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.5.2.3" xref="A2.SS2.p2.2.m2.2.3.5.2.3.cmml">d</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.5.1" xref="A2.SS2.p2.2.m2.2.3.5.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.5.3" xref="A2.SS2.p2.2.m2.2.3.5.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.5.3.2" xref="A2.SS2.p2.2.m2.2.3.5.3.2.cmml">â„“</mi><mrow id="A2.SS2.p2.2.m2.2.2.2.4" xref="A2.SS2.p2.2.m2.2.2.2.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.1.1.1.1" xref="A2.SS2.p2.2.m2.1.1.1.1.cmml">d</mi><mo id="A2.SS2.p2.2.m2.2.2.2.4.1" xref="A2.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="A2.SS2.p2.2.m2.2.2.2.2" xref="A2.SS2.p2.2.m2.2.2.2.2.cmml">small</mi></mrow></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1c" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.6" xref="A2.SS2.p2.2.m2.2.3.6.cmml"><msub id="A2.SS2.p2.2.m2.2.3.6.2" xref="A2.SS2.p2.2.m2.2.3.6.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.6.2.2" xref="A2.SS2.p2.2.m2.2.3.6.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.6.2.3" xref="A2.SS2.p2.2.m2.2.3.6.2.3.cmml">cov</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.6.1" xref="A2.SS2.p2.2.m2.2.3.6.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.6.3" xref="A2.SS2.p2.2.m2.2.3.6.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.6.3.2" xref="A2.SS2.p2.2.m2.2.3.6.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.6.3.3" xref="A2.SS2.p2.2.m2.2.3.6.3.3.cmml">cov</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1d" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.7" xref="A2.SS2.p2.2.m2.2.3.7.cmml"><msub id="A2.SS2.p2.2.m2.2.3.7.2" xref="A2.SS2.p2.2.m2.2.3.7.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.7.2.2" xref="A2.SS2.p2.2.m2.2.3.7.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.7.2.3" xref="A2.SS2.p2.2.m2.2.3.7.2.3.cmml">inst</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.7.1" xref="A2.SS2.p2.2.m2.2.3.7.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.7.3" xref="A2.SS2.p2.2.m2.2.3.7.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.7.3.2" xref="A2.SS2.p2.2.m2.2.3.7.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.7.3.3" xref="A2.SS2.p2.2.m2.2.3.7.3.3.cmml">inst</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1e" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.8" xref="A2.SS2.p2.2.m2.2.3.8.cmml"><msub id="A2.SS2.p2.2.m2.2.3.8.2" xref="A2.SS2.p2.2.m2.2.3.8.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.8.2.2" xref="A2.SS2.p2.2.m2.2.3.8.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.8.2.3" xref="A2.SS2.p2.2.m2.2.3.8.2.3.cmml">vrtx</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.8.1" xref="A2.SS2.p2.2.m2.2.3.8.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.8.3" xref="A2.SS2.p2.2.m2.2.3.8.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.8.3.2" xref="A2.SS2.p2.2.m2.2.3.8.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.8.3.3" xref="A2.SS2.p2.2.m2.2.3.8.3.3.cmml">vrtx</mi></msub></mrow><mo id="A2.SS2.p2.2.m2.2.3.1f" xref="A2.SS2.p2.2.m2.2.3.1.cmml">+</mo><mrow id="A2.SS2.p2.2.m2.2.3.9" xref="A2.SS2.p2.2.m2.2.3.9.cmml"><msub id="A2.SS2.p2.2.m2.2.3.9.2" xref="A2.SS2.p2.2.m2.2.3.9.2.cmml"><mi id="A2.SS2.p2.2.m2.2.3.9.2.2" xref="A2.SS2.p2.2.m2.2.3.9.2.2.cmml">Î»</mi><mi id="A2.SS2.p2.2.m2.2.3.9.2.3" xref="A2.SS2.p2.2.m2.2.3.9.2.3.cmml">cent</mi></msub><mo lspace="0em" rspace="0em" id="A2.SS2.p2.2.m2.2.3.9.1" xref="A2.SS2.p2.2.m2.2.3.9.1.cmml">â€‹</mo><msub id="A2.SS2.p2.2.m2.2.3.9.3" xref="A2.SS2.p2.2.m2.2.3.9.3.cmml"><mi mathvariant="normal" id="A2.SS2.p2.2.m2.2.3.9.3.2" xref="A2.SS2.p2.2.m2.2.3.9.3.2.cmml">â„“</mi><mi id="A2.SS2.p2.2.m2.2.3.9.3.3" xref="A2.SS2.p2.2.m2.2.3.9.3.3.cmml">cent</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.2.m2.2b"><apply id="A2.SS2.p2.2.m2.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3"><plus id="A2.SS2.p2.2.m2.2.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.1"></plus><apply id="A2.SS2.p2.2.m2.2.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.2"><times id="A2.SS2.p2.2.m2.2.3.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.2.1"></times><apply id="A2.SS2.p2.2.m2.2.3.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.2.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.2.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.2.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.2.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.2.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.2.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.2.2.3">seg</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.2.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.2.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.2.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.2.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.2.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.2.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.2.3.3">seg</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.3"><times id="A2.SS2.p2.2.m2.2.3.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.3.1"></times><apply id="A2.SS2.p2.2.m2.2.3.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.3.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.3.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.3.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.3.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.3.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.3.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.3.2.3">kp</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.3.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.3.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.3.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.3.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.3.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.3.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.3.3.3">kp</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.4.cmml" xref="A2.SS2.p2.2.m2.2.3.4"><times id="A2.SS2.p2.2.m2.2.3.4.1.cmml" xref="A2.SS2.p2.2.m2.2.3.4.1"></times><apply id="A2.SS2.p2.2.m2.2.3.4.2.cmml" xref="A2.SS2.p2.2.m2.2.3.4.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.4.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.4.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.4.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.4.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.4.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.4.2.3">d</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.4.3.cmml" xref="A2.SS2.p2.2.m2.2.3.4.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.4.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.4.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.4.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.4.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.4.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.4.3.3">d</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.5.cmml" xref="A2.SS2.p2.2.m2.2.3.5"><times id="A2.SS2.p2.2.m2.2.3.5.1.cmml" xref="A2.SS2.p2.2.m2.2.3.5.1"></times><apply id="A2.SS2.p2.2.m2.2.3.5.2.cmml" xref="A2.SS2.p2.2.m2.2.3.5.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.5.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.5.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.5.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.5.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.5.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.5.2.3">d</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.5.3.cmml" xref="A2.SS2.p2.2.m2.2.3.5.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.5.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.5.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.5.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.5.3.2">â„“</ci><list id="A2.SS2.p2.2.m2.2.2.2.3.cmml" xref="A2.SS2.p2.2.m2.2.2.2.4"><ci id="A2.SS2.p2.2.m2.1.1.1.1.cmml" xref="A2.SS2.p2.2.m2.1.1.1.1">d</ci><ci id="A2.SS2.p2.2.m2.2.2.2.2.cmml" xref="A2.SS2.p2.2.m2.2.2.2.2">small</ci></list></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.6.cmml" xref="A2.SS2.p2.2.m2.2.3.6"><times id="A2.SS2.p2.2.m2.2.3.6.1.cmml" xref="A2.SS2.p2.2.m2.2.3.6.1"></times><apply id="A2.SS2.p2.2.m2.2.3.6.2.cmml" xref="A2.SS2.p2.2.m2.2.3.6.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.6.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.6.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.6.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.6.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.6.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.6.2.3">cov</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.6.3.cmml" xref="A2.SS2.p2.2.m2.2.3.6.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.6.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.6.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.6.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.6.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.6.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.6.3.3">cov</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.7.cmml" xref="A2.SS2.p2.2.m2.2.3.7"><times id="A2.SS2.p2.2.m2.2.3.7.1.cmml" xref="A2.SS2.p2.2.m2.2.3.7.1"></times><apply id="A2.SS2.p2.2.m2.2.3.7.2.cmml" xref="A2.SS2.p2.2.m2.2.3.7.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.7.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.7.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.7.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.7.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.7.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.7.2.3">inst</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.7.3.cmml" xref="A2.SS2.p2.2.m2.2.3.7.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.7.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.7.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.7.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.7.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.7.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.7.3.3">inst</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.8.cmml" xref="A2.SS2.p2.2.m2.2.3.8"><times id="A2.SS2.p2.2.m2.2.3.8.1.cmml" xref="A2.SS2.p2.2.m2.2.3.8.1"></times><apply id="A2.SS2.p2.2.m2.2.3.8.2.cmml" xref="A2.SS2.p2.2.m2.2.3.8.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.8.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.8.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.8.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.8.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.8.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.8.2.3">vrtx</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.8.3.cmml" xref="A2.SS2.p2.2.m2.2.3.8.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.8.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.8.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.8.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.8.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.8.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.8.3.3">vrtx</ci></apply></apply><apply id="A2.SS2.p2.2.m2.2.3.9.cmml" xref="A2.SS2.p2.2.m2.2.3.9"><times id="A2.SS2.p2.2.m2.2.3.9.1.cmml" xref="A2.SS2.p2.2.m2.2.3.9.1"></times><apply id="A2.SS2.p2.2.m2.2.3.9.2.cmml" xref="A2.SS2.p2.2.m2.2.3.9.2"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.9.2.1.cmml" xref="A2.SS2.p2.2.m2.2.3.9.2">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.9.2.2.cmml" xref="A2.SS2.p2.2.m2.2.3.9.2.2">ğœ†</ci><ci id="A2.SS2.p2.2.m2.2.3.9.2.3.cmml" xref="A2.SS2.p2.2.m2.2.3.9.2.3">cent</ci></apply><apply id="A2.SS2.p2.2.m2.2.3.9.3.cmml" xref="A2.SS2.p2.2.m2.2.3.9.3"><csymbol cd="ambiguous" id="A2.SS2.p2.2.m2.2.3.9.3.1.cmml" xref="A2.SS2.p2.2.m2.2.3.9.3">subscript</csymbol><ci id="A2.SS2.p2.2.m2.2.3.9.3.2.cmml" xref="A2.SS2.p2.2.m2.2.3.9.3.2">â„“</ci><ci id="A2.SS2.p2.2.m2.2.3.9.3.3.cmml" xref="A2.SS2.p2.2.m2.2.3.9.3.3">cent</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.2.m2.2c">\lambda_{\rm seg}\ell_{\rm seg}+\lambda_{\rm kp}\ell_{\rm kp}+\lambda_{\rm d}\ell_{\rm d}+\lambda_{\rm d}\ell_{\rm d,small}+\lambda_{\rm cov}\ell_{\rm cov}+\lambda_{\rm inst}\ell_{\rm inst}+\lambda_{\rm vrtx}\ell_{\rm vrtx}+\lambda_{\rm cent}\ell_{\rm cent}</annotation></semantics></math>. To tune the loss weights, we use HyperBandÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> across 20 single gpu instances for 48 hours.
On a TITANXp GPU, our network can predict room level segmentation, OBBs of unknown objects, key-points and full resolution depth at 20 Hz.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Evaluation Criteria</h3>

<div id="A2.SS3.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.p1.1" class="ltx_p">In this section we will describe how mAP scores are computed for the 3D OBB and keypoint prediction tasks.</p>
</div>
<figure id="A2.F10" class="ltx_figure"><img src="/html/2106.16118/assets/figures/shirt_folds-min.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="A2.F10.2.1" class="ltx_text ltx_font_bold">Folded T-shirt Meshes:</span> Shown above is the fold sequence the robot is expected to perform during task execution. To simulate the distribution of shirt configurations encountered during folding, we source meshes of t-shirts created by artists and available for purchase online. We standardize the mesh dimensions in Blender, randomize their aspect ratios, and label vertices corresponding to keypoints.</figcaption>
</figure>
<section id="A2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.3.1 </span>OBB Prediction</h4>

<div id="A2.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.SSS1.p1.1" class="ltx_p">Similar to the 9DOF pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, we use 3D intersection over union or (3D IOU) to measure box fit. However, since the boxes computed from OBBs can rotate freely around symmetric axes, we purposely only consider a low acceptance criteria of <math id="A2.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="&gt;0.25" display="inline"><semantics id="A2.SS3.SSS1.p1.1.m1.1a"><mrow id="A2.SS3.SSS1.p1.1.m1.1.1" xref="A2.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="A2.SS3.SSS1.p1.1.m1.1.1.2" xref="A2.SS3.SSS1.p1.1.m1.1.1.2.cmml"></mi><mo id="A2.SS3.SSS1.p1.1.m1.1.1.1" xref="A2.SS3.SSS1.p1.1.m1.1.1.1.cmml">&gt;</mo><mn id="A2.SS3.SSS1.p1.1.m1.1.1.3" xref="A2.SS3.SSS1.p1.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.SSS1.p1.1.m1.1b"><apply id="A2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="A2.SS3.SSS1.p1.1.m1.1.1"><gt id="A2.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="A2.SS3.SSS1.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="A2.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="A2.SS3.SSS1.p1.1.m1.1.1.2">absent</csymbol><cn type="float" id="A2.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="A2.SS3.SSS1.p1.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.SSS1.p1.1.m1.1c">&gt;0.25</annotation></semantics></math> IOU. In practice, we notice there is significantly high correlation between this metric and grasp success during physical trials. The confidence value used during mAP calculation was the probability density of the predicted Gaussian peak around the object centroid.</p>
</div>
</section>
<section id="A2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.3.2 </span>Keypoint Prediction</h4>

<div id="A2.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS3.SSS2.p1.1" class="ltx_p">To classify whether a ground-truth keypoint was successfully predicted, we check whether a predicted keypoint for the class is within 20 pixels of the ground-truth keypoint. If no such predicted keypoint exists, then we consider it a false negative. If a predicted keypoint is not within 20 pixels of a ground-truth keypoint of the same class, it is considered a false positive. We average over the individual class scores to obtain mAP scores, and each point on each classâ€™s PR curve is computed by linearly varying the threshold for detection in the heatmap from 0 to 1.</p>
</div>
</section>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Manipulation Details</h3>

<div id="A2.SS4.p1" class="ltx_para ltx_noindent">
<p id="A2.SS4.p1.1" class="ltx_p">In this section, we will describe how robot manipulation policies are constructed for the object grasping and t-shirt folding experiments.</p>
</div>
<section id="A2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.4.1 </span>Novel Object Grasping</h4>

<div id="A2.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS4.SSS1.p1.1" class="ltx_p">Inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, given an OBB, the robot aligns the gripper with the largest principal axis. Thus, a bottle standing up would have a â€sideâ€ grasp where the robot grasps perpendicular to the face of the bottle. However, a stapler would have a â€topâ€ grasp where the robot aligns the gripper with the orientation of the object with a vertical approach direction. If an object has no dominant principle axes, like a cube, the robot favors â€sideâ€ grasps, which require less motion for the kinematics of the HSR. We note this is not meant to be an optimal grasping strategy for all objects, but is only meant to handle a wide variety of common household objects shown during the grasping trials.</p>
</div>
</section>
<section id="A2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.4.2 </span>T-Shirt Folding</h4>

<div id="A2.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS4.SSS2.p1.1" class="ltx_p">Given the 2D key-points, which predict neck, sleeves and bottom corners. We can encode a set of of grasps and pull behaviors that are relative to predicted key-points. Our t-shirt fold sequence is known colloquially as the Sideways Column method. The fold sequence consists of the following steps: 1) pull the left-most sleeve to the right-most sleeve, 2) pull the left-most bottom to the right-most bottom, 3) pull the sleeves inwards to be aligned with the neck of the shift, 4) pull the bottom of the t-shirt onto the top. An illustration of this fold sequence can be seen in Fig. Â <a href="#A2.F10" title="Figure 10 â€£ B.3 Evaluation Criteria â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div id="A2.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="A2.SS4.SSS2.p2.1" class="ltx_p">In order to compute the 3D locations of these grasp points, we need to project the 2D key-points onto the scene. We did this by leveraging the predicted disparity information and segmentation of the table. We first fit a 3D plane to the table using the disparity and segmentation output heads. We then project the 2D key-points onto the plane, by computing the ray-to-plane intersection point. Given the 3D locations of the keypoint the robot can then infer grasp position and pull locations.</p>
</div>
</section>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Perception Ablation Studies</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">We ablate different methods of using different sensing modalities (monocular, depth, RGB-D, stereo) in this section. We describe each baseline implementation in SectionÂ <a href="#A2.SS1" title="B.1 Baseline Implementation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>. Models are trained on the simulated small objects dataset generated with the Basler stereo pair camera model (SectionÂ <a href="#A1.SS1" title="A.1 Setup 1: Basler Stereo Pair â€£ Appendix A Stereo Data Collection Rig â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>). We collect a dataset of real images using the Basler stereo pair and annotate them with 3D oriented bounding boxes. This dataset only consists of optically easy scenarios, non-transparent objects with low amounts of natural lighting, and we therefore expect the depth-based networks to perform well. We hope that SimNet is able to roughly match or outperform as well on this task.</p>
</div>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p">We observe that predicting coarse disparity for the monocular, depth, and RGB-D networks results in very little difference in performance. We also find that simply stacking the RGB and D channels performs roughly the same as feeding them into separate feature extractors. Pure monocular reasoning thought does quite bad, but this is likely due to the 3D nature of the problem and not reflective of sim-to-real transfer. SimNet is able to have comparable performance to RGB-D transfer techniques, which suggest similar sim-to-real performance on optically easy scenarios between the methods.</p>
</div>
<figure id="A3.T5" class="ltx_table">
<table id="A3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T5.1.1.1" class="ltx_tr">
<th id="A3.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="A3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">3D mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T5.1.2.1" class="ltx_tr">
<td id="A3.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Mono</td>
<td id="A3.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.164</td>
</tr>
<tr id="A3.T5.1.3.2" class="ltx_tr">
<td id="A3.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Mono-Aux</td>
<td id="A3.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.169</td>
</tr>
<tr id="A3.T5.1.4.3" class="ltx_tr">
<td id="A3.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Depth</td>
<td id="A3.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.831</td>
</tr>
<tr id="A3.T5.1.5.4" class="ltx_tr">
<td id="A3.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Depth-Aux</td>
<td id="A3.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.838</td>
</tr>
<tr id="A3.T5.1.6.5" class="ltx_tr">
<td id="A3.T5.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RGB-D</td>
<td id="A3.T5.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.855</td>
</tr>
<tr id="A3.T5.1.7.6" class="ltx_tr">
<td id="A3.T5.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RGB-D-Aux</td>
<td id="A3.T5.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">0.864</td>
</tr>
<tr id="A3.T5.1.8.7" class="ltx_tr">
<td id="A3.T5.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RGB-D-Stack</td>
<td id="A3.T5.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">0.856</td>
</tr>
<tr id="A3.T5.1.9.8" class="ltx_tr">
<td id="A3.T5.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">RGB-D-Seq</td>
<td id="A3.T5.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">0.774</td>
</tr>
<tr id="A3.T5.1.10.9" class="ltx_tr">
<td id="A3.T5.1.10.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">SimNet</td>
<td id="A3.T5.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="A3.T5.1.10.9.2.1" class="ltx_text ltx_font_bold">0.921</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="A3.T5.3.1" class="ltx_text ltx_font_bold">Small Object Ablation Study:</span> We perform an ablation study of different perception procedures and modalities on the small objects dataset. We evaluate each model on real, annotated images on the OBB prediction task. We find that monocular networks perform very poorly, while depth and RGB-D perform much better. However, SimNet consistently performs the well on this dataset of optically easy scenarios.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Dataset Details</h2>

<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">2D Car Detection Dataset:</h5>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A4.SS0.SSS0.Px1.p1.1" class="ltx_p">To predict 2D bounding boxes for cars on the road, we designed a dataset that enables our network to learn relevant geometric features for cars. Given car poses from held out KITTIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> scenes, we sampled cars and camera positions to be in natural configurations to the natural world. Our car assets were taken from ShapeNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. For distractor objects, we used random meshes from Shapenet and scaled them to be the size of buildings, buses, pedestrians and buses. We then placed them randomly in the scene to be collision free and resting on the ground plane. Images of our generated environments can be seen in Fig. Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The car dataset generated in SimNet contains 50,000 stereo RGB images with annotations for 2D bounding boxes, segmentation masks, and disparity (FigureÂ <a href="#A4.F11" title="Figure 11 â€£ 2D Car Detection Dataset: â€£ Appendix D Dataset Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<figure id="A4.F11" class="ltx_figure"><img src="/html/2106.16118/assets/figures/car_simnet-min2.png" id="A4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span id="A4.F11.2.1" class="ltx_text ltx_font_bold">SimNet Car Dataset:</span> We simulate cars and large objects in SimNet to train networks for the KITTI object detection benchmark. Each stereo image is labeled in simulation with segmentation masks, bounding boxes, and disparity. We use bounding boxes instead of oriented bounding boxes in this dataset, because the KITTI task requires 2D bounding boxes.</figcaption>
</figure>
</section>
<section id="A4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Small Objects Dataset:</h5>

<div id="A4.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A4.SS0.SSS0.Px2.p1.1" class="ltx_p">For grasping objects on a table, we simulate indoor scenes by having a flat table with random objects from ShapenetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> placed on top of it. Our table is then surrounded by other random furniture from Shapenet that is randomly placed juxtaposed to the table. We then vary the size of the room and camera camera position relative to the table. Specifically, we sampled randomly positions from a half-sphere with a radius that is uniformly sampled between <math id="A4.SS0.SSS0.Px2.p1.1.m1.2" class="ltx_Math" alttext="[0.5,2]" display="inline"><semantics id="A4.SS0.SSS0.Px2.p1.1.m1.2a"><mrow id="A4.SS0.SSS0.Px2.p1.1.m1.2.3.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="A4.SS0.SSS0.Px2.p1.1.m1.2.3.2.1" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml">[</mo><mn id="A4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">0.5</mn><mo id="A4.SS0.SSS0.Px2.p1.1.m1.2.3.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml">,</mo><mn id="A4.SS0.SSS0.Px2.p1.1.m1.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.cmml">2</mn><mo stretchy="false" id="A4.SS0.SSS0.Px2.p1.1.m1.2.3.2.3" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.1.m1.2b"><interval closure="closed" id="A4.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.3.2"><cn type="float" id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1">0.5</cn><cn type="integer" id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.1.m1.2c">[0.5,2]</annotation></semantics></math> meters.</p>
</div>
<div id="A4.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="A4.SS0.SSS0.Px2.p2.1" class="ltx_p">This dataset consists of 50,000 stereo RGB images generated in SimNet. Each stereo image pair contains full annotations for full-resolution disparity images, OBBs, class and segmentation masks, as shown in Fig. <a href="#A4.F12" title="Figure 12 â€£ Small Objects Dataset: â€£ Appendix D Dataset Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.The same training set (and networks) is used for evaluation on both types of real objects (easy and hard). To train the RGB-D models, we make an equivalent dataset of simulated RGB-D images instead. We add structured noise to depth images to simulate artifacts commonly found in real images (FigureÂ <a href="#A2.F9" title="Figure 9 â€£ B.1 Baseline Implementation Details â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). For the real grasping experiments, we use the Zed 2 stereo pair for SimNet training and prediction and the HSRâ€™s Asus Xtion for RGB-D. For perception ablation studies, we use the Basler stereo pair and Kinect camera models, which are described in SectionÂ <a href="#A1.SS1" title="A.1 Setup 1: Basler Stereo Pair â€£ Appendix A Stereo Data Collection Rig â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<figure id="A4.F12" class="ltx_figure"><img src="/html/2106.16118/assets/figures/grasp_objects_simnet-min.png" id="A4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span id="A4.F12.2.1" class="ltx_text ltx_font_bold">SimNet Small Objects Dataset:</span> We simulate small objects placed on tabletops in SimNet. The dataset consists of stereo images with annotations for segmentation masks, oriented bounding boxes, and disparity.</figcaption>
</figure>
</section>
<section id="A4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">T-shirt Dataset:</h5>

<div id="A4.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="A4.SS0.SSS0.Px3.p1.1" class="ltx_p">We compile a dataset of 50 meshes of t-shirts and polos in various stages of folding that were purchased online (FigureÂ <a href="#A2.F10" title="Figure 10 â€£ B.3 Evaluation Criteria â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). We randomize the scale and aspect ratios of each shirt mesh before placing it on a flat table in the scene with surrounding furniture as in the small objects dataset. Each scene has 1-3 t-shirts, and we generate keypoint, segmentation, OBB, and full-resolution disparity annotations in simulation. This dataset consists of 50,000 stereo RGB images. Each stereo image pair contains annotations for full-resolution disparity images projected onto the left image, segmentation masks, oriented bounding boxes, and keypoints.</p>
</div>
<div id="A4.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="A4.SS0.SSS0.Px3.p2.1" class="ltx_p">The validation dataset is collected on the Toyota HSR using a mounted Zed 2 stereo pair and consists of 32 images. Each real image is manually annotated with keypoints. We consider shirt folding as a sequence of <math id="A4.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.SS0.SSS0.Px3.p2.1.m1.1a"><mn id="A4.SS0.SSS0.Px3.p2.1.m1.1.1" xref="A4.SS0.SSS0.Px3.p2.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px3.p2.1.m1.1b"><cn type="integer" id="A4.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px3.p2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px3.p2.1.m1.1c">4</annotation></semantics></math> folds (FigureÂ <a href="#A2.F10" title="Figure 10 â€£ B.3 Evaluation Criteria â€£ Appendix B Implementation Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>), and we label each stage of folding with a different semantic segmentation class. This is not strictly neccessary, but useful for failure detection when conducting robot experiments. If a grasp is missed, which is possible due to the large morphology of the gripper, this enables the planner to recognize that it is still in the same state. For the depth baselines, we use the Asus Xtion camera on the Toyota HSR. We present visualizations in FigureÂ <a href="#A4.F13" title="Figure 13 â€£ T-shirt Dataset: â€£ Appendix D Dataset Details â€£ SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure id="A4.F13" class="ltx_figure"><img src="/html/2106.16118/assets/figures/t-shirt-simnet-min.png" id="A4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span id="A4.F13.2.1" class="ltx_text ltx_font_bold">SimNet T-Shirt Folding Dataset:</span> We simulate t-shirts in various stages of folding on flat surfaces in the center of the workspace. We procedurally annotate each stereo image with segmentation masks, oriented bounding boxes, depth images, and keypoints. The keypoints correspond to the sleeves, neck, and bottom corners of each shirt. In this dataset, we label each stage of folding with different segmentation classes. This is useful for grasp failure detection when folding with the robot.</figcaption>
</figure>
</section>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Anonymous Submission"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Sim-to-Real, Computer Vision, Manipulation"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Proceedings of the 5th Conference on Robot Learning (CoRL 2021)"></div>
<div class="ltx_rdf" about="" property="dcterms:title"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.16116" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.16118" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.16118">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.16118" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.16119" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 12:49:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
