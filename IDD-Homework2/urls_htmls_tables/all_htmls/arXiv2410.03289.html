<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images</title>
<!--Generated on Fri Oct  4 10:01:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03289v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S1" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S2" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S3" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S5" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>RESULTS AND DISCUSSION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S6" title="In Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhijeet Patil, Garima Jain, Harsh Diwakar, Jay Sawant, Tripti Bameta, Swapnil Rane, Amit Sethi
</span><span class="ltx_author_notes">Abhijeet Patil (e-mail: abhijeetptl@iitb.ac.in), Garima Jain, Harsh Diwakar, Jay Sawant and Amit Sethi (e-mail: asethi@iitb.ac.in) are with Indian Institute of Technology, Mumbai - 400076, India Tripti Bameta and Swapnil Rane are with Tata Memorial Centre - ACTREC, HBNI, Navi Mumbai - 410210, India</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">We developed a software pipeline for quality control (QC) of histopathology whole slide images (WSIs) that segments various regions, such as blurs of different levels, tissue regions, tissue folds, and pen marks. Given the necessity and increasing availability of GPUs for processing WSIs, the proposed pipeline comprises multiple lightweight deep learning models to strike a balance between accuracy and speed. The pipeline was evaluated in all TCGAs, which is the largest publicly available WSI dataset containing more than 11,000 histopathological images from 28 organs. It was compared to a previous work, which was not based on deep learning, and it showed consistent improvement in segmentation results across organs. To minimize annotation effort for tissue and blur segmentation, annotated images were automatically prepared by mosaicking patches (sub-images) from various WSIs whose labels were identified using a patch classification tool HistoROI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib1" title="">1</a>]</cite>. Due to the generality of our trained QC pipeline and its extensive testing the potential impact of this work is broad. It can be used for automated pre-processing any WSI cohort to enhance the accuracy and reliability of large-scale histopathology image analysis for both research and clinical use. We have made the trained models, training scripts, training data, and inference results publicly available at <a class="ltx_ref ltx_href" href="https://github.com/abhijeetptl5/wsisegqc" title="">Github URL</a>, which should enable the research community to use the pipeline right out of the box or further customize it to new datasets and applications in the future.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The practice of pathology is undergoing a digitization process worldwide based on the adoption of whole slide scanners along with digital whole slide image (WSI) storage, management, and archiving solutions. Digital pathology not only supports telepathology and virtual tumor boards, but it also promises to enhance diagnostic accuracy, precision, and throughput with the application of computational pipelines based deep learning models and image processing algorithms. Such pipelines have shown promise in various problems, including detecting and classifying tumors in whole slide images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib2" title="">2</a>]</cite>, grading tumors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib3" title="">3</a>]</cite>, and predicting patient outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, these pipelines are usually not very robust to the real-world challenges of changes in slide and WSI quality by organizations, expertise, equipment, and digitization objectives. For instance, extra funding may be available to train technicians for preparing and scanning slides for a research project when compared to routine clinical reporting. Quality control of WSI archives is therefore essential to make computational diagnostic pipelines robust enough for adoption in clinical settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="582" id="S1.F1.g1" src="extracted/5901372/images/preds_all_colors.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Preview of results:</span> (A) WSI; (B) predictions of models that detects tissue (lilac), adipose (gray), pen mark (red), and tissue folds (purple), respectively; (C) predictions of blur-level detection model (brighter mask regions indicate more blur); and (D) a few zoomed in patches to show image details (boundary color coded to show their location in WSI).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this study, we introduce hand-annotated datasets and a novel technique for training segmentation models for histopathology. The datasets include annotations for pen marker segmentation and tissue fold segmentation, and we have developed four segmentation models for blur level segmentation, tissue segmentation, tissue fold segmentation, and pen marker segmentation. Our study aims to address the challenges associated with histopathology image analysis, particularly in identifying different tissue components and artifacts. The hand-annotated datasets and segmentation models we introduce can assist in improving the accuracy and efficiency of histopathology image analysis. Sample predictions from our models is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To develop our models, we utilized a novel technique for training segmentation models, which takes into account the unique characteristics of histopathology images. The four segmentation models we developed were designed to address different aspects of histopathology image analysis, such as identifying tissue components, blurry regions, tissue folds, and pen markers. We conducted a thorough analysis of the largest publicly available dataset to validate the accuracy and effectiveness of our models. Main contributions of our work are</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a novel method to train a segmentation model for histopathology images which utilizes domain knowledge generated from HistoROI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib1" title="">1</a>]</cite> for better sampling of data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We make hand-annotated datasets for pen marker segmentation and tissue fold segmentation publicly available. We release our training and inference codes along with weights of trained models publicly available. These datasets and models can help the research community to develop better and efficient quality control tools.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We also release the predictions of our models on more than 11,000 WSIs in TCGA data portal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib7" title="">7</a>]</cite>. These predictions can be used to develop weakly supervised learning algorithms for quality control of histopathology images. Also, researchers can utilize these predictions to prepare and choose their datasets keeping quality of images in check.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we give an overview of studies related to quality control in histopathology images. These studies can be categorized into two broad categories: traditional image processing-based methods and deep learning-based methods. Most of the studies focus on addressing specific types of tissue defects, while some attempt to solve multiple defects with a single model or separate models. HistoQC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib8" title="">8</a>]</cite> is by far the most widely used histopathology QC tool. It performs multiple tasks using conventional image processing, machine learning techniques and deep learning to identify regions such as useful tissue region, adipose-like region, background, tissue folds, out-of-focus region, etc. It also provides an easy to use HTML based UI to adjust parameters depending on staining protocols used for slide preparation. It has been shown that HistoQC can identify foreground regions correctly for more than 95% WSIs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A wide variety of work, inspired from natural image settings has been applied to pathology images to identify out-of-focus regions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib9" title="">9</a>]</cite>. This problem has seen a lot of application in the research community as well as in industrial settings. Identification of lens parameters is of paramount importance while capturing images through digital scanners, which can be guided by automatic focus assessment algorithms. It has been shown that by comparing various quality metrics that are used for natural images, such as SSIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib10" title="">10</a>]</cite> and IL-NIQE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib11" title="">11</a>]</cite>, sharply captured histopathology image patches can be distinguished from patches that are out-of-focus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib12" title="">12</a>]</cite>. A few studies have experimented with several full reference and no-reference image quality metrics and validated these metrics with pathologists <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib13" title="">13</a>]</cite>. Image gradient based features have also been used for identification of out-of-focus regions in WSIs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib14" title="">14</a>]</cite>. Few studies have trained machine learning based algorithms such as linear regression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib15" title="">15</a>]</cite>, random forest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib9" title="">9</a>]</cite>, etc. on top of image features to determine focus quality. Synthetically blurred images were also used in few studies to build classifiers to detect out-of-focus patches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib16" title="">16</a>]</cite>. Computationally-efficient kernels based on the human visual system, e.g. HVS-MaxPol, have also been utilized to detection out-of-focus regions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib17" title="">17</a>]</cite>. CNN based deep neural networks have also been employed for out-of-focus region detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib19" title="">19</a>]</cite>. Light weight neural networks have also shown capability to distinguish between sharp and out-of-focus images with high accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">There are relatively fewer studies concentrating on other types of defects compared to out-of-focus artifacts. A major challenge in addressing these defects is unavailability of annotated datasets and difficulty in creating them. These defects are also hard to generate synthetically. A few studies dealing with tissue fold segmentation utilize simple features, such as color and connectivity properties of tissue structures color saturation and luminance, to detect tissue folds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib21" title="">21</a>]</cite>. A few studies have also explored feature selection for distinguishing patches with tissue fold to be used by machine learning algorithms for classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib22" title="">22</a>]</cite>. Deep learning based classifiers have also been used to classify patches with tissue fold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">For tissue segmentation tasks, few studies have proposed CNN based segmentation models. Most of the studies used metrics like R/B ratio, pixel intensity, etc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib1" title="">1</a>]</cite> for tissue segmentation. GANs are being used to identify out-of-distribution patches as a proxy to segregate pen marker affected regions in WSIs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib25" title="">25</a>]</cite>. Systematic analysis of stains also uncovered potential to identify pen marker affected regions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Many studies try to identify problematic artifacts affected regions without segregating each type of artifact as a separate category. Approaches like supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib27" title="">27</a>]</cite>, weakly supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib28" title="">28</a>]</cite>, few-shot learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib29" title="">29</a>]</cite>, learning with noisy labels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib30" title="">30</a>]</cite>, active learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib1" title="">1</a>]</cite>, explainable AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib31" title="">31</a>]</cite>, etc. have been explored in this space. A few studies have also touched upon the importance of quality control, showcasing increase in WSI classification performance for various datasets. Reviews of QC techniques for histopathology images can also be found <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this study, we used a combination of publicly available datasets and manual annotations to train and evaluate our segmentation models. Specifically, we used two pre-existing datasets for training the tissue and blur segmentation models, and we created two custom datasets for training the pen marker and tissue fold segmentation models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We used the BRIGHT dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib34" title="">34</a>]</cite> for training the tissue segmentation and blur detection models. The BRIGHT dataset contains high-resolution whole slide images (WSIs) of breast cancer slides with a variety of conditions. The tissue segmentation and blur detection models were trained on patches extracted from this dataset while ensuring comprehensive coverage of relevant tissue features and artifacts. For blur detection, we introduced synthetic blur to tissue patches to augment the dataset and create a multi-class blur segmentation task to detect various levels of blur, which affect analysis tasks differently for various magnifications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To evaluate the performance of our models in real-world scenarios, we used over 11,000 WSIs from the TCGA data portal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib7" title="">7</a>]</cite>, the largest publicly available dataset for histopathology research. This dataset spans 28 different organs and provides a vast collection of histopathological images, which makes it ideal for testing the generalizability of our models. The TCGA dataset was used to analyze the effectiveness of our quality control (QC) pipeline and compare its performance to previous non-deep-learning-based methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">In addition to these public datasets, we created two custom hand-annotated datasets specifically for the pen marker and tissue fold segmentation tasks. While these artifacts pose significant challenges to the automation of histopathology image analysis, publicly available annotated datasets for these artifacts were lacking. The details of how these datasets were created are the following:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Pen Marker Dataset:</span> This dataset was created by selecting 350 WSIs from the TCGA data portal and annotating pen marks at a magnification of 0.625X. The pen marker annotations encompass a diverse set of tissue types and pen ink colors to ensure that the model trained on it can detect a wide range of marker artifacts commonly seen in clinical settings.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Tissue Fold Dataset:</span> For the tissue fold segmentation task, we utilized 250 patches from the BRIGHT dataset at 5X magnification. Tissue folds represent an endemic artifact in histopathology WSIs, and the diverse tissue samples in this dataset provided a strong foundation for training an accurate model. Each patch was carefully annotated to highlight fold regions, thus ensuring precise detection by our segmentation model.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">By utilizing a combination of publicly available and custom datasets, we ensured that our models were trained on diverse data covering a wide range of tissue types, staining protocols, and artifact presentations. This diversity is crucial for the models’ ability to generalize across different clinical and research datasets, making the pipeline robust and widely applicable. All datasets, along with the models, training scripts, and inference results, are publicly available on our <a class="ltx_ref ltx_href" href="https://github.com/abhijeetptl5/wsisegqc" title="">GitHub repository</a>, to enable further research and development for the problem of histopathology image quality control.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The accurate identification and classification of artifacts is essential for effective analysis of WSIs. In this study, we developed four distinct artifact segmentation models, each tailored to a different type of artifact: blur level, tissue fold, pen marker, and tissue segmentation. We utilized annotated datasets for training the pen marker and tissue fold segmentation models, and incorporated an image collage approach for the blur level segmentation and tissue segmentation models, using the predictions generated by HistoROI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib1" title="">1</a>]</cite>. In this section, we provide a detailed account of our methodology, including the architecture of our models, the datasets used for training and testing. Additionally, we describe the techniques we employed to optimize our models, including data augmentation and fine-tuning.</p>
</div>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Image collage method</h3>
<div class="ltx_para ltx_noindent" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">Training segmentation models for WSI level can be a challenging task as it requires obtaining a large amount of image annotations, which take substantial time and effort. Even if annotated data is available, the data distribution is often skewed, which can lead to a biased model. Training deep neural networks requires each batch of data to contain representative data, but this is not always possible with WSIs since ROIs generally span thousands of pixels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx1.p2">
<p class="ltx_p" id="S4.SSx1.p2.1">To address this challenge, we have used a novel image collage method for training segmentation networks. This method involves copying patches with known labels (corresponding to the majority of their pixels) from WSIs and pasting them together with patches from other WSIs after downscaling fixed size to form larger images composed of diverse patches. The annotation masks of these larger images are like pixelated grids, where pixels in each cell of the grid have the same annotation label. This not only ensures that we can easily get annotation masks for training segmentation models but also ensure that each training batch contains representative data to to overcome the class imbalance in WSIs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx1.p3">
<p class="ltx_p" id="S4.SSx1.p3.1">We have used the HistoROI patch classification model to assist batch formation in the image collage method. HistoROI model predicts the class of a patch of a WSI, which can be epithelial, stroma, lymphocytes, adipose, miscellaneous, or artifact. By using this model, we can select patches or regions from specific classes, which ensures that the balance between different types of tissue regions is maintained during the training of the segmentation model. The HistoROI model uses a deep neural network trained on patch-level data, which makes it highly accurate in predicting the class of a patch, but it is not trained for segmentation. By combining the image collage method and HistoROI model predictions, we were able to overcome the challenge of lack of annotated data and skewed data distribution. This allowed us to train highly accurate segmentation models for WSI level tasks.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="313" id="S4.F2.g1" src="extracted/5901372/images/cut_paste_tissue.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S4.F2.2.1">Image collage method</span> to generate uniform data distribution across all classes. (a) A grid of patches of size 64x64 placed on the empty array of size 512x512. (b),(c) A WSI alongwith its HistoROI predictions is sampled from a pool of WSIs selected from training dataset. (d) A patch of required class is randomly selected from WSI. Index of patch is obtained by HistoROI predictions. Patches from 2.5X magnification are used to train this model. A CNN segmentation model is trained using randomly generated masks and images created by pasting patches from WSI. Generated images can have patches from multiple WSIs, making the segmentation model more robust.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Tissue segmentation model</h3>
<div class="ltx_para ltx_noindent" id="S4.SSx2.p1">
<p class="ltx_p" id="S4.SSx2.p1.1">The training starts with dataset preparation, where empty 2D arrays of 512x512 are divided into 8x8 grid cells of size 64x64 pixels each. Each grid cell is randomly assigned one of three labels — foreground tissue, adipose, or background — to create annotation masks. For each cell, patches are mined from a pre-selected pool of twenty WSIs for each class, ensuring sufficient variation for accurate model training. These patches are extracted at a 2.5X magnification, balancing the need for high-resolution images with computational efficiency. This data generation process is demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.F2" title="Figure 2 ‣ Image collage method ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx2.p2">
<p class="ltx_p" id="S4.SSx2.p2.1">This segmentation strategy not only utilizes the precise predictions of HistoROI but also introduces necessary variability by randomly selecting patches from diverse WSIs. This approach guarantees that each class is evenly represented, making the model robust and capable of generalizing to new, unseen data. Adipose tissue is treated as a separate class, distinct from the background, to address the challenge of differentiating between non-diagnostic adipose tissue and the relevant background, which includes the environment outside tissue regions like coverslip artifacts and pen markers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx2.p3">
<p class="ltx_p" id="S4.SSx2.p3.1">To further enhance the model’s ability to discriminate tissue from non-tissue elements, background patches were augmented using color jitter. This technique randomly alters the color and intensity values, providing a richer training set that prepares the model to accurately identify and ignore irrelevant artifacts. By leveraging HistoROI predictions and strategic patch mining with color jitter augmentation, we create a balanced and varied training dataset that improves the performance and accuracy of our tissue detection algorithms on WSIs. We have also leveraged color normalization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib36" title="">36</a>]</cite> to make this model robust.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Blur level segmentation</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="416" id="S4.F3.g1" src="extracted/5901372/images/cut_paste_blur.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S4.F3.2.1">Image collage method</span> for training blur segmentation model.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SSx3.p1">
<p class="ltx_p" id="S4.SSx3.p1.1">Detecting blur levels in WSI can be challenging as they have a texture that varies according to the region of interest. For example, a normal stroma has a smoother texture than normal cellular region. The laplacian-based methods often fail to provide consistent performance in detecting blur. However, by using patch mining with HistoROI, the model can learn the normal texture of a particular type of region in the WSI. This helps to mitigate the problems that Laplacian-based methods often face. The HistoROI model assists in learning the texture of specific regions, enabling the segmentation model to identify the degree of blur more accurately. This synthetic data approach, along with data mining strategy from HistoROI is useful in situations where obtaining real-world annotated data is difficult or impossible.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx3.p2">
<p class="ltx_p" id="S4.SSx3.p2.1">To develop a segmentation model for detecting blur levels, we synthetically blur patches identified as foreground by the HistoROI model. This creates an eight-class segmentation problem, where Class 0 represents no blur and Class 7 represents maximum blur. To introduce blur, we use the Boxblur method available in the PIL library, and gradually increase the blur level from 1 to 7 using the parameters of Boxblur. The model is trained using patches from a magnification level of 5X. To generate the input and ground truth pair, we started by creating a mask of size 512x512. This mask was then divided into a grid of 4x4 equal-sized cells. Each cell was randomly assigned a label between 0 and 7, indicating the level of blur. The corresponding input image was created using the HistoROI predictions and the Boxblur function of the PIL library. A sample input and segmentation mask is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.F3" title="Figure 3 ‣ Blur level segmentation ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">3</span></a>. Sample predictions are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.F4" title="Figure 4 ‣ Blur level segmentation ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx3.p3">
<p class="ltx_p" id="S4.SSx3.p3.1">Synthetic data generation for blur level segmentation in histopathology images can be approached in multiple ways. In our final model, we have adopted the following strategy: We sample a patch of size 1024x1024 at 40X magnification. This patch is then blurred using the Boxblur function while still at 40X. After applying the blur, we resize the patch to 128x128, effectively bringing it to 5X magnification. Although ideally, the model should utilize 40X (or level 0) data for blur estimation, processing such high-resolution images is computationally intensive due to the large size of WSIs. To generate synthetic blur data, blurring can be applied to images at 5X, 10X, 20X, or 40X magnifications and then resizing them to 5X to obtain data with varying levels of blur. We experimented with all these combinations and found that applying blur at 40X and subsequently resizing to 5X provided the best results. This approach retains the high-quality details from the 40X images while ensuring the final data is at the 5X resolution required for training and inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx3.p4">
<p class="ltx_p" id="S4.SSx3.p4.1">By adopting this method, we achieve a balance between maintaining detailed image quality and managing computational efficiency. The blurring at 40X before resizing ensures a more realistic simulation of blur effects, and improves the model’s performance in blur level segmentation tasks. With this strategy, we not only detect regions with a high level of blur but also identify regions with subtle focus-related issues caused by scanner calibration problems, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.F4" title="Figure 4 ‣ Blur level segmentation ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">4</span></a>. This enhances the model’s ability to address both major and minor focus discrepancies in histopathology images and improves the accuracy and reliability of our segmentation model.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="643" id="S4.F4.g1" src="extracted/5901372/images/blur_model_pred_.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.2.1">Blur Segmentation model prediction:</span> Model predicts no blur for red patch. Blue patch shows out of focus region, correctly predicted as blur (class 7) by blur segmentation model. Green patch shows capability of our model to detect subtle blur artifacts due to potential scanner calibration issues.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SSx4">
<h3 class="ltx_title ltx_title_subsection">Tissue Folds and Pen marker segmentation</h3>
<div class="ltx_para ltx_noindent" id="S4.SSx4.p1">
<p class="ltx_p" id="S4.SSx4.p1.1">To train segmentation models for detecting pen markers and tissue folds on whole slide images, we used a consistent data sampling strategy. For pen markers, we identified a positive pixel in a randomly selected mask and used the surrounding 512x512 patch as the training input. This method ensures the model trains on a variety of pen marker colors, enhancing generalization. For tissue fold detection, we used similar data sampling strategy on tissue fold segmentation dataset. Both models were trained using an 80%-20% split for training and validation, ensuring a balanced approach to model development.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx5">
<h3 class="ltx_title ltx_title_subsection">Model architectures</h3>
<div class="ltx_para ltx_noindent" id="S4.SSx5.p1">
<p class="ltx_p" id="S4.SSx5.p1.1">The four segmentation models under consideration have been evaluated using two architectures: UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib37" title="">37</a>]</cite> and UNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib38" title="">38</a>]</cite>, each with multiple backbones, including ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib39" title="">39</a>]</cite>, ResNet34, and EfficientNet-b0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib40" title="">40</a>]</cite>. These backbones are pre-trained on the ImageNet dataset, ensuring the models can effectively extract useful features from input images. We have utilized PyTorch, a popular deep learning framework, for training these models, along with the segmentation-models-pytorch library, which is available via PIP. This library offers a range of pre-defined architectures and loss functions commonly used in image segmentation tasks, simplifying the implementation process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SSx5.p2">
<p class="ltx_p" id="S4.SSx5.p2.1">Model performance and inference time for each model are shown in the Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.T1" title="Table 1 ‣ Model architectures ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">1</span></a>. Considering both execution time and accuracy, we have selected the UNet with ResNet18 backbone for tissue fold, blur, and tissue segmentation tasks. For pen marker segmentation, we have chosen the UNet++ with ResNet34 backbone. Although UNet++ with the ResNet34 backbone performs better for all tasks, its inference time is almost three times that of UNet with the ResNet18 backbone. However, we opted for UNet++ with ResNet34 for pen marker segmentation because this model uses thumbnails at a 0.625X magnification level for predictions. The overall impact of the pen marker model on the inference time for combined model inference is not significant. Inference times for each combination of tried models for all the training task is given in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S4.T1" title="Table 1 ‣ Model architectures ‣ 4 Methodology ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:158.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.8pt,-7.2pt) scale(1.10047875053654,1.10047875053654) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.2.1">Tissue Seg</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.3.1">Blur Seg</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.4.1">Folds Seg</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.5.1">Pen Seg</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.2.1">Dice</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.2.2">Time</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.2.3">AUC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.2.4">Time</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.2.5">Dice</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.2.6">Time</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.2.7">Dice</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.2.2.8">Time</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.3.1.1">UNet-ENet-b0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.2">0.882</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.3">82.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.4">0.862</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.5">293.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.6">0.793</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.7">301.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.8">0.904</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.9">2.89</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.4.1.1">UNet-ResNet18</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.2">0.913</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.3">97.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.4">0.875</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.5">363.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.6">0.825</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.7">384.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.8">0.926</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.9">2.93</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.5.1.1">UNet-ResNet34</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.2">0.904</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.3">114.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.4">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.5">423.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.6">0.817</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.7">415.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.8">0.912</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.9">3.16</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.6.6.1.1">UNet++-ENet-b0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.2">0.891</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.3">215.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.4">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.5">781.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.6">0.782</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.7">841.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.8">0.892</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.9">4.71</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.7.7.1.1">UNet++-ResNet18</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.7.7.2">0.911</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.7.7.3">260.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.7.7.4">0.879</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.7.7.5">827.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.7.7.6">0.812</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.7.7.7">861.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.7.7.8">0.924</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.7.7.9">5.06</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.8.8.1.1">UNet++-ResNet34</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.8.8.2">0.914</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.3">320.68</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.8.8.4">0.893</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.5">1049.47</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.8.8.6">0.835</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.7">974.47</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.8.8.8">0.947</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.9">5.62</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance and inference times of different variants of four segmentation models in our study. Dice score on TCGA-Foreground dataset, AUC-ROC on FocusPath dataset and Dice scores on hand annotated tissue folds and pen marker segmentation dataset along with inference time (in seconds) on randomly selected 10 WSIs from TCGA dataset.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RESULTS AND DISCUSSION</h2>
<section class="ltx_subsection" id="S5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Comparison with HistoQC – WSI level</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="S5.F5.g1" src="extracted/5901372/images/experiment_1_.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Agreement between HistoQC and our pipeline for 11,650 WSIs in TCGA dataset. Bar charts shows distribution of WSIs according to Dice score agreement between our pipeline and HistoQC. Pie charts show subjective comparison on 20 WSIs sampled from each bucket of agreement by pathologist between masks predicted by our pipeline and HistoQC (Green - Our better, Red - HistoQC better, Blue - inconclusive). Our models perform better than HistoQC.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p1">
<p class="ltx_p" id="S5.SSx1.p1.1">To evaluate the performance of our pipeline, we compared it with HistoQC, a widely used quality control tool. We ran all 11,666 WSIs available on the TCGA data portal through HistoQC’s default parameters to generate a foreground tissue mask, and performed a similar procedure to predict foreground masks through our proposed pipeline. We then computed the Dice score between the masks to measure the agreement between the two predictions. A high Dice score indicates a high agreement between HistoQC and our pipeline, indicating that the masks generated by both strategies are either equally good or equally bad. However, in cases of lower agreement (low Dice score), there may be instances where HistoQC performs better, our pipeline performs better, or it may not be possible to determine which mask is better. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S5.F5" title="Figure 5 ‣ Comparison with HistoQC – WSI level ‣ 5 RESULTS AND DISCUSSION ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes findings from this experiment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p2">
<p class="ltx_p" id="S5.SSx1.p2.1">We sorted the Dice scores of the foreground tissue masks generated by HistoQC and our pipeline and divided them into buckets according to the Dice scores. We sampled 20 WSIs from each bucket, resulting in a total of 100 WSIs, which were then manually analyzed for quality by pathologists. Proportion of WSIs in each bucket is observed to be 60%, 20%, 10%, 5% and 5% for the bins of dice 0.8 to 1, 0.6 to 0.8, 0.4 to 0.6, 0.2 to 0.4 and 0 to 0.2 respectively. The pathologists were presented with two masks in QuPath<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#bib.bib41" title="">41</a>]</cite> for each WSI without being told how these masks were generated. For each pair of masks, the pathologists were asked to select the better one. The order of the two masks for each WSI was randomized.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx1.p3">
<p class="ltx_p" id="S5.SSx1.p3.1">From the bucket of the most agreement, the outcome of comparison to most of the WSIs was not conclusive, which was expected because of a higher Dice score. Out of 20 WSI mask pairs in this bucket, comparison for 14 was inconclusive, for 5 WSIs our pipeline performed better than HistoQC and for one WSI, HistoQC performed better. Results were more conclusive on the other end, where our pipeline performed better than HistoQC for 14 and 16 WSIs for the dice bucket of 0.2 to 0.4 and 0 to 0.2 respectively. A few samples were inconclusive in these buckets because both the models performed equally bad for few WSIs. Though our predictions are better than HistoQC, comparison based on WSIs predictions becomes a hectic task and can add too much subjectivity and bias while comparing different masks. Therefore we have carried out patch level mask comparison to address minute differences in HistoQC and our pipeline.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Comparison with HistoQC – Patch level</h3>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="412" id="S5.F6.g1" src="extracted/5901372/images/experiment_2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparative Analysis of Model Performances Against HistoQC. Left half shows comparison for SET-1 and right half for SET-2. For all masks, green color indicates foreground and red indicates background. For patches marked with green border, our mask is subjectively better than HistoQC and for red border, HistoQC mask is better. Bottom bar chart shows statistics for overall comparison on SET-1 and SET-2 (Green-Ours better, Red-HistoQC better, Blue-Non-conclusive). For both sets, our model performs better than HistoQC.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p1">
<p class="ltx_p" id="S5.SSx2.p1.1">In this experiment, we have systematically sampled patches from WSIs along with foreground masks predicted by our pipeline and by HistoQC. We have selected 100 WSIs from the TCGA dataset for extracting patches in this experiment. We first identify the regions of disagreements between both the masks, and then sample 5 patches from a region where HistoQC predicted a foreground but our pipeline predicted background and vice-versa. With this strategy, we sample 10 patches from each of 100 WSIs, generating a dataset of 1,000 patches along with their predictions by HistoQC and our pipeline. We call a set of 500 patches for which the center pixel of prediction is foreground for HistoQC but background for our pipeline, SET 1 and for another scenario, SET 2. Each of these sets contains 500 patches. Results from this experiment is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S5.F6" title="Figure 6 ‣ Comparison with HistoQC – Patch level ‣ 5 RESULTS AND DISCUSSION ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx2.p2">
<p class="ltx_p" id="S5.SSx2.p2.1">Out of 500 patches from SET 1, comparison for 138 patches is non conclusive. From the remainder of 362 patches, our pipeline performed better on 286 patches and HistoQC performed better on 86 patches. In the majority of cases, HistoQC can not identify tissue folds as background but our pipeline can predict tissue folds correctly. On the other hand, our model predicts background for debris, etc, which HistoQC correctly predicts as foreground. Analysis of SET 2 uncovers false positives predicted by our pipeline for background class. Overall, performance of our pipeline is better than that of HistoQC in this set as well. Out of 500 patches, 196 were not conclusive, whereas 243 patches from our pipeline were observed to be better than HistoQC. Out of 61 patches where HistoQC performed better, most of these patches correspond to small bubbles or wipes on glass slides.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SSx3">
<h3 class="ltx_title ltx_title_subsection">Quality Analysis of TCGA WSIs</h3>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="358" id="S5.F7.g1" src="extracted/5901372/images/experiment_3_boxes.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Proportion of different artifacts detected by our models. Few TCGA data centers exhibits higher proportion of artifacts, while some centers have cleaner WSIs.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p1">
<p class="ltx_p" id="S5.SSx3.p1.1">In this section, we present a summary of our analysis on the TCGA dataset’s data quality, evaluated using models across various data centers and organs. We document the strengths and weaknesses of our models, pinpointing specific pipeline issues with detailed examples to guide future enhancements. Our detailed analysis of TCGA diagnostic slides highlights the proportion of artifact regions across three centers: Asterend, Memorial Sloan Kettering, and MD Anderson, represented by colored box plots as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S5.F7" title="Figure 7 ‣ Quality Analysis of TCGA WSIs ‣ 5 RESULTS AND DISCUSSION ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">7</span></a>. We analyzed different types of artifacts, revealing that Asterend has the highest artifact presence, while Memorial Sloan Kettering has fewer. This data is crucial for model training and validation, helping to avoid biases introduced by artifacts. By understanding artifact distribution, we can better design experiments to ensure models handle real-world imperfections effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p2">
<p class="ltx_p" id="S5.SSx3.p2.1">We have conducted a thorough analysis of the masks generated by our models to identify their strengths and weaknesses. This analysis began by grouping all WSIs by TCGA center codes and calculating the average proportions of tissue, adipose, blur, pen markers, and tissue folds for each center. With 693 unique center codes in the TCGA data, we focused on the top 20 center codes, averaging these proportions to identify potential outliers. By manually reviewing WSIs from these top 20 center codes, we aimed to determine if our models performed unexpectedly in these outlier cases. The rationale was that outlier center codes might contain more mispredictions compared to a randomly selected set of WSIs, providing a targeted approach to identifying model shortcomings. Additionally, we sorted the WSIs based on their blur, tissue fold, and pen marker proportions. We then manually reviewed 50 WSIs from each of these sorted groups. This manual inspection was crucial in understanding how our models handled different artifacts and identifying specific scenarios where the models might falter. Few of the patches where models have predicted inaccurately are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03289v1#S5.F8" title="Figure 8 ‣ Quality Analysis of TCGA WSIs ‣ 5 RESULTS AND DISCUSSION ‣ Semantic Segmentation Based Quality Control of Histopathology Whole Slide Images"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p3">
<p class="ltx_p" id="S5.SSx3.p3.1">This meticulous manual checking process revealed several interesting findings, which are documented further. It highlighted areas where the models performed well and areas needing improvement, particularly in handling high proportions of blur, tissue folds, and pen markers. The insights gained from this analysis are invaluable for refining our models. By understanding the conditions under which our models struggle, we can make targeted improvements, such as enhancing training data diversity or adjusting model parameters. This ongoing evaluation and refinement process is essential for developing robust and reliable models capable of performing well across various real-world conditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p4">
<p class="ltx_p" id="S5.SSx3.p4.1">In our evaluation of tissue segmentation models, we have identified several instances of false positives related to artifacts on the glass slides. These false positive regions typically exhibit a regular texture resulting from mounting issues and DPX deposits. Despite these challenges, the model demonstrated robust performance even with lightly and dark stained images. This indicates that our tissue segmentation model is effective in identifying tissue regions under varying staining conditions. One notable strength of the model is its capability to distinguish between adipose tissue and foreground tissue types. For instance, in the predictions of several lung tissue WSIs, where the tissue has a very similar appearance to adipose tissue, the model successfully identified these regions as foreground tissue. This ability to differentiate between similar tissue types highlights the model’s precision and accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p5">
<p class="ltx_p" id="S5.SSx3.p5.1">However, one limitation of the model is its inability to identify out-of-focus tissue regions as tissue. While this might seem like a shortcoming, it is not particularly concerning since out-of-focus regions are generally unsuitable for any meaningful analysis. Another issue we observed in the predictions of a few WSIs was the presence of many tiny foreground contours. These regions mostly correspond to thick tissue cuts.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="132" id="S5.F8.g1" src="extracted/5901372/images/mispredictions.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="S5.F8.2.1">Sample mispredictions:</span> (A, B) Tissue detection model assigns background class to thick cuts and IHC stained images. (C) Blur segmentation model assigns high blur level to thyroid inter-follicular spaces filled with colloid. (D) Tissue fold model predicts combination of blurry and dark stained regions as tissue fold.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p6">
<p class="ltx_p" id="S5.SSx3.p6.1">The predictions generated by our blur detection model provided significant insights into the TCGA datasets, revealing distinct patterns of blur-related artifacts across various data centers. These artifacts often appear to be related to calibration issues with scanner lenses, as evidenced by the recurring patterns observed in specific centers. One of the strengths of our model lies in its ability to detect subtle blur-related artifacts, thanks to our synthetic data generation strategy employed at 40X magnification. This approach has enabled the model to identify even minor instances of blur with high accuracy. However, the model is not without its limitations and has shown some mispredictions. For instance, in several thyroid WSIs, the model mistakenly identified smooth inter-follicular spaces filled with colloids as blur artifacts. Similarly, in some lung cancer WSIs, cartilage areas were incorrectly identified as blur artifacts. These findings underscore that the model’s performance can vary depending on the specific tissue types it encounters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p7">
<p class="ltx_p" id="S5.SSx3.p7.1">Many of the artifacts present in WSIs, such as air bubbles, tissue folds, and issues related to tissue cutting, often generate blur in problematic regions. Our model has demonstrated proficiency in identifying these types of blur artifacts accurately. This capability is crucial for enhancing the quality and reliability of tissue segmentation in digital pathology, where the presence of such artifacts can significantly impact the accuracy of downstream analyses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p8">
<p class="ltx_p" id="S5.SSx3.p8.1">Our tissue folds segmentation model performed well in most cases, with few false positives observed when encountering a combination of out-of-focus regions with dark stains. These false positives are not particularly concerning because the model is not misclassifying useful tissue as an artifact; rather, it is misclassifying blur—another type of artifact—as tissue fold. This observation is consistent across other types of artifacts. Most artifacts create a blurry region around them. For instance, when a glass slide cracks, the tissue near the affected area becomes blurry. Similarly, air bubbles cause blurriness around their periphery or, in some cases, throughout the entire region within the bubble. Regarding the pen marker segmentation model, we observed a few false positives in the presence of pools of blood cells, particularly in organs like the breast and kidney. These misclassifications highlight specific challenges in distinguishing between certain biological structures and artifacts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SSx3.p9">
<p class="ltx_p" id="S5.SSx3.p9.1">Overall, these findings are valuable for the development of next-generation models with improved accuracy. By understanding the conditions under which current models falter—such as in the presence of blur, dark stains, or specific biological structures—we can refine our training datasets and model architectures. This iterative process of evaluation and improvement will enhance the reliability and performance of segmentation models in digital pathology, ultimately leading to more accurate and useful tools for both diagnostic and research purposes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we introduced a quality control pipeline for histopathology images using four different segmentation models, enhanced by HistoROI predictions and the image collage method to streamline tissue and blur segmentation. Our approach demonstrates superior performance on the largest publicly available dataset, and we have made all models, scripts, and results freely available to foster further advancements in this field. Our pipeline offers a promising method for improving histopathology image analysis, with significant potential for future enhancements that could lead to more accurate and efficient techniques.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abhijeet Patil, Harsh Diwakar, Jay Sawant, Nikhil Cherian Kurian, Subhash Yadav, Swapnil Rane, Tripti Bameta, and Amit Sethi.

</span>
<span class="ltx_bibblock">Efficient quality control of whole slide pathology images with human-in-the-loop training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Journal of Pathology Informatics</span>, page 100306, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood.

</span>
<span class="ltx_bibblock">Data-efficient and weakly supervised computational pathology on whole-slide images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Nature biomedical engineering</span>, 5(6):555–570, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Eirini Arvaniti, Kim S Fricker, Michael Moret, Niels Rupp, Thomas Hermanns, Christian Fankhauser, Norbert Wey, Peter J Wild, Jan H Rueschoff, and Manfred Claassen.

</span>
<span class="ltx_bibblock">Automated gleason grading of prostate cancer tissue microarrays via deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Scientific reports</span>, 8(1):12054, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Emad A Rakha, Mohamed Aleskandarani, Michael S Toss, Andrew R Green, Graham Ball, Ian O Ellis, and Leslie W Dalton.

</span>
<span class="ltx_bibblock">Breast cancer histologic grading using digital microscopy: concordance and outcome association.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Journal of clinical pathology</span>, 71(8):680–686, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Schömig-Markiefka, A. Pryalukhin, and W. et al. Hulla.

</span>
<span class="ltx_bibblock">Quality control stress test for deep learning-based diagnostic model in digital pathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Modern Pathology</span>, 34, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexander I. Wright, Catriona M. Dunn, Michael Hale, Gordon G. A. Hutchins, and Darren E. Treanor.

</span>
<span class="ltx_bibblock">The effect of quality control on accuracy of digital pathology image analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE Journal of Biomedical and Health Informatics</span>, 25(2):307–314, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kyle et. al. Chang.

</span>
<span class="ltx_bibblock">The cancer genome atlas pan-cancer analysis project.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Nature Genetics</span>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Janowczyk A, Zuo R, Gilmore H, Feldman M, and Madabhushi A.

</span>
<span class="ltx_bibblock">Histoqc: An open-source quality control tool for digital pathology slides.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">JCO Clin Cancer Inform.</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gabriele Campanella, Arjun R Rajanna, Lorraine Corsale, Peter J Schüffler, Yukako Yagi, and Thomas J Fuchs.

</span>
<span class="ltx_bibblock">Towards machine learned quality control: A benchmark for sharpness quantification in digital pathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Computerized medical imaging and graphics</span>, 65:142–151, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to structural similarity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Image Processing</span>, 13(4):600–612, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Lin Zhang, Lei Zhang, and A. C. Bovik.

</span>
<span class="ltx_bibblock">A feature-enriched completely blind image quality evaluator.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">IEEE Transactions on Image Processing</span>, 24(8):2579–2591, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. R. N. Avanaki, K. S. Espig, A. Xthona, C. Lanciault, and T. R. L. Kimpe.

</span>
<span class="ltx_bibblock">Automatic image quality assessment for digital pathology.

</span>
<span class="ltx_bibblock">In A. Tingberg, K. Lång, and P. Timberg, editors, <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Breast Imaging</span>, Lecture Notes in Computer Science, pages 431–438. Springer International Publishing, Cham, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Jiménez, G. Bueno, G. Cristobal, O. Deniz, D. Toomey, and C. Conway.

</span>
<span class="ltx_bibblock">Image quality metrics applied to digital pathology.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of SPIE - The International Society for Optical Engineering</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X. M. Lopez, E. D’Andrea, P. Barbot, A.-S. Bridoux, S. Rorive, I. Salmon, O. Debeir, and C. Decaestecker.

</span>
<span class="ltx_bibblock">An automated blur detection method for histological whole slide imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">PLOS ONE</span>, 8(12):e82710, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N. Hashimoto, P. A. Bautista, M. Yamaguchi, N. Ohyama, and Y. Yagi.

</span>
<span class="ltx_bibblock">Referenceless image quality evaluation for whole slide imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Journal of Pathology Informatics</span>, 3:9, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Gao, D. Padfield, J. Rittscher, and R. McKay.

</span>
<span class="ltx_bibblock">Automated training data generation for microscopy focus classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Medical Image Computing and Computer-Assisted Intervention – MICCAI 2010</span>, volume 6363 of <span class="ltx_text ltx_font_italic" id="bib.bib16.2.2">Lecture Notes in Computer Science</span>, pages 446–453. Springer, Berlin, Heidelberg, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mahdi S Hosseini, Jasper AZ Brawley-Hayes, Yueyang Zhang, Lyndon Chan, Konstantinos N Plataniotis, and Savvas Damaskinos.

</span>
<span class="ltx_bibblock">Focus quality assessment of high-throughput whole slide imaging in digital pathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE transactions on medical imaging</span>, 39(1):62–74, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Caglar Senaras, M Khalid Khan Niazi, Gerard Lozanski, and Metin N Gurcan.

</span>
<span class="ltx_bibblock">Deepfocus: detection of out-of-focus regions in whole slide digital images using deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">PloS one</span>, 13(10):e0205387, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tomé Albuquerque, Ana Moreira, and Jaime S Cardoso.

</span>
<span class="ltx_bibblock">Deep ordinal focus assessment for whole slide images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 657–663, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Zhongling Wang, Mahdi S Hosseini, Adyn Miles, Konstantinos N Plataniotis, and Zhou Wang.

</span>
<span class="ltx_bibblock">Focuslitenn: High efficiency focus quality assessment for digital pathology.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 403–413. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Kothari, J. H. Phan, and M. D. Wang.

</span>
<span class="ltx_bibblock">Eliminating tissue-fold artifacts in histopathological whole-slide images for improved image-based prediction of cancer grade.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Journal of Pathology Informatics</span>, 4:22, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P. A. Bautista and Y. Yagi.

</span>
<span class="ltx_bibblock">Detection of tissue folds in whole slide images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</span>, pages 3669–3672, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. A. Bautista and Y. Yagi.

</span>
<span class="ltx_bibblock">Improving the visualization and detection of tissue folds in whole slide images through color enhancement.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Journal of Pathology Informatics</span>, 1:25, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Palokangas, J. Selinummi, and O. Yli-Harja.

</span>
<span class="ltx_bibblock">Segmentation of folds in tissue section images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">2007 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</span>, pages 5642–5645, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Sharib Ali, Nasullah Khalid Alham, Clare Verrill, and Jens Rittscher.

</span>
<span class="ltx_bibblock">Ink removal from histopathology whole slide images by combining classification, detection and image generation models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">ISBI 2019</span>, pages 928–932. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Teng Zhang, Johanna Carvajal, Daniel F Smith, Kun Zhao, Arnold Wiliem, Peter Hobson, Anthony Jennings, and Brian C Lovell.

</span>
<span class="ltx_bibblock">Slidenet: Fast and accurate slide quality assessment based on deep neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">2018 24th International Conference on Pattern Recognition (ICPR)</span>, pages 2314–2319. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Haghighat, L. Browning, K. Sirinukunwattana, S. Malacrino, N. Khalid Alham, R. Colling, Y. Cui, E. Rakha, F. C. Hamdy, C. Verrill, and J. Rittscher.

</span>
<span class="ltx_bibblock">Automated quality assessment of large digitised histology cohorts by artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Scientific Reports</span>, 12(1):5002, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Foucart, O. Debeir, and C. Decaestecker.

</span>
<span class="ltx_bibblock">Artifact identification in digital pathology from weak and noisy supervision with deep residual networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">2018 4th International Conference on Cloud Computing Technologies and Applications (Cloudtech)</span>, pages 1–6, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
N. N. Shaikh, K. Wasag, and Y. Nie.

</span>
<span class="ltx_bibblock">Artifact identification in digital histopathology images using few-shot learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</span>, pages 1–4, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Foucart, O. Debeir, and C. Decaestecker.

</span>
<span class="ltx_bibblock">Snow supervision in digital pathology: Managing imperfect annotations for segmentation in deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Research Square</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Nikhil Cherian Kurian, S Varsha, Abhijit Patil, Shashikant Khade, and Amit Sethi.

</span>
<span class="ltx_bibblock">Robust semi-supervised learning for histopathology images through self-supervision guided out-of-distribution scoring.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE)</span>, pages 121–128. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Neel Kanwal, Fernando Pérez-Bueno, Arne Schmidt, Kjersti Engan, and Rafael Molina.

</span>
<span class="ltx_bibblock">The devil is in the details: Whole slide image acquisition and processing for artifacts detection, color variation, and data augmentation: A review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">IEEE Access</span>, 10:58821–58844, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Romain Brixtel, Sebastien Bougleux, Olivier Lézoray, Yann Caillot, Benoit Lemoine, Mathieu Fontaine, Dalal Nebati, and Arnaud Renouf.

</span>
<span class="ltx_bibblock">Whole slide image quality in digital pathology: review and perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">IEEE Access</span>, 10:131005–131035, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Nadia Brancati, Giuseppe De Pietro, Daniel Riccio, and Maria Frucci.

</span>
<span class="ltx_bibblock">Gigapixel histopathological image analysis using attention-based neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">IEEE Access</span>, 9:87552–87562, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Abhijeet Patil, Mohd. Talha, Aniket Bhatia, Nikhil Cherian Kurian, Sammed Mangale, Sunil Patel, and Amit Sethi.

</span>
<span class="ltx_bibblock">Fast, self supervised, fully convolutional color normalization of h&amp;e stained images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</span>, pages 1563–1567, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Abhishek Vahadane, Tingying Peng, Amit Sethi, Shadi Albarqouni, Lichao Wang, Maximilian Baust, Katja Steiger, Anna Melissa Schlitter, Irene Esposito, and Nassir Navab.

</span>
<span class="ltx_bibblock">Structure-preserving color normalization and sparse stain separation for histological images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">IEEE Transactions on Medical Imaging</span>, 35(8):1962–1971, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">MICCAI 2015</span>, pages 234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang.

</span>
<span class="ltx_bibblock">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Medical Imaging</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">International conference on machine learning</span>, pages 6105–6114. PMLR, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Bankhead P, Loughrey MB, Fernández JA, Dombrowski Y, Salto-Tellez M, and Hamilton PW.

</span>
<span class="ltx_bibblock">Qupath: Open source software for digital pathology image analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Sci Rep.</span>, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 10:01:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
