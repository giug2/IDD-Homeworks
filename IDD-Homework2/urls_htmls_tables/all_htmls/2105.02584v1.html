<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.02584] TABBIE: Pretrained Representations of Tabular Data</title><meta property="og:description" content="Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining im…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TABBIE: Pretrained Representations of Tabular Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TABBIE: Pretrained Representations of Tabular Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.02584">

<!--Generated on Sun Mar 17 09:54:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TABBIE: Pretrained Representations of Tabular Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hiroshi Iida<sup id="id1.1.1" class="ltx_sup"><math id="id1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\dagger</annotation></semantics></math></sup>     
Dung Thai<sup id="id2.2.2" class="ltx_sup"><math id="id2.2.2.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="id2.2.2.m1.1a"><mo id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\ddagger</annotation></semantics></math></sup>     
Varun Manjunatha<sup id="id3.3.3" class="ltx_sup"><math id="id3.3.3.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="id3.3.3.m1.1a"><mi mathvariant="normal" id="id3.3.3.m1.1.1" xref="id3.3.3.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="id3.3.3.m1.1b"><ci id="id3.3.3.m1.1.1.cmml" xref="id3.3.3.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m1.1c">\S</annotation></semantics></math></sup>     
Mohit Iyyer<sup id="id4.4.4" class="ltx_sup"><math id="id4.4.4.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="id4.4.4.m1.1a"><mo id="id4.4.4.m1.1.1" xref="id4.4.4.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id4.4.4.m1.1b"><ci id="id4.4.4.m1.1.1.cmml" xref="id4.4.4.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m1.1c">\ddagger</annotation></semantics></math></sup> 
<br class="ltx_break"><sup id="id5.5.5" class="ltx_sup"><math id="id5.5.5.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id5.5.5.m1.1a"><mo id="id5.5.5.m1.1.1" xref="id5.5.5.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id5.5.5.m1.1b"><ci id="id5.5.5.m1.1.1.cmml" xref="id5.5.5.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m1.1c">\dagger</annotation></semantics></math></sup>Sony Corporation     
<sup id="id6.6.6" class="ltx_sup"><math id="id6.6.6.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="id6.6.6.m1.1a"><mo id="id6.6.6.m1.1.1" xref="id6.6.6.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id6.6.6.m1.1b"><ci id="id6.6.6.m1.1.1.cmml" xref="id6.6.6.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m1.1c">\ddagger</annotation></semantics></math></sup>UMass Amherst     
<sup id="id7.7.id1" class="ltx_sup">§</sup>Adobe Research 
<br class="ltx_break"><span id="id8.8.id2" class="ltx_text ltx_font_typewriter">hiroshi.iida@sony.com</span> 
<br class="ltx_break"><span id="id9.9.id3" class="ltx_text ltx_font_typewriter">{dthai,miyyer}@cs.umass.edu</span> 
<br class="ltx_break"><span id="id10.10.id4" class="ltx_text ltx_font_typewriter">vmanjuna@adobe.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Existing work on tabular representation-learning <em id="id11.id1.1" class="ltx_emph ltx_font_italic">jointly</em> models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (<em id="id11.id1.2" class="ltx_emph ltx_font_italic">corrupt cell detection</em>) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (<span id="id11.id1.3" class="ltx_text ltx_font_smallcaps">tabbie</span>) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model’s learned cell, column, and row representations shows that it understands complex table semantics and numerical trends.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large-scale self-supervised pretraining has substantially advanced the state-of-the-art in natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Peters et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>; Liu et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. More recently, these pretraining methods have been extended to jointly learn representations of <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">tables</em> as well as text <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Yin et al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>, which enables improved modeling of tasks such as question answering over tables. However, many practical problems involve semantic understanding of tabular data without additional text-based input, such as extracting tables from documents, retrieving similar columns or cells, and filling in missing information <cite class="ltx_cite ltx_citemacro_cite">Zhang and Balog (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>. In this work, we design a pretraining methodology specifically for tables (<span id="S1.p1.1.2" class="ltx_text ltx_font_bold">Tab</span>ular <span id="S1.p1.1.3" class="ltx_text ltx_font_bold">I</span>nformation <span id="S1.p1.1.4" class="ltx_text ltx_font_bold">E</span>mbedding or <span id="S1.p1.1.5" class="ltx_text ltx_font_smallcaps">tabbie</span>) that resembles several core tasks in table extraction and decomposition pipelines and allows easy access to representations for different tabular substructures (cells, rows, and columns).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing table representation models such as TaBERT <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> and TaPas <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> concatenate tabular data with an associated piece of text and then use BERT’s masked language modeling objective for pretraining. These approaches are computationally expensive due to the long sequences that arise from concatenating text with linearized tables, which necessitates truncating the input sequences<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span> <cite class="ltx_cite ltx_citemacro_citet">Herzig et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> use a fixed limit of 128 tokens for both text and table, while <cite class="ltx_cite ltx_citemacro_citet">Yin et al. (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> drop all but three rows of the table during pretraining.</span></span></span> to make training feasible. We show that TaBERT underperforms on downstream table-based applications that operate independent of external text (e.g., deciding whether cell text was corrupted while extracting a table from a PDF), which motivates us to investigate an approach that preserves the full table during pretraining.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2105.02584/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> is a table embedding model trained to detect corrupted cells, inspired by the ELECTRA <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> objective function. This simple pretraining objective results in powerful embeddings of cells, columns, and rows, and it yields state-of-the-art results on downstream table-based tasks.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> architecture relies on two Transformers that independently encode rows and columns, respectively; their representations are pooled at each layer. This setup reduces the sequence length of each Transformer’s input, which cuts down on its complexity, while also allowing us to easily extract representations of cells, rows, and columns. Additionally,
<span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> uses a simplified training objective compared to masked language modeling: instead of predicting masked cells, we repurpose ELECTRA’s objective function <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> for tabular pretraining by asking the model to predict whether or not each cell in a table is real or corrupted. We emphasize that this pretraining objective is a fundamental task in table structure decomposition pipelines <cite class="ltx_cite ltx_citemacro_citep">(Nishida et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Tensmeyer et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Raja et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>, in which incorrectly predicting row/column separators or cell boundaries leads to corrupted cell text. Unlike <cite class="ltx_cite ltx_citemacro_citet">Clark et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, we do not require a separate “generator” model that produces corrupted candidates, as we observe that simple corruption processes (e.g., sampling cells from other tables, swapping cells within the same column) yield powerful representations after pretraining.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In a controlled comparison to TaBERT (pretraining on the same number of tables and using a similarly-sized model), we evaluate <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> on three table-based benchmarks: column population, row population, and column type prediction. On most configurations of these tasks, <span id="S1.p4.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> achieves state-of-the-art performance, outperforming TaBERT and other baselines, while in others it performs competitively with TaBERT. Additionally, <span id="S1.p4.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> was trained on <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">8</span> V100 GPUs in just over a week, compared to the <span id="S1.p4.1.5" class="ltx_text ltx_font_bold">128</span> V100 GPUs used to train TaBERT in six days. A qualitative nearest-neighbor analysis of embeddings derived from <span id="S1.p4.1.6" class="ltx_text ltx_font_smallcaps">tabbie</span> confirms that it encodes complex semantic properties about textual and numeric cells and substructures. We release our pretrained models and code to support further advances on table-based tasks.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/SFIG611/tabbie" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/SFIG611/tabbie</a></span></span></span></p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2105.02584/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S1.F2.3.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s computations at one layer. For a given table, the row Transformer contextualizes the representations of the cells in each row, while the column Transformer similarly contextualizes cells in each column. The final cell representation is an average of the row and column embeddings, which is passed as input to the next layer. <span id="S1.F2.4.2" class="ltx_text ltx_font_smallcaps">[cls]</span> tokens are prepended to each row and column to facilitate downstream tasks operating on table substructures.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Model</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> is a self-supervised pretraining approach trained exclusively on tables, unlike prior approaches <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Yin et al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> that jointly model tables and associated text snippets. At a high level, <span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> encodes each cell of a table using two different Transformer models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, one operating across the rows of the table and the other across columns. At each layer, the representations from the <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">row</em> and <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">column</em> Transformers are averaged and then passed as input to the next layer, which produces a contextualized representation of each cell within the table. We place a binary classifier over <span id="S2.p1.1.5" class="ltx_text ltx_font_smallcaps">tabbie</span>’s final-layer cell representations to predict whether or not it has been <em id="S2.p1.1.6" class="ltx_emph ltx_font_italic">corrupted</em>, or replaced by an intruder cell during preprocessing, inspired by the ELECTRA objective of <cite class="ltx_cite ltx_citemacro_citet">Clark et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. In the remainder of this section, we formalize both <span id="S2.p1.1.7" class="ltx_text ltx_font_smallcaps">tabbie</span>’s model architecture and pretraining objective.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Model Architecture</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p"><span id="S2.SS1.p1.6.1" class="ltx_text ltx_font_smallcaps">tabbie</span> takes an <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="M\times N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝑀</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">M\times N</annotation></semantics></math> table as input and produces embeddings <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{x}_{ij}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">𝒙</mi><mrow id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1.3.1" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝒙</ci><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><times id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.1"></times><ci id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">𝑖</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\boldsymbol{x}_{ij}</annotation></semantics></math> for each cell (where <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">i</annotation></semantics></math> and <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">j</annotation></semantics></math> are row and column indices, respectively), as well as embeddings for individual columns <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\boldsymbol{c}_{j}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">𝒄</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝒄</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\boldsymbol{c}_{j}</annotation></semantics></math> and rows <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="\boldsymbol{r}_{i}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">𝒓</mi><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">𝒓</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\boldsymbol{r}_{i}</annotation></semantics></math>.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Initialization:</h4>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.6" class="ltx_p">We begin by initializing the cell embeddings <math id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\boldsymbol{x}_{ij}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝒙</ci><apply id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3"><times id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">𝑖</ci><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">\boldsymbol{x}_{ij}</annotation></semantics></math> using a pretrained BERT model <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We use the BERT-base-uncased model in all experiments.</span></span></span> Specifically, for each cell <math id="S2.SS1.SSS0.Px1.p1.2.m2.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.2a"><mrow id="S2.SS1.SSS0.Px1.p1.2.m2.2.3.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.2.m2.2.3.2.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px1.p1.2.m2.2.3.2.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S2.SS1.SSS0.Px1.p1.2.m2.2.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.2.cmml">j</mi><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.2.m2.2.3.2.3" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.2b"><interval closure="open" id="S2.SS1.SSS0.Px1.p1.2.m2.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px1.p1.2.m2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.2c">(i,j)</annotation></semantics></math>, we feed its contents into BERT and extract the 768-<math id="S2.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.1c">d</annotation></semantics></math> <span id="S2.SS1.SSS0.Px1.p1.6.1" class="ltx_text ltx_font_smallcaps">[cls]</span> token representation. This step allows us to leverage the powerful semantic text encoder of BERT to compute representations of cells out-of-context, which is important because many tables contain cells with long-form text (e.g., <em id="S2.SS1.SSS0.Px1.p1.6.2" class="ltx_emph ltx_font_italic">Notes</em> columns). Additionally, BERT has been shown to encode some degree of numeracy <cite class="ltx_cite ltx_citemacro_citep">(Wallace et al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>, which helps represent cells with numerical content. We keep this BERT encoder fixed during training to reduce computational expense. Finally, we add learned positional embeddings to each of the <span id="S2.SS1.SSS0.Px1.p1.6.3" class="ltx_text ltx_font_smallcaps">[cls]</span> vectors to form the initialization of <math id="S2.SS1.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\boldsymbol{x}_{ij}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.1a"><msub id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.2">𝒙</ci><apply id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3"><times id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.1"></times><ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.2">𝑖</ci><ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.1c">\boldsymbol{x}_{ij}</annotation></semantics></math>. More specifically, we have two sets of positional embeddings, <math id="S2.SS1.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="p^{(r)}_{i}\in\mathbb{R}^{H}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><mrow id="S2.SS1.SSS0.Px1.p1.5.m5.1.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.cmml"><msubsup id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.2.cmml">p</mi><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.3" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.3.cmml">i</mi><mrow id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.3.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.3.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.cmml">)</mo></mrow></msubsup><mo id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.1.cmml">∈</mo><msup id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.2.cmml">ℝ</mi><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.3" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.3.cmml">H</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2"><in id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.1"></in><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2">subscript</csymbol><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.2.2">𝑝</ci><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.1">𝑟</ci></apply><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.2.3">𝑖</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.2">ℝ</ci><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.2.3.3">𝐻</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">p^{(r)}_{i}\in\mathbb{R}^{H}</annotation></semantics></math> and <math id="S2.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="p^{(c)}_{j}\in\mathbb{R}^{H}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a"><mrow id="S2.SS1.SSS0.Px1.p1.6.m6.1.2" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.cmml"><msubsup id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.2" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.2.cmml">p</mi><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.3" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.3.cmml">j</mi><mrow id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.3.1" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.1" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.1.cmml">c</mi><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.3.2" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.cmml">)</mo></mrow></msubsup><mo id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.1" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.1.cmml">∈</mo><msup id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.2.cmml">ℝ</mi><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.3" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.3.cmml">H</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.6.m6.1b"><apply id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2"><in id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.1"></in><apply id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2">subscript</csymbol><apply id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.2.2">𝑝</ci><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.1.1">𝑐</ci></apply><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.2.3">𝑗</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.2">ℝ</ci><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.2.3.3">𝐻</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">p^{(c)}_{j}\in\mathbb{R}^{H}</annotation></semantics></math>, which model the position of rows and columns, respectively, and are randomly initialized and fine-tuned via <span id="S2.SS1.SSS0.Px1.p1.6.4" class="ltx_text ltx_font_smallcaps">tabbie</span>’s self-supervised objective.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Contextualizing the cell embeddings:</h4>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">The cell embeddings we get from BERT are uncontextualized: they are computed in isolation of all of the other cells in the table. While methods such as TaBERT and TaPaS contextualize cell embeddings by linearizing the table into a single long sequence, we take a different and more computationally manageable approach. We define a <em id="S2.SS1.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">row</em> Transformer, which encodes cells across each row of the table, as well as a <em id="S2.SS1.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">column</em> Transformer, which does the same across columns.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p2.9" class="ltx_p">Concretely, assume row <math id="S2.SS1.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.1.m1.1a"><mi id="S2.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.1.m1.1c">i</annotation></semantics></math> contains cell embeddings <math id="S2.SS1.SSS0.Px2.p2.2.m2.10" class="ltx_Math" alttext="\boldsymbol{x}_{i,1},\boldsymbol{x}_{i,2},\dots,\boldsymbol{x}_{i,N}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.2.m2.10a"><mrow id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.4.cmml"><msub id="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.3.cmml">,</mo><mn id="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.4" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.4" xref="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.3.3.1.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.3.3.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.4.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.3.cmml">,</mo><mn id="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.2.cmml">2</mn></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.5" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS0.Px2.p2.2.m2.7.7" xref="S2.SS1.SSS0.Px2.p2.2.m2.7.7.cmml">…</mi><mo id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.6" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.4" xref="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.m2.5.5.1.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.5.5.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.4.1" xref="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.2" xref="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.2.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.2.m2.10b"><list id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.4.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3"><apply id="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.8.8.1.1.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.4"><ci id="S2.SS1.SSS0.Px2.p2.2.m2.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.1.1.1.1">𝑖</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.2.2.2.2">1</cn></list></apply><apply id="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.9.9.2.2.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.4"><ci id="S2.SS1.SSS0.Px2.p2.2.m2.3.3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.3.3.1.1">𝑖</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.4.4.2.2">2</cn></list></apply><ci id="S2.SS1.SSS0.Px2.p2.2.m2.7.7.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.7.7">…</ci><apply id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.10.10.3.3.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.4"><ci id="S2.SS1.SSS0.Px2.p2.2.m2.5.5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.5.5.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.2.m2.6.6.2.2">𝑁</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.2.m2.10c">\boldsymbol{x}_{i,1},\boldsymbol{x}_{i,2},\dots,\boldsymbol{x}_{i,N}</annotation></semantics></math>. We pass this sequence of embeddings into a <em id="S2.SS1.SSS0.Px2.p2.9.1" class="ltx_emph ltx_font_italic">row</em> Transformer block, which uses self-attention to produce contextualized output representations <math id="S2.SS1.SSS0.Px2.p2.3.m3.10" class="ltx_Math" alttext="\boldsymbol{r}_{i,1},\boldsymbol{r}_{i,2},\dots,\boldsymbol{r}_{i,N}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.3.m3.10a"><mrow id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.4.cmml"><msub id="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.2.cmml">𝒓</mi><mrow id="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.3.cmml">,</mo><mn id="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.4" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.2.cmml">𝒓</mi><mrow id="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.4" xref="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.3.3.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.3.3.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.4.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.3.cmml">,</mo><mn id="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.2.cmml">2</mn></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.5" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS0.Px2.p2.3.m3.7.7" xref="S2.SS1.SSS0.Px2.p2.3.m3.7.7.cmml">…</mi><mo id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.6" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.2.cmml">𝒓</mi><mrow id="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.4" xref="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m3.5.5.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.5.5.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.4.1" xref="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.2" xref="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.2.cmml">N</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.3.m3.10b"><list id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.4.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3"><apply id="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.8.8.1.1.2">𝒓</ci><list id="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.4"><ci id="S2.SS1.SSS0.Px2.p2.3.m3.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.1.1.1.1">𝑖</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.2.2.2.2">1</cn></list></apply><apply id="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.9.9.2.2.2">𝒓</ci><list id="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.4"><ci id="S2.SS1.SSS0.Px2.p2.3.m3.3.3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.3.3.1.1">𝑖</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.4.4.2.2">2</cn></list></apply><ci id="S2.SS1.SSS0.Px2.p2.3.m3.7.7.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.7.7">…</ci><apply id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.10.10.3.3.2">𝒓</ci><list id="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.4"><ci id="S2.SS1.SSS0.Px2.p2.3.m3.5.5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.5.5.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m3.6.6.2.2">𝑁</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m3.10c">\boldsymbol{r}_{i,1},\boldsymbol{r}_{i,2},\dots,\boldsymbol{r}_{i,N}</annotation></semantics></math>. Similarly, assume column <math id="S2.SS1.SSS0.Px2.p2.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.4.m4.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m4.1.1" xref="S2.SS1.SSS0.Px2.p2.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.4.m4.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.4.m4.1c">j</annotation></semantics></math> contains cell embeddings <math id="S2.SS1.SSS0.Px2.p2.5.m5.10" class="ltx_Math" alttext="\boldsymbol{x}_{1,j},\boldsymbol{x}_{2,j},\dots,\boldsymbol{x}_{M,j}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.5.m5.10a"><mrow id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.4.cmml"><msub id="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.3.cmml"><mn id="S2.SS1.SSS0.Px2.p2.5.m5.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.1.1.1.1.cmml">1</mn><mo id="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.4" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.4" xref="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.3.cmml"><mn id="S2.SS1.SSS0.Px2.p2.5.m5.3.3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.3.3.1.1.cmml">2</mn><mo id="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.4.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.2.cmml">j</mi></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.5" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS0.Px2.p2.5.m5.7.7" xref="S2.SS1.SSS0.Px2.p2.5.m5.7.7.cmml">…</mi><mo id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.6" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.4" xref="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.5.m5.5.5.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.5.5.1.1.cmml">M</mi><mo id="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.4.1" xref="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.2.cmml">j</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m5.10b"><list id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.4.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3"><apply id="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.8.8.1.1.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.4"><cn type="integer" id="S2.SS1.SSS0.Px2.p2.5.m5.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.1.1.1.1">1</cn><ci id="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.2.2.2.2">𝑗</ci></list></apply><apply id="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.9.9.2.2.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.4"><cn type="integer" id="S2.SS1.SSS0.Px2.p2.5.m5.3.3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.3.3.1.1">2</cn><ci id="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.4.4.2.2">𝑗</ci></list></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m5.7.7.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.7.7">…</ci><apply id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.10.10.3.3.2">𝒙</ci><list id="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.4"><ci id="S2.SS1.SSS0.Px2.p2.5.m5.5.5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.5.5.1.1">𝑀</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.5.m5.6.6.2.2">𝑗</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m5.10c">\boldsymbol{x}_{1,j},\boldsymbol{x}_{2,j},\dots,\boldsymbol{x}_{M,j}</annotation></semantics></math>; the <em id="S2.SS1.SSS0.Px2.p2.9.2" class="ltx_emph ltx_font_italic">column</em> Transformer produces contextualized representations <math id="S2.SS1.SSS0.Px2.p2.6.m6.10" class="ltx_Math" alttext="\boldsymbol{c}_{1,j},\boldsymbol{c}_{2,j},\dots,\boldsymbol{c}_{M,j}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.6.m6.10a"><mrow id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.4.cmml"><msub id="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.2.cmml">𝒄</mi><mrow id="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.3.cmml"><mn id="S2.SS1.SSS0.Px2.p2.6.m6.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.1.1.1.1.cmml">1</mn><mo id="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.4" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.2.cmml">𝒄</mi><mrow id="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.4" xref="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.3.cmml"><mn id="S2.SS1.SSS0.Px2.p2.6.m6.3.3.1.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.3.3.1.1.cmml">2</mn><mo id="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.4.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.2.cmml">j</mi></mrow></msub><mo id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.5" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS0.Px2.p2.6.m6.7.7" xref="S2.SS1.SSS0.Px2.p2.6.m6.7.7.cmml">…</mi><mo id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.6" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.4.cmml">,</mo><msub id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.2.cmml">𝒄</mi><mrow id="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.4" xref="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.6.m6.5.5.1.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.5.5.1.1.cmml">M</mi><mo id="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.4.1" xref="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.2" xref="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.2.cmml">j</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.6.m6.10b"><list id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.4.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3"><apply id="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.8.8.1.1.2">𝒄</ci><list id="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.4"><cn type="integer" id="S2.SS1.SSS0.Px2.p2.6.m6.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.1.1.1.1">1</cn><ci id="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.2.2.2.2">𝑗</ci></list></apply><apply id="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.9.9.2.2.2">𝒄</ci><list id="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.4"><cn type="integer" id="S2.SS1.SSS0.Px2.p2.6.m6.3.3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.3.3.1.1">2</cn><ci id="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.4.4.2.2">𝑗</ci></list></apply><ci id="S2.SS1.SSS0.Px2.p2.6.m6.7.7.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.7.7">…</ci><apply id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.10.10.3.3.2">𝒄</ci><list id="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.4"><ci id="S2.SS1.SSS0.Px2.p2.6.m6.5.5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.5.5.1.1">𝑀</ci><ci id="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.6.m6.6.6.2.2">𝑗</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.6.m6.10c">\boldsymbol{c}_{1,j},\boldsymbol{c}_{2,j},\dots,\boldsymbol{c}_{M,j}</annotation></semantics></math>. After running the two Transformers over all rows and columns, respectively, each cell <math id="S2.SS1.SSS0.Px2.p2.7.m7.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.7.m7.2a"><mrow id="S2.SS1.SSS0.Px2.p2.7.m7.2.3.2" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px2.p2.7.m7.2.3.2.1" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.3.1.cmml">(</mo><mi id="S2.SS1.SSS0.Px2.p2.7.m7.1.1" xref="S2.SS1.SSS0.Px2.p2.7.m7.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.7.m7.2.3.2.2" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.3.1.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.7.m7.2.2" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.2.cmml">j</mi><mo stretchy="false" id="S2.SS1.SSS0.Px2.p2.7.m7.2.3.2.3" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.7.m7.2b"><interval closure="open" id="S2.SS1.SSS0.Px2.p2.7.m7.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.3.2"><ci id="S2.SS1.SSS0.Px2.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.7.m7.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p2.7.m7.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.7.m7.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.7.m7.2c">(i,j)</annotation></semantics></math> of a table is associated with a row embedding <math id="S2.SS1.SSS0.Px2.p2.8.m8.2" class="ltx_Math" alttext="\boldsymbol{r}_{i,j}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.8.m8.2a"><msub id="S2.SS1.SSS0.Px2.p2.8.m8.2.3" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.8.m8.2.3.2" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.3.2.cmml">𝒓</mi><mrow id="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.8.m8.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.8.m8.1.1.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.8.m8.2b"><apply id="S2.SS1.SSS0.Px2.p2.8.m8.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.8.m8.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.8.m8.2.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.3.2">𝒓</ci><list id="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.4"><ci id="S2.SS1.SSS0.Px2.p2.8.m8.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.1.1.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.8.m8.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.8.m8.2c">\boldsymbol{r}_{i,j}</annotation></semantics></math> as well as a column embedding <math id="S2.SS1.SSS0.Px2.p2.9.m9.2" class="ltx_Math" alttext="\boldsymbol{c}_{i,j}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.9.m9.2a"><msub id="S2.SS1.SSS0.Px2.p2.9.m9.2.3" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.9.m9.2.3.2" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.3.2.cmml">𝒄</mi><mrow id="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.4" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p2.9.m9.1.1.1.1" xref="S2.SS1.SSS0.Px2.p2.9.m9.1.1.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.2" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.9.m9.2b"><apply id="S2.SS1.SSS0.Px2.p2.9.m9.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.9.m9.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.9.m9.2.3.2.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.3.2">𝒄</ci><list id="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.4"><ci id="S2.SS1.SSS0.Px2.p2.9.m9.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.1.1.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p2.9.m9.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.9.m9.2c">\boldsymbol{c}_{i,j}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p3.1" class="ltx_p">The final step of cell contextualization is to compose the row and column embeddings together before feeding the result to the next layer. Intuitively, if we do not aggregate the two sets of embeddings together, subsequent layers of the model will only have access to information from a specific row or column, which prevents contextualization across the whole table. We implement this aggregation through simple averaging: specifically, at layer <math id="S2.SS1.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.1.m1.1c">L</annotation></semantics></math> of <span id="S2.SS1.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>, we compute cell embeddings as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.6" class="ltx_Math" alttext="\boldsymbol{x}^{L+1}_{i,j}=\frac{\boldsymbol{r}^{L}_{i,j}+\boldsymbol{c}^{L}_{i,j}}{2}" display="block"><semantics id="S2.E1.m1.6a"><mrow id="S2.E1.m1.6.7" xref="S2.E1.m1.6.7.cmml"><msubsup id="S2.E1.m1.6.7.2" xref="S2.E1.m1.6.7.2.cmml"><mi id="S2.E1.m1.6.7.2.2.2" xref="S2.E1.m1.6.7.2.2.2.cmml">𝒙</mi><mrow id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">i</mi><mo id="S2.E1.m1.2.2.2.4.1" xref="S2.E1.m1.2.2.2.3.cmml">,</mo><mi id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">j</mi></mrow><mrow id="S2.E1.m1.6.7.2.2.3" xref="S2.E1.m1.6.7.2.2.3.cmml"><mi id="S2.E1.m1.6.7.2.2.3.2" xref="S2.E1.m1.6.7.2.2.3.2.cmml">L</mi><mo id="S2.E1.m1.6.7.2.2.3.1" xref="S2.E1.m1.6.7.2.2.3.1.cmml">+</mo><mn id="S2.E1.m1.6.7.2.2.3.3" xref="S2.E1.m1.6.7.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S2.E1.m1.6.7.1" xref="S2.E1.m1.6.7.1.cmml">=</mo><mfrac id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml"><mrow id="S2.E1.m1.6.6.4" xref="S2.E1.m1.6.6.4.cmml"><msubsup id="S2.E1.m1.6.6.4.6" xref="S2.E1.m1.6.6.4.6.cmml"><mi id="S2.E1.m1.6.6.4.6.2.2" xref="S2.E1.m1.6.6.4.6.2.2.cmml">𝒓</mi><mrow id="S2.E1.m1.4.4.2.2.2.4" xref="S2.E1.m1.4.4.2.2.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml">i</mi><mo id="S2.E1.m1.4.4.2.2.2.4.1" xref="S2.E1.m1.4.4.2.2.2.3.cmml">,</mo><mi id="S2.E1.m1.4.4.2.2.2.2" xref="S2.E1.m1.4.4.2.2.2.2.cmml">j</mi></mrow><mi id="S2.E1.m1.6.6.4.6.2.3" xref="S2.E1.m1.6.6.4.6.2.3.cmml">L</mi></msubsup><mo id="S2.E1.m1.6.6.4.5" xref="S2.E1.m1.6.6.4.5.cmml">+</mo><msubsup id="S2.E1.m1.6.6.4.7" xref="S2.E1.m1.6.6.4.7.cmml"><mi id="S2.E1.m1.6.6.4.7.2.2" xref="S2.E1.m1.6.6.4.7.2.2.cmml">𝒄</mi><mrow id="S2.E1.m1.6.6.4.4.2.4" xref="S2.E1.m1.6.6.4.4.2.3.cmml"><mi id="S2.E1.m1.5.5.3.3.1.1" xref="S2.E1.m1.5.5.3.3.1.1.cmml">i</mi><mo id="S2.E1.m1.6.6.4.4.2.4.1" xref="S2.E1.m1.6.6.4.4.2.3.cmml">,</mo><mi id="S2.E1.m1.6.6.4.4.2.2" xref="S2.E1.m1.6.6.4.4.2.2.cmml">j</mi></mrow><mi id="S2.E1.m1.6.6.4.7.2.3" xref="S2.E1.m1.6.6.4.7.2.3.cmml">L</mi></msubsup></mrow><mn id="S2.E1.m1.6.6.6" xref="S2.E1.m1.6.6.6.cmml">2</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.6b"><apply id="S2.E1.m1.6.7.cmml" xref="S2.E1.m1.6.7"><eq id="S2.E1.m1.6.7.1.cmml" xref="S2.E1.m1.6.7.1"></eq><apply id="S2.E1.m1.6.7.2.cmml" xref="S2.E1.m1.6.7.2"><csymbol cd="ambiguous" id="S2.E1.m1.6.7.2.1.cmml" xref="S2.E1.m1.6.7.2">subscript</csymbol><apply id="S2.E1.m1.6.7.2.2.cmml" xref="S2.E1.m1.6.7.2"><csymbol cd="ambiguous" id="S2.E1.m1.6.7.2.2.1.cmml" xref="S2.E1.m1.6.7.2">superscript</csymbol><ci id="S2.E1.m1.6.7.2.2.2.cmml" xref="S2.E1.m1.6.7.2.2.2">𝒙</ci><apply id="S2.E1.m1.6.7.2.2.3.cmml" xref="S2.E1.m1.6.7.2.2.3"><plus id="S2.E1.m1.6.7.2.2.3.1.cmml" xref="S2.E1.m1.6.7.2.2.3.1"></plus><ci id="S2.E1.m1.6.7.2.2.3.2.cmml" xref="S2.E1.m1.6.7.2.2.3.2">𝐿</ci><cn type="integer" id="S2.E1.m1.6.7.2.2.3.3.cmml" xref="S2.E1.m1.6.7.2.2.3.3">1</cn></apply></apply><list id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.4"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝑖</ci><ci id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6"><divide id="S2.E1.m1.6.6.5.cmml" xref="S2.E1.m1.6.6"></divide><apply id="S2.E1.m1.6.6.4.cmml" xref="S2.E1.m1.6.6.4"><plus id="S2.E1.m1.6.6.4.5.cmml" xref="S2.E1.m1.6.6.4.5"></plus><apply id="S2.E1.m1.6.6.4.6.cmml" xref="S2.E1.m1.6.6.4.6"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.4.6.1.cmml" xref="S2.E1.m1.6.6.4.6">subscript</csymbol><apply id="S2.E1.m1.6.6.4.6.2.cmml" xref="S2.E1.m1.6.6.4.6"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.4.6.2.1.cmml" xref="S2.E1.m1.6.6.4.6">superscript</csymbol><ci id="S2.E1.m1.6.6.4.6.2.2.cmml" xref="S2.E1.m1.6.6.4.6.2.2">𝒓</ci><ci id="S2.E1.m1.6.6.4.6.2.3.cmml" xref="S2.E1.m1.6.6.4.6.2.3">𝐿</ci></apply><list id="S2.E1.m1.4.4.2.2.2.3.cmml" xref="S2.E1.m1.4.4.2.2.2.4"><ci id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1">𝑖</ci><ci id="S2.E1.m1.4.4.2.2.2.2.cmml" xref="S2.E1.m1.4.4.2.2.2.2">𝑗</ci></list></apply><apply id="S2.E1.m1.6.6.4.7.cmml" xref="S2.E1.m1.6.6.4.7"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.4.7.1.cmml" xref="S2.E1.m1.6.6.4.7">subscript</csymbol><apply id="S2.E1.m1.6.6.4.7.2.cmml" xref="S2.E1.m1.6.6.4.7"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.4.7.2.1.cmml" xref="S2.E1.m1.6.6.4.7">superscript</csymbol><ci id="S2.E1.m1.6.6.4.7.2.2.cmml" xref="S2.E1.m1.6.6.4.7.2.2">𝒄</ci><ci id="S2.E1.m1.6.6.4.7.2.3.cmml" xref="S2.E1.m1.6.6.4.7.2.3">𝐿</ci></apply><list id="S2.E1.m1.6.6.4.4.2.3.cmml" xref="S2.E1.m1.6.6.4.4.2.4"><ci id="S2.E1.m1.5.5.3.3.1.1.cmml" xref="S2.E1.m1.5.5.3.3.1.1">𝑖</ci><ci id="S2.E1.m1.6.6.4.4.2.2.cmml" xref="S2.E1.m1.6.6.4.4.2.2">𝑗</ci></list></apply></apply><cn type="integer" id="S2.E1.m1.6.6.6.cmml" xref="S2.E1.m1.6.6.6">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.6c">\boldsymbol{x}^{L+1}_{i,j}=\frac{\boldsymbol{r}^{L}_{i,j}+\boldsymbol{c}^{L}_{i,j}}{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.SSS0.Px2.p4" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p4.2" class="ltx_p">The new cell representations <math id="S2.SS1.SSS0.Px2.p4.1.m1.2" class="ltx_Math" alttext="\boldsymbol{x}^{L+1}_{i,j}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p4.1.m1.2a"><msubsup id="S2.SS1.SSS0.Px2.p4.1.m1.2.3" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.2" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.2.cmml">𝒙</mi><mrow id="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.4" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p4.1.m1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p4.1.m1.1.1.1.1.cmml">i</mi><mo id="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.4.1" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.2" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.2.cmml">j</mi></mrow><mrow id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.cmml"><mi id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.2" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.2.cmml">L</mi><mo id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.1" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.1.cmml">+</mo><mn id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.3" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p4.1.m1.2b"><apply id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3">subscript</csymbol><apply id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.1.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.2.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.2">𝒙</ci><apply id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3"><plus id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.1.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.1"></plus><ci id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.2.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.2">𝐿</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.3.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.3.2.3.3">1</cn></apply></apply><list id="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.3.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.4"><ci id="S2.SS1.SSS0.Px2.p4.1.m1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.1.1.1.1">𝑖</ci><ci id="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px2.p4.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p4.1.m1.2c">\boldsymbol{x}^{L+1}_{i,j}</annotation></semantics></math> are then fed to the row and column Transformers at the next layer <math id="S2.SS1.SSS0.Px2.p4.2.m2.1" class="ltx_Math" alttext="L+1" display="inline"><semantics id="S2.SS1.SSS0.Px2.p4.2.m2.1a"><mrow id="S2.SS1.SSS0.Px2.p4.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.2" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.2.cmml">L</mi><mo id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.1" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.1.cmml">+</mo><mn id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.3" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p4.2.m2.1b"><apply id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1"><plus id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.1"></plus><ci id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p4.2.m2.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p4.2.m2.1c">L+1</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Extracting representations of an entire row or column:</h4>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">The row and column Transformers defined above produce separate representations for every cell in a particular row or column. However, many table-related downstream tasks (e.g., retrieve similar columns from a huge dataset of tables to some query column) can benefit from embeddings that capture the contents of an entire row or column. To enable this functionality in <span id="S2.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>, we simply prepend <span id="S2.SS1.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">[clsrow]</span> and <span id="S2.SS1.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">[clscol]</span> tokens to the beginning of each row and column in an input table as a preprocessing step. After pretraining, we can extract the final-layer cell representations of these <span id="S2.SS1.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_smallcaps">[cls]</span> tokens to use in downstream tasks.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pretraining</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Having described <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s model architecture, we turn now to its training objective. We adapt the self-supervised ELECTRA objective proposed by <cite class="ltx_cite ltx_citemacro_citet">Clark et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> for text representation learning, which places a binary classifier over each word in a piece of text and asks if the word either is part of the original text or has been corrupted. While this objective was originally motivated as enabling more efficient training compared to BERT’s masked language modeling objective, it is especially suited for tabular data, as corrupt cell detection is actually a fundamental task in table structure decomposition pipelines such as <cite class="ltx_cite ltx_citemacro_cite">Nishida et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>); Tensmeyer et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>); Raja et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>, in which incorrectly predicted row/column separators or cell boundaries can lead to corrupted cell text.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In our extension of ELECTRA to tables, a binary classifier takes a final-layer cell embedding as input to decide whether it has been corrupted. More concretely, for cell <math id="S2.SS2.p2.1.m1.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S2.SS2.p2.1.m1.2a"><mrow id="S2.SS2.p2.1.m1.2.3.2" xref="S2.SS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS2.p2.1.m1.2.3.2.1" xref="S2.SS2.p2.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">i</mi><mo id="S2.SS2.p2.1.m1.2.3.2.2" xref="S2.SS2.p2.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S2.SS2.p2.1.m1.2.3.2.3" xref="S2.SS2.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.2b"><interval closure="open" id="S2.SS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.p2.1.m1.2.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝑖</ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.2c">(i,j)</annotation></semantics></math>, we compute the corruption probability as</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.6" class="ltx_Math" alttext="P_{\text{corrupt}}(\text{cell}_{i,j})=\sigma(\boldsymbol{w}^{\intercal}\boldsymbol{x}^{L}_{i,j})" display="block"><semantics id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml"><mrow id="S2.E2.m1.5.5.1" xref="S2.E2.m1.5.5.1.cmml"><msub id="S2.E2.m1.5.5.1.3" xref="S2.E2.m1.5.5.1.3.cmml"><mi id="S2.E2.m1.5.5.1.3.2" xref="S2.E2.m1.5.5.1.3.2.cmml">P</mi><mtext id="S2.E2.m1.5.5.1.3.3" xref="S2.E2.m1.5.5.1.3.3a.cmml">corrupt</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.1.2" xref="S2.E2.m1.5.5.1.2.cmml">​</mo><mrow id="S2.E2.m1.5.5.1.1.1" xref="S2.E2.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.5.5.1.1.1.2" xref="S2.E2.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.5.5.1.1.1.1" xref="S2.E2.m1.5.5.1.1.1.1.cmml"><mtext id="S2.E2.m1.5.5.1.1.1.1.2" xref="S2.E2.m1.5.5.1.1.1.1.2a.cmml">cell</mtext><mrow id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">i</mi><mo id="S2.E2.m1.2.2.2.4.1" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo stretchy="false" id="S2.E2.m1.5.5.1.1.1.3" xref="S2.E2.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.6.6.3" xref="S2.E2.m1.6.6.3.cmml">=</mo><mrow id="S2.E2.m1.6.6.2" xref="S2.E2.m1.6.6.2.cmml"><mi id="S2.E2.m1.6.6.2.3" xref="S2.E2.m1.6.6.2.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.2.2" xref="S2.E2.m1.6.6.2.2.cmml">​</mo><mrow id="S2.E2.m1.6.6.2.1.1" xref="S2.E2.m1.6.6.2.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.2.1.1.2" xref="S2.E2.m1.6.6.2.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.6.6.2.1.1.1" xref="S2.E2.m1.6.6.2.1.1.1.cmml"><msup id="S2.E2.m1.6.6.2.1.1.1.2" xref="S2.E2.m1.6.6.2.1.1.1.2.cmml"><mi id="S2.E2.m1.6.6.2.1.1.1.2.2" xref="S2.E2.m1.6.6.2.1.1.1.2.2.cmml">𝒘</mi><mo id="S2.E2.m1.6.6.2.1.1.1.2.3" xref="S2.E2.m1.6.6.2.1.1.1.2.3.cmml">⊺</mo></msup><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.2.1.1.1.1" xref="S2.E2.m1.6.6.2.1.1.1.1.cmml">​</mo><msubsup id="S2.E2.m1.6.6.2.1.1.1.3" xref="S2.E2.m1.6.6.2.1.1.1.3.cmml"><mi id="S2.E2.m1.6.6.2.1.1.1.3.2.2" xref="S2.E2.m1.6.6.2.1.1.1.3.2.2.cmml">𝒙</mi><mrow id="S2.E2.m1.4.4.2.4" xref="S2.E2.m1.4.4.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml">i</mi><mo id="S2.E2.m1.4.4.2.4.1" xref="S2.E2.m1.4.4.2.3.cmml">,</mo><mi id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml">j</mi></mrow><mi id="S2.E2.m1.6.6.2.1.1.1.3.2.3" xref="S2.E2.m1.6.6.2.1.1.1.3.2.3.cmml">L</mi></msubsup></mrow><mo stretchy="false" id="S2.E2.m1.6.6.2.1.1.3" xref="S2.E2.m1.6.6.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6"><eq id="S2.E2.m1.6.6.3.cmml" xref="S2.E2.m1.6.6.3"></eq><apply id="S2.E2.m1.5.5.1.cmml" xref="S2.E2.m1.5.5.1"><times id="S2.E2.m1.5.5.1.2.cmml" xref="S2.E2.m1.5.5.1.2"></times><apply id="S2.E2.m1.5.5.1.3.cmml" xref="S2.E2.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.1.3.1.cmml" xref="S2.E2.m1.5.5.1.3">subscript</csymbol><ci id="S2.E2.m1.5.5.1.3.2.cmml" xref="S2.E2.m1.5.5.1.3.2">𝑃</ci><ci id="S2.E2.m1.5.5.1.3.3a.cmml" xref="S2.E2.m1.5.5.1.3.3"><mtext mathsize="70%" id="S2.E2.m1.5.5.1.3.3.cmml" xref="S2.E2.m1.5.5.1.3.3">corrupt</mtext></ci></apply><apply id="S2.E2.m1.5.5.1.1.1.1.cmml" xref="S2.E2.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.1.1.1.1.1.cmml" xref="S2.E2.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E2.m1.5.5.1.1.1.1.2a.cmml" xref="S2.E2.m1.5.5.1.1.1.1.2"><mtext id="S2.E2.m1.5.5.1.1.1.1.2.cmml" xref="S2.E2.m1.5.5.1.1.1.1.2">cell</mtext></ci><list id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.4"><ci id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1">𝑖</ci><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">𝑗</ci></list></apply></apply><apply id="S2.E2.m1.6.6.2.cmml" xref="S2.E2.m1.6.6.2"><times id="S2.E2.m1.6.6.2.2.cmml" xref="S2.E2.m1.6.6.2.2"></times><ci id="S2.E2.m1.6.6.2.3.cmml" xref="S2.E2.m1.6.6.2.3">𝜎</ci><apply id="S2.E2.m1.6.6.2.1.1.1.cmml" xref="S2.E2.m1.6.6.2.1.1"><times id="S2.E2.m1.6.6.2.1.1.1.1.cmml" xref="S2.E2.m1.6.6.2.1.1.1.1"></times><apply id="S2.E2.m1.6.6.2.1.1.1.2.cmml" xref="S2.E2.m1.6.6.2.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.2.1.1.1.2.1.cmml" xref="S2.E2.m1.6.6.2.1.1.1.2">superscript</csymbol><ci id="S2.E2.m1.6.6.2.1.1.1.2.2.cmml" xref="S2.E2.m1.6.6.2.1.1.1.2.2">𝒘</ci><ci id="S2.E2.m1.6.6.2.1.1.1.2.3.cmml" xref="S2.E2.m1.6.6.2.1.1.1.2.3">⊺</ci></apply><apply id="S2.E2.m1.6.6.2.1.1.1.3.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.2.1.1.1.3.1.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3">subscript</csymbol><apply id="S2.E2.m1.6.6.2.1.1.1.3.2.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.2.1.1.1.3.2.1.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3">superscript</csymbol><ci id="S2.E2.m1.6.6.2.1.1.1.3.2.2.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3.2.2">𝒙</ci><ci id="S2.E2.m1.6.6.2.1.1.1.3.2.3.cmml" xref="S2.E2.m1.6.6.2.1.1.1.3.2.3">𝐿</ci></apply><list id="S2.E2.m1.4.4.2.3.cmml" xref="S2.E2.m1.4.4.2.4"><ci id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1">𝑖</ci><ci id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2">𝑗</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.6c">P_{\text{corrupt}}(\text{cell}_{i,j})=\sigma(\boldsymbol{w}^{\intercal}\boldsymbol{x}^{L}_{i,j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.3" class="ltx_p">where <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">L</annotation></semantics></math> indexes <span id="S2.SS2.p4.3.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s final layer, <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">\sigma</annotation></semantics></math> is the sigmoid function, and <math id="S2.SS2.p4.3.m3.1" class="ltx_Math" alttext="\boldsymbol{w}" display="inline"><semantics id="S2.SS2.p4.3.m3.1a"><mi id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml">𝒘</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><ci id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1">𝒘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">\boldsymbol{w}</annotation></semantics></math> is a weight vector of the same dimensionality as the cell embedding. Our final loss function is the binary cross entropy loss of this classifier averaged across all cells in the table.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Cell corruption process</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Our formulation diverges from <cite class="ltx_cite ltx_citemacro_citet">Clark et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> in how the corrupted cells are generated. In ELECTRA, a separate generator model is trained with BERT’s masked language modeling objective to produce candidate corrupted tokens: for instance, given <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Jane went to the <span id="S2.SS3.p1.1.1.1" class="ltx_text ltx_font_smallcaps">[mask]</span> to check on her experiments</em>, the generator model might produce corrupted candidates such as <em id="S2.SS3.p1.1.2" class="ltx_emph ltx_font_italic">lab</em> or <em id="S2.SS3.p1.1.3" class="ltx_emph ltx_font_italic">office</em>. Simpler corruption strategies, such as randomly sampling words from the vocabulary, cannot induce powerful representations of text because local syntactic and semantic patterns are usually sufficient to detect obvious corruptions. For tabular data, however, we show that simple corruption strategies (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.3 Cell corruption process ‣ 2 Model ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) that take advantage of the intra-table structure actually do yield powerful representations without the need of a separate generator network. More specifically, we use two different corruption strategies:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Frequency-based cell sampling</span>: Our first strategy simply samples corrupt candidates from the training cell frequency distribution (i.e., more commonly-occurring cells are sampled more often than rare cells). One drawback of this method is that oftentimes it can result in samples that violate a particular column type (for instance, sampling a textual cell as a replacement for a cell in a numeric column). Despite its limitations, our analysis in Section <a href="#S4" title="4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that this strategy alone results in strong performance on most downstream table-based tasks, although it does not result in a rich semantic understanding of intra-table semantics.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Intra-table cell swapping</span>: To encourage the model to learn fine-grained distinctions between topically-similar data, our second strategy produces corrupted candidates by swapping two cells in the same table (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.3 Cell corruption process ‣ 2 Model ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>c, d). This task is more challenging than the frequency-based sampling strategy above, especially when the swapped cells occur within the same column. While it underperforms frequency-based sampling on downstream tasks, it qualitatively results in more semantic similarity among nearest neighbors of column and row embeddings.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2105.02584/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The different cell corruption strategies used in our experiments.</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Pretraining details</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p"><span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_bold">Data:</span>
We aim for as controlled of a comparison with TaBERT <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> as possible, as its performance on table QA tasks indicate the strength of its table encoder. TaBERT’s pretraining data was not publicly released at the time of our work, but their dataset consists of 26.6M tables from Wikipedia and the Common Crawl. We thus form a pretraining dataset of equivalent size by combining 1.8M Wikipedia tables with 24.8M preprocessed Common Crawl tables from Viznet <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The vast majority of text in these tables is in English.</span></span></span></p>
</div>
<section id="S2.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experimental settings:</h4>

<div id="S2.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p1.1" class="ltx_p">We train <span id="S2.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> for seven epochs for just over a week on 8 V100 GPUs using mixed precision. <span id="S2.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> has 12 layers and a hidden dimensionality of <math id="S2.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.1.m1.1a"><mn id="S2.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.1.m1.1b"><cn type="integer" id="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.1.m1.1c">768</annotation></semantics></math> for both row and column Transformers, in an effort to be comparably-sized to the TaBERT-Base model.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span id="footnote5.1" class="ltx_text ltx_font_smallcaps">tabbie</span> is slightly larger than TaBERT-Base (170M to 133M parameters) because its row and column Transformers are the same size, while TaBERT places a smaller “vertical” Transformer over the output of a fine-tuned BERT model.</span></span></span> Before computing the initial cell embeddings using BERT, we truncate each cell’s contents to the first 300 characters, as some cells contain huge amounts of text. We also truncate tables to 30 rows and 20 columns to avoid memory issues (note that this is much larger than the three rows used by TaBERT), and our maximum batch size is set at 4,800 cells (on average, 104 tables per batch). We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite> with a learning rate of 1e-5.</p>
</div>
<div id="S2.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p2.1" class="ltx_p">We compared two pretrained models trained with different cell corruption strategy for downstream tasks. The first strategy (<span id="S2.SS4.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">FREQ</span>) uses exclusively a frequency-based cell sampling. The second strategy is a 50/50 mixture (<span id="S2.SS4.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_bold">MIX</span>) of frequency-based sampling and intra-table cell swapping, where we additionally specify that half of the intra-table swaps must come from the same row or column to make the objective more challenging.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We validate <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s table representation quality through its performance on three downstream table-centric benchmarks (column population, row population, and column type prediction) that measure semantic table understanding. In most configurations of these tasks, <span id="S3.p1.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> outperforms TaBERT and other baselines to set new state-of-the-art numbers. Note that we do <em id="S3.p1.1.3" class="ltx_emph ltx_font_italic">not</em> investigate <span id="S3.p1.1.4" class="ltx_text ltx_font_smallcaps">tabbie</span>’s performance on table-and-text tasks such as WikiTableQuestions <cite class="ltx_cite ltx_citemacro_citep">(Pasupat and Liang, <a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>, as our focus is not on integrating <span id="S3.p1.1.5" class="ltx_text ltx_font_smallcaps">tabbie</span> into complex task-specific pipelines <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>, although this is an interesting avenue for future work.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2105.02584/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The inputs and outputs for each of our table-based prediction tasks. Column type prediction does not include headers as part of the table.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:206.6pt;height:57.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.8pt,7.2pt) scale(0.8,0.8) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Batch size</span></th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">LR</span></th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Max epochs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Column population</th>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">1e-05</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">20</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Row population</th>
<td id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_center">48</td>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_center">2e-05</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Col. type prediction</th>
<td id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">12</td>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">2e-05</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">15</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Fine-tuning hyperparameters of each downstream task for <span id="S3.T1.3.1" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Fine-tuning <span id="S3.SS1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In all of our downstream experiments, we apply essentially the same fine-tuning strategy to both <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT: we select a subset of its final-layer representations (i.e., cell or column representations) that correspond to the tabular substructures used in the downstream task, and we place a classifier over these representations to predict the training labels. We select task-specific hyperparameters based on the size of each dataset (full details in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiments ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and report the test performance of the best-performing validation checkpoint. For both models, we backpropagate the downstream error signal into all of the model’s parameters (i.e., we do not “freeze” our pretrained model).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Column Population</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the column population task, which is useful for attribute discovery, tabular data augmentation, and table retrieval <cite class="ltx_cite ltx_citemacro_cite">Das Sarma et al. (<a href="#bib.bib3" title="" class="ltx_ref">2012</a>)</cite>, a model is given the first <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">N</annotation></semantics></math> columns of a “seed” table and asked to predict the remaining column headers. <cite class="ltx_cite ltx_citemacro_citet">Zhang and Balog (<a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> compile a dataset for this task comprising 1.6M tables from Wikipedia with a test set of 1,000 tables, formulated as a multi-label classification task with 127,656 possible header labels. Importantly, we remove all of the tables in the column population test set from our pretraining data to avoid inflating our results in case <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> memorizes the missing columns during pretraining.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that TaBERT’s pretraining data likely includes the test set tables, which may give it an advantage in our comparisons.</span></span></span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To fine-tune <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> on this task, we first concatenate the column <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">[clscol]</span> embeddings of the seed table into a single vector and pass it through a single linear and softmax layer, training with a multi-label classification objective <cite class="ltx_cite ltx_citemacro_cite">Mahajan et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>. Our baselines include the generative probabilistic model (<span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_bold">GPM</span>) of <cite class="ltx_cite ltx_citemacro_citet">Zhang and Balog (<a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> as well as a word embedding-based extension called Table2VecH (<span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_bold">TH</span>) devised by <cite class="ltx_cite ltx_citemacro_citet">Deng et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. As fine-tuning on the full dataset is extremely expensive for <span id="S3.SS2.p2.1.5" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT, we fine-tune on a random subset of 100K training examples; as a further disadvantage to these, we do not use table captions (unlike GPM and GPM+TH) during training. Nevertheless, as Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Column Population ‣ 3 Experiments ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows, <span id="S3.SS2.p2.1.6" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT substantially outperform both baselines, and <span id="S3.SS2.p2.1.7" class="ltx_text ltx_font_smallcaps">tabbie</span> consistently outperforms TaBERT regardless of how many seed columns are provided, especially with only one seed column. This result indicates that <span id="S3.SS2.p2.1.8" class="ltx_text ltx_font_smallcaps">tabbie</span> encodes more semantics about headers and columns than TaBERT.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:215.4pt;height:224.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.4pt,31.7pt) scale(0.78,0.78) ;">
<table id="S3.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.3.1.1.1" class="ltx_tr">
<th id="S3.T2.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">N</th>
<th id="S3.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S3.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MAP</th>
<th id="S3.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MRR</th>
<th id="S3.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ndcg-10</th>
<th id="S3.T2.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ndcg-20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.3.1.2.1" class="ltx_tr">
<th id="S3.T2.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T2.3.1.2.1.1.1" class="ltx_text">1</span></th>
<td id="S3.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">GPM</td>
<td id="S3.T2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">25.1</td>
<td id="S3.T2.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">37.5</td>
<td id="S3.T2.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.3.1.3.2" class="ltx_tr">
<td id="S3.T2.3.1.3.2.1" class="ltx_td ltx_align_center">GPM+TH</td>
<td id="S3.T2.3.1.3.2.2" class="ltx_td ltx_align_center">25.5</td>
<td id="S3.T2.3.1.3.2.3" class="ltx_td ltx_align_center">0.38.0</td>
<td id="S3.T2.3.1.3.2.4" class="ltx_td ltx_align_center">27.1</td>
<td id="S3.T2.3.1.3.2.5" class="ltx_td ltx_align_center">31.5</td>
</tr>
<tr id="S3.T2.3.1.4.3" class="ltx_tr">
<td id="S3.T2.3.1.4.3.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T2.3.1.4.3.2" class="ltx_td ltx_align_center">33.1</td>
<td id="S3.T2.3.1.4.3.3" class="ltx_td ltx_align_center">41.3</td>
<td id="S3.T2.3.1.4.3.4" class="ltx_td ltx_align_center">35.1</td>
<td id="S3.T2.3.1.4.3.5" class="ltx_td ltx_align_center">38.1</td>
</tr>
<tr id="S3.T2.3.1.5.4" class="ltx_tr">
<td id="S3.T2.3.1.5.4.1" class="ltx_td ltx_align_center">
<span id="S3.T2.3.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T2.3.1.5.4.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.4.2.1" class="ltx_text ltx_font_bold">37.9</span></td>
<td id="S3.T2.3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.4.3.1" class="ltx_text ltx_font_bold">49.1</span></td>
<td id="S3.T2.3.1.5.4.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.4.4.1" class="ltx_text ltx_font_bold">41.2</span></td>
<td id="S3.T2.3.1.5.4.5" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.4.5.1" class="ltx_text ltx_font_bold">43.8</span></td>
</tr>
<tr id="S3.T2.3.1.6.5" class="ltx_tr">
<th id="S3.T2.3.1.6.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S3.T2.3.1.6.5.2" class="ltx_td ltx_align_center">
<span id="S3.T2.3.1.6.5.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T2.3.1.6.5.3" class="ltx_td ltx_align_center">37.1</td>
<td id="S3.T2.3.1.6.5.4" class="ltx_td ltx_align_center">48.7</td>
<td id="S3.T2.3.1.6.5.5" class="ltx_td ltx_align_center">40.4</td>
<td id="S3.T2.3.1.6.5.6" class="ltx_td ltx_align_center">43.1</td>
</tr>
<tr id="S3.T2.3.1.7.6" class="ltx_tr">
<th id="S3.T2.3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T2.3.1.7.6.1.1" class="ltx_text">2</span></th>
<td id="S3.T2.3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t">GPM</td>
<td id="S3.T2.3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t">28.5</td>
<td id="S3.T2.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t">40.4</td>
<td id="S3.T2.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.3.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.3.1.8.7" class="ltx_tr">
<td id="S3.T2.3.1.8.7.1" class="ltx_td ltx_align_center">GPM+TH</td>
<td id="S3.T2.3.1.8.7.2" class="ltx_td ltx_align_center">33.2</td>
<td id="S3.T2.3.1.8.7.3" class="ltx_td ltx_align_center">44.0</td>
<td id="S3.T2.3.1.8.7.4" class="ltx_td ltx_align_center">36.1</td>
<td id="S3.T2.3.1.8.7.5" class="ltx_td ltx_align_center">41.3</td>
</tr>
<tr id="S3.T2.3.1.9.8" class="ltx_tr">
<td id="S3.T2.3.1.9.8.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T2.3.1.9.8.2" class="ltx_td ltx_align_center">51.1</td>
<td id="S3.T2.3.1.9.8.3" class="ltx_td ltx_align_center">60.1</td>
<td id="S3.T2.3.1.9.8.4" class="ltx_td ltx_align_center">54.7</td>
<td id="S3.T2.3.1.9.8.5" class="ltx_td ltx_align_center">56.6</td>
</tr>
<tr id="S3.T2.3.1.10.9" class="ltx_tr">
<td id="S3.T2.3.1.10.9.1" class="ltx_td ltx_align_center">
<span id="S3.T2.3.1.10.9.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T2.3.1.10.9.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.10.9.2.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S3.T2.3.1.10.9.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.10.9.3.1" class="ltx_text ltx_font_bold">62.8</span></td>
<td id="S3.T2.3.1.10.9.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.10.9.4.1" class="ltx_text ltx_font_bold">55.8</span></td>
<td id="S3.T2.3.1.10.9.5" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.10.9.5.1" class="ltx_text ltx_font_bold">57.6</span></td>
</tr>
<tr id="S3.T2.3.1.11.10" class="ltx_tr">
<th id="S3.T2.3.1.11.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S3.T2.3.1.11.10.2" class="ltx_td ltx_align_center">
<span id="S3.T2.3.1.11.10.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T2.3.1.11.10.3" class="ltx_td ltx_align_center">51.7</td>
<td id="S3.T2.3.1.11.10.4" class="ltx_td ltx_align_center">62.3</td>
<td id="S3.T2.3.1.11.10.5" class="ltx_td ltx_align_center">55.6</td>
<td id="S3.T2.3.1.11.10.6" class="ltx_td ltx_align_center">57.2</td>
</tr>
<tr id="S3.T2.3.1.12.11" class="ltx_tr">
<th id="S3.T2.3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T2.3.1.12.11.1.1" class="ltx_text">3</span></th>
<td id="S3.T2.3.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t">GPM</td>
<td id="S3.T2.3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t">28.5</td>
<td id="S3.T2.3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t">35.5</td>
<td id="S3.T2.3.1.12.11.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.3.1.12.11.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.3.1.13.12" class="ltx_tr">
<td id="S3.T2.3.1.13.12.1" class="ltx_td ltx_align_center">GPM+TH</td>
<td id="S3.T2.3.1.13.12.2" class="ltx_td ltx_align_center">40.0</td>
<td id="S3.T2.3.1.13.12.3" class="ltx_td ltx_align_center">50.8</td>
<td id="S3.T2.3.1.13.12.4" class="ltx_td ltx_align_center">45.2</td>
<td id="S3.T2.3.1.13.12.5" class="ltx_td ltx_align_center">48.5</td>
</tr>
<tr id="S3.T2.3.1.14.13" class="ltx_tr">
<td id="S3.T2.3.1.14.13.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T2.3.1.14.13.2" class="ltx_td ltx_align_center">53.3</td>
<td id="S3.T2.3.1.14.13.3" class="ltx_td ltx_align_center">60.9</td>
<td id="S3.T2.3.1.14.13.4" class="ltx_td ltx_align_center">56.9</td>
<td id="S3.T2.3.1.14.13.5" class="ltx_td ltx_align_center">57.9</td>
</tr>
<tr id="S3.T2.3.1.15.14" class="ltx_tr">
<td id="S3.T2.3.1.15.14.1" class="ltx_td ltx_align_center">
<span id="S3.T2.3.1.15.14.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T2.3.1.15.14.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.15.14.2.1" class="ltx_text ltx_font_bold">54.5</span></td>
<td id="S3.T2.3.1.15.14.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.15.14.3.1" class="ltx_text ltx_font_bold">63.3</span></td>
<td id="S3.T2.3.1.15.14.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.15.14.4.1" class="ltx_text ltx_font_bold">57.9</span></td>
<td id="S3.T2.3.1.15.14.5" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.15.14.5.1" class="ltx_text ltx_font_bold">58.9</span></td>
</tr>
<tr id="S3.T2.3.1.16.15" class="ltx_tr">
<th id="S3.T2.3.1.16.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="S3.T2.3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T2.3.1.16.15.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T2.3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb">54.1</td>
<td id="S3.T2.3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb">62.3</td>
<td id="S3.T2.3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb">57.4</td>
<td id="S3.T2.3.1.16.15.6" class="ltx_td ltx_align_center ltx_border_bb">58.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S3.T2.7.1" class="ltx_text ltx_font_smallcaps">tabbie</span> outperforms all methods on the column population task, with the biggest improvement coming with just a single seed column (<math id="S3.T2.2.m1.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S3.T2.2.m1.1b"><mrow id="S3.T2.2.m1.1.1" xref="S3.T2.2.m1.1.1.cmml"><mi id="S3.T2.2.m1.1.1.2" xref="S3.T2.2.m1.1.1.2.cmml">N</mi><mo id="S3.T2.2.m1.1.1.1" xref="S3.T2.2.m1.1.1.1.cmml">=</mo><mn id="S3.T2.2.m1.1.1.3" xref="S3.T2.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.m1.1c"><apply id="S3.T2.2.m1.1.1.cmml" xref="S3.T2.2.m1.1.1"><eq id="S3.T2.2.m1.1.1.1.cmml" xref="S3.T2.2.m1.1.1.1"></eq><ci id="S3.T2.2.m1.1.1.2.cmml" xref="S3.T2.2.m1.1.1.2">𝑁</ci><cn type="integer" id="S3.T2.2.m1.1.1.3.cmml" xref="S3.T2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.m1.1d">N=1</annotation></semantics></math>). Despite its simplicity, the <span id="S3.T2.8.2" class="ltx_text ltx_font_bold">FREQ</span> corruption strategy yields better results than <span id="S3.T2.9.3" class="ltx_text ltx_font_bold">MIX</span>.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Row Population</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The row population task is more challenging than column population: given the first <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation></semantics></math> rows of a table in which the first column contains entities (e.g., “Country”), models must predict the remaining entries of the first column. Making reasonable predictions of which entities best fill the column requires understanding the full context of the seed table.
The <cite class="ltx_cite ltx_citemacro_citet">Zhang and Balog (<a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> dataset also contains a split for row population, which we use to evaluate our models. Again, since the dataset is too large for our large embedding models, we sample a subset of tables for fine-tuning.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We sample all tables that have at least five entries in the left-most column, which results in roughly 200K tables.</span></span></span> Our label space consists of 300K entities that occur at least twice in Wikipedia tables, and we again formulate this problem as multi-label classification, this time on top of the first column’s <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">[clscol]</span> representation.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Due to the large number of labels, we resort to negative sampling during training instead of the full softmax to cut down on fine-tuning time. Negative samples are formed by uniform random sampling on the label space.</span></span></span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">On this task, TaBERT and <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> again outperform the baseline Entitables model (which uses external information in the form of table captions). When given only one seed row, TaBERT slightly outperforms <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span>, but with more seed rows, <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> exhibits small improvements over TaBERT.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:214.1pt;height:182.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.2pt,25.7pt) scale(0.78,0.78) ;">
<table id="S3.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">N</th>
<th id="S3.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S3.T3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MAP</th>
<th id="S3.T3.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MRR</th>
<th id="S3.T3.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ndcg-10</th>
<th id="S3.T3.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ndcg-20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.1.2.1" class="ltx_tr">
<th id="S3.T3.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T3.3.1.2.1.1.1" class="ltx_text">1</span></th>
<td id="S3.T3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Entitables</td>
<td id="S3.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">36.8</td>
<td id="S3.T3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">45.2</td>
<td id="S3.T3.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T3.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T3.3.1.3.2" class="ltx_tr">
<td id="S3.T3.3.1.3.2.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T3.3.1.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.3.2.2.1" class="ltx_text ltx_font_bold">43.2</span></td>
<td id="S3.T3.3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.3.2.3.1" class="ltx_text ltx_font_bold">55.7</span></td>
<td id="S3.T3.3.1.3.2.4" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.3.2.4.1" class="ltx_text ltx_font_bold">45.6</span></td>
<td id="S3.T3.3.1.3.2.5" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.3.2.5.1" class="ltx_text ltx_font_bold">47.7</span></td>
</tr>
<tr id="S3.T3.3.1.4.3" class="ltx_tr">
<td id="S3.T3.3.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T3.3.1.4.3.2" class="ltx_td ltx_align_center">42.8</td>
<td id="S3.T3.3.1.4.3.3" class="ltx_td ltx_align_center">54.2</td>
<td id="S3.T3.3.1.4.3.4" class="ltx_td ltx_align_center">44.8</td>
<td id="S3.T3.3.1.4.3.5" class="ltx_td ltx_align_center">46.9</td>
</tr>
<tr id="S3.T3.3.1.5.4" class="ltx_tr">
<td id="S3.T3.3.1.5.4.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T3.3.1.5.4.2" class="ltx_td ltx_align_center">42.6</td>
<td id="S3.T3.3.1.5.4.3" class="ltx_td ltx_align_center">54.7</td>
<td id="S3.T3.3.1.5.4.4" class="ltx_td ltx_align_center">45.1</td>
<td id="S3.T3.3.1.5.4.5" class="ltx_td ltx_align_center">46.8</td>
</tr>
<tr id="S3.T3.3.1.6.5" class="ltx_tr">
<th id="S3.T3.3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.T3.3.1.6.5.1.1" class="ltx_text">2</span></th>
<td id="S3.T3.3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">Entitables</td>
<td id="S3.T3.3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">37.2</td>
<td id="S3.T3.3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">45.1</td>
<td id="S3.T3.3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T3.3.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T3.3.1.7.6" class="ltx_tr">
<td id="S3.T3.3.1.7.6.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T3.3.1.7.6.2" class="ltx_td ltx_align_center">43.8</td>
<td id="S3.T3.3.1.7.6.3" class="ltx_td ltx_align_center">56.0</td>
<td id="S3.T3.3.1.7.6.4" class="ltx_td ltx_align_center">46.4</td>
<td id="S3.T3.3.1.7.6.5" class="ltx_td ltx_align_center">48.8</td>
</tr>
<tr id="S3.T3.3.1.8.7" class="ltx_tr">
<td id="S3.T3.3.1.8.7.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.1.8.7.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T3.3.1.8.7.2" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.8.7.2.1" class="ltx_text ltx_font_bold">44.4</span></td>
<td id="S3.T3.3.1.8.7.3" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.8.7.3.1" class="ltx_text ltx_font_bold">57.2</span></td>
<td id="S3.T3.3.1.8.7.4" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.8.7.4.1" class="ltx_text ltx_font_bold">47.1</span></td>
<td id="S3.T3.3.1.8.7.5" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.8.7.5.1" class="ltx_text ltx_font_bold">49.5</span></td>
</tr>
<tr id="S3.T3.3.1.9.8" class="ltx_tr">
<td id="S3.T3.3.1.9.8.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.1.9.8.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T3.3.1.9.8.2" class="ltx_td ltx_align_center">43.7</td>
<td id="S3.T3.3.1.9.8.3" class="ltx_td ltx_align_center">55.7</td>
<td id="S3.T3.3.1.9.8.4" class="ltx_td ltx_align_center">46.2</td>
<td id="S3.T3.3.1.9.8.5" class="ltx_td ltx_align_center">48.6</td>
</tr>
<tr id="S3.T3.3.1.10.9" class="ltx_tr">
<th id="S3.T3.3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S3.T3.3.1.10.9.1.1" class="ltx_text">3</span></th>
<td id="S3.T3.3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_t">Entitables</td>
<td id="S3.T3.3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t">37.1</td>
<td id="S3.T3.3.1.10.9.4" class="ltx_td ltx_align_center ltx_border_t">44.6</td>
<td id="S3.T3.3.1.10.9.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T3.3.1.10.9.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T3.3.1.11.10" class="ltx_tr">
<td id="S3.T3.3.1.11.10.1" class="ltx_td ltx_align_center">TaBERT</td>
<td id="S3.T3.3.1.11.10.2" class="ltx_td ltx_align_center">42.9</td>
<td id="S3.T3.3.1.11.10.3" class="ltx_td ltx_align_center">55.1</td>
<td id="S3.T3.3.1.11.10.4" class="ltx_td ltx_align_center">45.6</td>
<td id="S3.T3.3.1.11.10.5" class="ltx_td ltx_align_center">48.5</td>
</tr>
<tr id="S3.T3.3.1.12.11" class="ltx_tr">
<td id="S3.T3.3.1.12.11.1" class="ltx_td ltx_align_center">
<span id="S3.T3.3.1.12.11.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</td>
<td id="S3.T3.3.1.12.11.2" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.12.11.2.1" class="ltx_text ltx_font_bold">43.4</span></td>
<td id="S3.T3.3.1.12.11.3" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.12.11.3.1" class="ltx_text ltx_font_bold">56.5</span></td>
<td id="S3.T3.3.1.12.11.4" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.12.11.4.1" class="ltx_text ltx_font_bold">46.6</span></td>
<td id="S3.T3.3.1.12.11.5" class="ltx_td ltx_align_center"><span id="S3.T3.3.1.12.11.5.1" class="ltx_text ltx_font_bold">49.0</span></td>
</tr>
<tr id="S3.T3.3.1.13.12" class="ltx_tr">
<td id="S3.T3.3.1.13.12.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T3.3.1.13.12.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</td>
<td id="S3.T3.3.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">42.9</td>
<td id="S3.T3.3.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">55.5</td>
<td id="S3.T3.3.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb">45.9</td>
<td id="S3.T3.3.1.13.12.5" class="ltx_td ltx_align_center ltx_border_bb">48.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S3.T3.7.1" class="ltx_text ltx_font_smallcaps">tabbie</span> outperforms baselines on row population when provided with more seed rows <math id="S3.T3.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.T3.2.m1.1b"><mi id="S3.T3.2.m1.1.1" xref="S3.T3.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.m1.1c"><ci id="S3.T3.2.m1.1.1.cmml" xref="S3.T3.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.m1.1d">N</annotation></semantics></math>, although TaBERT is superior given just a single seed row. Again, the <span id="S3.T3.8.2" class="ltx_text ltx_font_bold">FREQ</span> strategy produces better results than <span id="S3.T3.9.3" class="ltx_text ltx_font_bold">MIX</span>.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Column Type Prediction</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">While the prior two tasks involve predicting missing elements of a table, the column type prediction task involves predicting a high-level <em id="S3.SS4.p1.1.1" class="ltx_emph ltx_font_italic">type</em> of a particular column (e.g., <em id="S3.SS4.p1.1.2" class="ltx_emph ltx_font_italic">name</em>, <em id="S3.SS4.p1.1.3" class="ltx_emph ltx_font_italic">age</em>, etc.) without access to its header. This task is useful when indexing tables with missing column names, which happens relatively often in practice, or for schema matching<cite class="ltx_cite ltx_citemacro_cite">Hulsebos et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>); Rahm and Bernstein (<a href="#bib.bib23" title="" class="ltx_ref">2001</a>)</cite>, and like the other tasks, requires understanding the surrounding context. We evaluate our models on the same subset of VizNet Web Tables <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite><span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Again, we ensure that none of the test tables in this dataset occur in <span id="footnote9.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s pretraining data.</span></span></span> created by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> to evaluate their column type predictor, SATO<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://github.com/megagonlabs/sato" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/megagonlabs/sato</a></span></span></span>. They formulate this task as a multi-class classification problem (with 78 classes), with a training set of 64,000 tables and a test set consisting of 16,000 tables. We set aside 6,400 training tables to form a validation for both <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT, and we fine-tune each of these models with small random subsets of the training data (1000 and 10000 labeled tables) in addition to the full training set to evaluate their performance in a simulated low-resource setting.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Along with TaBERT, we compare with two recently-proposed column type prediction methods: Sherlock <cite class="ltx_cite ltx_citemacro_cite">Hulsebos et al. (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, which uses a multi-input neural network with hand-crafted features extracted from each column, and the aforementioned SATO <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, which improves Sherlock by incorporating table context, topic model outputs, and label co-occurrence information. Table <a href="#S3.T4" title="Table 4 ‣ 3.4 Column Type Prediction ‣ 3 Experiments ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the support-weighted F1-score for each method. Similar to the previous two tasks, <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT significantly outperform the prior state-of-the-art (SATO). Here, there are no clear differences between the two models, but both reach higher F1 scores than the other baselines even when given only 1,000 training examples, which demonstrates the power of table-based pretraining.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:187.0pt;height:91.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.5pt,8.1pt) scale(0.85,0.85) ;">
<table id="S3.T4.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.3.3.3" class="ltx_tr">
<th id="S3.T4.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S3.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T4.1.1.1.1.m1.1a"><mi id="S3.T4.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">n</annotation></semantics></math>=1000</th>
<th id="S3.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S3.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T4.2.2.2.2.m1.1a"><mi id="S3.T4.2.2.2.2.m1.1.1" xref="S3.T4.2.2.2.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.2.m1.1b"><ci id="S3.T4.2.2.2.2.m1.1.1.cmml" xref="S3.T4.2.2.2.2.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.2.m1.1c">n</annotation></semantics></math>=10000</th>
<th id="S3.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S3.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T4.3.3.3.3.m1.1a"><mi id="S3.T4.3.3.3.3.m1.1.1" xref="S3.T4.3.3.3.3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.3.m1.1b"><ci id="S3.T4.3.3.3.3.m1.1.1.cmml" xref="S3.T4.3.3.3.3.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.3.m1.1c">n</annotation></semantics></math>=all</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.3.3.4.1" class="ltx_tr">
<th id="S3.T4.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Sherlock</th>
<td id="S3.T4.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T4.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T4.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">86.7</td>
</tr>
<tr id="S3.T4.3.3.5.2" class="ltx_tr">
<th id="S3.T4.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SATO</th>
<td id="S3.T4.3.3.5.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T4.3.3.5.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T4.3.3.5.2.4" class="ltx_td ltx_align_center">90.8</td>
</tr>
<tr id="S3.T4.3.3.6.3" class="ltx_tr">
<th id="S3.T4.3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TaBERT</th>
<td id="S3.T4.3.3.6.3.2" class="ltx_td ltx_align_center"><span id="S3.T4.3.3.6.3.2.1" class="ltx_text ltx_font_bold">84.7</span></td>
<td id="S3.T4.3.3.6.3.3" class="ltx_td ltx_align_center">93.5</td>
<td id="S3.T4.3.3.6.3.4" class="ltx_td ltx_align_center"><span id="S3.T4.3.3.6.3.4.1" class="ltx_text ltx_font_bold">97.2</span></td>
</tr>
<tr id="S3.T4.3.3.7.4" class="ltx_tr">
<th id="S3.T4.3.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.T4.3.3.7.4.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S3.T4.3.3.7.4.2" class="ltx_td ltx_align_center"><span id="S3.T4.3.3.7.4.2.1" class="ltx_text ltx_font_bold">84.7</span></td>
<td id="S3.T4.3.3.7.4.3" class="ltx_td ltx_align_center"><span id="S3.T4.3.3.7.4.3.1" class="ltx_text ltx_font_bold">94.2</span></td>
<td id="S3.T4.3.3.7.4.4" class="ltx_td ltx_align_center">96.9</td>
</tr>
<tr id="S3.T4.3.3.8.5" class="ltx_tr">
<th id="S3.T4.3.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S3.T4.3.3.8.5.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S3.T4.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_bb">84.1</td>
<td id="S3.T4.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_bb">93.8</td>
<td id="S3.T4.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_bb">96.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Support-weighted F1-score of different models on column type prediction. TaBERT and <span id="S3.T4.9.1" class="ltx_text ltx_font_smallcaps">tabbie</span> perform similarly in low resource settings (<math id="S3.T4.6.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T4.6.m1.1b"><mi id="S3.T4.6.m1.1.1" xref="S3.T4.6.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T4.6.m1.1c"><ci id="S3.T4.6.m1.1.1.cmml" xref="S3.T4.6.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.m1.1d">n</annotation></semantics></math>=1000) and when the full training data is used (<math id="S3.T4.7.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T4.7.m2.1b"><mi id="S3.T4.7.m2.1.1" xref="S3.T4.7.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T4.7.m2.1c"><ci id="S3.T4.7.m2.1.1.cmml" xref="S3.T4.7.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.m2.1d">n</annotation></semantics></math>=all).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The results in the previous section show that <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> is a powerful table representation method, outperforming TaBERT in many downstream task configurations and remaining competitive in the rest. In this section, we dig deeper into <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span>’s representations by comparing them to TaBERT across a variety of quantitative and qualitative analysis tasks, including our own pretraining task of corrupt cell classification, as well as embedding clustering and nearest neighbors. Taken as a whole, the analysis suggests that <span id="S4.p1.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> is able to better capture fine-grained table semantics.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Corrupt Cell Detection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We first examine how TaBERT performs on <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s pretraining task of corrupt cell detection, which again is practically useful as a post-processing step after table structure decomposition <cite class="ltx_cite ltx_citemacro_cite">Tensmeyer et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>); Raja et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> because mistakes in predicting row/column/cell boundaries (sometimes compounded by OCR errors) can lead to inaccurate extraction. We fine-tune TaBERT on 100K tables using the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">MIX</span> corruption strategy for ten epochs, and construct a test set of 10K tables that are unseen by both TaBERT and <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> during pretraining. While <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">tabbie</span> of course sees an order of magnitude more tables for this task during pretraining, this is still a useful experiment to determine if TaBERT’s pretraining objective enables it to easily detect corrupted cells.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As shown in Table <a href="#S4.T5" title="Table 5 ‣ 4.1 Corrupt Cell Detection ‣ 4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, TaBERT performs significantly worse than <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> on all types of corrupt cells (both random corruption and intra-table swaps). Additionally, intra-column swaps are the most difficult for both models, as <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> achieves a <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">68.8</span> F1 on this subset compared to just <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">23.7</span> F1 by TaBERT. Interestingly, while the <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_bold">MIX</span> strategy consistently performs worse than <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_bold">FREQ</span> for the <span id="S4.SS1.p2.1.7" class="ltx_text ltx_font_smallcaps">tabbie</span> models evaluated on the three downstream tasks in the previous section, it is substantially better at detecting more challenging corruptions, and is almost equivalent to detecting random cells sampled by <span id="S4.SS1.p2.1.8" class="ltx_text ltx_font_bold">FREQ</span>. This result indicates that perhaps more complex table-based tasks are required to take advantage of representations derived using <span id="S4.SS1.p2.1.9" class="ltx_text ltx_font_bold">MIX</span> corruption.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:218.7pt;height:221.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.7pt,33.1pt) scale(0.77,0.77) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Corruption</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Prec.</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Rec.</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.2.1.1.1" class="ltx_text"><em id="S4.T5.1.1.2.1.1.1.1" class="ltx_emph ltx_font_italic">Intra-row swap</em></span></th>
<th id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">TaBERT</th>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">85.5</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">83.0</td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">84.2</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center">99.0</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center">81.4</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center">89.4</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.4.3.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center">99.6</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center">95.8</td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.3.5.1" class="ltx_text ltx_font_bold">97.7</span></td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<th id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.5.4.1.1" class="ltx_text"><em id="S4.T5.1.1.5.4.1.1.1" class="ltx_emph ltx_font_italic">Intra-column swap</em></span></th>
<th id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">TaBERT</th>
<td id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">31.2</td>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">19.0</td>
<td id="S4.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">23.7</td>
</tr>
<tr id="S4.T5.1.1.6.5" class="ltx_tr">
<th id="S4.T5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.6.5.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S4.T5.1.1.6.5.2" class="ltx_td ltx_align_center">90.9</td>
<td id="S4.T5.1.1.6.5.3" class="ltx_td ltx_align_center">22.3</td>
<td id="S4.T5.1.1.6.5.4" class="ltx_td ltx_align_center">35.8</td>
</tr>
<tr id="S4.T5.1.1.7.6" class="ltx_tr">
<th id="S4.T5.1.1.7.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.7.6.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S4.T5.1.1.7.6.3" class="ltx_td ltx_align_center">91.5</td>
<td id="S4.T5.1.1.7.6.4" class="ltx_td ltx_align_center">55.0</td>
<td id="S4.T5.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.7.6.5.1" class="ltx_text ltx_font_bold">68.8</span></td>
</tr>
<tr id="S4.T5.1.1.8.7" class="ltx_tr">
<th id="S4.T5.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.8.7.1.1" class="ltx_text"><em id="S4.T5.1.1.8.7.1.1.1" class="ltx_emph ltx_font_italic">Intra-table swap</em></span></th>
<th id="S4.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">TaBERT</th>
<td id="S4.T5.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">81.2</td>
<td id="S4.T5.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">69.5</td>
<td id="S4.T5.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">74.9</td>
</tr>
<tr id="S4.T5.1.1.9.8" class="ltx_tr">
<th id="S4.T5.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.9.8.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S4.T5.1.1.9.8.2" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T5.1.1.9.8.3" class="ltx_td ltx_align_center">73.3</td>
<td id="S4.T5.1.1.9.8.4" class="ltx_td ltx_align_center">84.0</td>
</tr>
<tr id="S4.T5.1.1.10.9" class="ltx_tr">
<th id="S4.T5.1.1.10.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.10.9.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S4.T5.1.1.10.9.3" class="ltx_td ltx_align_center">98.4</td>
<td id="S4.T5.1.1.10.9.4" class="ltx_td ltx_align_center">86.2</td>
<td id="S4.T5.1.1.10.9.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.10.9.5.1" class="ltx_text ltx_font_bold">91.9</span></td>
</tr>
<tr id="S4.T5.1.1.11.10" class="ltx_tr">
<th id="S4.T5.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.11.10.1.1" class="ltx_text"><em id="S4.T5.1.1.11.10.1.1.1" class="ltx_emph ltx_font_italic">Random FREQ cell</em></span></th>
<th id="S4.T5.1.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">TaBERT</th>
<td id="S4.T5.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t">86.7</td>
<td id="S4.T5.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t">87.0</td>
<td id="S4.T5.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t">86.8</td>
</tr>
<tr id="S4.T5.1.1.12.11" class="ltx_tr">
<th id="S4.T5.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.12.11.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S4.T5.1.1.12.11.2" class="ltx_td ltx_align_center">99.3</td>
<td id="S4.T5.1.1.12.11.3" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T5.1.1.12.11.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.12.11.4.1" class="ltx_text ltx_font_bold">98.8</span></td>
</tr>
<tr id="S4.T5.1.1.13.12" class="ltx_tr">
<th id="S4.T5.1.1.13.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.13.12.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S4.T5.1.1.13.12.3" class="ltx_td ltx_align_center">99.1</td>
<td id="S4.T5.1.1.13.12.4" class="ltx_td ltx_align_center">98.1</td>
<td id="S4.T5.1.1.13.12.5" class="ltx_td ltx_align_center">98.6</td>
</tr>
<tr id="S4.T5.1.1.14.13" class="ltx_tr">
<th id="S4.T5.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T5.1.1.14.13.1.1" class="ltx_text"><em id="S4.T5.1.1.14.13.1.1.1" class="ltx_emph ltx_font_italic">All</em></span></th>
<th id="S4.T5.1.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">TaBERT</th>
<td id="S4.T5.1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_t">75.6</td>
<td id="S4.T5.1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_t">65.2</td>
<td id="S4.T5.1.1.14.13.5" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
</tr>
<tr id="S4.T5.1.1.15.14" class="ltx_tr">
<th id="S4.T5.1.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S4.T5.1.1.15.14.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (FREQ)</th>
<td id="S4.T5.1.1.15.14.2" class="ltx_td ltx_align_center">98.2</td>
<td id="S4.T5.1.1.15.14.3" class="ltx_td ltx_align_center">69.5</td>
<td id="S4.T5.1.1.15.14.4" class="ltx_td ltx_align_center">81.4</td>
</tr>
<tr id="S4.T5.1.1.16.15" class="ltx_tr">
<th id="S4.T5.1.1.16.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<th id="S4.T5.1.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T5.1.1.16.15.2.1" class="ltx_text ltx_font_smallcaps">tabbie</span> (MIX)</th>
<td id="S4.T5.1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb">97.8</td>
<td id="S4.T5.1.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb">84.1</td>
<td id="S4.T5.1.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.1.16.15.5.1" class="ltx_text ltx_font_bold">90.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>A fine-grained comparison of different models on corrupt cell detection, with different types of corruption. TaBERT struggles on this task, especially in the challenging setting of <em id="S4.T5.5.1" class="ltx_emph ltx_font_italic">intra-column swaps</em>. Unlike our downstream tasks, the <span id="S4.T5.6.2" class="ltx_text ltx_font_bold">MIX</span> strategy is far superior to <span id="S4.T5.7.3" class="ltx_text ltx_font_bold">FREQ</span> here.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2105.02584/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>In this figure, (b) and (c) contain the predicted corruption probability of each cell in (a). Only <span id="S4.F5.3.1" class="ltx_text ltx_font_smallcaps">tabbie</span> <span id="S4.F5.4.2" class="ltx_text ltx_font_bold">MIX</span> is able to reliably identify violations of numerical trends in columns. </figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Nearest neighbors</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We now turn to a qualitative analysis of the representations learned by <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>. In Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Nearest neighbors ‣ 4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (top), we display the two nearest neighbor columns from our validation set to the <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">date</em> column marked by the red box. <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> is able to model the similarity of <em id="S4.SS2.p1.1.4" class="ltx_emph ltx_font_italic">feb. 16</em> and <em id="S4.SS2.p1.1.5" class="ltx_emph ltx_font_italic">saturday. february 5th</em> despite the formatting difference, while TaBERT’s neighbors more closely resemble the original column. Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Nearest neighbors ‣ 4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (bottom) shows that <span id="S4.SS2.p1.1.6" class="ltx_text ltx_font_smallcaps">tabbie</span>’s nearest neighbors are less reliant on matching headers than TaBERT, as the neighbors all have different headers (<em id="S4.SS2.p1.1.7" class="ltx_emph ltx_font_italic">nom</em>, <em id="S4.SS2.p1.1.8" class="ltx_emph ltx_font_italic">nombre</em>, <em id="S4.SS2.p1.1.9" class="ltx_emph ltx_font_italic">name</em>).</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2105.02584/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="275" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2105.02584/assets/x7.png" id="S4.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="244" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Nearest neighbors of the <em id="S4.F6.5.1" class="ltx_emph ltx_font_italic">date</em> and <em id="S4.F6.6.2" class="ltx_emph ltx_font_italic">nom</em> columns from the tables on the left, from both <span id="S4.F6.7.3" class="ltx_text ltx_font_smallcaps">tabbie</span> and TaBERT. <span id="S4.F6.8.4" class="ltx_text ltx_font_smallcaps">tabbie</span>’s nearest neighbors exhibit more diverse formatting and less reliance on the header, which is an example of its semantic representation capability.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Clustering</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">Are the embeddings produced by <span id="S4.SS3.p1.4.1" class="ltx_text ltx_font_smallcaps">tabbie</span> useful for clustering and data discovery? To find out, we perform clustering experiments on the FinTabNet dataset from <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. This dataset contains <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\sim</annotation></semantics></math>110K tables from financial reports of corporations in the S&amp;P-500. We use the <span id="S4.SS3.p1.4.2" class="ltx_text ltx_font_smallcaps">[cls]</span> embedding at the <math id="S4.SS3.p1.2.m2.2" class="ltx_Math" alttext="(0,0)" display="inline"><semantics id="S4.SS3.p1.2.m2.2a"><mrow id="S4.SS3.p1.2.m2.2.3.2" xref="S4.SS3.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS3.p1.2.m2.2.3.2.1" xref="S4.SS3.p1.2.m2.2.3.1.cmml">(</mo><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">0</mn><mo id="S4.SS3.p1.2.m2.2.3.2.2" xref="S4.SS3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS3.p1.2.m2.2.2" xref="S4.SS3.p1.2.m2.2.2.cmml">0</mn><mo stretchy="false" id="S4.SS3.p1.2.m2.2.3.2.3" xref="S4.SS3.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.2b"><interval closure="open" id="S4.SS3.p1.2.m2.2.3.1.cmml" xref="S4.SS3.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">0</cn><cn type="integer" id="S4.SS3.p1.2.m2.2.2.cmml" xref="S4.SS3.p1.2.m2.2.2">0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.2c">(0,0)</annotation></semantics></math> position (i.e., the top left-most cell in the table), extracted from a <span id="S4.SS3.p1.4.3" class="ltx_text ltx_font_smallcaps">tabbie</span> model trained with the <span id="S4.SS3.p1.4.4" class="ltx_text ltx_font_bold">FREQ</span> strategy, as a representative embedding for each table in the dataset. Next, we perform <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">k</annotation></semantics></math>-means clustering on these embeddings using the FAISS library <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>, with <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mi id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><ci id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">k</annotation></semantics></math>=1024 centroids. While the FinTabNet dataset is restricted to the homogenous domain of financial tables, these tables cluster into sub-types such as <em id="S4.SS3.p1.4.5" class="ltx_emph ltx_font_italic">consolidated financial tables</em>, <em id="S4.SS3.p1.4.6" class="ltx_emph ltx_font_italic">jurisdiction tables</em>, <em id="S4.SS3.p1.4.7" class="ltx_emph ltx_font_italic">insurance tables</em>, etc. We then examine the contents of these clusters (Figure  <a href="#S4.F7" title="Figure 7 ‣ 4.3 Clustering ‣ 4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) and observe that <span id="S4.SS3.p1.4.8" class="ltx_text ltx_font_smallcaps">tabbie</span> embeddings can not only be clustered into these sub-types, but also that tables from reports of the same company, but from different financial years, are placed into the same cluster.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2105.02584/assets/x8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Sample tables from clusters obtained by running <math id="S4.F7.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.F7.2.m1.1b"><mi id="S4.F7.2.m1.1.1" xref="S4.F7.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F7.2.m1.1c"><ci id="S4.F7.2.m1.1.1.cmml" xref="S4.F7.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.m1.1d">k</annotation></semantics></math>-means on <span id="S4.F7.7.1" class="ltx_text ltx_font_smallcaps">tabbie</span>’s <span id="S4.F7.8.2" class="ltx_text ltx_font_smallcaps">[cls]</span> embeddings on the FinTabNet dataset. <span id="S4.F7.9.3" class="ltx_text ltx_font_smallcaps">tabbie</span> not only clusters embeddings into reasonable semantic types, such as <em id="S4.F7.10.4" class="ltx_emph ltx_font_italic">Table of Contents</em> (first row), but it also places tables of the same type from the same company into the same cluster (second and third rows). We provide the source images of the corresponding tables in this figure. </figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Identifying numeric trends</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Next, we analyze how well <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> understands trends in numerical columns by looking at specific examples of our corrupt cell detection task. The first column of the table in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 Corrupt Cell Detection ‣ 4 Analysis ‣ TABBIE: Pretrained Representations of Tabular Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> contains jersey numbers sorted in ascending order. We swap two cells in this column, <em id="S4.SS4.p1.1.2" class="ltx_emph ltx_font_italic">16</em> and <em id="S4.SS4.p1.1.3" class="ltx_emph ltx_font_italic">18</em>, which violates the increasing trend. Both TaBERT (fine-tuned for corrupt cell detection) and <span id="S4.SS4.p1.1.4" class="ltx_text ltx_font_smallcaps">tabbie</span> <span id="S4.SS4.p1.1.5" class="ltx_text ltx_font_bold">FREQ</span> struggle to identify this swap, while <span id="S4.SS4.p1.1.6" class="ltx_text ltx_font_smallcaps">tabbie</span> <span id="S4.SS4.p1.1.7" class="ltx_text ltx_font_bold">MIX</span> is almost certain that the two cells have been corrupted. This qualitative result is further evidence that the <span id="S4.SS4.p1.1.8" class="ltx_text ltx_font_bold">MIX</span> model has potential for more complex table-based reasoning tasks.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The staggering amount of structured relational data in the form of tables on the Internet has attracted considerable attention from researchers over the past two decades <cite class="ltx_cite ltx_citemacro_cite">Cafarella et al. (<a href="#bib.bib1" title="" class="ltx_ref">2008</a>); Limaye et al. (<a href="#bib.bib15" title="" class="ltx_ref">2010</a>); Venetis et al. (<a href="#bib.bib28" title="" class="ltx_ref">2011</a>); Suchanek et al. (<a href="#bib.bib25" title="" class="ltx_ref">2007</a>); Embley et al. (<a href="#bib.bib7" title="" class="ltx_ref">2006</a>)</cite>, with applications including retrieval <cite class="ltx_cite ltx_citemacro_cite">Das Sarma et al. (<a href="#bib.bib3" title="" class="ltx_ref">2012</a>)</cite>, schema-matching <cite class="ltx_cite ltx_citemacro_cite">Madhavan et al. (<a href="#bib.bib18" title="" class="ltx_ref">2001</a>, <a href="#bib.bib17" title="" class="ltx_ref">2005</a>)</cite>, and entity linking <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Similar to popular large-scale language models pretrained on tasks involving unstructured natural language<cite class="ltx_cite ltx_citemacro_cite">Peters et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>); Devlin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>); Liu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>, our work is part of a recent trend of self-supervised models trained on structured tabular data. TaBERT <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> and TaPaS <cite class="ltx_cite ltx_citemacro_cite">Herzig et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> jointly model tables with text (typically captions or questions), and are thus more suited for tasks like question answering <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite>. For pretraining, TaBERT attempts to recover the name and data-type of masked column headers (masked column prediction), in addition to contents of a particular cell (cell value recovery). The pretraining objectives of TaPaS, on the other hand, encourage tabular textual entailment. In a concurrent work, the TUTA model <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite> uses masked language modeling, cell-level cloze prediction, and table-context retrieval as pretraining objectives. Further, in addition to traditional position embeddings, this work accounts for the hierarchical nature of tabular data using tree-based positional embeddings. Similiarly, in <cite class="ltx_cite ltx_citemacro_citet">Deng et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, the authors perform a variant of MLM called masked entity recovery. In contrast, <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span> is pretrained strictly on tabular data and intended for more general-purpose table-based tasks, and uses corrupt-cell classification as its pretraining task.

</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we proposed <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">tabbie</span>, a self-supervised pretraining method for tables without associated text. To reduce the computational cost of training our model, we repurpose the ELECTRA objective for corrupt cell detection, and we use two separate Transformers for rows and columns to minimize complexity associated with sequence length. On three downstream table-based tasks, <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">tabbie</span> achieves competitive or better performance to existing methods such as TaBERT, and an analysis reveals that its representations include a deep semantic understanding of cells, rows, and columns. We publicly release our <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">tabbie</span> pretrained models and code to facilitate future research on tabular representation learning.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Ethics Statement</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.2" class="ltx_p">As with any research work that involves training large language models, we acknowledge that our work has a negative carbon impact on the environment. A cumulative of 1344 GPU-hours of computation was performed on Tesla V100 GPUs. Total emissions are estimated to be 149.19 kg of CO<sub id="S7.p1.2.1" class="ltx_sub">2</sub> per run of our model (in total, there were two runs). While this is a significant amount (equivalent to <math id="S7.p1.2.m2.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S7.p1.2.m2.1a"><mo id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><approx id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">\approx</annotation></semantics></math> 17 gallons of fuel consumed by an average motor vehicle<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://www.epa.gov/greenvehicles/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.epa.gov/greenvehicles/</a></span></span></span>), it is lower than TaBERT’s cost per run by more than a factor of 10 assuming a similar computing platform was used.
Estimations were conducted using the <a target="_blank" href="https://mlco2.github.io/impact#compute" title="" class="ltx_ref ltx_href">Machine Learning Impact calculator</a> presented in <cite class="ltx_cite ltx_citemacro_citet">Lacoste et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We thank the anonymous reviewers for their useful comments. We thank Christopher Tensmeyer for helpful comments and pointing us to relevant datasets for some of our experiments. We also thank the UMass NLP group for feedback during the paper writing process. This work was made possible by research awards from Sony Corp. and Adobe Inc. MI is also partially supported by award IIS-1955567 from the National Science
Foundation (NSF).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cafarella et al. (2008)</span>
<span class="ltx_bibblock">
Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang.
2008.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14778/1453856.1453916" title="" class="ltx_ref ltx_href">Webtables:
Exploring the power of tables on the web</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 1(1):538–549.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2020)</span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=r1xMH1BtvB" title="" class="ltx_ref ltx_href">Electra:
Pre-training text encoders as discriminators rather than generators</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das Sarma et al. (2012)</span>
<span class="ltx_bibblock">
Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu,
Reynold Xin, and Cong Yu. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/2213836.2213962" title="" class="ltx_ref ltx_href">Finding related
tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2012 ACM SIGMOD International Conference
on Management of Data</em>, SIGMOD ’12, page 817–828, New York, NY, USA.
Association for Computing Machinery.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2019)</span>
<span class="ltx_bibblock">
Li Deng, Shuo Zhang, and Krisztian Balog. 2019.

</span>
<span class="ltx_bibblock">Table2vec: Neural word and entity embeddings for table population and
retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of SIGIR 2019</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2020.

</span>
<span class="ltx_bibblock">Turl: Table understanding through representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 14(3):307–319.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Embley et al. (2006)</span>
<span class="ltx_bibblock">
D. Embley, Matthew Hurst, D. Lopresti, and G. Nagy. 2006.

</span>
<span class="ltx_bibblock">Table-processing paradigms: a research survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Journal of Document Analysis and Recognition
(IJDAR)</em>, 8:66–86.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Herzig, P. Nowak, Thomas Müller, Francesco Piccinno, and
Julian Martin Eisenschlos. 2020.

</span>
<span class="ltx_bibblock">Tapas: Weakly supervised table parsing via pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2019)</span>
<span class="ltx_bibblock">
Kevin Zeng Hu, Snehalkumar (Neil) S. Gaikwad, Madelon Hulsebos, Michiel A.
Bakker, Emanuel Zgraggen, César A. Hidalgo, Tim Kraska, Guoliang Li,
Arvind Satyanarayan, and Çagatay Demiralp. 2019.

</span>
<span class="ltx_bibblock">Viznet: Towards A large-scale visualization learning and
benchmarking repository.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in
Computing Systems, CHI 2019, Glasgow, Scotland, UK, May 04-09, 2019</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hulsebos et al. (2019)</span>
<span class="ltx_bibblock">
M. Hulsebos, K. Hu, M. Bakker, Emanuel Zgraggen, Arvind Satyanarayan,
T. Kraska, cCaugatay Demiralp, and C’esar A. Hidalgo. 2019.

</span>
<span class="ltx_bibblock">Sherlock: A deep learning approach to semantic data type detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1702.08734</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lacoste et al. (2019)</span>
<span class="ltx_bibblock">
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.
2019.

</span>
<span class="ltx_bibblock">Quantifying the carbon emissions of machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.09700</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2018)</span>
<span class="ltx_bibblock">
Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc Le, and Ni Lao. 2018.

</span>
<span class="ltx_bibblock">Memory augmented policy optimization for program synthesis and
semantic parsing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on Neural
Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Limaye et al. (2010)</span>
<span class="ltx_bibblock">
Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14778/1920841.1921005" title="" class="ltx_ref ltx_href">Annotating and
searching web tables using entities, types and relationships</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 3(1):1338–1347.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1907.11692.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madhavan et al. (2005)</span>
<span class="ltx_bibblock">
Jayant Madhavan, Philip A. Bernstein, AnHai Doan, and Alon Halevy. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDE.2005.39" title="" class="ltx_ref ltx_href">Corpus-based schema
matching</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st International Conference on Data
Engineering</em>, ICDE ’05, page 57–68, USA. IEEE Computer Society.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madhavan et al. (2001)</span>
<span class="ltx_bibblock">
Jayant Madhavan, Philip A. Bernstein, and Erhard Rahm. 2001.

</span>
<span class="ltx_bibblock">Generic schema matching with cupid.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference on Very
Large Data Bases</em>, VLDB ’01, page 49–58, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahajan et al. (2018)</span>
<span class="ltx_bibblock">
D. Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
Y. Li, Ashwin Bharambe, and L. V. D. Maaten. 2018.

</span>
<span class="ltx_bibblock">Exploring the limits of weakly supervised pretraining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishida et al. (2017)</span>
<span class="ltx_bibblock">
Kyosuke Nishida, Kugatsu Sadamitsu, Ryuichiro Higashinaka, and Yoshihiro
Matsuo. 2017.

</span>
<span class="ltx_bibblock">Understanding the semantic structures of tables with a hybrid deep
neural network architecture.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Thirty-First AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang. 2015.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al. (2018)</span>
<span class="ltx_bibblock">
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. of NAACL</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahm and Bernstein (2001)</span>
<span class="ltx_bibblock">
Erhard Rahm and Philip A. Bernstein. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dblp.uni-trier.de/db/journals/vldb/vldb10.html#RahmB01" title="" class="ltx_ref ltx_href">A survey of approaches to automatic schema matching.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">VLDB J.</em>, 10(4):334–350.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raja et al. (2020)</span>
<span class="ltx_bibblock">
Sachin Raja, Ajoy Mondal, and C. V. Jawahar. 2020.

</span>
<span class="ltx_bibblock">Table structure recognition using top-down and bottom-up cues.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2020</em>, pages 70–86, Cham. Springer
International Publishing.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suchanek et al. (2007)</span>
<span class="ltx_bibblock">
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007.

</span>
<span class="ltx_bibblock">Yago: A core of semantic knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th International Conference on World
Wide Web</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tensmeyer et al. (2019)</span>
<span class="ltx_bibblock">
C. Tensmeyer, V. I. Morariu, B. Price, S. Cohen, and T. Martinez.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDAR.2019.00027" title="" class="ltx_ref ltx_href">Deep splitting and
merging for table structure decomposition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and
Recognition (ICDAR)</em>, pages 114–121.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ¥L ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" title="" class="ltx_ref ltx_href">Attention
is all you need</a>.

</span>
<span class="ltx_bibblock">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 30</em>, pages 5998–6008. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venetis et al. (2011)</span>
<span class="ltx_bibblock">
Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen,
Fei Wu, Gengxin Miao, and Chung Wu. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14778/2002938.2002939" title="" class="ltx_ref ltx_href">Recovering
semantics of tables on the web</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 4(9):528–538.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. (2019)</span>
<span class="ltx_bibblock">
Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019.

</span>
<span class="ltx_bibblock">Do NLP models know numbers? probing numeracy in embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang.
2020.

</span>
<span class="ltx_bibblock">Structure-aware pre-training for table understanding with tree-based
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.12537.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2020)</span>
<span class="ltx_bibblock">
Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. 2020.

</span>
<span class="ltx_bibblock">TaBERT: Pretraining for joint understanding of textual and tabular
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Annual Conference of the Association for Computational
Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Dan Zhang, Yoshihiko Suhara, Jinfeng Li, Madelon Hulsebos, Çağatay
Demiralp, and Wang-Chiew Tan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1911.06311" title="" class="ltx_ref ltx_href">Sato: Contextual semantic
type detection in tables</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Balog (2017)</span>
<span class="ltx_bibblock">
Shuo Zhang and Krisztian Balog. 2017.

</span>
<span class="ltx_bibblock">Entitables: Smart assistance for entity-focused tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Balog (2020)</span>
<span class="ltx_bibblock">
Shuo Zhang and Krisztian Balog. 2020.

</span>
<span class="ltx_bibblock">Web table extraction, retrieval, and augmentation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Shuo Zhang, Edgar Meij, Krisztian Balog, and Ridho Reinanda. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3366423.3380205" title="" class="ltx_ref ltx_href">Novel entity
discovery from web tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of The Web Conference 2020</em>, WWW ’20, page
1298–1308, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2021)</span>
<span class="ltx_bibblock">
Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang.
2021.

</span>
<span class="ltx_bibblock">Global table extractor (gte): A framework for joint table
identification and cell structure recognition using visual context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV)</em>, pages 697–706.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.02583" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.02584" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.02584">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.02584" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.02585" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 09:54:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
