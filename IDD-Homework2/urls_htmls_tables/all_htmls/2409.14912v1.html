<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficient Tabular Data Preprocessing of ML Pipelines</title>
<!--Generated on Mon Sep 23 09:24:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14912v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S1" title="In Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2" title="In Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Motivation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS1" title="In 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Preprocessing for Tabular Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS2" title="In 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep Learning Recommender Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS3" title="In 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>CPU-based Preprocessing Pipelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS4" title="In 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Inefficiencies in CPU-based Data Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS5" title="In 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>GPU-based Data Preprocessing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3" title="In Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Piper</span>: Accelerated Data Preprocessing</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS1" title="In 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Accelerator Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS2" title="In 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span> High-Performance Processing Elements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS3" title="In 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Efficient Raw Dataset Transformation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS4" title="In 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>System Integration</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS4.SSS1" title="In 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span><span class="ltx_text ltx_font_smallcaps">Piper</span> as a Local Accelerator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS4.SSS2" title="In 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span><span class="ltx_text ltx_font_smallcaps">Piper</span> as a Network-attached Accelerator</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4" title="In Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS1" title="In 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS2" title="In 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Optimized CPU Baseline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS2.SSS1" title="In 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Optimizing Meta’s DLRM Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS2.SSS2" title="In 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Google’s DLRM Preprocessing in Cloud</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS3" title="In 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Preprocessing in GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4" title="In 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span><span class="ltx_text ltx_font_smallcaps">Piper</span>: Performance and Efficiency</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS1" title="In 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>End-to-end Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS2" title="In 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span><span class="ltx_text ltx_font_smallcaps">Piper</span> with Decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS3" title="In 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Maximizing Kernel Performance by Offloading Decoding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS4" title="In 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>Execution Time Breakdown</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS5" title="In 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.5 </span><span class="ltx_text ltx_font_smallcaps">Piper</span> with Network</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS6" title="In 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.6 </span>Throughput of Pure Computation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S5" title="In 4.4.6. Throughput of Pure Computation ‣ 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S6" title="In 5. Discussion ‣ 4.4.6. Throughput of Pure Computation ‣ 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S7" title="In 6. Related Work ‣ 5. Discussion ‣ 4.4.6. Throughput of Pure Computation ‣ 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\UseTblrLibrary</span>
<p class="ltx_p" id="p1.2">booktabs</p>
</div>
<h1 class="ltx_title ltx_title_document">Efficient Tabular Data Preprocessing of ML Pipelines</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yu Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Systems Group, Department of Computer Science</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">ETH Zurich, Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yu.zhu@inf.ethz.ch">yu.zhu@inf.ethz.ch</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenqi Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Systems Group, Department of Computer Science</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">ETH Zurich, Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wenqi.jiang@inf.ethz.ch">wenqi.jiang@inf.ethz.ch</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gustavo Alonso
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Systems Group, Department of Computer Science</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">ETH Zurich, Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:alonso@inf.ethz.ch">alonso@inf.ethz.ch</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id4.4">Data preprocessing pipelines, which includes data decoding, cleaning, and transforming, are a crucial component of Machine Learning (ML) training.
Thy are computationally intensive and often become a major bottleneck, due to the increasing performance gap between the CPUs used for preprocessing and the GPUs used for model training.
Recent studies show that a significant number of CPUs across several machines are required to achieve sufficient throughput to saturate the GPUs, leading to increased resource and energy consumption.
When the pipeline involves
vocabulary generation, the preprocessing performance scales poorly due to significant row-wise synchronization overhead between different CPU cores and servers.
To address this limitation, in this paper we present the design of <span class="ltx_text ltx_font_smallcaps" id="id4.4.1">Piper</span>, a hardware accelerator for tabular data preprocessing, prototype it on FPGAs, and demonstrate its potential for training pipelines of commercial recommender systems.
<span class="ltx_text ltx_font_smallcaps" id="id4.4.2">Piper</span> achieves 4.7 <math alttext="\sim" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">∼</annotation></semantics></math> 71.3<math alttext="\times" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">×</annotation></semantics></math> speedup in latency over a 128-core CPU server and outperforms a data-center GPU by 4.8<math alttext="\sim" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><csymbol cd="latexml" id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">∼</annotation></semantics></math> 20.3<math alttext="\times" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><mo id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><times id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">×</annotation></semantics></math> when using binary input.
The impressive performance showcases <span class="ltx_text ltx_font_smallcaps" id="id4.4.3">Piper</span>’s potential to increase the efficiency of data preprocessing pipelines and significantly reduce their resource consumption.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="S0.F1.g1" src="extracted/5872791/images/preprocess_train.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">Preprocessing vs training (one epoch, different batch sizes on one GPU).</span></figcaption>
</figure>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="S0.F2.g1" src="extracted/5872791/images/overall.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F2.4.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S0.F2.5.2" style="font-size:90%;">System overview for DLRM data preprocessing pipeline on CPUs and <span class="ltx_text ltx_font_smallcaps" id="S0.F2.5.2.1">Piper</span>, respectively. <span class="ltx_text ltx_font_smallcaps" id="S0.F2.5.2.2">Piper</span> supports both PCIe and the network as the data movement interface. The white blocks represent parallel workers.
</span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Data preprocessing is a critical step in machine learning (ML) training systems, significantly influencing the quality of the resulting models.
It aims to improve model accuracy and involves several key steps, such as data normalization, handling missing values, feature encoding, or data augmentation.
Current ML training systems employ a hybrid CPU-GPU architecture <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib80" title="">2020a</a>)</cite>, where the CPU handles data preprocessing before the data is transferred to the GPU for training, (upper part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S0.F2" title="Figure 2 ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">GPU performance has seen rapid advancements in recent years <cite class="ltx_cite ltx_citemacro_citep">(Dally et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib16" title="">2021</a>)</cite>, while CPU performance improvements have lagged behind. As a result,
data preprocessing is often a bottleneck in ML training systems due to the increasing performance gap between CPUs and GPUs, as recently discussed in commercial cloud deployments <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib82" title="">2023b</a>)</cite>.
To accelerate preprocessing on CPUs, frameworks such as <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">tf.data</span> <cite class="ltx_cite ltx_citemacro_citep">(Murray et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib53" title="">2021</a>)</cite> are employed, combined with various distributed and parallel processing techniques to improve throughput <cite class="ltx_cite ltx_citemacro_citep">(Audibert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib7" title="">2023</a>; Graur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib24" title="">2022</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib82" title="">2023b</a>)</cite>.
A common approach is to use several machines to process the data needed to feed a GPU <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>)</cite> which provides the required performance but it implies a huge increase in the resources needed and negatively impacts the over all efficiency and cost of the system. But even ignoring the issues of resource and energy inefficiency, a significant challenge remains: <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">how to efficiently handle stateful operators in the preprocessing pipeline?</span> The costly synchronizations of internal states across many CPU cores and servers often negate the benefits of adding additional CPU resources <cite class="ltx_cite ltx_citemacro_citep">(Fatourou and Kallimanis, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib21" title="">2012</a>; Herlihy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib33" title="">2020</a>; Hendler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib32" title="">2010</a>; David et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib17" title="">2013</a>)</cite>, presenting a challenge that is difficult to handle at the software level and cannot be solved by simply adding more CPU cores.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We have conducted a number of initial experiments to understand the gap between the CPU and the GPU. We have trained a model similar to that used by Meta <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>)</cite> with various batch sizes in Google Cloud (12 vCPUs, 64GB RAM, 16GB Nvidia V100 GPU). In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S0.F1" title="Figure 1 ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> we show the result of comparing the time it takes to train one epoch on the GPU for different batch sizes and the time it takes for the corresponding data preprocessing pipeline. The first observation is that we measure a maximum GPU Utilization of 40%, clearly indicating that the GPU is being infrautilized. The bottleneck is clearly visible in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S0.F1" title="Figure 1 ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> when comparing the time the CPU needs to process the input and the time if taking the GPU to train. Even when using large batches, the training is significantly faster that the input preprocessing.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">These results confirm those reported in by Meta <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>)</cite> and motivate us to try to accelerate the preprocessing stage with the goal if increasing the overall efficiency of the system. To this end, in this paper we propose <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">Piper</span>, a network-attached accelerator for efficient stateful data preprocessing (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S0.F2" title="Figure 2 ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">2</span></a>). <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.2">Piper</span> achieves high-performance and scalable data preprocessing through several novel ideas.
First, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.3">Piper</span> avoids costly explicit synchronization. <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.4">Piper</span> adopts a column-wise pipelined execution mechanism, and the heterogeneous hardware processing elements operate on different feature columns independently.
Second, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.5">Piper</span> achieves high memory bandwidth by utilizing not only on-chip SRAM but also fast off-chip High-Bandwidth Memory (HBM).
Third, we propose a novel parallel decoding mechanism and implement it on <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.6">Piper</span>, such that the accelerator efficiently decodes raw datasets.
Finally, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.7">Piper</span> can be directly attached to the network, such that (a) it can be easily integrated into existing ML training systems without the need to install FPGAs on training servers; (b) the FPGA is capable of processing datasets larger than its memory capacity in a streaming fashion; and (c) the number of preprocessing accelerators and training accelerators can be scaled independently.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We choose FPGAs as the hardware platform to prototype <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">Piper</span> since FPGAs are widely available from major cloud vendors, including AWS <cite class="ltx_cite ltx_citemacro_citep">(Amazon, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib4" title="">2024</a>)</cite>, Microsoft Azure <cite class="ltx_cite ltx_citemacro_citep">(Putnam, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib65" title="">2017</a>; Firestone et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib22" title="">2018</a>)</cite>, and Alibaba Cloud <cite class="ltx_cite ltx_citemacro_citep">(Li, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib46" title="">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib81" title="">2020b</a>)</cite>, facilitating the deployment of <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">Piper</span> in data centers.
Moreover, FPGAs offer greater architectural flexibility as, e.g., they can be used as smart NICs on training servers, loading raw data from the network and feeding the preprocessed data directly to GPUs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib76" title="">2022a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.8">We have evaluated <span class="ltx_text ltx_font_smallcaps" id="S1.p6.8.1">Piper</span> on production Deep Learning Recommender Models (DLRMs) from Meta and Google <cite class="ltx_cite ltx_citemacro_citep">(Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib54" title="">2019</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib27" title="">2020b</a>)</cite>. <span class="ltx_text ltx_font_smallcaps" id="S1.p6.8.2">Piper</span> outperforms a 128-core CPU server, achieving speedups between 4.7<math alttext="\sim" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><csymbol cd="latexml" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">∼</annotation></semantics></math>71.3<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mo id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><times id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">×</annotation></semantics></math> across various configurations. When processing the raw encoded datasets, <span class="ltx_text ltx_font_smallcaps" id="S1.p6.8.3">Piper</span> attains 5.1<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mo id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><times id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">×</annotation></semantics></math> and 4.7<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.4.m4.1"><semantics id="S1.p6.4.m4.1a"><mo id="S1.p6.4.m4.1.1" xref="S1.p6.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.4.m4.1b"><times id="S1.p6.4.m4.1.1.cmml" xref="S1.p6.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.4.m4.1d">×</annotation></semantics></math> speedup over the CPU when using on-chip SRAM and off-chip HBM, respectively. With decoded binary datasets as input, the performance gains with <span class="ltx_text ltx_font_smallcaps" id="S1.p6.8.4">Piper</span> rise to 71.3<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.5.m5.1"><semantics id="S1.p6.5.m5.1a"><mo id="S1.p6.5.m5.1.1" xref="S1.p6.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.5.m5.1b"><times id="S1.p6.5.m5.1.1.cmml" xref="S1.p6.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.5.m5.1d">×</annotation></semantics></math> and 25.7<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.6.m6.1"><semantics id="S1.p6.6.m6.1a"><mo id="S1.p6.6.m6.1.1" xref="S1.p6.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.6.m6.1b"><times id="S1.p6.6.m6.1.1.cmml" xref="S1.p6.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.6.m6.1d">×</annotation></semantics></math> when using on-chip and off-chip memory.
Compared to a GPU, especially dealing with binary format, <span class="ltx_text ltx_font_smallcaps" id="S1.p6.8.5">Piper</span> provides speedups ranging from 4.8<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.7.m7.1"><semantics id="S1.p6.7.m7.1a"><mo id="S1.p6.7.m7.1.1" xref="S1.p6.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.7.m7.1b"><times id="S1.p6.7.m7.1.1.cmml" xref="S1.p6.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.7.m7.1d">×</annotation></semantics></math> to 20.3<math alttext="\times" class="ltx_Math" display="inline" id="S1.p6.8.m8.1"><semantics id="S1.p6.8.m8.1a"><mo id="S1.p6.8.m8.1.1" xref="S1.p6.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p6.8.m8.1b"><times id="S1.p6.8.m8.1.1.cmml" xref="S1.p6.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.8.m8.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.8.m8.1d">×</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p7.1.1">Piper</span> makes the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We measure the performance of embedding generations during the overall ML training process, and analyze both CPU-optimized and GPU-accelerated solutions to establish a baseline for tabular data preprocessing.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We describe the design of <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i2.p1.1.1">Piper</span>, a hardware accelerator targeting stateful preprocessing data pipelines for ML that includes efficient novel data decoding units and specialized hardware units for various operators.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We integrate <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i3.p1.1.1">Piper</span> with a hardware network stack, facilitating its flexible and scalable deployments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We evaluate Piper on production Deep Learning Recommendation Models (DLRMs), representative ML models for recommender systems, showing that it can be used across different datasets and demonstrating its advantages over powerful server-level CPUs and GPUs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Motivation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Data preprocessing pipelines are fundamental for ensuring model quality in ML systems.
Such pipelines process raw data through multiple stages to transform it into a refined format suitable for model training.
Preprocessing tasks include, e.g., decoding, transforming different data types, normalization, and format conversions, which are crucial for capturing critical information.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Preprocessing for Tabular Dataset</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The input format for ML training varies for different applications, including tabular data, audio/images/videos, texts, graphs, etc.
The processing for tabular data often involves embedding generation, an efficient feature representation method, referring to the technique to represent high-dimensional, categorical, or structured data in a low-dimensional, continuous vector space, which helps capture the relationships and similarities among data points in the embedding space.
Various ML tasks have integrate embedding to help improve the model performance, for example, NLP-related models (RNNs <cite class="ltx_cite ltx_citemacro_citep">(Medsker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib50" title="">2001</a>)</cite>, LSTMs <cite class="ltx_cite ltx_citemacro_citep">(Graves and Graves, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib25" title="">2012</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib18" title="">2018</a>)</cite>, GPT <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib2" title="">2023</a>)</cite>) rely on embeddings to represent words or tokens as dense vectors to feed into the input layer.
Pre-trained word embeddings with limited vocabulary size, such as Word2Vec <cite class="ltx_cite ltx_citemacro_citep">(Church, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib14" title="">2017</a>)</cite>, GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib63" title="">2014</a>)</cite>, are popular and help developers directly map their texts into the corresponding embeddings and initialize training easily.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">For ML-based recommender systems, such as DLRM <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib27" title="">2020b</a>)</cite> , Wide and Deep <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib12" title="">2016</a>)</cite>, Neural Collaborative Filtering <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib30" title="">2017</a>)</cite>, Variational Autoencoder <cite class="ltx_cite ltx_citemacro_citep">(Kingma, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib38" title="">2013</a>)</cite>, BERT4REC <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib69" title="">2019</a>)</cite>, the scenario is a bit different.
These ML-based models share a similar data input format and embedding is a common technique to help transform non-trainable parameters into learnable representations.
The difference from NLP-based models lies in the non-unified embedding space, e.g., for attributes such as User-ID which leads to sparse embeddings requiring a much larger space to explore and is highly application dependent.
In such conditions, use cases in social or streaming media need to maintain tailored embedding spaces for independent training of models.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Deep Learning Recommender Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">DLRMs, widely used in recommender systems, are popular machine learning models that provide content recommendations based on the user’s personal preferences.
They are used in many areas, ranging from e-commerce <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib86" title="">2018</a>)</cite>, content streaming <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib85" title="">2019</a>)</cite>, as well as online advertising <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib26" title="">2020a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Recommender systems mainly rely on two primary categories of features: <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">dense features</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">sparse features</span>.
Dense features are predominantly non-zero or complete. Examples of attributes that lead to dense features are user age, item pricing, or a movie’s average rating. These features, often numerical, are generally normalized to ensure zero mean and unit variance or adjusted to fit a specific range, which helps ML algorithms converge.
Sparse features, typically categorical, are those with predominantly zero or absent values because they capture a domain not easily represented in a linear scale. For instance, features turn sparse when a vast dataset is one-hot encoded for user IDs or when textual content gets described as a bag of words. These features are then turned into binary vectors through embedding.
ML-based recommender systems need to handle both dense and sparse features, which poses a design challenge. Most of the work done on the data preprocessing pipelines involves generating the proper embeddings and data representations for the raw training data.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>CPU-based Preprocessing Pipelines</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In this paper, we use two representative examples from Meta <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib51" title="">2024</a>)</cite> and Google <cite class="ltx_cite ltx_citemacro_citep">(Tensorflow, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib72" title="">2024</a>)</cite> to illustrate the preprocessing stages used in practice and run on CPUs.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.T1" title="Table 1 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> lists the detailed functionalities of the involved operators.
Among them, <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">GenVocab</span> is responsible to create a vocabulary table for all columns of sparse features, and <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">ApplyVocab</span> then iterates anew over the dataset to generate the final embedding table.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="414" id="S2.F3.g1" src="extracted/5872791/images/cpuflow.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Dataflow of preprocessing pipelines in CPU.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">Meta’s DLRM pipeline</span>.
Meta <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib51" title="">2024</a>)</cite> released an open-source DLRM project as a benchmark for personalized recommendation models (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.F3" title="Figure 3 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">3</span></a>).
Aside from some common transformation operators, like <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.2">modulus, logarithm</span>, one particular step, the generation of vocabulary table, makes the pipeline <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.3">stateful</span> and introduces extra overhead for synchronization when employing multi-threading.
After retrieving data from the disk, the CPU sequentially processes the dataflow and writes the computed results back into memory or storage.
The input data format is typically encoded in UTF-8, consisting of both sparse and dense features.
To make it easier to understand the pipeline and the rest of the design we propose, we divide the process into four well-separated stages: <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.4">Split Input File, Generate Vocabulary, Apply Vocabulary &amp; Concatenate Final Results</span> (we merge some other data transformations into <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.5">Generate Vocabulary &amp; Apply Vocabulary</span> for simplicity).
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.F4" title="Figure 4 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4</span></a> shows an example of the inputs and outputs of the data preprocessing. The input file is encoded using UTF-8 with ASCII characters, while the output consists of the transformed features.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S2.F4.g1" src="extracted/5872791/images/utf8_row.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">An example of data preprocessing for a row of raw UTF-8 data, in which orange represents the labels, blue denotes tabs, green indicates dense features, and yellow denotes sparse features (8-byte hash values). </span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_italic" id="S2.SS3.p3.1.1">Split Input File (SIF)</span>. This step involves (1) reading the entire dataset from storage, counting the number of rows, and (2) partitioning the input equally as intermediate sub-files. The number of sub-files corresponds to the pre-defined number of threads.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS3.p4.1.1">Generate Vocabulary (GV)</span>. This step deals with the first part of processing individual sub-files and constructs embedding tables for the sparse features.
The program processes sub-files in parallel using multiple threads.
(3 &amp; 4) Each thread reads intermediate sub-files created during the SIF step and decodes the UTF-8 data into 32-bit width. The delimiter of the original dataset is <math alttext="\backslash t" class="ltx_Math" display="inline" id="S2.SS3.p4.1.m1.1"><semantics id="S2.SS3.p4.1.m1.1a"><mrow id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml"><mi id="S2.SS3.p4.1.m1.1.1.2" xref="S2.SS3.p4.1.m1.1.1.2.cmml"></mi><mo id="S2.SS3.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p4.1.m1.1.1.1.cmml">\</mo><mi id="S2.SS3.p4.1.m1.1.1.3" xref="S2.SS3.p4.1.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><apply id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1"><ci id="S2.SS3.p4.1.m1.1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1.1">\</ci><csymbol cd="latexml" id="S2.SS3.p4.1.m1.1.1.2.cmml" xref="S2.SS3.p4.1.m1.1.1.2">absent</csymbol><ci id="S2.SS3.p4.1.m1.1.1.3.cmml" xref="S2.SS3.p4.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.1.m1.1d">\ italic_t</annotation></semantics></math>, with default value 0 for empty entries, irrespective of whether the feature is sparse or dense.
(5) Original sparse features are hashed into hexadecimal values for security reasons. Thus, each thread has to convert them first to decimal values before processing.
(6) A positive modulus operation sets the range of sparse features to limit the size and determine the dimensionality of the embedding table.
(7) Each thread creates a sub-dictionary to collect the appearing sequence for each unique sparse feature and stores the partially processed data during the GV step into the disk for the following operations.
The program then synchronizes the threads and combines these sub-dictionaries for a unified embedding table.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_italic" id="S2.SS3.p5.1.1">Apply Vocabulary (AV)</span>. This step starts after generating the full vocabulary table, and the partially processed data from GV serves as input. Enabling multi-threading can also expedite this process.
(8) Each thread maps sparse features to their corresponding values in the shared vocabulary table.
(9) Each thread sets negative values of dense features as zero due to non-negativity constraints.
(10) Logarithm operation of dense features is optional, contributing to reducing skewness and scaling down large values.
Each thread then saves intermediate results to memory or disk.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1"><span class="ltx_text ltx_font_italic" id="S2.SS3.p6.1.1">Concatenate Final Results (CFR)</span>. (11 &amp; 12) In the final step, merging multiple intermediate results and consolidating them into a single file is necessary as ML models require complete rows as the input.
The default algorithm is the simple concatenation operation in sequence.</p>
</div>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p7.1.1">Google’s DLRM pipeline</span>.
Google <cite class="ltx_cite ltx_citemacro_citep">(Tensorflow, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib72" title="">2024</a>)</cite> open-sourced another DLRM project, where the pipeline is similar
and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.T1" title="Table 1 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> covers all operators in both pipelines.
An advantage of Google’s solution for preprocessing is its integration in Apache Beam, making it easier to use in cloud environments.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.2.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S2.T1.3.2" style="font-size:90%;">Preprocessing transformations available.</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.1"><span class="ltx_text" id="S2.T1.4.1.1.1.1" style="font-size:90%;">Op Name</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.2"><span class="ltx_text" id="S2.T1.4.1.1.2.1" style="font-size:90%;">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.2.1.1"><span class="ltx_text" id="S2.T1.4.2.1.1.1" style="font-size:90%;">Decode</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.2.1.2"><span class="ltx_text" id="S2.T1.4.2.1.2.1" style="font-size:90%;">Decode UTF-8 dataset for processing</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.4.3.2.1"><span class="ltx_text" id="S2.T1.4.3.2.1.1" style="font-size:90%;">FillMissing</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.3.2.2"><span class="ltx_text" id="S2.T1.4.3.2.2.1" style="font-size:90%;">Fill missing values for all features</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.4.4.3.1"><span class="ltx_text" id="S2.T1.4.4.3.1.1" style="font-size:90%;">Hex2Int</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.4.3.2"><span class="ltx_text" id="S2.T1.4.4.3.2.1" style="font-size:90%;">Convert hexadecimal values to decimal (sparse)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.4.5.4.1"><span class="ltx_text" id="S2.T1.4.5.4.1.1" style="font-size:90%;">Modulus</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.5.4.2"><span class="ltx_text" id="S2.T1.4.5.4.2.1" style="font-size:90%;">Compute positive modulus (sparse)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.6.5">
<td class="ltx_td ltx_align_left" id="S2.T1.4.6.5.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.6.5.1.1" style="font-size:90%;">GenVocab</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.6.5.2"><span class="ltx_text" id="S2.T1.4.6.5.2.1" style="font-size:90%;">Extract a set of unique IDs (sparse)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.7.6">
<td class="ltx_td ltx_align_left" id="S2.T1.4.7.6.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.7.6.1.1" style="font-size:90%;">ApplyVocab</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.7.6.2"><span class="ltx_text" id="S2.T1.4.7.6.2.1" style="font-size:90%;">Generate integer-encoded mappings (sparse)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.8.7">
<td class="ltx_td ltx_align_left" id="S2.T1.4.8.7.1"><span class="ltx_text" id="S2.T1.4.8.7.1.1" style="font-size:90%;">Neg2Zero</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.8.7.2"><span class="ltx_text" id="S2.T1.4.8.7.2.1" style="font-size:90%;">Change negative values to zero (dense)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.9.8">
<td class="ltx_td ltx_align_left" id="S2.T1.4.9.8.1"><span class="ltx_text" id="S2.T1.4.9.8.1.1" style="font-size:90%;">Logarithm</span></td>
<td class="ltx_td ltx_align_left" id="S2.T1.4.9.8.2"><span class="ltx_text" id="S2.T1.4.9.8.2.1" style="font-size:90%;">Do log(x+1) operation (dense)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.10.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.4.10.9.1"><span class="ltx_text" id="S2.T1.4.10.9.1.1" style="font-size:90%;">Concatenate</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.4.10.9.2"><span class="ltx_text" id="S2.T1.4.10.9.2.1" style="font-size:90%;">Concatenate final results from multiple threads</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Inefficiencies in CPU-based Data Preprocessing</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">As reported by Meta <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>)</cite>, data preprocessing is a major bottleneck in production DLRM systems, leading to significant GPUs under-utilization.
Meta uses the DSI (Data Storage &amp; Ingestion) pipeline to produce data for training, which consists of <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.1">offline data generation, dataset storage</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.1.2">online preprocessing services</span>.
Such pipeline is conceptually similar to that used in databases for Extract, Transform, and Load (ETL) operations <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib70" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib79" title="">2015</a>; Raman and Hellerstein, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib66" title="">2001</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib47" title="">2012</a>)</cite>.
In the context of DLRMs, the large scale of the models involved, the need to frequently update them, and the fast-evolving features of the model result in a massive amount of data that has to be preprocessed and fed to the domain-specific accelerators.
In their experiments, they run a training job as the baseline on a two-socket, 28-core CPU machine for preprocessing, two 100 Gbps NICs for data ingestion, and 8 Nvidia V100 GPUs for training.
They observe that almost 56% of the GPU cycles are wasted while waiting for training data despite the CPUs operating at 92% utilization.
Alibaba <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib82" title="">2023b</a>)</cite> has also reported similar inefficiencies in their data centers as a result of the CPU-GPU performance mismatch.
Given the growth in dataset sizes and the need to retrain models on a regular basis, the CPU bottleneck will become even more prominent, especially considering that the performance of GPUs is evolving much faster than that of CPUs <cite class="ltx_cite ltx_citemacro_citep">(Choquette, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib13" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Meta addresses the GPU resource underutilization problem by disaggregating and scaling out the data preprocessing tasks across many servers <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib83" title="">2022</a>)</cite>. However, this scale-out strategy is not efficient for two reasons.
First, while the performance improves by using more CPU servers, it also results in a much higher resource and energy consumption.
Second, the performance of stateful row-based multi-processing does not scale linearly with the amount of CPU resources, as we will show in the experiments.
This is due to the high synchronization overheads between the CPU threads of different processing stages: after each thread processes rows of data, a costly synchronization step must be used to exchange internal states to form the unified embedding table for single column.
</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p3.1.1">Choice of multi-processing</span>.
The default data partitioning method in CPU-based preprocessing pipelines typically divides the dataset into chunks of rows, with each thread handling a portion of these rows using the same operations. This row-wise processing approach supports varying numbers of CPU threads and aligns well with the row-wise input format commonly required for machine learning training.
However, it comes with the downside of necessary synchronization overhead, which can affect performance.
Alternatively, column-wise multiprocessing assigns each thread to handle independent columns, which can reduce the execution time for processing each column. This method is advantageous for tasks where column-level operations dominate. However, a key limitation is that the number of CPU threads must match the number of columns, which can be restrictive.
Another disadvantage is the unbalanced workload for different columns.
Additionally, since most ML models require row-wise input, column-wise processing necessitates the concatenation of all columns back into rows, potentially leading to I/O bottlenecks.
The choice between these two multiprocessing methods depends on several factors, including the hardware platform, the structure of the dataset, and the specific requirements of the processing pipeline.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5. </span>GPU-based Data Preprocessing</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">GPU-accelerated preprocessing is an appealing approach to avoid having to move the data from the CPU to the GPU. For instance, Nvidia’s Data Loading Library (DALI) <cite class="ltx_cite ltx_citemacro_citep">(Nvidia, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib56" title="">2024a</a>)</cite> is used for image preprocessing.
The acceleration for recommender systems in GPU is also possible based on Nvidia RAPIDS suite <cite class="ltx_cite ltx_citemacro_citep">(Nvidia, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib57" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib58" title="">c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib59" title="">d</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">The acceleration of GPU also benefits from column-wise processing. For example, Parquet <cite class="ltx_cite ltx_citemacro_citep">(Vohra and Vohra, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib75" title="">2016</a>)</cite>, a popular columnar storage file format, allows GPU to process columns independently among Streaming Multiprocessors (SMs) and maximize row-level parallelism by CUDA cores within an SM. This method is a combination of row-wise and column-wise multi-processing. We will compare our design against these methods in the experimental analysis.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Piper</span>: Accelerated Data Preprocessing</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">Piper</span>, a pure column-wise high-performance accelerator for tabular data preprocessing pipelines.
We propose multiple techniques to optimize accelerator performance, including transforming raw datasets in parallel, broadcast-gather processing element (PE) design, high-bandwidth memory (HBM) as a cache, and reducing data movement via a direct network interface.
Other intrinsic factors contributing to the high performance of <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">Piper</span> involve pipelined processing and low synchronization overhead.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="647" id="S3.F5.g1" src="extracted/5872791/images/fpgaflow.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.3.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text ltx_font_smallcaps" id="S3.F5.4.2" style="font-size:90%;">Piper<span class="ltx_text ltx_font_upright" id="S3.F5.4.2.1"> accelerator overview. The dataflow involves two consecutive loops ① &amp; ②. We use the same color of blocks as in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.F3" title="Figure 3 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">3</span></a> to represent different types of operators.</span></span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Accelerator Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Accelerator components.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F5" title="Figure 5 ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">5</span></a> shows the accelerator overview of <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">Piper</span>.
The accelerator consists of various specialized hardware Processing Elements (PEs), including <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">LoadData, Decode, Neg2Zero, Logarithm, Hex2Int, Modulus, GenVocab, ApplyVocab &amp; StoreData.</span>
The performance of each processing stage can be controlled via instantiating multiple PEs.
The accelerator can work as either a local accelerator, loading data from and storing results to local DRAM, or as a network-attached accelerator using the FPGA TCP/IP stack.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Processing control flow.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F5" title="Figure 5 ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">5</span></a> shows the dataflow with the operators in the FPGA and the slight adjustment of the original sequence of operators, as we can merge some of them to simplify the overall dataflow.
Once the dataset is decoded, the data preprocessing is conducted via two consecutive loops.
In the first loop, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.2">Piper</span> reads the whole dataset and generates the corresponding vocabulary table. The size of vocabulary determines whether it is stored in on-chip SRAM or off-chip HBM, which significantly influences the overall performance due to random memory accesses.
In the second loop, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p2.1.3">Piper</span> rereads the dataset and maps each feature into the corresponding value in the vocabulary table.
<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">GenVocab</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.5">ApplyVocab</span> behave differently in the 1st and 2nd loops, while other PEs behave the same to process input features.
For <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.6">GenVocab</span>, it filters some unique inputs in the first loop and passes all inputs in the second loop.
For <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.7">ApplyVocab</span>, it writes the appearing sequence of unique inputs into memory in the first loop and reads corresponding values in the second loop.
Some operators listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.T1" title="Table 1 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> are missing in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F5" title="Figure 5 ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">5</span></a>, like <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.8">FillMissing &amp; Hex2Int</span>, because the FPGA handles bits directly and there is no need for representing <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.9">Null</span> as in software, so the default value for the empty element after <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.10">Decode</span> is 0, and there is no need to transform from hexadecimal to decimal explicitly.
Each PE is a computing unit on the FPGA, and different PEs are interconnected via FIFO channels.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Differences between CPU and <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p3.1.1.1">Piper</span></span>.
In multi-threaded CPU implementations, the dataset is partitioned by rows. Each thread handles a small portion of the entire dataset, and synchronization is required once all threads have completed their tasks, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.F3" title="Figure 3 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">3</span></a>.
In contrast, the FPGA is a spatial dataflow processor with heterogeneous hardware processing elements (PEs) that can process columns of data in parallel.
Instead of partitioning data by rows, an FPGA can efficiently process raw data in a column-wise fashion.
Here a dataflow in FPGA corresponds to the concept of SM in GPU.
This is because the heterogeneous hardware PEs are specialized for processing each feature column, allowing them to handle different columns with consistent throughput.
Besides, the output of processing different features in one FPGA can aggregate together easily, which helps store the processed dataset in the row-wise manner.
This is not easy on the CPU due to the homogeneous nature of CPU cores. Thus, compared to CPUs, the FPGA not only includes specialized high-performance PEs but also eliminates the need for synchronization required in CPU-based solutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span> High-Performance Processing Elements</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We now introduce the high-performance Processing Elements (PEs) instantiated on <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">Piper</span> in detail.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">LoadData</span>. This PE aims to load the dataset from either FPGA’s off-chip memory or the network. If the data is loaded from the memory, the bandwidth is determined by the width of the memory interface and the number of memory channels. The achieved <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Initialization Interval (II)</span>, which defines the minimum number of clock cycles required between successive launches of operations in a pipelined design, is as low as one clock cycle.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="610" id="S3.F6.g1" src="extracted/5872791/images/utf8.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Flow chart to decode UTF-8.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.13"><span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.7">Decode</span>. This PE aims to decode the input UTF-8 data and convert them to various features used for the model, and is one of the accelerator’s bottlenecks that we will discuss later.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F6" title="Figure 6 ‣ 3.2. High-Performance Processing Elements ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">6</span></a> shows a rough dataflow of implementing <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.8">Decode</span> in FPGA.
The valid input memory port width is one byte, and the achieved II is one cycle.
For the dataset of DLRM, five kinds of ASCII values are possible: <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.4.4">horizontal tab <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS2.p3.1.1.m1.1"><semantics id="S3.SS2.p3.1.1.m1.1a"><mrow id="S3.SS2.p3.1.1.m1.1.1" xref="S3.SS2.p3.1.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.1.m1.1.1.2" xref="S3.SS2.p3.1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p3.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.1.1.m1.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.1.1.m1.1.1.3" xref="S3.SS2.p3.1.1.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.1.m1.1b"><apply id="S3.SS2.p3.1.1.m1.1.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1"><ci id="S3.SS2.p3.1.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.1.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.1.m1.1.1.2">absent</csymbol><ci id="S3.SS2.p3.1.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.1.m1.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.1.m1.1d">\ italic_t</annotation></semantics></math>, new line <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS2.p3.2.2.m2.1"><semantics id="S3.SS2.p3.2.2.m2.1a"><mrow id="S3.SS2.p3.2.2.m2.1.1" xref="S3.SS2.p3.2.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.2.m2.1.1.2" xref="S3.SS2.p3.2.2.m2.1.1.2.cmml"></mi><mo id="S3.SS2.p3.2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.2.2.m2.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.2.2.m2.1.1.3" xref="S3.SS2.p3.2.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.2.m2.1b"><apply id="S3.SS2.p3.2.2.m2.1.1.cmml" xref="S3.SS2.p3.2.2.m2.1.1"><ci id="S3.SS2.p3.2.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.2.m2.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.2.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.2.m2.1.1.2">absent</csymbol><ci id="S3.SS2.p3.2.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.2.m2.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.2.m2.1d">\ italic_n</annotation></semantics></math>, minus sign <math alttext="-" class="ltx_Math" display="inline" id="S3.SS2.p3.3.3.m3.1"><semantics id="S3.SS2.p3.3.3.m3.1a"><mo id="S3.SS2.p3.3.3.m3.1.1" xref="S3.SS2.p3.3.3.m3.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.3.m3.1b"><minus id="S3.SS2.p3.3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.3.m3.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.3.m3.1c">-</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.3.m3.1d">-</annotation></semantics></math>, 0<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p3.4.4.m4.1"><semantics id="S3.SS2.p3.4.4.m4.1a"><mo id="S3.SS2.p3.4.4.m4.1.1" xref="S3.SS2.p3.4.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.4.m4.1b"><csymbol cd="latexml" id="S3.SS2.p3.4.4.m4.1.1.cmml" xref="S3.SS2.p3.4.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.4.m4.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.4.m4.1d">∼</annotation></semantics></math>9</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.5.5">a<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p3.5.5.m1.1"><semantics id="S3.SS2.p3.5.5.m1.1a"><mo id="S3.SS2.p3.5.5.m1.1.1" xref="S3.SS2.p3.5.5.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.5.m1.1b"><csymbol cd="latexml" id="S3.SS2.p3.5.5.m1.1.1.cmml" xref="S3.SS2.p3.5.5.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.5.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.5.m1.1d">∼</annotation></semantics></math>f</span>.
<span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p3.13.9">Piper</span> use <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m1.1"><semantics id="S3.SS2.p3.6.m1.1a"><mrow id="S3.SS2.p3.6.m1.1.1" xref="S3.SS2.p3.6.m1.1.1.cmml"><mi id="S3.SS2.p3.6.m1.1.1.2" xref="S3.SS2.p3.6.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p3.6.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.6.m1.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.6.m1.1.1.3" xref="S3.SS2.p3.6.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m1.1b"><apply id="S3.SS2.p3.6.m1.1.1.cmml" xref="S3.SS2.p3.6.m1.1.1"><ci id="S3.SS2.p3.6.m1.1.1.1.cmml" xref="S3.SS2.p3.6.m1.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.6.m1.1.1.2.cmml" xref="S3.SS2.p3.6.m1.1.1.2">absent</csymbol><ci id="S3.SS2.p3.6.m1.1.1.3.cmml" xref="S3.SS2.p3.6.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m1.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m1.1d">\ italic_t</annotation></semantics></math> and <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m2.1"><semantics id="S3.SS2.p3.7.m2.1a"><mrow id="S3.SS2.p3.7.m2.1.1" xref="S3.SS2.p3.7.m2.1.1.cmml"><mi id="S3.SS2.p3.7.m2.1.1.2" xref="S3.SS2.p3.7.m2.1.1.2.cmml"></mi><mo id="S3.SS2.p3.7.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.7.m2.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.7.m2.1.1.3" xref="S3.SS2.p3.7.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m2.1b"><apply id="S3.SS2.p3.7.m2.1.1.cmml" xref="S3.SS2.p3.7.m2.1.1"><ci id="S3.SS2.p3.7.m2.1.1.1.cmml" xref="S3.SS2.p3.7.m2.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.7.m2.1.1.2.cmml" xref="S3.SS2.p3.7.m2.1.1.2">absent</csymbol><ci id="S3.SS2.p3.7.m2.1.1.3.cmml" xref="S3.SS2.p3.7.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m2.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m2.1d">\ italic_n</annotation></semantics></math> as delimiters: <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m3.1"><semantics id="S3.SS2.p3.8.m3.1a"><mrow id="S3.SS2.p3.8.m3.1.1" xref="S3.SS2.p3.8.m3.1.1.cmml"><mi id="S3.SS2.p3.8.m3.1.1.2" xref="S3.SS2.p3.8.m3.1.1.2.cmml"></mi><mo id="S3.SS2.p3.8.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.8.m3.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.8.m3.1.1.3" xref="S3.SS2.p3.8.m3.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m3.1b"><apply id="S3.SS2.p3.8.m3.1.1.cmml" xref="S3.SS2.p3.8.m3.1.1"><ci id="S3.SS2.p3.8.m3.1.1.1.cmml" xref="S3.SS2.p3.8.m3.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.8.m3.1.1.2.cmml" xref="S3.SS2.p3.8.m3.1.1.2">absent</csymbol><ci id="S3.SS2.p3.8.m3.1.1.3.cmml" xref="S3.SS2.p3.8.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m3.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m3.1d">\ italic_t</annotation></semantics></math> to split features and <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m4.1"><semantics id="S3.SS2.p3.9.m4.1a"><mrow id="S3.SS2.p3.9.m4.1.1" xref="S3.SS2.p3.9.m4.1.1.cmml"><mi id="S3.SS2.p3.9.m4.1.1.2" xref="S3.SS2.p3.9.m4.1.1.2.cmml"></mi><mo id="S3.SS2.p3.9.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.9.m4.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.9.m4.1.1.3" xref="S3.SS2.p3.9.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m4.1b"><apply id="S3.SS2.p3.9.m4.1.1.cmml" xref="S3.SS2.p3.9.m4.1.1"><ci id="S3.SS2.p3.9.m4.1.1.1.cmml" xref="S3.SS2.p3.9.m4.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.9.m4.1.1.2.cmml" xref="S3.SS2.p3.9.m4.1.1.2">absent</csymbol><ci id="S3.SS2.p3.9.m4.1.1.3.cmml" xref="S3.SS2.p3.9.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m4.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m4.1d">\ italic_n</annotation></semantics></math> to denote the end of the row.
It creates a boolean value <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.10">negative_flag</span> to represent <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.11">minus sign</span> and decode ASCII values of <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.10.6">0<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p3.10.6.m1.1"><semantics id="S3.SS2.p3.10.6.m1.1a"><mo id="S3.SS2.p3.10.6.m1.1.1" xref="S3.SS2.p3.10.6.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.6.m1.1b"><csymbol cd="latexml" id="S3.SS2.p3.10.6.m1.1.1.cmml" xref="S3.SS2.p3.10.6.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.6.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.10.6.m1.1d">∼</annotation></semantics></math>f</span> to corresponding hexadecimal bits.
Then, it keeps track of the input character in a 32-bit register to transform the combination of characters <math alttext="0\sim f" class="ltx_Math" display="inline" id="S3.SS2.p3.11.m5.1"><semantics id="S3.SS2.p3.11.m5.1a"><mrow id="S3.SS2.p3.11.m5.1.1" xref="S3.SS2.p3.11.m5.1.1.cmml"><mn id="S3.SS2.p3.11.m5.1.1.2" xref="S3.SS2.p3.11.m5.1.1.2.cmml">0</mn><mo id="S3.SS2.p3.11.m5.1.1.1" xref="S3.SS2.p3.11.m5.1.1.1.cmml">∼</mo><mi id="S3.SS2.p3.11.m5.1.1.3" xref="S3.SS2.p3.11.m5.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m5.1b"><apply id="S3.SS2.p3.11.m5.1.1.cmml" xref="S3.SS2.p3.11.m5.1.1"><csymbol cd="latexml" id="S3.SS2.p3.11.m5.1.1.1.cmml" xref="S3.SS2.p3.11.m5.1.1.1">similar-to</csymbol><cn id="S3.SS2.p3.11.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p3.11.m5.1.1.2">0</cn><ci id="S3.SS2.p3.11.m5.1.1.3.cmml" xref="S3.SS2.p3.11.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m5.1c">0\sim f</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.11.m5.1d">0 ∼ italic_f</annotation></semantics></math> to expected values.
(a) For dense features (decimal values), it multiplies with ten and add the current input.
Due to the existence of negative values, it keeps the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.12">negative_flag</span> to denote whether the current value should be negative or not and regard the following binary characters as positive in the register.
(b) For sparse features (hexadecimal values), it shifts the cached register 4 bits left every cycle and add the current input.
Sparse features are always positive, and it omits <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.13">negative_flag</span> for them.
(c) When reaching delimiters <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS2.p3.12.m6.1"><semantics id="S3.SS2.p3.12.m6.1a"><mrow id="S3.SS2.p3.12.m6.1.1" xref="S3.SS2.p3.12.m6.1.1.cmml"><mi id="S3.SS2.p3.12.m6.1.1.2" xref="S3.SS2.p3.12.m6.1.1.2.cmml"></mi><mo id="S3.SS2.p3.12.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.12.m6.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.12.m6.1.1.3" xref="S3.SS2.p3.12.m6.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m6.1b"><apply id="S3.SS2.p3.12.m6.1.1.cmml" xref="S3.SS2.p3.12.m6.1.1"><ci id="S3.SS2.p3.12.m6.1.1.1.cmml" xref="S3.SS2.p3.12.m6.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.12.m6.1.1.2.cmml" xref="S3.SS2.p3.12.m6.1.1.2">absent</csymbol><ci id="S3.SS2.p3.12.m6.1.1.3.cmml" xref="S3.SS2.p3.12.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m6.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.12.m6.1d">\ italic_t</annotation></semantics></math> or <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS2.p3.13.m7.1"><semantics id="S3.SS2.p3.13.m7.1a"><mrow id="S3.SS2.p3.13.m7.1.1" xref="S3.SS2.p3.13.m7.1.1.cmml"><mi id="S3.SS2.p3.13.m7.1.1.2" xref="S3.SS2.p3.13.m7.1.1.2.cmml"></mi><mo id="S3.SS2.p3.13.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.13.m7.1.1.1.cmml">\</mo><mi id="S3.SS2.p3.13.m7.1.1.3" xref="S3.SS2.p3.13.m7.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m7.1b"><apply id="S3.SS2.p3.13.m7.1.1.cmml" xref="S3.SS2.p3.13.m7.1.1"><ci id="S3.SS2.p3.13.m7.1.1.1.cmml" xref="S3.SS2.p3.13.m7.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS2.p3.13.m7.1.1.2.cmml" xref="S3.SS2.p3.13.m7.1.1.2">absent</csymbol><ci id="S3.SS2.p3.13.m7.1.1.3.cmml" xref="S3.SS2.p3.13.m7.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m7.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.13.m7.1d">\ italic_n</annotation></semantics></math>, it extracts the current value in the register.
If <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.14">negative_flag</span> is true, it transforms the value to the corresponding <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.13.15">two’s complement</span>; otherwise, it outputs the value directly.
It transfer the final output to downstream modules and reset the register to zero for the subsequent decoding.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F6" title="Figure 6 ‣ 3.2. High-Performance Processing Elements ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">6</span></a> presents the flow chart of decoding UTF-8 in FPGA.
This is a common solution, as it can explicitly separate the processing of decimal and hexadecimal values, and what we should know in advance is the data format for each feature.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.1">Neg2Zero</span>.
This is a ternary operator. It sets the negative input dense feature to 0, otherwise it keeps the original value as output.
The achieved II is one cycle.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">Logarithm</span>. We calculate the logarithm using the default operator. The achieved II is one cycle.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p6.1.1">Modulus</span>. We use the modulus operator to limit the range of sparse features. The achieved II is one cycle.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.1">GenVocab-1</span>. This PE aims to extract unique values for inputs. We keep a bitmap in BRAM/URAM and pass unique inputs to downstream modules. The achieved II is two cycles.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p8.1.1">GenVocab-2</span>. We pass inputs to downstream modules directly.
The achieved II is two cycles because the performance is limited by the PEs for <span class="ltx_text ltx_font_italic" id="S3.SS2.p8.1.2">GenVocab-1</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p9.1.1">ApplyVocab-1</span>. This PE aims to count the appearing sequence for unique inputs from upstream modules. We keep a counter and write the current counter value into the corresponding position in the vocabulary table. The achieved II is two cycles for a small vocabulary table in on-chip RAM and is about 15 cycles for a large vocabulary table in off-chip HBM due to random memory write.</p>
</div>
<div class="ltx_para" id="S3.SS2.p10">
<p class="ltx_p" id="S3.SS2.p10.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p10.1.1">ApplyVocab-2</span>. This PE aims to assign values to all sparse features. We fetch the corresponding value from the vocabulary table for each input feature. The achieved II is two cycles for a small vocabulary table in on-chip RAM and is about 15 cycles for a large vocabulary table due to random memory read.</p>
</div>
<div class="ltx_para" id="S3.SS2.p11">
<p class="ltx_p" id="S3.SS2.p11.1"><span class="ltx_text ltx_font_italic" id="S3.SS2.p11.1.1">StoreData</span>. We combine the results from different dataflows and write them back to FPGA’s off-chip memory or to the network. The achieved II is one cycle.</p>
</div>
<div class="ltx_para" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.1">We implement all aforementioned operators in the high-level modular design which allows the easy connection in the pipeline and potentially add/remove/change some operators to meet the new requirement.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Efficient Raw Dataset Transformation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Data format transformation from UTF-8 to binary, like Parquet <cite class="ltx_cite ltx_citemacro_citep">(Vohra and Vohra, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib75" title="">2016</a>)</cite> or TFRecord <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib23" title="">2024</a>)</cite>, is not free, no matter in CPU or in GPU.
A straightforward implementation of the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">Decode</span> PE in FPGA would become the system bottleneck and result in low accelerator performance.
The memory interface width on FPGA is up to 512 bits, which retrieves 64 bytes of data per cycle. As FPGA runs different modules in the pipeline, the operator with the largest II determines the performance of the entire dataflow.
The achieved II for a simple <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">Decode</span> PE in FPGA is one cycle, but the effective throughput is very low as <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">Decode</span> can only process one byte per cycle.
The theoretical throughput of one DDR channel is 19GB/s (512-bit wide memory lane, 300MHz), but decoding data per byte is 64 times slower and limits the valid throughput to 300MB/s.
Each row of the input UTF-8 encoded data comprises hundreds of bytes and <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.1.4">Piper</span> takes the same number of cycles to read them.
During this process, downstream operations for both sparse and dense features must wait and can not run in parallel because they compete for the input.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.5">To mitigate this bottleneck, we propose a high-performance parallel UTF-8 decoding unit to improve the overall throughput of the accelerator.
Here, we make some assumptions for the simplified description. Firstly, we regard <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"></mi><mo id="S3.SS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.1.m1.1.1.1.cmml">\</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><ci id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">absent</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">\ italic_t</annotation></semantics></math> and <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"></mi><mo id="S3.SS3.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.1.cmml">\</mo><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><ci id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">absent</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">\ italic_n</annotation></semantics></math> the same as they both serve as delimiters.
Secondly, we ignore the <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.2">minus sign</span> because it only requires to create a <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.3">negative_flag</span> to represent the sign of decimal values and calculate the <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.4">two’s complement</span> for output.
Thirdly, we only consider the condition for hexadecimal values because we can transform from hexadecimal values to the original integers easily.
Finally, we split the decoding process into multiple modules to make the entire structure more straightforward:
(a) the upstream module serves to map ASCII values to <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"></mi><mo id="S3.SS3.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.3.m3.1.1.1.cmml">\</mo><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><ci id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">absent</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">\ italic_t</annotation></semantics></math>, <math alttext="\backslash n" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"></mi><mo id="S3.SS3.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.4.m4.1.1.1.cmml">\</mo><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><ci id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">absent</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\backslash n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">\ italic_n</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.5">-</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.1">0<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS3.p2.5.1.m1.1"><semantics id="S3.SS3.p2.5.1.m1.1a"><mo id="S3.SS3.p2.5.1.m1.1.1" xref="S3.SS3.p2.5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.1.m1.1b"><csymbol cd="latexml" id="S3.SS3.p2.5.1.m1.1.1.cmml" xref="S3.SS3.p2.5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.1.m1.1d">∼</annotation></semantics></math>f</span>;
(b) the downstream module is a state machine that extracts valid 32-bit outputs from the 128-bit wide input stream (the number of valid outputs ranges from 0 to 4).</p>
</div>
<figure class="ltx_float ltx_lstlisting" id="LST1">
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="LST1.1" style="border-color: #000000;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,MGIxMTExOiAkb18wJCA9IHY7ICRvXzEkID0gMDsgJG9fMiQgPSAwOyAkb18zJCA9IDA7IHYgPSAwOwoKMGIxMTEwOiAkb18wJCA9IHY7ICRvXzEkID0gMDsgJG9fMiQgPSAwOyB2ID0gJHNfMyQ7CjBiMTEwMTogJG9fMCQgPSB2OyAkb18xJCA9IDA7ICRvXzIkID0gJHNfMiQ7IHYgPSAwOwowYjEwMTE6ICRvXzAkID0gdjsgJG9fMSQgPSAkc18xJDsgJG9fMiQgPSAwOyB2ID0gMDsKMGIwMTExOiAkb18wJCA9IHYgPDwgNCArICRzXzAkOyAkb18xJCA9IDA7ICRvXzIkID0gMDsgdiA9IDA7CgowYjExMDA6ICRvXzAkID0gdjsgJG9fMSQgPSAwOyB2ID0gJHNfMiQgPDwgNCArICRzXzMkOwowYjEwMTA6ICRvXzAkID0gdjsgJG9fMSQgPSAkc18xJDsgdiA9ICRzXzMkOwowYjEwMDE6ICRvXzAkID0gdjsgJG9fMSQgPSAkc18yJCA8PCA0ICsgJHNfMyQ7IHYgPSAwOwowYjAxMTA6ICRvXzAkID0gdiA8PCA0ICsgJHNfMCQ7ICRvXzEkID0gMDsgdiA9ICRzXzMkOwowYjAxMDE6ICRvXzAkID0gdiA8PCA0ICsgJHNfMCQ7ICRvXzEkID0gJHNfMiQ7IHYgPSAwOwowYjAwMTE6ICRvXzAkID0gdiA8PCA4ICsgJHNfMCQgPDwgNCArICRzXzEkOyAkb18yJCA9IDA7IHYgPSAwOwoKMGIxMDAwOiAkb18wJCA9IDA7IHYgPSAkc18xJCA8PCA4ICsgJHNfMiQgPDwgNCArICRzXzMkOwowYjAxMDA6ICRvXzAkID0gdiA8PCA0ICsgJHNfMCQ7IHYgPSAkc18yJCA8PCA0ICsgJHNfMyQ7CjBiMDAxMDogJG9fMCQgPSB2IDw8IDggKyAkc18wJCA8PCA0ICsgJHNfMSQ7IHYgPSAkc18zJDsKMGIwMDAxOiAkb18wJCA9IHYgPDwgMTIgKyAkc18wJCA8PCA4ICsgJHNfMSQgPDwgNCArICRzXzIkOyB2ID0gMDsKCjBiMDAwMDogdiA9IHYgPDwgMTYgKyAkc18wJCA8PCAxMiArICRzXzEkIDw8IDggKyAkc18yJCA8PCA0ICsgJHNfMyQ7">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx1.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx1.6.1" style="color:#000000;">b1111</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx1.1.m1.1"><semantics id="lstnumberx1.1.m1.1a"><msub id="lstnumberx1.1.m1.1.1" xref="lstnumberx1.1.m1.1.1.cmml"><mi id="lstnumberx1.1.m1.1.1.2" xref="lstnumberx1.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx1.1.m1.1.1.3" xref="lstnumberx1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx1.1.m1.1b"><apply id="lstnumberx1.1.m1.1.1.cmml" xref="lstnumberx1.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx1.1.m1.1.1.1.cmml" xref="lstnumberx1.1.m1.1.1">subscript</csymbol><ci id="lstnumberx1.1.m1.1.1.2.cmml" xref="lstnumberx1.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx1.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx1.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx1.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx1.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx1.2.m1.1"><semantics id="lstnumberx1.2.m1.1a"><msub id="lstnumberx1.2.m1.1.1" xref="lstnumberx1.2.m1.1.1.cmml"><mi id="lstnumberx1.2.m1.1.1.2" xref="lstnumberx1.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx1.2.m1.1.1.3" xref="lstnumberx1.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx1.2.m1.1b"><apply id="lstnumberx1.2.m1.1.1.cmml" xref="lstnumberx1.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx1.2.m1.1.1.1.cmml" xref="lstnumberx1.2.m1.1.1">subscript</csymbol><ci id="lstnumberx1.2.m1.1.1.2.cmml" xref="lstnumberx1.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx1.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx1.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx1.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx1.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.18" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.3" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx1.3.m1.1"><semantics id="lstnumberx1.3.m1.1a"><msub id="lstnumberx1.3.m1.1.1" xref="lstnumberx1.3.m1.1.1.cmml"><mi id="lstnumberx1.3.m1.1.1.2" xref="lstnumberx1.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx1.3.m1.1.1.3" xref="lstnumberx1.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx1.3.m1.1b"><apply id="lstnumberx1.3.m1.1.1.cmml" xref="lstnumberx1.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx1.3.m1.1.1.1.cmml" xref="lstnumberx1.3.m1.1.1">subscript</csymbol><ci id="lstnumberx1.3.m1.1.1.2.cmml" xref="lstnumberx1.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx1.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx1.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx1.3.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx1.3.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.21" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.23" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.4" style="font-size:70%;"><math alttext="o_{3}" class="ltx_Math" display="inline" id="lstnumberx1.4.m1.1"><semantics id="lstnumberx1.4.m1.1a"><msub id="lstnumberx1.4.m1.1.1" xref="lstnumberx1.4.m1.1.1.cmml"><mi id="lstnumberx1.4.m1.1.1.2" xref="lstnumberx1.4.m1.1.1.2.cmml">o</mi><mn id="lstnumberx1.4.m1.1.1.3" xref="lstnumberx1.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx1.4.m1.1b"><apply id="lstnumberx1.4.m1.1.1.cmml" xref="lstnumberx1.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx1.4.m1.1.1.1.cmml" xref="lstnumberx1.4.m1.1.1">subscript</csymbol><ci id="lstnumberx1.4.m1.1.1.2.cmml" xref="lstnumberx1.4.m1.1.1.2">𝑜</ci><cn id="lstnumberx1.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx1.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx1.4.m1.1c">o_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx1.4.m1.1d">italic_o start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.25" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.26" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.27" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.28" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.29" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.30" style="font-size:70%;"><span class="ltx_text" id="lstnumberx1.30.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.31" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.32" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.33" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.34" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx3.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx3.6.1" style="color:#000000;">b1110</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx3.1.m1.1"><semantics id="lstnumberx3.1.m1.1a"><msub id="lstnumberx3.1.m1.1.1" xref="lstnumberx3.1.m1.1.1.cmml"><mi id="lstnumberx3.1.m1.1.1.2" xref="lstnumberx3.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx3.1.m1.1.1.3" xref="lstnumberx3.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx3.1.m1.1b"><apply id="lstnumberx3.1.m1.1.1.cmml" xref="lstnumberx3.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx3.1.m1.1.1.1.cmml" xref="lstnumberx3.1.m1.1.1">subscript</csymbol><ci id="lstnumberx3.1.m1.1.1.2.cmml" xref="lstnumberx3.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx3.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx3.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx3.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx3.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx3.2.m1.1"><semantics id="lstnumberx3.2.m1.1a"><msub id="lstnumberx3.2.m1.1.1" xref="lstnumberx3.2.m1.1.1.cmml"><mi id="lstnumberx3.2.m1.1.1.2" xref="lstnumberx3.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx3.2.m1.1.1.3" xref="lstnumberx3.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx3.2.m1.1b"><apply id="lstnumberx3.2.m1.1.1.cmml" xref="lstnumberx3.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx3.2.m1.1.1.1.cmml" xref="lstnumberx3.2.m1.1.1">subscript</csymbol><ci id="lstnumberx3.2.m1.1.1.2.cmml" xref="lstnumberx3.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx3.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx3.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx3.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.18" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.3" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx3.3.m1.1"><semantics id="lstnumberx3.3.m1.1a"><msub id="lstnumberx3.3.m1.1.1" xref="lstnumberx3.3.m1.1.1.cmml"><mi id="lstnumberx3.3.m1.1.1.2" xref="lstnumberx3.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx3.3.m1.1.1.3" xref="lstnumberx3.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx3.3.m1.1b"><apply id="lstnumberx3.3.m1.1.1.cmml" xref="lstnumberx3.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx3.3.m1.1.1.1.cmml" xref="lstnumberx3.3.m1.1.1">subscript</csymbol><ci id="lstnumberx3.3.m1.1.1.2.cmml" xref="lstnumberx3.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx3.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx3.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx3.3.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx3.3.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.21" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.23" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.24" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.25" style="font-size:70%;"><span class="ltx_text" id="lstnumberx3.25.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.27" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx3.4.m1.1"><semantics id="lstnumberx3.4.m1.1a"><msub id="lstnumberx3.4.m1.1.1" xref="lstnumberx3.4.m1.1.1.cmml"><mi id="lstnumberx3.4.m1.1.1.2" xref="lstnumberx3.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx3.4.m1.1.1.3" xref="lstnumberx3.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx3.4.m1.1b"><apply id="lstnumberx3.4.m1.1.1.cmml" xref="lstnumberx3.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx3.4.m1.1.1.1.cmml" xref="lstnumberx3.4.m1.1.1">subscript</csymbol><ci id="lstnumberx3.4.m1.1.1.2.cmml" xref="lstnumberx3.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx3.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx3.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx3.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx3.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.29" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx4.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx4.6.1" style="color:#000000;">b1101</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx4.1.m1.1"><semantics id="lstnumberx4.1.m1.1a"><msub id="lstnumberx4.1.m1.1.1" xref="lstnumberx4.1.m1.1.1.cmml"><mi id="lstnumberx4.1.m1.1.1.2" xref="lstnumberx4.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx4.1.m1.1.1.3" xref="lstnumberx4.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx4.1.m1.1b"><apply id="lstnumberx4.1.m1.1.1.cmml" xref="lstnumberx4.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx4.1.m1.1.1.1.cmml" xref="lstnumberx4.1.m1.1.1">subscript</csymbol><ci id="lstnumberx4.1.m1.1.1.2.cmml" xref="lstnumberx4.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx4.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx4.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx4.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx4.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx4.2.m1.1"><semantics id="lstnumberx4.2.m1.1a"><msub id="lstnumberx4.2.m1.1.1" xref="lstnumberx4.2.m1.1.1.cmml"><mi id="lstnumberx4.2.m1.1.1.2" xref="lstnumberx4.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx4.2.m1.1.1.3" xref="lstnumberx4.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx4.2.m1.1b"><apply id="lstnumberx4.2.m1.1.1.cmml" xref="lstnumberx4.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx4.2.m1.1.1.1.cmml" xref="lstnumberx4.2.m1.1.1">subscript</csymbol><ci id="lstnumberx4.2.m1.1.1.2.cmml" xref="lstnumberx4.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx4.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx4.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx4.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx4.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.18" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.3" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx4.3.m1.1"><semantics id="lstnumberx4.3.m1.1a"><msub id="lstnumberx4.3.m1.1.1" xref="lstnumberx4.3.m1.1.1.cmml"><mi id="lstnumberx4.3.m1.1.1.2" xref="lstnumberx4.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx4.3.m1.1.1.3" xref="lstnumberx4.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx4.3.m1.1b"><apply id="lstnumberx4.3.m1.1.1.cmml" xref="lstnumberx4.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx4.3.m1.1.1.1.cmml" xref="lstnumberx4.3.m1.1.1">subscript</csymbol><ci id="lstnumberx4.3.m1.1.1.2.cmml" xref="lstnumberx4.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx4.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx4.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx4.3.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx4.3.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.21" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.4" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx4.4.m1.1"><semantics id="lstnumberx4.4.m1.1a"><msub id="lstnumberx4.4.m1.1.1" xref="lstnumberx4.4.m1.1.1.cmml"><mi id="lstnumberx4.4.m1.1.1.2" xref="lstnumberx4.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx4.4.m1.1.1.3" xref="lstnumberx4.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx4.4.m1.1b"><apply id="lstnumberx4.4.m1.1.1.cmml" xref="lstnumberx4.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx4.4.m1.1.1.1.cmml" xref="lstnumberx4.4.m1.1.1">subscript</csymbol><ci id="lstnumberx4.4.m1.1.1.2.cmml" xref="lstnumberx4.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx4.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx4.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx4.4.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx4.4.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.23" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.24" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.25" style="font-size:70%;"><span class="ltx_text" id="lstnumberx4.25.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.27" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.29" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx5.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx5.6.1" style="color:#000000;">b1011</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx5.1.m1.1"><semantics id="lstnumberx5.1.m1.1a"><msub id="lstnumberx5.1.m1.1.1" xref="lstnumberx5.1.m1.1.1.cmml"><mi id="lstnumberx5.1.m1.1.1.2" xref="lstnumberx5.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx5.1.m1.1.1.3" xref="lstnumberx5.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx5.1.m1.1b"><apply id="lstnumberx5.1.m1.1.1.cmml" xref="lstnumberx5.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx5.1.m1.1.1.1.cmml" xref="lstnumberx5.1.m1.1.1">subscript</csymbol><ci id="lstnumberx5.1.m1.1.1.2.cmml" xref="lstnumberx5.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx5.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx5.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx5.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx5.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx5.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx5.2.m1.1"><semantics id="lstnumberx5.2.m1.1a"><msub id="lstnumberx5.2.m1.1.1" xref="lstnumberx5.2.m1.1.1.cmml"><mi id="lstnumberx5.2.m1.1.1.2" xref="lstnumberx5.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx5.2.m1.1.1.3" xref="lstnumberx5.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx5.2.m1.1b"><apply id="lstnumberx5.2.m1.1.1.cmml" xref="lstnumberx5.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx5.2.m1.1.1.1.cmml" xref="lstnumberx5.2.m1.1.1">subscript</csymbol><ci id="lstnumberx5.2.m1.1.1.2.cmml" xref="lstnumberx5.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx5.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx5.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx5.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx5.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.3" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx5.3.m1.1"><semantics id="lstnumberx5.3.m1.1a"><msub id="lstnumberx5.3.m1.1.1" xref="lstnumberx5.3.m1.1.1.cmml"><mi id="lstnumberx5.3.m1.1.1.2" xref="lstnumberx5.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx5.3.m1.1.1.3" xref="lstnumberx5.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx5.3.m1.1b"><apply id="lstnumberx5.3.m1.1.1.cmml" xref="lstnumberx5.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx5.3.m1.1.1.1.cmml" xref="lstnumberx5.3.m1.1.1">subscript</csymbol><ci id="lstnumberx5.3.m1.1.1.2.cmml" xref="lstnumberx5.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx5.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx5.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx5.3.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx5.3.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.18" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.4" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx5.4.m1.1"><semantics id="lstnumberx5.4.m1.1a"><msub id="lstnumberx5.4.m1.1.1" xref="lstnumberx5.4.m1.1.1.cmml"><mi id="lstnumberx5.4.m1.1.1.2" xref="lstnumberx5.4.m1.1.1.2.cmml">o</mi><mn id="lstnumberx5.4.m1.1.1.3" xref="lstnumberx5.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx5.4.m1.1b"><apply id="lstnumberx5.4.m1.1.1.cmml" xref="lstnumberx5.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx5.4.m1.1.1.1.cmml" xref="lstnumberx5.4.m1.1.1">subscript</csymbol><ci id="lstnumberx5.4.m1.1.1.2.cmml" xref="lstnumberx5.4.m1.1.1.2">𝑜</ci><cn id="lstnumberx5.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx5.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx5.4.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx5.4.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.21" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.23" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.24" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.25" style="font-size:70%;"><span class="ltx_text" id="lstnumberx5.25.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.27" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.29" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx6.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx6.6.1" style="color:#000000;">b0111</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx6.1.m1.1"><semantics id="lstnumberx6.1.m1.1a"><msub id="lstnumberx6.1.m1.1.1" xref="lstnumberx6.1.m1.1.1.cmml"><mi id="lstnumberx6.1.m1.1.1.2" xref="lstnumberx6.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx6.1.m1.1.1.3" xref="lstnumberx6.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx6.1.m1.1b"><apply id="lstnumberx6.1.m1.1.1.cmml" xref="lstnumberx6.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx6.1.m1.1.1.1.cmml" xref="lstnumberx6.1.m1.1.1">subscript</csymbol><ci id="lstnumberx6.1.m1.1.1.2.cmml" xref="lstnumberx6.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx6.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx6.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx6.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx6.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx6.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.16" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx6.2.m1.1"><semantics id="lstnumberx6.2.m1.1a"><msub id="lstnumberx6.2.m1.1.1" xref="lstnumberx6.2.m1.1.1.cmml"><mi id="lstnumberx6.2.m1.1.1.2" xref="lstnumberx6.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx6.2.m1.1.1.3" xref="lstnumberx6.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx6.2.m1.1b"><apply id="lstnumberx6.2.m1.1.1.cmml" xref="lstnumberx6.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx6.2.m1.1.1.1.cmml" xref="lstnumberx6.2.m1.1.1">subscript</csymbol><ci id="lstnumberx6.2.m1.1.1.2.cmml" xref="lstnumberx6.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx6.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx6.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx6.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx6.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.20" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx6.3.m1.1"><semantics id="lstnumberx6.3.m1.1a"><msub id="lstnumberx6.3.m1.1.1" xref="lstnumberx6.3.m1.1.1.cmml"><mi id="lstnumberx6.3.m1.1.1.2" xref="lstnumberx6.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx6.3.m1.1.1.3" xref="lstnumberx6.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx6.3.m1.1b"><apply id="lstnumberx6.3.m1.1.1.cmml" xref="lstnumberx6.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx6.3.m1.1.1.1.cmml" xref="lstnumberx6.3.m1.1.1">subscript</csymbol><ci id="lstnumberx6.3.m1.1.1.2.cmml" xref="lstnumberx6.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx6.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx6.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx6.3.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx6.3.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.23" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.25" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.4" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx6.4.m1.1"><semantics id="lstnumberx6.4.m1.1a"><msub id="lstnumberx6.4.m1.1.1" xref="lstnumberx6.4.m1.1.1.cmml"><mi id="lstnumberx6.4.m1.1.1.2" xref="lstnumberx6.4.m1.1.1.2.cmml">o</mi><mn id="lstnumberx6.4.m1.1.1.3" xref="lstnumberx6.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx6.4.m1.1b"><apply id="lstnumberx6.4.m1.1.1.cmml" xref="lstnumberx6.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx6.4.m1.1.1.1.cmml" xref="lstnumberx6.4.m1.1.1">subscript</csymbol><ci id="lstnumberx6.4.m1.1.1.2.cmml" xref="lstnumberx6.4.m1.1.1.2">𝑜</ci><cn id="lstnumberx6.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx6.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx6.4.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx6.4.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.27" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.28" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.29" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.30" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.31" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.32" style="font-size:70%;"><span class="ltx_text" id="lstnumberx6.32.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.33" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.34" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.35" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.36" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx8.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx8.6.1" style="color:#000000;">b1100</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx8.1.m1.1"><semantics id="lstnumberx8.1.m1.1a"><msub id="lstnumberx8.1.m1.1.1" xref="lstnumberx8.1.m1.1.1.cmml"><mi id="lstnumberx8.1.m1.1.1.2" xref="lstnumberx8.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx8.1.m1.1.1.3" xref="lstnumberx8.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx8.1.m1.1b"><apply id="lstnumberx8.1.m1.1.1.cmml" xref="lstnumberx8.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx8.1.m1.1.1.1.cmml" xref="lstnumberx8.1.m1.1.1">subscript</csymbol><ci id="lstnumberx8.1.m1.1.1.2.cmml" xref="lstnumberx8.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx8.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx8.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx8.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx8.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx8.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx8.2.m1.1"><semantics id="lstnumberx8.2.m1.1a"><msub id="lstnumberx8.2.m1.1.1" xref="lstnumberx8.2.m1.1.1.cmml"><mi id="lstnumberx8.2.m1.1.1.2" xref="lstnumberx8.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx8.2.m1.1.1.3" xref="lstnumberx8.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx8.2.m1.1b"><apply id="lstnumberx8.2.m1.1.1.cmml" xref="lstnumberx8.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx8.2.m1.1.1.1.cmml" xref="lstnumberx8.2.m1.1.1">subscript</csymbol><ci id="lstnumberx8.2.m1.1.1.2.cmml" xref="lstnumberx8.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx8.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx8.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx8.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx8.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.18" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.19" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.20" style="font-size:70%;"><span class="ltx_text" id="lstnumberx8.20.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.22" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.23" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.3" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx8.3.m1.1"><semantics id="lstnumberx8.3.m1.1a"><msub id="lstnumberx8.3.m1.1.1" xref="lstnumberx8.3.m1.1.1.cmml"><mi id="lstnumberx8.3.m1.1.1.2" xref="lstnumberx8.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx8.3.m1.1.1.3" xref="lstnumberx8.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx8.3.m1.1b"><apply id="lstnumberx8.3.m1.1.1.cmml" xref="lstnumberx8.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx8.3.m1.1.1.1.cmml" xref="lstnumberx8.3.m1.1.1">subscript</csymbol><ci id="lstnumberx8.3.m1.1.1.2.cmml" xref="lstnumberx8.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx8.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx8.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx8.3.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx8.3.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.25" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.27" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.29" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx8.4.m1.1"><semantics id="lstnumberx8.4.m1.1a"><msub id="lstnumberx8.4.m1.1.1" xref="lstnumberx8.4.m1.1.1.cmml"><mi id="lstnumberx8.4.m1.1.1.2" xref="lstnumberx8.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx8.4.m1.1.1.3" xref="lstnumberx8.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx8.4.m1.1b"><apply id="lstnumberx8.4.m1.1.1.cmml" xref="lstnumberx8.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx8.4.m1.1.1.1.cmml" xref="lstnumberx8.4.m1.1.1">subscript</csymbol><ci id="lstnumberx8.4.m1.1.1.2.cmml" xref="lstnumberx8.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx8.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx8.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx8.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx8.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.31" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx9.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx9.6.1" style="color:#000000;">b1010</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx9.1.m1.1"><semantics id="lstnumberx9.1.m1.1a"><msub id="lstnumberx9.1.m1.1.1" xref="lstnumberx9.1.m1.1.1.cmml"><mi id="lstnumberx9.1.m1.1.1.2" xref="lstnumberx9.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx9.1.m1.1.1.3" xref="lstnumberx9.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx9.1.m1.1b"><apply id="lstnumberx9.1.m1.1.1.cmml" xref="lstnumberx9.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx9.1.m1.1.1.1.cmml" xref="lstnumberx9.1.m1.1.1">subscript</csymbol><ci id="lstnumberx9.1.m1.1.1.2.cmml" xref="lstnumberx9.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx9.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx9.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx9.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx9.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx9.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx9.2.m1.1"><semantics id="lstnumberx9.2.m1.1a"><msub id="lstnumberx9.2.m1.1.1" xref="lstnumberx9.2.m1.1.1.cmml"><mi id="lstnumberx9.2.m1.1.1.2" xref="lstnumberx9.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx9.2.m1.1.1.3" xref="lstnumberx9.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx9.2.m1.1b"><apply id="lstnumberx9.2.m1.1.1.cmml" xref="lstnumberx9.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx9.2.m1.1.1.1.cmml" xref="lstnumberx9.2.m1.1.1">subscript</csymbol><ci id="lstnumberx9.2.m1.1.1.2.cmml" xref="lstnumberx9.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx9.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx9.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx9.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx9.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.3" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx9.3.m1.1"><semantics id="lstnumberx9.3.m1.1a"><msub id="lstnumberx9.3.m1.1.1" xref="lstnumberx9.3.m1.1.1.cmml"><mi id="lstnumberx9.3.m1.1.1.2" xref="lstnumberx9.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx9.3.m1.1.1.3" xref="lstnumberx9.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx9.3.m1.1b"><apply id="lstnumberx9.3.m1.1.1.cmml" xref="lstnumberx9.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx9.3.m1.1.1.1.cmml" xref="lstnumberx9.3.m1.1.1">subscript</csymbol><ci id="lstnumberx9.3.m1.1.1.2.cmml" xref="lstnumberx9.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx9.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx9.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx9.3.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx9.3.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.18" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.19" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.20" style="font-size:70%;"><span class="ltx_text" id="lstnumberx9.20.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.22" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.23" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx9.4.m1.1"><semantics id="lstnumberx9.4.m1.1a"><msub id="lstnumberx9.4.m1.1.1" xref="lstnumberx9.4.m1.1.1.cmml"><mi id="lstnumberx9.4.m1.1.1.2" xref="lstnumberx9.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx9.4.m1.1.1.3" xref="lstnumberx9.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx9.4.m1.1b"><apply id="lstnumberx9.4.m1.1.1.cmml" xref="lstnumberx9.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx9.4.m1.1.1.1.cmml" xref="lstnumberx9.4.m1.1.1">subscript</csymbol><ci id="lstnumberx9.4.m1.1.1.2.cmml" xref="lstnumberx9.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx9.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx9.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx9.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx9.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.24" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx10.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx10.6.1" style="color:#000000;">b1001</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx10.1.m1.1"><semantics id="lstnumberx10.1.m1.1a"><msub id="lstnumberx10.1.m1.1.1" xref="lstnumberx10.1.m1.1.1.cmml"><mi id="lstnumberx10.1.m1.1.1.2" xref="lstnumberx10.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx10.1.m1.1.1.3" xref="lstnumberx10.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx10.1.m1.1b"><apply id="lstnumberx10.1.m1.1.1.cmml" xref="lstnumberx10.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx10.1.m1.1.1.1.cmml" xref="lstnumberx10.1.m1.1.1">subscript</csymbol><ci id="lstnumberx10.1.m1.1.1.2.cmml" xref="lstnumberx10.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx10.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx10.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx10.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx10.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx10.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.13" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.2" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx10.2.m1.1"><semantics id="lstnumberx10.2.m1.1a"><msub id="lstnumberx10.2.m1.1.1" xref="lstnumberx10.2.m1.1.1.cmml"><mi id="lstnumberx10.2.m1.1.1.2" xref="lstnumberx10.2.m1.1.1.2.cmml">o</mi><mn id="lstnumberx10.2.m1.1.1.3" xref="lstnumberx10.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx10.2.m1.1b"><apply id="lstnumberx10.2.m1.1.1.cmml" xref="lstnumberx10.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx10.2.m1.1.1.1.cmml" xref="lstnumberx10.2.m1.1.1">subscript</csymbol><ci id="lstnumberx10.2.m1.1.1.2.cmml" xref="lstnumberx10.2.m1.1.1.2">𝑜</ci><cn id="lstnumberx10.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx10.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx10.2.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx10.2.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.3" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx10.3.m1.1"><semantics id="lstnumberx10.3.m1.1a"><msub id="lstnumberx10.3.m1.1.1" xref="lstnumberx10.3.m1.1.1.cmml"><mi id="lstnumberx10.3.m1.1.1.2" xref="lstnumberx10.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx10.3.m1.1.1.3" xref="lstnumberx10.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx10.3.m1.1b"><apply id="lstnumberx10.3.m1.1.1.cmml" xref="lstnumberx10.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx10.3.m1.1.1.1.cmml" xref="lstnumberx10.3.m1.1.1">subscript</csymbol><ci id="lstnumberx10.3.m1.1.1.2.cmml" xref="lstnumberx10.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx10.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx10.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx10.3.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx10.3.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.18" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.19" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.21" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.23" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx10.4.m1.1"><semantics id="lstnumberx10.4.m1.1a"><msub id="lstnumberx10.4.m1.1.1" xref="lstnumberx10.4.m1.1.1.cmml"><mi id="lstnumberx10.4.m1.1.1.2" xref="lstnumberx10.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx10.4.m1.1.1.3" xref="lstnumberx10.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx10.4.m1.1b"><apply id="lstnumberx10.4.m1.1.1.cmml" xref="lstnumberx10.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx10.4.m1.1.1.1.cmml" xref="lstnumberx10.4.m1.1.1">subscript</csymbol><ci id="lstnumberx10.4.m1.1.1.2.cmml" xref="lstnumberx10.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx10.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx10.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx10.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx10.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.25" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.26" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.27" style="font-size:70%;"><span class="ltx_text" id="lstnumberx10.27.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.29" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.31" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx11.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx11.6.1" style="color:#000000;">b0110</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx11.1.m1.1"><semantics id="lstnumberx11.1.m1.1a"><msub id="lstnumberx11.1.m1.1.1" xref="lstnumberx11.1.m1.1.1.cmml"><mi id="lstnumberx11.1.m1.1.1.2" xref="lstnumberx11.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx11.1.m1.1.1.3" xref="lstnumberx11.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx11.1.m1.1b"><apply id="lstnumberx11.1.m1.1.1.cmml" xref="lstnumberx11.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx11.1.m1.1.1.1.cmml" xref="lstnumberx11.1.m1.1.1">subscript</csymbol><ci id="lstnumberx11.1.m1.1.1.2.cmml" xref="lstnumberx11.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx11.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx11.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx11.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx11.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx11.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.16" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx11.2.m1.1"><semantics id="lstnumberx11.2.m1.1a"><msub id="lstnumberx11.2.m1.1.1" xref="lstnumberx11.2.m1.1.1.cmml"><mi id="lstnumberx11.2.m1.1.1.2" xref="lstnumberx11.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx11.2.m1.1.1.3" xref="lstnumberx11.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx11.2.m1.1b"><apply id="lstnumberx11.2.m1.1.1.cmml" xref="lstnumberx11.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx11.2.m1.1.1.1.cmml" xref="lstnumberx11.2.m1.1.1">subscript</csymbol><ci id="lstnumberx11.2.m1.1.1.2.cmml" xref="lstnumberx11.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx11.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx11.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx11.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx11.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.20" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.3" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx11.3.m1.1"><semantics id="lstnumberx11.3.m1.1a"><msub id="lstnumberx11.3.m1.1.1" xref="lstnumberx11.3.m1.1.1.cmml"><mi id="lstnumberx11.3.m1.1.1.2" xref="lstnumberx11.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx11.3.m1.1.1.3" xref="lstnumberx11.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx11.3.m1.1b"><apply id="lstnumberx11.3.m1.1.1.cmml" xref="lstnumberx11.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx11.3.m1.1.1.1.cmml" xref="lstnumberx11.3.m1.1.1">subscript</csymbol><ci id="lstnumberx11.3.m1.1.1.2.cmml" xref="lstnumberx11.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx11.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx11.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx11.3.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx11.3.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.23" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.25" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.26" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.27" style="font-size:70%;"><span class="ltx_text" id="lstnumberx11.27.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.29" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx11.4.m1.1"><semantics id="lstnumberx11.4.m1.1a"><msub id="lstnumberx11.4.m1.1.1" xref="lstnumberx11.4.m1.1.1.cmml"><mi id="lstnumberx11.4.m1.1.1.2" xref="lstnumberx11.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx11.4.m1.1.1.3" xref="lstnumberx11.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx11.4.m1.1b"><apply id="lstnumberx11.4.m1.1.1.cmml" xref="lstnumberx11.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx11.4.m1.1.1.1.cmml" xref="lstnumberx11.4.m1.1.1">subscript</csymbol><ci id="lstnumberx11.4.m1.1.1.2.cmml" xref="lstnumberx11.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx11.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx11.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx11.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx11.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.31" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx12.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx12.6.1" style="color:#000000;">b0101</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx12.1.m1.1"><semantics id="lstnumberx12.1.m1.1a"><msub id="lstnumberx12.1.m1.1.1" xref="lstnumberx12.1.m1.1.1.cmml"><mi id="lstnumberx12.1.m1.1.1.2" xref="lstnumberx12.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx12.1.m1.1.1.3" xref="lstnumberx12.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx12.1.m1.1b"><apply id="lstnumberx12.1.m1.1.1.cmml" xref="lstnumberx12.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx12.1.m1.1.1.1.cmml" xref="lstnumberx12.1.m1.1.1">subscript</csymbol><ci id="lstnumberx12.1.m1.1.1.2.cmml" xref="lstnumberx12.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx12.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx12.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx12.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx12.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx12.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.16" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx12.2.m1.1"><semantics id="lstnumberx12.2.m1.1a"><msub id="lstnumberx12.2.m1.1.1" xref="lstnumberx12.2.m1.1.1.cmml"><mi id="lstnumberx12.2.m1.1.1.2" xref="lstnumberx12.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx12.2.m1.1.1.3" xref="lstnumberx12.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx12.2.m1.1b"><apply id="lstnumberx12.2.m1.1.1.cmml" xref="lstnumberx12.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx12.2.m1.1.1.1.cmml" xref="lstnumberx12.2.m1.1.1">subscript</csymbol><ci id="lstnumberx12.2.m1.1.1.2.cmml" xref="lstnumberx12.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx12.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx12.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx12.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx12.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.20" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.3" style="font-size:70%;"><math alttext="o_{1}" class="ltx_Math" display="inline" id="lstnumberx12.3.m1.1"><semantics id="lstnumberx12.3.m1.1a"><msub id="lstnumberx12.3.m1.1.1" xref="lstnumberx12.3.m1.1.1.cmml"><mi id="lstnumberx12.3.m1.1.1.2" xref="lstnumberx12.3.m1.1.1.2.cmml">o</mi><mn id="lstnumberx12.3.m1.1.1.3" xref="lstnumberx12.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx12.3.m1.1b"><apply id="lstnumberx12.3.m1.1.1.cmml" xref="lstnumberx12.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx12.3.m1.1.1.1.cmml" xref="lstnumberx12.3.m1.1.1">subscript</csymbol><ci id="lstnumberx12.3.m1.1.1.2.cmml" xref="lstnumberx12.3.m1.1.1.2">𝑜</ci><cn id="lstnumberx12.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx12.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx12.3.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx12.3.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.23" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx12.4.m1.1"><semantics id="lstnumberx12.4.m1.1a"><msub id="lstnumberx12.4.m1.1.1" xref="lstnumberx12.4.m1.1.1.cmml"><mi id="lstnumberx12.4.m1.1.1.2" xref="lstnumberx12.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx12.4.m1.1.1.3" xref="lstnumberx12.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx12.4.m1.1b"><apply id="lstnumberx12.4.m1.1.1.cmml" xref="lstnumberx12.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx12.4.m1.1.1.1.cmml" xref="lstnumberx12.4.m1.1.1">subscript</csymbol><ci id="lstnumberx12.4.m1.1.1.2.cmml" xref="lstnumberx12.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx12.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx12.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx12.4.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx12.4.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.25" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.26" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.27" style="font-size:70%;"><span class="ltx_text" id="lstnumberx12.27.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.29" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.31" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx13.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx13.6.1" style="color:#000000;">b0011</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx13.1.m1.1"><semantics id="lstnumberx13.1.m1.1a"><msub id="lstnumberx13.1.m1.1.1" xref="lstnumberx13.1.m1.1.1.cmml"><mi id="lstnumberx13.1.m1.1.1.2" xref="lstnumberx13.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx13.1.m1.1.1.3" xref="lstnumberx13.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx13.1.m1.1b"><apply id="lstnumberx13.1.m1.1.1.cmml" xref="lstnumberx13.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx13.1.m1.1.1.1.cmml" xref="lstnumberx13.1.m1.1.1">subscript</csymbol><ci id="lstnumberx13.1.m1.1.1.2.cmml" xref="lstnumberx13.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx13.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx13.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx13.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx13.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx13.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.16" style="font-size:70%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx13.2.m1.1"><semantics id="lstnumberx13.2.m1.1a"><msub id="lstnumberx13.2.m1.1.1" xref="lstnumberx13.2.m1.1.1.cmml"><mi id="lstnumberx13.2.m1.1.1.2" xref="lstnumberx13.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx13.2.m1.1.1.3" xref="lstnumberx13.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx13.2.m1.1b"><apply id="lstnumberx13.2.m1.1.1.cmml" xref="lstnumberx13.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx13.2.m1.1.1.1.cmml" xref="lstnumberx13.2.m1.1.1">subscript</csymbol><ci id="lstnumberx13.2.m1.1.1.2.cmml" xref="lstnumberx13.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx13.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx13.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx13.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx13.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.21" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.23" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.25" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.3" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx13.3.m1.1"><semantics id="lstnumberx13.3.m1.1a"><msub id="lstnumberx13.3.m1.1.1" xref="lstnumberx13.3.m1.1.1.cmml"><mi id="lstnumberx13.3.m1.1.1.2" xref="lstnumberx13.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx13.3.m1.1.1.3" xref="lstnumberx13.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx13.3.m1.1b"><apply id="lstnumberx13.3.m1.1.1.cmml" xref="lstnumberx13.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx13.3.m1.1.1.1.cmml" xref="lstnumberx13.3.m1.1.1">subscript</csymbol><ci id="lstnumberx13.3.m1.1.1.2.cmml" xref="lstnumberx13.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx13.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx13.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx13.3.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx13.3.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.27" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.4" style="font-size:70%;"><math alttext="o_{2}" class="ltx_Math" display="inline" id="lstnumberx13.4.m1.1"><semantics id="lstnumberx13.4.m1.1a"><msub id="lstnumberx13.4.m1.1.1" xref="lstnumberx13.4.m1.1.1.cmml"><mi id="lstnumberx13.4.m1.1.1.2" xref="lstnumberx13.4.m1.1.1.2.cmml">o</mi><mn id="lstnumberx13.4.m1.1.1.3" xref="lstnumberx13.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx13.4.m1.1b"><apply id="lstnumberx13.4.m1.1.1.cmml" xref="lstnumberx13.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx13.4.m1.1.1.1.cmml" xref="lstnumberx13.4.m1.1.1">subscript</csymbol><ci id="lstnumberx13.4.m1.1.1.2.cmml" xref="lstnumberx13.4.m1.1.1.2">𝑜</ci><cn id="lstnumberx13.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx13.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx13.4.m1.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx13.4.m1.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.29" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.30" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.31" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.32" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.33" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.34" style="font-size:70%;"><span class="ltx_text" id="lstnumberx13.34.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.35" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.36" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.37" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.38" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx15.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx15.6.1" style="color:#000000;">b1000</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx15.1.m1.1"><semantics id="lstnumberx15.1.m1.1a"><msub id="lstnumberx15.1.m1.1.1" xref="lstnumberx15.1.m1.1.1.cmml"><mi id="lstnumberx15.1.m1.1.1.2" xref="lstnumberx15.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx15.1.m1.1.1.3" xref="lstnumberx15.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx15.1.m1.1b"><apply id="lstnumberx15.1.m1.1.1.cmml" xref="lstnumberx15.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx15.1.m1.1.1.1.cmml" xref="lstnumberx15.1.m1.1.1">subscript</csymbol><ci id="lstnumberx15.1.m1.1.1.2.cmml" xref="lstnumberx15.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx15.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx15.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx15.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx15.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.11" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.12" style="font-size:70%;">0;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.13" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.14" style="font-size:70%;"><span class="ltx_text" id="lstnumberx15.14.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.16" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.2" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx15.2.m1.1"><semantics id="lstnumberx15.2.m1.1a"><msub id="lstnumberx15.2.m1.1.1" xref="lstnumberx15.2.m1.1.1.cmml"><mi id="lstnumberx15.2.m1.1.1.2" xref="lstnumberx15.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx15.2.m1.1.1.3" xref="lstnumberx15.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx15.2.m1.1b"><apply id="lstnumberx15.2.m1.1.1.cmml" xref="lstnumberx15.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx15.2.m1.1.1.1.cmml" xref="lstnumberx15.2.m1.1.1">subscript</csymbol><ci id="lstnumberx15.2.m1.1.1.2.cmml" xref="lstnumberx15.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx15.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx15.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx15.2.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx15.2.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.18" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.19" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.21" style="font-size:70%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.23" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.3" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx15.3.m1.1"><semantics id="lstnumberx15.3.m1.1a"><msub id="lstnumberx15.3.m1.1.1" xref="lstnumberx15.3.m1.1.1.cmml"><mi id="lstnumberx15.3.m1.1.1.2" xref="lstnumberx15.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx15.3.m1.1.1.3" xref="lstnumberx15.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx15.3.m1.1b"><apply id="lstnumberx15.3.m1.1.1.cmml" xref="lstnumberx15.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx15.3.m1.1.1.1.cmml" xref="lstnumberx15.3.m1.1.1">subscript</csymbol><ci id="lstnumberx15.3.m1.1.1.2.cmml" xref="lstnumberx15.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx15.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx15.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx15.3.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx15.3.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.25" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.26" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.27" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.28" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.29" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.30" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.31" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx15.4.m1.1"><semantics id="lstnumberx15.4.m1.1a"><msub id="lstnumberx15.4.m1.1.1" xref="lstnumberx15.4.m1.1.1.cmml"><mi id="lstnumberx15.4.m1.1.1.2" xref="lstnumberx15.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx15.4.m1.1.1.3" xref="lstnumberx15.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx15.4.m1.1b"><apply id="lstnumberx15.4.m1.1.1.cmml" xref="lstnumberx15.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx15.4.m1.1.1.1.cmml" xref="lstnumberx15.4.m1.1.1">subscript</csymbol><ci id="lstnumberx15.4.m1.1.1.2.cmml" xref="lstnumberx15.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx15.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx15.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx15.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx15.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.32" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx16.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx16.6.1" style="color:#000000;">b0100</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx16.1.m1.1"><semantics id="lstnumberx16.1.m1.1a"><msub id="lstnumberx16.1.m1.1.1" xref="lstnumberx16.1.m1.1.1.cmml"><mi id="lstnumberx16.1.m1.1.1.2" xref="lstnumberx16.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx16.1.m1.1.1.3" xref="lstnumberx16.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx16.1.m1.1b"><apply id="lstnumberx16.1.m1.1.1.cmml" xref="lstnumberx16.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx16.1.m1.1.1.1.cmml" xref="lstnumberx16.1.m1.1.1">subscript</csymbol><ci id="lstnumberx16.1.m1.1.1.2.cmml" xref="lstnumberx16.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx16.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx16.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx16.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx16.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx16.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.16" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx16.2.m1.1"><semantics id="lstnumberx16.2.m1.1a"><msub id="lstnumberx16.2.m1.1.1" xref="lstnumberx16.2.m1.1.1.cmml"><mi id="lstnumberx16.2.m1.1.1.2" xref="lstnumberx16.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx16.2.m1.1.1.3" xref="lstnumberx16.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx16.2.m1.1b"><apply id="lstnumberx16.2.m1.1.1.cmml" xref="lstnumberx16.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx16.2.m1.1.1.1.cmml" xref="lstnumberx16.2.m1.1.1">subscript</csymbol><ci id="lstnumberx16.2.m1.1.1.2.cmml" xref="lstnumberx16.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx16.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx16.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx16.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx16.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.20" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.21" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.22" style="font-size:70%;"><span class="ltx_text" id="lstnumberx16.22.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.23" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.24" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.25" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.3" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx16.3.m1.1"><semantics id="lstnumberx16.3.m1.1a"><msub id="lstnumberx16.3.m1.1.1" xref="lstnumberx16.3.m1.1.1.cmml"><mi id="lstnumberx16.3.m1.1.1.2" xref="lstnumberx16.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx16.3.m1.1.1.3" xref="lstnumberx16.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx16.3.m1.1b"><apply id="lstnumberx16.3.m1.1.1.cmml" xref="lstnumberx16.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx16.3.m1.1.1.1.cmml" xref="lstnumberx16.3.m1.1.1">subscript</csymbol><ci id="lstnumberx16.3.m1.1.1.2.cmml" xref="lstnumberx16.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx16.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx16.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx16.3.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx16.3.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.27" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.29" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.31" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.32" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx16.4.m1.1"><semantics id="lstnumberx16.4.m1.1a"><msub id="lstnumberx16.4.m1.1.1" xref="lstnumberx16.4.m1.1.1.cmml"><mi id="lstnumberx16.4.m1.1.1.2" xref="lstnumberx16.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx16.4.m1.1.1.3" xref="lstnumberx16.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx16.4.m1.1b"><apply id="lstnumberx16.4.m1.1.1.cmml" xref="lstnumberx16.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx16.4.m1.1.1.1.cmml" xref="lstnumberx16.4.m1.1.1">subscript</csymbol><ci id="lstnumberx16.4.m1.1.1.2.cmml" xref="lstnumberx16.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx16.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx16.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx16.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx16.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.33" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx17.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx17.6.1" style="color:#000000;">b0010</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx17.1.m1.1"><semantics id="lstnumberx17.1.m1.1a"><msub id="lstnumberx17.1.m1.1.1" xref="lstnumberx17.1.m1.1.1.cmml"><mi id="lstnumberx17.1.m1.1.1.2" xref="lstnumberx17.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx17.1.m1.1.1.3" xref="lstnumberx17.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx17.1.m1.1b"><apply id="lstnumberx17.1.m1.1.1.cmml" xref="lstnumberx17.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx17.1.m1.1.1.1.cmml" xref="lstnumberx17.1.m1.1.1">subscript</csymbol><ci id="lstnumberx17.1.m1.1.1.2.cmml" xref="lstnumberx17.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx17.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx17.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx17.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx17.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx17.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.16" style="font-size:70%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx17.2.m1.1"><semantics id="lstnumberx17.2.m1.1a"><msub id="lstnumberx17.2.m1.1.1" xref="lstnumberx17.2.m1.1.1.cmml"><mi id="lstnumberx17.2.m1.1.1.2" xref="lstnumberx17.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx17.2.m1.1.1.3" xref="lstnumberx17.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx17.2.m1.1b"><apply id="lstnumberx17.2.m1.1.1.cmml" xref="lstnumberx17.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx17.2.m1.1.1.1.cmml" xref="lstnumberx17.2.m1.1.1">subscript</csymbol><ci id="lstnumberx17.2.m1.1.1.2.cmml" xref="lstnumberx17.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx17.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx17.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx17.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx17.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.21" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.23" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.25" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.3" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx17.3.m1.1"><semantics id="lstnumberx17.3.m1.1a"><msub id="lstnumberx17.3.m1.1.1" xref="lstnumberx17.3.m1.1.1.cmml"><mi id="lstnumberx17.3.m1.1.1.2" xref="lstnumberx17.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx17.3.m1.1.1.3" xref="lstnumberx17.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx17.3.m1.1b"><apply id="lstnumberx17.3.m1.1.1.cmml" xref="lstnumberx17.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx17.3.m1.1.1.1.cmml" xref="lstnumberx17.3.m1.1.1">subscript</csymbol><ci id="lstnumberx17.3.m1.1.1.2.cmml" xref="lstnumberx17.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx17.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx17.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx17.3.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx17.3.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.27" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.28" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.29" style="font-size:70%;"><span class="ltx_text" id="lstnumberx17.29.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.31" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.32" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx17.4.m1.1"><semantics id="lstnumberx17.4.m1.1a"><msub id="lstnumberx17.4.m1.1.1" xref="lstnumberx17.4.m1.1.1.cmml"><mi id="lstnumberx17.4.m1.1.1.2" xref="lstnumberx17.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx17.4.m1.1.1.3" xref="lstnumberx17.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx17.4.m1.1b"><apply id="lstnumberx17.4.m1.1.1.cmml" xref="lstnumberx17.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx17.4.m1.1.1.1.cmml" xref="lstnumberx17.4.m1.1.1">subscript</csymbol><ci id="lstnumberx17.4.m1.1.1.2.cmml" xref="lstnumberx17.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx17.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx17.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx17.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx17.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.33" style="font-size:70%;">;</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx18.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx18.6.1" style="color:#000000;">b0001</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.8" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.1" style="font-size:70%;"><math alttext="o_{0}" class="ltx_Math" display="inline" id="lstnumberx18.1.m1.1"><semantics id="lstnumberx18.1.m1.1a"><msub id="lstnumberx18.1.m1.1.1" xref="lstnumberx18.1.m1.1.1.cmml"><mi id="lstnumberx18.1.m1.1.1.2" xref="lstnumberx18.1.m1.1.1.2.cmml">o</mi><mn id="lstnumberx18.1.m1.1.1.3" xref="lstnumberx18.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx18.1.m1.1b"><apply id="lstnumberx18.1.m1.1.1.cmml" xref="lstnumberx18.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx18.1.m1.1.1.1.cmml" xref="lstnumberx18.1.m1.1.1">subscript</csymbol><ci id="lstnumberx18.1.m1.1.1.2.cmml" xref="lstnumberx18.1.m1.1.1.2">𝑜</ci><cn id="lstnumberx18.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx18.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx18.1.m1.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx18.1.m1.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.9" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.10" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.11" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.12" style="font-size:70%;"><span class="ltx_text" id="lstnumberx18.12.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.14" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.15" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.16" style="font-size:70%;">12</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.17" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.18" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.19" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx18.2.m1.1"><semantics id="lstnumberx18.2.m1.1a"><msub id="lstnumberx18.2.m1.1.1" xref="lstnumberx18.2.m1.1.1.cmml"><mi id="lstnumberx18.2.m1.1.1.2" xref="lstnumberx18.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx18.2.m1.1.1.3" xref="lstnumberx18.2.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx18.2.m1.1b"><apply id="lstnumberx18.2.m1.1.1.cmml" xref="lstnumberx18.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx18.2.m1.1.1.1.cmml" xref="lstnumberx18.2.m1.1.1">subscript</csymbol><ci id="lstnumberx18.2.m1.1.1.2.cmml" xref="lstnumberx18.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx18.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx18.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx18.2.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx18.2.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.21" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.22" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.23" style="font-size:70%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.24" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.25" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.26" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.3" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx18.3.m1.1"><semantics id="lstnumberx18.3.m1.1a"><msub id="lstnumberx18.3.m1.1.1" xref="lstnumberx18.3.m1.1.1.cmml"><mi id="lstnumberx18.3.m1.1.1.2" xref="lstnumberx18.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx18.3.m1.1.1.3" xref="lstnumberx18.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx18.3.m1.1b"><apply id="lstnumberx18.3.m1.1.1.cmml" xref="lstnumberx18.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx18.3.m1.1.1.1.cmml" xref="lstnumberx18.3.m1.1.1">subscript</csymbol><ci id="lstnumberx18.3.m1.1.1.2.cmml" xref="lstnumberx18.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx18.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx18.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx18.3.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx18.3.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.27" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.28" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.29" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.30" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.31" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.32" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.33" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx18.4.m1.1"><semantics id="lstnumberx18.4.m1.1a"><msub id="lstnumberx18.4.m1.1.1" xref="lstnumberx18.4.m1.1.1.cmml"><mi id="lstnumberx18.4.m1.1.1.2" xref="lstnumberx18.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx18.4.m1.1.1.3" xref="lstnumberx18.4.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx18.4.m1.1b"><apply id="lstnumberx18.4.m1.1.1.cmml" xref="lstnumberx18.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx18.4.m1.1.1.1.cmml" xref="lstnumberx18.4.m1.1.1">subscript</csymbol><ci id="lstnumberx18.4.m1.1.1.2.cmml" xref="lstnumberx18.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx18.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx18.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx18.4.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx18.4.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.34" style="font-size:70%;">;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.35" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.36" style="font-size:70%;"><span class="ltx_text" id="lstnumberx18.36.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.37" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.38" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.39" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.40" style="font-size:70%;">0;</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx20.5" style="font-size:70%;">0</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.6" style="font-size:70%;"><span class="ltx_text" id="lstnumberx20.6.1" style="color:#000000;">b0000</span></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.7" style="font-size:70%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.8" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.9" style="font-size:70%;"><span class="ltx_text" id="lstnumberx20.9.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.10" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.11" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.12" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.13" style="font-size:70%;"><span class="ltx_text" id="lstnumberx20.13.1" style="color:#000000;">v</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.14" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.15" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.16" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.17" style="font-size:70%;">16</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.18" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.19" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.20" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.1" style="font-size:70%;"><math alttext="s_{0}" class="ltx_Math" display="inline" id="lstnumberx20.1.m1.1"><semantics id="lstnumberx20.1.m1.1a"><msub id="lstnumberx20.1.m1.1.1" xref="lstnumberx20.1.m1.1.1.cmml"><mi id="lstnumberx20.1.m1.1.1.2" xref="lstnumberx20.1.m1.1.1.2.cmml">s</mi><mn id="lstnumberx20.1.m1.1.1.3" xref="lstnumberx20.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx20.1.m1.1b"><apply id="lstnumberx20.1.m1.1.1.cmml" xref="lstnumberx20.1.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx20.1.m1.1.1.1.cmml" xref="lstnumberx20.1.m1.1.1">subscript</csymbol><ci id="lstnumberx20.1.m1.1.1.2.cmml" xref="lstnumberx20.1.m1.1.1.2">𝑠</ci><cn id="lstnumberx20.1.m1.1.1.3.cmml" type="integer" xref="lstnumberx20.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx20.1.m1.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx20.1.m1.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.21" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.22" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.23" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.24" style="font-size:70%;">12</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.25" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.26" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.27" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.2" style="font-size:70%;"><math alttext="s_{1}" class="ltx_Math" display="inline" id="lstnumberx20.2.m1.1"><semantics id="lstnumberx20.2.m1.1a"><msub id="lstnumberx20.2.m1.1.1" xref="lstnumberx20.2.m1.1.1.cmml"><mi id="lstnumberx20.2.m1.1.1.2" xref="lstnumberx20.2.m1.1.1.2.cmml">s</mi><mn id="lstnumberx20.2.m1.1.1.3" xref="lstnumberx20.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx20.2.m1.1b"><apply id="lstnumberx20.2.m1.1.1.cmml" xref="lstnumberx20.2.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx20.2.m1.1.1.1.cmml" xref="lstnumberx20.2.m1.1.1">subscript</csymbol><ci id="lstnumberx20.2.m1.1.1.2.cmml" xref="lstnumberx20.2.m1.1.1.2">𝑠</ci><cn id="lstnumberx20.2.m1.1.1.3.cmml" type="integer" xref="lstnumberx20.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx20.2.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx20.2.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.28" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.29" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.30" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.31" style="font-size:70%;">8</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.32" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.33" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.34" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.3" style="font-size:70%;"><math alttext="s_{2}" class="ltx_Math" display="inline" id="lstnumberx20.3.m1.1"><semantics id="lstnumberx20.3.m1.1a"><msub id="lstnumberx20.3.m1.1.1" xref="lstnumberx20.3.m1.1.1.cmml"><mi id="lstnumberx20.3.m1.1.1.2" xref="lstnumberx20.3.m1.1.1.2.cmml">s</mi><mn id="lstnumberx20.3.m1.1.1.3" xref="lstnumberx20.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx20.3.m1.1b"><apply id="lstnumberx20.3.m1.1.1.cmml" xref="lstnumberx20.3.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx20.3.m1.1.1.1.cmml" xref="lstnumberx20.3.m1.1.1">subscript</csymbol><ci id="lstnumberx20.3.m1.1.1.2.cmml" xref="lstnumberx20.3.m1.1.1.2">𝑠</ci><cn id="lstnumberx20.3.m1.1.1.3.cmml" type="integer" xref="lstnumberx20.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx20.3.m1.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx20.3.m1.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.35" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.36" style="font-size:70%;">&lt;&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.37" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.38" style="font-size:70%;">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.39" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.40" style="font-size:70%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.41" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.4" style="font-size:70%;"><math alttext="s_{3}" class="ltx_Math" display="inline" id="lstnumberx20.4.m1.1"><semantics id="lstnumberx20.4.m1.1a"><msub id="lstnumberx20.4.m1.1.1" xref="lstnumberx20.4.m1.1.1.cmml"><mi id="lstnumberx20.4.m1.1.1.2" xref="lstnumberx20.4.m1.1.1.2.cmml">s</mi><mn id="lstnumberx20.4.m1.1.1.3" xref="lstnumberx20.4.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="lstnumberx20.4.m1.1b"><apply id="lstnumberx20.4.m1.1.1.cmml" xref="lstnumberx20.4.m1.1.1"><csymbol cd="ambiguous" id="lstnumberx20.4.m1.1.1.1.cmml" xref="lstnumberx20.4.m1.1.1">subscript</csymbol><ci id="lstnumberx20.4.m1.1.1.2.cmml" xref="lstnumberx20.4.m1.1.1.2">𝑠</ci><cn id="lstnumberx20.4.m1.1.1.3.cmml" type="integer" xref="lstnumberx20.4.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="lstnumberx20.4.m1.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="lstnumberx20.4.m1.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.42" style="font-size:70%;">;</span>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float">Script 1: </span>Parallel decoding for UTF-8</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.10">Script <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#LST1" title="Script 1 ‣ 3.3. Efficient Raw Dataset Transformation ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a> shows the parallel decoding process. The module’s input is four bytes, and we split them into 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mo id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><times id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">×</annotation></semantics></math>8-bit sub-inputs as <math alttext="s_{0}" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">s</mi><mn id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑠</ci><cn id="S3.SS3.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">s_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.SS3.p3.5.3">, <math alttext="s_{1}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.1.m1.1"><semantics id="S3.SS3.p3.3.1.m1.1a"><msub id="S3.SS3.p3.3.1.m1.1.1" xref="S3.SS3.p3.3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.3.1.m1.1.1.2" xref="S3.SS3.p3.3.1.m1.1.1.2.cmml">s</mi><mn id="S3.SS3.p3.3.1.m1.1.1.3" xref="S3.SS3.p3.3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.1.m1.1b"><apply id="S3.SS3.p3.3.1.m1.1.1.cmml" xref="S3.SS3.p3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.3.1.m1.1.1.2">𝑠</ci><cn id="S3.SS3.p3.3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.1.m1.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.1.m1.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="s_{2}" class="ltx_Math" display="inline" id="S3.SS3.p3.4.2.m2.1"><semantics id="S3.SS3.p3.4.2.m2.1a"><msub id="S3.SS3.p3.4.2.m2.1.1" xref="S3.SS3.p3.4.2.m2.1.1.cmml"><mi id="S3.SS3.p3.4.2.m2.1.1.2" xref="S3.SS3.p3.4.2.m2.1.1.2.cmml">s</mi><mn id="S3.SS3.p3.4.2.m2.1.1.3" xref="S3.SS3.p3.4.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.2.m2.1b"><apply id="S3.SS3.p3.4.2.m2.1.1.cmml" xref="S3.SS3.p3.4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.2.m2.1.1.1.cmml" xref="S3.SS3.p3.4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.2.m2.1.1.2.cmml" xref="S3.SS3.p3.4.2.m2.1.1.2">𝑠</ci><cn id="S3.SS3.p3.4.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p3.4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.2.m2.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.2.m2.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> &amp; <math alttext="s_{3}" class="ltx_Math" display="inline" id="S3.SS3.p3.5.3.m3.1"><semantics id="S3.SS3.p3.5.3.m3.1a"><msub id="S3.SS3.p3.5.3.m3.1.1" xref="S3.SS3.p3.5.3.m3.1.1.cmml"><mi id="S3.SS3.p3.5.3.m3.1.1.2" xref="S3.SS3.p3.5.3.m3.1.1.2.cmml">s</mi><mn id="S3.SS3.p3.5.3.m3.1.1.3" xref="S3.SS3.p3.5.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.3.m3.1b"><apply id="S3.SS3.p3.5.3.m3.1.1.cmml" xref="S3.SS3.p3.5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.3.m3.1.1.1.cmml" xref="S3.SS3.p3.5.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.3.m3.1.1.2.cmml" xref="S3.SS3.p3.5.3.m3.1.1.2">𝑠</ci><cn id="S3.SS3.p3.5.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p3.5.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.3.m3.1c">s_{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.3.m3.1d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span>.
We store the cached value in the register as <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.7">v</span>.
We set four possible outputs as <math alttext="o_{0}" class="ltx_Math" display="inline" id="S3.SS3.p3.6.m3.1"><semantics id="S3.SS3.p3.6.m3.1a"><msub id="S3.SS3.p3.6.m3.1.1" xref="S3.SS3.p3.6.m3.1.1.cmml"><mi id="S3.SS3.p3.6.m3.1.1.2" xref="S3.SS3.p3.6.m3.1.1.2.cmml">o</mi><mn id="S3.SS3.p3.6.m3.1.1.3" xref="S3.SS3.p3.6.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m3.1b"><apply id="S3.SS3.p3.6.m3.1.1.cmml" xref="S3.SS3.p3.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m3.1.1.1.cmml" xref="S3.SS3.p3.6.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.6.m3.1.1.2.cmml" xref="S3.SS3.p3.6.m3.1.1.2">𝑜</ci><cn id="S3.SS3.p3.6.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p3.6.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m3.1c">o_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.6.m3.1d">italic_o start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.SS3.p3.9.6">, <math alttext="o_{1}" class="ltx_Math" display="inline" id="S3.SS3.p3.7.4.m1.1"><semantics id="S3.SS3.p3.7.4.m1.1a"><msub id="S3.SS3.p3.7.4.m1.1.1" xref="S3.SS3.p3.7.4.m1.1.1.cmml"><mi id="S3.SS3.p3.7.4.m1.1.1.2" xref="S3.SS3.p3.7.4.m1.1.1.2.cmml">o</mi><mn id="S3.SS3.p3.7.4.m1.1.1.3" xref="S3.SS3.p3.7.4.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.4.m1.1b"><apply id="S3.SS3.p3.7.4.m1.1.1.cmml" xref="S3.SS3.p3.7.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.4.m1.1.1.1.cmml" xref="S3.SS3.p3.7.4.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.7.4.m1.1.1.2.cmml" xref="S3.SS3.p3.7.4.m1.1.1.2">𝑜</ci><cn id="S3.SS3.p3.7.4.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.7.4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.4.m1.1c">o_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.7.4.m1.1d">italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="o_{2}" class="ltx_Math" display="inline" id="S3.SS3.p3.8.5.m2.1"><semantics id="S3.SS3.p3.8.5.m2.1a"><msub id="S3.SS3.p3.8.5.m2.1.1" xref="S3.SS3.p3.8.5.m2.1.1.cmml"><mi id="S3.SS3.p3.8.5.m2.1.1.2" xref="S3.SS3.p3.8.5.m2.1.1.2.cmml">o</mi><mn id="S3.SS3.p3.8.5.m2.1.1.3" xref="S3.SS3.p3.8.5.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.5.m2.1b"><apply id="S3.SS3.p3.8.5.m2.1.1.cmml" xref="S3.SS3.p3.8.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.8.5.m2.1.1.1.cmml" xref="S3.SS3.p3.8.5.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.8.5.m2.1.1.2.cmml" xref="S3.SS3.p3.8.5.m2.1.1.2">𝑜</ci><cn id="S3.SS3.p3.8.5.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p3.8.5.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.5.m2.1c">o_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.8.5.m2.1d">italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> &amp; <math alttext="o_{3}" class="ltx_Math" display="inline" id="S3.SS3.p3.9.6.m3.1"><semantics id="S3.SS3.p3.9.6.m3.1a"><msub id="S3.SS3.p3.9.6.m3.1.1" xref="S3.SS3.p3.9.6.m3.1.1.cmml"><mi id="S3.SS3.p3.9.6.m3.1.1.2" xref="S3.SS3.p3.9.6.m3.1.1.2.cmml">o</mi><mn id="S3.SS3.p3.9.6.m3.1.1.3" xref="S3.SS3.p3.9.6.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.6.m3.1b"><apply id="S3.SS3.p3.9.6.m3.1.1.cmml" xref="S3.SS3.p3.9.6.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.9.6.m3.1.1.1.cmml" xref="S3.SS3.p3.9.6.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.9.6.m3.1.1.2.cmml" xref="S3.SS3.p3.9.6.m3.1.1.2">𝑜</ci><cn id="S3.SS3.p3.9.6.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p3.9.6.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.6.m3.1c">o_{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.9.6.m3.1d">italic_o start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></span>.
First, we count how many <math alttext="\backslash t" class="ltx_Math" display="inline" id="S3.SS3.p3.10.m4.1"><semantics id="S3.SS3.p3.10.m4.1a"><mrow id="S3.SS3.p3.10.m4.1.1" xref="S3.SS3.p3.10.m4.1.1.cmml"><mi id="S3.SS3.p3.10.m4.1.1.2" xref="S3.SS3.p3.10.m4.1.1.2.cmml"></mi><mo id="S3.SS3.p3.10.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p3.10.m4.1.1.1.cmml">\</mo><mi id="S3.SS3.p3.10.m4.1.1.3" xref="S3.SS3.p3.10.m4.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.10.m4.1b"><apply id="S3.SS3.p3.10.m4.1.1.cmml" xref="S3.SS3.p3.10.m4.1.1"><ci id="S3.SS3.p3.10.m4.1.1.1.cmml" xref="S3.SS3.p3.10.m4.1.1.1">\</ci><csymbol cd="latexml" id="S3.SS3.p3.10.m4.1.1.2.cmml" xref="S3.SS3.p3.10.m4.1.1.2">absent</csymbol><ci id="S3.SS3.p3.10.m4.1.1.3.cmml" xref="S3.SS3.p3.10.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.10.m4.1c">\backslash t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.10.m4.1d">\ italic_t</annotation></semantics></math> exist in the input because it determines the number of valid outputs.
There are in total 16 combinations: four outputs (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.8">0b1111</span>), three outputs (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.9">0b1110, 1101, 1011, 0111</span>), two outputs (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.10">0b1100, 1010, 1001, 0110, 0101, 0011</span>), one output (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.11">0b1000, 0100, 0010, 0001</span>) and no output (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.12">0b0000</span>), as shown in Script <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#LST1" title="Script 1 ‣ 3.3. Efficient Raw Dataset Transformation ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">1</span></a>.
We use the four-byte version in the final design to increase the efficiency of <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.13">Decode</span>, and in this case, <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.10.14">GenVocab</span> becomes the bottleneck which runs slower due to the requirement of updating in two cycles for each feature.
Decoding eight bytes or higher in parallel is also feasible, which would contain 256 combinations or we can assemble it from two four-byte versions.
</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Besides, dataset compression/decompression is a popular topic for hardware accelerator <cite class="ltx_cite ltx_citemacro_citep">(Bartík et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib9" title="">2015</a>; Rigler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib67" title="">2007</a>; Ledwon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib45" title="">2020</a>; Peltenburg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib60" title="">2020</a>)</cite>. For example, converting storage-focused file formats Apache Parquet to in-memory data structures Apache Arrow has drawn the attention of FPGA community <cite class="ltx_cite ltx_citemacro_citep">(Peltenburg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib60" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib61" title="">2019</a>)</cite>, which helps generate the output in line-rate.
In this paper, we omit the discussion of compressing binary file or not.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>System Integration</h3>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F7.1">
<div class="ltx_block" id="S3.F7.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="58" id="S3.F7.1.g1" src="extracted/5872791/images/architecture_1.png" width="299"/><span class="ltx_ERROR undefined" id="S3.F7.1.1.1">\phantomsubcaption</span>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F7.2">
<div class="ltx_block" id="S3.F7.2.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="58" id="S3.F7.2.g1" src="extracted/5872791/images/architecture_2.png" width="299"/><span class="ltx_ERROR undefined" id="S3.F7.2.1.1">\phantomsubcaption</span>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F7.3">
<div class="ltx_block" id="S3.F7.3.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="56" id="S3.F7.3.g1" src="extracted/5872791/images/architecture_3.png" width="299"/><span class="ltx_ERROR undefined" id="S3.F7.3.1.1">\phantomsubcaption</span>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F7.4">
<div class="ltx_block" id="S3.F7.4.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="85" id="S3.F7.4.g1" src="extracted/5872791/images/architecture_4.png" width="299"/><span class="ltx_ERROR undefined" id="S3.F7.4.1.1">\phantomsubcaption</span>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.6.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S3.F7.7.2" style="font-size:90%;">Data movements and processing patterns. Traditional: (a). Ours: (b), (c), (d).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> presents four different communication patterns between CPU and FPGA to conduct data preprocessing.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> shows the architecture of the conventional CPU-based preprocessing pipeline.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> displays the conventional way of offloading all operations into FPGA, which serves as a complementary accelerator to CPUs.
CPU is responsible for data loading, first storing data in its buffer and transferring it to FPGA’s off-chip memory via the PCIe channel.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates that we can enable co-processing between CPU and FPGA for higher performance.
Now, we partition the pipeline into two parts, finishing the data decoding in CPU and transferring decoded data to FPGA to finish the rest of the operations.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> shows the network-based solution of <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p1.1.1">Piper</span> for online preprocessing where the host-side logic is removed and final results are transferred to ML accelerators for further training/inference directly.
This network-based configuration not only improves end-to-end processing performance by eliminating host-side overhead but also enables streaming data processing, supporting datasets larger than the FPGA’s memory capacity.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS1.1.1">Piper</span> as a Local Accelerator</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">The execution of <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS1.p1.1.1">Piper</span> in the local environment shows the effectiveness of offloading preprocessing tasks into FPGA.
However, it also exposes some shortcomings compared to the network-attached mode, as we will explain below.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Kernel launching procedure.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> shows how we start by offloading all operations to the FPGA as a PCIe-attached accelerator, initiating with the decoding of UTF-8 data. As described earlier, FPGAs cannot read data directly from the disk.
Following the common procedure, we create a buffer on the host server, load data into this buffer, and transfer it to the corresponding off-chip memory channels of the FPGA.
The disadvantage of this architecture is that the buffer-related operations are costly, as we will present in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4" title="4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4</span></a>. This is because such a communication pattern is that all these stages must execute in sequence, and there is no overlap among them to help increase throughput.
Besides, as <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p2.1.2">Decode</span> cannot fully utilize all memory channels, the dataflow processes one feature at a time rather than processing the entire row. The valid width of streams between modules is limited to 32-bit.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p3.1.1">Relocate decoding to CPU.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the communication pattern for the co-processing between the host (CPU) and the kernel (FPGA).
Offloading all operations into FPGA limits the degree of parallelism as we can at most handle one feature at a time due to the low throughput of <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p3.1.2">Decode</span>.
An intriguing point of comparison arises when relocating the <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p3.1.3">Decode</span> function to the CPU and subsequently transferring the decoded data into FPGA’s off-chip memory for further processing.
When making this comparison, we consistently consider the total execution time for both the host and the kernel.
We achieve maximum parallelism by segregating sparse and dense features into separate memory channels, digesting 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.1.m1.1"><semantics id="S3.SS4.SSS1.p3.1.m1.1a"><mo id="S3.SS4.SSS1.p3.1.m1.1.1" xref="S3.SS4.SSS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.1.m1.1b"><times id="S3.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p3.1.m1.1d">×</annotation></semantics></math>512 bits per cycle from DDR.
Consequently, individual column processing emerges as the optimal choice, and FPGA benefits from this kind of column-based parallelism.
Currently, one DDR channel reads the label key and dense features (1+13 integers, yielding 448 valid bits from one 512-bit wide memory lane and setting any invalid bits to 0), while two DDR channels handle sparse features (26 integers, producing 832/1024 valid bits from two memory lanes, also setting invalid bits to 0). This arrangement enables division into independent sub-tasks to handle each input feature in parallel.
Leveraging FPGA’s streaming capabilities ensures seamless integration of results from all features, regardless of varying feature processing latency.
When we put <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p3.1.4">Decode</span> function in the attached CPU, the overall dataflow in the kernel is the same as in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F5" title="Figure 5 ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">5</span></a>. The difference is that we can now increase the number of PEs to maximize parallelism.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p4.1.1">Binary Input &amp; Burst Read</span>.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> displays the difference of executing <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p4.1.2">Decode</span> in the host or in the kernel, and one significant difference is the valid memory throughput limited by <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p4.1.3">Decode</span>.
Here, we explore the potential of utilizing a binary dataset as the input because the overhead introduced by the decoding process is unnecessary when the content is binary.
To highlight the positive effect, we do thorough experiments based on the pre-decoded binary dataset, followed by the same procedure in section <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS3" title="2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
We maximize the performance with parallel dataflows, and the communication pattern is the same as Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a>, eliminating the need for an additional decoding operator.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4" title="4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4</span></a>, We test the performance of the CPU/GPU with binary input for a fair comparison, and we further demonstrate the inefficiency challenges posed by decoding UTF-8.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS2.1.1">Piper</span> as a Network-attached Accelerator</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> presents a solution leveraging disaggregated storage and computation through a high-throughput network to overlap data movement with computing operation, inspired by previous work that disaggregate storage &amp; compute <cite class="ltx_cite ltx_citemacro_citep">(Klimovic et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib39" title="">2016</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib87" title="">2019</a>; Angel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib5" title="">2020</a>; Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib62" title="">2020</a>; Korolija et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib40" title="">2021</a>; Koshiba et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib42" title="">2023</a>)</cite>
and SmartNIC <cite class="ltx_cite ltx_citemacro_citep">(Firestone et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib22" title="">2018</a>; Tokusashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib73" title="">2019</a>; Eran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib20" title="">2019</a>; Lazarev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib44" title="">2020</a>; Alvarez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib3" title="">2020</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib34" title="">2021</a>; Brunella et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib11" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib76" title="">2022a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib31" title="">2023</a>)</cite>.
In this way, we integrate the data loading process into the whole dataflow, and now all steps run in a fully pipelined method. In this setup, the datasets are sent to the target FPGA for preprocessing via the network. After data preprocessing, FPGA can send the results to CPUs, which can then either write them back to disk or forward them to other ML inference or training accelerators such as GPUs.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.1">Advantages.</span> Such a network-attached design offers several advantages over the local accelerator architecture in terms of flexibility.
First, the network-attached design avoids the host-side processing, which involves expensive operations including allocating a large buffer and data movements between disks, CPUs, and FPGAs.
Second, the FPGA can process large-than memory datasets in a streaming fashion, without the need of storing the entire dataset in the FPGA memory before the processing.
Thirdly, the disaggregated architecture offers the flexibility scale the number of FPGAs (preprocessing) and GPUs (training) individually according to various performance requirements.
Finally, with a simple network-based interface, <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS2.p2.1.2">Piper</span> can be integrated into future ML systems seamlessly for online data preprocessing,</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Piper</span> to answer these questions:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">I.</span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1">How much performance advantage can <span class="ltx_text ltx_font_smallcaps" id="S4.I1.ix1.p1.1.1">Piper</span> gain over multi-core CPUs and data-center GPUs? Can <span class="ltx_text ltx_font_smallcaps" id="S4.I1.ix1.p1.1.2">Piper</span> speed up all the operators in the pipeline? <math alttext="\S" class="ltx_Math" display="inline" id="S4.I1.ix1.p1.1.m1.1"><semantics id="S4.I1.ix1.p1.1.m1.1a"><mi id="S4.I1.ix1.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.ix1.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p1.1.m1.1b"><ci id="S4.I1.ix1.p1.1.m1.1.1.cmml" xref="S4.I1.ix1.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p1.1.m1.1c">\S</annotation><annotation encoding="application/x-llamapun" id="S4.I1.ix1.p1.1.m1.1d">§</annotation></semantics></math> <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS6" title="4.4.6. Throughput of Pure Computation ‣ 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.6</span></a></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">II.</span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p" id="S4.I1.ix2.p1.1">Can <span class="ltx_text ltx_font_smallcaps" id="S4.I1.ix2.p1.1.1">Piper</span> gain extra performance by exposing the network interface? Can the performance meet the requirements for online training? <math alttext="\S" class="ltx_Math" display="inline" id="S4.I1.ix2.p1.1.m1.1"><semantics id="S4.I1.ix2.p1.1.m1.1a"><mi id="S4.I1.ix2.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.ix2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p1.1.m1.1b"><ci id="S4.I1.ix2.p1.1.m1.1.1.cmml" xref="S4.I1.ix2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p1.1.m1.1c">\S</annotation><annotation encoding="application/x-llamapun" id="S4.I1.ix2.p1.1.m1.1d">§</annotation></semantics></math> <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS5" title="4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.5</span></a></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">III.</span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p" id="S4.I1.ix3.p1.1">What are the pros and cons of using HBM as a cache and using binary datasets as input? <math alttext="\S" class="ltx_Math" display="inline" id="S4.I1.ix3.p1.1.m1.1"><semantics id="S4.I1.ix3.p1.1.m1.1a"><mi id="S4.I1.ix3.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.ix3.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix3.p1.1.m1.1b"><ci id="S4.I1.ix3.p1.1.m1.1.1.cmml" xref="S4.I1.ix3.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix3.p1.1.m1.1c">\S</annotation><annotation encoding="application/x-llamapun" id="S4.I1.ix3.p1.1.m1.1d">§</annotation></semantics></math> <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS3" title="4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS5" title="4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.5</span></a></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">IV.</span>
<div class="ltx_para" id="S4.I1.ix4.p1">
<p class="ltx_p" id="S4.I1.ix4.p1.1">What are the implications of allocating <span class="ltx_text ltx_font_italic" id="S4.I1.ix4.p1.1.1">Decode</span> to host or kernel? What can we learn from the local execution time breakdown? <math alttext="\S" class="ltx_Math" display="inline" id="S4.I1.ix4.p1.1.m1.1"><semantics id="S4.I1.ix4.p1.1.m1.1a"><mi id="S4.I1.ix4.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.ix4.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix4.p1.1.m1.1b"><ci id="S4.I1.ix4.p1.1.m1.1.1.cmml" xref="S4.I1.ix4.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix4.p1.1.m1.1c">\S</annotation><annotation encoding="application/x-llamapun" id="S4.I1.ix4.p1.1.m1.1d">§</annotation></semantics></math> <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS4" title="4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS2" title="4.4.2. Piper with Decoder ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.2</span></a></p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Dataset</span>.
We evaluate the data preprocessing on the Criteo Kaggle dataset <cite class="ltx_cite ltx_citemacro_citep">(Criteo, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib15" title="">2024</a>)</cite>, a well-known DLRM dataset containing online advertising data for seven days.
Each row contains one label key, 13 dense features (such as the number of times a user clicks on an advertisement), and 26 sparse features (anonymous and hashed string values representing various categorical information about the ads, user, and context).
The raw UTF-8 dataset is 11GB, and the decoded binary dataset is 8.2GB.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Software Configuration</span>.
For the Meta baseline, we make optimizations for their native data preprocessing module <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib51" title="">2024</a>)</cite> as the baseline.
We execute the Google pipeline based on their Apache Beam implementation <cite class="ltx_cite ltx_citemacro_citep">(Tensorflow, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib72" title="">2024</a>)</cite> that is integrated within Google Cloud.
For the GPU part, we rent NVidia 16GB V100 in Google Cloud.
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p2.1.2">Piper</span>, we use Vitis HLS 2022.1 to compile the bitstream.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Hardware</span>.
We evaluate Meta’s CPU baseline on a two-socket server with AMD EPYC 7V13 CPU (128 cores in total without hyperthreading) and 512 GB DRAM.
We evaluate Google’s preprocessing pipeline using a Google Cloud instance (c2d-highcpu-32) with AMD EPYC 7B13 16-core (32 threads) and 64 GB DRAM.
We run GPU experiments in Nvidia 16GB-HBM2 V100, attached with Intel Skylake N1 12 vCPUs and 64 GB DRAM.
We use two different FPGAs for <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.2">Piper</span> as local and network-attached accelerators, respectively, due to memory capacity considerations: using <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.3">Piper</span> as a local accelerator requires larger memory capacity to store input and output datasets, whereas the network-attached accelerator processes data in a streaming fashion without the need for large memory capacity.
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.4">Piper</span> as a local accelerator, we use Xilinx Alveo U250 that is equipped with 64GB DDR (4 memory channels, maximum throughput 77GB/s) and 54MB SRAM. The attached CPU is an Intel Xeon 16-core Processor (Cascadelake).
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.5">Piper</span> with the network interface, we use Xilinx Alveo U55c equipped with 16GB HBM (32 memory channels, maximum throughput 460GB/s) and 43MB SRAM.
The attached CPU is an AMD EPYC 7302P 16-core Processor with 32 hyper-threads.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Optimized CPU Baseline</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To report the best baseline performance, we have made several optimizations to mitigate some unnecessary overheads.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Optimizing Meta’s DLRM Preprocessing</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We show the effect of step-by-step optimizations for the baseline, including removing I/O overhead for input and output datasets, caching intermediate states in memory, and using decoded binary input datasets. The I/O overhead is similar in all designs, CPU or FPGA, with the difference that the FPGA network version can process data at line rate without copying to memory, which gives it a significant advantage. Thus, in the evaluation we focus solely on the processing performance of both approaches.
We use three versions of the baseline.
</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">Config I</span>.
In this setup, we assume the input dataset is loaded from memory instead of from disk, and the results are written back to memory as well, such that the I/O overheads do not count into the end-to-end processing latency.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">Config II</span>.
Developed upon Config I, we further use in-memory buffers to store intermediate results.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.F3" title="Figure 3 ‣ 2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the program needs to frequently generate intermediate results between different stages and store them back into the disk, which introduces extra I/O overhead, especially when the disk bandwidth is low.
This is an important step for extra-large dataset, but considering that all required data can entirely reside in the CPU’s DRAM, we use in-memory buffers to store the intermediate states instead.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p4.1.1">Config III</span>.
Building upon Config II, we assume that the dataset is already decoded and in a ready-to-use binary format. Although most datasets are typically encoded in UTF-8, it is possible that the dataset has been previously processed and stored as a binary dataset.
For the CPU baseline, we need to unpack and map binary input to be expected tuples.
The overhead partially counteracts the positive impact of removing the decoding function.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="231" id="S4.F8.sf1.g1" src="extracted/5872791/images/cpu_baseline_5k.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F8.sf1.3.2" style="font-size:90%;">CPU baseline with vocab 5K.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="231" id="S4.F8.sf2.g1" src="extracted/5872791/images/cpu_baseline_1m.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F8.sf2.3.2" style="font-size:90%;">CPU baseline with vocab 1M.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">CPU baseline with different vocabulary sizes. The bars with numbers represents the best performance.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F8" title="Figure 8 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">8</span></a> compares the CPU performance for the three configurations, showcasing the effectiveness of our step-by-step optimizations.
The two sub-figures show the performance given different vocabulary sizes of 5K and 1M, respectively.
We split the whole process into four stages as illustrated in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S2.SS3" title="2.3. CPU-based Preprocessing Pipelines ‣ 2. Background and Motivation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">2.3</span></a>, including <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.1">Split Input File</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.2">Generate Vocab</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.3">Apply Vocab</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.4">Concatenate</span>.
These stages run sequentially and exhibit different scalability with multiple threads.
The figure compares the performance with different numbers of threads for the same setup, neglecting the cases with few threads due to the long execution time.
For all cases, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F8" title="Figure 8 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">8</span></a> demonstrates that the preprocessing performance does not scale linearly with the number of threads.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p6">
<p class="ltx_p" id="S4.SS2.SSS1.p6.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F8.sf1" title="In Figure 8 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">8(a)</span></a> shows the performance gains of different baseline configurations given a small vocabulary size of 5K.
Both Config I and II work on the UTF-8 input datasets, while Config III works on the decoded binary datasets.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p7">
<p class="ltx_p" id="S4.SS2.SSS1.p7.1">For Config I that writes all intermediate results back to disk, we make several observations.
Firstly, the initialization of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p7.1.1">Split Input File</span> remains constant because the overhead is dominated by reading data rather than distributing it to different sub-files.
Secondly, the execution time of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p7.1.2">Generate Vocab</span> keeps halving until the number of threads reaches 64, with only slight improvements beyond 64 threads.
Thirdly, the execution time of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p7.1.3">Apply Vocab</span> continues to halve up to 128 threads.
Fourthly, the execution time of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p7.1.4">Concatenate</span> keeps doubling as the number of sub-files increases. This indicates that the overhead is dominated by the calls to read each sub-file rather than the reading process itself.
Finally, it achieves the best performance with 64 threads.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p8">
<p class="ltx_p" id="S4.SS2.SSS1.p8.1">For Config II, which allows intermediate results to be written to memory as well, we have several observations.
Firstly, the initialization of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p8.1.1">Split Input File</span> remains constant.
Secondly, the performance of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p8.1.2">Generate Vocab</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p8.1.3">Apply Vocab</span> saturates at 64 threads.
For 64 and 128 threads, these two stages take significantly longer than in Config I, even though intermediate results are stored in DRAM.
One possible reason is that a shared dictionary is introduced for row-wise multi-processing, and the synchronization overhead is significant when too many threads are involved.
Thirdly, the execution time of the <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p8.1.4">Concatenate</span> stage doubles with the increase in threads. Compared to Config I, the cost for the same number of threads is smaller, which can be attributed to the in-memory storage of intermediate results after <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p8.1.5">Apply Vocab</span>.
Finally, Config II achieves the best performance with 32 threads.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p9">
<p class="ltx_p" id="S4.SS2.SSS1.p9.1">For Config III, which uses a decoded binary dataset as input, we make several observations.
Firstly, the initialization of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p9.1.1">Split Input File</span> remains constant but is much shorter than in the other two configurations. Since the input is now a binary dataset, we omit the step of explicitly counting the number of rows with a loop; instead, we simply obtain the file size and calculate it.
Secondly, the performance of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p9.1.2">Generate Vocab</span> is nearly the same as in Config I. Within the reading loop, we unpack the binary input into corresponding tuples.
Thirdly, the overheads for <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p9.1.3">Apply Vocab</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p9.1.4">Concatenate</span> are nearly the same as in Config I.
Finally, Config III achieves the best performance with 32 threads.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p10">
<p class="ltx_p" id="S4.SS2.SSS1.p10.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F8.sf2" title="In Figure 8 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">8(b)</span></a> shows that the processing latency increases given a larger vocabulary size of 1M, mainly due to the random mapping process of input features to the corresponding vocabulary.
Firstly, Config I now reaches the best performance with 32 threads, while Config II and III reach the best performance with 16 threads.
Secondly, for <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p10.1.1">Split Input File</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p10.1.2">Apply Vocab</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p10.1.3">Concatenate</span>, the performance stays the same as Vocab 5K.
Thirdly, for <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p10.1.4">Generate Vocab</span>, the speedup of multi-threading is not prominent because the software needs to maintain a much larger dictionary to support multi-processing, and the resulting synchronization overhead also increases with the number of threads.</p>
</div>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="14" id="S4.F9.1.g1" src="extracted/5872791/images/legend_figure.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="392" id="S4.F9.sf1.g1" src="extracted/5872791/images/utf8_5k.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F9.sf1.3.2" style="font-size:90%;">UTF-8, 5K</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="599" id="S4.F9.sf2.g1" src="extracted/5872791/images/utf8_1m.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F9.sf2.3.2" style="font-size:90%;">UTF-8, 1M</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="599" id="S4.F9.sf3.g1" src="extracted/5872791/images/binary_5k.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S4.F9.sf3.3.2" style="font-size:90%;">Binary, 5K</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="825" id="S4.F9.sf4.g1" src="extracted/5872791/images/binary_1m.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S4.F9.sf4.3.2" style="font-size:90%;">Binary, 1M</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.3.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S4.F9.4.2" style="font-size:90%;">Performance comparison between CPU, GPU and FPGA for various configurations.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Google’s DLRM Preprocessing in Cloud</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Google’s preprocessing pipeline is based on their Apache Beam implementation, which supports both local mode (Direct Runner) and distributed mode (Dataflow Runner).
The direct Runner mode in the local environment only aims to validate the pipeline, and it performs poorly in our local server.
The dataflow Runner works with Google Cloud.
In this experiment, we only consider the execution in Google Cloud using Dataflow Runner due to its higher performance.
We observe that the initialization overhead of Apache Beam pipeline is too high for both <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">Generate Vocab</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.2">Apply Vocab</span>.
To report the best baseline performance, we measure and deduct the initialization time from the reported performance.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Preprocessing in GPU</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We implement the preprocessing for DLRM in GPU with the support of Nvidia RAPIDS Suite, including <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">rmm, nvtabular, cudf</span> <cite class="ltx_cite ltx_citemacro_citep">(Nvidia, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib57" title="">2024b</a>)</cite>.
Its acceleration highly depends on the binary input format, like Parquet, so transforming the original dataset is a non-trivial step.
The following step is to initiate a WorkFlow, which defines the pipeline and then finishes the preprocessing for each column independently.
The top GPU utilization is over 85%.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS4.1.1">Piper</span>: Performance and Efficiency</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">We evaluate <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.1">Piper</span> to compare its performance with CPUs and GPUs.
Specifically, we compare the end-to-end data preprocessing performance between <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.2">Piper</span> and the optimized baselines, break down the performance for each operator, and show the performance implications of using <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p1.1.3">Piper</span> as either a local or a networked accelerator.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Evaluation configurations.</span> Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.T2" title="Table 2 ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">2</span></a> lists the configurations we used in the comparison.
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.2">Piper</span>, we focus on the network-based version due to its advantages of avoiding host-side execution overhead and its ability to process datasets larger than memory. We use the local mode only to verify the functionality of the dataflow for small vocabulary sizes.
For Google’s implementation, the generated binary dataset cannot be used as the input format, so we compare Google Cloud with <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.p2.1.3">Piper</span> exclusively for the UTF-8 dataset.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.21.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S4.T2.22.2" style="font-size:90%;">Configurations available of CPU, GPU, and <span class="ltx_text ltx_font_smallcaps" id="S4.T2.22.2.1">Piper</span>.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.18" style="width:391.0pt;height:118.7pt;vertical-align:-112.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.7pt,0.3pt) scale(0.9,0.9) ;"><span class="ltx_ERROR undefined" id="S4.T2.18.19">{tblr}</span>
<p class="ltx_p" id="S4.T2.18.18">colspec=c—c—c—c,
hline1,2,3,4,5,6,7,8,9,10,11,
cell21 = r=5m, cell71 = r=4m, 
Vocab &amp; Platform  UTF8  Binary
<br class="ltx_break"/>5K  CPU Local  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.1.1.m1.1"><semantics id="S4.T2.1.1.m1.1a"><mi id="S4.T2.1.1.m1.1.1" mathvariant="normal" xref="S4.T2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.m1.1b"><ci id="S4.T2.1.1.m1.1.1.cmml" xref="S4.T2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.m1.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.2.2.m2.1"><semantics id="S4.T2.2.2.m2.1a"><mi id="S4.T2.2.2.m2.1.1" mathvariant="normal" xref="S4.T2.2.2.m2.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.m2.1b"><ci id="S4.T2.2.2.m2.1.1.cmml" xref="S4.T2.2.2.m2.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.m2.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.m2.1d">✓</annotation></semantics></math>
<br class="ltx_break"/> CPU Cloud  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.3.3.m3.1"><semantics id="S4.T2.3.3.m3.1a"><mi id="S4.T2.3.3.m3.1.1" mathvariant="normal" xref="S4.T2.3.3.m3.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.m3.1b"><ci id="S4.T2.3.3.m3.1.1.cmml" xref="S4.T2.3.3.m3.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.m3.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.m3.1d">✓</annotation></semantics></math> <math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.4.4.m4.1"><semantics id="S4.T2.4.4.m4.1a"><mo id="S4.T2.4.4.m4.1.1" xref="S4.T2.4.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.m4.1b"><times id="S4.T2.4.4.m4.1.1.cmml" xref="S4.T2.4.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.m4.1d">×</annotation></semantics></math>
<br class="ltx_break"/> GPU Cloud  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.5.5.m5.1"><semantics id="S4.T2.5.5.m5.1a"><mi id="S4.T2.5.5.m5.1.1" mathvariant="normal" xref="S4.T2.5.5.m5.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.m5.1b"><ci id="S4.T2.5.5.m5.1.1.cmml" xref="S4.T2.5.5.m5.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.m5.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.m5.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.6.6.m6.1"><semantics id="S4.T2.6.6.m6.1a"><mi id="S4.T2.6.6.m6.1.1" mathvariant="normal" xref="S4.T2.6.6.m6.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.m6.1b"><ci id="S4.T2.6.6.m6.1.1.cmml" xref="S4.T2.6.6.m6.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.m6.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.m6.1d">✓</annotation></semantics></math>
<br class="ltx_break"/> FPGA Local  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.7.7.m7.1"><semantics id="S4.T2.7.7.m7.1a"><mi id="S4.T2.7.7.m7.1.1" mathvariant="normal" xref="S4.T2.7.7.m7.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.m7.1b"><ci id="S4.T2.7.7.m7.1.1.cmml" xref="S4.T2.7.7.m7.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.m7.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.m7.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.8.8.m8.1"><semantics id="S4.T2.8.8.m8.1a"><mi id="S4.T2.8.8.m8.1.1" mathvariant="normal" xref="S4.T2.8.8.m8.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.m8.1b"><ci id="S4.T2.8.8.m8.1.1.cmml" xref="S4.T2.8.8.m8.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.m8.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.m8.1d">✓</annotation></semantics></math>
<br class="ltx_break"/> FPGA Network  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.9.9.m9.1"><semantics id="S4.T2.9.9.m9.1a"><mi id="S4.T2.9.9.m9.1.1" mathvariant="normal" xref="S4.T2.9.9.m9.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.m9.1b"><ci id="S4.T2.9.9.m9.1.1.cmml" xref="S4.T2.9.9.m9.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.m9.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.m9.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.10.10.m10.1"><semantics id="S4.T2.10.10.m10.1a"><mi id="S4.T2.10.10.m10.1.1" mathvariant="normal" xref="S4.T2.10.10.m10.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.m10.1b"><ci id="S4.T2.10.10.m10.1.1.cmml" xref="S4.T2.10.10.m10.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.m10.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.m10.1d">✓</annotation></semantics></math>
<br class="ltx_break"/>1M  CPU Local  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.11.11.m11.1"><semantics id="S4.T2.11.11.m11.1a"><mi id="S4.T2.11.11.m11.1.1" mathvariant="normal" xref="S4.T2.11.11.m11.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.m11.1b"><ci id="S4.T2.11.11.m11.1.1.cmml" xref="S4.T2.11.11.m11.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.m11.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.m11.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.12.12.m12.1"><semantics id="S4.T2.12.12.m12.1a"><mi id="S4.T2.12.12.m12.1.1" mathvariant="normal" xref="S4.T2.12.12.m12.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.m12.1b"><ci id="S4.T2.12.12.m12.1.1.cmml" xref="S4.T2.12.12.m12.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.m12.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.m12.1d">✓</annotation></semantics></math>
<br class="ltx_break"/> CPU Cloud  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.13.13.m13.1"><semantics id="S4.T2.13.13.m13.1a"><mi id="S4.T2.13.13.m13.1.1" mathvariant="normal" xref="S4.T2.13.13.m13.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.m13.1b"><ci id="S4.T2.13.13.m13.1.1.cmml" xref="S4.T2.13.13.m13.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.m13.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.m13.1d">✓</annotation></semantics></math> <math alttext="\times" class="ltx_Math" display="inline" id="S4.T2.14.14.m14.1"><semantics id="S4.T2.14.14.m14.1a"><mo id="S4.T2.14.14.m14.1.1" xref="S4.T2.14.14.m14.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.m14.1b"><times id="S4.T2.14.14.m14.1.1.cmml" xref="S4.T2.14.14.m14.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.m14.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.m14.1d">×</annotation></semantics></math>
<br class="ltx_break"/> GPU Cloud  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.15.15.m15.1"><semantics id="S4.T2.15.15.m15.1a"><mi id="S4.T2.15.15.m15.1.1" mathvariant="normal" xref="S4.T2.15.15.m15.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.m15.1b"><ci id="S4.T2.15.15.m15.1.1.cmml" xref="S4.T2.15.15.m15.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.m15.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.m15.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.16.16.m16.1"><semantics id="S4.T2.16.16.m16.1a"><mi id="S4.T2.16.16.m16.1.1" mathvariant="normal" xref="S4.T2.16.16.m16.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.m16.1b"><ci id="S4.T2.16.16.m16.1.1.cmml" xref="S4.T2.16.16.m16.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.m16.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.m16.1d">✓</annotation></semantics></math>
<br class="ltx_break"/> FPGA Network  <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.17.17.m17.1"><semantics id="S4.T2.17.17.m17.1a"><mi id="S4.T2.17.17.m17.1.1" mathvariant="normal" xref="S4.T2.17.17.m17.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.m17.1b"><ci id="S4.T2.17.17.m17.1.1.cmml" xref="S4.T2.17.17.m17.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.m17.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.m17.1d">✓</annotation></semantics></math> <math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T2.18.18.m18.1"><semantics id="S4.T2.18.18.m18.1a"><mi id="S4.T2.18.18.m18.1.1" mathvariant="normal" xref="S4.T2.18.18.m18.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.m18.1b"><ci id="S4.T2.18.18.m18.1.1.cmml" xref="S4.T2.18.18.m18.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.m18.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.m18.1d">✓</annotation></semantics></math>
<br class="ltx_break"/></p>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>End-to-end Performance</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.7">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9" title="Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9</span></a> compares the end-to-end preprocessing performance between the CPU, GPU and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.1">Piper</span> for various configurations.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf1" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(a)</span></a> shows the performance gains of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.2">Piper</span> in both local and network mode for the UTF-8 dataset with a small vocabulary table.
When compared with the best performance in CPU, <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.3">Piper</span> achieves 2.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.1.m1.1"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><mo id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><times id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.1.m1.1d">×</annotation></semantics></math> and 2.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.2.m2.1"><semantics id="S4.SS4.SSS1.p1.2.m2.1a"><mo id="S4.SS4.SSS1.p1.2.m2.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.2.m2.1b"><times id="S4.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.2.m2.1d">×</annotation></semantics></math> speedup in local mode, respectively, depending on decoding in the kernel or the host, and achieves 5.1<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.3.m3.1"><semantics id="S4.SS4.SSS1.p1.3.m3.1a"><mo id="S4.SS4.SSS1.p1.3.m3.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.3.m3.1b"><times id="S4.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.3.m3.1d">×</annotation></semantics></math> speedup in network mode.
When compared with Google Cloud, the performance gain is much higher.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf2" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(b)</span></a> displays the acceleration of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.4">Piper</span> in network mode only for the UTF-8 dataset with a large vocabulary table.
Here, the CPU baseline in the local machine performs better than Google Cloud, and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.5">Piper</span> reaches 4.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.4.m4.1"><semantics id="S4.SS4.SSS1.p1.4.m4.1a"><mo id="S4.SS4.SSS1.p1.4.m4.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.4.m4.1b"><times id="S4.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.4.m4.1d">×</annotation></semantics></math> speedup over the most performant CPU baseline.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf3" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(c)</span></a> demonstrates the positive effect of using the binary dataset as input, which significantly increases the parallelism of the kernel.
Specifically, the speedups of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.6">Piper</span> over CPUs boost to 5.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.5.m5.1"><semantics id="S4.SS4.SSS1.p1.5.m5.1a"><mo id="S4.SS4.SSS1.p1.5.m5.1.1" xref="S4.SS4.SSS1.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.5.m5.1b"><times id="S4.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.5.m5.1d">×</annotation></semantics></math> and 71.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.6.m6.1"><semantics id="S4.SS4.SSS1.p1.6.m6.1a"><mo id="S4.SS4.SSS1.p1.6.m6.1.1" xref="S4.SS4.SSS1.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.6.m6.1b"><times id="S4.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS4.SSS1.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.6.m6.1d">×</annotation></semantics></math> respectively in local and network mode.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf4" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(d)</span></a> further indicates the performance enhancement of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.7">Piper</span> in network mode for the binary dataset with a large vocabulary table, where <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p1.7.8">Piper</span> obtains speedup by 25.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.7.m7.1"><semantics id="S4.SS4.SSS1.p1.7.m7.1a"><mo id="S4.SS4.SSS1.p1.7.m7.1.1" xref="S4.SS4.SSS1.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.7.m7.1b"><times id="S4.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS4.SSS1.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p1.7.m7.1d">×</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.4">Compared to GPU, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9" title="Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9</span></a> shows that it also reaches 2.6<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.1.m1.1"><semantics id="S4.SS4.SSS1.p2.1.m1.1a"><mo id="S4.SS4.SSS1.p2.1.m1.1.1" xref="S4.SS4.SSS1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.1.m1.1d">∼</annotation></semantics></math>5.4<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.2.m2.1"><semantics id="S4.SS4.SSS1.p2.2.m2.1a"><mo id="S4.SS4.SSS1.p2.2.m2.1.1" xref="S4.SS4.SSS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.2.m2.1b"><times id="S4.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.2.m2.1d">×</annotation></semantics></math> speedup when compared with the local CPU baseline.
When the input data format is UTF-8, <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p2.4.1">Piper</span> achieves slightly better performance.
However, when the input is binary format, the speedup of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS1.p2.4.2">Piper</span> grows significantly, ranging from 4.8<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.3.m3.1"><semantics id="S4.SS4.SSS1.p2.3.m3.1a"><mo id="S4.SS4.SSS1.p2.3.m3.1.1" xref="S4.SS4.SSS1.p2.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.3.m3.1b"><csymbol cd="latexml" id="S4.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.3.m3.1d">∼</annotation></semantics></math>20.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p2.4.m4.1"><semantics id="S4.SS4.SSS1.p2.4.m4.1a"><mo id="S4.SS4.SSS1.p2.4.m4.1.1" xref="S4.SS4.SSS1.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.4.m4.1b"><times id="S4.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p2.4.m4.1d">×</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS2.1.1">Piper</span> with Decoder</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">For <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS2.p1.1.1">Piper</span>, the default input data format is UTF-8 due to its prevalence in DLRM datasets.
To ensure ease of use and compatibility with this commonly used data format, we optimized the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.2">Decode</span> function as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.SS3" title="3.3. Efficient Raw Dataset Transformation ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">3.3</span></a> to increase the decoding throughput.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.2">While decoding on FPGA is faster compared to the CPU version, the <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p2.2.1">Decode</span> implementation still limits the degree of parallelism in the <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS2.p2.2.2">Piper</span> dataflow. Specifically, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf1" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(a)</span></a> shows that the acceleration ratio of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS2.p2.2.3">Piper</span> can reach 2.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p2.1.m1.1"><semantics id="S4.SS4.SSS2.p2.1.m1.1a"><mo id="S4.SS4.SSS2.p2.1.m1.1.1" xref="S4.SS4.SSS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p2.1.m1.1b"><times id="S4.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p2.1.m1.1d">×</annotation></semantics></math> compared to the best performance of a powerful server-level 128-core CPU, while the hybrid CPU-FPGA architecture only achieves 2.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS2.p2.2.m2.1"><semantics id="S4.SS4.SSS2.p2.2.m2.1a"><mo id="S4.SS4.SSS2.p2.2.m2.1.1" xref="S4.SS4.SSS2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p2.2.m2.1b"><times id="S4.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS2.p2.2.m2.1d">×</annotation></semantics></math> speedup over the CPU.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Maximizing Kernel Performance by Offloading Decoding</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.2">Maximizing the throughput of FPGA’s off-chip memory is essential to achieve the best performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS3.p1.2.1">Piper</span>, and there are two ways to achieve it.
Firstly, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf1" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(a)</span></a> illustrates that if we relocate <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS3.p1.2.2">Decode</span> in the host, the kernel execution time drops significantly. Nevertheless, the end-to-end speedup of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS3.p1.2.3">Piper</span> decreases to 2.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.p1.1.m1.1"><semantics id="S4.SS4.SSS3.p1.1.m1.1a"><mo id="S4.SS4.SSS3.p1.1.m1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.1.m1.1b"><times id="S4.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.p1.1.m1.1d">×</annotation></semantics></math> because the extra overhead in the host for decoding is significant for end-to-end execution.
Secondly, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf3" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(c)</span></a> reveals that if we use a pre-decoded binary dataset instead as the input, the speedup of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS3.p1.2.4">Piper</span> in the local mode increases to 5.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.p1.2.m2.1"><semantics id="S4.SS4.SSS3.p1.2.m2.1a"><mo id="S4.SS4.SSS3.p1.2.m2.1.1" xref="S4.SS4.SSS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.2.m2.1b"><times id="S4.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.p1.2.m2.1d">×</annotation></semantics></math> because the kernel execution benefits a lot from parallel PE design and the overhead in the host only involves data transmission.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S4.F10.g1" src="extracted/5872791/images/time_breakdown.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.3.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S4.F10.4.2" style="font-size:90%;">Time breakdown of <span class="ltx_text ltx_font_smallcaps" id="S4.F10.4.2.1">Piper</span> in local mode.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.SS4.SSS3.15">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.SS4.SSS3.15.16.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S4.SS4.SSS3.15.17.2" style="font-size:90%;">Throughput in row per second for pure computation (original &amp; config I: UTF-8, config II: binary).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS4.SSS3.15.15" style="width:371.8pt;height:163.2pt;vertical-align:-157.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.8pt,0.5pt) scale(0.85,0.85) ;"><span class="ltx_ERROR undefined" id="S4.SS4.SSS3.15.15.16">{tblr}</span>
<p class="ltx_p" id="S4.SS4.SSS3.15.15.17">colspec=cccccccccc,
cell11 = r=2m, cell12 = r=2m, cell13 = c=6c, cell19 = c=2c, cell31 = r=6m, cell32 = r=2m, cell52 = r=2m, cell72 = r=2m, cell91 = r=6m, cell92 = r=2m, cell112 = r=2m, cell132 = r=2m, 
Vocab  Dataset  CPU       FPGA   
<br class="ltx_break"/></p>
<p class="ltx_p" id="S4.SS4.SSS3.15.15.18">CPU-1  CPU-8  CPU-16  CPU-32  CPU-64  CPU-128  Local  Network
<br class="ltx_break"/></p>
<p class="ltx_p" id="S4.SS4.SSS3.9.9.9">5K  Config I  1.84E+4  1.32E+5  2.32E+5  4.32E+5  7.39E+5  9.75E+5  1.87E+6  1.56E+6
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.1.1.1.1">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.1.1.1.1.m1.1"><semantics id="S4.SS4.SSS3.1.1.1.1.m1.1a"><mo id="S4.SS4.SSS3.1.1.1.1.m1.1.1" xref="S4.SS4.SSS3.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.1.1.1.1.m1.1b"><times id="S4.SS4.SSS3.1.1.1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.1.1.1.1.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.2.2.2.2">1.92<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.2.2.2.2.m1.1"><semantics id="S4.SS4.SSS3.2.2.2.2.m1.1a"><mo id="S4.SS4.SSS3.2.2.2.2.m1.1.1" xref="S4.SS4.SSS3.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.2.2.2.2.m1.1b"><times id="S4.SS4.SSS3.2.2.2.2.m1.1.1.cmml" xref="S4.SS4.SSS3.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.2.2.2.2.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.3.3.3.3">1.60<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.3.3.3.3.m1.1"><semantics id="S4.SS4.SSS3.3.3.3.3.m1.1a"><mo id="S4.SS4.SSS3.3.3.3.3.m1.1.1" xref="S4.SS4.SSS3.3.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.3.3.3.3.m1.1b"><times id="S4.SS4.SSS3.3.3.3.3.m1.1.1.cmml" xref="S4.SS4.SSS3.3.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.3.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.3.3.3.3.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/>5K  Config II  4.02E+4  2.30E+5  3.27E+5  4.16E+5  4.82E+5  4.53E+5  1.87E+6  1.56E+6
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.4.4.4.4">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.4.4.4.4.m1.1"><semantics id="S4.SS4.SSS3.4.4.4.4.m1.1a"><mo id="S4.SS4.SSS3.4.4.4.4.m1.1.1" xref="S4.SS4.SSS3.4.4.4.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.4.4.4.4.m1.1b"><times id="S4.SS4.SSS3.4.4.4.4.m1.1.1.cmml" xref="S4.SS4.SSS3.4.4.4.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.4.4.4.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.4.4.4.4.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.5.5.5.5">3.88<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.5.5.5.5.m1.1"><semantics id="S4.SS4.SSS3.5.5.5.5.m1.1a"><mo id="S4.SS4.SSS3.5.5.5.5.m1.1.1" xref="S4.SS4.SSS3.5.5.5.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.5.5.5.5.m1.1b"><times id="S4.SS4.SSS3.5.5.5.5.m1.1.1.cmml" xref="S4.SS4.SSS3.5.5.5.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.5.5.5.5.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.5.5.5.5.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.6.6.6.6">3.24<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.6.6.6.6.m1.1"><semantics id="S4.SS4.SSS3.6.6.6.6.m1.1a"><mo id="S4.SS4.SSS3.6.6.6.6.m1.1.1" xref="S4.SS4.SSS3.6.6.6.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.6.6.6.6.m1.1b"><times id="S4.SS4.SSS3.6.6.6.6.m1.1.1.cmml" xref="S4.SS4.SSS3.6.6.6.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.6.6.6.6.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.6.6.6.6.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/> Config III  4.96E+4  2.61E+5  3.69E+5  4.67E+5  5.09E+5  4.92E+5  1.77E+7  2.36E+7 
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.7.7.7.7">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.7.7.7.7.m1.1"><semantics id="S4.SS4.SSS3.7.7.7.7.m1.1a"><mo id="S4.SS4.SSS3.7.7.7.7.m1.1.1" xref="S4.SS4.SSS3.7.7.7.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.7.7.7.7.m1.1b"><times id="S4.SS4.SSS3.7.7.7.7.m1.1.1.cmml" xref="S4.SS4.SSS3.7.7.7.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.7.7.7.7.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.7.7.7.7.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.8.8.8.8">34.77<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.8.8.8.8.m1.1"><semantics id="S4.SS4.SSS3.8.8.8.8.m1.1a"><mo id="S4.SS4.SSS3.8.8.8.8.m1.1.1" xref="S4.SS4.SSS3.8.8.8.8.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.8.8.8.8.m1.1b"><times id="S4.SS4.SSS3.8.8.8.8.m1.1.1.cmml" xref="S4.SS4.SSS3.8.8.8.8.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.8.8.8.8.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.8.8.8.8.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.9.9.9.9">46.36<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.9.9.9.9.m1.1"><semantics id="S4.SS4.SSS3.9.9.9.9.m1.1a"><mo id="S4.SS4.SSS3.9.9.9.9.m1.1.1" xref="S4.SS4.SSS3.9.9.9.9.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.9.9.9.9.m1.1b"><times id="S4.SS4.SSS3.9.9.9.9.m1.1.1.cmml" xref="S4.SS4.SSS3.9.9.9.9.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.9.9.9.9.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.9.9.9.9.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/></p>
<p class="ltx_p" id="S4.SS4.SSS3.15.15.15">1M  Config I  1.50E+4 1.08E+5  1.52E+5  1.93E+5  2.01E+5  1.98E+5   8.45E+5 
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.10.10.10.1">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.10.10.10.1.m1.1"><semantics id="S4.SS4.SSS3.10.10.10.1.m1.1a"><mo id="S4.SS4.SSS3.10.10.10.1.m1.1.1" xref="S4.SS4.SSS3.10.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.10.10.10.1.m1.1b"><times id="S4.SS4.SSS3.10.10.10.1.m1.1.1.cmml" xref="S4.SS4.SSS3.10.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.10.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.10.10.10.1.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.11.11.11.2">4.20<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.11.11.11.2.m1.1"><semantics id="S4.SS4.SSS3.11.11.11.2.m1.1a"><mo id="S4.SS4.SSS3.11.11.11.2.m1.1.1" xref="S4.SS4.SSS3.11.11.11.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.11.11.11.2.m1.1b"><times id="S4.SS4.SSS3.11.11.11.2.m1.1.1.cmml" xref="S4.SS4.SSS3.11.11.11.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.11.11.11.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.11.11.11.2.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/>1M  Config II  3.81E+4 1.71E+5  2.05E+5  2.06E+5  1.99E+5  1.83E+5   8.45E+5 
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.12.12.12.3">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.12.12.12.3.m1.1"><semantics id="S4.SS4.SSS3.12.12.12.3.m1.1a"><mo id="S4.SS4.SSS3.12.12.12.3.m1.1.1" xref="S4.SS4.SSS3.12.12.12.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.12.12.12.3.m1.1b"><times id="S4.SS4.SSS3.12.12.12.3.m1.1.1.cmml" xref="S4.SS4.SSS3.12.12.12.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.12.12.12.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.12.12.12.3.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.13.13.13.4">4.10<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.13.13.13.4.m1.1"><semantics id="S4.SS4.SSS3.13.13.13.4.m1.1a"><mo id="S4.SS4.SSS3.13.13.13.4.m1.1.1" xref="S4.SS4.SSS3.13.13.13.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.13.13.13.4.m1.1b"><times id="S4.SS4.SSS3.13.13.13.4.m1.1.1.cmml" xref="S4.SS4.SSS3.13.13.13.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.13.13.13.4.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.13.13.13.4.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/> Config III 4.51E+4  1.92E+5  2.15E+5  2.20E+5  2.00E+5  1.87E+5   4.99E+6 
<br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.14.14.14.5">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.14.14.14.5.m1.1"><semantics id="S4.SS4.SSS3.14.14.14.5.m1.1a"><mo id="S4.SS4.SSS3.14.14.14.5.m1.1.1" xref="S4.SS4.SSS3.14.14.14.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.14.14.14.5.m1.1b"><times id="S4.SS4.SSS3.14.14.14.5.m1.1.1.cmml" xref="S4.SS4.SSS3.14.14.14.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.14.14.14.5.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.14.14.14.5.m1.1d">×</annotation></semantics></math></span> <span class="ltx_text ltx_font_bold" id="S4.SS4.SSS3.15.15.15.6">22.68<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS3.15.15.15.6.m1.1"><semantics id="S4.SS4.SSS3.15.15.15.6.m1.1a"><mo id="S4.SS4.SSS3.15.15.15.6.m1.1.1" xref="S4.SS4.SSS3.15.15.15.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.15.15.15.6.m1.1b"><times id="S4.SS4.SSS3.15.15.15.6.m1.1.1.cmml" xref="S4.SS4.SSS3.15.15.15.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.15.15.15.6.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS3.15.15.15.6.m1.1d">×</annotation></semantics></math></span>
<br class="ltx_break"/></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsubsection ltx_figure_panel" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4. </span>Execution Time Breakdown</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p" id="S4.SS4.SSS4.p1.1">It would be ideal to understand the performance of each of the processing stages: <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p1.1.1">Get Row Number</span>, <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p1.1.2">Initialize Buffer</span>, <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p1.1.3">Assign Values</span> &amp; <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p1.1.4">Kernel Execution</span>. To this end, we break down the time consumption per stage using <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS4.p1.1.5">Piper</span> as a local accelerator. Specifically, there are several aspects worth noticing during the profiling.
Firstly, we need to know the number of rows of the input dataset to determine the size of the CPU’s buffer and FPGA’s off-chip memory.
Secondly, the design of regarding FPGA as an attached accelerator must consider data movements.
When transferring the dataset into FPGA’s memory, we can read large chunks of data to increase the efficiency.
Here, the initialization overhead of creating large buffers dominates, and it can reach tens of seconds.
Thirdly, for the scenario of implementing a decoding function in CPU as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a>, the program can only read the file per byte, and it is time-consuming; otherwise, it can simply read the file and store the content into the buffer.
Finally, the execution time includes transferring data from host to kernel, kernel operations, and transferring data from kernel to host.
For our case, we focus on kernel operations, and profiling tools are very helpful in generating a summary report.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS4.p2">
<p class="ltx_p" id="S4.SS4.SSS4.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F10" title="Figure 10 ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">10</span></a> shows the time breakdown of running <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS4.p2.1.1">Piper</span> in local mode, with the extra host-side overhead partially explaining why using <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS4.p2.1.2">Piper</span> as a local accelerator is sub-optimal for preprocessing.
We focus on two modes: <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.3">Decode in Kernel</span> and <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.4">Decode in Host</span>, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S3.F7" title="Figure 7 ‣ 3.4. System Integration ‣ 3. Piper: Accelerated Data Preprocessing ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">7</span></a>.
For <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.5">Decode in Kernel</span>, the decoding function takes a significant amount of time in the kernel, but <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.6">Initialize Buffer</span> still occupies a large portion of execution time.
For <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.7">Decode in Host</span>, where the decoding function is moved to the host, the execution takes about 50% longer than performing the decoding function twice in the kernel. In this condition, the proportion of time spent on <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS4.p2.1.8">Initialize Buffer</span> remains very high.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5. </span><span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS5.1.1">Piper</span> with Network</h4>
<div class="ltx_para" id="S4.SS4.SSS5.p1">
<p class="ltx_p" id="S4.SS4.SSS5.p1.4">When serving as a network-attached rather than a local accelerator, <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS5.p1.4.1">Piper</span> not only becomes more flexible in data centers but also achieves better performance as the host-side initialization overhead is removed and we then expect a fully-pipelined running fashion.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf1" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf2" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(b)</span></a> show <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS5.p1.4.2">Piper</span>’s performance when acting as a network-attached accelerator.
It achieves speedups of 5.1<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS5.p1.1.m1.1"><semantics id="S4.SS4.SSS5.p1.1.m1.1a"><mo id="S4.SS4.SSS5.p1.1.m1.1.1" xref="S4.SS4.SSS5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS5.p1.1.m1.1b"><times id="S4.SS4.SSS5.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS5.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS5.p1.1.m1.1d">×</annotation></semantics></math> and 4.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS5.p1.2.m2.1"><semantics id="S4.SS4.SSS5.p1.2.m2.1a"><mo id="S4.SS4.SSS5.p1.2.m2.1.1" xref="S4.SS4.SSS5.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS5.p1.2.m2.1b"><times id="S4.SS4.SSS5.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS5.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS5.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS5.p1.2.m2.1d">×</annotation></semantics></math>, respectively, compared to the best performance of a 128-core CPU for the UTF-8 dataset.
Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf3" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(c)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.F9.sf4" title="In Figure 9 ‣ 4.2.1. Optimizing Meta’s DLRM Preprocessing ‣ 4.2. Optimized CPU Baseline ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">9(d)</span></a> show that <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS5.p1.4.3">Piper</span> reaches speedups of 71.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS5.p1.3.m3.1"><semantics id="S4.SS4.SSS5.p1.3.m3.1a"><mo id="S4.SS4.SSS5.p1.3.m3.1.1" xref="S4.SS4.SSS5.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS5.p1.3.m3.1b"><times id="S4.SS4.SSS5.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS5.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS5.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS5.p1.3.m3.1d">×</annotation></semantics></math> and 25.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS5.p1.4.m4.1"><semantics id="S4.SS4.SSS5.p1.4.m4.1a"><mo id="S4.SS4.SSS5.p1.4.m4.1.1" xref="S4.SS4.SSS5.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS5.p1.4.m4.1b"><times id="S4.SS4.SSS5.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS5.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS5.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS5.p1.4.m4.1d">×</annotation></semantics></math> for the binary dataset, where <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS5.p1.4.4">Piper</span> maximizes its parallelism and the kernel execution time is negligible.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.6. </span>Throughput of Pure Computation</h4>
<div class="ltx_para" id="S4.SS4.SSS6.p1">
<p class="ltx_p" id="S4.SS4.SSS6.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS3" title="4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.3</span></a> shows the throughput for pure computation of all configurations to demonstrate the computing capability of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p1.1.1">Piper</span> versus multi-core CPUs.
We measure the throughput without data loading and storing steps to eliminate potential data movement overheads.
In the CPU setup, we exclude <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS6.p1.1.2">Split Input File &amp; Concatenate</span>, as these steps are not directly involved in the computation.
To ensure the validity of results, we conducted repetitive tests to guarantee that all data comes from DRAM and used the average performance of three runs as the final result.
For <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p1.1.3">Piper</span>, we report the kernel execution time. The difference between local and network mode lies in the kernel clock frequency of the FPGA.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS6.p2">
<p class="ltx_p" id="S4.SS4.SSS6.p2.4">By eliminating the non-computing steps, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS3" title="4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.3</span></a> compares performance gains of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.1">Piper</span> for stateful preprocessing pipelines with/without raw dataset transformation.
Given the UTF-8 input dataset (Config I &amp; II) and a small vocabulary table, the CPU reaches the highest throughput in Config II with 64 threads, and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.2">Piper</span> achieves 4.1<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS6.p2.1.m1.1"><semantics id="S4.SS4.SSS6.p2.1.m1.1a"><mo id="S4.SS4.SSS6.p2.1.m1.1.1" xref="S4.SS4.SSS6.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.p2.1.m1.1b"><times id="S4.SS4.SSS6.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS6.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.p2.1.m1.1d">×</annotation></semantics></math> speedup over CPU.
Given the binary input dataset, the speedup of <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.3">Piper</span> increase to 46.4<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS6.p2.2.m2.1"><semantics id="S4.SS4.SSS6.p2.2.m2.1a"><mo id="S4.SS4.SSS6.p2.2.m2.1.1" xref="S4.SS4.SSS6.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.p2.2.m2.1b"><times id="S4.SS4.SSS6.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS6.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.p2.2.m2.1d">×</annotation></semantics></math>.
When the vocabulary size increases to 1M, the performance of both CPU and <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.4">Piper</span> decreases due to the more frequent random memory accesses.
In this setup, <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.5">Piper</span> achieves speedups of 4.2<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS6.p2.3.m3.1"><semantics id="S4.SS4.SSS6.p2.3.m3.1a"><mo id="S4.SS4.SSS6.p2.3.m3.1.1" xref="S4.SS4.SSS6.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.p2.3.m3.1b"><times id="S4.SS4.SSS6.p2.3.m3.1.1.cmml" xref="S4.SS4.SSS6.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.p2.3.m3.1d">×</annotation></semantics></math> for the UTF-8 dataset and 22.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.SSS6.p2.4.m4.1"><semantics id="S4.SS4.SSS6.p2.4.m4.1a"><mo id="S4.SS4.SSS6.p2.4.m4.1.1" xref="S4.SS4.SSS6.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.p2.4.m4.1b"><times id="S4.SS4.SSS6.p2.4.m4.1.1.cmml" xref="S4.SS4.SSS6.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.p2.4.m4.1d">×</annotation></semantics></math> for the binary dataset.
Given these numbers, <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p2.4.6">Piper</span> running in networked mode is preferable for adapting to various configurations, and serving binary dataset as input can significantly increase its performance for both end-to-end execution and pure computation.
The theoretical maximum throughput for binary input can be 30.2 Gbit/s for 5K vocabulary and 6.4 Gbit/s for 1M vocabulary, and using multiple FPGAs can further improve the overall performance.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS6.p3">
<p class="ltx_p" id="S4.SS4.SSS6.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#S4.SS4.SSS6" title="4.4.6. Throughput of Pure Computation ‣ 4.4.5. Piper with Network ‣ 4.4.4. Execution Time Breakdown ‣ 4.4.3. Maximizing Kernel Performance by Offloading Decoding ‣ 4.4. Piper: Performance and Efficiency ‣ 4. Evaluation ‣ Efficient Tabular Data Preprocessing of ML Pipelines"><span class="ltx_text ltx_ref_tag">4.4.6</span></a> compares the performance of each individual operator between <span class="ltx_text ltx_font_smallcaps" id="S4.SS4.SSS6.p3.1.1">Piper</span> and the CPU.
It shows that for some operators like <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS6.p3.1.2">Neg2Zero, Logarithm</span>, CPU outperforms FPGA greatly; while for some other operators, especially for <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS6.p3.1.3">Hex2Int &amp; Modulus</span>, FPGA performs much better.
For <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS6.p3.1.4">ApplyVocab</span>, using HBM as a cache to support a large vocabulary table results in an II of one cycle, because independent channels are used for different features and memory access occurs in a round-robin manner. The time span for accessing the same HBM channel is longer than the allowed II for random access.
Besides, FPGA can process the task in a pipelined manner, so we only need to carefully optimize the critical operator, i.e. <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS6.p3.1.5">GenVocab</span> in this case.
However, on the CPU, the total execution time includes all operators, requiring each to be optimized to minimize the overall latency.</p>
</div>
<figure class="ltx_table" id="S4.SS4.SSS6.16">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.SS4.SSS6.16.17.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S4.SS4.SSS6.16.18.2" style="font-size:90%;">Comparison of execution time of operators in seconds for the whole dataset in different platforms (CPU single thread; FPGA 250MHz for vocab 5K and 135MHz for vocab 1M). 7.33s and 13.58s represents II=1 in 250MHz and 135MHz respectively.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS4.SSS6.16.16" style="width:215.7pt;height:99pt;vertical-align:-93.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.9pt,0.9pt) scale(0.75,0.75) ;"><span class="ltx_ERROR undefined" id="S4.SS4.SSS6.16.16.17">{tblr}</span>
<p class="ltx_p" id="S4.SS4.SSS6.16.16.16">colspec=ccccc,
cell12 = c=2c, cell14 = c=2c, 
Vocab  5K   1M  
<br class="ltx_break"/> 
Platform  CPU  FPGA  CPU  FPGA
<br class="ltx_break"/>Decode &amp; FillMissing  182.29<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.1.1.1.m1.1"><semantics id="S4.SS4.SSS6.1.1.1.m1.1a"><mo id="S4.SS4.SSS6.1.1.1.m1.1.1" xref="S4.SS4.SSS6.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.1.1.1.m1.1.1.cmml" xref="S4.SS4.SSS6.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.1.1.1.m1.1d">±</annotation></semantics></math>1.10  11.00 182.29<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.2.2.2.m2.1"><semantics id="S4.SS4.SSS6.2.2.2.m2.1a"><mo id="S4.SS4.SSS6.2.2.2.m2.1.1" xref="S4.SS4.SSS6.2.2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.2.2.2.m2.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.2.2.2.m2.1.1.cmml" xref="S4.SS4.SSS6.2.2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.2.2.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.2.2.2.m2.1d">±</annotation></semantics></math>1.10  20.37
<br class="ltx_break"/>Binary Unpack  35.77<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.3.3.3.m3.1"><semantics id="S4.SS4.SSS6.3.3.3.m3.1a"><mo id="S4.SS4.SSS6.3.3.3.m3.1.1" xref="S4.SS4.SSS6.3.3.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.3.3.3.m3.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.3.3.3.m3.1.1.cmml" xref="S4.SS4.SSS6.3.3.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.3.3.3.m3.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.3.3.3.m3.1d">±</annotation></semantics></math>0.57  7.33  35.77<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.4.4.4.m4.1"><semantics id="S4.SS4.SSS6.4.4.4.m4.1a"><mo id="S4.SS4.SSS6.4.4.4.m4.1.1" xref="S4.SS4.SSS6.4.4.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.4.4.4.m4.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.4.4.4.m4.1.1.cmml" xref="S4.SS4.SSS6.4.4.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.4.4.4.m4.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.4.4.4.m4.1d">±</annotation></semantics></math>0.57  13.58 
<br class="ltx_break"/>Hex2Int &amp; Modulus  655.17<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.5.5.5.m5.1"><semantics id="S4.SS4.SSS6.5.5.5.m5.1a"><mo id="S4.SS4.SSS6.5.5.5.m5.1.1" xref="S4.SS4.SSS6.5.5.5.m5.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.5.5.5.m5.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.5.5.5.m5.1.1.cmml" xref="S4.SS4.SSS6.5.5.5.m5.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.5.5.5.m5.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.5.5.5.m5.1d">±</annotation></semantics></math>1.55 7.33  655.17<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.6.6.6.m6.1"><semantics id="S4.SS4.SSS6.6.6.6.m6.1a"><mo id="S4.SS4.SSS6.6.6.6.m6.1.1" xref="S4.SS4.SSS6.6.6.6.m6.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.6.6.6.m6.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.6.6.6.m6.1.1.cmml" xref="S4.SS4.SSS6.6.6.6.m6.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.6.6.6.m6.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.6.6.6.m6.1d">±</annotation></semantics></math>1.55 13.58
<br class="ltx_break"/>GenVocab-1  365.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.7.7.7.m7.1"><semantics id="S4.SS4.SSS6.7.7.7.m7.1a"><mo id="S4.SS4.SSS6.7.7.7.m7.1.1" xref="S4.SS4.SSS6.7.7.7.m7.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.7.7.7.m7.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.7.7.7.m7.1.1.cmml" xref="S4.SS4.SSS6.7.7.7.m7.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.7.7.7.m7.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.7.7.7.m7.1d">±</annotation></semantics></math>1.59 14.67  410.82<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.8.8.8.m8.1"><semantics id="S4.SS4.SSS6.8.8.8.m8.1a"><mo id="S4.SS4.SSS6.8.8.8.m8.1.1" xref="S4.SS4.SSS6.8.8.8.m8.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.8.8.8.m8.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.8.8.8.m8.1.1.cmml" xref="S4.SS4.SSS6.8.8.8.m8.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.8.8.8.m8.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.8.8.8.m8.1d">±</annotation></semantics></math>5.34  27.16 
<br class="ltx_break"/>GenVocab-2  NOP 7.33  NOP  13.58
<br class="ltx_break"/>ApplyVocab-1  0.0065<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.9.9.9.m9.1"><semantics id="S4.SS4.SSS6.9.9.9.m9.1a"><mo id="S4.SS4.SSS6.9.9.9.m9.1.1" xref="S4.SS4.SSS6.9.9.9.m9.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.9.9.9.m9.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.9.9.9.m9.1.1.cmml" xref="S4.SS4.SSS6.9.9.9.m9.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.9.9.9.m9.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.9.9.9.m9.1d">±</annotation></semantics></math>0.00024  7.33  0.74<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.10.10.10.m10.1"><semantics id="S4.SS4.SSS6.10.10.10.m10.1a"><mo id="S4.SS4.SSS6.10.10.10.m10.1.1" xref="S4.SS4.SSS6.10.10.10.m10.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.10.10.10.m10.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.10.10.10.m10.1.1.cmml" xref="S4.SS4.SSS6.10.10.10.m10.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.10.10.10.m10.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.10.10.10.m10.1d">±</annotation></semantics></math>0.015 13.58
<br class="ltx_break"/>ApplyVocab-2  331.79<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.11.11.11.m11.1"><semantics id="S4.SS4.SSS6.11.11.11.m11.1a"><mo id="S4.SS4.SSS6.11.11.11.m11.1.1" xref="S4.SS4.SSS6.11.11.11.m11.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.11.11.11.m11.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.11.11.11.m11.1.1.cmml" xref="S4.SS4.SSS6.11.11.11.m11.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.11.11.11.m11.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.11.11.11.m11.1d">±</annotation></semantics></math>1.51 7.33 367.11<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.12.12.12.m12.1"><semantics id="S4.SS4.SSS6.12.12.12.m12.1a"><mo id="S4.SS4.SSS6.12.12.12.m12.1.1" xref="S4.SS4.SSS6.12.12.12.m12.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.12.12.12.m12.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.12.12.12.m12.1.1.cmml" xref="S4.SS4.SSS6.12.12.12.m12.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.12.12.12.m12.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.12.12.12.m12.1d">±</annotation></semantics></math>9.54 13.58
<br class="ltx_break"/>Neg2Zero  0.61<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.13.13.13.m13.1"><semantics id="S4.SS4.SSS6.13.13.13.m13.1a"><mo id="S4.SS4.SSS6.13.13.13.m13.1.1" xref="S4.SS4.SSS6.13.13.13.m13.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.13.13.13.m13.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.13.13.13.m13.1.1.cmml" xref="S4.SS4.SSS6.13.13.13.m13.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.13.13.13.m13.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.13.13.13.m13.1d">±</annotation></semantics></math>0.00717.33  0.61<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.14.14.14.m14.1"><semantics id="S4.SS4.SSS6.14.14.14.m14.1a"><mo id="S4.SS4.SSS6.14.14.14.m14.1.1" xref="S4.SS4.SSS6.14.14.14.m14.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.14.14.14.m14.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.14.14.14.m14.1.1.cmml" xref="S4.SS4.SSS6.14.14.14.m14.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.14.14.14.m14.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.14.14.14.m14.1d">±</annotation></semantics></math>0.0071 13.58
<br class="ltx_break"/>Logarithm  1.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.15.15.15.m15.1"><semantics id="S4.SS4.SSS6.15.15.15.m15.1a"><mo id="S4.SS4.SSS6.15.15.15.m15.1.1" xref="S4.SS4.SSS6.15.15.15.m15.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.15.15.15.m15.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.15.15.15.m15.1.1.cmml" xref="S4.SS4.SSS6.15.15.15.m15.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.15.15.15.m15.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.15.15.15.m15.1d">±</annotation></semantics></math>0.0127.33 1.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.SS4.SSS6.16.16.16.m16.1"><semantics id="S4.SS4.SSS6.16.16.16.m16.1a"><mo id="S4.SS4.SSS6.16.16.16.m16.1.1" xref="S4.SS4.SSS6.16.16.16.m16.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS6.16.16.16.m16.1b"><csymbol cd="latexml" id="S4.SS4.SSS6.16.16.16.m16.1.1.cmml" xref="S4.SS4.SSS6.16.16.16.m16.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS6.16.16.16.m16.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS6.16.16.16.m16.1d">±</annotation></semantics></math>0.012 13.58
<br class="ltx_break"/></p>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<section class="ltx_section ltx_figure_panel" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we discuss how <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">Piper</span> can be extended for various deployment and algorithm requirements.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Integration into ML systems.</span>
To integrate <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">Piper</span> into the current training system, cloud vendors can add an interface between <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.3">Piper</span> and machine learning training frameworks such as PyTorch and TensorFlow, facilitating data transfer between GPUs and FPGAs. As <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.4">Piper</span> already supports networked execution modes, the interface can be as simple as initiating the service of sending/receiving training data over the network, like Remote Procedure Call (RPC) <cite class="ltx_cite ltx_citemacro_citep">(Nelson, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib55" title="">1981</a>; D’Hollander et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib19" title="">2017</a>)</cite>.
The recently released FPGA-based smart NIC products like MangoBoost RDMA System <cite class="ltx_cite ltx_citemacro_citep">(MangoBoost, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib49" title="">2024</a>)</cite> enable 200Gbps throughput, which provides a solid foundation for the communication between peripheral PCIe devices (FPGAs) and GPUs.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Generalizability for other preprocessing pipelines.</span>
In this paper, DLRM works as a representation of ML-based recommender systems where other models share a similar tabular data input and preprocessing pipeline.
In order to support other data preprocessing pipelines, we build <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.2">Piper</span> in a modular fashion, such that the operators <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.3">Piper</span> supports can be easily integrated into alternative pipelines.
Using available FPGA virtualization and multi-tenancy techniques <cite class="ltx_cite ltx_citemacro_citep">(Korolija et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib41" title="">2020</a>; Khawaja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib36" title="">2018</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib48" title="">2020</a>)</cite>, it is feasible to dynamically configure the operators in the pipeline at runtime .</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Cater to tabular datasets</span>.
The concept of sparse and dense features works for many tabular datasets, like MovieLens <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib29" title="">2015</a>)</cite>, Netflix Prize Dataset <cite class="ltx_cite ltx_citemacro_citep">(Bennett et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib10" title="">2007</a>)</cite>, Amazon Product Review Data <cite class="ltx_cite ltx_citemacro_citep">(Haque et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib28" title="">2018</a>)</cite>, Yelp Dataset <cite class="ltx_cite ltx_citemacro_citep">(Asghar, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib6" title="">2016</a>)</cite>.
The modular design of <span class="ltx_text ltx_font_smallcaps" id="S5.p4.1.2">Piper</span> allows users to easily adjust the number of dataflows to adapt to different numbers of feature columns.</p>
</div>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Related Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">To our knowledge, <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">Piper</span> is the first attempt at addressing the efficiency of tabular data preprocessing pipelines with specialized hardware design.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Data preprocessing frameworks for CPUs</span>.
Currently, the CPU is the mainstream platform for preprocessing tasks of machine learning.
Google introduced
tf.data <cite class="ltx_cite ltx_citemacro_citep">(Murray et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib53" title="">2021</a>)</cite> and developed tf.data service <cite class="ltx_cite ltx_citemacro_citep">(Audibert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib7" title="">2023</a>)</cite> to strengthen disaggregated data processing service.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Preprocessing performance optimizations on CPUs</span>.
UPLIFT <cite class="ltx_cite ltx_citemacro_citep">(Phani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib64" title="">2022</a>)</cite> focuses on the parallelization of feature transformations.
Plumber <cite class="ltx_cite ltx_citemacro_citep">(Kuchnik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib43" title="">2022</a>)</cite>
help users find bottlenecks in ML input pipelines.
Cachew <cite class="ltx_cite ltx_citemacro_citep">(Graur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib24" title="">2022</a>)</cite> supports <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">auto-scaling</span> and <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">auto-caching</span> policies to minimize training time and cost.
GoldMiner <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib82" title="">2023b</a>)</cite> decouples stateless data preprocessing from model training in the cloud environment.
FastFlow <cite class="ltx_cite ltx_citemacro_citep">(Um et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib74" title="">2023</a>)</cite> offloads input pipelines to remote CPUs, and
FusionFlow <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib37" title="">2023</a>)</cite> utilizes both CPUs and GPUs to accelerate preprocessing.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Improved DLRM training</span>.
Neo <cite class="ltx_cite ltx_citemacro_citep">(Mudigere et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib52" title="">2022</a>)</cite> proposes a SW-HW co-designed system for distributed training of large-scale DLRMs.
Dhiraj <cite class="ltx_cite ltx_citemacro_citep">(Kalamkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib35" title="">2020</a>)</cite> optimizes DLRM training on CPU cluster architectures.
EL-rec <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib77" title="">2022b</a>)</cite> harness the tensor-train technique for large-scale DLRMs with limited GPU resources.
cDLRM <cite class="ltx_cite ltx_citemacro_citep">(Balasubramanian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib8" title="">2021</a>)</cite> trains on a single GPU by storing all embedding tables in CPU memory.
RecD <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib84" title="">2023a</a>)</cite> optimizes DLRM data generation pipelines to decrease dataset storage.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1"><span class="ltx_text ltx_font_bold" id="S6.p5.1.1">FPGA in Data Center</span>.
Microsoft’s Catapult <cite class="ltx_cite ltx_citemacro_citep">(Putnam, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib65" title="">2017</a>)</cite> uses FPGAs to implement network virtualization in hyper-scale data centers.
StRoM <cite class="ltx_cite ltx_citemacro_citep">(Sidler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib68" title="">2020</a>)</cite> presents a programmable, FPGA-based RoCE v2 NIC.
IBM <cite class="ltx_cite ltx_citemacro_citep">(Weerasinghe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib78" title="">2015</a>)</cite> proposes a cloud computing software service to integrate FPGAs in the cloud.
Naif <cite class="ltx_cite ltx_citemacro_citep">(Tarafdar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14912v1#bib.bib71" title="">2017</a>)</cite> create network FPGA clusters in a heterogeneous cloud data center.</p>
</div>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.4">We present <span class="ltx_text ltx_font_smallcaps" id="S7.p1.4.1">Piper</span>, a network-based hardware accelerator to support tabular stateful data preprocessing for embedding generation.
To address the computationally intensive data preprocessing workload, <span class="ltx_text ltx_font_smallcaps" id="S7.p1.4.2">Piper</span> incorporates high-performance data transformation units and various operator processing units.
For flexible integration into end-to-end machine learning training systems, <span class="ltx_text ltx_font_smallcaps" id="S7.p1.4.3">Piper</span> can function both as a local accelerator and as a network-attached accelerator.
<span class="ltx_text ltx_font_smallcaps" id="S7.p1.4.4">Piper</span> achieves 4.7<math alttext="\sim" class="ltx_Math" display="inline" id="S7.p1.1.m1.1"><semantics id="S7.p1.1.m1.1a"><mo id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><csymbol cd="latexml" id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S7.p1.1.m1.1d">∼</annotation></semantics></math>71.3<math alttext="\times" class="ltx_Math" display="inline" id="S7.p1.2.m2.1"><semantics id="S7.p1.2.m2.1a"><mo id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><times id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S7.p1.2.m2.1d">×</annotation></semantics></math> speedup over a 128-core CPU server and 4.8<math alttext="\sim" class="ltx_Math" display="inline" id="S7.p1.3.m3.1"><semantics id="S7.p1.3.m3.1a"><mo id="S7.p1.3.m3.1.1" xref="S7.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.p1.3.m3.1b"><csymbol cd="latexml" id="S7.p1.3.m3.1.1.cmml" xref="S7.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S7.p1.3.m3.1d">∼</annotation></semantics></math>20.3<math alttext="\times" class="ltx_Math" display="inline" id="S7.p1.4.m4.1"><semantics id="S7.p1.4.m4.1a"><mo id="S7.p1.4.m4.1.1" xref="S7.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p1.4.m4.1b"><times id="S7.p1.4.m4.1.1.cmml" xref="S7.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S7.p1.4.m4.1d">×</annotation></semantics></math> over a data-center GPU.
This impressive performance highlights <span class="ltx_text ltx_font_smallcaps" id="S7.p1.4.5">Piper</span>’s potential for future integration into production ML training systems.</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib2.3.3.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib2.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.5.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alvarez et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Catalina Alvarez, Zhenhao He, Gustavo Alonso, and Ankit Singla. 2020.

</span>
<span class="ltx_bibblock">Specializing the network for scatter-gather workloads. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 11th ACM Symposium on Cloud Computing</em>. 267–280.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon (2024)</span>
<span class="ltx_bibblock">
Amazon. 2024.

</span>
<span class="ltx_bibblock">AQUA (Advanced Query Accelerator) – A Speed Boost for Your Amazon Redshift Queries.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/blogs/aws/new-aqua-advanced-query-accelerator-for-amazon-redshift" title="">https://aws.amazon.com/blogs/aws/new-aqua-advanced-query-accelerator-for-amazon-redshift</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angel et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sebastian Angel, Mihir Nanavati, and Siddhartha Sen. 2020.

</span>
<span class="ltx_bibblock">Disaggregation and the Application. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asghar (2016)</span>
<span class="ltx_bibblock">
Nabiha Asghar. 2016.

</span>
<span class="ltx_bibblock">Yelp dataset challenge: Review rating prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1605.05362</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Audibert et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Andrew Audibert, Yang Chen, Dan Graur, Ana Klimovic, Jiří Šimša, and A Chandramohan. 2023.

</span>
<span class="ltx_bibblock">tf.data service: A Case for Disaggregating ML Input Data Processing.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balasubramanian et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Keshav Balasubramanian, Abdulla Alshabanah, Joshua D Choe, and Murali Annavaram. 2021.

</span>
<span class="ltx_bibblock">cDLRM: Look ahead caching for scalable training of recommendation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 15th ACM Conference on Recommender Systems</em>. 263–272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartík et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Matěj Bartík, Sven Ubik, and Pavel Kubalik. 2015.

</span>
<span class="ltx_bibblock">LZ4 compression algorithm on FPGA. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">2015 IEEE International Conference on Electronics, Circuits, and Systems (ICECS)</em>. IEEE, 179–182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bennett et al<span class="ltx_text" id="bib.bib10.3.3.1">.</span> (2007)</span>
<span class="ltx_bibblock">
James Bennett, Stan Lanning, et al<span class="ltx_text" id="bib.bib10.4.1">.</span> 2007.

</span>
<span class="ltx_bibblock">The netflix prize. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.5.1">Proceedings of KDD cup and workshop</em>, Vol. 2007. New York, 35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunella et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Marco Spaziani Brunella, Giacomo Belocchi, Marco Bonola, Salvatore Pontarelli, Giuseppe Siracusano, Giuseppe Bianchi, Aniello Cammarano, Alessandro Palumbo, Luca Petrucci, and Roberto Bifulco. 2022.

</span>
<span class="ltx_bibblock">hXDP: Efficient software packet processing on FPGA NICs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Commun. ACM</em> 65, 8 (2022), 92–100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib12.3.3.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al<span class="ltx_text" id="bib.bib12.4.1">.</span> 2016.

</span>
<span class="ltx_bibblock">Wide &amp; deep learning for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.5.1">Proceedings of the 1st workshop on deep learning for recommender systems</em>. 7–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choquette (2023)</span>
<span class="ltx_bibblock">
Jack Choquette. 2023.

</span>
<span class="ltx_bibblock">Nvidia hopper h100 gpu: Scaling performance.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Church (2017)</span>
<span class="ltx_bibblock">
Kenneth Ward Church. 2017.

</span>
<span class="ltx_bibblock">Word2Vec.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Natural Language Engineering</em> 23, 1 (2017), 155–162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Criteo (2024)</span>
<span class="ltx_bibblock">
Criteo. 2024.

</span>
<span class="ltx_bibblock">Criteo Kaggle Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/datasets/mrkmakr/criteo-dataset" title="">https://www.kaggle.com/datasets/mrkmakr/criteo-dataset</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dally et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
William J Dally, Stephen W Keckler, and David B Kirk. 2021.

</span>
<span class="ltx_bibblock">Evolution of the graphics processing unit (GPU).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">IEEE Micro</em> 41, 6 (2021), 42–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">David et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Tudor David, Rachid Guerraoui, and Vasileios Trigonakis. 2013.

</span>
<span class="ltx_bibblock">Everything you always wanted to know about synchronization but were afraid to ask. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</em>. 33–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’Hollander et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Erik H D’Hollander, Bruno Chevalier, and Koen De Bosschere. 2017.

</span>
<span class="ltx_bibblock">Calling hardware procedures in a reconfigurable accelerator using RPC-FPGA. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">2017 International Conference on Field Programmable Technology (ICFPT)</em>. IEEE, 271–274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eran et al<span class="ltx_text" id="bib.bib20.4.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Haggai Eran, Lior Zeno, Maroun Tork, Gabi Malka, and Mark Silberstein. 2019.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib20.1.m1.1"><semantics id="bib.bib20.1.m1.1a"><mo id="bib.bib20.1.m1.1.1" stretchy="false" xref="bib.bib20.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib20.1.m1.1b"><ci id="bib.bib20.1.m1.1.1.cmml" xref="bib.bib20.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib20.1.m1.1d">{</annotation></semantics></math>NICA<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib20.2.m2.1"><semantics id="bib.bib20.2.m2.1a"><mo id="bib.bib20.2.m2.1.1" stretchy="false" xref="bib.bib20.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib20.2.m2.1b"><ci id="bib.bib20.2.m2.1.1.cmml" xref="bib.bib20.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib20.2.m2.1d">}</annotation></semantics></math>: An infrastructure for inline acceleration of network applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.5.1">2019 USENIX Annual Technical Conference (USENIX ATC 19)</em>. 345–362.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fatourou and Kallimanis (2012)</span>
<span class="ltx_bibblock">
Panagiota Fatourou and Nikolaos D Kallimanis. 2012.

</span>
<span class="ltx_bibblock">Revisiting the combining synchronization technique. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming</em>. 257–266.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firestone et al<span class="ltx_text" id="bib.bib22.7.3.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric Chung, et al<span class="ltx_text" id="bib.bib22.8.1">.</span> 2018.

</span>
<span class="ltx_bibblock">Azure accelerated networking: Smartnics in the public cloud. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.4">15th <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib22.1.1.m1.1"><semantics id="bib.bib22.1.1.m1.1a"><mo id="bib.bib22.1.1.m1.1.1" stretchy="false" xref="bib.bib22.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.1.1.m1.1b"><ci id="bib.bib22.1.1.m1.1.1.cmml" xref="bib.bib22.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.1.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.1.1.m1.1d">{</annotation></semantics></math>USENIX<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib22.2.2.m2.1"><semantics id="bib.bib22.2.2.m2.1a"><mo id="bib.bib22.2.2.m2.1.1" stretchy="false" xref="bib.bib22.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.2.2.m2.1b"><ci id="bib.bib22.2.2.m2.1.1.cmml" xref="bib.bib22.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.2.2.m2.1d">}</annotation></semantics></math> Symposium on Networked Systems Design and Implementation (<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib22.3.3.m3.1"><semantics id="bib.bib22.3.3.m3.1a"><mo id="bib.bib22.3.3.m3.1.1" stretchy="false" xref="bib.bib22.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.3.3.m3.1b"><ci id="bib.bib22.3.3.m3.1.1.cmml" xref="bib.bib22.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.3.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.3.3.m3.1d">{</annotation></semantics></math>NSDI<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib22.4.4.m4.1"><semantics id="bib.bib22.4.4.m4.1a"><mo id="bib.bib22.4.4.m4.1.1" stretchy="false" xref="bib.bib22.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib22.4.4.m4.1b"><ci id="bib.bib22.4.4.m4.1.1.cmml" xref="bib.bib22.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.4.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.4.4.m4.1d">}</annotation></semantics></math> 18)</em>. 51–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2024)</span>
<span class="ltx_bibblock">
Google. 2024.

</span>
<span class="ltx_bibblock">Google Tensorflow TFRecord.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" title="">https://www.tensorflow.org/tutorials/load_data/tfrecord</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graur et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Dan Graur, Damien Aymon, Dan Kluser, Tanguy Albrici, Chandramohan A Thekkath, and Ana Klimovic. 2022.

</span>
<span class="ltx_bibblock">Cachew: Machine learning input data processing as a service. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">2022 USENIX Annual Technical Conference (USENIX ATC 22)</em>. 689–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves and Graves (2012)</span>
<span class="ltx_bibblock">
Alex Graves and Alex Graves. 2012.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Supervised sequence labelling with recurrent neural networks</em> (2012), 37–45.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S Lee, David Brooks, and Carole-Jean Wu. 2020a.

</span>
<span class="ltx_bibblock">Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>. IEEE, 982–995.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib27.3.3.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al<span class="ltx_text" id="bib.bib27.4.1">.</span> 2020b.

</span>
<span class="ltx_bibblock">The architectural implications of facebook’s dnn-based personalized recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.5.1">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</em>. IEEE, 488–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tanjim Ul Haque, Nudrat Nawal Saber, and Faisal Muhammad Shah. 2018.

</span>
<span class="ltx_bibblock">Sentiment analysis on large scale Amazon product reviews. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">2018 IEEE international conference on innovative research and development (ICIRD)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)</span>
<span class="ltx_bibblock">
F Maxwell Harper and Joseph A Konstan. 2015.

</span>
<span class="ltx_bibblock">The movielens datasets: History and context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Acm transactions on interactive intelligent systems (tiis)</em> 5, 4 (2015), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017.

</span>
<span class="ltx_bibblock">Neural collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 26th international conference on world wide web</em>. 173–182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenhao He, Dario Korolija, Yu Zhu, Benjamin Ramhorst, Tristan Laan, Lucian Petrica, Michaela Blott, and Gustavo Alonso. 2023.

</span>
<span class="ltx_bibblock">ACCL+: an FPGA-Based Collective Engine for Distributed Applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:2312.11742</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendler et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Danny Hendler, Itai Incze, Nir Shavit, and Moran Tzafrir. 2010.

</span>
<span class="ltx_bibblock">Flat combining and the synchronization-parallelism tradeoff. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the twenty-second annual ACM symposium on Parallelism in algorithms and architectures</em>. 355–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herlihy et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Maurice Herlihy, Nir Shavit, Victor Luchangco, and Michael Spear. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">The art of multiprocessor programming</em>.

</span>
<span class="ltx_bibblock">Newnes.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib34.3.3.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wenqi Jiang, Zhenhao He, Shuai Zhang, Kai Zeng, Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, Ce Zhang, et al<span class="ltx_text" id="bib.bib34.4.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Fleetrec: Large-scale recommendation inference on hybrid gpu-fpga clusters. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.5.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 3097–3105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalamkar et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping Chen, Mikhail Shiryaev, and Alexander Heinecke. 2020.

</span>
<span class="ltx_bibblock">Optimizing deep learning recommender systems training on cpu cluster architectures. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>. IEEE, 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khawaja et al<span class="ltx_text" id="bib.bib36.4.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ahmed Khawaja, Joshua Landgraf, Rohith Prakash, Michael Wei, Eric Schkufza, and Christopher J Rossbach. 2018.

</span>
<span class="ltx_bibblock">Sharing, Protection, and Compatibility for Reconfigurable Fabric with <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib36.1.m1.1"><semantics id="bib.bib36.1.m1.1a"><mo id="bib.bib36.1.m1.1.1" stretchy="false" xref="bib.bib36.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib36.1.m1.1b"><ci id="bib.bib36.1.m1.1.1.cmml" xref="bib.bib36.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib36.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib36.1.m1.1d">{</annotation></semantics></math>AmorphOS<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib36.2.m2.1"><semantics id="bib.bib36.2.m2.1a"><mo id="bib.bib36.2.m2.1.1" stretchy="false" xref="bib.bib36.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib36.2.m2.1b"><ci id="bib.bib36.2.m2.1.1.cmml" xref="bib.bib36.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib36.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib36.2.m2.1d">}</annotation></semantics></math>. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.5.1">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</em>. 107–127.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Taeyoon Kim, ChanHo Park, Mansur Mukimbekov, Heelim Hong, Minseok Kim, Ze Jin, Changdae Kim, Ji-Yong Shin, and Myeongjae Jeon. 2023.

</span>
<span class="ltx_bibblock">FusionFlow: Accelerating Data Preprocessing for Machine Learning with CPU-GPU Cooperation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the VLDB Endowment</em> 17, 4 (2023), 863–876.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma (2013)</span>
<span class="ltx_bibblock">
Diederik P Kingma. 2013.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:1312.6114</em> (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klimovic et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Ana Klimovic, Christos Kozyrakis, Eno Thereska, Binu John, and Sanjeev Kumar. 2016.

</span>
<span class="ltx_bibblock">Flash storage disaggregation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the Eleventh European Conference on Computer Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korolija et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov, Dejan Milojičić, and Gustavo Alonso. 2021.

</span>
<span class="ltx_bibblock">Farview: Disaggregated memory with operator off-loading for database engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2106.07102</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korolija et al<span class="ltx_text" id="bib.bib41.6.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dario Korolija, Timothy Roscoe, and Gustavo Alonso. 2020.

</span>
<span class="ltx_bibblock">Do <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib41.1.m1.1"><semantics id="bib.bib41.1.m1.1a"><mo id="bib.bib41.1.m1.1.1" stretchy="false" xref="bib.bib41.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.1.m1.1b"><ci id="bib.bib41.1.m1.1.1.cmml" xref="bib.bib41.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.1.m1.1d">{</annotation></semantics></math>OS<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib41.2.m2.1"><semantics id="bib.bib41.2.m2.1a"><mo id="bib.bib41.2.m2.1.1" stretchy="false" xref="bib.bib41.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.2.m2.1b"><ci id="bib.bib41.2.m2.1.1.cmml" xref="bib.bib41.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.2.m2.1d">}</annotation></semantics></math> abstractions make sense on <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib41.3.m3.1"><semantics id="bib.bib41.3.m3.1a"><mo id="bib.bib41.3.m3.1.1" stretchy="false" xref="bib.bib41.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.3.m3.1b"><ci id="bib.bib41.3.m3.1.1.cmml" xref="bib.bib41.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.3.m3.1d">{</annotation></semantics></math>FPGAs<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib41.4.m4.1"><semantics id="bib.bib41.4.m4.1a"><mo id="bib.bib41.4.m4.1.1" stretchy="false" xref="bib.bib41.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib41.4.m4.1b"><ci id="bib.bib41.4.m4.1.1.cmml" xref="bib.bib41.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib41.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib41.4.m4.1d">}</annotation></semantics></math>?. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.7.1">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</em>. 991–1010.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koshiba et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Atsushi Koshiba, Felix Gust, Julian Pritzi, Anjo Vahldiek-Oberwagner, Nuno Santos, and Pramod Bhatotia. 2023.

</span>
<span class="ltx_bibblock">Trusted Heterogeneous Disaggregated Architectures. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the 14th ACM SIGOPS Asia-Pacific Workshop on Systems</em>. 72–79.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuchnik et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michael Kuchnik, Ana Klimovic, Jiri Simsa, Virginia Smith, and George Amvrosiadis. 2022.

</span>
<span class="ltx_bibblock">Plumber: Diagnosing and removing performance bottlenecks in machine learning data pipelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Proceedings of Machine Learning and Systems</em> 4 (2022), 33–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazarev et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nikita Lazarev, Neil Adit, Shaojie Xiang, Zhiru Zhang, and Christina Delimitrou. 2020.

</span>
<span class="ltx_bibblock">Dagger: Towards efficient rpcs in cloud microservices with near-memory reconfigurable nics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">IEEE Computer Architecture Letters</em> 19, 2 (2020), 134–138.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ledwon et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Morgan Ledwon, Bruce F Cockburn, and Jie Han. 2020.

</span>
<span class="ltx_bibblock">High-throughput FPGA-based hardware accelerators for deflate compression and decompression using high-level synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">IEEE Access</em> 8 (2020), 62207–62217.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2019)</span>
<span class="ltx_bibblock">
Feifei Li. 2019.

</span>
<span class="ltx_bibblock">Cloud-native database systems at Alibaba: Opportunities and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the VLDB Endowment</em> 12, 12 (2019), 2263–2272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Xiufeng Liu, Christian Thomsen, and Torben Bach Pedersen. 2012.

</span>
<span class="ltx_bibblock">MapReduce-Based Dimensional ETL Made Easy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proc. VLDB Endow.</em> 5, 12 (2012).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiacheng Ma, Gefei Zuo, Kevin Loughlin, Xiaohe Cheng, Yanqiang Liu, Abel Mulugeta Eneyew, Zhengwei Qi, and Baris Kasikci. 2020.

</span>
<span class="ltx_bibblock">A hypervisor for shared-memory FPGA platforms. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</em>. 827–844.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MangoBoost (2024)</span>
<span class="ltx_bibblock">
MangoBoost. 2024.

</span>
<span class="ltx_bibblock">MangoBoost GPU over RDMA System.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mangoboost.io/products/gpu-over-rdma#fastFact" title="">https://www.mangoboost.io/products/gpu-over-rdma#fastFact</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Medsker et al<span class="ltx_text" id="bib.bib50.3.3.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Larry R Medsker, Lakhmi Jain, et al<span class="ltx_text" id="bib.bib50.4.1">.</span> 2001.

</span>
<span class="ltx_bibblock">Recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.5.1">Design and Applications</em> 5, 64-67 (2001), 2.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024)</span>
<span class="ltx_bibblock">
Meta. 2024.

</span>
<span class="ltx_bibblock">Meta’s DLRM Preprocessing Pipeline.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/dlrm" title="">https://github.com/facebookresearch/dlrm</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mudigere et al<span class="ltx_text" id="bib.bib52.3.3.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, et al<span class="ltx_text" id="bib.bib52.4.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Software-hardware co-design for fast and scalable training of deep learning recommendation models. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.5.1">Proceedings of the 49th Annual International Symposium on Computer Architecture</em>. 993–1011.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murray et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Derek G Murray, Jiri Simsa, Ana Klimovic, and Ihor Indyk. 2021.

</span>
<span class="ltx_bibblock">tf.data: A machine learning data processing framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">arXiv preprint arXiv:2101.12127</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naumov et al<span class="ltx_text" id="bib.bib54.3.3.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al<span class="ltx_text" id="bib.bib54.4.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for personalization and recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.5.1">arXiv preprint arXiv:1906.00091</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nelson (1981)</span>
<span class="ltx_bibblock">
Bruce Jay Nelson. 1981.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Remote procedure call</em>.

</span>
<span class="ltx_bibblock">Carnegie Mellon University.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nvidia (2024a)</span>
<span class="ltx_bibblock">
Nvidia. 2024a.

</span>
<span class="ltx_bibblock">Nvidia Data Loading Library.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/DALI/tree/main/dali/pipeline" title="">https://github.com/NVIDIA/DALI/tree/main/dali/pipeline</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nvidia (2024b)</span>
<span class="ltx_bibblock">
Nvidia. 2024b.

</span>
<span class="ltx_bibblock">Nvidia DLRM Example.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM" title="">https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nvidia (2024c)</span>
<span class="ltx_bibblock">
Nvidia. 2024c.

</span>
<span class="ltx_bibblock">Nvidia DLRM Optimization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/optimizing-dlrm-on-nvidia-gpus/" title="">https://developer.nvidia.com/blog/optimizing-dlrm-on-nvidia-gpus/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nvidia (2024d)</span>
<span class="ltx_bibblock">
Nvidia. 2024d.

</span>
<span class="ltx_bibblock">Nvidia Rapids.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/rapids" title="">https://developer.nvidia.com/rapids</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peltenburg et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Johan Peltenburg, Lars TJ Van Leeuwen, Joost Hoozemans, Jian Fang, Zaid Al-Ars, and H Peter Hofstee. 2020.

</span>
<span class="ltx_bibblock">Battling the CPU bottleneck in apache parquet to arrow conversion using FPGA. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">2020 international conference on Field-Programmable technology (ICFPT)</em>. IEEE, 281–286.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peltenburg et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Johan Peltenburg, Jeroen Van Straten, Lars Wijtemans, Lars Van Leeuwen, Zaid Al-Ars, and Peter Hofstee. 2019.

</span>
<span class="ltx_bibblock">Fletcher: A framework to efficiently integrate FPGA accelerators with apache arrow. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">2019 29th International Conference on Field Programmable Logic and Applications (FPL)</em>. IEEE, 270–277.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ivy Peng, Roger Pearce, and Maya Gokhale. 2020.

</span>
<span class="ltx_bibblock">On the memory underutilization: Exploring disaggregated memory on hpc systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</em>. IEEE, 183–190.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>. 1532–1543.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phani et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Arnab Phani, Lukas Erlbacher, and Matthias Boehm. 2022.

</span>
<span class="ltx_bibblock">UPLIFT: parallelization strategies for feature transformations in machine learning workloads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the VLDB Endowment</em> 15, 11 (2022), 2929–2938.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Putnam (2017)</span>
<span class="ltx_bibblock">
Andrew Putnam. 2017.

</span>
<span class="ltx_bibblock">FPGAs in the datacenter: Combining the worlds of hardware and software development. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the on Great Lakes Symposium on VLSI 2017</em>. 5–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raman and Hellerstein (2001)</span>
<span class="ltx_bibblock">
Vijayshankar Raman and Joseph M. Hellerstein. 2001.

</span>
<span class="ltx_bibblock">Potter’s Wheel: An Interactive Data Cleaning System. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 27th International Conference on Very Large Data Bases</em> <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">(VLDB ’01)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rigler et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Suzanne Rigler, William Bishop, and Andrew Kennings. 2007.

</span>
<span class="ltx_bibblock">FPGA-based lossless data compression using Huffman and LZ77 algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">2007 Canadian conference on electrical and computer engineering</em>. IEEE, 1235–1238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidler et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
David Sidler, Zeke Wang, Monica Chiosa, Amit Kulkarni, and Gustavo Alonso. 2020.

</span>
<span class="ltx_bibblock">StRoM: smart remote memory. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings of the Fifteenth European Conference on Computer Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>. 1441–1450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yutian Sun, Tim Meehan, Rebecca Schlussel, Wenlei Xie, Masha Basmanova, Orri Erling, Andrii Rosa, Shixuan Fan, Rongrong Zhong, Arun Thirupathi, Nikhil Collooru, Ke Wang, Sameer Agarwal, Arjun Gupta, Dionysios Logothetis, Kostas Xirogiannopoulos, Amit Dutta, Varun Gajjala, Rohit Jain, Ajay Palakuzhy, Prithvi Pandian, Sergey Pershin, Abhisek Saikia, Pranjal Shankhdhar, Neerad Somanchi, Swapnil Tailor, Jialiang Tan, Sreeni Viswanadha, Zac Wen, Biswapesh
Chattopadhyay, Bin Fan, Deepak Majeti, and Aditi Pandit. 2023.

</span>
<span class="ltx_bibblock">Presto: A Decade of SQL Analytics at Meta.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">Proceedings of the ACM Conference on Management of Data (SIGMOD)</em> 1, 2 (jun 2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarafdar et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Naif Tarafdar, Thomas Lin, Eric Fukuda, Hadi Bannazadeh, Alberto Leon-Garcia, and Paul Chow. 2017.

</span>
<span class="ltx_bibblock">Enabling flexible network FPGA clusters in a heterogeneous cloud data center. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</em>. 237–246.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tensorflow (2024)</span>
<span class="ltx_bibblock">
Tensorflow. 2024.

</span>
<span class="ltx_bibblock">Tensorflow DLRM.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tensorflow/models/tree/master/official/recommendation/ranking" title="">https://github.com/tensorflow/models/tree/master/official/recommendation/ranking</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tokusashi et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yuta Tokusashi, Huynh Tu Dang, Fernando Pedone, Robert Soulé, and Noa Zilberman. 2019.

</span>
<span class="ltx_bibblock">The case for in-network computing on demand. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">Proceedings of the Fourteenth EuroSys Conference 2019</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Um et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Taegeon Um, Byungsoo Oh, Byeongchan Seo, Minhyeok Kweun, Goeun Kim, and Woo-Yeon Lee. 2023.

</span>
<span class="ltx_bibblock">Fastflow: Accelerating deep learning model training with smart offloading of input data pipeline.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the VLDB Endowment</em> 16, 5 (2023), 1086–1099.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vohra and Vohra (2016)</span>
<span class="ltx_bibblock">
Deepak Vohra and Deepak Vohra. 2016.

</span>
<span class="ltx_bibblock">Apache parquet.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Practical Hadoop Ecosystem: A Definitive Guide to Hadoop-Related Frameworks and Tools</em> (2016), 325–335.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Zeke Wang, Hongjing Huang, Jie Zhang, Fei Wu, and Gustavo Alonso. 2022a.

</span>
<span class="ltx_bibblock">FpgaNIC: An FPGA-based Versatile 100Gb SmartNIC for GPUs. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">2022 USENIX Annual Technical Conference (ATC)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Zheng Wang, Yuke Wang, Boyuan Feng, Dheevatsa Mudigere, Bharath Muthiah, and Yufei Ding. 2022b.

</span>
<span class="ltx_bibblock">EL-rec: Efficient large-scale recommendation model training via tensor-train embedding table. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">SC22: International Conference for High Performance Computing, Networking, Storage and Analysis</em>. IEEE, 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weerasinghe et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jagath Weerasinghe, Francois Abel, Christoph Hagleitner, and Andreas Herkersdorf. 2015.

</span>
<span class="ltx_bibblock">Enabling FPGAs in hyperscale data centers. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)</em>. IEEE, 1078–1086.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Ying Yang, Niccolò Meneghetti, Ronny Fehling, Zhen Hua Liu, and Oliver Kennedy. 2015.

</span>
<span class="ltx_bibblock">Lenses: An on-Demand Approach to ETL.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proc. VLDB Endow.</em> 8, 12 (aug 2015), 1578–1589.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Qizhen Zhang, Yifan Cai, Xinyi Chen, Sebastian Angel, Ang Chen, Vincent Liu, and Boon Thau Loo. 2020a.

</span>
<span class="ltx_bibblock">Understanding the effect of data center resource disaggregation on production dbmss.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">Proceedings of the VLDB Endowment</em> 13, 9 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib81.9.3.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Teng Zhang, Jianying Wang, Xuntao Cheng, Hao Xu, Nanlong Yu, Gui Huang, Tieying Zhang, Dengcheng He, Feifei Li, Wei Cao, et al<span class="ltx_text" id="bib.bib81.10.1">.</span> 2020b.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib81.1.m1.1"><semantics id="bib.bib81.1.m1.1a"><mo id="bib.bib81.1.m1.1.1" stretchy="false" xref="bib.bib81.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.1.m1.1b"><ci id="bib.bib81.1.m1.1.1.cmml" xref="bib.bib81.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.1.m1.1d">{</annotation></semantics></math>FPGA-Accelerated<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib81.2.m2.1"><semantics id="bib.bib81.2.m2.1a"><mo id="bib.bib81.2.m2.1.1" stretchy="false" xref="bib.bib81.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.2.m2.1b"><ci id="bib.bib81.2.m2.1.1.cmml" xref="bib.bib81.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.2.m2.1d">}</annotation></semantics></math> Compactions for <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib81.3.m3.1"><semantics id="bib.bib81.3.m3.1a"><mo id="bib.bib81.3.m3.1.1" stretchy="false" xref="bib.bib81.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.3.m3.1b"><ci id="bib.bib81.3.m3.1.1.cmml" xref="bib.bib81.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.3.m3.1d">{</annotation></semantics></math>LSM-based<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib81.4.m4.1"><semantics id="bib.bib81.4.m4.1a"><mo id="bib.bib81.4.m4.1.1" stretchy="false" xref="bib.bib81.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.4.m4.1b"><ci id="bib.bib81.4.m4.1.1.cmml" xref="bib.bib81.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.4.m4.1d">}</annotation></semantics></math><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib81.5.m5.1"><semantics id="bib.bib81.5.m5.1a"><mo id="bib.bib81.5.m5.1.1" stretchy="false" xref="bib.bib81.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.5.m5.1b"><ci id="bib.bib81.5.m5.1.1.cmml" xref="bib.bib81.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.5.m5.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.5.m5.1d">{</annotation></semantics></math>Key-Value<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib81.6.m6.1"><semantics id="bib.bib81.6.m6.1a"><mo id="bib.bib81.6.m6.1.1" stretchy="false" xref="bib.bib81.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib81.6.m6.1b"><ci id="bib.bib81.6.m6.1.1.cmml" xref="bib.bib81.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.6.m6.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib81.6.m6.1d">}</annotation></semantics></math> Store. In <em class="ltx_emph ltx_font_italic" id="bib.bib81.11.1">18th USENIX Conference on File and Storage Technologies (FAST 20)</em>. 225–237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib82.3.3.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hanyu Zhao, Zhi Yang, Yu Cheng, Chao Tian, Shiru Ren, Wencong Xiao, Man Yuan, Langshi Chen, Kaibo Liu, Yang Zhang, et al<span class="ltx_text" id="bib.bib82.4.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">GoldMiner: Elastic Scaling of Training Data Pre-Processing Pipelines for Deep Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.5.1">Proceedings of the ACM on Management of Data</em> 1, 2 (2023), 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib83.3.3.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mark Zhao, Niket Agarwal, Aarti Basant, Buğra Gedik, Satadru Pan, Mustafa Ozdal, Rakesh Komuravelli, Jerry Pan, Tianshu Bao, Haowei Lu, et al<span class="ltx_text" id="bib.bib83.4.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Understanding data storage and ingestion for large-scale deep recommendation model training: Industrial product. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.5.1">Proceedings of the 49th Annual International Symposium on Computer Architecture</em>. 1042–1057.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib84.3.3.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Mark Zhao, Dhruv Choudhary, Devashish Tyagi, Ajay Somani, Max Kaplan, Sung-Han Lin, Sarunya Pumma, Jongsoo Park, Aarti Basant, Niket Agarwal, et al<span class="ltx_text" id="bib.bib84.4.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">RecD: Deduplication for end-to-end deep learning recommendation model training infrastructure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.5.1">Proceedings of Machine Learning and Systems</em> 5 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019.

</span>
<span class="ltx_bibblock">Recommending what video to watch next: a multitask ranking system. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Proceedings of the 13th ACM Conference on Recommender Systems</em>. 43–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Deep interest network for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1059–1068.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yue Zhu, Weikuan Yu, Bing Jiao, Kathryn Mohror, Adam Moody, and Fahim Chowdhury. 2019.

</span>
<span class="ltx_bibblock">Efficient user-level storage disaggregation for deep learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">2019 IEEE International Conference on Cluster Computing (CLUSTER)</em>. IEEE, 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 09:24:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
