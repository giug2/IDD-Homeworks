<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.13270] Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models</title><meta property="og:description" content="Machine translation (MT) systems translate text between different languages by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples.
Although neural machine â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.13270">

<!--Generated on Sat Mar  2 11:41:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shilin He 
<br class="ltx_break">The Chinese University of Hong Kong 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_sansserif">slhe@cse.cuhk.edu.hk </span>&amp;<span id="id2.2.id2" class="ltx_text ltx_font_sansserif">Xing Wang 
<br class="ltx_break">Tencent AI Lab 
<br class="ltx_break">brightxwang@tencent.com <span id="id2.2.id2.1" class="ltx_ERROR undefined">\AND</span>Shuming Shi 
<br class="ltx_break">Tencent AI Lab 
<br class="ltx_break">shumingshi@tencent.com </span>&amp;<span id="id3.3.id3" class="ltx_text ltx_font_sansserif">Michael R. Lyu 
<br class="ltx_break">The Chinese University of Hong Kong 
<br class="ltx_break">lyu@cse.cuhk.edu.hk </span>&amp;<span id="id4.4.id4" class="ltx_text ltx_font_sansserif">Zhaopeng Tu 
<br class="ltx_break">Tencent AI Lab 
<br class="ltx_break">zptu@tencent.com
</span>
</span><span class="ltx_author_notes">Â Â Work done when interning at Tencent AI Lab.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Machine translation (MT) systems translate text between different languages by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples.
Although neural machine translation (NMT) has led the field of MT, we have a poor understanding on how and why it works. In this paper, we bridge the gap by assessing the bilingual knowledge learned by NMT models with <span id="id5.id1.1" class="ltx_text ltx_font_italic">phrase table</span> â€“ an interpretable table of bilingual lexicons.
We extract the phrase table from the training examples that a NMT model correctly predicts.
Extensive experiments on widely-used datasets show that the phrase table is reasonable and consistent against language pairs and random seeds. Equipped with the interpretable phrase table, we find that NMT models learn patterns from simple to complex and distill essential bilingual knowledge from the training examples.
We also revisit some advances that potentially affect the learning of bilingual knowledge (e.g., back-translation), and report some interesting findings.
We believe this work opens a new angle to interpret NMT with statistic models, and provides empirical supports for recent advances in improving NMT models.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Modern machine translation (MT) systems aim to produce fluent and adequate translations by automatically learning in-depth knowledge of bilingual lexicons, grammar and semantics from the training examples.
Two technological advances solving this problem with statistical and neural techniques, statistical machine translation (SMT) and neural machine translation (NMT), have seen vast progress over the last two decades.
SMT models generate translations on the basis of several statistical models that <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">explicitly</span> represent the knowledge bases, such as translation model for bilingual lexicons, reordering and language models for grammar and semanticsÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib23" title="" class="ltx_ref">2009</a>)</cite>. Recently, NMT models have advanced the state-of-the-art by <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">implicitly</span> modeling the knowledge bases in a large neural network, which are trained jointly to maximize the translation performanceÂ <cite class="ltx_cite ltx_citemacro_cite">Bahdanau etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>); Gehring etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>); Vaswani etÂ al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>.
Despite their power with a massive amount of parameters, we have limited understanding of how and why NMT models work, which poses great challenges for error analysis and model refinement.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we bridge the gap by assessing the knowledge bases learned by NMT models with the statistical models of SMT systems.
We believed (and in fact, provide some evidence to support the claim) that although using different forms (e.g., continuous vs. discrete) to represent the knowledge, NMT and SMT models are identical in modeling the essential knowledge.
In the long-goal journey, we start with probing the bilingual knowledge with the translation model, also known as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">phrase table</span>, which is one core component of SMT systems to represent the bilingual lexicons.
Bilingual knowledge is at the core of adequacy modeling, which is a major weakness of the NMT modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Tu etÂ al. (<a href="#bib.bib43" title="" class="ltx_ref">2016</a>)</cite>.
Phrase table has proven its effectiveness for carrying useful bilingual knowledge, which can be seamlessly integrated to NMT modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib48" title="" class="ltx_ref">2018</a>); Lample etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>. For instance, Â <cite class="ltx_cite ltx_citemacro_citet">Lample etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> have advanced the SOTA of unsupervised NMT by learning to align for phrase embeddings based on an external phrase table.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="232" height="51" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Output of an English <math id="S1.F1.sf1.2.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S1.F1.sf1.2.m1.1b"><mo stretchy="false" id="S1.F1.sf1.2.m1.1.1" xref="S1.F1.sf1.2.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S1.F1.sf1.2.m1.1c"><ci id="S1.F1.sf1.2.m1.1.1.cmml" xref="S1.F1.sf1.2.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.sf1.2.m1.1d">\Rightarrow</annotation></semantics></math> German NMT model</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x2.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="232" height="102" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Phrase table extracted from the NMT model</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The output of a NMT model (a) can be explained by the extracted phrase table (b). </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Specifically, we extract the phrase table from the predictions of NMT models, which is inspired by recent work on investigating the forgetting phenomenon of training examples in the image classification taskÂ <cite class="ltx_cite ltx_citemacro_cite">Toneva etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>. Intuitively, if a trained NMT model can successfully recover (part of) a training example, the NMT model is more likely to have learned the necessary bilingual lexicons for the recovery.
Experimental results on three representative language pairs and random seeds show that the extracted phrase table correlates well with the NMT model performance, demonstrating that the phrase table can reasonably represent the bilingual knowledge learned by NMT models.
FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example, in which the phrase table extracted from a NMT model can well explain the generated output.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With the interpretable phrase table in hand, we are able to better understand behaviors of NMT models in many aspects. We start with investigating the learning dynamics of bilingual knowledge. We find that NMT tend to first learn simple patterns and then complex patterns, and the catastrophic forgetting phenomenon occurs during the training.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We followÂ <cite class="ltx_cite ltx_citemacro_citet">Toneva etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> to define â€œforgetting eventâ€ to have occurred when a training example transitions from being predicted correctly to incorrectly during training.</span></span></span>
We also reveal that one of the strengths of NMT models over SMT models lie in their ability to distill high-quality bilingual knowledge from the training data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We then revisit some advances in improving NMT models, which potentially affect the learning of bilingual knowledge. Though we cannot claim causality, we have several observations:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Model Capacity</span>:
We thought it likely that increasing model capacity learns more bilingual lexicons. This turned out to be false. Transformer-Big outperforms Transformer-Base by 1.3 BLEU points, while the extracted phrase tables are almost the same. We conjecture that the strengths of larger models lie in a better learning of more complex knowledge, such as composition rules to combine the bilingual lexicons.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Data Augmentation</span>: We investigate back-translationÂ <cite class="ltx_cite ltx_citemacro_cite">Sennrich etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2016</a>)</cite> and forward-translationÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang and Zong (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>); He etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>, which introduce additionally synthetic parallel corpus. Both techniques improve performance not only by introducing new bilingual knowledge, but also with a better quality estimation of existing knowledge.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Domain Adaptation</span>: Fine-tune is a simple yet effective technique in domain adaptation, which learns to transfer out-domain knowledge to in-domainÂ <cite class="ltx_cite ltx_citemacro_cite">Luong and Manning (<a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite>. As expected, by adapting to the target-domain, the fine-tune approach learns more and better bilingual knowledge from the target-domain data.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">The key contributions of this paper are:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">Our study demonstrates the reasonableness and effectiveness of assessing the NMT knowledge with statistic models, which opens up a new angle to interpret NMT models.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We report several interesting findings that can help humans better analyze and understand NMT models and some recent advances.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evolution of MT Models.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The MT task has a long history, in which the techniques have evolved from rule-based MT (RBMT)Â <cite class="ltx_cite ltx_citemacro_cite">Hayes-Roth (<a href="#bib.bib15" title="" class="ltx_ref">1985</a>); Sato (<a href="#bib.bib37" title="" class="ltx_ref">1992</a>)</cite>, through SMTÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">1993</a>); Och and Ney (<a href="#bib.bib32" title="" class="ltx_ref">2004</a>)</cite>, to NMTÂ <cite class="ltx_cite ltx_citemacro_cite">Sutskever etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2014</a>); Bahdanau etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. RBMT methods require large sets of linguistic rules and extensive lexicons with morphological, syntactic, and semantic information, which are manually constructed by humans. Benefiting from the availability of large amounts of parallel data in 1990s, SMT approaches relieve the labor-intensive problem of RBMT by automatically learning the linguistic knowledge from bilingual corpora with statistic models.
More recently, NMT models have taken the field of MT by building a single network that can be trained on the corpora in an end-to-end manner.
Several studies have shown that representations learned by NMT models contain a substantial amount of linguistic information on multiple levels: morphologicalÂ <cite class="ltx_cite ltx_citemacro_cite">Belinkov etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite>, syntacticÂ <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2016</a>)</cite>, and semanticÂ <cite class="ltx_cite ltx_citemacro_cite">Hill etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">In the development circle of each generation, MT models are generally improved with techniques that are essential in the last generation. For example,Â <cite class="ltx_cite ltx_citemacro_citet">Chiang (<a href="#bib.bib7" title="" class="ltx_ref">2005</a>)</cite> andÂ <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2006</a>)</cite> relieved the nonfluent translation problem of SMT models by automatically learning syntactic rules from the parallel corpus, which are created manually by humans in RBMT systems.Â <cite class="ltx_cite ltx_citemacro_citet">Tu etÂ al. (<a href="#bib.bib43" title="" class="ltx_ref">2016</a>)</cite> alleviated the inadequate translation problem of NMT models by introducing the coverage mechanism, which is a standard concept in SMT to indicate how many source words have been translated.
Inspired by previous these studies, we hypothesize that MT models of different generations are possibly identical to model the essential knowledge. In this work, we propose to leverage the phrase table â€“ a basic module of SMT system, to assess the bilingual knowledge learned by NMT models.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Exploiting Phrase Table for NMT.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Phrase table is an essential component of SMT systems, which records the correspondence between bilingual lexiconsÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib23" title="" class="ltx_ref">2009</a>)</cite>.
Previous studies have incorporated phrase table as an external signal to guide the generation of NMT modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>); Zhang etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2017</a>); Zhao etÂ al. (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>); Guo etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>.
All these works show that the bilingual knowledge in phrase table can be identical to those in NMT models, and thereby can be seamlessly integrated to NMT models. Based on this observation, we employ the phrase table as an assessment tool of bilingual knowledge for NMT models.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lample etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> have advanced the SOTA of unsupervised NMT by evolving from learning alignment of word embeddings to learning to align for phrase embeddings based on an external phrase table, which is identical to the evolution of SMT from word-based modelÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">1993</a>)</cite> to phrase-based modelÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2003</a>)</cite>. This reconfirms our hypothesis that MT models of different generations are identical to model the essential knowledge, and thus share similar evolving trends.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Interpretability of NMT Models.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The interpretability of NMT models has recently been approached mainly from two aspectsÂ <cite class="ltx_cite ltx_citemacro_cite">Alvarez-Melis and Jaakkola (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>: (1) <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">model interpretability</span>, which aims to understand the internal properties of NMT models, such as layer representationsÂ <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2016</a>); Belinkov etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>); Yang etÂ al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>); Voita etÂ al. (<a href="#bib.bib45" title="" class="ltx_ref">2019a</a>)</cite> and attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Voita etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2019b</a>); Jain and Wallace (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>); Wiegreffe and Pinter (<a href="#bib.bib49" title="" class="ltx_ref">2019</a>); Li etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>; and (2) <span id="S2.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">behavior interpretability</span>, which aims to explain particular behaviors of a NMT model, such as the input-output behaviorÂ <cite class="ltx_cite ltx_citemacro_cite">Alvarez-Melis and Jaakkola (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>); Ding etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>); He etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>. In this paper, we focus on the second thread from a complementary viewpoint â€“ assessing the bilingual knowledge learned by NMT models, which can provide explanations for model output, as shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Assessing Bilingual Knowledge with Phrase Table</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe how to extract phrase table from the predictions of NMT models (SectionÂ <a href="#S3.SS1" title="3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), and verify our hypothesis by checking the correlation between the extracted phrase table and NMT performance (SectionÂ <a href="#S3.SS3" title="3.3 Evaluating the Phrase Table â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Methodology</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">There are many possible ways to implement the general idea of extracting phrase table from the predictions of NMT models. The aim of this paper is not to explore this whole space but simply to show that one fairly straightforward implementation works well and the proposed framework is reasonable. We leave the exploitation of more advanced forms of statistic models on bilingual knowledge such as syntax rulesÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2006</a>)</cite> and discontinuous phrasesÂ <cite class="ltx_cite ltx_citemacro_cite">Galley and Manning (<a href="#bib.bib12" title="" class="ltx_ref">2010</a>)</cite> for future work.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We follow the standard pipeline in SMT to construct the phrase table with a two-phase approach.
The first phase, which is the focus of this paper, is <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">phrase extraction</span> where the bilingual phrase pairs are extracted from a word-aligned parallel data. Secondly, each phrase pair is assigned with some scores, which are estimated based on the occurrences of these phrases or their words on the same word-aligned training data.
The key challenge lies in how to incorporate the prior of NMT predictions into the SMT pipeline. In this study, we model the NMT priors as a mask sequence, which is integrated into the standard SMT pipeline as a constraint, as listed in AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Constructing Phrase Table</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1:</span><span id="alg1.l1.1" class="ltx_text ltx_font_bold">Input</span>: training example (<math id="alg1.l1.m1.1" class="ltx_Math" alttext="\bf x" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">\bf x</annotation></semantics></math>, <math id="alg1.l1.m2.1" class="ltx_Math" alttext="\bf y" display="inline"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">\bf y</annotation></semantics></math>), alignment <math id="alg1.l1.m3.1" class="ltx_Math" alttext="\bf a" display="inline"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\bf a</annotation></semantics></math>, mask <math id="alg1.l1.m4.1" class="ltx_Math" alttext="\bf m" display="inline"><semantics id="alg1.l1.m4.1a"><mi id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">\bf m</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2:</span><span id="alg1.l2.1" class="ltx_text ltx_font_bold">Output</span>: phrase set <math id="alg1.l2.m1.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="alg1.l2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">\mathcal{R}</annotation></semantics></math>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3:</span><span id="alg1.l3.1" class="ltx_text ltx_font_bold">procedure</span>Â <span id="alg1.l3.2" class="ltx_text ltx_font_smallcaps">PhraseTable</span>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4:</span>Â Â Â Â Â <span id="alg1.l4.1" class="ltx_text ltx_font_smallcaps">Extraction</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5:</span>Â Â Â Â Â <span id="alg1.l5.1" class="ltx_text ltx_font_smallcaps">Estimation</span>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6:</span><span id="alg1.l6.1" class="ltx_text ltx_font_bold">procedure</span>Â <span id="alg1.l6.2" class="ltx_text ltx_font_smallcaps">Extraction</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7:</span>Â Â Â Â Â <math id="alg1.l7.m1.1" class="ltx_Math" alttext="\widehat{\mathcal{R}}\leftarrow" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mover accent="true" id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l7.m1.1.1.2.2" xref="alg1.l7.m1.1.1.2.2.cmml">â„›</mi><mo id="alg1.l7.m1.1.1.2.1" xref="alg1.l7.m1.1.1.2.1.cmml">^</mo></mover><mo stretchy="false" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">â†</mo><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">â†</ci><apply id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2"><ci id="alg1.l7.m1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.2.1">^</ci><ci id="alg1.l7.m1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.2.2">â„›</ci></apply><csymbol cd="latexml" id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\widehat{\mathcal{R}}\leftarrow</annotation></semantics></math> extract candidates from {(<math id="alg1.l7.m2.1" class="ltx_Math" alttext="\bf x" display="inline"><semantics id="alg1.l7.m2.1a"><mi id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">\bf x</annotation></semantics></math>, <math id="alg1.l7.m3.1" class="ltx_Math" alttext="\bf y" display="inline"><semantics id="alg1.l7.m3.1a"><mi id="alg1.l7.m3.1.1" xref="alg1.l7.m3.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m3.1b"><ci id="alg1.l7.m3.1.1.cmml" xref="alg1.l7.m3.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m3.1c">\bf y</annotation></semantics></math>), <math id="alg1.l7.m4.1" class="ltx_Math" alttext="\bf a" display="inline"><semantics id="alg1.l7.m4.1a"><mi id="alg1.l7.m4.1.1" xref="alg1.l7.m4.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m4.1b"><ci id="alg1.l7.m4.1.1.cmml" xref="alg1.l7.m4.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m4.1c">\bf a</annotation></semantics></math>}

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8:</span>Â Â Â Â Â <span id="alg1.l8.2" class="ltx_text ltx_font_bold">for</span>Â each <math id="alg1.l8.m1.1" class="ltx_Math" alttext="r\in\widehat{\mathcal{R}}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">r</mi><mo id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">âˆˆ</mo><mover accent="true" id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml">â„›</mi><mo id="alg1.l8.m1.1.1.3.1" xref="alg1.l8.m1.1.1.3.1.cmml">^</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><in id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1"></in><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">ğ‘Ÿ</ci><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><ci id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.1">^</ci><ci id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2">â„›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">r\in\widehat{\mathcal{R}}</annotation></semantics></math>Â <span id="alg1.l8.3" class="ltx_text ltx_font_bold">do</span> <span id="alg1.l8.1" class="ltx_text" style="float:right;"><math id="alg1.l8.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l8.1.m1.1a"><mo id="alg1.l8.1.m1.1.1" xref="alg1.l8.1.m1.1.1.cmml">â–·</mo><annotation-xml encoding="MathML-Content" id="alg1.l8.1.m1.1b"><ci id="alg1.l8.1.m1.1.1.cmml" xref="alg1.l8.1.m1.1.1">â–·</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.1.m1.1c">\triangleright</annotation></semantics></math> <span id="alg1.l8.1.1" class="ltx_text ltx_font_italic">priors of NMT predictions</span>
</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9:</span>Â Â Â Â Â Â Â Â Â <span id="alg1.l9.1" class="ltx_text ltx_font_bold">if</span>Â <math id="alg1.l9.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">r</annotation></semantics></math> is consistent with <math id="alg1.l9.m2.1" class="ltx_Math" alttext="\bf m" display="inline"><semantics id="alg1.l9.m2.1a"><mi id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml">ğ¦</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><ci id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1">ğ¦</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">\bf m</annotation></semantics></math>Â <span id="alg1.l9.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10:</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â <math id="alg1.l10.m1.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="alg1.l10.m1.1a"><mi class="ltx_font_mathcaligraphic" id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">\mathcal{R}</annotation></semantics></math>.append(<math id="alg1.l10.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="alg1.l10.m2.1a"><mi id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><ci id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">r</annotation></semantics></math>)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â 
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11:</span><span id="alg1.l11.1" class="ltx_text ltx_font_bold">procedure</span>Â <span id="alg1.l11.2" class="ltx_text ltx_font_smallcaps">Estimation</span>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12:</span>Â Â Â Â Â standard procedure

</div>
</div>
</figure>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Building <span id="S3.SS1.SSS0.Px1.1.1" class="ltx_text ltx_font_italic">Masked</span> Word-Aligned Parallel Data.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.7" class="ltx_p">Inspired byÂ <cite class="ltx_cite ltx_citemacro_citet">Toneva etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, we define â€œ<span id="S3.SS1.SSS0.Px1.p1.7.1" class="ltx_text ltx_font_italic">memorized phrase pair</span>â€ to be extracted from the associated (partial) training example, which is predicted correctly by the NMT model.
To this end, we first decompose the sequence generation of NMT into a series of classification tasks. Given a training example <math id="S3.SS1.SSS0.Px1.p1.1.m1.3" class="ltx_Math" alttext="(\textbf{x}=\{x_{1},\dots,x_{I}\},\textbf{y}=\{y_{1},\dots,y_{J}\})" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.3a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.2">(</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.3.cmml"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4a.cmml">x</mtext><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.3.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml">{</mo><msub id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.4" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.5" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.2.cmml">x</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.3.cmml">I</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.6" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml">}</mo></mrow></mrow><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4a.cmml">y</mtext><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.3.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml">{</mo><msub id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.4" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2.cmml">â€¦</mi><mo id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.5" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.3.cmml">J</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.6" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml">}</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.3b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.3a.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1"><eq id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.3"></eq><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4a.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.4">x</mtext></ci><set id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">â€¦</ci><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.2">ğ‘¥</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.1.1.2.2.2.3">ğ¼</ci></apply></set></apply><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2"><eq id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.3"></eq><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4a.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.4">y</mtext></ci><set id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.2">ğ‘¦</ci><cn type="integer" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.2.2">â€¦</ci><apply id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.3.3.1.1.2.2.2.2.2.3">ğ½</ci></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.3c">(\textbf{x}=\{x_{1},\dots,x_{I}\},\textbf{y}=\{y_{1},\dots,y_{J}\})</annotation></semantics></math> and a model <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">M</annotation></semantics></math>, we use the model <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">M</annotation></semantics></math> to force-decode <span id="S3.SS1.SSS0.Px1.p1.7.2" class="ltx_text ltx_markedasmath ltx_font_bold">x</span> to <span id="S3.SS1.SSS0.Px1.p1.7.3" class="ltx_text ltx_markedasmath ltx_font_bold">y</span>, and check whether each <math id="S3.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="y_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.1c">y_{j}</annotation></semantics></math> is correctly predicted by <math id="S3.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.7.m7.1a"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.7.m7.1b"><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.7.m7.1c">M</annotation></semantics></math>:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="m_{j}=\begin{cases}1,&amp;\text{if }y_{j}=\operatorname*{arg\,max}_{y}\mathcal{N}_{j}[y]\\
0,&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.5" xref="S3.Ex1.m1.4.5.cmml"><msub id="S3.Ex1.m1.4.5.2" xref="S3.Ex1.m1.4.5.2.cmml"><mi id="S3.Ex1.m1.4.5.2.2" xref="S3.Ex1.m1.4.5.2.2.cmml">m</mi><mi id="S3.Ex1.m1.4.5.2.3" xref="S3.Ex1.m1.4.5.2.3.cmml">j</mi></msub><mo id="S3.Ex1.m1.4.5.1" xref="S3.Ex1.m1.4.5.1.cmml">=</mo><mrow id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.5.3.1.cmml"><mo id="S3.Ex1.m1.4.4.5" xref="S3.Ex1.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.Ex1.m1.4.4.4" xref="S3.Ex1.m1.4.5.3.1.cmml"><mtr id="S3.Ex1.m1.4.4.4a" xref="S3.Ex1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.4.4.4b" xref="S3.Ex1.m1.4.5.3.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.4.5.3.1.cmml"><mn id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml">1</mn><mo id="S3.Ex1.m1.1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.4.5.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.4.4.4c" xref="S3.Ex1.m1.4.5.3.1.cmml"><mrow id="S3.Ex1.m1.2.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.2.1.cmml"><mrow id="S3.Ex1.m1.2.2.2.2.2.1.3" xref="S3.Ex1.m1.2.2.2.2.2.1.3.cmml"><mtext id="S3.Ex1.m1.2.2.2.2.2.1.3.2" xref="S3.Ex1.m1.2.2.2.2.2.1.3.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.2.1.3.1" xref="S3.Ex1.m1.2.2.2.2.2.1.3.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.2.2.2.2.2.1.3.3" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.1.3.3.2" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3.2.cmml">y</mi><mi id="S3.Ex1.m1.2.2.2.2.2.1.3.3.3" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3.3.cmml">j</mi></msub></mrow><mo id="S3.Ex1.m1.2.2.2.2.2.1.2" xref="S3.Ex1.m1.2.2.2.2.2.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.2.2.2.1.4" xref="S3.Ex1.m1.2.2.2.2.2.1.4.cmml"><mrow id="S3.Ex1.m1.2.2.2.2.2.1.4.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.cmml"><msub id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.cmml"><mrow id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.1" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.3" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.3.cmml">max</mi></mrow><mi id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.3" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.3.cmml">y</mi></msub><mo id="S3.Ex1.m1.2.2.2.2.2.1.4.2a" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.cmml">â¡</mo><msub id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.2.cmml">ğ’©</mi><mi id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.3" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.3.cmml">j</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.2.2.2.2.2.1.4.1" xref="S3.Ex1.m1.2.2.2.2.2.1.4.1.cmml">â€‹</mo><mrow id="S3.Ex1.m1.2.2.2.2.2.1.4.3.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.2.1.4.3.2.1" xref="S3.Ex1.m1.2.2.2.2.2.1.4.3.1.1.cmml">[</mo><mi id="S3.Ex1.m1.2.2.2.2.2.1.1" xref="S3.Ex1.m1.2.2.2.2.2.1.1.cmml">y</mi><mo stretchy="false" id="S3.Ex1.m1.2.2.2.2.2.1.4.3.2.2" xref="S3.Ex1.m1.2.2.2.2.2.1.4.3.1.1.cmml">]</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.Ex1.m1.4.4.4d" xref="S3.Ex1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.4.4.4e" xref="S3.Ex1.m1.4.5.3.1.cmml"><mrow id="S3.Ex1.m1.3.3.3.3.1.1.3" xref="S3.Ex1.m1.4.5.3.1.cmml"><mn id="S3.Ex1.m1.3.3.3.3.1.1.1" xref="S3.Ex1.m1.3.3.3.3.1.1.1.cmml">0</mn><mo id="S3.Ex1.m1.3.3.3.3.1.1.3.1" xref="S3.Ex1.m1.4.5.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.Ex1.m1.4.4.4f" xref="S3.Ex1.m1.4.5.3.1.cmml"><mtext id="S3.Ex1.m1.4.4.4.4.2.1" xref="S3.Ex1.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.5.cmml" xref="S3.Ex1.m1.4.5"><eq id="S3.Ex1.m1.4.5.1.cmml" xref="S3.Ex1.m1.4.5.1"></eq><apply id="S3.Ex1.m1.4.5.2.cmml" xref="S3.Ex1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.5.2.1.cmml" xref="S3.Ex1.m1.4.5.2">subscript</csymbol><ci id="S3.Ex1.m1.4.5.2.2.cmml" xref="S3.Ex1.m1.4.5.2.2">ğ‘š</ci><ci id="S3.Ex1.m1.4.5.2.3.cmml" xref="S3.Ex1.m1.4.5.2.3">ğ‘—</ci></apply><apply id="S3.Ex1.m1.4.5.3.1.cmml" xref="S3.Ex1.m1.4.4"><csymbol cd="latexml" id="S3.Ex1.m1.4.5.3.1.1.cmml" xref="S3.Ex1.m1.4.4.5">cases</csymbol><cn type="integer" id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1">1</cn><apply id="S3.Ex1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1"><eq id="S3.Ex1.m1.2.2.2.2.2.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.2"></eq><apply id="S3.Ex1.m1.2.2.2.2.2.1.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3"><times id="S3.Ex1.m1.2.2.2.2.2.1.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.1"></times><ci id="S3.Ex1.m1.2.2.2.2.2.1.3.2a.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.2"><mtext id="S3.Ex1.m1.2.2.2.2.2.1.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.2">ifÂ </mtext></ci><apply id="S3.Ex1.m1.2.2.2.2.2.1.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.3.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.1.3.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3.2">ğ‘¦</ci><ci id="S3.Ex1.m1.2.2.2.2.2.1.3.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.3.3.3">ğ‘—</ci></apply></apply><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4"><times id="S3.Ex1.m1.2.2.2.2.2.1.4.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.1"></times><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2"><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1">subscript</csymbol><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2"><times id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.1"></times><ci id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.2">arg</ci><ci id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.2.3">max</ci></apply><ci id="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.1.3">ğ‘¦</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.2">ğ’©</ci><ci id="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.2.2.3">ğ‘—</ci></apply></apply><apply id="S3.Ex1.m1.2.2.2.2.2.1.4.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.3.2"><csymbol cd="latexml" id="S3.Ex1.m1.2.2.2.2.2.1.4.3.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.4.3.2.1">delimited-[]</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.1.1">ğ‘¦</ci></apply></apply></apply><cn type="integer" id="S3.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1.1">0</cn><ci id="S3.Ex1.m1.4.4.4.4.2.1a.cmml" xref="S3.Ex1.m1.4.4.4.4.2.1"><mtext id="S3.Ex1.m1.4.4.4.4.2.1.cmml" xref="S3.Ex1.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">m_{j}=\begin{cases}1,&amp;\text{if }y_{j}=\operatorname*{arg\,max}_{y}\mathcal{N}_{j}[y]\\
0,&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px1.p1.11" class="ltx_p">where <math id="S3.SS1.SSS0.Px1.p1.8.m1.1" class="ltx_Math" alttext="\mathcal{N}_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.8.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.8.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1.2.cmml">ğ’©</mi><mi id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.8.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1.2">ğ’©</ci><ci id="S3.SS1.SSS0.Px1.p1.8.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.8.m1.1c">\mathcal{N}_{j}</annotation></semantics></math> is the probability distribution of model prediction at step <math id="S3.SS1.SSS0.Px1.p1.9.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.9.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.9.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.9.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.9.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.9.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.9.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.9.m2.1c">j</annotation></semantics></math>. A token <math id="S3.SS1.SSS0.Px1.p1.10.m3.1" class="ltx_Math" alttext="y_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.10.m3.1a"><msub id="S3.SS1.SSS0.Px1.p1.10.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.10.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p1.10.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.10.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.10.m3.1c">y_{j}</annotation></semantics></math> is predicted correctly if it is assigned the highest probability by the model (â€œ<math id="S3.SS1.SSS0.Px1.p1.11.m4.1" class="ltx_Math" alttext="y_{j}=\operatorname*{arg\,max}_{y}\mathcal{N}_{j}[y]" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.11.m4.1a"><mrow id="S3.SS1.SSS0.Px1.p1.11.m4.1.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.cmml"><msub id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.3.cmml">j</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.cmml"><mrow id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.cmml"><msub id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.cmml"><mrow id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.1.cmml">â€‹</mo><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.3.cmml">max</mi></mrow><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.3" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.3.cmml">y</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2a" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.cmml">â¡</mo><msub id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.2.cmml">ğ’©</mi><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.3" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.3.cmml">j</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.2.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.1.1.cmml">[</mo><mi id="S3.SS1.SSS0.Px1.p1.11.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.2.2" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.11.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2"><eq id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.1"></eq><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.2.3">ğ‘—</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3"><times id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.1"></times><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2"><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1">subscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2"><times id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.1"></times><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.2">arg</ci><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.2.3">max</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.1.3">ğ‘¦</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.2">ğ’©</ci><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.2.2.3">ğ‘—</ci></apply></apply><apply id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.2"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.SSS0.Px1.p1.11.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.11.m4.1.1">ğ‘¦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.11.m4.1c">y_{j}=\operatorname*{arg\,max}_{y}\mathcal{N}_{j}[y]</annotation></semantics></math>â€).</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.4" class="ltx_p">Intuitively, a token <math id="S3.SS1.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="y_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.1.m1.1c">y_{j}</annotation></semantics></math> with mask <math id="S3.SS1.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="m_{j}=0" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.2.m2.1a"><mrow id="S3.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml"><msub id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.2.cmml">m</mi><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1"><eq id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1"></eq><apply id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.2">ğ‘š</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.3">ğ‘—</ci></apply><cn type="integer" id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.2.m2.1c">m_{j}=0</annotation></semantics></math> denotes that this token is not correctly predicted by the model. Accordingly, any phrase pairs that contain the token <math id="S3.SS1.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="y_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.3.m3.1a"><msub id="S3.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2">ğ‘¦</ci><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.3.m3.1c">y_{j}</annotation></semantics></math> should not be extracted from the training example <math id="S3.SS1.SSS0.Px1.p2.4.m4.2" class="ltx_Math" alttext="(\textbf{x},\textbf{y})" display="inline"><semantics id="S3.SS1.SSS0.Px1.p2.4.m4.2a"><mrow id="S3.SS1.SSS0.Px1.p2.4.m4.2.3.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.4.m4.2.3.2.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.3.1.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1a.cmml">x</mtext><mo id="S3.SS1.SSS0.Px1.p2.4.m4.2.3.2.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.3.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p2.4.m4.2.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2a.cmml">y</mtext><mo stretchy="false" id="S3.SS1.SSS0.Px1.p2.4.m4.2.3.2.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.4.m4.2b"><interval closure="open" id="S3.SS1.SSS0.Px1.p2.4.m4.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.3.2"><ci id="S3.SS1.SSS0.Px1.p2.4.m4.1.1a.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1">x</mtext></ci><ci id="S3.SS1.SSS0.Px1.p2.4.m4.2.2a.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS1.SSS0.Px1.p2.4.m4.2.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.2.2">y</mtext></ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.4.m4.2c">(\textbf{x},\textbf{y})</annotation></semantics></math>, since these phrase pairs are not fully learned by the NMT model. A lightweight implementation is to replace these tokens with a special symbol â€œ$MASK$â€, and run the standard phrase extraction phase as in the SMT pipeline. Then we remove all the phrase pairs that contain the symbol â€œ$MASK$â€ (lines 6-8 in AlgorithmÂ <a href="#alg1" title="Algorithm 1 â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and feed the pruned phrase pairs to the second phase of parameter estimation.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x3.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="145" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>En<math id="S3.F2.sf1.2.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.F2.sf1.2.m1.1b"><mo stretchy="false" id="S3.F2.sf1.2.m1.1.1" xref="S3.F2.sf1.2.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.F2.sf1.2.m1.1c"><ci id="S3.F2.sf1.2.m1.1.1.cmml" xref="S3.F2.sf1.2.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.sf1.2.m1.1d">\Rightarrow</annotation></semantics></math>De</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x4.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="145" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>En<math id="S3.F2.sf2.2.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.F2.sf2.2.m1.1b"><mo stretchy="false" id="S3.F2.sf2.2.m1.1.1" xref="S3.F2.sf2.2.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.F2.sf2.2.m1.1c"><ci id="S3.F2.sf2.2.m1.1.1.cmml" xref="S3.F2.sf2.2.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.sf2.2.m1.1d">\Rightarrow</annotation></semantics></math>Ja</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x5.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="135" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>En<math id="S3.F2.sf3.2.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.F2.sf3.2.m1.1b"><mo stretchy="false" id="S3.F2.sf3.2.m1.1.1" xref="S3.F2.sf3.2.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.F2.sf3.2.m1.1c"><ci id="S3.F2.sf3.2.m1.1.1.cmml" xref="S3.F2.sf3.2.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.sf3.2.m1.1d">\Rightarrow</annotation></semantics></math>De with Different Seeds</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Evaluating the correlation between quality metrics of phrase table and NMT performance (â€œNMT BLEUâ€) on (a) En<math id="S3.F2.3.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.F2.3.m1.1b"><mo stretchy="false" id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><ci id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">\Rightarrow</annotation></semantics></math>De and (b) En<math id="S3.F2.4.m2.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.F2.4.m2.1b"><mo stretchy="false" id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><ci id="S3.F2.4.m2.1.1.cmml" xref="S3.F2.4.m2.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">\Rightarrow</annotation></semantics></math>Ja datasets. All metrics are scaled by the corresponding best score to fit in the figure. We also report results on phrase table size for NMT models with different random seeds (c). </figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Setup</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data and Models</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">We conduct experiments on both the widely-used WMT2014 English<math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">\Rightarrow</annotation></semantics></math>German (En<math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">\Rightarrow</annotation></semantics></math>De) and the syntactically-distant WAT2017 English<math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">\Rightarrow</annotation></semantics></math>Japanese (En<math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\Rightarrow</annotation></semantics></math>Ja)Â <cite class="ltx_cite ltx_citemacro_cite">Neubig etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2015</a>)</cite> datasets. We use 4-gram NIST BLEU scoreÂ <cite class="ltx_cite ltx_citemacro_cite">Papineni etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">2002</a>)</cite> as the evaluation metric.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">For SMT experiments, we follow the standard SMT pipeline and the setting of Edinburghâ€™s phrase-based system in WMT-2014Â <cite class="ltx_cite ltx_citemacro_cite">Durrani etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2014</a>)</cite> with as few human heuristics as possible.
We use MosesÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2007</a>)</cite> with default system setting and the toolkit Fast_AlignÂ <cite class="ltx_cite ltx_citemacro_cite">Dyer etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2013</a>)</cite> for building word-aligned corpus, which is fast and automatic.
FollowingÂ <cite class="ltx_cite ltx_citemacro_citet">Johnson etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2007</a>)</cite> to reduce redundancy, we further remove phrase pairs that occur only once in the training data.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p3.1" class="ltx_p">For NMT experiments, We use the toolkit FairseqÂ <cite class="ltx_cite ltx_citemacro_cite">Ott etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> to implement NMT modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>. We train the NMT models for 100,000 steps and save the checkpoint models at each epoch. In the first epoch, we save the model per 200 steps and extract phrase tables from training examples that have seen on so far only.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Metrics</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">To verify our claim in this paper, we propose several metrics to quantitatively evaluate quality of the phrase table. If the metrics correlate well with NMT performance, then the phrase table is a reasonable assessment to represent the bilingual knowledge learned by NMT models.
The metrics are as follows:</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p"><span id="S3.SS2.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">Phrase Table Size:</span> As a straightforward metric, the size measures the number of distinct phrase pairs in a phrase table. A larger phrase table size indicates more abundant bilingual knowledge.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p"><span id="S3.SS2.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_italic">Recovery Percent:</span> The phrase table size might be less accurate due to duplicate counting of compositions of existing phrase pairs. Accordingly, we propose another metric, recovery percent, to measure the distinct knowledge on data reconstruction. In detail, we use the phrase table to force decode the target sentence to recover as many target tokens as possible, and the ratio is denoted as the recovery percentage. A higher recovery percent indicates more distinct knowledge since more data can be reconstructed based on the phrase table.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p4.1" class="ltx_p"><span id="S3.SS2.SSS0.Px2.p4.1.1" class="ltx_text ltx_font_italic">Translation Quality:</span> Finally, we directly evaluate the essential knowledge in the phrase table for the ultimate translation. Specifically, we train a SMT model with the extracted phrase table by the off-the-shelf Moses toolkit, and evaluate its BLEU score on the test set. For fair comparison, we keep other SMT components unchanged and only alter the phrase table, therefore the relative SMT BLEU values is our focus of interest.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluating the Phrase Table</h3>

<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The extracted phrase table correlates well with the NMT performance.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">FigureÂ <a href="#S3.F2.sf1" title="In Figure 2 â€£ Building Masked Word-Aligned Parallel Data. â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a> illustrates the results of the above metrics on the English<math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\Rightarrow</annotation></semantics></math>German dataset.
As seen, all three metrics are highly in line with the NMT performance (â€œNMT BLEUâ€) during the entire learning process.
The Pearson correlations between NMT BLEU scores and phrase table size, recovery percent, and the translation quality are 0.975, 0.987, and 0.956, respectively, demonstrating very high correlations between the phrase table and NMT performance. This confirms our claim that phrase table is a reasonable assessment to represent the bilingual knowledge learned by NMT models.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The conclusion is robust across language pairs and random seeds.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">We also validate our approach on the English<math id="S3.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\Rightarrow</annotation></semantics></math>Japanese dataset, as shown in FigureÂ <a href="#S3.F2.sf2" title="In Figure 2 â€£ Building Masked Word-Aligned Parallel Data. â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a>.
The Pearson correlations are respectively 0.988, 0.990, and 0.908, demonstrating the universality of our conclusions .
To avoid the potential bias, we vary the initialization seed and analyze whether the extracted phrase table is robust. FigureÂ <a href="#S3.F2.sf3" title="In Figure 2 â€£ Building Masked Word-Aligned Parallel Data. â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2c</span></a> depicts the results. The phrase table size increases similarly in different seeds. Additionally, at each epoch, more than 85% phrase pairs are same among three seeds (â€œOverlapâ€), which shows that its robustness against random seeds.</p>
</div>
<div id="S3.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p2.1" class="ltx_p">Given the general applicability of the phrase table, we use the English<math id="S3.SS3.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S3.SS3.SSS0.Px2.p2.1.m1.1a"><mo stretchy="false" id="S3.SS3.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p2.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p2.1.m1.1b"><ci id="S3.SS3.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p2.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p2.1.m1.1c">\Rightarrow</annotation></semantics></math>German translation as our test bed for further analyses. We will interchangeably use the terms â€œphrase tableâ€ and â€œbilingual knowledgeâ€ in the following sections.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Learning of Bilingual Knowledge</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">With the interpretable phrase table in hand, we attempt to understand how NMT models learn the bilingual knowledge from two perspectives:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">How do NMT models learn the bilingual knowledge during training? (SectionÂ <a href="#S4.SS1" title="4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>)</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Does the trained NMT model sufficiently explore the bilingual knowledge embedded in the training examples? (SectionÂ <a href="#S4.SS2" title="4.2 Learned Bilingual Knowledge â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>)</p>
</div>
</li>
</ul>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Learning Dynamics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this section, we investigate the evolvement of bilingual knowledge during the training. To this end, we first categorize the phrase pair into different complexity levels using several metrics that are widely used in the SMT research:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.2" class="ltx_p"><span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_italic">Phrase Length:</span> A longer phrase is usually of more complexityÂ <cite class="ltx_cite ltx_citemacro_cite">Lu (<a href="#bib.bib29" title="" class="ltx_ref">2010</a>)</cite>. We categorize the phrase length into three types with increasing complexity: <span id="S4.SS1.p2.2.2" class="ltx_text ltx_font_italic">short</span> (1-3) <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><lt id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p2.2.3" class="ltx_text ltx_font_italic">middle</span> (3-5) <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><lt id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p2.2.4" class="ltx_text ltx_font_italic">long</span> (5-7).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.2" class="ltx_p"><span id="S4.SS1.p3.2.1" class="ltx_text ltx_font_italic">Reordering Type:</span>
This metric measures the order of two phrases with lexicalized reorderingÂ <cite class="ltx_cite ltx_citemacro_cite">Tillmann (<a href="#bib.bib41" title="" class="ltx_ref">2004</a>)</cite>, and disordered phrases are often hard to translateÂ <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib23" title="" class="ltx_ref">2009</a>)</cite>. We have three types with increasing complexity: <span id="S4.SS1.p3.2.2" class="ltx_text ltx_font_italic">monotone</span> <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><lt id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p3.2.3" class="ltx_text ltx_font_italic">swap</span> <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><lt id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p3.2.4" class="ltx_text ltx_font_italic">discontinuous</span>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.2" class="ltx_p"><span id="S4.SS1.p4.2.1" class="ltx_text ltx_font_italic">Word Fertility:</span> Word fertility measures the alignment relations between the words inside the phrase pair. Words with a complex fertility might indicate inherent translation difficultyÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">1990</a>)</cite>. We have three fertility types with increasing difficulty: <span id="S4.SS1.p4.2.2" class="ltx_text ltx_font_italic">1-1 align</span> <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mo id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><lt id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p4.2.3" class="ltx_text ltx_font_italic">M-1 align</span> <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mo id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><lt id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">&lt;</annotation></semantics></math> <span id="S4.SS1.p4.2.4" class="ltx_text ltx_font_italic">1-M align</span>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x6.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="145" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Phrase Length</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x7.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="145" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Reordering Type</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x8.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_img_square" width="145" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Word Fertility</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Learning dynamics of bilingual knowledge according to different complexity metrics.</figcaption>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">For each metric, we normalize the value by the maximum phrase pair size in each category.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NMT models tend to learn simple patterns first and complex patterns later.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">As shown in FigureÂ <a href="#S4.F3.sf1" title="In Figure 3 â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>, NMT models learn short phrases faster than medium phrases and long phrases, embodied by a fastest convergence and a highest slope among three categories in the first epoch. As the learning continues, medium and long phrases start to converge to a relative stable state slowly. Besides, NMT BLEU scores show a very similar increasing trend as the short phrase, demonstrating a high correlation (Pearson correlation: 0.992) between the NMT performance and short phrases.</p>
</div>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p2.1" class="ltx_p">We can observe similar findings on the phrase reordering type (FigureÂ <a href="#S4.F3.sf2" title="In Figure 3 â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3b</span></a>) and word fertility (FigureÂ <a href="#S4.F3.sf3" title="In Figure 3 â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3c</span></a>). Simple patterns like monotone and 1-1 aligned phrase can be quickly learned by NMT models, while complex patterns are learned in a slower manner.
This is in line with the findings ofÂ <cite class="ltx_cite ltx_citemacro_citet">Rahaman etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>: deep networks will first learn low-complexity functional components, before absorbing high-complexity features.
These results also indicate that NMT models might by nature has the learning ability similar to the <span id="S4.SS1.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">curriculum learning</span>Â <cite class="ltx_cite ltx_citemacro_cite">Bengio etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2009</a>); Kocmi and Bojar (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> without any explicit curriculum.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Forgetting dynamics occur in the learning of bilingual knowledge.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">As shown in FiguresÂ <a href="#S3.F2" title="Figure 2 â€£ Building Masked Word-Aligned Parallel Data. â€£ 3.1 Methodology â€£ 3 Assessing Bilingual Knowledge with Phrase Table â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> andÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the size of learned phrase table is monotonically increasing as the learning processes. One question naturally arises: <span id="S4.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">are the phrase pairs never forgotten once learnt?</span></p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">FigureÂ <a href="#S4.F4" title="Figure 4 â€£ Forgetting dynamics occur in the learning of bilingual knowledge. â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the result. Note that we only plot the first 15 epochs to ensure that the phrases are never forgotten for at least 6 epochs.
Around 80% of learned phrase table is unforgettable phrases (always learned phrase pairs), while the rest phrase pairs are forgotten. This is consistent with the findings ofÂ <cite class="ltx_cite ltx_citemacro_citet">Toneva etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> on the image classification tasks.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2004.13270/assets/x9.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="145" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of unforgettable phrases.</figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r" rowspan="2"><span id="S4.T1.1.1.1.1" class="ltx_text ltx_font_bold">Phrase Table</span></td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S4.T1.1.1.2.1" class="ltx_text ltx_font_bold">Shared</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S4.T1.1.1.3.1" class="ltx_text ltx_font_bold">Non-Shared</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center" colspan="2"><span id="S4.T1.1.1.4.1" class="ltx_text ltx_font_bold">All</span></td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.2.1.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.2.3.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.2.4.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.2.5.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S4.T1.1.2.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T1.1.2.6.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Full</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.0M</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">17.32</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">8.5M</td>
<td id="S4.T1.1.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">4.50</td>
<td id="S4.T1.1.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">17.5M</td>
<td id="S4.T1.1.3.7" class="ltx_td ltx_align_right ltx_border_tt">17.91</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NMT</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.0M</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">17.90</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0M</td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0</td>
<td id="S4.T1.1.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.0M</td>
<td id="S4.T1.1.4.7" class="ltx_td ltx_align_right ltx_border_t">17.90</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the phrase table extracted from the full training data (â€œFullâ€) and NMT models (â€œNMTâ€).
â€œAllâ€ denotes the whole phrase table, â€œSharedâ€ denotes the intersection of two tables, and â€œNon-sharedâ€ denotes the complement. Note that the probabilities of â€œSharedâ€ phrases are different for the two tables.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Learned Bilingual Knowledge</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this experiment, we evaluate whether NMT models have sufficiently explore the bilingual knowledge in the training examples, by comparing the phrase tables extracted from NMT predictions and from the raw training data. We use the latter to represent the full bilingual knowledge embedded in the training examples.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ Forgetting dynamics occur in the learning of bilingual knowledge. â€£ 4.1 Learning Dynamics â€£ 4 Learning of Bilingual Knowledge â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the bilingual knowledge learned by NMT model (â€œNMTâ€) shows comparable translation quality with the full-data knowledge (â€œFullâ€) (17.90 vs. 17.91), but with only <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">a half of </span> phrases (9.0M vs. 17.5M).<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>When considering the filtered one-shot phrases, NMT phrase only takes 22.8% of the full table (76M vs. 335M).</span></span></span>
In addition, NMT provides a better probability estimation for the distilled phrases (â€œSharedâ€, 17.90 vs. 17.32). In the â€œNon-Sharedâ€ table, 78.2% of the phrase pairs share the same source phrase with the â€œSharedâ€ table, of which 83.2% have a lower translation probability. The results empirically confirm our hypothesis that <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">NMT models distill the bilingual knowledge by discarding those low-quality phrase pairs</span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Revisiting Recent Advances</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we revisit recent advances that potentially affect the learning of bilingual knowledge. Specifically, we investigate three types of techniques: (1) <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">model capacity</span> that indicates how complicated patterns a model can express (SectionÂ <a href="#S5.SS1" title="5.1 Model Capacity â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>); (2) <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">data augmentation</span> that introduces additional knowledge with external data (SectionÂ <a href="#S5.SS2" title="5.2 Data Augmentation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>); and (3) <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">domain adaptation</span> that transfers knowledge across different domains (SectionÂ <a href="#S5.SS3" title="5.3 Domain Adaptation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model Capacity </h3>

<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_rr" rowspan="2"><span id="S5.T2.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_rr" colspan="2"><span id="S5.T2.1.1.2.1" class="ltx_text ltx_font_bold">NMT</span></td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T2.1.1.3.1" class="ltx_text ltx_font_bold">Phrase Table</span></td>
</tr>
<tr id="S5.T2.1.2" class="ltx_tr">
<td id="S5.T2.1.2.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T2.1.2.1.1" class="ltx_text ltx_font_italic">#Para</span></td>
<td id="S5.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S5.T2.1.2.2.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T2.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T2.1.2.3.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.2.4.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T2.1.3" class="ltx_tr">
<td id="S5.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S5.T2.1.3.1.1" class="ltx_text ltx_font_smallcaps">Small</span></td>
<td id="S5.T2.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">38M</td>
<td id="S5.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">25.45</td>
<td id="S5.T2.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.7M</td>
<td id="S5.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">17.35</td>
</tr>
<tr id="S5.T2.1.4" class="ltx_tr">
<td id="S5.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S5.T2.1.4.1.1" class="ltx_text ltx_font_smallcaps">Base</span></td>
<td id="S5.T2.1.4.2" class="ltx_td ltx_align_right ltx_border_r">98M</td>
<td id="S5.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_rr">27.11</td>
<td id="S5.T2.1.4.4" class="ltx_td ltx_align_right ltx_border_r">9.0M</td>
<td id="S5.T2.1.4.5" class="ltx_td ltx_align_center">17.90</td>
</tr>
<tr id="S5.T2.1.5" class="ltx_tr">
<td id="S5.T2.1.5.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="S5.T2.1.5.1.1" class="ltx_text ltx_font_smallcaps">Big</span></td>
<td id="S5.T2.1.5.2" class="ltx_td ltx_align_right ltx_border_r">284M</td>
<td id="S5.T2.1.5.3" class="ltx_td ltx_align_center ltx_border_rr">28.40</td>
<td id="S5.T2.1.5.4" class="ltx_td ltx_align_right ltx_border_r">9.2M</td>
<td id="S5.T2.1.5.5" class="ltx_td ltx_align_center">17.89</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of NMT models of different capacities.</figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r" rowspan="2"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_rr" colspan="2"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Shared</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T3.1.1.3.1" class="ltx_text ltx_font_bold">Non-Shared</span></td>
</tr>
<tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span id="S5.T3.1.2.2.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T3.1.2.3.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T3.1.2.4.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T3.1.3" class="ltx_tr">
<td id="S5.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T3.1.3.1.1" class="ltx_text ltx_font_smallcaps">Small</span></td>
<td id="S5.T3.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">7.0M</td>
<td id="S5.T3.1.3.3" class="ltx_td ltx_align_right ltx_border_rr ltx_border_tt">17.53</td>
<td id="S5.T3.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.7M</td>
<td id="S5.T3.1.3.5" class="ltx_td ltx_align_right ltx_border_tt">2.37</td>
</tr>
<tr id="S5.T3.1.4" class="ltx_tr">
<td id="S5.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.4.1.1" class="ltx_text ltx_font_smallcaps">Base</span></td>
<td id="S5.T3.1.4.2" class="ltx_td ltx_align_right ltx_border_r">7.0M</td>
<td id="S5.T3.1.4.3" class="ltx_td ltx_align_right ltx_border_rr">17.49</td>
<td id="S5.T3.1.4.4" class="ltx_td ltx_align_right ltx_border_r">2.0M</td>
<td id="S5.T3.1.4.5" class="ltx_td ltx_align_right">3.57</td>
</tr>
<tr id="S5.T3.1.5" class="ltx_tr">
<td id="S5.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.5.1.1" class="ltx_text ltx_font_smallcaps">Big</span></td>
<td id="S5.T3.1.5.2" class="ltx_td ltx_align_right ltx_border_r">7.0M</td>
<td id="S5.T3.1.5.3" class="ltx_td ltx_align_right ltx_border_rr">17.29</td>
<td id="S5.T3.1.5.4" class="ltx_td ltx_align_right ltx_border_r">2.2M</td>
<td id="S5.T3.1.5.5" class="ltx_td ltx_align_right">3.47</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of phrases from three capacities.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We vary the layer dimensionality of Transformer, and obtain three model variants: <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Small</span> (256), <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">Base</span> (512), and <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">Big</span> (1024). As listed in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.1 Model Capacity â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, increasing model capacity consistently improves translation performance. However, the extracted phrase table is only marginally increased.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We compare the phrase tables learned by different models, as shown in TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.1 Model Capacity â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The phrases shared by all models take the overwhelming majority, which add most value to the translation performance. We conjecture that enlarging capacity improves NMT performance by better exploiting complex patterns beyond bilingual lexicons. This also confirms our intuition that bilingual lexicons can be a crucial early step in assessing the knowledge in NMT models.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Data Augmentation</h3>

<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_rr" rowspan="2"><span id="S5.T4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_rr" colspan="2"><span id="S5.T4.1.1.2.1" class="ltx_text ltx_font_bold">NMT</span></td>
<td id="S5.T4.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T4.1.1.3.1" class="ltx_text ltx_font_bold">Phrase Table</span></td>
</tr>
<tr id="S5.T4.1.2" class="ltx_tr">
<td id="S5.T4.1.2.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T4.1.2.1.1" class="ltx_text ltx_font_italic">#Para</span></td>
<td id="S5.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S5.T4.1.2.2.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T4.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T4.1.2.3.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.2.4.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T4.1.3" class="ltx_tr">
<td id="S5.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt"><span id="S5.T4.1.3.1.1" class="ltx_text ltx_font_smallcaps">Base</span></td>
<td id="S5.T4.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">98M</td>
<td id="S5.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">27.11</td>
<td id="S5.T4.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.0M</td>
<td id="S5.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_tt">17.90</td>
</tr>
<tr id="S5.T4.1.4" class="ltx_tr">
<td id="S5.T4.1.4.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Â Â Â Â Â  + BT</td>
<td id="S5.T4.1.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">98M</td>
<td id="S5.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">29.75</td>
<td id="S5.T4.1.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">20.9M</td>
<td id="S5.T4.1.4.5" class="ltx_td ltx_align_center ltx_border_t">19.26</td>
</tr>
<tr id="S5.T4.1.5" class="ltx_tr">
<td id="S5.T4.1.5.1" class="ltx_td ltx_align_left ltx_border_rr">Â Â Â Â Â  + FT</td>
<td id="S5.T4.1.5.2" class="ltx_td ltx_align_right ltx_border_r">98M</td>
<td id="S5.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_rr">28.43</td>
<td id="S5.T4.1.5.4" class="ltx_td ltx_align_right ltx_border_r">28.0M</td>
<td id="S5.T4.1.5.5" class="ltx_td ltx_align_center">19.33</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of back-translation (â€œBTâ€) and forward-translation (â€œFTâ€).</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T5.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r" rowspan="2"><span id="S5.T5.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S5.T5.1.1.2.1" class="ltx_text ltx_font_bold">Shared</span></td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T5.1.1.3.1" class="ltx_text ltx_font_bold">Non-Shared</span></td>
</tr>
<tr id="S5.T5.1.2" class="ltx_tr">
<td id="S5.T5.1.2.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T5.1.2.1.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T5.1.2.2" class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span id="S5.T5.1.2.2.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T5.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T5.1.2.3.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T5.1.2.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T5.1.2.4.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T5.1.3" class="ltx_tr">
<td id="S5.T5.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T5.1.3.1.1" class="ltx_text ltx_font_smallcaps">Base</span></td>
<td id="S5.T5.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">8.3M</td>
<td id="S5.T5.1.3.3" class="ltx_td ltx_align_right ltx_border_rr ltx_border_tt">17.67</td>
<td id="S5.T5.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.7M</td>
<td id="S5.T5.1.3.5" class="ltx_td ltx_align_right ltx_border_tt">1.78</td>
</tr>
<tr id="S5.T5.1.4" class="ltx_tr">
<td id="S5.T5.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Â Â Â  + BT</td>
<td id="S5.T5.1.4.2" class="ltx_td ltx_align_right ltx_border_r">8.3M</td>
<td id="S5.T5.1.4.3" class="ltx_td ltx_align_right ltx_border_rr">18.61</td>
<td id="S5.T5.1.4.4" class="ltx_td ltx_align_right ltx_border_r">12.6M</td>
<td id="S5.T5.1.4.5" class="ltx_td ltx_align_right">10.45</td>
</tr>
<tr id="S5.T5.1.5" class="ltx_tr">
<td id="S5.T5.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T5.1.5.1.1" class="ltx_text ltx_font_smallcaps">Base</span></td>
<td id="S5.T5.1.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">8.4M</td>
<td id="S5.T5.1.5.3" class="ltx_td ltx_align_right ltx_border_rr ltx_border_tt">17.83</td>
<td id="S5.T5.1.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.5M</td>
<td id="S5.T5.1.5.5" class="ltx_td ltx_align_right ltx_border_tt">1.21</td>
</tr>
<tr id="S5.T5.1.6" class="ltx_tr">
<td id="S5.T5.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Â Â Â  + FT</td>
<td id="S5.T5.1.6.2" class="ltx_td ltx_align_right ltx_border_r">8.4M</td>
<td id="S5.T5.1.6.3" class="ltx_td ltx_align_right ltx_border_rr">18.30</td>
<td id="S5.T5.1.6.4" class="ltx_td ltx_align_right ltx_border_r">19.6M</td>
<td id="S5.T5.1.6.5" class="ltx_td ltx_align_right">11.25</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of phrases learned by BT and FT.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x10.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_img_portrait" width="75" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Length</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x11.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_img_portrait" width="54" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Reordering</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.13270/assets/x12.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_img_portrait" width="54" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Fertility</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Characteristic distributions of phrases newly introduced by BT and FT. For a better illustration, we also give the distribution of the baseline (â€œBaseâ€).</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this experiment, we investigate two representative data augmentation approaches, back-translationÂ <cite class="ltx_cite ltx_citemacro_cite">Sennrich etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2016</a>)</cite> and forward-translationÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang and Zong (<a href="#bib.bib52" title="" class="ltx_ref">2016</a>)</cite>, which differ at exploiting target or source-side monolingual data. We select a same-size (around 4.5M) English and German monolingual dataset from the WMT website, and construct the synthetic corpus with <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">Base</span> models that are trained on the parallel data.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Data Augmentation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> lists the results. Both techniques significantly improves the performance of NMT models by exploiting a larger and better phrase table.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The different sizes of BT and FT phrase tables are due to the different monolingual datasets used for them, the averaged length of which are 24.8 and 28.4, respectively.</span></span></span>
TableÂ <a href="#S5.T5" title="Table 5 â€£ 5.2 Data Augmentation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the detailed comparison of the phrase tables. Both augmentation methods <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">induce new knowledge and enhance existing knowledge over the baseline</span>, and the newly introduced knowledge contribute a lot to the performance improvement.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We further analyze the characteristics of the newly introduced phrase pairs, as illustrated in FigureÂ <a href="#S5.F5" title="Figure 5 â€£ 5.2 Data Augmentation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. One interesting finding is that the newly introduced phrase pairs are notably longer than the original ones. Besides, the new phrase pairs show less reordered patterns and more monotone patterns, which may explain the producing of longer phrases. The finding is consistent with previous studies, which show that the BT text is simpler than naturally occurring text <cite class="ltx_cite ltx_citemacro_cite">Edunov etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Domain Adaptation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In the last experiment, we analyze the transferability of the bilingual knowledge by directly applying it to another domain. To this end, we use the IWSLT14 English<math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\Rightarrow" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mo stretchy="false" id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">â‡’</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">â‡’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\Rightarrow</annotation></semantics></math>German data (160,234 sentence pairs) as the target domain (Speech domain), and fine-tune the NMT model trained on the WMT14 dataset (News domain) for several epochs.
We extract the phrase table using the training data of target domain, and the results are shown in TableÂ <a href="#S5.T6" title="Table 6 â€£ 5.3 Domain Adaptation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Clearly, the fine-tuned NMT model benefits from a larger and better phrase table, by adapting the model to the target domain. The analysis results in TableÂ <a href="#S5.T7" title="Table 7 â€£ 5.3 Domain Adaptation â€£ 5 Revisiting Recent Advances â€£ Assessing the Bilingual Knowledge Learned by Neural Machine Translation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> further show that the fine-tuned phrase table improves performance with both more phrases (â€œNon-Sharedâ€) and better estimation of original phrases (â€œSharedâ€).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In addition, we re-extract the phrase from the source domain (WMT data) with the fine-tuned model.
The phrase table achieves only a BLEU score of 4.77 with 2.6M phrase pairs, while the original model without fine-tune shows a BLEU score of 17.90 with 9.0M phrase pairs. The fine-tune approach increases new knowledge of the target domain while forgets previous-learned knowledge of the source domain.
The results provide an empirical validation of the phenomenon of catastrophic forgetting in domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">Kirkpatrick etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, which inversely demonstrate the reasonableness of our approach.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T6.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.1.1.1" class="ltx_text ltx_font_bold">Fine</span></td>
<td id="S5.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S5.T6.1.1.2.1" class="ltx_text ltx_font_bold">NMT</span></td>
<td id="S5.T6.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T6.1.1.3.1" class="ltx_text ltx_font_bold">Phrase Table</span></td>
</tr>
<tr id="S5.T6.1.2" class="ltx_tr">
<td id="S5.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T6.1.2.1.1" class="ltx_text ltx_font_bold">Tune</span></td>
<td id="S5.T6.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T6.1.2.2.1" class="ltx_text ltx_font_italic"># Para.</span></td>
<td id="S5.T6.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T6.1.2.3.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T6.1.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T6.1.2.4.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T6.1.2.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T6.1.2.5.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T6.1.3" class="ltx_tr">
<td id="S5.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Ã—</td>
<td id="S5.T6.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">98M</td>
<td id="S5.T6.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">15.78</td>
<td id="S5.T6.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">168K</td>
<td id="S5.T6.1.3.5" class="ltx_td ltx_align_right ltx_border_tt">16.08</td>
</tr>
<tr id="S5.T6.1.4" class="ltx_tr">
<td id="S5.T6.1.4.1" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S5.T6.1.4.2" class="ltx_td ltx_align_right ltx_border_r">98M</td>
<td id="S5.T6.1.4.3" class="ltx_td ltx_align_right ltx_border_r">31.26</td>
<td id="S5.T6.1.4.4" class="ltx_td ltx_align_right ltx_border_r">316K</td>
<td id="S5.T6.1.4.5" class="ltx_td ltx_align_right">18.50</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of domain adaptation.</figcaption>
</figure>
<figure id="S5.T7" class="ltx_table">
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T7.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T7.1.1.1.1" class="ltx_text ltx_font_bold">Fine</span></td>
<td id="S5.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2"><span id="S5.T7.1.1.2.1" class="ltx_text ltx_font_bold">Shared</span></td>
<td id="S5.T7.1.1.3" class="ltx_td ltx_align_center" colspan="2"><span id="S5.T7.1.1.3.1" class="ltx_text ltx_font_bold">Non-Shared</span></td>
</tr>
<tr id="S5.T7.1.2" class="ltx_tr">
<td id="S5.T7.1.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T7.1.2.1.1" class="ltx_text ltx_font_bold">Tune</span></td>
<td id="S5.T7.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T7.1.2.2.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T7.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T7.1.2.3.1" class="ltx_text ltx_font_italic">BLEU</span></td>
<td id="S5.T7.1.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T7.1.2.4.1" class="ltx_text ltx_font_italic">Size</span></td>
<td id="S5.T7.1.2.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T7.1.2.5.1" class="ltx_text ltx_font_italic">BLEU</span></td>
</tr>
<tr id="S5.T7.1.3" class="ltx_tr">
<td id="S5.T7.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Ã—</td>
<td id="S5.T7.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.16M</td>
<td id="S5.T7.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">15.95</td>
<td id="S5.T7.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">0.01M</td>
<td id="S5.T7.1.3.5" class="ltx_td ltx_align_right ltx_border_tt">1.65</td>
</tr>
<tr id="S5.T7.1.4" class="ltx_tr">
<td id="S5.T7.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ“</td>
<td id="S5.T7.1.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.16M</td>
<td id="S5.T7.1.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16.92</td>
<td id="S5.T7.1.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.16M</td>
<td id="S5.T7.1.4.5" class="ltx_td ltx_align_right ltx_border_t">6.95</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison of phrase tables for domain adaptation with or without fine tune.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we propose to assess the bilingual knowledge learned by NMT models with statistic models â€“ phrase table. The reported results provide a better understanding of NMT models and recent technological advances in learning the essential bilingual lexicons, which also indicate several potential applications:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Error diagnosis</span> that debugs mistaken predictions by tracing associated phrase pairsÂ <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>;</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Curriculum learning</span> that dynamically assigns more weights to instances associated with the unlearned knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">Platanios etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>;</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Phrase memory</span> that stores unlearned phrases in NMT to query when generating translationsÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>); Zhang etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Although the phrase table successfully explains many model behaviors, it cannot explain certain techniques such as enlarging model capacity. The explored bilingual lexicon is only one of the critical knowledge bases in the translation process. In the future, we will investigate more advanced forms of bilingual knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2006</a>); Galley and Manning (<a href="#bib.bib12" title="" class="ltx_ref">2010</a>)</cite>, as well as explore other types of knowledge bases such as grammar and semantics with statistic models (e.g., reordering and language models). This paper is the first step in what we hope will be a long and fruitful journey.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alvarez-Melis and Jaakkola (2017)</span>
<span class="ltx_bibblock">
David Alvarez-Melis and Tommi Jaakkola. 2017.

</span>
<span class="ltx_bibblock">A causal framework for explaining the predictions of black-box
sequence-to-sequence models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau etÂ al. (2015)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and
translate.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass.
2017.

</span>
<span class="ltx_bibblock">What do neural machine translation models learn about morphology?

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio etÂ al. (2009)</span>
<span class="ltx_bibblock">
Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston.
2009.

</span>
<span class="ltx_bibblock">Curriculum learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (1990)</span>
<span class="ltx_bibblock">
PeterÂ F Brown, John Cocke, StephenÂ A DellaÂ Pietra, VincentÂ J DellaÂ Pietra,
Frederik Jelinek, JohnÂ D Lafferty, RobertÂ L Mercer, and PaulÂ S Roossin. 1990.

</span>
<span class="ltx_bibblock">A statistical approach to machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computational linguistics</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (1993)</span>
<span class="ltx_bibblock">
PeterÂ F Brown, Vincent JÂ Della Pietra, Stephen AÂ Della Pietra, and RobertÂ L
Mercer. 1993.

</span>
<span class="ltx_bibblock">The mathematics of statistical machine translation: Parameter
estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Computational linguistics</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang (2005)</span>
<span class="ltx_bibblock">
David Chiang. 2005.

</span>
<span class="ltx_bibblock">A hierarchical phrase-based model for statistical machine
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017.

</span>
<span class="ltx_bibblock">Visualizing and understanding neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Durrani etÂ al. (2014)</span>
<span class="ltx_bibblock">
Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. 2014.

</span>
<span class="ltx_bibblock">Edinburghâ€™s phrase-based machine translation systems for wmt-14.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">WMT</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dyer etÂ al. (2013)</span>
<span class="ltx_bibblock">
Chris Dyer, Victor Chahuneau, and NoahÂ A Smith. 2013.

</span>
<span class="ltx_bibblock">A simple, fast, and effective reparameterization of ibm model 2.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Marcâ€™Aurelio Ranzato, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">On the evaluation of machine translation systems trained with
back-translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.05204</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galley and Manning (2010)</span>
<span class="ltx_bibblock">
Michel Galley and ChristopherÂ D. Manning. 2010.

</span>
<span class="ltx_bibblock">Accurate non-hierarchical phrase-based translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehring etÂ al. (2017)</span>
<span class="ltx_bibblock">
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and YannÂ N. Dauphin.
2017.

</span>
<span class="ltx_bibblock">Convolutional sequence to sequence learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2019)</span>
<span class="ltx_bibblock">
Junliang Guo, XuÂ Tan, DiÂ He, Tao Qin, Linli Xu, and Tie-Yan Liu. 2019.

</span>
<span class="ltx_bibblock">Non-autoregressive neural machine translation with enhanced decoder
input.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayes-Roth (1985)</span>
<span class="ltx_bibblock">
Frederick Hayes-Roth. 1985.

</span>
<span class="ltx_bibblock">Rule-based systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 28(9):921â€“932.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2020)</span>
<span class="ltx_bibblock">
Junxian He, Jiatao Gu, Jiajun Shen, and Marcâ€™Aurelio Ranzato. 2020.

</span>
<span class="ltx_bibblock">Revisiting self-training for neural sequence generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2019)</span>
<span class="ltx_bibblock">
Shilin He, Zhaopeng Tu, Xing Wang, Longyue Wang, Michael Lyu, and Shuming Shi.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/D19-1088" title="" class="ltx_ref ltx_href">Towards
understanding neural machine translation with word importance</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hill etÂ al. (2017)</span>
<span class="ltx_bibblock">
Felix Hill, Kyunghyun Cho, SÃ©bastien Jean, and Yoshua Bengio. 2017.

</span>
<span class="ltx_bibblock">The representational geometry of word meanings acquired by neural
machine translation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Machine Translation</em>, 31(1-2):3â€“18.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain and Wallace (2019)</span>
<span class="ltx_bibblock">
Sarthak Jain and ByronÂ C. Wallace. 2019.

</span>
<span class="ltx_bibblock">Attention is not explanation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2007)</span>
<span class="ltx_bibblock">
Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007.

</span>
<span class="ltx_bibblock">Improving translation quality by discarding most of the phrasetable.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick etÂ al. (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, AndreiÂ A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, etÂ al. 2017.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the national academy of sciences</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi and Bojar (2017)</span>
<span class="ltx_bibblock">
Tom Kocmi and OndÅ™ej Bojar. 2017.

</span>
<span class="ltx_bibblock">Curriculum learning and minibatch bucketing in neural machine
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">RANLP</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2009)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Statistical machine translation</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn etÂ al. (2007)</span>
<span class="ltx_bibblock">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, etÂ al. 2007.

</span>
<span class="ltx_bibblock">Moses: Open source toolkit for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn etÂ al. (2003)</span>
<span class="ltx_bibblock">
Philipp Koehn, FranzÂ Josef Och, and Daniel Marcu. 2003.

</span>
<span class="ltx_bibblock">Statistical phrase-based translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample etÂ al. (2018)</span>
<span class="ltx_bibblock">
Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marcâ€™Aurelio
Ranzato. 2018.

</span>
<span class="ltx_bibblock">Phrase-based &amp; neural unsupervised machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2018)</span>
<span class="ltx_bibblock">
Jian Li, Zhaopeng Tu, Baosong Yang, MichaelÂ R. Lyu, and Tong Zhang. 2018.

</span>
<span class="ltx_bibblock">Multi-head attention with disagreement regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2006)</span>
<span class="ltx_bibblock">
Yang Liu, Qun Liu, and Shouxun Lin. 2006.

</span>
<span class="ltx_bibblock">Tree-to-string alignment template for statistical machine
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu (2010)</span>
<span class="ltx_bibblock">
Xiaofei Lu. 2010.

</span>
<span class="ltx_bibblock">Automatic analysis of syntactic complexity in second language
writing.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International journal of corpus linguistics</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong and Manning (2015)</span>
<span class="ltx_bibblock">
Minh-Thang Luong and ChristopherÂ D Manning. 2015.

</span>
<span class="ltx_bibblock">Stanford neural machine translation systems for spoken language
domains.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IWSLT</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neubig etÂ al. (2015)</span>
<span class="ltx_bibblock">
Graham Neubig, Makoto Morishita, and Satoshi Nakamura. 2015.

</span>
<span class="ltx_bibblock">Neural reranking improves subjective quality of machine translation:
Naist at wat2015.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">WAT</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Och and Ney (2004)</span>
<span class="ltx_bibblock">
FranzÂ Josef Och and Hermann Ney. 2004.

</span>
<span class="ltx_bibblock">The alignment template approach to statistical machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CL</em>, 30(4):417â€“449.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott etÂ al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">Fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Platanios etÂ al. (2019)</span>
<span class="ltx_bibblock">
EmmanouilÂ Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos,
and Tom Mitchell. 2019.

</span>
<span class="ltx_bibblock">Competence-based curriculum learning for neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">NAACL</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahaman etÂ al. (2019)</span>
<span class="ltx_bibblock">
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
Hamprecht, Yoshua Bengio, and Aaron Courville. 2019.

</span>
<span class="ltx_bibblock">On the spectral bias of neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ICML</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sato (1992)</span>
<span class="ltx_bibblock">
Satoshi Sato. 1992.

</span>
<span class="ltx_bibblock">Example-based machine translation.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich etÂ al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2016)</span>
<span class="ltx_bibblock">
Xing Shi, Inkit Padhi, and Kevin Knight. 2016.

</span>
<span class="ltx_bibblock">Does string-based neural MT learn source syntax?

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever etÂ al. (2014)</span>
<span class="ltx_bibblock">
Ilya Sutskever, Oriol Vinyals, and QuocÂ V Le. 2014.

</span>
<span class="ltx_bibblock">Sequence to sequence learning with neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">NIPS</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tillmann (2004)</span>
<span class="ltx_bibblock">
Christoph Tillmann. 2004.

</span>
<span class="ltx_bibblock">A unigram orientation model for statistical machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">HLT-NAACL</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva etÂ al. (2019)</span>
<span class="ltx_bibblock">
Mariya Toneva, Alessandro Sordoni, Remi TachetÂ des Combes, Adam Trischler,
Yoshua Bengio, and GeoffreyÂ J Gordon. 2019.

</span>
<span class="ltx_bibblock">An empirical study of example forgetting during deep neural network
learning.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016.

</span>
<span class="ltx_bibblock">Modeling coverage for neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is All You Need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">NIPS</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita etÂ al. (2019a)</span>
<span class="ltx_bibblock">
Elena Voita, Rico Sennrich, and Ivan Titov. 2019a.

</span>
<span class="ltx_bibblock">The bottom-up evolution of representations in the transformer: A
study with machine translation and language modeling objectives.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita etÂ al. (2019b)</span>
<span class="ltx_bibblock">
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/P19-1580" title="" class="ltx_ref ltx_href">Analyzing
multi-head self-attention: Specialized heads do the heavy lifting, the rest
can be pruned</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li, Deyi Xiong, and Min Zhang. 2017.

</span>
<span class="ltx_bibblock">Neural machine translation advised by statistical machine
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Xing Wang, Zhaopeng Tu, and Min Zhang. 2018.

</span>
<span class="ltx_bibblock">Incorporating statistical machine translation word knowledge into
neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 26(12):2255â€“2266.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiegreffe and Pinter (2019)</span>
<span class="ltx_bibblock">
Sarah Wiegreffe and Yuval Pinter. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/D19-1002" title="" class="ltx_ref ltx_href">Attention is not
not explanation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Baosong Yang, Longyue Wang, DerekÂ F. Wong, LidiaÂ S. Chao, and Zhaopeng Tu.
2019.

</span>
<span class="ltx_bibblock">Convolutional self-attention networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, and Maosong Sun. 2017.

</span>
<span class="ltx_bibblock">Prior knowledge integration for neural machine translation using
posterior regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Zong (2016)</span>
<span class="ltx_bibblock">
Jiajun Zhang and Chengqing Zong. 2016.

</span>
<span class="ltx_bibblock">Exploiting source-side monolingual data in neural machine
translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yang Zhao, Yining Wang, Jiajun Zhang, and Chengqing Zong. 2018.

</span>
<span class="ltx_bibblock">Phrase table as recommendation memory for neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.13269" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.13270" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.13270">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.13270" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.13271" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 11:41:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
