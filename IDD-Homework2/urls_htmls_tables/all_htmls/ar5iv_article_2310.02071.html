<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Liang Chen
    <sup class="ltx_sup" id="id12.12.id1">
     1
    </sup>
    , Yichi Zhang
    <sup class="ltx_sup" id="id13.13.id2">
     1
    </sup>
    , Shuhuai Ren
    <sup class="ltx_sup" id="id14.14.id3">
     1
    </sup>
    , Haozhe Zhao
    <sup class="ltx_sup" id="id15.15.id4">
     1
    </sup>
    , Zefan Cai
    <sup class="ltx_sup" id="id16.16.id5">
     1
    </sup>
    ,
    <span class="ltx_text ltx_font_bold" id="id7.7.2">
     Yuchi Wang
     <sup class="ltx_sup" id="id7.7.2.1">
      <span class="ltx_text ltx_font_medium" id="id7.7.2.1.1">
       1
      </span>
     </sup>
     <br class="ltx_break"/>
     Peiyi Wang
     <sup class="ltx_sup" id="id7.7.2.2">
      <span class="ltx_text ltx_font_medium" id="id7.7.2.2.1">
       1
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id8.8.3">
     Tianyu Liu
     <sup class="ltx_sup" id="id8.8.3.1">
      <span class="ltx_text ltx_font_medium" id="id8.8.3.1.1">
       2
      </span>
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id10.10.5">
     Baobao Chang
     <sup class="ltx_sup" id="id10.10.5.1">
      <span class="ltx_text ltx_font_medium" id="id10.10.5.1.1">
       1
      </span>
     </sup>
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id10.10.5.2">
      <span class="ltx_text ltx_font_medium" id="id10.10.5.2.1">
       1
      </span>
     </sup>
    </span>
    Peking University,
    <sup class="ltx_sup" id="id17.17.id6">
     2
    </sup>
    Tencent Cloud AI
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id18.18.id7">
     {leo.liang.chen, yczhang, shuhuai_ren}@stu.pku.edu.cn
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id19.19.id8">
     {hanszhao, zefan, wangyuchi, wangpeiyi}@stu.pku.edu.cn
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id20.20.id9">
     {tianyu0421, chbb}@pku.edu.cn
    </span>
    <br class="ltx_break"/>
    <a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_bold" href="https://github.com/pkunlp-icler/PCA-EVAL" target="_blank" title="">
     https://github.com/pkunlp-icler/PCA-EVAL
    </a>
   </span>
   <span class="ltx_author_notes">
    Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id21.id1">
   In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called
   <span class="ltx_text ltx_font_bold" id="id21.id1.1">
    PCA-EVAL
   </span>
   , which evaluates embodied decision-making from the perspectives of
   <span class="ltx_text ltx_font_bold" id="id21.id1.2">
    P
   </span>
   erception,
   <span class="ltx_text ltx_font_bold" id="id21.id1.3">
    C
   </span>
   ognition, and
   <span class="ltx_text ltx_font_bold" id="id21.id1.4">
    A
   </span>
   ction. Additionally, we propose
   <span class="ltx_text ltx_font_bold" id="id21.id1.5">
    HOLMES
   </span>
   , a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The capacity to make well-informed decisions is essential for the survival and success of living organisms in their respective environments. Similarly, a major goal in embodied artificial intelligence is to develop agents, like robots, with sophisticated decision-making abilities. This could enable artificial agents to intelligently interact with their surroundings and efficiently accomplish a variety of real-world tasks such as autonomous driving
    <cite class="ltx_cite ltx_citemacro_citep">
     (Hu et al.,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ; Wayve,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023
     </a>
     )
    </cite>
    , domestic assistance
    <cite class="ltx_cite ltx_citemacro_citep">
     (Kolve et al.,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2017
     </a>
     ; Shridhar et al.,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2020
     </a>
     ; Huang et al.,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2022b
     </a>
     )
    </cite>
    , and game playing
    <cite class="ltx_cite ltx_citemacro_citep">
     (Fan et al.,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2022
     </a>
     ; Wang et al.,
     <a class="ltx_ref" href="#bib.bib41" title="">
      2023a
     </a>
     ; Zhu et al.,
     <a class="ltx_ref" href="#bib.bib57" title="">
      2023b
     </a>
     )
    </cite>
    . Recently, there has been a notable increase in leveraging exceptional reasoning capabilities and world knowledge of Large Language Models (LLMs) to enhance decision making in agents. However, LLMs are primarily designed to process textual context, creating a modality gap
    <cite class="ltx_cite ltx_citemacro_citep">
     (Liang et al.,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2022
     </a>
     ; Ren et al.,
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023a
     </a>
     )
    </cite>
    for the LLM-powered agent when dealing with multimodal observations in real-world scenarios.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    To bridge this modality gap, a common approach is converting multimodal observations into text using various APIs
    <cite class="ltx_cite ltx_citemacro_citep">
     (Wu et al.,
     <a class="ltx_ref" href="#bib.bib48" title="">
      2023
     </a>
     ; Yang et al.,
     <a class="ltx_ref" href="#bib.bib50" title="">
      2023
     </a>
     )
    </cite>
    . However, this conversion can result in information loss during the transition from multimodal to unimodal text.
At the same time, recent advances in Multimodal Large Language Models (MLLMs), particularly Visual Large Language Models (VLLMs) like GPT4-Vision
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023a
     </a>
     )
    </cite>
    , have showcased impressive general-purpose visual understanding and reasoning abilities
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhu et al.,
     <a class="ltx_ref" href="#bib.bib56" title="">
      2023a
     </a>
     ; Dai et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     ; Liu et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023a
     </a>
     ; Li et al.,
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023b
     </a>
     ; Zhao et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2023
     </a>
     )
    </cite>
    . These VLLMs can directly perceive the visual information rather than relying on textual intermediaries, potentially enabling more sophisticated reasoning and decision making for embodied agents operating in complex real-world environments.
Considering these developments, two research questions naturally arise:
    <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">
     (1)
    </span>
    Can current state-of-the-art VLLMs perform various embodied decision making tasks in an end-to-end manner? What are the current strengths and limitations when compared to LLM-powered agents?
    <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">
     (2)
    </span>
    Can LLMs and VLLMs collaborate to enhance embodied decision-making capabilities?
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="331" id="S1.F1.g1" src="/html/2310.02071/assets/figs/sun.jpg" width="329"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Domain and required ability distribution of PCA-EVAL.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    However, addressing these questions is challenging due to the absence of an existing evaluation benchmark that satisfies the following criteria:
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     (1)
    </span>
    supporting end-to-end embodied decision making by providing agents with direct multimodal observations;
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">
     (2)
    </span>
    enabling multi-dimensional evaluation of the decision-making process, encompassing perception, reasoning, and action perspectives, rather than relying solely on final rewards or success rate; and
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">
     (3)
    </span>
    covering diverse domains, drawing from different areas of embodied AI.
The development of more comprehensive benchmarks that meet these desiderata could substantially advance research on decision making in embodied systems.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    In this paper, we propose a new benchmark,
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">
     PCA-EVAL
    </span>
    , for evaluating the embodied decision-making ability of agents from three perspectives, i.e.,
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">
     P
    </span>
    erception,
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">
     C
    </span>
    ognition, and
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">
     A
    </span>
    ction. Our benchmark covers three domains as illustrated in Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    : autonomous driving, domestic assistance, and game-playing. The corresponding data are collected from real-world transportation scenes
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhu et al.,
     <a class="ltx_ref" href="#bib.bib58" title="">
      2016
     </a>
     )
    </cite>
    , domestic housekeeper environment based on ALFRED
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shridhar et al.,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2020
     </a>
     )
    </cite>
    , and Open-world environment Minedojo
    <cite class="ltx_cite ltx_citemacro_citep">
     (Fan et al.,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2022
     </a>
     )
    </cite>
    based on the famous game Minecraft. This diverse set of domains allows for a comprehensive assessment of embodied decision-making capabilities across various contexts. Distinct from the MDP-based evaluation that solely focuses on maximizing cumulative rewards, we divide the sequential decision making process into multiple one-step decision problems based on a task-specific topology graph. Each instance in the benchmark consists of a 6-element tuple:
    <math alttext="&lt;\textit{image},~{}\textit{question},~{}\textit{action candidates},~{}\textit{answer},~{}\textit{reason},~{}\textit{key concept}&gt;" class="ltx_math_unparsed" display="inline" id="S1.p4.1.m1.6">
     <semantics id="S1.p4.1.m1.6a">
      <mrow id="S1.p4.1.m1.6b">
       <mo id="S1.p4.1.m1.6.7">
        &lt;
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.1.1">
        image
       </mtext>
       <mo id="S1.p4.1.m1.6.8" rspace="0.497em">
        ,
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.2.2">
        question
       </mtext>
       <mo id="S1.p4.1.m1.6.9" rspace="0.497em">
        ,
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.3.3">
        action candidates
       </mtext>
       <mo id="S1.p4.1.m1.6.10" rspace="0.497em">
        ,
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.4.4">
        answer
       </mtext>
       <mo id="S1.p4.1.m1.6.11" rspace="0.497em">
        ,
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.5.5">
        reason
       </mtext>
       <mo id="S1.p4.1.m1.6.12" rspace="0.497em">
        ,
       </mo>
       <mtext class="ltx_mathvariant_italic" id="S1.p4.1.m1.6.6">
        key concept
       </mtext>
       <mo id="S1.p4.1.m1.6.13">
        &gt;
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="S1.p4.1.m1.6c">
       &lt;\textit{image},~{}\textit{question},~{}\textit{action candidates},~{}\textit{answer},~{}\textit{reason},~{}\textit{key concept}&gt;
      </annotation>
     </semantics>
    </math>
    . Adopting this approach offers two major advantages:
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.5">
     (1)
    </span>
    It enables a more comprehensive evaluation of the decision-making process, with each decision step being assessed in terms of perception, cognition, and action.
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.6">
     (2)
    </span>
    The evaluation can be conducted outside complex simulation environments, simplifying the process of evaluating different agents and their performance.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    With the proposed benchmark, we conduct two series of evaluation: (1) We examine multiple state-of-the-art VLLMs, like InstructBLIP
    <cite class="ltx_cite ltx_citemacro_citep">
     (Dai et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     )
    </cite>
    , MMICL
    <cite class="ltx_cite ltx_citemacro_citep">
     (Zhao et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2023
     </a>
     )
    </cite>
    , QwenVL-Chat
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bai et al.,
     <a class="ltx_ref" href="#bib.bib2" title="">
      2023
     </a>
     )
    </cite>
    and the latest GPT4-Vision
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023a
     </a>
     )
    </cite>
    , in an end-to-end decision making context. (2) We introduce
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     HOLMES
    </span>
    ,
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       The system is aptly named after the renowned detective, Sherlock Holmes.
      </span>
     </span>
    </span>
    a multi-agent cooperation framework. In this framework, we provide large language models, such as ChatGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib27" title="">
      2022
     </a>
     )
    </cite>
    , GPT4
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023b
     </a>
     )
    </cite>
    , and Vicuna
    <cite class="ltx_cite ltx_citemacro_citep">
     (Chiang et al.,
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     )
    </cite>
    , with descriptions of vision models like image captioning, object detection, Optical Character Recognition (OCR), and traffic sign detection models. Additionally, we supply descriptions of valid APIs within the simulated environment. The large language model subsequently initiates a search for clues pertaining to the question by engaging in a multi-turn conversation. This process involves alternating between invoking models or APIs to find clues and analyzing the discovered clues to facilitate informed decision making.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    From our experimental results, we discerned that within the end-to-end framework, GPT4-Vision significantly outshines the contemporary state-of-the-art vision-language model, MMICL, boasting an average action accuracy improvement of 26%. Notably, GPT4-Vision can furnish a detailed rationale behind its embodied decision-making process, a feature absent in present open-source VLLMs. When assessing HOLMES models, GPT4 consistently emerges superior across all three domains. Drawing a comparison between GPT4-Vision and HOLMES, we observed that GPT4-Vision surpasses GPT4-HOLMES with multiple expert visual APIs in terms of cognition and action scores. This underscores its broad adaptability across a spectrum of visual tasks and its good fusion of visual understanding, world knowledge, and embodied decision making.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    In summary, we introduce three key contributions in this study:
   </p>
   <ol class="ltx_enumerate" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We propose PCA-EVAL, a novel evaluation benchmark for multi-domain embodied decision making that evaluates performance in perception, cognition, and action.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We present HOLMES, a multi-agent cooperation framework designed to tackle various embodied decision-making tasks that include multimodal observations. It mimics the process of playing a detective game in which the LLM uncovers clues by utilizing various multimodal models or APIs supplied by the environment.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       We conducted a systematic comparison of two embodied decision-making methods: end2end and HOLMES, across various models. Our findings suggest that when utilizing MLLM with the end2end method, it not only achieves decision accuracy better than the top-performing model (GPT-4) in HOLMES but also secures a superior cognition score. However, this level of performance is exclusive to the latest GPT4-Vision model, which significantly outpaces the open-source state-of-the-art VLLMs.
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    We believe that powerful MLLMs like GPT4-Vision pave a new and promising way toward decision making in embodied agents using LLMs. It enables decisions across diverse domains to be made and justified seamlessly in an end-to-end manner. PCA-EVAL serves as an effective metric for evaluating the embodied decision-making capabilities of both end-to-end and HOLMES-based models.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Embodied Decision Making.
   </h4>
   <div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">
     Research on embodied decision-making is an emerging trend for artificial intelligent agents to interact with their surroundings and accomplish numerous tasks. This necessitates proficiency in vision perception, world knowledge, and commonsense reasoning, areas where a large language model can provide some level of expertise.
We group prior work on embodied decision-making with LLM into two main trends.
The first trend is to transform multimodal information, including object and scenery identification, the current states of AI agents, and the feedback from the environments, to texts. Text-based LLMs can then reason over the textual clues to determine the next action towards completing a designated task
     <cite class="ltx_cite ltx_citemacro_citep">
      (Huang et al.,
      <a class="ltx_ref" href="#bib.bib13" title="">
       2022a
      </a>
      ; Li et al.,
      <a class="ltx_ref" href="#bib.bib20" title="">
       2022
      </a>
      ; Huang et al.,
      <a class="ltx_ref" href="#bib.bib14" title="">
       2022b
      </a>
      ; Chen et al.,
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     . This line of research divides the entire decision-making process into two phases: (1) information seeking, usually involving VLLMs to verbalize the current status of AI agents in the vision-based environment with natural language; (2) reasoning and planning with text-based LLMs to decide what the AI agent should do in the next step with textual clues.
The other line of research uses multimodal LLMs directly for end-to-end decision making, such as PALM-E
     <cite class="ltx_cite ltx_citemacro_citep">
      (Driess et al.,
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023b
      </a>
      )
     </cite>
     .
The end-to-end decision making poses greater challenges to multimodal LLMs as it requires the combination of different functionalities including perception, cognition, and action, whereas decision making without explicit multiple steps mitigates the error propagation between information seeking and reasoning.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    LLM-Powered Agents.
   </h4>
   <div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">
     Large language models pre-trained on large-scale multimodal (including text, image, video, etc.) corpus demonstrate impressive emergent abilities and immense popularity
     <cite class="ltx_cite ltx_citemacro_citep">
      (Brown et al.,
      <a class="ltx_ref" href="#bib.bib3" title="">
       2020
      </a>
      ; Wei et al.,
      <a class="ltx_ref" href="#bib.bib46" title="">
       2022
      </a>
      )
     </cite>
     , and have seen tremendous success across various domains covering various natural language processing and computer vision tasks
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford et al.,
      <a class="ltx_ref" href="#bib.bib32" title="">
       2019
      </a>
      ; Chowdhery et al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2022
      </a>
      ; Touvron et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2023
      </a>
      ; Alayrac et al.,
      <a class="ltx_ref" href="#bib.bib1" title="">
       2022
      </a>
      ; Zhu et al.,
      <a class="ltx_ref" href="#bib.bib56" title="">
       2023a
      </a>
      ; Li et al.,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023a
      </a>
      )
     </cite>
     .
Consequently, using LLMs to empower the AI agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (Xi et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2023
      </a>
      ; Liu et al.,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023b
      </a>
      ; Park et al.,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib44" title="">
       2023d
      </a>
      ; Yuan et al.,
      <a class="ltx_ref" href="#bib.bib53" title="">
       2023
      </a>
      )
     </cite>
     becomes more and more promising.
Specifically, we can employ LLMs to enhance the decision making ability of the agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (Nakano et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2022
      </a>
      ; Yao et al.,
      <a class="ltx_ref" href="#bib.bib51" title="">
       2022
      </a>
      ; Li et al.,
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023c
      </a>
      ; Song et al.,
      <a class="ltx_ref" href="#bib.bib38" title="">
       2023
      </a>
      )
     </cite>
     , expanding their perception and action space through strategies like tool utilization
     <cite class="ltx_cite ltx_citemacro_citep">
      (Schick et al.,
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023
      </a>
      ; Qin et al.,
      <a class="ltx_ref" href="#bib.bib31" title="">
       2023
      </a>
      ; Lu et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2023
      </a>
      )
     </cite>
     .
Although LLM-based agents demonstrate reasoning and planning abilities through techniques like Chain of Thought or problem decomposition
     <cite class="ltx_cite ltx_citemacro_citep">
      (Wei et al.,
      <a class="ltx_ref" href="#bib.bib47" title="">
       2023
      </a>
      ; Yao et al.,
      <a class="ltx_ref" href="#bib.bib52" title="">
       2023
      </a>
      ; Kojima et al.,
      <a class="ltx_ref" href="#bib.bib15" title="">
       2022
      </a>
      )
     </cite>
     , they inherently lack visual perception, and are limited to the discrete textual content. Therefore, integrating visual information or other modalities can offer agents a broader context and a more precise understanding
     <cite class="ltx_cite ltx_citemacro_citep">
      (Driess et al.,
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023a
      </a>
      )
     </cite>
     , enhancing their environmental perception. However, no evaluation protocol or benchmark is currently available to evaluate decision making within the multimodal context.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   PCA-EVAL
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we propose to evaluate the decision-making ability of embodied agents from three perspectives: perception, cognition, and action. Accordingly, we present a novel benchmark named PCA-EVAL.
Our PCA-EVAL benchmark consists of 300 multimodal multiple-choice questions with diverse embodied topics and annotations of their answers with corresponding explanations.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p2">
   <p class="ltx_p" id="S3.p2.2">
    As shown in Figure
    <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    , each instance in the benchmark consists of a 6-element tuple:
    <math alttext="&lt;" class="ltx_Math" display="inline" id="S3.p2.1.m1.1">
     <semantics id="S3.p2.1.m1.1a">
      <mo id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">
       &lt;
      </mo>
      <annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b">
       <lt id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">
       </lt>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">
       &lt;
      </annotation>
     </semantics>
    </math>
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.2">
     image
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.3">
     question
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.4">
     action candidates
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.5">
     answer
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.6">
     reason
    </span>
    ,
    <span class="ltx_text ltx_font_italic" id="S3.p2.2.1">
     key concept
     <math alttext="&gt;" class="ltx_Math" display="inline" id="S3.p2.2.1.m1.1">
      <semantics id="S3.p2.2.1.m1.1a">
       <mo id="S3.p2.2.1.m1.1.1" xref="S3.p2.2.1.m1.1.1.cmml">
        &gt;
       </mo>
       <annotation-xml encoding="MathML-Content" id="S3.p2.2.1.m1.1b">
        <gt id="S3.p2.2.1.m1.1.1.cmml" xref="S3.p2.2.1.m1.1.1">
        </gt>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.p2.2.1.m1.1c">
        &gt;
       </annotation>
      </semantics>
     </math>
    </span>
    . The image is collected from various embodied environments, like transportation scenes, housekeeper environments, and game worlds in Minecraft. Questions, action candidates, and answers are derived from real tasks within the corresponding environment. The reasoning explains why the answer is the best choice for the current image, while the key concept highlights the most question-related aspect in the image.
   </p>
  </div>
  <figure class="ltx_figure ltx_align_floatright" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="218" id="S3.F2.1.g1" src="/html/2310.02071/assets/x1.png" width="169"/>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    An instance of PCA-EVAL.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    Unlike traditional visual question-answering datasets that emphasize visual perception (e.g., VQA
    <cite class="ltx_cite ltx_citemacro_citep">
     (Goyal et al.,
     <a class="ltx_ref" href="#bib.bib11" title="">
      2017
     </a>
     )
    </cite>
    ), visual reasoning (e.g., NLVR
    <cite class="ltx_cite ltx_citemacro_citep">
     (Suhr et al.,
     <a class="ltx_ref" href="#bib.bib39" title="">
      2017
     </a>
     )
    </cite>
    ), or world knowledge (e.g., OKVQA
    <cite class="ltx_cite ltx_citemacro_citep">
     (Marino et al.,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2019
     </a>
     )
    </cite>
    ), the most distinctive characteristic of PCA-EVAL is its grounding in embodied actions. Compared to embodied simulation environments like ALFRED
    <cite class="ltx_cite ltx_citemacro_citep">
     (Shridhar et al.,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2020
     </a>
     )
    </cite>
    and Minedojo
    <cite class="ltx_cite ltx_citemacro_citep">
     (Fan et al.,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2022
     </a>
     )
    </cite>
    , PCA-EVAL proves to be more effective in evaluating various LLM-based agents. This is primarily due to PCA-EVAL’s provision of high-level actions that can be readily implemented or programmed using the low-level actions in the corresponding domains. The high-level actions are more comprehensible for LLMs than the direct low-level actions like robotic movements in the simulation environments because (1) the high-level actions are in the form of natural languages, making it easier for LLMs to understand the meaning and connect with world knowledge. (2) LLMs are not grounded with low-level actions during the pretraining or finetuning stage, making it hard for LLMs to understand the consequences of executing an action.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S3.p4">
   <p class="ltx_p" id="S3.p4.1">
    To answer a question in PCA-EVAL, the agent must possess the following abilities: (1) Perception: accurately identify the concept related to the question within the image; (2) Cognition: engage in reasoning based on image perception and worldly knowledge; (3) Action: comprehend the potential actions, selecting the one that best aligns with the outcome of the reasoning process. A deficiency in any of these abilities would inevitably result in an incorrect answer, posing a significant challenge to the more complex capabilities of embodied agents. Although challenging, all the aforementioned abilities are essential for the decision-making process in embodied environments.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Evaluation Metrics
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.4">
     For each instance, we instruct the agent to deliver an answer triplet comprising an image description
     <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1">
      <semantics id="S3.SS1.p1.1.m1.1a">
       <mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">
        d
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b">
        <ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">
         𝑑
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">
        d
       </annotation>
      </semantics>
     </math>
     , a reasoning process
     <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1">
      <semantics id="S3.SS1.p1.2.m2.1a">
       <mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">
        r
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b">
        <ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">
         𝑟
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">
        r
       </annotation>
      </semantics>
     </math>
     , and a final action
     <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1">
      <semantics id="S3.SS1.p1.3.m3.1a">
       <mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">
        a
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b">
        <ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">
         𝑎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">
        a
       </annotation>
      </semantics>
     </math>
     , represented as
     <math alttext="&lt;d,r,a&gt;" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.4.m4.3">
      <semantics id="S3.SS1.p1.4.m4.3a">
       <mrow id="S3.SS1.p1.4.m4.3b">
        <mo id="S3.SS1.p1.4.m4.3.4">
         &lt;
        </mo>
        <mi id="S3.SS1.p1.4.m4.1.1">
         d
        </mi>
        <mo id="S3.SS1.p1.4.m4.3.5">
         ,
        </mo>
        <mi id="S3.SS1.p1.4.m4.2.2">
         r
        </mi>
        <mo id="S3.SS1.p1.4.m4.3.6">
         ,
        </mo>
        <mi id="S3.SS1.p1.4.m4.3.3">
         a
        </mi>
        <mo id="S3.SS1.p1.4.m4.3.7">
         &gt;
        </mo>
       </mrow>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.3c">
        &lt;d,r,a&gt;
       </annotation>
      </semantics>
     </math>
     . By comparing the model prediction with the ground truth answer, we can obtain a fine-grained diagnosis of the decision making process.
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Perception Score.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">
      The Perception Score (P-Score) measures the model’s ability to accurately perceive and interpret the observation. It is computed based on whether the agent’s output image description
      <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1">
       <semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a">
        <mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">
         d
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b">
         <ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">
          𝑑
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">
         d
        </annotation>
       </semantics>
      </math>
      includes the key concept of the instance. If the agent accurately describes the question-related key concept in the image, the P-score is assigned a value of 1; otherwise, it is assigned a value of 0. For the instance in Figure
      <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , the agent should output “clear road” or “no car visible” or other semantically equivalent concepts in its description of the image to get the perception score.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Cognition Score.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">
      The Cognition Score (C-Score) assesses the model’s ability to reason, comprehend, and make informed decisions based on the perceived input data and world knowledge. The score is 1 if the reasoning process is correct, otherwise the score is 0. For the instance in Figure
      <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , the agent should link the “clear road” to the action “keep driving” based on transportation commonsense to get the score.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Action Score.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">
      The Action Score (A-Score) measures the model’s ability to generate appropriate and effective responses or actions based on the perceived input data and the cognitive understanding of the context. The score is assigned a value of 1 if the agent selects the correct action; otherwise, the score is set to 0.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px3.p2">
     <p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">
      The final Perception, Cognition, and Action scores of the agents are obtained by averaging the scores across all instances and domains in our PCA-EVAL dataset.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Automatic Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Recent advancements have seen researchers harnessing powerful LLMs for the evaluation of output of language models. Studies have revealed that the outcomes from LLMs could exhibit remarkable alignment with human judgments
     <cite class="ltx_cite ltx_citemacro_cite">
      Zheng et al. (
      <a class="ltx_ref" href="#bib.bib55" title="">
       2023
      </a>
      ); Wang et al. (
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023c
      </a>
      ;
      <a class="ltx_ref" href="#bib.bib42" title="">
       b
      </a>
      )
     </cite>
     . In our investigation, we employed GPT-4 to automatically evaluate perception, cognition, and action scores based on the model’s outputs. Our findings underscore a significant agreement between GPT-4 annotations and human annotator results. This is substantiated by Pearson correlation coefficients of 0.8, 0.9, and 0.95 for perception, cognition, and action evaluations, respectively. To facilitate ongoing and future research endeavors, we share our automatic evaluation script
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pkunlp-icler/PCA-EVAL/blob/main/pca-eval/evaluation/pca_auto_scoring.py" target="_blank" title="">
         https://github.com/pkunlp-icler/PCA-EVAL/blob/main/pca-eval/evaluation/pca_auto_scoring.py
        </a>
       </span>
      </span>
     </span>
     for seamless adoption, which could also be improved in the future. For a detailed description of our evaluation methodology, kindly refer to Appendix
     <a class="ltx_ref" href="#A3" title="Appendix C Automatic Evaluation ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Dataset Overview
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     The PCA-EVAL benchmark currently comprises three domains, with a total of 300 instances, including 100 instances per domain.
In our preliminary study, we find that the annotation process requires proactive thinking of the questions, actions, and corresponding answers, which makes quality control difficult. In order to ensure the quality of PCA-Eval, every single test case has been verified by at least three authors of this paper. Although challenging, we would keep scaling this benchmark in order to advocate further attention to end-to-end decision-making. We introduce the three domains encompassed by our dataset as follows:
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Autonomous Driving.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">
      In the autonomous driving domain, instances are derived from real-world transportation scenes, which requires the agent to have particular abilities such as traffic sign recognition, obstacle detection, and decision-making at intersections. The dataset aims to evaluate an agent’s ability to perceive and interpret visual information while making safe and efficient driving decisions. The images are collected from TT100K
      <cite class="ltx_cite ltx_citemacro_citep">
       (Zhu et al.,
       <a class="ltx_ref" href="#bib.bib58" title="">
        2016
       </a>
       )
      </cite>
      dataset and annotators are instructed to propose an image-conditioned question that is grounded with real actions of vehicles.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Domestic Robot.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">
      The domestic assistance domain features instances from the ALFRED
      <cite class="ltx_cite ltx_citemacro_citep">
       (Shridhar et al.,
       <a class="ltx_ref" href="#bib.bib37" title="">
        2020
       </a>
       ; Kolve et al.,
       <a class="ltx_ref" href="#bib.bib16" title="">
        2017
       </a>
       )
      </cite>
      environment, which simulates a housekeeper robot performing tasks within a household setting. These tasks may include object manipulation, navigation, and interaction with various appliances. The environment assesses an agent’s ability to understand and execute complex instructions while navigating and interacting with a dynamic environment. Annotators are asked to select one image from the randomly generated scenes in the environment, propose a question related to the items on the scene, and annotate the full information of the instance.
     </p>
    </div>
    <figure class="ltx_figure ltx_align_floatright" id="S3.F3">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="S3.F3.1.g1" src="/html/2310.02071/assets/x2.png" width="169"/>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       Figure 3:
      </span>
      Illustration of task topology graph. Events in green represent the leaf nodes of the graph.
     </figcaption>
    </figure>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Open-World Game.
    </h4>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">
      In the open-world game domain, instances are sourced from the Minecraft environment, where agents are tasked with exploring, crafting, and surviving in a procedurally generated world. This dataset evaluates an agent’s ability to reason and plan actions within a complex, open-ended environment, which often requires long-term strategizing and adaptability. Annotators receive predefined tasks from MineDojo
      <cite class="ltx_cite ltx_citemacro_citep">
       (Fan et al.,
       <a class="ltx_ref" href="#bib.bib10" title="">
        2022
       </a>
       )
      </cite>
      as a reference during the task generation phase. For each task, we instruct the annotator to sketch a task topology graph, exemplified in Figure
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ Domestic Robot. ‣ 3.3 Dataset Overview ‣ 3 PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      . The task should be completed in accordance with the topological order of the graph, where the event located in the leaf nodes should be finished first. Each node in the task topology graph can be viewed as a step in the sequential decision. We list the in-domain task distribution and examples for each domain in Appendix
      <a class="ltx_ref" href="#A1" title="Appendix A Examples of PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
       <span class="ltx_text ltx_ref_tag">
        A
       </span>
      </a>
      .
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Annotation Pipelines
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     The annotation process consists of two stages: (1) Dataset Annotation, and (2) Dataset Refinement. During the initial stage, three annotators are assigned to each domain, adhering strictly to the respective annotation guidelines. They first pinpoint the source images from each domain that are informative and meaningful so that they can write questions for each image.
The annotators have the responsibility to ensure every question has only one correct answer and accurate rationales. In the subsequent stage, annotators are instructed to scrutinize the output actions and rationales presented by ChatGPT and check the annotations. This process aims to address the challenge of multiple correct answers, as ChatGPT can furnish comprehensive explanations for its actions. These explanations assist annotators in assessing the acceptability of ChatGPT’s response, particularly when it deviates from the established ground truth answer. This enables annotators to refine annotations to ensure the presence of a single correct answer.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Methods
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    End2End Decision Making via VLLMs
   </h3>
   <figure class="ltx_figure" id="S4.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S4.F4.g1" src="/html/2310.02071/assets/x3.png" width="422"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Three examples of HOLMES solving questions from different domains of PCA-EVAL.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     In this subsection, we detail the evaluation process for assessing state-of-the-art VLLMs, e.g., InstructBLIP, MMICL, and GPT4-Vision, on end-to-end embodied decision-making using the proposed PCA-EVAL benchmark. End2End embodied decision making is straightforward since we can directly feed the visual observation and the textual question to the multi-modal agent. As illustrated in Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , the agent is prompted to output the image description and reasoning process before giving the final action.
    </p>
   </div>
   <figure class="ltx_figure ltx_align_floatright" id="S4.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="215" id="S4.F5.1.g1" src="/html/2310.02071/assets/x4.png" width="169"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     An example of end-to-end decision making.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    HOLMES: Multi-Agent Cooperation
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Different from End2End embodied decision making, within HOLMES, we prompt large language models like ChatGPT-3.5
     <cite class="ltx_cite ltx_citemacro_citep">
      (OpenAI,
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     , GPT4
     <cite class="ltx_cite ltx_citemacro_citep">
      (OpenAI,
      <a class="ltx_ref" href="#bib.bib29" title="">
       2023b
      </a>
      )
     </cite>
     to call different visual models or APIs to gather information about the environment.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     We provide these models with descriptions of the input and output for different visual models such as the image caption model based on InstructBLIP, the object detection model based on POMP
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ren et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023b
      </a>
      )
     </cite>
     , and the traffic sign detection model based on YOLO
     <cite class="ltx_cite ltx_citemacro_citep">
      (Redmon &amp; Farhadi,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2018
      </a>
      )
     </cite>
     . Additionally, we supply descriptions of valid APIs within the simulated environment, such as
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">
      list_nearby_mobs_in_minecraft()
     </span>
     to tell what creatures can current player see and
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">
      list_items_at_hand_in_alfred()
     </span>
     to tell what item the robot is holding in hand. Full API description files for each domain are shown in Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B API Description and Implementation of HOLMES ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     These integrations enable the large language model to initiate a search for clues pertaining to a given question through a multi-turn conversation. As shown in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , the process involves alternating between invoking models or APIs to gather relevant information and analyzing the discovered clues to facilitate informed decision making. The HOLMES framework is designed to enhance cooperation and coordination among multiple agents in dynamic and complex environments.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     In HOLMES, there are four key components as depicted in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 End2End Decision Making via VLLMs ‣ 4 Methods ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     : the image, the user, the LLM, and the Model/API Hub. Initially, the user poses a question about the optimal action to take based on the environment shown in the image, providing potential action choices. As the LLM cannot directly view the image, it’s briefed with descriptions of available visual models and APIs supplied by the simulation environment. It’s then tasked with gathering relevant data via these models and APIs to determine the appropriate action. When the LLM responds, the system checks if it has invoked a legitimate model or API, subsequently relaying the results from the invoked API. This feedback is logged into the dialogue history, allowing the LLM to analyze and form subsequent responses. Once equipped with sufficient information, the LLM proposes the final action, accompanied by its underlying rationale. HOLMES emulates the detective game process, where one alternates between searching for clues using various tools and analyzing them before arriving at a conclusion.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Configurations
   </h3>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     End2End.
    </h4>
    <div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">
      Under this setting same image and prompts are provided to different VLLMs. Additionally, the non-visual information “items in hand” and “items in inventory” for domestic and game domains are directly given to the models in the prompt since these information is hard to perceive from the image and is easy to obtain from the simulation environments. We would also make the prompts we use open-source for fair and convenient evaluation.
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p2">
     <p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">
      We compare four different models, InstructBLIP-Vicuna-13B
      <span class="ltx_note ltx_role_footnote" id="footnote3">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          3
         </sup>
         <span class="ltx_tag ltx_tag_note">
          3
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/salesforce/LAVIS/tree/main/projects/instructblip" target="_blank" title="">
          https://github.com/salesforce/LAVIS/tree/main/projects/instructblip
         </a>
        </span>
       </span>
      </span>
      , MMICL-FLANT5XXL
      <span class="ltx_note ltx_role_footnote" id="footnote4">
       <sup class="ltx_note_mark">
        4
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          4
         </sup>
         <span class="ltx_tag ltx_tag_note">
          4
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BleachNick/MMICL-Instructblip-T5-xxl" target="_blank" title="">
          https://huggingface.co/BleachNick/MMICL-Instructblip-T5-xxl
         </a>
        </span>
       </span>
      </span>
      , QwenVL-Chat
      <span class="ltx_note ltx_role_footnote" id="footnote5">
       <sup class="ltx_note_mark">
        5
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          5
         </sup>
         <span class="ltx_tag ltx_tag_note">
          5
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Qwen/Qwen-VL-Chat" target="_blank" title="">
          https://huggingface.co/Qwen/Qwen-VL-Chat
         </a>
        </span>
       </span>
      </span>
      and GPT4-Vision
      <span class="ltx_note ltx_role_footnote" id="footnote6">
       <sup class="ltx_note_mark">
        6
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          6
         </sup>
         <span class="ltx_tag ltx_tag_note">
          6
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com" target="_blank" title="">
          https://chat.openai.com
         </a>
        </span>
       </span>
      </span>
      . We apply default inference configurations for the corresponding models.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     HOLMES.
    </h4>
    <div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">
      In HOLMES framework, the LLM is required to continuously invoke various APIs and retrieve their return information. To streamline the evaluation process, we initially execute all APIs for every instance in PCA-EVAL, storing the result for each instance. This approach allows us to directly access the specific result of a given API without the need to run the model each time an evaluation is conducted. We would also make the API results open-source together with the benchmark. The description and implementation details of the APIs are listed in Appendix
      <a class="ltx_ref" href="#A2" title="Appendix B API Description and Implementation of HOLMES ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
       <span class="ltx_text ltx_ref_tag">
        B
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p2">
     <p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">
      We compare three LLMs: Vicuna
      <span class="ltx_note ltx_role_footnote" id="footnote7">
       <sup class="ltx_note_mark">
        7
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          7
         </sup>
         <span class="ltx_tag ltx_tag_note">
          7
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/lmsys" target="_blank" title="">
          https://huggingface.co/lmsys
         </a>
        </span>
       </span>
      </span>
      , ChatGPT-3.5-Turbo and GPT4
      <span class="ltx_note ltx_role_footnote" id="footnote8">
       <sup class="ltx_note_mark">
        8
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          8
         </sup>
         <span class="ltx_tag ltx_tag_note">
          8
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com" target="_blank" title="">
          https://platform.openai.com
         </a>
        </span>
       </span>
      </span>
      . However we found Vicuna models lack the capability to call various APIs for information gathering, thus we have only reported the results for ChatGPT and GPT4. We anticipate supplementing these results as soon as open-source models become available, which can understand API descriptions and correspondingly call different APIs.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Evaluation
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     PCA-Eval assesses embodied decision-making through three distinct lenses: perception, cognition, and action. The scores we reported in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.3 Main Results ‣ 5 Experiments ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     rely on the consensus score from three human evaluators. We compute the average kappa correlation coefficient for these evaluators, resulting in 0.91 for the Perception Score and 0.88 for the Cognition Score. These figures
indicate a good consistency in the evaluation process.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Main Results
   </h3>
   <figure class="ltx_table" id="S5.T1">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.6" style="width:397.5pt;height:115.9pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-48.1pt,14.0pt) scale(0.805140172703987,0.805140172703987) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.6.6">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S5.T1.6.6.7.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.6.6.7.1.1" rowspan="2">
          <span class="ltx_text" id="S5.T1.6.6.7.1.1.1">
           Method
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.6.6.7.1.2" rowspan="2">
          <span class="ltx_text" id="S5.T1.6.6.7.1.2.1">
           Model
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T1.6.6.7.1.3">
          Traffic
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T1.6.6.7.1.4">
          Domestic
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T1.6.6.7.1.5">
          Game
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T1.6.6.7.1.6">
          Average
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.6.6.8.2">
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.1">
          P
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.2">
          C
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.6.6.8.2.3">
          A
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.4">
          P
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.5">
          C
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.6.6.8.2.6">
          A
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.7">
          P
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.8">
          C
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.6.6.8.2.9">
          A
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.10">
          P
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.11">
          C
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.8.2.12">
          A
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.1.1.2" rowspan="4">
          <span class="ltx_text" id="S5.T1.1.1.1.2.1">
           End2End
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1">
          InstructBLIP
          <sup class="ltx_sup" id="S5.T1.1.1.1.1.1">
           <span class="ltx_text" id="S5.T1.1.1.1.1.1.1" style="color:#FF0000;">
            †
           </span>
          </sup>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.3">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.4">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.5">
          0.42
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.6">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.7">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.8">
          0.41
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.9">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.10">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.11">
          0.24
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.12">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.13">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.1.1.14">
          0.36
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.2.2.2">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.2.2.1">
          MMICL
          <sup class="ltx_sup" id="S5.T1.2.2.2.1.1">
           <span class="ltx_text" id="S5.T1.2.2.2.1.1.1" style="color:#FF0000;">
            †
           </span>
          </sup>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.2">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.3">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.2.2.2.4">
          0.63
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.5">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.6">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.2.2.2.7">
          0.51
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.8">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.9">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.2.2.2.10">
          0.29
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.11">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.12">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.2.2.2.13">
          0.48
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.3.3.3">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.3.1">
          QwenVL-Chat
          <sup class="ltx_sup" id="S5.T1.3.3.3.1.1">
           <span class="ltx_text" id="S5.T1.3.3.3.1.1.1" style="color:#FF0000;">
            †
           </span>
          </sup>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.2">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.3">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.3.3.4">
          0.59
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.5">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.6">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.3.3.7">
          0.55
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.8">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.9">
          -
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.3.3.10">
          0.24
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.11">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.12">
          -
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.13">
          0.46
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.4.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.4.4.4.1">
          GPT-4V
          <sup class="ltx_sup" id="S5.T1.4.4.4.1.1">
           ‡
          </sup>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.2">
          0.75
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.3">
          0.73
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.4.4.4.4">
          0.78
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.5">
          0.81
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.6">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.6.1">
           0.69
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.4.4.4.7">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.7.1">
           0.67
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.8">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.8.1">
           0.95
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.9">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.9.1">
           0.79
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.4.4.4.10">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.10.1">
           0.77
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.11">
          0.84
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.12">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.12.1">
           0.74
          </span>
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.13">
          <span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.13.1">
           0.74
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.5.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T1.5.5.5.2" rowspan="2">
          <span class="ltx_text" id="S5.T1.5.5.5.2.1">
           HOLMES
          </span>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.5.5.5.1">
          ChatGPT
          <sup class="ltx_sup" id="S5.T1.5.5.5.1.1">
           ‡
          </sup>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.3">
          0.75
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.4">
          0.68
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.5.5">
          0.66
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.6">
          <span class="ltx_text ltx_font_bold" id="S5.T1.5.5.5.6.1">
           0.88
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.7">
          0.52
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.5.8">
          0.50
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.9">
          0.78
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.10">
          0.40
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.5.11">
          0.36
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.12">
          0.80
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.13">
          0.53
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.5.5.5.14">
          0.51
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.6.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.6.6.6.1">
          GPT4
          <sup class="ltx_sup" id="S5.T1.6.6.6.1.1">
           ‡
          </sup>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.2">
          <span class="ltx_text ltx_font_bold" id="S5.T1.6.6.6.2.1">
           0.87
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.3">
          <span class="ltx_text ltx_font_bold" id="S5.T1.6.6.6.3.1">
           0.82
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.6.6.6.4">
          <span class="ltx_text ltx_font_bold" id="S5.T1.6.6.6.4.1">
           0.82
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.5">
          0.85
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.6">
          0.61
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.6.6.6.7">
          0.56
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.8">
          0.91
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.9">
          0.77
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.6.6.6.10">
          0.74
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.11">
          <span class="ltx_text ltx_font_bold" id="S5.T1.6.6.6.11.1">
           0.88
          </span>
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.12">
          0.73
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.6.6.6.13">
          0.71
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Main results on PCA-EVAL. Models with
     <math alttext="\dagger" class="ltx_Math" display="inline" id="S5.T1.9.m1.1">
      <semantics id="S5.T1.9.m1.1b">
       <mo id="S5.T1.9.m1.1.1" mathcolor="#FF0000" xref="S5.T1.9.m1.1.1.cmml">
        †
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.T1.9.m1.1c">
        <ci id="S5.T1.9.m1.1.1.cmml" xref="S5.T1.9.m1.1.1">
         †
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.T1.9.m1.1d">
        \dagger
       </annotation>
      </semantics>
     </math>
     <span class="ltx_text" id="S5.T1.10.1" style="color:#FF0000;">
      <span class="ltx_text" id="S5.T1.10.1.1" style="color:#000000;">
       are fully open-source. Models with
       <math alttext="\ddagger" class="ltx_Math" display="inline" id="S5.T1.10.1.1.m1.1">
        <semantics id="S5.T1.10.1.1.m1.1b">
         <mo id="S5.T1.10.1.1.m1.1.1" mathcolor="#000000" xref="S5.T1.10.1.1.m1.1.1.cmml">
          ‡
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.10.1.1.m1.1c">
          <ci id="S5.T1.10.1.1.m1.1.1.cmml" xref="S5.T1.10.1.1.m1.1.1">
           ‡
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.10.1.1.m1.1d">
          \ddagger
         </annotation>
        </semantics>
       </math>
       only provide API to access. P, C, and A represent Perception, Cognition, and Action Scores, respectively. For the open-source models in End2End setting, we find it hard to prompt them to output correct cross-modal reasoning information, so their Perception and Cognition scores are not reported.
      </span>
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     We evaluate various methods and models on the PCA-EVAL benchmark, as shown in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.3 Main Results ‣ 5 Experiments ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     In the upper block concerning End2End-VLLMs, the recently unveiled closed-source model, GPT-4V, outperforms existing open-source models by achieving the highest scores of 0.84, 0.74, and 0.74 in the perception, cognition, and action dimensions respectively. This performance represents a 26% action score improvement over its open-source counterpart, MMICL. The impressive performance of GPT-4V is primarily attributed to its exceptional ability to perceive visual information across different domains, particularly in the challenging game domain.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p3">
    <p class="ltx_p" id="S5.SS3.p3.1">
     We also assessed the performance of embodied decision making using our HOLMES system.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p4">
    <p class="ltx_p" id="S5.SS3.p4.1">
     As shown in the bottom block of the table, the HOLMES system, based on GPT4, achieves an Action Score of 0.71, matching the performance of GPT-4V (0.74). This suggests that the HOLMES system is proficient in understanding the task goal, breaking down the larger goal into multiple smaller steps, and accurately invoking the relevant APIs to accomplish each step.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p5">
    <p class="ltx_p" id="S5.SS3.p5.1">
     Specifically, the GPT4-HOLMES system can identify key concepts in an image through the results returned by APIs such as
     <span class="ltx_text ltx_font_italic" id="S5.SS3.p5.1.1">
      list_nearby_mobs_in_minecraft()
     </span>
     . As a result, the system achieves an average Perception Score of 0.88, surpassing GPT-4V’s 0.84. However, when compared to End2End methods, HOLMES relies on multi-step reasoning for the final decision. This approach can lead to the accumulation of reasoning errors, resulting in a lower Cognition Score in both Domestic and Game domains.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Discussion
  </h2>
  <section class="ltx_subsection" id="S6.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.1
    </span>
    Comparison Between End2End and HOLMES
   </h3>
   <div class="ltx_para ltx_noindent" id="S6.SS1.p1">
    <p class="ltx_p" id="S6.SS1.p1.1">
     We conduct an analysis and comparison of the outputs generated by the End2End method with GPT4-Vision, as well as the HOLMES method with GPT4. Our findings indicate that the End2End method effectively mitigates information loss during the modality conversion process. As illustrated in Figure
     <a class="ltx_ref" href="#S6.F6.sf1" title="In Figure 6 ‣ 6.1 Comparison Between End2End and HOLMES ‣ 6 Discussion ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       6a
      </span>
     </a>
     , an image depicts a road with several nearby cars. GPT4-Vision is capable of discerning that these cars are situated in a safe space, thereby suggesting that the driver can continue driving.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S6.SS1.p2">
    <p class="ltx_p" id="S6.SS1.p2.1">
     Conversely, GPT4, while aware of the number of cars, lacks information about their spatial relation, leading it to recommend slowing down. This suggests that the End2End method is superior in perceiving certain visual features that are not captured by the APIs. Conversely, some specialized APIs, such as traffic sign detection, outperform GPT4-Vision in tasks like traffic sign detection, as they are specifically trained for this task. This could enable the HOLMES method to gather more accurate information than the End2End model.
    </p>
   </div>
   <figure class="ltx_figure" id="S6.F6">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="232" id="S6.F6.sf1.g1" src="/html/2310.02071/assets/x5.png" width="200"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        A Comparison between GPT4-V and GPT4-HOLMES
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F6.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="232" id="S6.F6.sf2.g1" src="/html/2310.02071/assets/x6.png" width="212"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        An example of decision misalignment.
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Case studies.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S6.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.2
    </span>
    Alignment between Agent Decisions and Human Values
   </h3>
   <div class="ltx_para ltx_noindent" id="S6.SS2.p1">
    <p class="ltx_p" id="S6.SS2.p1.1">
     We have observed instances where the decisions made by the agent contradict human values. For instance, consider the scenario depicted in Figure
     <a class="ltx_ref" href="#S6.F6.sf2" title="In Figure 6 ‣ 6.1 Comparison Between End2End and HOLMES ‣ 6 Discussion ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       6b
      </span>
     </a>
     . The image illustrates a crosswalk devoid of pedestrians. The appropriate response in this situation would be to slow down, as caution is paramount when approaching a crosswalk, regardless of the presence or absence of pedestrians. However, upon processing the information that the crosswalk is unoccupied, ChatGPT suggests that maintaining the current speed is the optimal action, arguing that the absence of pedestrians eliminates the need to slow down. The rationale provided by ChatGPT is logical, yet it does not align with human values. We believe it is crucial for embodied agents to make decisions that are in harmony with human values, rather than solely focusing on maximizing their advantage.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S6.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     6.3
    </span>
    Limitation and Future Work
   </h3>
   <div class="ltx_para ltx_noindent" id="S6.SS3.p1">
    <p class="ltx_p" id="S6.SS3.p1.1">
     The current scope of PCA-EVAL is confined to merely three domains, with a cap of 100 instances per domain. One of our future work aims to broaden this scope to encompass more domains and embodied environments where MLLMs could keep getting feedback. Furthermore, we plan to increase the number of instances for both the existing and newly introduced domains.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para ltx_noindent" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    In this study, we present PCA-EVAL, a comprehensive evaluation benchmark for embodied decision-making that gauges performance in perception, cognition, and action, thereby offering an all-encompassing measure for various embodied agents. We conduct a systematic comparison between End2End embodied decision-making and HOLMES, a multi-agent cooperation framework developed by us. Our findings reveal that MLLM, when applied with the end2end method, surpasses the top-performing model in HOLMES, GPT-4, in terms of decision accuracy and cognition score. However, it is crucial to underscore that this superior performance is specific to the GPT4-Vision model, which significantly outperforms the open-source state-of-the-art VLLMs. These results and subsequent analysis underscore the necessity for ongoing exploration in embodied decision-making and the development of open-source MLLMs to ensure wider accessibility and progress in the field.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alayrac et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
    </span>
    <span class="ltx_bibblock">
     Flamingo: a visual language model for few-shot learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:23716–23736, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bai et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
    </span>
    <span class="ltx_bibblock">
     Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen.
    </span>
    <span class="ltx_bibblock">
     Asking before action: Gather information in embodied decision making with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2305.15695
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chiang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
    </span>
    <span class="ltx_bibblock">
     Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank" title="">
      https://lmsys.org/blog/2023-03-30-vicuna/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chowdhery et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.
    </span>
    <span class="ltx_bibblock">
     Palm: Scaling language modeling with pathways.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2204.02311
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dai et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
    </span>
    <span class="ltx_bibblock">
     Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Driess et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
    </span>
    <span class="ltx_bibblock">
     Palm-e: An embodied multimodal language model, 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Driess et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
    </span>
    <span class="ltx_bibblock">
     Palm-e: An embodied multimodal language model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2303.03378
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.
    </span>
    <span class="ltx_bibblock">
     Minedojo: Building open-ended embodied agents with internet-scale knowledge.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track
     </em>
     , 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rc8o_j8I8PX" target="_blank" title="">
      https://openreview.net/forum?id=rc8o_j8I8PX
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Goyal et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
    </span>
    <span class="ltx_bibblock">
     Making the V in VQA matter: Elevating the role of image understanding in visual question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017
     </em>
     , pp.  6325–6334, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li.
    </span>
    <span class="ltx_bibblock">
     Planning-oriented autonomous driving.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      International Conference on Machine Learning
     </em>
     , pp.  9118–9147. PMLR, 2022a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.
    </span>
    <span class="ltx_bibblock">
     Inner monologue: Embodied reasoning through planning with language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint arXiv:2207.05608
     </em>
     , 2022b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      Advances in neural information processing systems
     </em>
     , 35:22199–22213, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kolve et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.
    </span>
    <span class="ltx_bibblock">
     AI2-THOR: An Interactive 3D Environment for Visual AI.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      arXiv
     </em>
     , 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2301.12597
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu.
    </span>
    <span class="ltx_bibblock">
     M
     <sup class="ltx_sup" id="bib.bib18.2.1">
      3
     </sup>
     it: A large-scale dataset towards multi-modal multilingual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">
      arXiv preprint arXiv:2306.04387
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li.
    </span>
    <span class="ltx_bibblock">
     Api-bank: A benchmark for tool-augmented llms, 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al.
    </span>
    <span class="ltx_bibblock">
     Pre-trained language models for interactive decision-making.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:31199–31212, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou.
    </span>
    <span class="ltx_bibblock">
     Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      ArXiv
     </em>
     , abs/2203.02053, 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:247244904" target="_blank" title="">
      https://api.semanticscholar.org/CorpusID:247244904
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi.
    </span>
    <span class="ltx_bibblock">
     Training socially aligned language models in simulated human society.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2305.16960
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
    </span>
    <span class="ltx_bibblock">
     Chameleon: Plug-and-play compositional reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:2304.09842
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Marino et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
    </span>
    <span class="ltx_bibblock">
     OK-VQA: A visual question answering benchmark requiring external knowledge.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019
     </em>
     , pp.  3195–3204, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.
    </span>
    <span class="ltx_bibblock">
     Webgpt: Browser-assisted question-answering with human feedback, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2022)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" target="_blank" title="">
      https://chat.openai.com/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023a)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak" target="_blank" title="">
      https://openai.com/blog/chatgpt-can-now-see-hear-and-speak
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023b)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report, 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.
    </span>
    <span class="ltx_bibblock">
     Generative agents: Interactive simulacra of human behavior.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2304.03442
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al.
    </span>
    <span class="ltx_bibblock">
     Tool learning with foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2304.08354
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are unsupervised multitask learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      OpenAI blog
     </em>
     , 1(8):9, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Redmon &amp; Farhadi (2018)
    </span>
    <span class="ltx_bibblock">
     Joseph Redmon and Ali Farhadi.
    </span>
    <span class="ltx_bibblock">
     Yolov3: An incremental improvement.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      arXiv
     </em>
     , 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ren et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, and Xu Sun.
    </span>
    <span class="ltx_bibblock">
     Delving into the openness of CLIP.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      Findings of the Association for Computational Linguistics: ACL 2023
     </em>
     . Association for Computational Linguistics, July 2023a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-acl.610" target="_blank" title="">
      https://aclanthology.org/2023.findings-acl.610
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ren et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alex Smola, and Xu Sun.
    </span>
    <span class="ltx_bibblock">
     Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      arXiv preprint arXiv:2304.04704
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shridhar et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox.
    </span>
    <span class="ltx_bibblock">
     ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     , 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1912.01734" target="_blank" title="">
      https://arxiv.org/abs/1912.01734
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, and Sujian Li.
    </span>
    <span class="ltx_bibblock">
     Restgpt: Connecting large language models with real-world restful apis, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Suhr et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
    </span>
    <span class="ltx_bibblock">
     A corpus of natural language for visual reasoning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)
     </em>
     , pp.  217–223, 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
    </span>
    <span class="ltx_bibblock">
     Voyager: An open-ended embodied agent with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv preprint arXiv:2305.16291
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui.
    </span>
    <span class="ltx_bibblock">
     Making large language models better reasoners with alignment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2309.02144
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.
    </span>
    <span class="ltx_bibblock">
     Large language models are not fair evaluators.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      arXiv preprint arXiv:2305.17926
     </em>
     , 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023d)
    </span>
    <span class="ltx_bibblock">
     Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.
    </span>
    <span class="ltx_bibblock">
     Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      ArXiv
     </em>
     , abs/2302.01560, 2023d.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:256598146" target="_blank" title="">
      https://api.semanticscholar.org/CorpusID:256598146
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wayve (2023)
    </span>
    <span class="ltx_bibblock">
     Wayve.
    </span>
    <span class="ltx_bibblock">
     Lingo.
    </span>
    <span class="ltx_bibblock">
     2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/" target="_blank" title="">
      https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
    </span>
    <span class="ltx_bibblock">
     Emergent abilities of large language models, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
    </span>
    <span class="ltx_bibblock">
     Visual chatgpt: Talking, drawing and editing with visual foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      ArXiv
     </em>
     , abs/2303.04671, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257404891" target="_blank" title="">
      https://api.semanticscholar.org/CorpusID:257404891
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.
    </span>
    <span class="ltx_bibblock">
     The rise and potential of large language model based agents: A survey, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     Mm-react: Prompting chatgpt for multimodal reasoning and action.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      ArXiv
     </em>
     , abs/2303.11381, 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257637012" target="_blank" title="">
      https://api.semanticscholar.org/CorpusID:257637012
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      The Eleventh International Conference on Learning Representations
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al.
    </span>
    <span class="ltx_bibblock">
     Retroformer: Retrospective large language agents with policy gradient optimization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      arXiv preprint arXiv:2308.02151
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yuan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu.
    </span>
    <span class="ltx_bibblock">
     Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      arXiv preprint arXiv:2303.16563
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang.
    </span>
    <span class="ltx_bibblock">
     Mmicl: Empowering vision-language model with multi-modal in-context learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      arXiv preprint arXiv:2309.07915
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
    </span>
    <span class="ltx_bibblock">
     Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      arXiv preprint arXiv:2304.10592
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai.
    </span>
    <span class="ltx_bibblock">
     Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      arXiv preprint arXiv:2305.17144
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Zhe Zhu, Dun Liang, Songhai Zhang, Xiaolei Huang, Baoli Li, and Shimin Hu.
    </span>
    <span class="ltx_bibblock">
     Traffic-sign detection and classification in the wild.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">
      The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
     </em>
     , 2016.
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Examples of PCA-EVAL
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Data Distribution
   </h3>
   <div class="ltx_para ltx_noindent" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     The PCA-EVAL benchmark data distribution across various domains is outlined in Table
     <a class="ltx_ref" href="#A1.T2" title="Table 2 ‣ A.1 Data Distribution ‣ Appendix A Examples of PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A1.SS1.p2">
    <p class="ltx_p" id="A1.SS1.p2.1">
     For the Autonomous Driving domain, instances are grouped by their respective task types. In the Domestic Robot domain, instances are grouped by their locations. In the Open-World Game domain, instances are grouped by the tasks they aim to accomplish.
    </p>
   </div>
   <figure class="ltx_table" id="A1.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Data Distribution in the PCA-EVAL Benchmark
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T2.1">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="A1.T2.1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T2.1.1.1.1">
        <span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.1.1">
         Domain
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T2.1.1.1.2">
        <span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.2.1">
         Task Type/Location
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T2.1.1.1.3">
        <span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.3.1">
         Instances
        </span>
       </th>
       <td class="ltx_td ltx_border_tt" id="A1.T2.1.1.1.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.2.2">
       <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.2.2.1">
        Autonomous Driving
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.2.2.2">
        Traffic Sign Detection
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.2.2.3">
        44
       </td>
       <td class="ltx_td ltx_border_t" id="A1.T2.1.2.2.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.3.3">
       <td class="ltx_td" id="A1.T2.1.3.3.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.3.3.2">
        Car Detection
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.3.3.3">
        33
       </td>
       <td class="ltx_td" id="A1.T2.1.3.3.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.4.4">
       <td class="ltx_td" id="A1.T2.1.4.4.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.4.4.2">
        Human Detection
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.4.4.3">
        30
       </td>
       <td class="ltx_td" id="A1.T2.1.4.4.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.5.5">
       <td class="ltx_td" id="A1.T2.1.5.5.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.5.5.2">
        Weather Detection
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.5.5.3">
        9
       </td>
       <td class="ltx_td" id="A1.T2.1.5.5.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.6.6">
       <td class="ltx_td" id="A1.T2.1.6.6.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.6.6.2">
        Road Detection
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.6.6.3">
        3
       </td>
       <td class="ltx_td" id="A1.T2.1.6.6.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.7.7">
       <td class="ltx_td" id="A1.T2.1.7.7.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.7.7.2">
        Character Recognition
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.7.7.3">
        13
       </td>
       <td class="ltx_td" id="A1.T2.1.7.7.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.8.8">
       <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.8.8.1">
        Domestic Robot
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.8.8.2">
        Living Room
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.8.8.3">
        31
       </td>
       <td class="ltx_td ltx_border_t" id="A1.T2.1.8.8.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.9.9">
       <td class="ltx_td" id="A1.T2.1.9.9.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.9.9.2">
        Dining Room
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.9.9.3">
        11
       </td>
       <td class="ltx_td" id="A1.T2.1.9.9.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.10.10">
       <td class="ltx_td" id="A1.T2.1.10.10.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.10.10.2">
        Bedroom
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.10.10.3">
        6
       </td>
       <td class="ltx_td" id="A1.T2.1.10.10.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.11.11">
       <td class="ltx_td" id="A1.T2.1.11.11.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.11.11.2">
        Bathroom
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.11.11.3">
        3
       </td>
       <td class="ltx_td" id="A1.T2.1.11.11.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.12.12">
       <td class="ltx_td" id="A1.T2.1.12.12.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.12.12.2">
        Kitchen
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.12.12.3">
        37
       </td>
       <td class="ltx_td" id="A1.T2.1.12.12.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.13.13">
       <td class="ltx_td" id="A1.T2.1.13.13.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.13.13.2">
        Corridor
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.13.13.3">
        12
       </td>
       <td class="ltx_td" id="A1.T2.1.13.13.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.14.14">
       <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T2.1.14.14.1">
        Open-World Game
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.14.14.2">
        Find Objects
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.14.14.3">
        52
       </td>
       <td class="ltx_td ltx_border_t" id="A1.T2.1.14.14.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.15.15">
       <td class="ltx_td" id="A1.T2.1.15.15.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.15.15.2">
        Kill Enemies
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.15.15.3">
        6
       </td>
       <td class="ltx_td" id="A1.T2.1.15.15.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.16.16">
       <td class="ltx_td" id="A1.T2.1.16.16.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.16.16.2">
        Craft Items
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.16.16.3">
        32
       </td>
       <td class="ltx_td" id="A1.T2.1.16.16.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.17.17">
       <td class="ltx_td" id="A1.T2.1.17.17.1">
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.17.17.2">
        Place Blocks
       </td>
       <td class="ltx_td ltx_align_center" id="A1.T2.1.17.17.3">
        7
       </td>
       <td class="ltx_td" id="A1.T2.1.17.17.4">
       </td>
      </tr>
      <tr class="ltx_tr" id="A1.T2.1.18.18">
       <td class="ltx_td ltx_border_bb" id="A1.T2.1.18.18.1">
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.18.18.2">
        Interact with Creatures
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.18.18.3">
        3
       </td>
       <td class="ltx_td ltx_border_bb" id="A1.T2.1.18.18.4">
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    PCA-EVAL Examples
   </h3>
   <div class="ltx_para ltx_noindent" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     We list three examples of each domain from PCA-EVAL, as shown in Figure
     <a class="ltx_ref" href="#A1.F7" title="Figure 7 ‣ A.2 PCA-EVAL Examples ‣ Appendix A Examples of PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     , Figure
     <a class="ltx_ref" href="#A1.F8" title="Figure 8 ‣ A.2 PCA-EVAL Examples ‣ Appendix A Examples of PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     and Figure
     <a class="ltx_ref" href="#A1.F9" title="Figure 9 ‣ A.2 PCA-EVAL Examples ‣ Appendix A Examples of PCA-EVAL ‣ Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F7">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="A1.F7.g1" src="/html/2310.02071/assets/x7.png" width="380"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     Three examples of PCA-EVAL in the autonomous driving domain.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F8">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="A1.F8.g1" src="/html/2310.02071/assets/x8.png" width="380"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 8:
     </span>
     Three examples of PCA-EVAL in the domestic robot domain.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F9">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="A1.F9.g1" src="/html/2310.02071/assets/x9.png" width="351"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 9:
     </span>
     Three examples of PCA-EVAL in the open-world game domain.
    </figcaption>
   </figure>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   API Description and Implementation of HOLMES
  </h2>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Traffic Domain.
   </h4>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">
     Below is the API description for the traffic domain.
    </p>
    <div class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_listing" id="A2.SS0.SSS0.Px1.p1.2" style="background-color:#F2F2EB;">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,IyBBUEkgRGVzY3JpcHRpb24gZm9yIFRyYWZmaWMgRG9tYWluOgpkZWYgZGV0ZWN0X3RyYWZmaWNfc2lnbigpOgogICAgIiIiCiAgICBEZXRlY3RzIHRyYWZmaWMgc2lnbnMgaW4gdGhlIGltYWdlLgogICAgOnJldHVybjogbGlzdCBvZiBkZXRlY3RlZCB0cmFmZmljIHNpZ25zIGFuZCBjb29yZGluYXRlcywgZS5nLiBbJ3N0b3AnLCdtYXggc3BlZWQgbGltaXQnXQogICAgIiIiCiAgICBwYXNzCgpkZWYgb2JqZWN0X2RldGVjdGlvbigpOgogICAgIiIiCiAgICBEZXRlY3RzIG9iamVjdHMgaW4gdGhlIGltYWdlLgogICAgOnJldHVybjogZGljdCBvZiBkZXRlY3RlZCBvYmplY3RzIGFuZCBudW1iZXIgb2YgdGhlIG9iamVjdHMsIGUuZy4geydjYXInOjEwLCAncGVyc29uJzoxfQogICAgIiIiCiAgICBwYXNzCgpkZWYgb2NyKCk6CiAgICAiIiIKICAgIFBlcmZvcm1zIE9DUiBvbiB0aGUgaW1hZ2UuCiAgICA6cmV0dXJuOiBsaXN0IG9mIGRldGVjdGVkIHRleHQsIGUuZy4gWydDaGFuZ2ppYW5nIHJvYWQnLCAnUmlnaHQgbGFuZSBjbG9zdXJlJ10KICAgICIiIgogICAgcGFzcwoKZGVmIGltYWdlX2NhcHRpb24oKToKICAgICIiIgogICAgR2VuZXJhdGVzIGEgY2FwdGlvbiBmb3IgdGhlIGltYWdlLgogICAgOnJldHVybjogY2FwdGlvbiwgZS5nLiAnQSByZWQgY2FyIGRyaXZpbmcgZG93biB0aGUgc3RyZWV0JwogICAgIiIiCiAgICBwYXNzCgpkZWYgd2VhdGhlcl9kZXRlY3Rpb24oKToKICAgICIiIgogICAgRGV0ZWN0IGN1cnJlbnQgd2VhdGhlci4KICAgIDpyZXR1cm46IHdlYXRoZXIsIGUuZy4gJ3JhaW55JyBvciAnY2xlYXInCiAgICAiIiIKICAgIHBhc3M=">
       ⬇
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx1">
      <span class="ltx_tag ltx_tag_listingline">
       1
      </span>
      <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx1.1" style="color:#009900;">
       #
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.1">
       </span>
       API
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.2">
       </span>
       Description
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.3">
       </span>
       for
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.4">
       </span>
       Traffic
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.5">
       </span>
       Domain:
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx2">
      <span class="ltx_tag ltx_tag_listingline">
       2
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx2.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3">
       detect_traffic_sign
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx2.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx3">
      <span class="ltx_tag ltx_tag_listingline">
       3
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx3.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx4">
      <span class="ltx_tag ltx_tag_listingline">
       4
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.2" style="color:#9400D1;">
       Detects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.4" style="color:#9400D1;">
       traffic
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.6" style="color:#9400D1;">
       signs
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.8" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.10" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx4.12" style="color:#9400D1;">
       image.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx5">
      <span class="ltx_tag ltx_tag_listingline">
       5
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.8" style="color:#9400D1;">
       detected
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.10" style="color:#9400D1;">
       traffic
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.12" style="color:#9400D1;">
       signs
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.14" style="color:#9400D1;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.16" style="color:#9400D1;">
       coordinates,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.18" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.20" style="color:#9400D1;">
       [’stop’,’max
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.22" style="color:#9400D1;">
       speed
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.23" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx5.24" style="color:#9400D1;">
       limit’]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx6">
      <span class="ltx_tag ltx_tag_listingline">
       6
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx6.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx6.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx7">
      <span class="ltx_tag ltx_tag_listingline">
       7
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx7.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx8">
      <span class="ltx_tag ltx_tag_listingline">
       8
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx9">
      <span class="ltx_tag ltx_tag_listingline">
       9
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.3">
       object_detection
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx10">
      <span class="ltx_tag ltx_tag_listingline">
       10
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx10.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx10.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx11">
      <span class="ltx_tag ltx_tag_listingline">
       11
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.2" style="color:#9400D1;">
       Detects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.4" style="color:#9400D1;">
       objects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.6" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.8" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10" style="color:#9400D1;">
       image.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx12">
      <span class="ltx_tag ltx_tag_listingline">
       12
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4" style="color:#9400D1;">
       dict
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.8" style="color:#9400D1;">
       detected
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.10" style="color:#9400D1;">
       objects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.12" style="color:#9400D1;">
       and
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.14" style="color:#9400D1;">
       number
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.16" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.18" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.20" style="color:#9400D1;">
       objects,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.22" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.23" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.24" style="color:#9400D1;">
       {’car’:10,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.25" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx12.26" style="color:#9400D1;">
       ’person’:1}
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx13">
      <span class="ltx_tag ltx_tag_listingline">
       13
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx13.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx13.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx14">
      <span class="ltx_tag ltx_tag_listingline">
       14
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx14.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx15">
      <span class="ltx_tag ltx_tag_listingline">
       15
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx16">
      <span class="ltx_tag ltx_tag_listingline">
       16
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx16.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3">
       ocr
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx16.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx17">
      <span class="ltx_tag ltx_tag_listingline">
       17
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx17.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx17.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx18">
      <span class="ltx_tag ltx_tag_listingline">
       18
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.2" style="color:#9400D1;">
       Performs
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4" style="color:#9400D1;">
       OCR
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.6" style="color:#9400D1;">
       on
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.8" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx18.10" style="color:#9400D1;">
       image.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx19">
      <span class="ltx_tag ltx_tag_listingline">
       19
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.8" style="color:#9400D1;">
       detected
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.10" style="color:#9400D1;">
       text,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.12" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.14" style="color:#9400D1;">
       [’Changjiang
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.16" style="color:#9400D1;">
       road’,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.18" style="color:#9400D1;">
       ’Right
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.20" style="color:#9400D1;">
       lane
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx19.22" style="color:#9400D1;">
       closure’]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx20">
      <span class="ltx_tag ltx_tag_listingline">
       20
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx20.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx20.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx20.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx21">
      <span class="ltx_tag ltx_tag_listingline">
       21
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx21.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx22">
      <span class="ltx_tag ltx_tag_listingline">
       22
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx23">
      <span class="ltx_tag ltx_tag_listingline">
       23
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx23.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.3">
       image_caption
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx23.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx24">
      <span class="ltx_tag ltx_tag_listingline">
       24
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx24.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx24.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx25">
      <span class="ltx_tag ltx_tag_listingline">
       25
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.2" style="color:#9400D1;">
       Generates
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.4" style="color:#9400D1;">
       a
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.6" style="color:#9400D1;">
       caption
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.8" style="color:#9400D1;">
       for
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.10" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx25.12" style="color:#9400D1;">
       image.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx26">
      <span class="ltx_tag ltx_tag_listingline">
       26
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.4" style="color:#9400D1;">
       caption,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.6" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.8" style="color:#9400D1;">
       ’A
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.10" style="color:#9400D1;">
       red
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.12" style="color:#9400D1;">
       car
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.14" style="color:#9400D1;">
       driving
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.16" style="color:#9400D1;">
       down
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.18" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx26.20" style="color:#9400D1;">
       street’
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx27">
      <span class="ltx_tag ltx_tag_listingline">
       27
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx27.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx27.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx27.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx28">
      <span class="ltx_tag ltx_tag_listingline">
       28
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx28.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx29">
      <span class="ltx_tag ltx_tag_listingline">
       29
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx30">
      <span class="ltx_tag ltx_tag_listingline">
       30
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx30.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.3">
       weather_detection
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx30.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx31">
      <span class="ltx_tag ltx_tag_listingline">
       31
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx31.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx31.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx32">
      <span class="ltx_tag ltx_tag_listingline">
       32
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx32.2" style="color:#9400D1;">
       Detect
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx32.4" style="color:#9400D1;">
       current
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx32.6" style="color:#9400D1;">
       weather.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx33">
      <span class="ltx_tag ltx_tag_listingline">
       33
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.4" style="color:#9400D1;">
       weather,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.6" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.8" style="color:#9400D1;">
       ’rainy’
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.10" style="color:#9400D1;">
       or
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx33.12" style="color:#9400D1;">
       ’clear’
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx34">
      <span class="ltx_tag ltx_tag_listingline">
       34
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx34.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx34.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx34.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx35">
      <span class="ltx_tag ltx_tag_listingline">
       35
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx35.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
    </div>
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.3">
     •
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p1.3.1">
      detect_traffic_sign()
     </span>
     : The detection of road traffic signs model utilize YOLO
     <cite class="ltx_cite ltx_citemacro_citep">
      (Redmon &amp; Farhadi,
      <a class="ltx_ref" href="#bib.bib33" title="">
       2018
      </a>
      )
     </cite>
     which trained on the Tsinghua-Tencent 100K dataset
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zhu et al.,
      <a class="ltx_ref" href="#bib.bib58" title="">
       2016
      </a>
      )
     </cite>
     . TT100K comprises 100,000 images encompassing 30,000 instances of traffic signs. The end-to-end YOLO enables simultaneous detection and classification of traffic signs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p2">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">
     •
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p2.1.1">
      object_detection()
     </span>
     : Objects demanding attention during vehicle operation primarily encompass cars, pedestrians, and bicycles. A surfeit of vehicles can lead to traffic congestion, while the presence of pedestrians or bicycles ahead necessitates cars to decelerate and proceed cautiously. Hence, the
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p2.1.2">
      object_detection()
     </span>
     API predominantly identifies three key object categories: cars, pedestrians, and bicycles. We utilize PMOP
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ren et al.,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023b
      </a>
      )
     </cite>
     , a model trained on vision-language models through the prompt pre-training method, which enables the detection and counting of the three mentioned objectives by modifying specific class names.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p3">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p3.1">
     •
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p3.1.1">
      ocr()
     </span>
     : We employ PaddleOCR
     <span class="ltx_note ltx_role_footnote" id="footnote9">
      <sup class="ltx_note_mark">
       9
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         9
        </sup>
        <span class="ltx_tag ltx_tag_note">
         9
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.7" target="_blank" title="">
         https://github.com/PaddlePaddle/PaddleOCR/tree/release/2.7
        </a>
       </span>
      </span>
     </span>
     to extract textual information from images, providing crucial road data for real-time navigation.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p4">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p4.1">
     •
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p4.1.1">
      image_caption()
     </span>
     : To initially streamline the road information within the image, we employ the BLIP2-flan-t5-xl
     <span class="ltx_note ltx_role_footnote" id="footnote10">
      <sup class="ltx_note_mark">
       10
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         10
        </sup>
        <span class="ltx_tag ltx_tag_note">
         10
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Salesforce/blip2-flan-t5-xl" target="_blank" title="">
         https://huggingface.co/Salesforce/blip2-flan-t5-xl
        </a>
       </span>
      </span>
     </span>
     to generate an initial caption for the picture. This caption, derived from basic image data, is then utilized as input for the model to facilitate decision-making.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px1.p5">
    <p class="ltx_p" id="A2.SS0.SSS0.Px1.p5.1">
     •
     <span class="ltx_text ltx_font_italic" id="A2.SS0.SSS0.Px1.p5.1.1">
      weather_detection()
     </span>
     : Weather detection leverages a pre-trained ResNet50 model
     <span class="ltx_note ltx_role_footnote" id="footnote11">
      <sup class="ltx_note_mark">
       11
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         11
        </sup>
        <span class="ltx_tag ltx_tag_note">
         11
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mengxianglong123/weather-recognition" target="_blank" title="">
         https://github.com/mengxianglong123/weather-recognition
        </a>
       </span>
      </span>
     </span>
     , derived from a dataset of more than 70,000 weather records. This model extracts weather information from provided images to inform decision-making.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Domestic Robot Domain.
   </h4>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">
     Below is the API description for the Domestic Robot domain.
    </p>
    <div class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_listing" id="A2.SS0.SSS0.Px2.p1.2" style="background-color:#F2F2EB;">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,I0FQSSBEZXNjcmlwdGlvbiBmb3IgRG9tZXN0aWMgUm9ib3QgRG9tYWluCmRlZiBvYmplY3RcX2RldGVjdGlvbigpOgogICAgIiIiCiAgICBEZXRlY3RzIG9iamVjdHMgaW4gY3VycmVudCB2aWV3LCB3aGljaCB5b3UgZG9uJ3QgbmVlZCBkbyBmaW5kLgogICAgOnJldHVybjogbGlzdCBvZiBkZXRlY3RlZCBvYmplY3RzLCBlLmcuIFsnY2hhaXInLCd0YWJsZSddCiAgICAiIiIKICAgIHBhc3MKCmRlZiBsaXN0X2l0ZW1zX2luX2hhbmRzKCk6CiAgICAiIiIKICAgIExpc3RzIGl0ZW1zIGluIHlvdXIgaGFuZCwgd2hpY2ggeW91IGRvbid0IG5lZWQgdG8gcGljayB1cAogICAgOnJldHVybjogbGlzdCBvZiBpdGVtcyBpbiBoYW5kLCBlLmcuIFsnY29mZmVlIGN1cCcsJ21pbGsnXQogICAgIiIiCiAgICBwYXNz">
       ⬇
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx36">
      <span class="ltx_tag ltx_tag_listingline">
       1
      </span>
      <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx36.1" style="color:#009900;">
       #API
       <span class="ltx_text ltx_lst_space" id="lstnumberx36.1.1">
       </span>
       Description
       <span class="ltx_text ltx_lst_space" id="lstnumberx36.1.2">
       </span>
       for
       <span class="ltx_text ltx_lst_space" id="lstnumberx36.1.3">
       </span>
       Domestic
       <span class="ltx_text ltx_lst_space" id="lstnumberx36.1.4">
       </span>
       Robot
       <span class="ltx_text ltx_lst_space" id="lstnumberx36.1.5">
       </span>
       Domain
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx37">
      <span class="ltx_tag ltx_tag_listingline">
       2
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx37.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.2">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx37.3" style="color:#FF00FF;">
       object
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx37.4">
       \
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.5">
       _detection
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx37.6">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx38">
      <span class="ltx_tag ltx_tag_listingline">
       3
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx38.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx38.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx38.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx39">
      <span class="ltx_tag ltx_tag_listingline">
       4
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.2" style="color:#9400D1;">
       Detects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.4" style="color:#9400D1;">
       objects
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.6" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.8" style="color:#9400D1;">
       current
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.10" style="color:#9400D1;">
       view,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.12" style="color:#9400D1;">
       which
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.14" style="color:#9400D1;">
       you
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.16" style="color:#9400D1;">
       don’t
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.18" style="color:#9400D1;">
       need
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.20" style="color:#9400D1;">
       do
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx39.22" style="color:#9400D1;">
       find.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx40">
      <span class="ltx_tag ltx_tag_listingline">
       5
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.8" style="color:#9400D1;">
       detected
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.10" style="color:#9400D1;">
       objects,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.12" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx40.14" style="color:#9400D1;">
       [’chair’,’table’]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx41">
      <span class="ltx_tag ltx_tag_listingline">
       6
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx41.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx41.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx41.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx42">
      <span class="ltx_tag ltx_tag_listingline">
       7
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx42.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx43">
      <span class="ltx_tag ltx_tag_listingline">
       8
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx44">
      <span class="ltx_tag ltx_tag_listingline">
       9
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx44.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.3">
       list_items_in_hands
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx44.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx45">
      <span class="ltx_tag ltx_tag_listingline">
       10
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx45.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx45.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx46">
      <span class="ltx_tag ltx_tag_listingline">
       11
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.2" style="color:#9400D1;">
       Lists
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.4" style="color:#9400D1;">
       items
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.6" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.8" style="color:#9400D1;">
       your
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.10" style="color:#9400D1;">
       hand,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.12" style="color:#9400D1;">
       which
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.14" style="color:#9400D1;">
       you
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.16" style="color:#9400D1;">
       don’t
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.18" style="color:#9400D1;">
       need
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.20" style="color:#9400D1;">
       to
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.22" style="color:#9400D1;">
       pick
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.23" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx46.24" style="color:#9400D1;">
       up
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx47">
      <span class="ltx_tag ltx_tag_listingline">
       12
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.8" style="color:#9400D1;">
       items
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.10" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.12" style="color:#9400D1;">
       hand,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.14" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.16" style="color:#9400D1;">
       [’coffee
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx47.18" style="color:#9400D1;">
       cup’,’milk’]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx48">
      <span class="ltx_tag ltx_tag_listingline">
       13
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx48.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx48.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx48.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx48.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx49">
      <span class="ltx_tag ltx_tag_listingline">
       14
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx49.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx49.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
    </div>
   </div>
  </section>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Game Domain.
   </h4>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1">
     Below is the API description for the Game domain (Minedojo).
    </p>
    <div class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_listing" id="A2.SS0.SSS0.Px3.p1.2" style="background-color:#F2F2EB;">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,I0FQSSBEZXNjcmlwdGlvbiBmb3IgR2FtZSBEb21haW4KZGVmIGxpc3RfbmVhcmJ5X21vYnNfaW5fbWluZWNyYWZ0KCk6CiAgICAiIiIKICAgIExpc3RzIG5lYXJieSBtb2JzIGluIE1pbmVjcmFmdC4KICAgIDpyZXR1cm46IGxpc3Qgb2YgbmVhcmJ5IG1vYnMsIGUuZy4gWydjcmVlcGVyJywgJ3BpZyddCiAgICAiIiIKICAgIHBhc3MKCmRlZiBsaXN0X2ludmVudG9yeV9pbmZvcm1hdGlvbigpOgogICAgIiIiCiAgICBMaXN0cyBpbnZlbnRvcnkgaW5mb3JtYXRpb24gb2YgdGhlIHBsYXllciBpbiBNaW5lY3JhZnQuCiAgICA6cmV0dXJuOiBsaXN0IG9mIGludmVudG9yeSBpbmZvcm1hdGlvbiB3aXRoIG51bWJlciwgZS5nLiBbKCdkaWFtb25kJywgNjQpLCAoJ2lyb24nLCAzMildCiAgICAiIiIKICAgIHBhc3M=">
       ⬇
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx50">
      <span class="ltx_tag ltx_tag_listingline">
       1
      </span>
      <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx50.1" style="color:#009900;">
       #API
       <span class="ltx_text ltx_lst_space" id="lstnumberx50.1.1">
       </span>
       Description
       <span class="ltx_text ltx_lst_space" id="lstnumberx50.1.2">
       </span>
       for
       <span class="ltx_text ltx_lst_space" id="lstnumberx50.1.3">
       </span>
       Game
       <span class="ltx_text ltx_lst_space" id="lstnumberx50.1.4">
       </span>
       Domain
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx51">
      <span class="ltx_tag ltx_tag_listingline">
       2
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx51.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx51.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx51.3">
       list_nearby_mobs_in_minecraft
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx51.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx52">
      <span class="ltx_tag ltx_tag_listingline">
       3
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx52.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx52.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx52.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx53">
      <span class="ltx_tag ltx_tag_listingline">
       4
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx53.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx53.2" style="color:#9400D1;">
       Lists
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx53.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx53.4" style="color:#9400D1;">
       nearby
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx53.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx53.6" style="color:#9400D1;">
       mobs
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx53.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx53.8" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx53.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx53.10" style="color:#9400D1;">
       Minecraft.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx54">
      <span class="ltx_tag ltx_tag_listingline">
       5
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.8" style="color:#9400D1;">
       nearby
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.10" style="color:#9400D1;">
       mobs,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.12" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.14" style="color:#9400D1;">
       [’creeper’,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx54.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx54.16" style="color:#9400D1;">
       ’pig’]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx55">
      <span class="ltx_tag ltx_tag_listingline">
       6
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx55.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx55.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx55.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx55.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx56">
      <span class="ltx_tag ltx_tag_listingline">
       7
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx56.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx56.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx57">
      <span class="ltx_tag ltx_tag_listingline">
       8
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx58">
      <span class="ltx_tag ltx_tag_listingline">
       9
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx58.1" style="color:#FF00FF;">
       def
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx58.2">
      </span>
      <span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx58.3">
       list_inventory_information
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx58.4">
       ():
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx59">
      <span class="ltx_tag ltx_tag_listingline">
       10
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx59.1">
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx59.2" style="color:#9400D1;">
       ""
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx59.3" style="color:#9400D1;">
       "
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx60">
      <span class="ltx_tag ltx_tag_listingline">
       11
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.2" style="color:#9400D1;">
       Lists
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.4" style="color:#9400D1;">
       inventory
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.6" style="color:#9400D1;">
       information
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.8" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.10" style="color:#9400D1;">
       the
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.12" style="color:#9400D1;">
       player
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.14" style="color:#9400D1;">
       in
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx60.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx60.16" style="color:#9400D1;">
       Minecraft.
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx61">
      <span class="ltx_tag ltx_tag_listingline">
       12
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.2" style="color:#9400D1;">
       :return:
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.3" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.4" style="color:#9400D1;">
       list
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.5" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.6" style="color:#9400D1;">
       of
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.7" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.8" style="color:#9400D1;">
       inventory
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.9" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.10" style="color:#9400D1;">
       information
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.11" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.12" style="color:#9400D1;">
       with
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.13" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.14" style="color:#9400D1;">
       number,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.15" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.16" style="color:#9400D1;">
       e.g.
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.17" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.18" style="color:#9400D1;">
       [(’diamond’,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.19" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.20" style="color:#9400D1;">
       64),
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.21" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.22" style="color:#9400D1;">
       (’iron’,
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx61.23" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx61.24" style="color:#9400D1;">
       32)]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx62">
      <span class="ltx_tag ltx_tag_listingline">
       13
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx62.1" style="color:#9400D1;">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx62.2" style="color:#9400D1;">
       "
      </span>
      <span class="ltx_text ltx_lst_string" id="lstnumberx62.3">
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx62.4" style="color:#9400D1;">
       ""
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx63">
      <span class="ltx_tag ltx_tag_listingline">
       14
      </span>
      <span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx63.1">
      </span>
      <span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx63.2" style="color:#FF00FF;">
       pass
      </span>
     </div>
    </div>
   </div>
   <div class="ltx_para ltx_noindent" id="A2.SS0.SSS0.Px3.p2">
    <p class="ltx_p" id="A2.SS0.SSS0.Px3.p2.1">
     Note that within the Domestic Robot Domain and Game Domain, APIs can be directly accessed within the virtual environment, allowing for the perception of the surrounding objects and the current picture context.
    </p>
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Automatic Evaluation
  </h2>
  <figure class="ltx_table" id="A3.T3">
   <svg class="ltx_picture" height="406.69" id="A3.T3.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,406.69) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 400.79 C 0 404.05 2.64 406.69 5.91 406.69 L 594.09 406.69 C 597.36 406.69 600 404.05 600 400.79 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 400.79 C 1.97 402.96 3.73 404.72 5.91 404.72 L 594.09 404.72 C 596.27 404.72 598.03 402.96 598.03 400.79 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="379.13" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.T3.pic1.1.1.1.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.1">
         [Question]: {question}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.2">
         [Action Choices]: {actions}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.3">
         [Agent Answer]: {model_output}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.4">
         [Correct Action]: {true_action}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.5">
         [Key Concepts]: {key_concept}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.6">
         [Reference Reasoning Process]: {reason}
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.7">
         [System]
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.8">
         We would like you to access the agent’s performance in the multimodal reasoning task about domain.
In this task, the agent is given an image, a [Question], and several candidate [Action Choices], and is asked to give an [Agent Answer] for the [Question].
The [Agent Answer] encapsulates the agent’s perception of the image’s [Key Concepts], the agent’s cognition reasoning process and the final selected action.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.9">
         We request you to give three types of scores for the agent’s [Agent Answer] in comparison to the given [Key Concepts], [Reference Reasoning Process] and [Correct Action]:
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.10">
         1. action score: If the selected action in the [Agent Answer] matches that of the [Correct Action], the action score is 1; otherwise, it is 0.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.11">
         2. perception score: This score evaluates the model’s capability to perceive and interpret observations. It is contingent on whether the [Agent Answer] includes any of the [Key Concepts] of the instance. If it accurately describes any one of the [Key Concepts], the score is 1; otherwise, it is 0.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.12">
         3. cognition score: This score gauges the model’s ability to reason, comprehend, and make informed decisions based on perceived input data and world knowledge. If the reasoning process in the [Agent Answer] aligns with the [Reference Reasoning Process], the score is 1; otherwise, it is 0.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.13">
         Please note that there are only scores of 0 and 1.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.14">
         You should carefully compare the [Agent Answer] with the [Correct Action], [Key Concepts] and [Reference Reasoning Process] to give your assessment.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.15">
         You need first to give your assessment evidence and then the scores.
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.16">
         Your output MUST contain 6 lines with the following format:
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.17">
         action assessment evidence: (assessment evidence here)
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.18">
         action score: (score here)
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.19">
         perception assessment evidence: (assessment evidence here)
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.20">
         perception score: (score here)
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.21">
         cognition assessment evidence: (assessment evidence here)
        </span>
        <span class="ltx_p" id="A3.T3.pic1.1.1.1.1.1.22">
         cognition score: (score here)
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_table">
     Table 3:
    </span>
    We utilize the template to query GPT-4, aiming to evaluate its responses and assign scores for perception, cognition, and action. By feeding both the agent’s output and the ground truth answer to GPT-4, based on this template, we can then extract the three distinct scores from the conclusion of GPT-4’s response.
   </figcaption>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
