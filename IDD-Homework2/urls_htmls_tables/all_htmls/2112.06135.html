<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.06135] Communication-Efficient Federated Learning for Neural Machine Translation</title><meta property="og:description" content="Training neural machine translation (NMT) models in federated learning (FL) settings could be inefficient both computationally and communication-wise, due to the large size of translation engines as well as the multipl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Communication-Efficient Federated Learning for Neural Machine Translation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Communication-Efficient Federated Learning for Neural Machine Translation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.06135">

<!--Generated on Fri Mar  1 16:44:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Communication-Efficient Federated Learning for Neural Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tanya Roosta           Peyman Passban<sup id="id3.2.id1" class="ltx_sup">∗</sup>           Ankit Chadha
<br class="ltx_break">Amazon
<br class="ltx_break"><span id="id4.3.id2" class="ltx_text ltx_font_typewriter">{troosta, peymp, ankitrc}@amazon.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal Contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Training neural machine translation (NMT) models in federated learning (FL) settings could be inefficient both computationally and communication-wise, due to the large size of translation engines as well as the multiple rounds of updates required to train clients and a central server. In this paper, we explore how to <span id="id5.id1.1" class="ltx_text ltx_font_italic">efficiently</span> build NMT models in an FL setup by proposing a novel solution. In order to reduce the communication overhead, out of all neural layers we only exchange what we term “Controller” layers. Controllers are a small number of additional neural components connected to our pre-trained architectures. These new components are placed in between original layers. They act as liaisons to communicate with the central server and learn minimal information that is sufficient enough to update clients.</p>
<p id="id2.1" class="ltx_p">We evaluated the performance of our models on five datasets from different domains to translate from <span id="id2.1.1" class="ltx_text ltx_font_italic">German</span> into <span id="id2.1.2" class="ltx_text ltx_font_italic">English</span>. We noted that the models equipped with Controllers preform on par with those trained in a central and non-FL setting. In addition, we observed a substantial reduction in the communication traffic of the FL pipeline, which is a direct consequence of using Controllers. Based on our experiments, Controller-based models are <math id="id2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id2.1.m1.1a"><mo id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><csymbol cd="latexml" id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">\sim</annotation></semantics></math> 6 times less expensive than their other peers. This reduction is significantly important when we consider the number of parameters in large models and it becomes even more critical when such parameters need to be exchanged for multiple rounds in FL settings.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a paradigm in which models can be trained in a <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">decentralized</span> and <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">private</span> fashion. Model training in FL is distributed over multiple clients, each with their own set of data. This form of distributed training enables building models that can benefit from data residing in other clients without having direct access to them. The FL framework is different than the more traditional distributed training <cite class="ltx_cite ltx_citemacro_citep">(Langer et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, where the assumption is that data on devices is <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">accessible</span> and <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">identically distributed</span>, referred to as the IID condition. However, this assumption does not hold in FL where each client could have a private dataset very different than others (non-IID data). This dataset heterogeneity introduces new algorithmic challenges in FL that do not exist in the distributed setting.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">FL has been attracting more attention in recent years due to the inherent privacy it offers for model training. Customer privacy has been a top priority at many large companies and governing authorities, and the FL framework offers the setup for training models without the need for moving data from clients to a central server, or sharing data among clients. Despite this appealing characteristic, FL systems are still hard to implement and deploy in practice, since (in addition to the heterogeneity problem) they require an iterative communication update method. In each round, models’ parameters have to be exchanged between clients and a central server. The communication overhead of sending such a large number of parameters could become prohibitive and surpass the power budget on clients. This factor affects the practicality of FL in real-world scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Motivated by the aforementioned problem, in this paper we try to reduce the communication bandwidth. We apply our technique to train neural machine translation (NMT) engines. The reason for this choice is that bi-lingual datasets provide a natural way to incorporate data heterogeneity at local nodes. Generally, various models are used to generate synthetic datasets that simulate this non-IID data distribution <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>. However, having a truly heterogeneous dataset without the need for simulation is crucial, and helps produce realistic results. NMT and the rich dataset it offers, enables us to setup such an experimental environment organically.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">NMT also allows us to challenge the capabilities of our setup. Bilingual engines are usually large and data-hungry models. Demonstrating that such models are trainable in an FL setting has its own research values. Moreover, we leverage our novel idea to reduce communication and computation costs in the same setting. We design a structure we refer to as <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Controller</span>. A Controller is a layer inserted between two existing layers of a neural model. During training, the non-Controller (original) layers are frozen and only the Controller layer parameters are updated. Similarly, during communication, only the parameters of Controllers are shared between the clients and the central server. This is the main contribution of our research, that we will demonstrate through running experiments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">FL was introduced in <cite class="ltx_cite ltx_citemacro_citet">Bonawitz et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> as a distributed training framework to handle large scale data on edge devices, where data local to each edge device remains private <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib8" title="" class="ltx_ref">2020a</a>)</cite>. Each client is unable to generalize well given they only have access to local data with limited samples, thus limiting high-performance on practical use-cases. Therefore, clients could learn local domains but might fail to generalize to other domains. FL aims to address this as well as some other concerns through providing a platform for parties to share their model parameters. FL has been wide spread in different applications such as text analysis <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, big data <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, and computer vision <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.5" class="ltx_p">FL can be implemented in various ways <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, but in this paper we use the <span id="S2.p2.5.1" class="ltx_text ltx_font_italic">cross-silo</span> form, where a server node orchestrates learning and communication among clients and maintains a central model. In each FL round, the server pulls information (gradients or parameters) from clients and combines it to produce global knowledge. In our experiments, we use <span id="S2.p2.5.2" class="ltx_text ltx_font_typewriter">FedAVG</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> to aggregate clients’ information via a simple parameter averaging technique, as shown in Equation <a href="#S2.E1" title="In 2 Federated Learning ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="w_{i}\leftarrow\sum_{m=1}^{M}\frac{n_{m}}{n}w^{m}_{i}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">i</mi></msub><mo rspace="0.111em" stretchy="false" id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">←</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><munderover id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.1.1.3.1.2.2" xref="S2.E1.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.1.1.3.1.2.3" xref="S2.E1.m1.1.1.3.1.2.3.cmml"><mi id="S2.E1.m1.1.1.3.1.2.3.2" xref="S2.E1.m1.1.1.3.1.2.3.2.cmml">m</mi><mo id="S2.E1.m1.1.1.3.1.2.3.1" xref="S2.E1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.1.1.3.1.2.3.3" xref="S2.E1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.1.1.3.1.3" xref="S2.E1.m1.1.1.3.1.3.cmml">M</mi></munderover><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mfrac id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml"><msub id="S2.E1.m1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="S2.E1.m1.1.1.3.2.2.2.3" xref="S2.E1.m1.1.1.3.2.2.2.3.cmml">m</mi></msub><mi id="S2.E1.m1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">​</mo><msubsup id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.2.3.2.2" xref="S2.E1.m1.1.1.3.2.3.2.2.cmml">w</mi><mi id="S2.E1.m1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.3.2.3.3.cmml">i</mi><mi id="S2.E1.m1.1.1.3.2.3.2.3" xref="S2.E1.m1.1.1.3.2.3.2.3.cmml">m</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">←</ci><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><apply id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.1.1.cmml" xref="S2.E1.m1.1.1.3.1">superscript</csymbol><apply id="S2.E1.m1.1.1.3.1.2.cmml" xref="S2.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.1.2.1.cmml" xref="S2.E1.m1.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.1.2.2.cmml" xref="S2.E1.m1.1.1.3.1.2.2"></sum><apply id="S2.E1.m1.1.1.3.1.2.3.cmml" xref="S2.E1.m1.1.1.3.1.2.3"><eq id="S2.E1.m1.1.1.3.1.2.3.1.cmml" xref="S2.E1.m1.1.1.3.1.2.3.1"></eq><ci id="S2.E1.m1.1.1.3.1.2.3.2.cmml" xref="S2.E1.m1.1.1.3.1.2.3.2">𝑚</ci><cn type="integer" id="S2.E1.m1.1.1.3.1.2.3.3.cmml" xref="S2.E1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.1.1.3.1.3.cmml" xref="S2.E1.m1.1.1.3.1.3">𝑀</ci></apply><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><times id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></times><apply id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2"><divide id="S2.E1.m1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2"></divide><apply id="S2.E1.m1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2.2">𝑛</ci><ci id="S2.E1.m1.1.1.3.2.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.2.3">𝑚</ci></apply><ci id="S2.E1.m1.1.1.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.3">𝑛</ci></apply><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3">subscript</csymbol><apply id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.3">superscript</csymbol><ci id="S2.E1.m1.1.1.3.2.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.3.2.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3.2.3">𝑚</ci></apply><ci id="S2.E1.m1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">w_{i}\leftarrow\sum_{m=1}^{M}\frac{n_{m}}{n}w^{m}_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p2.4" class="ltx_p">where <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">M</annotation></semantics></math> is the number of clients, <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S2.p2.2.m2.1a"><msub id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">w</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">𝑤</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">w_{i}</annotation></semantics></math> is the server parameter set for the <span id="S2.p2.4.1" class="ltx_text ltx_font_italic">i</span>-<span id="S2.p2.4.2" class="ltx_text ltx_font_italic">th</span> training iteration, <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="n_{m}" display="inline"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">n</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝑛</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">n_{m}</annotation></semantics></math> is the number of data points in the <span id="S2.p2.4.3" class="ltx_text ltx_font_italic">m</span>-th client’s dataset, and <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.4.m4.1a"><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">n</annotation></semantics></math> is the total number of all training data. Besides <span id="S2.p2.4.4" class="ltx_text ltx_font_typewriter">FedAVG</span>, there exist more sophisticated aggregation algorithms, such as FedOpt <cite class="ltx_cite ltx_citemacro_citep">(Asad et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> and FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib9" title="" class="ltx_ref">2020b</a>)</cite>. Though these algorithms could lead to some performance improvement, we choose to use <span id="S2.p2.4.5" class="ltx_text ltx_font_typewriter">FedAVG</span> for two reasons:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">It offers an intuitive form of aggregation for FL, which provides a widely acceptable and easily reproducible baseline. This is crucial for our work since it is one of the few, if not the only, research efforts that combines FL and NMT.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">It has shown good performance across many applications of FL despite its simplicity. This feature plays a crucial role when it comes to model training in real world applications.</p>
</div>
</li>
</ul>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Transfer Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Traditionally, in machine learning problems, a model is trained and validated using data from a particular domain. Such a model usually does not generalize well on domains outside of its training dataset. In order to address this issue, transfer learning was proposed <cite class="ltx_cite ltx_citemacro_citep">(K. Weiss and Wang, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite>. The idea is to train a neural model in a particular domain, and then use the model in a different domain. Transfer learning has worked well in many fields, and the trained models in one domain can mostly generalize to other domains either straight away or with some fine-tuning. However, when fine tuning a model in a new domain, all neural model parameters are usually updated, which might not be necessary. <cite class="ltx_cite ltx_citemacro_citet">Houlsby et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Pfeiffer et al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Rücklé et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> proposed a new set of architectures, called Adapters, to address this shortcoming. Adapters are low-cost plug-ins that are mounted on pre-trained models inside their internal layers. When fine-tuning the model, only these adapter components are updated, which makes training more parameter-efficient and less expensive. However, adding adapter layers requires changing the internal architecture of the model, and the question of where to exactly insert these adapters to obtain performance improvement is not easily answered <cite class="ltx_cite ltx_citemacro_citep">(Rücklé et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We argue that using FL framework to train mixed-domain NMT models (the scope of this paper) is similar to transfer learning. Collecting parameters from clients of different domains, aggregating their information, and sending back aggregated knowledge to clients is a form of transfer learning. With this understanding, we ask <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">how can we make this transfer learning more communication and computation efficient?</span> Given that adding adapter layers is not straight forward, we propose a simple yet effective <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">Controller</span> alternative which we cover in detail in Section <a href="#S3" title="3 Communication and Computation-Efficient Model Training in FL ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Communication and Computation-Efficient Model Training in FL</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In FL, there is an aggregation phase in which information from multiple clients is fused on the server side, and the result is sent back to clients so they can learn from each other. This round-trip update requires a considerable communication bandwidth since <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">all</span> the model parameters are exchanged in each update round. One way to limit the communication overhead is to only send a subset of parameters that are more critical and sufficient for model training. However, for such an approach we might need to discover precise selection heuristics that add to the computation overhead.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Controller Layers</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To tackle the parameter selection problem, we propose a simple yet effective architectural addition, which we call Controllers. The Controller is another neural layer inserted in between two existing model layers. The goal of these <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">dedicated</span> layers is to adapt and promote information flow from the layers <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">below</span> them, and communicate that information to the central server. During the local update phase, these Controllers will disseminate the aggregate information from the server to the local models. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Controller Layers ‣ 3 Communication and Computation-Efficient Model Training in FL ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates this concept.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2112.06135/assets/controller3.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="395" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">A comparison between a client model equipped with Controller layers and its ordinary peer.</span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the model with Controllers, only the parameters of these particular layers are communicated with the server. Not only the communication overhead is reduced, there are additional privacy gains from this approach. Moreover, Controllers help manage the computation cost on the client side. Our Controller mechanism can be used in any neural architecture, but in our experiments, we embedded them into the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> architecture since it is the current state-of-the-art and a well-studied model in the community, and this makes our work easy to replicate by others.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In our models, we treat the Controller layers with two strategies during training. In the first set, we update all model parameters in conjunction with the Controller layers. In the second set, we freeze the original parameters of the model and only update the Controller layers. This is to compare the performance of the model in both scenarios and understand how much we lose in performance, if we reduce the layers involved in the training process. Regardless of which option we choose, the fact that we only communicate the Controller layer parameters to the server, and only need to update them when pushing back the information to clients, helps greatly to reduce the communication overhead. Moreover, if we freeze the original layers and only train the Controller layers, we are able to reduce the training time and computation overhead. Usually, clients in FL settings have limited resources and might not be able to train large models. Our proposed Controller layers provide benefits of an in-domain large model without incurring the computation cost on edge devices.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Study</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We designed our experiments using five bi-lingual, German–English datasets of <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">WMT</span>,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://statmt.org/wmt14/translation-task.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://statmt.org/wmt14/translation-task.html</a></span></span></span> <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">OpenSubtitle</span> or <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">OS</span> <cite class="ltx_cite ltx_citemacro_citep">(Lison and Tiedemann, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>, <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">PHP</span>, <span id="S4.p1.1.5" class="ltx_text ltx_font_italic">Ubuntu</span> or <span id="S4.p1.1.6" class="ltx_text ltx_font_italic">UB</span>, and <span id="S4.p1.1.7" class="ltx_text ltx_font_italic">TED</span> <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a href="#bib.bib18" title="" class="ltx_ref">2012</a>)</cite>. <span id="S4.p1.1.8" class="ltx_text ltx_font_italic">OS</span> is a large collection and for our experiments we only randomly selected 4.5 Million sentences of it, to make its size comparable to <span id="S4.p1.1.9" class="ltx_text ltx_font_italic">WMT</span>. Each dataset is from a different genre with a different size, which provides a heterogeneous collection of data for evaluation. For the test and development sets, we use <span id="S4.p1.1.10" class="ltx_text ltx_font_italic">newstest-14</span> and <span id="S4.p1.1.11" class="ltx_text ltx_font_italic">newstest-13</span>, respectively, for <span id="S4.p1.1.12" class="ltx_text ltx_font_italic">WMT</span>, and for the other datasets, we randomly select 2K sentences for the test and 2K for the development sets.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">All datasets are pre-processed with a <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">BPE</span> model <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> to extract sub-word units. Both English and German sides are used to train the <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">
BPE</span> model; we extracted 30K shared sub-units for each side. As our client and server models, we use Transformers. Each model has 6 layers on the encoder side and 6 layers on the decoder side. <span id="S4.p2.1.3" class="ltx_text ltx_font_italic">All</span> settings including hyper-parameters, dimensions, and training steps are identical to those of <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>. All of our baseline and FL results are summarized in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experimental Study ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.8.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.8.8.8" class="ltx_tr">
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">
<span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:90%;">Model </span><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\blacktriangledown" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">▼</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">▼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\blacktriangledown</annotation></semantics></math><span id="S4.T1.3.3.3.3.2" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T1.2.2.2.2.m2.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.T1.2.2.2.2.m2.1a"><mo fence="false" maxsize="90%" minsize="90%" id="S4.T1.2.2.2.2.m2.1.1" xref="S4.T1.2.2.2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m2.1b"><ci id="S4.T1.2.2.2.2.m2.1.1.cmml" xref="S4.T1.2.2.2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m2.1c">|</annotation></semantics></math><span id="S4.T1.3.3.3.3.3" class="ltx_text" style="font-size:90%;"> Testset </span><math id="S4.T1.3.3.3.3.m3.1" class="ltx_Math" alttext="\blacktriangleright" display="inline"><semantics id="S4.T1.3.3.3.3.m3.1a"><mo mathsize="90%" id="S4.T1.3.3.3.3.m3.1.1" xref="S4.T1.3.3.3.3.m3.1.1.cmml">▶</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m3.1b"><ci id="S4.T1.3.3.3.3.m3.1.1.cmml" xref="S4.T1.3.3.3.3.m3.1.1">▶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m3.1c">\blacktriangleright</annotation></semantics></math>
</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.4.4.4.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">WMT<sub id="S4.T1.4.4.4.4.1.1" class="ltx_sub">ts</sub></span></td>
<td id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.5.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">OS<sub id="S4.T1.5.5.5.5.1.1" class="ltx_sub">ts</sub></span></td>
<td id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.6.6.6.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TED<sub id="S4.T1.6.6.6.6.1.1" class="ltx_sub">ts</sub></span></td>
<td id="S4.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.7.7.7.7.1" class="ltx_text ltx_font_italic" style="font-size:90%;">PHP<sub id="S4.T1.7.7.7.7.1.1" class="ltx_sub">ts</sub></span></td>
<td id="S4.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.8.8.8.8.1" class="ltx_text ltx_font_italic" style="font-size:90%;">UB<sub id="S4.T1.8.8.8.8.1.1" class="ltx_sub">ts</sub></span></td>
<td id="S4.T1.8.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.8.8.8.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Average</span></td>
<td id="S4.T1.8.8.8.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.8.8.8.10.1" class="ltx_text ltx_font_italic" style="font-size:90%;">C-Cost</span></td>
<td id="S4.T1.8.8.8.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.8.8.8.11.1" class="ltx_text ltx_font_italic" style="font-size:90%;">T-Cost</span></td>
</tr>
<tr id="S4.T1.8.8.9.1" class="ltx_tr">
<td id="S4.T1.8.8.9.1.1" class="ltx_td ltx_align_center ltx_border_t" colspan="9"><span id="S4.T1.8.8.9.1.1.1" class="ltx_text" style="font-size:90%;">Non-FL Setting</span></td>
</tr>
<tr id="S4.T1.8.8.10.2" class="ltx_tr">
<td id="S4.T1.8.8.10.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T1.8.8.10.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1</span><span id="S4.T1.8.8.10.2.1.2" class="ltx_text" style="font-size:90%;">: 6L-6D WMT</span>
</td>
<td id="S4.T1.8.8.10.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.2.1" class="ltx_text" style="font-size:90%;">33.66</span></td>
<td id="S4.T1.8.8.10.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.3.1" class="ltx_text" style="font-size:90%;">18.57</span></td>
<td id="S4.T1.8.8.10.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.4.1" class="ltx_text" style="font-size:90%;">29.22</span></td>
<td id="S4.T1.8.8.10.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.5.1" class="ltx_text" style="font-size:90%;">8.04</span></td>
<td id="S4.T1.8.8.10.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.10.2.6.1" class="ltx_text" style="font-size:90%;">12.41</span></td>
<td id="S4.T1.8.8.10.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.10.2.7.1" class="ltx_text" style="font-size:90%;">20.38</span></td>
<td id="S4.T1.8.8.10.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.10.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.10.2.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.11.3" class="ltx_tr">
<td id="S4.T1.8.8.11.3.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.11.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2</span><span id="S4.T1.8.8.11.3.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D OS</span>
</td>
<td id="S4.T1.8.8.11.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.2.1" class="ltx_text" style="font-size:90%;">13.66</span></td>
<td id="S4.T1.8.8.11.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.3.1" class="ltx_text" style="font-size:90%;">23.58</span></td>
<td id="S4.T1.8.8.11.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.4.1" class="ltx_text" style="font-size:90%;">24.22</span></td>
<td id="S4.T1.8.8.11.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.5.1" class="ltx_text" style="font-size:90%;">7.84</span></td>
<td id="S4.T1.8.8.11.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.11.3.6.1" class="ltx_text" style="font-size:90%;">13.83</span></td>
<td id="S4.T1.8.8.11.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.11.3.7.1" class="ltx_text" style="font-size:90%;">16.62</span></td>
<td id="S4.T1.8.8.11.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.11.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.11.3.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.12.4" class="ltx_tr">
<td id="S4.T1.8.8.12.4.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.12.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">3</span><span id="S4.T1.8.8.12.4.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D TED</span>
</td>
<td id="S4.T1.8.8.12.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.2.1" class="ltx_text" style="font-size:90%;">12.09</span></td>
<td id="S4.T1.8.8.12.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.3.1" class="ltx_text" style="font-size:90%;">13.59</span></td>
<td id="S4.T1.8.8.12.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.4.1" class="ltx_text" style="font-size:90%;">29.32</span></td>
<td id="S4.T1.8.8.12.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.5.1" class="ltx_text" style="font-size:90%;">6.67</span></td>
<td id="S4.T1.8.8.12.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.12.4.6.1" class="ltx_text" style="font-size:90%;">10.15</span></td>
<td id="S4.T1.8.8.12.4.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.12.4.7.1" class="ltx_text" style="font-size:90%;">14.36</span></td>
<td id="S4.T1.8.8.12.4.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.12.4.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.12.4.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.13.5" class="ltx_tr">
<td id="S4.T1.8.8.13.5.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.13.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">4</span><span id="S4.T1.8.8.13.5.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D PHP</span>
</td>
<td id="S4.T1.8.8.13.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.2.1" class="ltx_text" style="font-size:90%;">0.00</span></td>
<td id="S4.T1.8.8.13.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.3.1" class="ltx_text" style="font-size:90%;">0.26</span></td>
<td id="S4.T1.8.8.13.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.4.1" class="ltx_text" style="font-size:90%;">0.26</span></td>
<td id="S4.T1.8.8.13.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.5.1" class="ltx_text" style="font-size:90%;">34.48</span></td>
<td id="S4.T1.8.8.13.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.13.5.6.1" class="ltx_text" style="font-size:90%;">0.00</span></td>
<td id="S4.T1.8.8.13.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.13.5.7.1" class="ltx_text" style="font-size:90%;">7.00</span></td>
<td id="S4.T1.8.8.13.5.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.13.5.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.13.5.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.14.6" class="ltx_tr">
<td id="S4.T1.8.8.14.6.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.14.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">5</span><span id="S4.T1.8.8.14.6.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D UB</span>
</td>
<td id="S4.T1.8.8.14.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.2.1" class="ltx_text" style="font-size:90%;">0.28</span></td>
<td id="S4.T1.8.8.14.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.3.1" class="ltx_text" style="font-size:90%;">0.78</span></td>
<td id="S4.T1.8.8.14.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.4.1" class="ltx_text" style="font-size:90%;">0.75</span></td>
<td id="S4.T1.8.8.14.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.5.1" class="ltx_text" style="font-size:90%;">2.30</span></td>
<td id="S4.T1.8.8.14.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.14.6.6.1" class="ltx_text" style="font-size:90%;">30.15</span></td>
<td id="S4.T1.8.8.14.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.14.6.7.1" class="ltx_text" style="font-size:90%;">6.85</span></td>
<td id="S4.T1.8.8.14.6.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.14.6.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.14.6.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.15.7" class="ltx_tr">
<td id="S4.T1.8.8.15.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T1.8.8.15.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">6</span><span id="S4.T1.8.8.15.7.1.2" class="ltx_text" style="font-size:90%;">: Fine Tuning</span>
</td>
<td id="S4.T1.8.8.15.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.2.1" class="ltx_text" style="font-size:90%;">33.50</span></td>
<td id="S4.T1.8.8.15.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.3.1" class="ltx_text" style="font-size:90%;">21.82</span></td>
<td id="S4.T1.8.8.15.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.4.1" class="ltx_text" style="font-size:90%;">31.51</span></td>
<td id="S4.T1.8.8.15.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.5.1" class="ltx_text" style="font-size:90%;">37.56</span></td>
<td id="S4.T1.8.8.15.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.15.7.6.1" class="ltx_text" style="font-size:90%;">35.61</span></td>
<td id="S4.T1.8.8.15.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.15.7.7.1" class="ltx_text" style="font-size:90%;">32.00</span></td>
<td id="S4.T1.8.8.15.7.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.15.7.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.15.7.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.16.8" class="ltx_tr">
<td id="S4.T1.8.8.16.8.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.16.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">7</span><span id="S4.T1.8.8.16.8.1.2" class="ltx_text" style="font-size:90%;">: Chained Training</span>
</td>
<td id="S4.T1.8.8.16.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.2.1" class="ltx_text" style="font-size:90%;">18.26</span></td>
<td id="S4.T1.8.8.16.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.3.1" class="ltx_text" style="font-size:90%;">23.51</span></td>
<td id="S4.T1.8.8.16.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.4.1" class="ltx_text" style="font-size:90%;">28.19</span></td>
<td id="S4.T1.8.8.16.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.5.1" class="ltx_text" style="font-size:90%;">16.14</span></td>
<td id="S4.T1.8.8.16.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.16.8.6.1" class="ltx_text" style="font-size:90%;">23.05</span></td>
<td id="S4.T1.8.8.16.8.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.16.8.7.1" class="ltx_text" style="font-size:90%;">21.83</span></td>
<td id="S4.T1.8.8.16.8.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.8.1" class="ltx_text" style="font-size:90%;">NA</span></td>
<td id="S4.T1.8.8.16.8.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.16.8.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.17.9" class="ltx_tr">
<td id="S4.T1.8.8.17.9.1" class="ltx_td ltx_align_center ltx_border_t" colspan="9"><span id="S4.T1.8.8.17.9.1.1" class="ltx_text" style="font-size:90%;">FL Setting</span></td>
</tr>
<tr id="S4.T1.8.8.18.10" class="ltx_tr">
<td id="S4.T1.8.8.18.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T1.8.8.18.10.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">8</span><span id="S4.T1.8.8.18.10.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D/A-A</span>
</td>
<td id="S4.T1.8.8.18.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.2.1" class="ltx_text" style="font-size:90%;">33.97</span></td>
<td id="S4.T1.8.8.18.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.3.1" class="ltx_text" style="font-size:90%;">19.17</span></td>
<td id="S4.T1.8.8.18.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.4.1" class="ltx_text" style="font-size:90%;">30.8</span></td>
<td id="S4.T1.8.8.18.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.5.1" class="ltx_text" style="font-size:90%;">37.32</span></td>
<td id="S4.T1.8.8.18.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.18.10.6.1" class="ltx_text" style="font-size:90%;">47.9</span></td>
<td id="S4.T1.8.8.18.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.18.10.7.1" class="ltx_text" style="font-size:90%;">33.83</span></td>
<td id="S4.T1.8.8.18.10.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.8.1" class="ltx_text" style="font-size:90%;">6E-6D</span></td>
<td id="S4.T1.8.8.18.10.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.18.10.9.1" class="ltx_text" style="font-size:90%;">6E-6D + W</span></td>
</tr>
<tr id="S4.T1.8.8.19.11" class="ltx_tr">
<td id="S4.T1.8.8.19.11.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.19.11.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">9</span><span id="S4.T1.8.8.19.11.1.2" class="ltx_text" style="font-size:90%;">: 8E-8D/A-A</span>
</td>
<td id="S4.T1.8.8.19.11.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.2.1" class="ltx_text" style="font-size:90%;">29.17</span></td>
<td id="S4.T1.8.8.19.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.3.1" class="ltx_text" style="font-size:90%;">18.6</span></td>
<td id="S4.T1.8.8.19.11.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.4.1" class="ltx_text" style="font-size:90%;">28.62</span></td>
<td id="S4.T1.8.8.19.11.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.5.1" class="ltx_text" style="font-size:90%;">32.26</span></td>
<td id="S4.T1.8.8.19.11.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.19.11.6.1" class="ltx_text" style="font-size:90%;">33.36</span></td>
<td id="S4.T1.8.8.19.11.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.19.11.7.1" class="ltx_text" style="font-size:90%;">28.40</span></td>
<td id="S4.T1.8.8.19.11.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.8.1" class="ltx_text" style="font-size:90%;">8E-8D</span></td>
<td id="S4.T1.8.8.19.11.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.19.11.9.1" class="ltx_text" style="font-size:90%;">8E-8D + W</span></td>
</tr>
<tr id="S4.T1.8.8.20.12" class="ltx_tr">
<td id="S4.T1.8.8.20.12.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.20.12.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10</span><span id="S4.T1.8.8.20.12.1.2" class="ltx_text" style="font-size:90%;">: 8E-8D/A-C (2-6)</span>
</td>
<td id="S4.T1.8.8.20.12.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.2.1" class="ltx_text" style="font-size:90%;">29.03</span></td>
<td id="S4.T1.8.8.20.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.3.1" class="ltx_text" style="font-size:90%;">18.59</span></td>
<td id="S4.T1.8.8.20.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.4.1" class="ltx_text" style="font-size:90%;">28.44</span></td>
<td id="S4.T1.8.8.20.12.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.5.1" class="ltx_text" style="font-size:90%;">31.99</span></td>
<td id="S4.T1.8.8.20.12.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.20.12.6.1" class="ltx_text" style="font-size:90%;">33.75</span></td>
<td id="S4.T1.8.8.20.12.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.20.12.7.1" class="ltx_text" style="font-size:90%;">28.36</span></td>
<td id="S4.T1.8.8.20.12.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.8.1" class="ltx_text" style="font-size:90%;">8E-8D</span></td>
<td id="S4.T1.8.8.20.12.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.20.12.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
<tr id="S4.T1.8.8.21.13" class="ltx_tr">
<td id="S4.T1.8.8.21.13.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.21.13.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">11</span><span id="S4.T1.8.8.21.13.1.2" class="ltx_text" style="font-size:90%;">: 8E-8D/A-C (0-6)</span>
</td>
<td id="S4.T1.8.8.21.13.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.2.1" class="ltx_text" style="font-size:90%;">31.79</span></td>
<td id="S4.T1.8.8.21.13.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.3.1" class="ltx_text" style="font-size:90%;">20.02</span></td>
<td id="S4.T1.8.8.21.13.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.4.1" class="ltx_text" style="font-size:90%;">30.6</span></td>
<td id="S4.T1.8.8.21.13.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.5.1" class="ltx_text" style="font-size:90%;">32.43</span></td>
<td id="S4.T1.8.8.21.13.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.21.13.6.1" class="ltx_text" style="font-size:90%;">33.41</span></td>
<td id="S4.T1.8.8.21.13.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.21.13.7.1" class="ltx_text" style="font-size:90%;">29.65</span></td>
<td id="S4.T1.8.8.21.13.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.8.1" class="ltx_text" style="font-size:90%;">8E-8D</span></td>
<td id="S4.T1.8.8.21.13.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.21.13.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
<tr id="S4.T1.8.8.22.14" class="ltx_tr">
<td id="S4.T1.8.8.22.14.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.22.14.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">12</span><span id="S4.T1.8.8.22.14.1.2" class="ltx_text" style="font-size:90%;">: 8E-8D/C-C (2-6)</span>
</td>
<td id="S4.T1.8.8.22.14.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.2.1" class="ltx_text" style="font-size:90%;">33.75</span></td>
<td id="S4.T1.8.8.22.14.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.3.1" class="ltx_text" style="font-size:90%;">19.51</span></td>
<td id="S4.T1.8.8.22.14.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.4.1" class="ltx_text" style="font-size:90%;">30.34</span></td>
<td id="S4.T1.8.8.22.14.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.5.1" class="ltx_text" style="font-size:90%;">32.29</span></td>
<td id="S4.T1.8.8.22.14.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.22.14.6.1" class="ltx_text" style="font-size:90%;">31.44</span></td>
<td id="S4.T1.8.8.22.14.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.22.14.7.1" class="ltx_text" style="font-size:90%;">29.46</span></td>
<td id="S4.T1.8.8.22.14.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.8.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
<td id="S4.T1.8.8.22.14.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.22.14.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
<tr id="S4.T1.8.8.23.15" class="ltx_tr">
<td id="S4.T1.8.8.23.15.1" class="ltx_td ltx_align_left ltx_border_r">
<span id="S4.T1.8.8.23.15.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">13</span><span id="S4.T1.8.8.23.15.1.2" class="ltx_text" style="font-size:90%;">: 8E-8D/C-C (0-5)</span>
</td>
<td id="S4.T1.8.8.23.15.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.2.1" class="ltx_text" style="font-size:90%;">31.9</span></td>
<td id="S4.T1.8.8.23.15.3" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.3.1" class="ltx_text" style="font-size:90%;">20.00</span></td>
<td id="S4.T1.8.8.23.15.4" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.4.1" class="ltx_text" style="font-size:90%;">30.91</span></td>
<td id="S4.T1.8.8.23.15.5" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.5.1" class="ltx_text" style="font-size:90%;">31.88</span></td>
<td id="S4.T1.8.8.23.15.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.23.15.6.1" class="ltx_text" style="font-size:90%;">32.74</span></td>
<td id="S4.T1.8.8.23.15.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.8.8.23.15.7.1" class="ltx_text" style="font-size:90%;">29.48</span></td>
<td id="S4.T1.8.8.23.15.8" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.8.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
<td id="S4.T1.8.8.23.15.9" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.23.15.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
<tr id="S4.T1.8.8.24.16" class="ltx_tr">
<td id="S4.T1.8.8.24.16.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S4.T1.8.8.24.16.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">14</span><span id="S4.T1.8.8.24.16.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D/C-C (0-3)</span>
</td>
<td id="S4.T1.8.8.24.16.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.2.1" class="ltx_text" style="font-size:90%;">31.13</span></td>
<td id="S4.T1.8.8.24.16.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.3.1" class="ltx_text" style="font-size:90%;">19.19</span></td>
<td id="S4.T1.8.8.24.16.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.4.1" class="ltx_text" style="font-size:90%;">30.95</span></td>
<td id="S4.T1.8.8.24.16.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.5.1" class="ltx_text" style="font-size:90%;">33.79</span></td>
<td id="S4.T1.8.8.24.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.24.16.6.1" class="ltx_text" style="font-size:90%;">32.85</span></td>
<td id="S4.T1.8.8.24.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.8.8.24.16.7.1" class="ltx_text" style="font-size:90%;">29.58</span></td>
<td id="S4.T1.8.8.24.16.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.8.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
<td id="S4.T1.8.8.24.16.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.8.8.24.16.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
<tr id="S4.T1.8.8.25.17" class="ltx_tr">
<td id="S4.T1.8.8.25.17.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S4.T1.8.8.25.17.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">15</span><span id="S4.T1.8.8.25.17.1.2" class="ltx_text" style="font-size:90%;">: 6E-6D/C-C (1-4)</span>
</td>
<td id="S4.T1.8.8.25.17.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.2.1" class="ltx_text" style="font-size:90%;">29.51</span></td>
<td id="S4.T1.8.8.25.17.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.3.1" class="ltx_text" style="font-size:90%;">18.62</span></td>
<td id="S4.T1.8.8.25.17.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.4.1" class="ltx_text" style="font-size:90%;">29.5</span></td>
<td id="S4.T1.8.8.25.17.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.5.1" class="ltx_text" style="font-size:90%;">31.13</span></td>
<td id="S4.T1.8.8.25.17.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.8.8.25.17.6.1" class="ltx_text" style="font-size:90%;">29.61</span></td>
<td id="S4.T1.8.8.25.17.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.8.8.25.17.7.1" class="ltx_text" style="font-size:90%;">27.67</span></td>
<td id="S4.T1.8.8.25.17.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.8.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
<td id="S4.T1.8.8.25.17.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.8.8.25.17.9.1" class="ltx_text" style="font-size:90%;">2E-2D</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span> Results from our NMT engines when trained in both central and FL settings. The first column shows the model and its configuration, e.g. in Row <span id="S4.T1.37.2" class="ltx_text ltx_font_bold">1</span> the <span id="S4.T1.38.3" class="ltx_text ltx_font_italic">6E-6D WMT</span> model is a Transformer with 6 encoder and 6 decoder layers trained using the WMT training set. The <span id="S4.T1.39.4" class="ltx_text ltx_font_italic">ts</span> subscript in other columns indicate the test sets, i.e. the score in Row <span id="S4.T1.40.5" class="ltx_text ltx_font_bold">1</span>, Column <span id="S4.T1.10.10.1" class="ltx_text ltx_font_italic">OS<sub id="S4.T1.10.10.1.1" class="ltx_sub">ts</sub></span> shows the performance of the <span id="S4.T1.41.6" class="ltx_text ltx_font_italic">6E-6D WMT</span> model on the <span id="S4.T1.42.7" class="ltx_text ltx_font_italic">OS</span> testset. In the FL setting, each model is labeled with a character tuple followed by a pair of digits. The first letter in the tuple indicates if all (A) or only Controller layers (C) are shared during communication. The second letter shows if we train all layers (A) or only update values of the Controller layers (C). The digit pair defines the position of the Controller layers, e.g. in the (2-6) setting in Row <span id="S4.T1.43.8" class="ltx_text ltx_font_bold">13</span>, the first Controller layers is placed after the second encoder layer and the second Controller is located after the sixth encoder layer. We use the same positioning system as in the encoder for the Controllers of the decoder. The last two columns show the <span id="S4.T1.44.9" class="ltx_text ltx_font_italic">Communication</span> (<span id="S4.T1.45.10" class="ltx_text ltx_font_italic">C-Cost</span>) and model <span id="S4.T1.46.11" class="ltx_text ltx_font_italic">Training</span> (<span id="S4.T1.47.12" class="ltx_text ltx_font_italic">T-Cost</span>) costs, where W stands for the word embedding table.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Rows 1 to 5 in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experimental Study ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> report BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib14" title="" class="ltx_ref">2002</a>)</cite> scores obtained from our stand-alone engines. These models are trained in a non-FL setting with their own training sets, e.g. <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">6E-6D WMT</span> is a Transformer with 6 encoder and 6 decoder layers trained on the WMT dataset. As expected, the model shows its best performance on in-domain data but it fails to successfully handle other domains’ data. The lowest BLEU score belongs to the PHP test set. Clearly, technical terms and special form of sentences in the PHP dataset is not fully understandable for the WMT model and makes it challenging for the model to generate high-quality translations. We see a similar trend for other models too, namely they perform well on in-domain data but could barely work for out-of-domain samples. However, for datasets from similar domains this performance gap is small(er), e.g. the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">OS</span> model is able to handle the <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">TED</span> testset relatively well as there are lots of similarities between the <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_italic">OS</span> (film subtitles) and <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_italic">TED</span> (TED talks) sets. <span id="S4.SS1.p1.1.6" class="ltx_text ltx_font_italic">PHP</span> is an extreme case in our pool; due to its size and technical genre with unique terms and special grammatical constituents, it cannot consume other domains’ sentences, so the BLEU scores are almost zero for all out-of-domain datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In our setting, a single model trained on in-domain data cannot be a good translation engine to support all domains. Therefore, we train a mixed-domain engine to address this problem. Rows 6 and 7 report related results. In the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">Fine Tuning</span> approach (Row 6), we combined all datasets to create a huge corpus of training and development sets. We then train a translation engine using such a collection which leads to high-quality results. The average BLEU score of this new model is 32.00 (higher than all others), which is a clear indication of its success. This model is capable of handling different domains quite well. However, this approach relies on a strong assumption, i.e. that all datasets are fully accessible at training time in a centralized repository, which might not be the case in real-world scenarios, and especially in distributed and private settings such as FL. Therefore, we propose another alternative, namely <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">Chained Training</span> (Row 7), where we start from a well-trained model and gradually fine-tune it with other datasets.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">Chained Training</span>, we assume that the WMT model is our base model. We share it with a client and it can fine-tune it with its own data. After each phase of fine-tuning, the updated model is shared with another client, so that it can also add its own data. In this scenario, there is no central training and the model is gradually updated by different clients. As seen in Row 7, results obtained are not as competitive as those in Row 6. This shows how model training can be dramatically impacted in the presence of non-centralized data repositories. It should be noted that, Row 7 does not even address the privacy aspect of the problem and only tries to gradually access data from different clients. Rows 6 or 7 can be considered as a baseline for our FL model, where Row 7 is a more relevant and fair candidate. The setting defined in Row 6 is only suitable for research purposes which discards complications of real-world settings.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>FL Experiments</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Row 8 in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experimental Study ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is our first FL system where we have five clients. Each client’s model is initialized with the WMT engine parameters and is updated by the local data in each round of FL. The client model’s parameters are sent to the server at the end of each round and the server aggregates clients’ information via <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">FedAVG</span>. The average BLEU score of this FL system is 33.83, which was impressive and at the same time unexpected given the fact that the server has no access to clients’ data, and the entire process is carried out in a decentralized and private environment. The communication cost (C-Cost) for each client is “<span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">6E-6D</span>” which means it exchanges 6 encoder and 6 decode layers with the server. The training cost (T-Cost) is “<span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">6E-6D + W</span>” since every client updates all the network parameters including those of encoder and decoder, and word embeddings.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In Row 9, we introduce a new FL configuration where instead of 6 layers we use 8 (for each encoder and decoder). Controllers define dedicated and extra layers responsible for communication and model updates. This increases the number of layers from 6 to 8, so Row 9 would be a fair baseline for Controller-based models. The expectation for Row 9 was a higher BLEU score than that of Row 8, since we increased the number of layers, and in general the quality of deep learning models should increase proportionally with the increase of their size. The same assumption also holds for our model, but the larger model in Row 9 requires more steps of training to generate better results. For all these models, we ensure that at the end of the training process, either centralized or distributed, each model is only trained for a total of 150K steps.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The core model, WMT, is trained for 100K steps then we run extra 50K steps of fine-tuning during FL rounds.</span></span></span> In order to have a fair setting, we also trained the 8E-8D/A-A model for 150K steps.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In Row 10, we apply the Controller idea for the first time. We freeze all layers during fine-tuning on the client side and only update Controller layers. The <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">T-Cost</span> for this setup is “2E-2D”, which is significantly lower than all previously reported results. We still communicate all the encoder/decoder layers in-between clients and the server, so <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">C-Cost</span> is still “8E-8D”. In this configuration, the first and second Controller layers are placed after the second and sixth encoder layers, respectively. We have an identical setting for the decoder Controllers. In Row 11, we have another configuration with a different positioning strategy. The average BLEU score of latter configuration is higher than the previous one, and it shows how placing Controllers in the right place can impact model quality. In Row 12, we have the same setting as in Row 10, but this time not only do we freeze Controllers during fine-tuning but we also limit the FL communication to these layers. Both <span id="S4.SS2.p3.1.3" class="ltx_text ltx_font_italic">C-Cost</span> and <span id="S4.SS2.p3.1.4" class="ltx_text ltx_font_italic">T-Cost</span> in this case is “2E-2D” which is considerably more affordable. Each encoder layer has 3,416,320 and each decoder layer has 4,204,032 parameters. The embedding table is a matrix with 33,116,512 parameters. Considering these numbers, the bandwidth used (for both communication and training) in Rows 9 and 12 are worth 94,079,328 and 15,240,704 parameters, respectively, which means that our Controller-based models <span id="S4.SS2.p3.1.5" class="ltx_text ltx_font_italic">are about 6 times less expensive</span>. Another interesting observation is that we are able to achieve better performance than all other 8E-8D models in Row 12, even though we update/communicate fewer parameters.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">In Rows 14 and 15, we challenged our proposed solution and instead of assigning additional layers we pick some layers of the 6E-6D model to play the role of Controllers. Our findings show that these 6E-6D models can perform as accurately as their 8E-8D peers, if we pick the right layers as the Controllers. However, 6E-6D models are much more sensitive to the position of Controllers.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Controllers Vs Adapters</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Adaptors (Section <a href="#S2.SS1" title="2.1 Transfer Learning ‣ 2 Federated Learning ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>) follow a similar logic as our Controllers, but they are solutions for centralized settings. In Adapters, a small unit is implanted <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">inside</span> each layer of a neural model whose job is (<span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">i</span>) to connect two consecutive layers to each other in a way that they do not deviate from their main distribution, and (<span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">ii</span>) take care of the transition between domains. Such a similar architecture persuaded us to try Adapters in our FL setting but our experiments were not successful. Our investigation showed that when an Adapter unit is sent out to the server side to be combined with other Adaptors, it becomes so distant from its original form that when it is placed back in the body of a client model, it destroys the internal information flow and stops the client from converging. We ran preliminary experiments with Adapters instead of Controllers and obtained gravely lower BLEU scores.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Aside from this qualitative aspect of the problem, we also do not find Adapters suitable for FL settings, because of a design overhead they introduce, e.g. some references recommend that the best position for Adapters to be placed is before Layer Norm <cite class="ltx_cite ltx_citemacro_citep">(Rücklé et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>, others placed them after Feed Forward sub-layers <cite class="ltx_cite ltx_citemacro_citep">(Bapna et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, and the placement options are not limited to only these alternatives. Moreover, each model proposes a different internal neural architecture for Adapters. These design decisions are made through exhaustive empirical investigations. This form of architecture search is not affordable in distributed learning settings like FL. Any exchange of information and re-run of training could be quite costly.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">This design dilemma is not unique to Adapters, and our Controllers could also suffer from similar issues, but in the Controller case the situation is far less severe. As Table <a href="#S4.T1" title="Table 1 ‣ 4 Experimental Study ‣ Communication-Efficient Federated Learning for Neural Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows, we have tried different positioning strategies and our findings show that positioning Controllers layers affect the final quality. This also becomes even more important when working with smaller models (6E-6D vs 8E-8E). However, this does not stop us from training clients in the FL setting, because almost any random positioning strategy leads to <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">acceptable</span> results. There is only one exception and that is when we placed the first Controller in between the embedding table and the first layer of the encoder. If we define a configuration such as <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">8E-8D/A-C (-1-6)</span> where the first layer of an encoder/decoder is a Controller, a client model would never converge. We could not find any mathematical justification for this observation, but we assume that because the first layers plays a critical role in building input representations, any noisy information can affect the entire model’s sustainability. It seems information added from other domains to the first layer after the embedding table, when it is used as a Controller, has a similar effect and stops clients from converging.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we focused our effort on developing a communication- and computation-efficient FL solution to train NMT models. Our research proposed a novel idea to include what we call <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Controller</span> layers. These layers are the ones trained, while the original network layers are frozen. By doing so, these layers learn the necessary information and communicate it to the central server during updates. The server aggregates the information from all the clients, and then sends the updates to each client through the Controllers. Our Controller-based models are about six times less expensive to train in comparison to baseline NMT models. This reduction is significantly important when we consider the large number of model parameters that are exchanged during multiple update rounds. For our future work, we are planning to apply Controllers to other tasks and discover a systematic way of finding the best position for placing Controllers.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asad et al. (2020)</span>
<span class="ltx_bibblock">
Muhammad Asad, Ahmed Moustafa, and Takayuki Ito. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3390/app10082864" title="" class="ltx_ref ltx_href">Fedopt: Towards
communication efficiency and privacy preservation in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 10(8).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna et al. (2019)</span>
<span class="ltx_bibblock">
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Simple, scalable adaptation for neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08478</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2016)</span>
<span class="ltx_bibblock">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H.B. McMahan, S. Patel,
D. Ramage, A. Segal, and K Seth. 2016.

</span>
<span class="ltx_bibblock">Practical secure aggregation for federated learning on user-held
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
2790–2799. PMLR.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K. Weiss and Wang (2016)</span>
<span class="ltx_bibblock">
T.M. Khoshgoftaar K. Weiss and D Wang. 2016.

</span>
<span class="ltx_bibblock">A survey of transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">https://doi.org/10.1186/s40537-016-0043-6</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Langer et al. (2020)</span>
<span class="ltx_bibblock">
Matthias Langer, Zhen He, Wenny Rahayu, and Yanbo Xue. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2007.03970" title="" class="ltx_ref ltx_href">Distributed training of deep
learning models: A taxonomic perspective</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2007.03970.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He. 2019.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09693</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MSP.2020.2975749" title="" class="ltx_ref ltx_href">Federated learning:
Challenges, methods, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(3):50–60.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, and Ameet
Talwalkarand Virginia Smith. 2020b.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:arXiv:1812.06127</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lison and Tiedemann (2016)</span>
<span class="ltx_bibblock">
Pierre Lison and Jörg Tiedemann. 2016.

</span>
<span class="ltx_bibblock">OpenSubtitles2016: Extracting large parallel corpora from movie
and TV subtitles.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC’16)</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
D. Liu, D. Dligach, and T.A Miller. 2019.

</span>
<span class="ltx_bibblock">T.a. two-stage federated phenotyping and patient representation
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">In Proceedings of the 18th ACL Workshop on Biomedical Natural
Language Processing, Florence, Italy</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Y. Liu, A. Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen, H. Yu,
and Q. Yang. 2020.

</span>
<span class="ltx_bibblock">Fedvision: An online visual object detection platform powered by
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">In Proceedings of the Thirty-Fourth AAAI Conference on
Artificial Intelligence, New York, NY, USA, 7–12</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, pages 1273–1282.
PMLR.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2020)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan
Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock">Adapterhub: A framework for adapting transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.07779</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rücklé et al. (2020)</span>
<span class="ltx_bibblock">
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
Pfeiffer, Nils Reimers, and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock">Adapterdrop: On the efficiency of adapters in transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11918</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1162" title="" class="ltx_ref ltx_href">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European
Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeerand Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.03762</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang,
and Christopher G Brinton. 2021.

</span>
<span class="ltx_bibblock">Device sampling for heterogeneous federated learning: Theory,
algorithms, and implementation.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00787</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Jiale Zhang, Bing Chen, Shui Yu, and Hai Deng. 2019.

</span>
<span class="ltx_bibblock">Pefl: A privacy-enhanced federated learning scheme for big data
analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Global Communications Conference (GLOBECOM)</em>, pages 1–6.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.06134" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.06135" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.06135">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.06135" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.06136" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 16:44:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
