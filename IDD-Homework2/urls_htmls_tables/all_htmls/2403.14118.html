<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation</title>
<!--Generated on Thu Mar 21 04:01:42 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
machine translation,  quality estimation,  large language model
" lang="en" name="keywords"/>
<base href="/html/2403.14118v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S1" title="I Introduction ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S1.SS1" title="I-A Outline ‣ I Introduction ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-A</span> </span><span class="ltx_text ltx_font_italic">Outline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S1.SS2" title="I-B Contribution ‣ I Introduction ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">I-B</span> </span><span class="ltx_text ltx_font_italic">Contribution</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2" title="II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS1" title="II-A Datasets ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS2" title="II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Annotation Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS2.SSS1" title="II-B1 Human Translation Error Rate (HTER) ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Human Translation Error Rate (HTER)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS2.SSS2" title="II-B2 Direct Assessment (DA) ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Direct Assessment (DA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS2.SSS3" title="II-B3 Multi-dimensional Quality Metrics (MQM) ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span>Multi-dimensional Quality Metrics (MQM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS2.SSS4" title="II-B4 Discussion of Difficulty ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>4 </span>Discussion of Difficulty</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS3" title="II-C Shared Tasks ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Shared Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS3.SSS1" title="II-C1 Word-level QE Shared Tasks ‣ II-C Shared Tasks ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Word-level QE Shared Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS3.SSS2" title="II-C2 Sentence-level QE Shared Tasks ‣ II-C Shared Tasks ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Sentence-level QE Shared Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS3.SSS3" title="II-C3 Document-level QE Shared Tasks ‣ II-C Shared Tasks ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>3 </span>Document-level QE Shared Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.SS3.SSS4" title="II-C4 Explainable QE Shared Tasks ‣ II-C Shared Tasks ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>4 </span>Explainable QE Shared Tasks</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3" title="III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods of Quality Estimation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS1" title="III-A Quality Estimation Based on Handcrafted Features ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Quality Estimation Based on Handcrafted Features</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS2" title="III-B Quality Estimation Based on Deep Learning ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Quality Estimation Based on Deep Learning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS2.SSS1" title="III-B1 Classic Deep Learning Methods ‣ III-B Quality Estimation Based on Deep Learning ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Classic Deep Learning Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS2.SSS2" title="III-B2 Incorporating Pre-trained Language Models Methods ‣ III-B Quality Estimation Based on Deep Learning ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Incorporating Pre-trained Language Models Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3" title="III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Quality Estimation Based on Large Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3.SSS1" title="III-C1 Direct prediction based on content generated by LLMs ‣ III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Direct prediction based on content generated by LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3.SSS2" title="III-C2 Based on the generative probabilities of LLMs ‣ III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Based on the generative probabilities of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3.SSS3" title="III-C3 Leveraging LLMs to generate pseudo data ‣ III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>3 </span>Leveraging LLMs to generate pseudo data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3.SSS4" title="III-C4 LLMs as the foundation for QE models ‣ III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>4 </span>LLMs as the foundation for QE models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3.SS3.SSS5" title="III-C5 Retrieval-based methods ‣ III-C Quality Estimation Based on Large Language Models ‣ III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>5 </span>Retrieval-based methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S4" title="IV Finding ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Finding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S5" title="V Conclusion ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.14118v1 [cs.CL] 21 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anonymous Authors
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Machine Translation Quality Estimation (MTQE) is the task of estimating the quality of machine-translated text in real time without the need for reference translations, which is of great importance for the development of MT. After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and Large Language Models (LLMs), with a further division of deep learning-based methods into classic deep learning and those incorporating pre-trained language models (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
machine translation, quality estimation, large language model

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">As a critical subfield within NLP, MT has witnessed groundbreaking developments with the advent of deep learning technologies. Nonetheless, the quality of MT remains inherently uncertain. Traditional evaluation metrics, such as BLEU <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">papineni2002bleu</span>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">banerjee2005meteor</span>]</cite>, TER <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">snover2006study</span>]</cite>, and CHRF rely on reference translations to assess translation quality. In contrast, QE techniques are capable of automatically evaluating the quality of translations without the need for reference, offering a valuable alternative for appraising the performance of MT systems.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In real-world application scenarios, the use of MT systems often operates without the availability of reference translations. In such contexts, the importance of QE is particularly highlighted. Without access to references, QE provides a crucial independent assessment of translation quality for users, developers, and translation service providers alike. For users, this allows them to more accurately determine the level of translation quality; for developers, QE serves as a robust means of measuring MT system performance; and for translation service providers, QE offers a way to filter out low-quality translations before delivery. These applications demonstrate the extensive applicability and critical role of QE across various levels and sectors within the field.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the initial stages of QE for MT, there was no unified and clear definition of the field, and the research primarily concentrated on statistical machine translation systems. In 2009, researchers such as Specia et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">specia2009estimating</span>]</cite> introduced an innovative QE framework, which involved manual scoring annotations on translation, the implementation of feature engineering, and the use of ML algorithms to train models capable of predicting translation quality. Since the Workshop on Machine Translation (WMT) established QE as a separate task in 2012, the research has evolved into three main methods: the first is handcrafted feature-based QE; the second leverages deep learning for QE, which further includes subdivisions such as classic deep learning approaches and those incorporating pre-trained LMs; and the third, an emerging method, is based on LLMs. The development of these methods has significantly advanced the progress of QE and gradually improved the accuracy of QE models’ assessments.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Undoubtedly, approaches based on LLMs have become a focal point of research within the QE domain. Researchers are seeking to harness the extensive knowledge base and learning capabilities of LLMs to achieve new breakthroughs in QE studies. Currently, QE research based on LLMs mainly encompasses the following directions: first, direct prediction of translation quality scores <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kocmi2023large</span>]</cite>, error levels <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lu2023error</span>]</cite>, or fluency <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2023knowledge</span>]</cite> using LLMs; second, leveraging the generative probabilities from LLMs, which involves using various prompts and examples to obtain multiple generative probabilities for the translated sentences of source texts, thereby calculating the mean and variance to gain a more accurate measure of uncertainty for translation quality <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2023towards</span>]</cite>; third, generating pseudo data based on the knowledge within LLMs to then transfer to QE models <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2023instructscore</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">HUANG2024102022</span>]</cite>; fourth, employing LLMs as pre-trained foundation models to enhance QE systems <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2023instructscore</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">gladkoff2023predicting</span>]</cite>; and fifth, adopting retrieval-based approaches to infuse translation knowledge into LLMs <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2023towards</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">HUANG2024102022</span>]</cite>. Although the performance of LLM-based QE methods has not yet surpassed that of QE methods incorporating pre-trained LMs, it is anticipated that with ongoing research, LLM-based approaches have the potential to reach state-of-the-art (SOTA) performance levels.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Indeed, despite the significant advancements made in QE, there remain several challenges that urgently need to be addressed, including data scarcity, insufficient interpretability, the rarity of word-level and document-level QE methods, the high computational resource requirements of pre-trained LMs and LLMs, and the lack of standardized evaluation benchmarks. To improve the accuracy, interpretability, and sustainability of QE, these challenges must be tackled one by one.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, our aim is to provide a clear and concise overview of MTQE for practitioners engaged in QE research and scholars interested in entering this field. In contrast to shared task overviews, our work not only synthesizes the WMT QE shared tasks from the past four years but also broadens the scope of content. Specifically, this paper reviews datasets, annotation methods, shared tasks, and all the seminal classic methods within the QE domain, with a particular emphasis on the currently highly-regarded QE approaches based on LLMs. Moreover, we explore the specific impact of LLMs on QE, a topic not yet covered in other survey reviews. Ultimately, we engage in an in-depth discussion of the current challenges faced by QE and the future research directions in this area.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS1.5.1.1">I-A</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS1.6.2">Outline</span>
</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2" title="II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">II</span></a>, we discuss commonly used datasets for QE and categorize annotation methods into Human Translation Error Rate (HTER), Direct Assessment (DA), and Multi-dimensional Quality Metrics (MQM) based on their application scenarios. We also classify QE shared tasks into word-level, sentence-level, document-level and explainable QE. However, QE tasks are still evolving and there is a need for more reasonable objectives and data annotation principles.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S3" title="III Methods of Quality Estimation ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">III</span></a>, we review different methods in the QE field and classify them into feature engineering and ML-based methods, deep learning-based methods, and LLM-based methods. Among deep learning-based methods, we further categorize them into classic deep learning methods and those incorporating pre-trained LMs. We also list notable methods in each box of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S1.F1" title="Figure 1 ‣ I-A Outline ‣ I Introduction ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S4" title="IV Finding ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">IV</span></a>, we list five major challenges that currently exist in the QE field. Finally, in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S5" title="V Conclusion ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">V</span></a>, we provide our conclusions.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="476" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>All Methods Mentioned in This Paper.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S1.SS2.5.1.1">I-B</span> </span><span class="ltx_text ltx_font_italic" id="S1.SS2.6.2">Contribution</span>
</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Our contributions can be summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We provide a clear and concise overview for practitioners engaged in QE and scholars interested in entering this field of study, covering the research development in QE, a significant and innovative area within NLP. This includes datasets, annotation methods, shared tasks, and nearly all the key methods within the QE domain, with a special emphasis on the currently popular QE approaches based on LLMs, a topic not yet covered in other survey reviews.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We classify the methods that have emerged throughout the development of the QE field into three main categories: those that employ handcrafted features, those grounded in deep learning, and those leveraging LLMs. We have conducted an in-depth exploration of nearly all the representative methods within the QE domain, placing particular emphasis on elucidating the intrinsic connections among them. Our goal is to provide a thorough and professional understanding of the current state of QE methodologies.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Compared to overviews of shared tasks, we have synthesized the QE shared tasks from WMT over the past four years, and included additional content. Furthermore, we delve into a discussion of the five challenges faced by QE, as well as prospective research directions for future.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section offers a comprehensive overview of QE, covering datasets, annotation methods, and shared tasks. It reviews datasets used in QE studies, explores annotation methods, and introduces shared tasks at word-level, sentence-level, document-level, and explainable QE. These aspects provide valuable resources and evaluation methods for researchers.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Datasets</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The MLQE-PE dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">fomicheva2020mlqepe</span>]</cite> is a significant milestone in QE and
Automatic Post-Editing (APE) research, providing annotations in a multilingual environment. The dataset is constructed using sentences from Wikipedia and Reddit articles. Parallel corpora are generated for 11 different language pairs (LPs), including 7 conventional resource LPs (English-German – En-De, English-Chinese – En-Zh, Russian-English – Ru-En, Romanian-English – Ro-En, Estonian-English – Et-En, Nepali-English – Ne-En, and Sinhala-English – Si-En) with 10K sentences each, divided into training, development, and two test sets (test20 and test21). Additionally, the dataset includes 4 zero-shot LPs (Pashto-English – Ps-En, Khmer-English – Km-En, English-Japanese – En-Ja, and English-Czech – En-Cs) with 2K sentences each, evenly split into two test sets too.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The WMT2023 QE dataset, provided by the organizers of WMT2023, encompasses DA and post-editing (PE) data, as well as data based on MQM. It is noteworthy that the data for English-Hindi – En-Hi, English-Gujarati – En-Gu, English-Tamil – En-Ta, English-Telugu – En-Te, English-Persian – En-Fa, and Hebrew-English – He-En LPs in the WMT2023 QE dataset are newly released in 2023.
</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The DA &amp; PE data includes all LPs from the MLQE-PE dataset and has added new LPs such as English-Yoruba – En-Yo, English-Marathi – En-Mr, En-Hi, En-Ta, En-Te, En-Gu, and En-Fa. Within this dataset, there are 14 LPs provided with PE information and 17 LPs with DA annotations. The training set comprises all LPs from the MLQE-PE dataset, with approximately 10,000 samples per LP; about 7,000 samples for En-Hi, En-Gu, En-Ta, and En-Te respectively; and approximately 27,000 samples for En-Mr. The test set includes LPs such as En-Mr, En-Hi, En-Gu, En-Ta, En-Te, and En-Fa, each with more than 1,000 samples.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The MQM data section covers four LPs: En-De, En-Ru, Zh-En, and He-En. The training set includes En-De, En-Ru, and Zh-En LPs with 30,425, 17,144, and 36,851 samples, respectively. The test set encompasses En-De, Zh-En, and He-En pairs, each with over 1,000 samples.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">These datasets provide an extremely important resource for research in the field of QE. They offer researchers corpora with rich texts and detailed annotations, which have propelled the progress of QE research. Specific related information can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.T1" title="TABLE I ‣ II-A Datasets ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">I</span></a>. It should be noted that, for the sake of brevity in presentation, the WMT2023 QE dataset omits all LPs from the MLQE-PE dataset.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Datasets for Quality Estimation.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1">
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.2" rowspan="2">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1">LPs</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1">Sentences</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T1.1.1.4">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.4.1">Tokens</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S2.T1.1.1.5">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.1">Annotations</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.6" rowspan="2">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.6.1">Data Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.7" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.7.1">Release Date</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1">Train</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.2">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.1">Dev</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.3">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.3.1">Test</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.4">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.4.1">Train</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.5">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.5.1">Dev</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.6">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.6.1">Test</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.7">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.7.1">DA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.8">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.8.1">PE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.9">
<span class="ltx_text ltx_font_bold" id="S2.T1.1.2.9.1">MQM</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.3.1" rowspan="11"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.1.3.1.1">MLQE-PE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2">En-De</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.3">7,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.4">1,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.5">1,000/1,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.6">114,980</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.7">16,519</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.8">16,371/16,545</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.10">✓</td>
<td class="ltx_td ltx_border_t" id="S2.T1.1.3.11"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.12">Wikipedia</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.13">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4">
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.1">En-Zh</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.5">115,585</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.6">16,307</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.7">16,765/16,637</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.9">✓</td>
<td class="ltx_td" id="S2.T1.1.4.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.4.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5">
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.1">Ru-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.5">82,229</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.6">11,992</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.7">11,760/11,650</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.9">✓</td>
<td class="ltx_td" id="S2.T1.1.5.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.11">Reddit</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6">
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.1">Ro-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.5">120,198</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.6">17,268</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.7">17,001/17,359</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.9">✓</td>
<td class="ltx_td" id="S2.T1.1.6.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.6.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7">
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.1">Et-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.5">98,080</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.6">14,423</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.7">14,358/14,044</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.9">✓</td>
<td class="ltx_td" id="S2.T1.1.7.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8">
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.1">Ne-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.5">104,934</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.6">15,144</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.7">14,770/15,017</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.9">✓</td>
<td class="ltx_td" id="S2.T1.1.8.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.8.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9">
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.1">Si-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.4">1,000/1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.5">109,515</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.6">15,708</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.7">15,821/15,709</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.9">✓</td>
<td class="ltx_td" id="S2.T1.1.9.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.9.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10">
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.1">Ps-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.6">27,045</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.7">27,414</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.9">✓</td>
<td class="ltx_td" id="S2.T1.1.10.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.10.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11">
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.1">Km-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.6">21,981</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.7">22,048</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.9">✓</td>
<td class="ltx_td" id="S2.T1.1.11.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.11.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12">
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.1">En-Ja</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.6">20,626</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.7">20,646</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.9">✓</td>
<td class="ltx_td" id="S2.T1.1.12.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.12.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13">
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.1">En-Cs</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.6">20,394</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.7">20,244</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.8">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.9">✓</td>
<td class="ltx_td" id="S2.T1.1.13.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.11">Wikipedia</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.13.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.14.1" rowspan="9"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.1.14.1.1">WMT2023 QE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.2">En-Mr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.3">27,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.4">1,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.5">1,086</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.6">717,581</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.7">26,253</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.8">27,951</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.10">✓</td>
<td class="ltx_td ltx_border_t" id="S2.T1.1.14.11"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.12">multi-domain/multi-corpus</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.14.13">2023</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.14">En-Hi</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.15">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.16">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.17">1,074</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.18">181,336</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.19">25,943</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.20">28,032</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.21">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.22">multi-domain/multi-corpus</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.14.23">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15">
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.1">En-Gu</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.4">1,075</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.5">153,685</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.6">21,238</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.7">23,084</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.8">✓</td>
<td class="ltx_td" id="S2.T1.1.15.9"></td>
<td class="ltx_td" id="S2.T1.1.15.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.11">multi-domain/multi-corpus</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.15.12">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16">
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.1">En-Ta</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.3">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.4">1,067</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.5">150,670</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.6">21,655</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.7">20,342</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.8">✓</td>
<td class="ltx_td" id="S2.T1.1.16.9"></td>
<td class="ltx_td" id="S2.T1.1.16.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.11">multi-domain/multi-corpus</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.16.12">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.17">
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.1">En-Te</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.2">7,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.3">1,028</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.5">147,492</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.6">20,686</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.7">22,640</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.8">✓</td>
<td class="ltx_td" id="S2.T1.1.17.9"></td>
<td class="ltx_td" id="S2.T1.1.17.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.11">multi-domain/multi-corpus</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.17.12">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.18">
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.1">En-Fa</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.3">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.4">1,000</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.6">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.7">26,807</td>
<td class="ltx_td" id="S2.T1.1.18.8"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.9">✓</td>
<td class="ltx_td" id="S2.T1.1.18.10"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.11">news (multi-domain)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.18.12">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.19">
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.1">En-De</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.2">30,425</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.3">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.4">1,897</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.5">877,066</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.6">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.7">37,996</td>
<td class="ltx_td" id="S2.T1.1.19.8"></td>
<td class="ltx_td" id="S2.T1.1.19.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.10">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.11">multi-domain</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.19.12">2021/23</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.20">
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.1">En-Ru</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.2">17,144</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.3">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.4">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.5">395,045</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.6">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.7">-</td>
<td class="ltx_td" id="S2.T1.1.20.8"></td>
<td class="ltx_td" id="S2.T1.1.20.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.10">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.11">multi-domain</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.20.12">2021/22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.21">
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.1">Zh-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.2">36,851</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.3">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.4">1,675</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.5">1,654,454</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.6">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.7">39,770</td>
<td class="ltx_td" id="S2.T1.1.21.8"></td>
<td class="ltx_td" id="S2.T1.1.21.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.10">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.11">multi-domain</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.21.12">2021/23</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.22">
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.1">He-En</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.3">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.4">1,182</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.6">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.7">35,592</td>
<td class="ltx_td" id="S2.T1.1.22.8"></td>
<td class="ltx_td" id="S2.T1.1.22.9"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.10">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.11">multi-domain</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.22.12">2023</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.23">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.1.23.1"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.2"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.3"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.4"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.5"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.6"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.7"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.8"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.9"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.10"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.11"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.12"></td>
<td class="ltx_td ltx_border_tt" id="S2.T1.1.23.13"></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Annotation Methods</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">This section discusses annotation methods in QE, which serve QE systems by providing labeled data. Three main methods will be introduced: HTER, DA, and MQM, each with its unique advantages and limitations, and suitable for different application scenarios. Finally, this section provides a discussion on the difficulty level associated with each of the three annotation methods.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span>Human Translation Error Rate (HTER)</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">HTER is a common method used for annotating translated sentences based on the volume of PE effort required. It builds upon word-level QE and is calculated from the results thereof. The goal of the reference translation is to make the least possible modifications while maintaining the original meaning and grammatical correctness. HTER scores a translated sentence by calculating the proportion of
the number of edits (insertions, deletions, and replacements) made during the PE process to the number of words in the post-edition, with the formula provided in (<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.E1" title="1 ‣ II-B1 Human Translation Error Rate (HTER) ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">1</span></a>). In previous research, HTER has been analyzed as a substitute for human assessment, with some studies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">Snover2005ASO</span>]</cite> recommending its use as a gold standard for evaluation. However, opinions vary within the academic community regarding the adequacy of HTER as a substitute <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">Graham2016IsAT</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="HTER=\frac{\#\text{ of edits}}{\#\text{ of words in the post-edition}}." class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.2.2.cmml">H</mi><mo id="S2.E1.m1.1.1.1.1.2.1" xref="S2.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.2.3.cmml">T</mi><mo id="S2.E1.m1.1.1.1.1.2.1a" xref="S2.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.2.4" xref="S2.E1.m1.1.1.1.1.2.4.cmml">E</mi><mo id="S2.E1.m1.1.1.1.1.2.1b" xref="S2.E1.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.2.5" xref="S2.E1.m1.1.1.1.1.2.5.cmml">R</mi></mrow><mo id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2.2" mathvariant="normal" xref="S2.E1.m1.1.1.1.1.3.2.2.cmml">#</mi><mo id="S2.E1.m1.1.1.1.1.3.2.1" xref="S2.E1.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mtext id="S2.E1.m1.1.1.1.1.3.2.3" xref="S2.E1.m1.1.1.1.1.3.2.3a.cmml"> of edits</mtext></mrow><mrow id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.3.2" mathvariant="normal" xref="S2.E1.m1.1.1.1.1.3.3.2.cmml">#</mi><mo id="S2.E1.m1.1.1.1.1.3.3.1" xref="S2.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mtext id="S2.E1.m1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.3.3.3a.cmml"> of words in the post-edition</mtext></mrow></mfrac></mrow><mo id="S2.E1.m1.1.1.1.2" lspace="0em" xref="S2.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"></eq><apply id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"><times id="S2.E1.m1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2">𝐻</ci><ci id="S2.E1.m1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.3">𝑇</ci><ci id="S2.E1.m1.1.1.1.1.2.4.cmml" xref="S2.E1.m1.1.1.1.1.2.4">𝐸</ci><ci id="S2.E1.m1.1.1.1.1.2.5.cmml" xref="S2.E1.m1.1.1.1.1.2.5">𝑅</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3"></divide><apply id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2"><times id="S2.E1.m1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2.1"></times><ci id="S2.E1.m1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2">#</ci><ci id="S2.E1.m1.1.1.1.1.3.2.3a.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3"><mtext id="S2.E1.m1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3"> of edits</mtext></ci></apply><apply id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3"><times id="S2.E1.m1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.3.1"></times><ci id="S2.E1.m1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.3.2">#</ci><ci id="S2.E1.m1.1.1.1.1.3.3.3a.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3"><mtext id="S2.E1.m1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3"> of words in the post-edition</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">HTER=\frac{\#\text{ of edits}}{\#\text{ of words in the post-edition}}.</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_H italic_T italic_E italic_R = divide start_ARG # of edits end_ARG start_ARG # of words in the post-edition end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span>Direct Assessment (DA)</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">DA is a widely utilized manual evaluation method that provides a subjective quality assessment, taking into account the overall effect of translation outputs and serving as an alternative to HTER. During the DA evaluation process, annotators rate the quality of translations directly within a range of 0-100. When utilizing multiple DA scores as targets for QE tasks, these scores are typically normalized first, and the normalized averages are then used to represent the quality score of the MT output. DA scores are prone to inconsistency due to the influence of annotators’ individual preferences. However, solutions have been proposed to enhance the consistency among annotators <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">graham-etal-2013-continuous</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">guzman-etal-2019-flores</span>]</cite>. As a result, DA has established itself as a reliable manual evaluation method <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shapira2019crowdsourcing</span>]</cite> and is extensively employed in QE tasks. Some advocate human assessments using DA, while others <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">fomicheva2020mlqepe</span>]</cite> believe that DA and HTER offer distinct perspectives on MT quality. Both viewpoints are considered valid.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.5.1.1">II-B</span>3 </span>Multi-dimensional Quality Metrics (MQM)</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.4">MQM <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lommel2014multidimensional</span>]</cite> is an innovative and more objective annotation method that combines multiple evaluation indicators. It divides MT errors into 7 dimensions: Terminology, Accuracy, Linguistic conventions, Style, Locale conventions, Audience appropriateness, and Design and markup. Each dimension is subdivided into different error types, allowing for a more fine-grained QE of MT. Each dimension corresponds to four severity levels: no errors, minor errors, major errors, and critical errors, with different penalty scores for each level. Annotators can adjust parameters based on specific needs and integrate MQM into specific scenarios. Points are deducted based on the type of errors triggered by the MT, and the MT scores are calculated by subtracting the penalty scores from the full scores. MQM provides a more comprehensive and objective evaluation of MT quality compared to HTER and DA. It offers flexibility and personalized assessments, but requires annotators with domain knowledge and careful parameter settings. The formula is shown as in (<a class="ltx_ref" href="https://arxiv.org/html/2403.14118v1#S2.E2" title="2 ‣ II-B3 Multi-dimensional Quality Metrics (MQM) ‣ II-B Annotation Methods ‣ II DATA, ANNOTATIONS METHODS, AND SHARED TASKS FOR QUALITY ESTIMATION ‣ From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"><span class="ltx_text ltx_ref_tag">2</span></a>), where <math alttext="n_{\text{minor}}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.1.m1.1"><semantics id="S2.SS2.SSS3.p1.1.m1.1a"><msub id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS3.p1.1.m1.1.1.2" xref="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml">n</mi><mtext id="S2.SS2.SSS3.p1.1.m1.1.1.3" xref="S2.SS2.SSS3.p1.1.m1.1.1.3a.cmml">minor</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.1b"><apply id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.2">𝑛</ci><ci id="S2.SS2.SSS3.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.3"><mtext id="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS3.p1.1.m1.1.1.3">minor</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.1c">n_{\text{minor}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.1.m1.1d">italic_n start_POSTSUBSCRIPT minor end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="n_{\text{major}}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.2.m2.1"><semantics id="S2.SS2.SSS3.p1.2.m2.1a"><msub id="S2.SS2.SSS3.p1.2.m2.1.1" xref="S2.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS3.p1.2.m2.1.1.2" xref="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml">n</mi><mtext id="S2.SS2.SSS3.p1.2.m2.1.1.3" xref="S2.SS2.SSS3.p1.2.m2.1.1.3a.cmml">major</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.2.m2.1b"><apply id="S2.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1.2">𝑛</ci><ci id="S2.SS2.SSS3.p1.2.m2.1.1.3a.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1.3"><mtext id="S2.SS2.SSS3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS3.p1.2.m2.1.1.3">major</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.2.m2.1c">n_{\text{major}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.2.m2.1d">italic_n start_POSTSUBSCRIPT major end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="n_{\text{critical}}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.3.m3.1"><semantics id="S2.SS2.SSS3.p1.3.m3.1a"><msub id="S2.SS2.SSS3.p1.3.m3.1.1" xref="S2.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS3.p1.3.m3.1.1.2" xref="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml">n</mi><mtext id="S2.SS2.SSS3.p1.3.m3.1.1.3" xref="S2.SS2.SSS3.p1.3.m3.1.1.3a.cmml">critical</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.3.m3.1b"><apply id="S2.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.2">𝑛</ci><ci id="S2.SS2.SSS3.p1.3.m3.1.1.3a.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.3"><mtext id="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS3.p1.3.m3.1.1.3">critical</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.3.m3.1c">n_{\text{critical}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.3.m3.1d">italic_n start_POSTSUBSCRIPT critical end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="n" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.4.m4.1"><semantics id="S2.SS2.SSS3.p1.4.m4.1a"><mi id="S2.SS2.SSS3.p1.4.m4.1.1" xref="S2.SS2.SSS3.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.4.m4.1b"><ci id="S2.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.4.m4.1d">italic_n</annotation></semantics></math> correspond to the counts of minor errors, major errors, critical errors, and the total number of words, respectively.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="MQM=1\textendash\frac{n_{\text{minor}}\textendash 5n_{\text{major}}\textendash
1%
0n_{\text{critical}}}{n}." class="ltx_Math" display="block" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.2.2.cmml">M</mi><mo id="S2.E2.m1.1.1.1.1.2.1" xref="S2.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.2.3.cmml">Q</mi><mo id="S2.E2.m1.1.1.1.1.2.1a" xref="S2.E2.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.1.1.2.4" xref="S2.E2.m1.1.1.1.1.2.4.cmml">M</mi></mrow><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><mn id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml">1</mn><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.1.1.3.3" mathvariant="normal" xref="S2.E2.m1.1.1.1.1.3.3.cmml">–</mi><mo id="S2.E2.m1.1.1.1.1.3.1a" xref="S2.E2.m1.1.1.1.1.3.1.cmml">⁢</mo><mfrac id="S2.E2.m1.1.1.1.1.3.4" xref="S2.E2.m1.1.1.1.1.3.4.cmml"><mrow id="S2.E2.m1.1.1.1.1.3.4.2" xref="S2.E2.m1.1.1.1.1.3.4.2.cmml"><msub id="S2.E2.m1.1.1.1.1.3.4.2.2" xref="S2.E2.m1.1.1.1.1.3.4.2.2.cmml"><mi id="S2.E2.m1.1.1.1.1.3.4.2.2.2" xref="S2.E2.m1.1.1.1.1.3.4.2.2.2.cmml">n</mi><mtext id="S2.E2.m1.1.1.1.1.3.4.2.2.3" xref="S2.E2.m1.1.1.1.1.3.4.2.2.3a.cmml">minor</mtext></msub><mo id="S2.E2.m1.1.1.1.1.3.4.2.1" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.1.1.3.4.2.3" mathvariant="normal" xref="S2.E2.m1.1.1.1.1.3.4.2.3.cmml">–</mi><mo id="S2.E2.m1.1.1.1.1.3.4.2.1a" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><mn id="S2.E2.m1.1.1.1.1.3.4.2.4" xref="S2.E2.m1.1.1.1.1.3.4.2.4.cmml">5</mn><mo id="S2.E2.m1.1.1.1.1.3.4.2.1b" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><msub id="S2.E2.m1.1.1.1.1.3.4.2.5" xref="S2.E2.m1.1.1.1.1.3.4.2.5.cmml"><mi id="S2.E2.m1.1.1.1.1.3.4.2.5.2" xref="S2.E2.m1.1.1.1.1.3.4.2.5.2.cmml">n</mi><mtext id="S2.E2.m1.1.1.1.1.3.4.2.5.3" xref="S2.E2.m1.1.1.1.1.3.4.2.5.3a.cmml">major</mtext></msub><mo id="S2.E2.m1.1.1.1.1.3.4.2.1c" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><mi id="S2.E2.m1.1.1.1.1.3.4.2.6" mathvariant="normal" xref="S2.E2.m1.1.1.1.1.3.4.2.6.cmml">–</mi><mo id="S2.E2.m1.1.1.1.1.3.4.2.1d" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><mn id="S2.E2.m1.1.1.1.1.3.4.2.7" xref="S2.E2.m1.1.1.1.1.3.4.2.7.cmml">10</mn><mo id="S2.E2.m1.1.1.1.1.3.4.2.1e" xref="S2.E2.m1.1.1.1.1.3.4.2.1.cmml">⁢</mo><msub id="S2.E2.m1.1.1.1.1.3.4.2.8" xref="S2.E2.m1.1.1.1.1.3.4.2.8.cmml"><mi id="S2.E2.m1.1.1.1.1.3.4.2.8.2" xref="S2.E2.m1.1.1.1.1.3.4.2.8.2.cmml">n</mi><mtext id="S2.E2.m1.1.1.1.1.3.4.2.8.3" xref="S2.E2.m1.1.1.1.1.3.4.2.8.3a.cmml">critical</mtext></msub></mrow><mi id="S2.E2.m1.1.1.1.1.3.4.3" xref="S2.E2.m1.1.1.1.1.3.4.3.cmml">n</mi></mfrac></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" lspace="0em" xref="S2.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></eq><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><times id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2.1"></times><ci id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2">𝑀</ci><ci id="S2.E2.m1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.3">𝑄</ci><ci id="S2.E2.m1.1.1.1.1.2.4.cmml" xref="S2.E2.m1.1.1.1.1.2.4">𝑀</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><times id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></times><cn id="S2.E2.m1.1.1.1.1.3.2.cmml" type="integer" xref="S2.E2.m1.1.1.1.1.3.2">1</cn><ci id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3">–</ci><apply id="S2.E2.m1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.1.1.1.1.3.4"><divide id="S2.E2.m1.1.1.1.1.3.4.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4"></divide><apply id="S2.E2.m1.1.1.1.1.3.4.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2"><times id="S2.E2.m1.1.1.1.1.3.4.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.1"></times><apply id="S2.E2.m1.1.1.1.1.3.4.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.4.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.4.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.2.2">𝑛</ci><ci id="S2.E2.m1.1.1.1.1.3.4.2.2.3a.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.2.3"><mtext id="S2.E2.m1.1.1.1.1.3.4.2.2.3.cmml" mathsize="70%" xref="S2.E2.m1.1.1.1.1.3.4.2.2.3">minor</mtext></ci></apply><ci id="S2.E2.m1.1.1.1.1.3.4.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.3">–</ci><cn id="S2.E2.m1.1.1.1.1.3.4.2.4.cmml" type="integer" xref="S2.E2.m1.1.1.1.1.3.4.2.4">5</cn><apply id="S2.E2.m1.1.1.1.1.3.4.2.5.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.5"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.4.2.5.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.5">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.4.2.5.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.5.2">𝑛</ci><ci id="S2.E2.m1.1.1.1.1.3.4.2.5.3a.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.5.3"><mtext id="S2.E2.m1.1.1.1.1.3.4.2.5.3.cmml" mathsize="70%" xref="S2.E2.m1.1.1.1.1.3.4.2.5.3">major</mtext></ci></apply><ci id="S2.E2.m1.1.1.1.1.3.4.2.6.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.6">–</ci><cn id="S2.E2.m1.1.1.1.1.3.4.2.7.cmml" type="integer" xref="S2.E2.m1.1.1.1.1.3.4.2.7">10</cn><apply id="S2.E2.m1.1.1.1.1.3.4.2.8.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.8"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.4.2.8.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.8">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.4.2.8.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.8.2">𝑛</ci><ci id="S2.E2.m1.1.1.1.1.3.4.2.8.3a.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2.8.3"><mtext id="S2.E2.m1.1.1.1.1.3.4.2.8.3.cmml" mathsize="70%" xref="S2.E2.m1.1.1.1.1.3.4.2.8.3">critical</mtext></ci></apply></apply><ci id="S2.E2.m1.1.1.1.1.3.4.3.cmml" xref="S2.E2.m1.1.1.1.1.3.4.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">MQM=1\textendash\frac{n_{\text{minor}}\textendash 5n_{\text{major}}\textendash
1%
0n_{\text{critical}}}{n}.</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">italic_M italic_Q italic_M = 1 – divide start_ARG italic_n start_POSTSUBSCRIPT minor end_POSTSUBSCRIPT – 5 italic_n start_POSTSUBSCRIPT major end_POSTSUBSCRIPT – 10 italic_n start_POSTSUBSCRIPT critical end_POSTSUBSCRIPT end_ARG start_ARG italic_n end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS4.5.1.1">II-B</span>4 </span>Discussion of Difficulty</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">HTER places high demands on the annotators’ language skills and editing expertise, requiring a deep understanding of the linguistic characteristics of both the source and target languages. Its results may be influenced by the annotators’ editing styles, leading to inconsistencies among different annotators. DA necessitates direct scoring by annotators, who need training to ensure consistent application of the scoring standards. MQM demands high levels of professional knowledge from annotators, who must have a thorough understanding of error types and perform precise annotations.
Overall, no one paradigm perfectly resolves the intrinsic trade-offs in MTQE. Ongoing research toward standardized best practices aims to combine these metrics’ respective strengths.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Shared Tasks</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">QE shared tasks aim to advance the SOTA in QE by providing standardized datasets and evaluation metrics. These tasks cover QE at different levels and have diverse objectives. The popular QE shared tasks can be categorized into word-level, sentence-level, document-level, and explainable QE, each with its own objectives, evaluation metrics, and rationale.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.5.1.1">II-C</span>1 </span>Word-level QE Shared Tasks</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The objective of word-level QE is to use words as the basic unit of assessment, automatically identifying the correctness of each word’s position in a translated sentence, as well as detecting any mistranslation and omission phenomena, with reference to the source sentence. The input for this task includes the source text and the MT text, while the output is a series of labeled tag sequences (including source tags, MT tags, and gap tags). Each tag corresponds to each word or gap in the translated sentence, indicating whether there is an error at that location.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">After summarizing the word-level QE shared tasks of the WMT in the past four years, we have categorized them into three types: classification, regression, and fine-grained error span detection. Classification tasks involve categorizing both the source and the target, with a further distinction between word classification and gap classification; translations that are correct are marked as OK, while those with errors are marked as BAD. Regression tasks employ semi-supervised or unsupervised models to score words based on sentence-level scores, with a threshold set to label words above it as OK and those below it as BAD.
Fine-grained error span detection is a new task introduced in WMT2023 QE<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://wmt-qe-task.github.io/subtasks/task2/</span></span></span>, which categorizes translated words into no errors, minor errors, and major errors, and predicts error spans by linking indices of words within the same category through post-processing.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.1">The primary evaluation metric for word-level QE is the Matthews correlation coefficient (MCC) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lin2004automatic</span>]</cite>, accompanied by the F1-score as a secondary metric. MCC is particularly suited for binary classification models and datasets with an uneven distribution. It is used to measure the correlation between incorrectly translated words and manual annotations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.5.1.1">II-C</span>2 </span>Sentence-level QE Shared Tasks</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Sentence-level QE aims to predict the quality score for each LP, indicative of the translation quality, akin to a regression task in ML. It employs annotation methods such as HTER, DA, and MQM <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">freitag2021experts</span>]</cite> to assess the quality of translations. The sentence-level QE tasks at WMT2021<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.statmt.org/wmt21/quality-estimation-task.html</span></span></span>, WMT2022, and WMT2023<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://wmt-qe-task.github.io/subtasks/task1/</span></span></span> have employed HTER, DA, and MQM annotations.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.2">The primary evaluation metric for sentence-level QE is the Spearman’s <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lin2004automatic</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">specia2009estimating</span>]</cite> rank correlation coefficient (Spearman’s <math alttext="\rho" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p2.1.m1.1"><semantics id="S2.SS3.SSS2.p2.1.m1.1a"><mi id="S2.SS3.SSS2.p2.1.m1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.1.m1.1b"><ci id="S2.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p2.1.m1.1d">italic_ρ</annotation></semantics></math>), while the Pearson’s and Kendall’s correlation coefficients are used as auxiliary evaluation metrics. The Spearman’s <math alttext="\rho" class="ltx_Math" display="inline" id="S2.SS3.SSS2.p2.2.m2.1"><semantics id="S2.SS3.SSS2.p2.2.m2.1a"><mi id="S2.SS3.SSS2.p2.2.m2.1.1" xref="S2.SS3.SSS2.p2.2.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.2.m2.1b"><ci id="S2.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p2.2.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.2.m2.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS2.p2.2.m2.1d">italic_ρ</annotation></semantics></math> does not rely on assumptions of normality and equal variance in translation quality scores and is less affected by outliers. Thus, it provides a better reflection of the correlation between the translation quality predicted by the MT model and manual annotation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS3.5.1.1">II-C</span>3 </span>Document-level QE Shared Tasks</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">Compared to finer-grained word-level and sentence-level QE, document-level QE is much more complex and requires a significant amount of data resources. The core objective of document-level QE is to perform QE on translation documents, where “document” often refers to a text containing at least 3 sentences, rather than just a single document. Traditional MT tasks typically treat a single sentence as the basic unit of input and translation, overlooking the interdependence between sentences within a document. This approach may result in a lack of semantic coherence throughout the entire document. Since its development in 2016, document-level QE tasks have primarily focused on two types of prediction targets. One type involves calculating quality scores using two-step PE methods, while the other involves predicting MQM scores computed by MQM, as well as word-level and sentence-level error types.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS3.p2">
<p class="ltx_p" id="S2.SS3.SSS3.p2.1">Predicting both two-step PE scores and MQM scores use Pearson’s correlation coefficient as the main evaluation metric, along with Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) as supplementary metrics. On the other hand, predicting word-level error types uses F1-score as the evaluation metric.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS4.5.1.1">II-C</span>4 </span>Explainable QE Shared Tasks</h4>
<div class="ltx_para" id="S2.SS3.SSS4.p1">
<p class="ltx_p" id="S2.SS3.SSS4.p1.1">In QE, interpretability is important for enhancing user trust and facilitating error analysis. Unlike sentence-level QE, which focuses on overall quality scores, explainable QE is concerned mainly with errors in the translation. This paper categorizes explainable QE into two scenarios. The first scenario aims to predict sentence-level binary scores to signal whether the translation contains critical errors. These errors, mainly caused by mistranslations, hallucinations, and deletions of content from the source sentence, could potentially lead to misinformation in areas like health, safety, law, reputation, and religion. Based on the scores, users can determine whether critical errors have occurred in the translation. The second scenario provides sentence-level quality scores that signal the presence of translation errors in the sentence but do not identify which specific words are mistranslated. These scores help users understand why a sentence may be deemed low quality.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS4.p2">
<p class="ltx_p" id="S2.SS3.SSS4.p2.1">In explainable QE, Recall at Top K is the primary evaluation metric, which measures the model’s ability to detect and rank mistranslated words within the top K predictions made by the MT model. Area Under the Curve (AUC) and Average Precision are used as auxiliary evaluation metrics. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.SS3.SSS4.p3">
<p class="ltx_p" id="S2.SS3.SSS4.p3.1">In conclusion, QE shared tasks are designed with different objectives in mind, focusing on defining quality indicators across various aspects. Each task is equipped with unique evaluation metrics to measure model performance. Word-level QE is akin to classification tasks, where words are labeled as OK or BAD. Sentence-level QE is similar to regression tasks, aiming to predict the quality scores for translated sentences. Document-level QE is more complex, responsible for scoring entire translated documents or text blocks containing multiple sentences. Explainable QE, on the other hand, is mainly concerned with errors in the translation, rather than the quality score of the translation. It not only identifies specific types of errors but also points out the words where translation errors exist based on the scores given to sentences, although it does not specify exactly which word is erroneous.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods of Quality Estimation</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section reviews relevant research work within the three main categories of methods that have emerged throughout the evolution of QE. It discusses the advantages and limitations of the respective methods within each category and provides a brief comparison between different approaches.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Quality Estimation Based on Handcrafted Features</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Before 2009, QE research primarily focused on predicting quality labels for the output of Statistical Machine Translation (SMT) using handcrafted features <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">blatz2004confidence</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">ueffing2005word</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">ueffing2007word</span>]</cite>. Subsequently, the focus of QE research shifted towards predicting human-annotated quality scores. For instance, the QuEst <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">specia-etal-2013-quest</span>]</cite> framework
utilized a feature extraction module to extract quality labels from the source and translated text. These features
were then applied to ML algorithms to construct QE systems. de Souza et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">de2014fbk</span>]</cite> used a supervised tree-based ensemble learning method to predict PE effort and time under various features and BLSTM-RNNs to predict word-level labels.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">QuEst++ <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">specia2015multi</span>]</cite> is an improved and expanded version of QuEst, with added feature extraction modules designed for word-level and document-level QE. It integrates predictions at three different levels into a single workflow, facilitating interactions between word-level, sentence-level, and document-level QE. Additionally, QuEst++ incorporates sequence labeling learning algorithms for word-level QE. This tool can be conveniently extended with new features to meet the requirements of different text levels, offering high flexibility.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Quality Estimation Based on Deep Learning</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since the 2010s, deep learning technologies have been widely applied in the field of NLP, and from around 2015, they began to be integrated into QE methodologies. These approaches can be categorized into those based on classic deep learning techniques and those incorporating pre-trained LMs.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Classic Deep Learning Methods</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">With the advancement of QE, the advent of word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">turian2010word</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mikolov2013linguistic</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mikolov2013efficient</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">pennington2014glove</span>]</cite> and neural machine translation (NMT) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">bahdanau2014neural</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">sutskever2014sequence</span>]</cite> technologies has led some researchers to apply neural networks to QE tasks. The progression from initially utilizing neural networks for feature extraction to the emergence of fully neural network-based QE systems has greatly enhanced the performance of QE systems.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Besides utilizing the handcrafted features from QuEst, SHAH et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shah2015shef</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">shah2015investigating</span>]</cite> also employed additional word-level QE features extracted from Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">mikolov2013linguistic</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mikolov2013efficient</span>]</cite> embeddings and the similarity in the embedding space between source and target language words. They combined language model probabilities generated from trained continuous space models with these handcrafted features for sentence-level QE. Furthermore, Scarton et al. proposed word embedding features <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">scarton2016word</span>]</cite>, discourse features, and features extracted from pseudo-reference translations <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">scarton2015searching</span>]</cite> for document-level QE. Inspired by their work, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2017improving</span>]</cite> proposed the use of sentence embedding features and cross-entropy features to enhance the correlation of QE with human evaluations and investigated several factors affecting the performance of QE systems.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Subsequently, some researchers explored the use of neural networks solely for feature extraction and QE. QUETCH <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kreutzer-etal-2015-quality</span>]</cite> is an early example of this approach, utilizing pretrained word representations and a Deep Neural Network (DNN) architecture. The QUETCH
comprises an input layer, lookup tables, a multilayer perceptron (MLP), and an output layer. It feeds bilingual context representations through a fixed-size word window into the MLP, ultimately completing the word-level QE task via the output layer. However, its effectiveness did not match that of QUETCH+, which integrated additional baseline features. Expanding on QUETCH, Martins et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">martins-etal-2016-unbabels</span>]</cite> introduced a 200-unit bi-directional Gated Recurrent Unit (BiGRU) network and stacked feed-forward neural networks, subsequently incorporating POS tags for both source and target words to achieve the best performance of that time. Similar to QUETCH, Patel et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">patel2016translation</span>]</cite> made a switch from DNN to RNN, utilizing LSTM and GRU for extracting representations of bilingual sequences, and introduced sub-labels to address the challenge of label imbalance.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1">While the QUETCH <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kreutzer-etal-2015-quality</span>]</cite> approach relies entirely on neural networks for feature extraction, it requires bilingual alignment information, which is typically obtained via statistical methods and is prone to significant errors. With the advancement of deep learning technologies, the trend in QE research has been gradually shifting towards completely neural network-based methods.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1">In 2016, Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kim-lee-2016-recurrent</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kim2016recurrent</span>]</cite> made the first attempt to use an NMT model for QE, proposing the inaugural purely neural approach for sentence-level, word-level, and phrase-level QE that does not require manually extracted features. In 2017, Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kim2017predictor</span>]</cite> presented a more in-depth study and named it the Predictor-Estimator (PredEst) model, which is a method to address the issues of expensive QE annotations and limited annotated QE data. It consists of two components: the predictor and the estimator. The predictor is a neural word prediction model trained using parallel corpora. It masks the target word, feeds the source language and the corrupted target language into bidirectional RNN (Bi-RNN), and predicts the probability distribution of the masked word. On the other hand, the estimator is a neural QE model trained on QE data, extracting QE feature vectors (QEFVs) and training them on a feedforward network. QEFVs are processed by FNN, RNN, or Bi-RNN to obtain hidden representations, which are then used to predict quality labels for sentence, phrase, or word-level tasks. Later, to train the model effectively, Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kim-etal-2017-predictor</span>]</cite> introduced stack propagation and multi-level task algorithms to improve the original method. In 2018, Ive et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ive2018deepquest</span>]</cite> proposed the deepQuest framework for sentence-level and document-level QE, marking the first purely neural approach for document-level QE, which for the first time attempted to experiment with the outputs of both SMT and NMT. Upon testing, the framework proved to be faster and more cost-effective, and greatly improved the performance of the document-level QE framework.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.1">Martins et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">martins2017pushing</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">martins2017unbabel</span>]</cite> introduced a STACKEDQE system that stacked both linear and neural systems in the WMT17 word-level QE task, followed by combining APE with word-level QE to create an APEQE system. Ultimately, they merged these two systems to form the FULLSTACKEDQE system tailored for word-level QE and extended the FULLSTACKEDQE to sentence-level QE. These systems all achieved commendable results. Building on the approach of Martins et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">martins2017pushing</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">martins2017unbabel</span>]</cite>, Hokamp et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hokamp2017ensembling</span>]</cite> incorporated features that had been demonstrated to be effective for word-level QE into the input of an NMT system, thus proposing the APE-QE model. This unified APE with word-level QE models and achieved the best performance of the time in both APE and QE tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p7">
<p class="ltx_p" id="S3.SS2.SSS1.p7.1">As the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/VaswaniSPUJGKP17</span>]</cite> model has garnered significant success in the field of MT, Fan, Wang, et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-1807-09433</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2018alibaba</span>]</cite> developed a Bilingual Expert model based on a bidirectional Transformer and a PredEst architecture that includes a word prediction module and a QE module. The word prediction module leverages prior knowledge obtained from pretraining on a large parallel corpus and the joint latent representations between the source language and the translation for token prediction, extracting a set of features. Then, they introduce mismatch features that measure the discrepancy between the prior knowledge obtained from well-trained Bilingual Expert and the targets in the QE dataset to train the QE module, which uses a bidirectional LSTM model, achieving SOTA performance at that time. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang-etal-2020-hw-tscs</span>]</cite> employed a pre-trained Transformer as the predictor and integrated Bottleneck Adapter Layers (BAL) for efficient transfer learning, with specific classifier and regressor as the estimator. They also conducted joint training for word and sentence-level tasks using a unified model and proposed a pseudo PE assisted QE method. This demonstrated the effectiveness of using pre-trained NMT models for transfer learning in QE tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p8">
<p class="ltx_p" id="S3.SS2.SSS1.p8.1">However, Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cui2021directqe</span>]</cite> argued that the gap between data quality and training objectives in the PredEst framework hindered its ability to benefit from parallel corpora. Consequently, they proposed a framework called DirectQE, which includes a generator for creating pseudo QE data and a detector pre-trained with these pseudo data. This framework allows for pre-training using large parallel corpora and fine-tuning on real QE data, thereby addressing the issues inherent in the PredEst framework.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Incorporating Pre-trained Language Models Methods</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">With the emergence and development of pre-trained LMs such as ELMo <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">peters2018deep</span>]</cite>, BERT <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">devlin2018bert</span>]</cite>, XLM <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lample2019cross</span>]</cite>, and XLM-R <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">conneau2019unsupervised</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-1911-02116</span>]</cite>, some studies have begun to integrate pre-trained LMs into QE models. This integration has enabled better extraction of quality vectors from source texts and translated texts, thereby enhancing the performance of QE systems.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">Kepler et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kepler2019unbabel</span>]</cite> expanded OpenKiwi <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kepler2019openkiwi</span>]</cite> into a Transformer-based PredEst model, replacing the predictor with the pre-trained LMs BERT and XLM, and proposed an ensemble method using the POWELL technique to combine word-level and sentence-level predictions. Moreover, they suggested a simple technique for converting word labels into document-level predictions. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wu2020tencent</span>]</cite> extended the OpenKiwi by integrating PredEst models based on XLM and Transformer in their submission to WMT20. The former predictor generates both masked and nonmasked representations, while the latter produces only nonmasked representations. The estimator is trained using either LSTM or Transformer, employing top-K and multi-head attention strategies to enhance sentence feature representation. Ranasinghe et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-2011-01536</span>]</cite> proposed TransQuest, a PredEst model aimed at reducing the dependence of sentence-level QE on large-scale parallel corpora. TransQuest does not use parallel data to pre-train the predictor but instead directly employs SOTA cross-lingual embedding models like XLM-R <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">conneau2019unsupervised</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">DBLP:journals/corr/abs-1911-02116</span>]</cite> to encode the source and target sentences. It consists of two neural networks: MonoTransQuest (MTransQuest) and SiameseTransQuest (STransQuest). MTransQuest uses a single XLM-R model to encode concatenated source and target sentences, whereas STransQuest adopts a Siamese architecture, using separate XLM-R models for the source and translation. Both models use mean squared error loss as the objective function and have shown improved results with specific pooling strategies.
Zerva et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zerva2021unbabel</span>]</cite> used a pre-trained multilingual encoder combined with adapters, training multilingual models on the OpenKiwi <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kepler2019openkiwi</span>]</cite> PredEst and found that adapter tuning can resist overfitting. Additionally, they demonstrated that integrating uncertainty information and using out-of-domain data for pre-training can improve QE system performance.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou2019source</span>]</cite> primarily investigated the application of pre-trained translation models in QE and compared the effectiveness of a bilingual expert, ELMo, and BERT on QE tasks. Yankovskaya et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yankovskaya2019quality</span>]</cite> contrasted two approaches: one using only BERT and LASER <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">artetxe2019massively</span>]</cite> embeddings as features, and the other additionally incorporating log probability features from MT systems. Their research demonstrated the importance of the log probabilities from MT systems.
</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1">In 2020, Rei et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rei2020comet</span>]</cite> introduced COMET, a neural framework for training multilingual and adaptable MT evaluation models, which is often used for reference-based evaluation to generate predictive estimates of human judgments such as HTER, DA, and MQM. The COMET framework supports two different architectures: an Estimator model and a Translation Ranking model, both consisting of a cross-lingual encoder and pooling layers, with the fundamental difference being the training objective. The Estimator model, which is the most commonly used, is trained to directly regress to a quality score, while the Translation Ranking model is trained to minimize the distance between a “better” hypothesis and its corresponding reference translation and source language.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1">In 2022, Rei et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rei2022cometkiwi</span>]</cite> combined the strengths of COMET and OpenKiwi <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kepler2019openkiwi</span>]</cite> by connecting COMET with the PredEst architecture of OpenKiwi, and equipped it with a word-level sequence tagger and explanation extractor, forming COMETKIWI for QE. COMETKIWI pre-trained the model on metrics data with the learning objective proposed by UniTE model <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wan2022unite</span>]</cite>, which incorporates reference translation into the training, serving as a form of data augmentation. Additionally, COMETKIWI proposed an interpretability method using attention and gradient information and further refined the influence of attention heads on predictions through a Head Mix module. COMETKIWI also demonstrated the effectiveness of few-shot learning, achieving significant improvements in model performance with only 500 samples.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Quality Estimation Based on Large Language Models</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">With the development of LLMs, more researchers are turning their attention to utilizing LLMs for QE. The current methods can be roughly divided into five types, which have contributed significantly to the advancement of QE.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.5.1.1">III-C</span>1 </span>Direct prediction based on content generated by LLMs</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Kocmi and Federmann <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kocmi2023large</span>]</cite> proposed GEMBA, a GPT-based <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">brown2020language</span>]</cite> translation quality assessment metric that uses single-step prompting and can be applied to scenarios with reference translations as well as to QE. They evaluated 9 different GPT models and concluded that only GPT-3.5 and larger models are capable of performing QE. GEMBA focuses on zero-shot prompting, with the authors employing 4 different prompt templates to execute quality assessments for both reference-based and nonreference-based translation modes. GEMBA predicts scores directly based on content generated by LLMs, assessing each segment independently, then averaging the scores of all segments to obtain a final system-level score, achieving SOTA performance at the system level, but lacking in segment-level analysis. To enhance the performance of LLMs in quality assessment, Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lu2023error</span>]</cite> introduced Error Analysis Prompting (EAPrompt), a new prompting method that combines Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wei2022chain</span>]</cite> with EA <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lu2022toward</span>]</cite>. Using ChatGPT, this approach predicts the degree and number of errors and provides a score based on the severity of those errors, creating MQM-like assessments. It achieved better results on CPT-3.5-turbo than GEMBA.
Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2023knowledge</span>]</cite> introduced the Knowledge-Prompted Estimator (KPE), which is a CoT approach that integrates three single-step prompting techniques, using LLMs to predict fluency, word-level similarity <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2023implicit</span>]</cite>, and sentence-level similarity <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2023teachersim</span>]</cite>, resulting in better performance for segment-level QE. Furthermore, KPE also demonstrated its advantages in terms of interpretability <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">tao2022crossqe</span>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.5.1.1">III-C</span>2 </span>Based on the generative probabilities of LLMs</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2023towards</span>]</cite> leveraged various prompts and examples to obtain multiple generation probabilities for a certain source sentence and its corresponding translated sentence in the GPT-3.5 interface. They then calculated a more accurate measure of uncertainty as an assessment of the quality of the translated sentence by computing the mean and variance of these probabilities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS3.5.1.1">III-C</span>3 </span>Leveraging LLMs to generate pseudo data</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2023instructscore</span>]</cite> introduced INSTRUCTSCORE, a method that learns interpretable text generation metrics without the need for human-annotated scores, by constructing MQM-like data using the knowledge provided by GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">openai2023gpt4</span>]</cite> to train the LLaMA model <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">touvron2023llama</span>]</cite>. Moreover, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">HUANG2024102022</span>]</cite> also leveraged LLMs to corrupt reference sentences, subsequently generating a fluent sentence from the corrupted one and outputting it to obtain a noisy negative view. Since this approach does not require data annotation, it has strong generalization capabilities.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS4.5.1.1">III-C</span>4 </span>LLMs as the foundation for QE models</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">Gladkoff et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">gladkoff2023predicting</span>]</cite> fine-tuned LLMs using the OpenAI API interface to assess whether translations require editing. Similarly, Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2023instructscore</span>]</cite>, as mentioned above, also utilized pseudo data generated by GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">openai2023gpt4</span>]</cite> to train the LLaMA model <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">touvron2023llama</span>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS5.5.1.1">III-C</span>5 </span>Retrieval-based methods</h4>
<div class="ltx_para" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">This is an auxiliary augmentation strategy. Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2023towards</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">HUANG2024102022</span>]</cite>, as previously mentioned, used BM25 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">robertson2009probabilistic</span>]</cite> to retrieve similar parallel corpora as examples to enhance the translation knowledge of LLMs.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Finding</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Based on our observations of these methods, we have identified the following findings regarding the current challenges and developments in QE:</p>
</div>
<div class="ltx_para" id="S4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Data Scarcity: There is a scarcity of manually annotated data, particularly for low-resource languages. Acquiring sufficient annotated data involves significant costs, which to a large extent hinders the progress of QE research.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Insufficient Interpretability: Earlier QE methods lacked interpretability, making it difficult to identify specific types of errors and their locations. In contrast, LLMs possess a strong knowledge base and learning capabilities. Future research should focus more on leveraging LLMs for enhancing the interpretability of QE.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Word-level and document-level QE methods are rare. Current QE approaches predominantly focus on sentence-level, with limited work targeting word-level and document-level QE, particularly with word-level methods being few in number and lacking in performance. However, word-level QE can extract more fine-grained information, and future research should pay more attention to word-level QE.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Pre-trained LMs and LLMs require a lot of hardware resources. Many research teams are unable to independently pre-train LMs due to a lack of sufficient hardware resources, forcing them to rely on open-source pre-trained LMs, which impedes the development of QE.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Lack of standardized evaluation metrics: Due to the subjectivity of the QE task and varying preferences for translation quality, the absence of uniform evaluation metrics makes it challenging to compare and integrate model performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Over the past 20 years, significant progress has been made in QE. As an application that can evaluate the quality of translated texts in real time without the need for reference translations, QE has strong practicality and plays a significant role in advancing the development of MT. This paper provides a comprehensive introduction and analysis of QE, offering an extensive overview of datasets, annotation methods, shared tasks, and methodologies. Specifically, the paper presents the specific concepts and details of word-level, sentence-level, document-level, and explainable QE shared tasks. It categorizes the methods developed throughout the evolution of QE into those based on handcrafted features, those grounded in deep learning, and those leveraging LLMs, further subdividing the deep learning-based methods into classic deep learning and those incorporating pre-trained models. This paper provides a detailed account of the advantages and limitations of each type of method and offers a simple comparison of different approaches. Finally, the paper discusses the current challenges in the QE field and suggests future research directions.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Mar 21 04:01:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
