<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.10405] Generative Visual Question Answering</title><meta property="og:description" content="Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The curreâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generative Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generative Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.10405">

<!--Generated on Wed Feb 28 17:28:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generative Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ethan Shen
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ethans03@uw.edu</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Scotty Singh
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">asingh06@uw.edu</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bhavesh Kumar
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">bkumar2@uw.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Multi-modal tasks involving vision and language in deep learning continue to rise in popularity and are leading to the development of newer models that can generalize beyond the extent of their training data. The current models lack temporal generalization which enables models to adapt to changes in future data. This paper discusses a viable approach to creating an advanced Visual Question Answering (VQA) model which can produce successful results on temporal generalization. We propose a new data set, GenVQA, utilizing images and captions from the VQAv2 and MS-COCO dataset to generate new images through stable diffusion. This augmented dataset is then used to test a combination of seven baseline and cutting edge VQA models. Performance evaluation focuses on questions mirroring the original VQAv2 dataset, with the answers having been adjusted to the new images. This paperâ€™s purpose is to investigate the robustness of several successful Visual Question Answering (VQA) models to assess their performance on future data distributions. Model architectures are analyzed to identify common stylistic choices that improve generalization under temporal distribution shifts. This research highlights the importance of creating a large-scale future shifted dataset. This data can enhance the robustness of VQA models, allowing their future peers to have improved ability to adapt to temporal distribution shifts.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past several years, multi-modal tasks involving both vision and language have gained great popularity in the deep learning space. Visual Question Answering (VQA), which involves answering questions about an image, has become one common standard for vision and language models. The ultimate goal is to create models that generalize to data beyond what they are trained on. Temporal generalization is one key aspect of models that allows them to be deployed and used in real life situations.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we investigate several successful VQA models to assess their robustness on future data distribution shifts. We create a new dataset from the VQAv2 validation set by applying transformations to images to model visual changes in data that may be seen in the future. We test our dataset on a combination of seven baseline and state of the art models. Similar to the VQAv2 dataset, we consider model performance on open-ended questions but use the GenVQA dataset to draw conclusions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We make several findings. We find that models that perform better on the original VQAv2 test dataset tend to be more robust. We also examine robustness with respect to overall accuracy as well as stability of predictions. In addition, we analyze the architectures of the models to identify common design choices that may help models generalize to temporal distribution shifts. Our work serves as a proof of concept for the development of a large future shifted dataset, which can be used to benchmark and improve the robustness of VQA models.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.10405/assets/gen1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Dataset Example: original MS-COCO image (left) and generated image (right). The image caption used for generation was â€œFuturistic. A large white couch in a living room in front of a TV. Living room that includes television, chair and multiple windows. A living room with furniture and a small tv in the corner. a living room with a tv a couch and a dresser A living room couch facing a small television.â€</span></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2307.10405/assets/gen3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Example where the a modelâ€™s response changes between the original image and the generated image.</span></figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Work</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p"><span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_bold">Generalizability of ImageNet Classifiers</span>. The robustness of image classifiers has been a persistant field of research for the last five years. For example, similar to our approach, researchers have created new datasets for models trained on ImageNet and CIFAR-10 by utilizing the sources from which these datasets were created to closely mimic the same data distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Their work has found that there is often a large drop in performance when evaluating models on the new dataset, though relative performance ranking of models from the original dataset is often maintained.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p"><span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_bold">Temporal Distribution Shifts</span>. Groups have also investigated the effects of temporal distribution shifts on language models. Experiments have tested models on data that was created/published after the training period and found that model performance on future data decreases over time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In addition, it has been shown that model size does not affect the robustness, and smaller but more up-to-date models often perform significantly better than larger outdated models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p"><span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_bold">Distributional Robustness for Question Answering</span>. Additionally, there has been work done to determine language model robustness in the task of question answering. This work has shown that zero-shot learning methods often provide better model robustness than fine-tuning methods. Additionally, researchers have found evidence that the architecture of language models do not have an effect on robustness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p"><span id="S1.SS1.p4.1.1" class="ltx_text ltx_font_bold">Robust Question Answering Datasets</span>.
With the rise of VQA, researchers have unsurprisingly grown interested in developing new benchmarks to test VQA robustness. Similar to how a model trained to recognize a flip phone will often fail to recognize an iPhone, previous benchmarks for VQA models represent an inaccurate reflection of their ability to perform in todayâ€™s world. Indeed, the original VQAv2 dataset is almost six years old. This has led to the development of several new VQA datasets. One is VQA Rephrased, which replaced each of 40,504 questions in the original VQA 2.0 validation dataset with three new human generated questions. When state of the art models were tested against the dataset, all performed significantly worse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In 2020, Causal VQA used GANs to remove objects from VQA test images to evaluate the reliance of models on shaky and incorrect correlations. Researchers found that models produced extremely inconsistent answers when faced with the new dataset, especially when asked counting questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. There has also been an increased interest in testing the ability of VQA models to legitimately reason about images, driving the creation of datasets such as TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our approach builds on top of the original VQAv2 dataset by creating a new evaluation dataset, GenVQA, by applying temporal shifts to the original MS-COCO images. We utilize the VQAv2 validation set as a starting point for creating our new dataset. The reason we use the validation split instead of the test split from VQAv2 is because the test set answer annotations are not publicly available, preventing us from accurately evaluating a modelâ€™s performance on our new dataset. The VQAv2 dataset contains 2,143,540 image annotations, 214,354 questions, and 40,504 images from MS-COCO. VQAv2 contains only open-ended questions. There are no multiple choice question types in VQAv2. The dataset provides 10 ground truth answers for each question to evaluate performance. For a predicted answer to be marked correct, it must exactly match at least 3 of the ground truth answers.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset Creation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Question answering robustness with respect to linguistic shifts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and object removal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has been studied, however the performance of VQA models on generative input data is still unknown. In 2021, an Adversarial VQA dataset came close to breaching this area of study; however, the researchers opted to use human volunteers instead of a generative model to create adversarial questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We seek to create a new, generated dataset for question answering to test the robustness of state of the art VQA models. We build our dataset using existing VQAv2 and MS-COCO images. First, we randomly sample 200 questions from each VQA question category in the validation dataset: yes/no, number, and other. We do this because the style of questions has been shown to greatly affect the accuracy of a modelâ€™s answers, with numerical questions often the most difficult. We want to see if that relationship persists with our dataset. Next, we use API calls to Stable Diffusion 2.1 to future shift each questionâ€™s corresponding image. For each image, we concatenate all of its MS-COCO captions to preserve as much semantic information as possible. We then add the phrase â€Futuristic.â€ to each aggregate caption before passing it into the generative model. Finally, we revised the correct answer annotations for each question to reflect the new image on an as-needed basis. The final dataset consists of 600 questions and images.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Baseline</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To create a consistent baseline for all models we evaluate each model on the original VQAv2 test dataset. We then compare these accuracies against the performance of the models on our dataset. Included below are the baseline performances of the models from our pre-trained model suite.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Accuracy (VQAv2 Test) %</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<th id="S2.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BLIP-2</th>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">82.30</td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<th id="S2.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">OFA</th>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_align_center">78.10</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr">
<th id="S2.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VLMo</th>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_align_center">76.64</td>
</tr>
<tr id="S2.T1.2.5.4" class="ltx_tr">
<th id="S2.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ALBEF</th>
<td id="S2.T1.2.5.4.2" class="ltx_td ltx_align_center">75.84</td>
</tr>
<tr id="S2.T1.2.6.5" class="ltx_tr">
<th id="S2.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ViLT</th>
<td id="S2.T1.2.6.5.2" class="ltx_td ltx_align_center">71.26</td>
</tr>
<tr id="S2.T1.2.7.6" class="ltx_tr">
<th id="S2.T1.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VisualBERT</th>
<td id="S2.T1.2.7.6.2" class="ltx_td ltx_align_center">70.80</td>
</tr>
<tr id="S2.T1.2.8.7" class="ltx_tr">
<th id="S2.T1.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT</th>
<td id="S2.T1.2.8.7.2" class="ltx_td ltx_align_center">69.90</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Baseline accuracies on VQAv2 test set.</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In addition to the accuracy on VQAv2 test dataset we utilize another metric, the accuracy on the original images that correspond to the images from our dataset as another baseline for model performance. This metric enables us to understand how the models performed on the subset of questions and images that our dataset was created from in the original VQAv2 test set. This metric is labeled in Figure Â <a href="#S2.T2" title="Table 2 â€£ 2.2 Baseline â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as Original Images (%).</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.2.3.1" class="ltx_tr">
<th id="S2.T2.2.3.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S2.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">LXMERT</th>
<th id="S2.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">VisualBERT</th>
<th id="S2.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">ViLT</th>
<th id="S2.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">ALBEF</th>
<th id="S2.T2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">VLMo</th>
<th id="S2.T2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">OFA</th>
<th id="S2.T2.2.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLIP-2</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.2.4.1" class="ltx_tr">
<th id="S2.T2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VQAv2 Test (%)</th>
<td id="S2.T2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">69.90</td>
<td id="S2.T2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">70.80</td>
<td id="S2.T2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">71.26</td>
<td id="S2.T2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">75.84</td>
<td id="S2.T2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">76.64</td>
<td id="S2.T2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">78.10</td>
<td id="S2.T2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">82.30</td>
</tr>
<tr id="S2.T2.2.5.2" class="ltx_tr">
<th id="S2.T2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Original Images (%)</th>
<td id="S2.T2.2.5.2.2" class="ltx_td ltx_align_center">62.74</td>
<td id="S2.T2.2.5.2.3" class="ltx_td ltx_align_center">32.98</td>
<td id="S2.T2.2.5.2.4" class="ltx_td ltx_align_center">76.34</td>
<td id="S2.T2.2.5.2.5" class="ltx_td ltx_align_center">79.06</td>
<td id="S2.T2.2.5.2.6" class="ltx_td ltx_align_center">54.03</td>
<td id="S2.T2.2.5.2.7" class="ltx_td ltx_align_center">52.42</td>
<td id="S2.T2.2.5.2.8" class="ltx_td ltx_align_center">81.49</td>
</tr>
<tr id="S2.T2.2.6.3" class="ltx_tr">
<th id="S2.T2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Our Dataset (%)</th>
<td id="S2.T2.2.6.3.2" class="ltx_td ltx_align_center">46.32</td>
<td id="S2.T2.2.6.3.3" class="ltx_td ltx_align_center">31.97</td>
<td id="S2.T2.2.6.3.4" class="ltx_td ltx_align_center">53.00</td>
<td id="S2.T2.2.6.3.5" class="ltx_td ltx_align_center">55.70</td>
<td id="S2.T2.2.6.3.6" class="ltx_td ltx_align_center">42.97</td>
<td id="S2.T2.2.6.3.7" class="ltx_td ltx_align_center">39.77</td>
<td id="S2.T2.2.6.3.8" class="ltx_td ltx_align_center">55.74</td>
</tr>
<tr id="S2.T2.2.7.4" class="ltx_tr">
<th id="S2.T2.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Total Flip (%)</th>
<td id="S2.T2.2.7.4.2" class="ltx_td ltx_align_center">20.96</td>
<td id="S2.T2.2.7.4.3" class="ltx_td ltx_align_center">2.31</td>
<td id="S2.T2.2.7.4.4" class="ltx_td ltx_align_center">19.90</td>
<td id="S2.T2.2.7.4.5" class="ltx_td ltx_align_center">21.13</td>
<td id="S2.T2.2.7.4.6" class="ltx_td ltx_align_center">27.53</td>
<td id="S2.T2.2.7.4.7" class="ltx_td ltx_align_center">16.88</td>
<td id="S2.T2.2.7.4.8" class="ltx_td ltx_align_center">23.80</td>
</tr>
<tr id="S2.T2.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S2.T2.1.1.1.m1.1" class="ltx_Math" alttext="pos\rightarrow neg" display="inline"><semantics id="S2.T2.1.1.1.m1.1a"><mrow id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml"><mrow id="S2.T2.1.1.1.m1.1.1.2" xref="S2.T2.1.1.1.m1.1.1.2.cmml"><mi id="S2.T2.1.1.1.m1.1.1.2.2" xref="S2.T2.1.1.1.m1.1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.1.1.2.1" xref="S2.T2.1.1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.T2.1.1.1.m1.1.1.2.3" xref="S2.T2.1.1.1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.1.1.2.1a" xref="S2.T2.1.1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.T2.1.1.1.m1.1.1.2.4" xref="S2.T2.1.1.1.m1.1.1.2.4.cmml">s</mi></mrow><mo stretchy="false" id="S2.T2.1.1.1.m1.1.1.1" xref="S2.T2.1.1.1.m1.1.1.1.cmml">â†’</mo><mrow id="S2.T2.1.1.1.m1.1.1.3" xref="S2.T2.1.1.1.m1.1.1.3.cmml"><mi id="S2.T2.1.1.1.m1.1.1.3.2" xref="S2.T2.1.1.1.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.1.1.3.1" xref="S2.T2.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.T2.1.1.1.m1.1.1.3.3" xref="S2.T2.1.1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T2.1.1.1.m1.1.1.3.1a" xref="S2.T2.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.T2.1.1.1.m1.1.1.3.4" xref="S2.T2.1.1.1.m1.1.1.3.4.cmml">g</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><apply id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1"><ci id="S2.T2.1.1.1.m1.1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1.1">â†’</ci><apply id="S2.T2.1.1.1.m1.1.1.2.cmml" xref="S2.T2.1.1.1.m1.1.1.2"><times id="S2.T2.1.1.1.m1.1.1.2.1.cmml" xref="S2.T2.1.1.1.m1.1.1.2.1"></times><ci id="S2.T2.1.1.1.m1.1.1.2.2.cmml" xref="S2.T2.1.1.1.m1.1.1.2.2">ğ‘</ci><ci id="S2.T2.1.1.1.m1.1.1.2.3.cmml" xref="S2.T2.1.1.1.m1.1.1.2.3">ğ‘œ</ci><ci id="S2.T2.1.1.1.m1.1.1.2.4.cmml" xref="S2.T2.1.1.1.m1.1.1.2.4">ğ‘ </ci></apply><apply id="S2.T2.1.1.1.m1.1.1.3.cmml" xref="S2.T2.1.1.1.m1.1.1.3"><times id="S2.T2.1.1.1.m1.1.1.3.1.cmml" xref="S2.T2.1.1.1.m1.1.1.3.1"></times><ci id="S2.T2.1.1.1.m1.1.1.3.2.cmml" xref="S2.T2.1.1.1.m1.1.1.3.2">ğ‘›</ci><ci id="S2.T2.1.1.1.m1.1.1.3.3.cmml" xref="S2.T2.1.1.1.m1.1.1.3.3">ğ‘’</ci><ci id="S2.T2.1.1.1.m1.1.1.3.4.cmml" xref="S2.T2.1.1.1.m1.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">pos\rightarrow neg</annotation></semantics></math> (%)</th>
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_center">17.23</td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_center">1.42</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_center">18.12</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_align_center">19.89</td>
<td id="S2.T2.1.1.6" class="ltx_td ltx_align_center">18.29</td>
<td id="S2.T2.1.1.7" class="ltx_td ltx_align_center">13.68</td>
<td id="S2.T2.1.1.8" class="ltx_td ltx_align_center">22.56</td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<th id="S2.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S2.T2.2.2.1.m1.1" class="ltx_Math" alttext="neg\rightarrow pos" display="inline"><semantics id="S2.T2.2.2.1.m1.1a"><mrow id="S2.T2.2.2.1.m1.1.1" xref="S2.T2.2.2.1.m1.1.1.cmml"><mrow id="S2.T2.2.2.1.m1.1.1.2" xref="S2.T2.2.2.1.m1.1.1.2.cmml"><mi id="S2.T2.2.2.1.m1.1.1.2.2" xref="S2.T2.2.2.1.m1.1.1.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T2.2.2.1.m1.1.1.2.1" xref="S2.T2.2.2.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.T2.2.2.1.m1.1.1.2.3" xref="S2.T2.2.2.1.m1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T2.2.2.1.m1.1.1.2.1a" xref="S2.T2.2.2.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.T2.2.2.1.m1.1.1.2.4" xref="S2.T2.2.2.1.m1.1.1.2.4.cmml">g</mi></mrow><mo stretchy="false" id="S2.T2.2.2.1.m1.1.1.1" xref="S2.T2.2.2.1.m1.1.1.1.cmml">â†’</mo><mrow id="S2.T2.2.2.1.m1.1.1.3" xref="S2.T2.2.2.1.m1.1.1.3.cmml"><mi id="S2.T2.2.2.1.m1.1.1.3.2" xref="S2.T2.2.2.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.T2.2.2.1.m1.1.1.3.1" xref="S2.T2.2.2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.T2.2.2.1.m1.1.1.3.3" xref="S2.T2.2.2.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T2.2.2.1.m1.1.1.3.1a" xref="S2.T2.2.2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.T2.2.2.1.m1.1.1.3.4" xref="S2.T2.2.2.1.m1.1.1.3.4.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.m1.1b"><apply id="S2.T2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1"><ci id="S2.T2.2.2.1.m1.1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1.1">â†’</ci><apply id="S2.T2.2.2.1.m1.1.1.2.cmml" xref="S2.T2.2.2.1.m1.1.1.2"><times id="S2.T2.2.2.1.m1.1.1.2.1.cmml" xref="S2.T2.2.2.1.m1.1.1.2.1"></times><ci id="S2.T2.2.2.1.m1.1.1.2.2.cmml" xref="S2.T2.2.2.1.m1.1.1.2.2">ğ‘›</ci><ci id="S2.T2.2.2.1.m1.1.1.2.3.cmml" xref="S2.T2.2.2.1.m1.1.1.2.3">ğ‘’</ci><ci id="S2.T2.2.2.1.m1.1.1.2.4.cmml" xref="S2.T2.2.2.1.m1.1.1.2.4">ğ‘”</ci></apply><apply id="S2.T2.2.2.1.m1.1.1.3.cmml" xref="S2.T2.2.2.1.m1.1.1.3"><times id="S2.T2.2.2.1.m1.1.1.3.1.cmml" xref="S2.T2.2.2.1.m1.1.1.3.1"></times><ci id="S2.T2.2.2.1.m1.1.1.3.2.cmml" xref="S2.T2.2.2.1.m1.1.1.3.2">ğ‘</ci><ci id="S2.T2.2.2.1.m1.1.1.3.3.cmml" xref="S2.T2.2.2.1.m1.1.1.3.3">ğ‘œ</ci><ci id="S2.T2.2.2.1.m1.1.1.3.4.cmml" xref="S2.T2.2.2.1.m1.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.m1.1c">neg\rightarrow pos</annotation></semantics></math> (%)</th>
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_center">3.73</td>
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_center">0.89</td>
<td id="S2.T2.2.2.4" class="ltx_td ltx_align_center">1.78</td>
<td id="S2.T2.2.2.5" class="ltx_td ltx_align_center">1.24</td>
<td id="S2.T2.2.2.6" class="ltx_td ltx_align_center">9.24</td>
<td id="S2.T2.2.2.7" class="ltx_td ltx_align_center">3.20</td>
<td id="S2.T2.2.2.8" class="ltx_td ltx_align_center">1.24</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.5.2" class="ltx_text" style="font-size:90%;">Model accuracies on VQAv2.0, the original validation images corresponding to our datasetâ€™s generated images, our dataset, the % of predictions flipped, and what direction predictions flipped in.</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Evaluation Metrics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To evaluate the robustness of the various models we primarily analyze the overall accuracy on GenVQA as well as model consistency.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Overall Performance</span>. Our robustness metric looks at the overall performance on the new generated synthetic dataset. To calculate the overall performance, we utilize the evaluation code provided by VQA. We modify it to fit GenVQA. Similar to the original evaluation metric, a correct answer must match 3 of 10 ground truth answers.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.3" class="ltx_p"><span id="S2.SS3.p3.3.1" class="ltx_text ltx_font_bold">Consistency</span>. In addition to overall performance, we also observe the relative difference in performance on our dataset and the original validation samples that our samples are based on in order to evaluate the consistency of models on shifted data. We expect that models with higher accuracies on VQAv2 will be more consistent as they can likely learn features that generalize better on unseen data. For this metric we use a method similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> where we observe the percentage of predictions that flip on our new dataset compared to corresponding predictions on the original VQAv2 dataset. Specifically we investigate two categories, 1. <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="pos\rightarrow neg" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mrow id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mrow id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml"><mi id="S2.SS3.p3.1.m1.1.1.2.2" xref="S2.SS3.p3.1.m1.1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.1.m1.1.1.2.1" xref="S2.SS3.p3.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.1.m1.1.1.2.3" xref="S2.SS3.p3.1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.1.m1.1.1.2.1a" xref="S2.SS3.p3.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.1.m1.1.1.2.4" xref="S2.SS3.p3.1.m1.1.1.2.4.cmml">s</mi></mrow><mo stretchy="false" id="S2.SS3.p3.1.m1.1.1.1" xref="S2.SS3.p3.1.m1.1.1.1.cmml">â†’</mo><mrow id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml"><mi id="S2.SS3.p3.1.m1.1.1.3.2" xref="S2.SS3.p3.1.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.1.m1.1.1.3.1" xref="S2.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.1.m1.1.1.3.3" xref="S2.SS3.p3.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.1.m1.1.1.3.1a" xref="S2.SS3.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.1.m1.1.1.3.4" xref="S2.SS3.p3.1.m1.1.1.3.4.cmml">g</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><ci id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1.1">â†’</ci><apply id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2"><times id="S2.SS3.p3.1.m1.1.1.2.1.cmml" xref="S2.SS3.p3.1.m1.1.1.2.1"></times><ci id="S2.SS3.p3.1.m1.1.1.2.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2.2">ğ‘</ci><ci id="S2.SS3.p3.1.m1.1.1.2.3.cmml" xref="S2.SS3.p3.1.m1.1.1.2.3">ğ‘œ</ci><ci id="S2.SS3.p3.1.m1.1.1.2.4.cmml" xref="S2.SS3.p3.1.m1.1.1.2.4">ğ‘ </ci></apply><apply id="S2.SS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3"><times id="S2.SS3.p3.1.m1.1.1.3.1.cmml" xref="S2.SS3.p3.1.m1.1.1.3.1"></times><ci id="S2.SS3.p3.1.m1.1.1.3.2.cmml" xref="S2.SS3.p3.1.m1.1.1.3.2">ğ‘›</ci><ci id="S2.SS3.p3.1.m1.1.1.3.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3">ğ‘’</ci><ci id="S2.SS3.p3.1.m1.1.1.3.4.cmml" xref="S2.SS3.p3.1.m1.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">pos\rightarrow neg</annotation></semantics></math>, 2. <math id="S2.SS3.p3.2.m2.1" class="ltx_Math" alttext="neg\rightarrow pos" display="inline"><semantics id="S2.SS3.p3.2.m2.1a"><mrow id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml"><mrow id="S2.SS3.p3.2.m2.1.1.2" xref="S2.SS3.p3.2.m2.1.1.2.cmml"><mi id="S2.SS3.p3.2.m2.1.1.2.2" xref="S2.SS3.p3.2.m2.1.1.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.2.m2.1.1.2.1" xref="S2.SS3.p3.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.2.m2.1.1.2.3" xref="S2.SS3.p3.2.m2.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.2.m2.1.1.2.1a" xref="S2.SS3.p3.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.2.m2.1.1.2.4" xref="S2.SS3.p3.2.m2.1.1.2.4.cmml">g</mi></mrow><mo stretchy="false" id="S2.SS3.p3.2.m2.1.1.1" xref="S2.SS3.p3.2.m2.1.1.1.cmml">â†’</mo><mrow id="S2.SS3.p3.2.m2.1.1.3" xref="S2.SS3.p3.2.m2.1.1.3.cmml"><mi id="S2.SS3.p3.2.m2.1.1.3.2" xref="S2.SS3.p3.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.2.m2.1.1.3.1" xref="S2.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.2.m2.1.1.3.3" xref="S2.SS3.p3.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.2.m2.1.1.3.1a" xref="S2.SS3.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.2.m2.1.1.3.4" xref="S2.SS3.p3.2.m2.1.1.3.4.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><apply id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1"><ci id="S2.SS3.p3.2.m2.1.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1.1">â†’</ci><apply id="S2.SS3.p3.2.m2.1.1.2.cmml" xref="S2.SS3.p3.2.m2.1.1.2"><times id="S2.SS3.p3.2.m2.1.1.2.1.cmml" xref="S2.SS3.p3.2.m2.1.1.2.1"></times><ci id="S2.SS3.p3.2.m2.1.1.2.2.cmml" xref="S2.SS3.p3.2.m2.1.1.2.2">ğ‘›</ci><ci id="S2.SS3.p3.2.m2.1.1.2.3.cmml" xref="S2.SS3.p3.2.m2.1.1.2.3">ğ‘’</ci><ci id="S2.SS3.p3.2.m2.1.1.2.4.cmml" xref="S2.SS3.p3.2.m2.1.1.2.4">ğ‘”</ci></apply><apply id="S2.SS3.p3.2.m2.1.1.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3"><times id="S2.SS3.p3.2.m2.1.1.3.1.cmml" xref="S2.SS3.p3.2.m2.1.1.3.1"></times><ci id="S2.SS3.p3.2.m2.1.1.3.2.cmml" xref="S2.SS3.p3.2.m2.1.1.3.2">ğ‘</ci><ci id="S2.SS3.p3.2.m2.1.1.3.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3">ğ‘œ</ci><ci id="S2.SS3.p3.2.m2.1.1.3.4.cmml" xref="S2.SS3.p3.2.m2.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">neg\rightarrow pos</annotation></semantics></math>. The first metric measures the percent of correct predictions that flipped to incorrect predictions and the second the percent of incorrect predictions flipping to correct predictions. The original paper also uses a third metric, <math id="S2.SS3.p3.3.m3.1" class="ltx_Math" alttext="neg\rightarrow neg" display="inline"><semantics id="S2.SS3.p3.3.m3.1a"><mrow id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml"><mrow id="S2.SS3.p3.3.m3.1.1.2" xref="S2.SS3.p3.3.m3.1.1.2.cmml"><mi id="S2.SS3.p3.3.m3.1.1.2.2" xref="S2.SS3.p3.3.m3.1.1.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.3.m3.1.1.2.1" xref="S2.SS3.p3.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.3.m3.1.1.2.3" xref="S2.SS3.p3.3.m3.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.3.m3.1.1.2.1a" xref="S2.SS3.p3.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S2.SS3.p3.3.m3.1.1.2.4" xref="S2.SS3.p3.3.m3.1.1.2.4.cmml">g</mi></mrow><mo stretchy="false" id="S2.SS3.p3.3.m3.1.1.1" xref="S2.SS3.p3.3.m3.1.1.1.cmml">â†’</mo><mrow id="S2.SS3.p3.3.m3.1.1.3" xref="S2.SS3.p3.3.m3.1.1.3.cmml"><mi id="S2.SS3.p3.3.m3.1.1.3.2" xref="S2.SS3.p3.3.m3.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.3.m3.1.1.3.1" xref="S2.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.3.m3.1.1.3.3" xref="S2.SS3.p3.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.3.m3.1.1.3.1a" xref="S2.SS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p3.3.m3.1.1.3.4" xref="S2.SS3.p3.3.m3.1.1.3.4.cmml">g</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><apply id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1"><ci id="S2.SS3.p3.3.m3.1.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1.1">â†’</ci><apply id="S2.SS3.p3.3.m3.1.1.2.cmml" xref="S2.SS3.p3.3.m3.1.1.2"><times id="S2.SS3.p3.3.m3.1.1.2.1.cmml" xref="S2.SS3.p3.3.m3.1.1.2.1"></times><ci id="S2.SS3.p3.3.m3.1.1.2.2.cmml" xref="S2.SS3.p3.3.m3.1.1.2.2">ğ‘›</ci><ci id="S2.SS3.p3.3.m3.1.1.2.3.cmml" xref="S2.SS3.p3.3.m3.1.1.2.3">ğ‘’</ci><ci id="S2.SS3.p3.3.m3.1.1.2.4.cmml" xref="S2.SS3.p3.3.m3.1.1.2.4">ğ‘”</ci></apply><apply id="S2.SS3.p3.3.m3.1.1.3.cmml" xref="S2.SS3.p3.3.m3.1.1.3"><times id="S2.SS3.p3.3.m3.1.1.3.1.cmml" xref="S2.SS3.p3.3.m3.1.1.3.1"></times><ci id="S2.SS3.p3.3.m3.1.1.3.2.cmml" xref="S2.SS3.p3.3.m3.1.1.3.2">ğ‘›</ci><ci id="S2.SS3.p3.3.m3.1.1.3.3.cmml" xref="S2.SS3.p3.3.m3.1.1.3.3">ğ‘’</ci><ci id="S2.SS3.p3.3.m3.1.1.3.4.cmml" xref="S2.SS3.p3.3.m3.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">neg\rightarrow neg</annotation></semantics></math>, but we do not use it because a questionâ€™s correct answer in GenVQA is sometimes different from its correct answer in the original dataset.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2307.10405/assets/accuracy_plot_1.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Comparison between GenGQA accuracy and VQAv2 accuracy on various VQA models. The area of each data point represents the relative model size.</span></figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2307.10405/assets/accuracy_plot_2.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Comparison between GenGQA accuracy and Original Images from COCO accuracy on various VQA models. The area of each data point represents the relative model size.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Quantitative</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Table <a href="#S2.T2" title="Table 2 â€£ 2.2 Baseline â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the accuracy of each model on the original VQAv2 test dataset, the base images corresponding to our custom dataset. It also shows net answer flipping percentages for each model.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Overall Accuracy</span>. On the VQAv2 dataset, we can see that BLIP-2 is the most accurate model (82.30%), followed by OFA (78.10%) and VLMo (76.64%). This order, however, is not maintained when models are tested using the samples in our dataset with the original images. The top three models become BLIP-2, ALBEF, and ViLT. Whereas OFA was the third most accurate model before, it now has an accuracy of 39.77%, which is the sixth most accurate out of all the models. We also see a significant drop in the accuracy of VisualBERT and VLMo. Although this was concerning, we checked our data processing and prediction code for each of the models and are inclined to attribute these drops to randomness in our dataset, especially since our dataset is an extremely small proportion of the total VQA validation set. The relationship between model performance on the VQAv2 test dataset, the original images of GenVQA, and our generated images is visualized in figures <a href="#S2.F3" title="Figure 3 â€£ 2.3 Evaluation Metrics â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S2.F4" title="Figure 4 â€£ 2.3 Evaluation Metrics â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The accuracy of each model on our dataset parallels the accuracy of each model on the original images, with the top three models being BLIP-2, ALBEF, and ViLT. This suggests that a modelâ€™s performance on the original VQA dataset is a good indicator of how well it adapts to temporal shifts relative to other models. Still, the highest accuracy on GenVQA was only 55.74% on BLIP-2, suggesting that no VQA models actually perform outstandingly.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.5" class="ltx_p"><span id="S3.SS1.p4.5.1" class="ltx_text ltx_font_bold">Overall Flipping</span>. VisualBERT is the most stable model, with a total flip rate of 2.31%. Next, OFA (16.88%) and ViLT (19.90%) have the lowest flip rates. The highest flip rates, on the other hand, belong to VLMo (27.53%) and BLIP-2 (23.80%). However, it is important to note that stability and robustness are not equivalent. Indeed, VisualBERT performed the worst on the generated images of our custom dataset. On the other hand, ALBEF and ViLT were two models that performed well on GenVQA (55.70%, 53.00%) while being relatively stable (21.13%, 19.90%) compared to other well-performing models (GenVQA accuracy <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mo id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><gt id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">&gt;</annotation></semantics></math> 40%). To quantitatively evaluate robustness to future shifts, we devised a simple equation to calculate a modelâ€™s robustness <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">r</annotation></semantics></math>, where <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\gamma</annotation></semantics></math> weights the modelâ€™s accuracy <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mi id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">a</annotation></semantics></math> on our custom dataset, subtracting the flip rate <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mi id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">f</annotation></semantics></math>. 
<br class="ltx_break"></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="r=\gamma a-f" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">r</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">Î³</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">a</mi></mrow><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">âˆ’</mo><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">f</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ğ‘Ÿ</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><minus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></minus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><times id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"></times><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">ğ›¾</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">ğ‘</ci></apply><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">r=\gamma a-f</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.11" class="ltx_p">We chose a <math id="S3.SS1.p4.6.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS1.p4.6.m1.1a"><mi id="S3.SS1.p4.6.m1.1.1" xref="S3.SS1.p4.6.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m1.1b"><ci id="S3.SS1.p4.6.m1.1.1.cmml" xref="S3.SS1.p4.6.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m1.1c">\gamma</annotation></semantics></math> of 1.5 because we believe that performance on GenVQA is a stronger indicator of robustness compared to stability. Applying this statistic to our data, we find that the most robust models are ALBEF (<math id="S3.SS1.p4.7.m2.1" class="ltx_Math" alttext="r=62.42" display="inline"><semantics id="S3.SS1.p4.7.m2.1a"><mrow id="S3.SS1.p4.7.m2.1.1" xref="S3.SS1.p4.7.m2.1.1.cmml"><mi id="S3.SS1.p4.7.m2.1.1.2" xref="S3.SS1.p4.7.m2.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.7.m2.1.1.1" xref="S3.SS1.p4.7.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.7.m2.1.1.3" xref="S3.SS1.p4.7.m2.1.1.3.cmml">62.42</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m2.1b"><apply id="S3.SS1.p4.7.m2.1.1.cmml" xref="S3.SS1.p4.7.m2.1.1"><eq id="S3.SS1.p4.7.m2.1.1.1.cmml" xref="S3.SS1.p4.7.m2.1.1.1"></eq><ci id="S3.SS1.p4.7.m2.1.1.2.cmml" xref="S3.SS1.p4.7.m2.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS1.p4.7.m2.1.1.3.cmml" xref="S3.SS1.p4.7.m2.1.1.3">62.42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m2.1c">r=62.42</annotation></semantics></math>), BLIP-2 (<math id="S3.SS1.p4.8.m3.1" class="ltx_Math" alttext="r=59.81" display="inline"><semantics id="S3.SS1.p4.8.m3.1a"><mrow id="S3.SS1.p4.8.m3.1.1" xref="S3.SS1.p4.8.m3.1.1.cmml"><mi id="S3.SS1.p4.8.m3.1.1.2" xref="S3.SS1.p4.8.m3.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.8.m3.1.1.1" xref="S3.SS1.p4.8.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.8.m3.1.1.3" xref="S3.SS1.p4.8.m3.1.1.3.cmml">59.81</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m3.1b"><apply id="S3.SS1.p4.8.m3.1.1.cmml" xref="S3.SS1.p4.8.m3.1.1"><eq id="S3.SS1.p4.8.m3.1.1.1.cmml" xref="S3.SS1.p4.8.m3.1.1.1"></eq><ci id="S3.SS1.p4.8.m3.1.1.2.cmml" xref="S3.SS1.p4.8.m3.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS1.p4.8.m3.1.1.3.cmml" xref="S3.SS1.p4.8.m3.1.1.3">59.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m3.1c">r=59.81</annotation></semantics></math>), and ViLT (<math id="S3.SS1.p4.9.m4.1" class="ltx_Math" alttext="r=59.60" display="inline"><semantics id="S3.SS1.p4.9.m4.1a"><mrow id="S3.SS1.p4.9.m4.1.1" xref="S3.SS1.p4.9.m4.1.1.cmml"><mi id="S3.SS1.p4.9.m4.1.1.2" xref="S3.SS1.p4.9.m4.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.9.m4.1.1.1" xref="S3.SS1.p4.9.m4.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.9.m4.1.1.3" xref="S3.SS1.p4.9.m4.1.1.3.cmml">59.60</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m4.1b"><apply id="S3.SS1.p4.9.m4.1.1.cmml" xref="S3.SS1.p4.9.m4.1.1"><eq id="S3.SS1.p4.9.m4.1.1.1.cmml" xref="S3.SS1.p4.9.m4.1.1.1"></eq><ci id="S3.SS1.p4.9.m4.1.1.2.cmml" xref="S3.SS1.p4.9.m4.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS1.p4.9.m4.1.1.3.cmml" xref="S3.SS1.p4.9.m4.1.1.3">59.60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m4.1c">r=59.60</annotation></semantics></math>). On the other hand, the least robust models are OFA (<math id="S3.SS1.p4.10.m5.1" class="ltx_Math" alttext="r=42.78" display="inline"><semantics id="S3.SS1.p4.10.m5.1a"><mrow id="S3.SS1.p4.10.m5.1.1" xref="S3.SS1.p4.10.m5.1.1.cmml"><mi id="S3.SS1.p4.10.m5.1.1.2" xref="S3.SS1.p4.10.m5.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.10.m5.1.1.1" xref="S3.SS1.p4.10.m5.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.10.m5.1.1.3" xref="S3.SS1.p4.10.m5.1.1.3.cmml">42.78</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m5.1b"><apply id="S3.SS1.p4.10.m5.1.1.cmml" xref="S3.SS1.p4.10.m5.1.1"><eq id="S3.SS1.p4.10.m5.1.1.1.cmml" xref="S3.SS1.p4.10.m5.1.1.1"></eq><ci id="S3.SS1.p4.10.m5.1.1.2.cmml" xref="S3.SS1.p4.10.m5.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS1.p4.10.m5.1.1.3.cmml" xref="S3.SS1.p4.10.m5.1.1.3">42.78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m5.1c">r=42.78</annotation></semantics></math>) and VLMo (<math id="S3.SS1.p4.11.m6.1" class="ltx_Math" alttext="r=36.93" display="inline"><semantics id="S3.SS1.p4.11.m6.1a"><mrow id="S3.SS1.p4.11.m6.1.1" xref="S3.SS1.p4.11.m6.1.1.cmml"><mi id="S3.SS1.p4.11.m6.1.1.2" xref="S3.SS1.p4.11.m6.1.1.2.cmml">r</mi><mo id="S3.SS1.p4.11.m6.1.1.1" xref="S3.SS1.p4.11.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.p4.11.m6.1.1.3" xref="S3.SS1.p4.11.m6.1.1.3.cmml">36.93</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.11.m6.1b"><apply id="S3.SS1.p4.11.m6.1.1.cmml" xref="S3.SS1.p4.11.m6.1.1"><eq id="S3.SS1.p4.11.m6.1.1.1.cmml" xref="S3.SS1.p4.11.m6.1.1.1"></eq><ci id="S3.SS1.p4.11.m6.1.1.2.cmml" xref="S3.SS1.p4.11.m6.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS1.p4.11.m6.1.1.3.cmml" xref="S3.SS1.p4.11.m6.1.1.3">36.93</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.11.m6.1c">r=36.93</annotation></semantics></math>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Qualitative</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Model Size.</span> In our model suite, the models have a wide range of sizes. Most models have roughly 100-200M parameters with the largest model, BLIP-2, having 1.2B parameters. From our experiments, we observe that model size does not seem to have a strong correlation with robustness. From figures <a href="#S2.F3" title="Figure 3 â€£ 2.3 Evaluation Metrics â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S2.F4" title="Figure 4 â€£ 2.3 Evaluation Metrics â€£ 2 Method â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we see that ViLT with 87.4M parameters was able to reach similar GenVQA accuracy to BLIP-2 which had more than 10 times as many parameters. Additionally, OFA (930M parameters) was able to obtain only a 39.77% accuracy on GenVQA. Table <a href="#S5.T3" title="Table 3 â€£ 5 Appendix â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in the appendix showcases that model size also did not have a clear correlation with our defined robustness criteria with models of various sizes obtaining widely different robustness scores.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Model Architecture.</span> In Table <a href="#S5.T3" title="Table 3 â€£ 5 Appendix â€£ Generative Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we summarize each modelâ€™s architecture and pretraining process. First, we observe that models using vision transformers to create image embeddings (ViLT, BLIP-2, ALBEF) are more robust on GenVQA compared to models that use convolutional networks to generate image embeddings (e.g. Faster-RCNN and ResNet). Consequently, attention based embeddings may help models generalize to distributional drift. In addition, BLIP-2 and ALBEF, the two best models we evaluated, both rely on contrastive learning in their pipelines. BLIP-2 uses CLIPâ€™s pretrained ViT, while ALBEF pretrains on contrastive image-text matching and aligns the input image and text before multimodal encoding. This suggests that contrastive tasks are also helpful for robust model training. This fits with what we know about contrastive learning. Contrastive learning trains a model to understand a semantic space for visual and textual relationships, which allows contrastive models to predict well on previously unseen inputs. Indeed, CLIP demonstrates exemplary performance on zero shot classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Finally, we suspect that separate encoding of image and question improves model robustness. LXMERT (<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="r=48.52" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">r</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">48.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><eq id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">48.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">r=48.52</annotation></semantics></math>) and VisualBERT (<math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="r=45.65" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">r</mi><mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">45.65</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><eq id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></eq><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ‘Ÿ</ci><cn type="float" id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">45.65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">r=45.65</annotation></semantics></math>) are two near-identical models. However, while LXMERT encodes image and text embeddings separately, VisualBERT first concatenates image and text embeddings before encoding them through transformer layers together. Since all other architectural components were controlled between the two models, we believe this design difference likely explains LXMERTâ€™s better performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and Future Research</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our paper develops GenVQA, a new VQA dataset using future shifted images to test the robustness of VQA models to distributional drift. We find that many current VQA models struggle with temporal shifts in data. While newer architectures incorporating components such as vision transformers do perform better, we are still far from temporal generalization in VQA models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">While our paper shows intriguing results, we believe more research is needed on future shifted VQA datasets. One important step would be expanding the GenVQA dataset. After all, 600 random samples is not enough to accurately reflect the original distribution of the VQAv2 dataset. It would also be beneficial to test GenVQA on a larger variety of models in order to control for different architectures better during analysis. These are both time intensive tasks, requiring large amounts of manual work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Vedika Agarwal, Rakshith Shetty, and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Towards causal vqa: Revealing and reducing spurious correlations by
invariant and covariant semantic editing, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson,
Hannaneh Hajishirzi, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Exploring the landscape of distributional robustness for question
answering models, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam
Liska, Tayfun Terzi, Mai Gimenez, Cyprien de MassonÂ dâ€™Autume, Tomas Kocisky,
Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Mind the gap: Assessing temporal generalization in neural language
models, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Adversarial VQA: A new benchmark for evaluating the robustness of
VQA models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, abs/2106.00245, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Do imagenet classifiers generalize to imagenet?, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Cycle-consistency for robust visual question answering, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Appendix</h2>

<figure id="S5.T3" class="ltx_table ltx_align_center">
<table id="S5.T3.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.2.1.1" class="ltx_p" style="width:113.8pt;">Components</span>
</span>
</th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T3.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.3.1.1" class="ltx_p" style="width:113.8pt;">Pretraining</span>
</span>
</th>
<th id="S5.T3.2.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T3.2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.4.1.1" class="ltx_p" style="width:56.9pt;">Robustness</span>
</span>
</th>
<th id="S5.T3.2.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S5.T3.2.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.5.1.1" class="ltx_p" style="width:56.9pt;">Model Size</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VLMo</th>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">Mixture-of-Modality-Experts (MOME) Transformer, Multi-Head Attention</span>
</span>
</td>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T3.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.3.1.1" class="ltx_p" style="width:113.8pt;">Image Text Contrastive Learning, Image Text Matching, Masked Language Modelling</span>
</span>
</td>
<td id="S5.T3.2.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T3.2.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.4.1.1" class="ltx_p" style="width:56.9pt;">36.93</span>
</span>
</td>
<td id="S5.T3.2.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.2.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.5.1.1" class="ltx_p" style="width:56.9pt;">175M</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OFA</th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">ResNet for Image Patches, BPE for Word Embedding, Transformers with Self Attention and Cross Attention</span>
</span>
</td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.3.1.1" class="ltx_p" style="width:113.8pt;">Visual grounding, Grounded captioning, Image-Text
Matching, Image Captioning, VQA, Object Detection, Image Infilling, Text Infilling</span>
</span>
</td>
<td id="S5.T3.2.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.4.1.1" class="ltx_p" style="width:56.9pt;">42.78</span>
</span>
</td>
<td id="S5.T3.2.3.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.5.1.1" class="ltx_p" style="width:56.9pt;">930M</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VisualBERT</th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">BERT, Faster-RCNN, Single Stream Vision + Language Transformer</span>
</span>
</td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.3.1.1" class="ltx_p" style="width:113.8pt;">Masked Language Modelling with Image and Using Image to Distinguish Between Sentences</span>
</span>
</td>
<td id="S5.T3.2.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.4.1.1" class="ltx_p" style="width:56.9pt;">45.65</span>
</span>
</td>
<td id="S5.T3.2.4.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.5.1.1" class="ltx_p" style="width:56.9pt;">170.3M</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<th id="S5.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LXMERT</th>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.5.4.2.1.1" class="ltx_p" style="width:113.8pt;">BERT, Faster-RCNN, Self-Attention, Cross Modality Attention</span>
</span>
</td>
<td id="S5.T3.2.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.5.4.3.1.1" class="ltx_p" style="width:113.8pt;">Masked Cross Modality LM, Masked Object Prediction, Cross Modality Matching, Question Answering</span>
</span>
</td>
<td id="S5.T3.2.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.5.4.4.1.1" class="ltx_p" style="width:56.9pt;">48.52</span>
</span>
</td>
<td id="S5.T3.2.5.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.5.4.5.1.1" class="ltx_p" style="width:56.9pt;">239.8M</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.6.5" class="ltx_tr">
<th id="S5.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViLT</th>
<td id="S5.T3.2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.6.5.2.1.1" class="ltx_p" style="width:113.8pt;">Patch Image Embedding, Vision Language Transformer using ViT weights</span>
</span>
</td>
<td id="S5.T3.2.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.6.5.3.1.1" class="ltx_p" style="width:113.8pt;">Image Text Matching, Image Augmentation</span>
</span>
</td>
<td id="S5.T3.2.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.6.5.4.1.1" class="ltx_p" style="width:56.9pt;">59.60</span>
</span>
</td>
<td id="S5.T3.2.6.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.6.5.5.1.1" class="ltx_p" style="width:56.9pt;">87.4M</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.7.6" class="ltx_tr">
<th id="S5.T3.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BLIP-2</th>
<td id="S5.T3.2.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.7.6.2.1.1" class="ltx_p" style="width:113.8pt;">ViT, OPT LLM, Query-Transfomer with Cross-Attention</span>
</span>
</td>
<td id="S5.T3.2.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.7.6.3.1.1" class="ltx_p" style="width:113.8pt;">Pre-trained Query Transformer, Pre-trained ViT from CLIP, and Pre-trained LLM models</span>
</span>
</td>
<td id="S5.T3.2.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.7.6.4.1.1" class="ltx_p" style="width:56.9pt;">59.81</span>
</span>
</td>
<td id="S5.T3.2.7.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.7.6.5.1.1" class="ltx_p" style="width:56.9pt;">1.2B</span>
</span>
</td>
</tr>
<tr id="S5.T3.2.8.7" class="ltx_tr">
<th id="S5.T3.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ALBEF</th>
<td id="S5.T3.2.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.8.7.2.1.1" class="ltx_p" style="width:113.8pt;">ViT for image, BERT for text, Contrastive Loss, Cross Modal Attention</span>
</span>
</td>
<td id="S5.T3.2.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.8.7.3.1.1" class="ltx_p" style="width:113.8pt;">Image Text Contrastive Learning, Masked Language Modeling (image + text to predict masked words), Image Text Matching</span>
</span>
</td>
<td id="S5.T3.2.8.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T3.2.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.8.7.4.1.1" class="ltx_p" style="width:56.9pt;">62.42</span>
</span>
</td>
<td id="S5.T3.2.8.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T3.2.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.8.7.5.1.1" class="ltx_p" style="width:56.9pt;">314M</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.4.2" class="ltx_text" style="font-size:90%;">Robustness scores for models, along with each modelâ€™s components and pretraining techniques.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.10404" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.10405" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.10405">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.10405" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.10406" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:28:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
