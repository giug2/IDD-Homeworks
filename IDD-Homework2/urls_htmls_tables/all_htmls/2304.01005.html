<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.01005] FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS</title><meta property="og:description" content="Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of cl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.01005">

<!--Generated on Mon Feb 26 20:36:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">First Author 
<br class="ltx_break">Affiliation / Address line 1 
<br class="ltx_break">Affiliation / Address line 2 
<br class="ltx_break">Affiliation / Address line 3 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">email@domain</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Second Author 
<br class="ltx_break">Affiliation / Address line 1 
<br class="ltx_break">Affiliation / Address line 2 
<br class="ltx_break">Affiliation / Address line 3 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">email@domain</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users’ sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attacked scenarios performs similarly to centralized training in multilingual emoji prediction on seen and unseen languages under different data sources and distributions. Our trained transformers perform better than other techniques on the SemEval emoji dataset in addition to the privacy as well as distributed benefits of federated learning.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning FL is an emerging field in machine learning first introduced by Google in 2017 <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>. FL is a distributed learning framework, where multiple distributed clients collaborate in training machine learning models under the orchestration of a central server. This server aggregates the trained models into a final global model. FL is a major shift from centralized
machine learning to a distributed manner that
uses many distributed computing resources such as devices and data. Policies such as General
Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_citep">(Regulation, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite> imposes data privacy rules among different organizations. Thus, FL is essential to enhance data privacy by keeping the raw data on the local device while taking into account that some clients may have a Non-IID distribution of data or poisoned data limiting federated learning accuracy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning is used in many natural language processing applications <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Singh et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> such as adapting to pedestrian behavior in data generated by distributed sensors and devices, enabling real-time analysis and decision-making. In this paper, we focus on the task of emoji prediction because emojis enhance communication quality among users and associated text data is private. For example, the use of emojis in tweets has been steadily increasing over the years, with 35% of tweets in 2019 <cite class="ltx_cite ltx_citemacro_citep">(McShane et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> containing at least one emoji, compared to 9.9% in 2012. This trend has led to a 25% increase in engagement for tweets that feature at least one emoji <cite class="ltx_cite ltx_citemacro_citep">(McShane et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, as compared to those without any emojis. Moreover, the use of emojis in combination with brand names has increased by 49% since 2015, <cite class="ltx_cite ltx_citemacro_citep">(Agnew, <a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While traditional next-word prediction models are limited by the relatively narrow range of options available, emoji prediction presents unique challenges due to the diverse and context-dependent nature of emoji usage. For example, when predicting the next word after ’you’ in the sentence ’Thank you so much’, the options are relatively limited and predictable, such as ’guys’ or ’so’. However, for the next emoji prediction after ’you’, the options are more diverse and context-dependent, ranging from different colored hearts to facial expressions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Reviewing the literature on emoji prediction, we can
find that several papers indeed proposed methods in either centralized or federated settings <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib6" title="" class="ltx_ref">2018b</a>; Weller et al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Ramaswamy et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>; Gandhi et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Lee et al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>; Tomihira et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>; Peng and Zhao, <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Barbieri et al., <a href="#bib.bib7" title="" class="ltx_ref">2018c</a>; Yang et al., <a href="#bib.bib40" title="" class="ltx_ref">2018</a>; Barbieri et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Venkit et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>; Edwards et al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>; Camacho-Collados et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>; Barbieri et al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>; Loureiro et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Caldarola et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. For example, SemEval shared task in <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib6" title="" class="ltx_ref">2018b</a>)</cite> is dedicated to emoji prediction where multiple methods were developed in a centralized setting with a maximum F1 score of 35.99%. In this shared task, methods did not consider the widely used transformer architecture, multilinguality, and federation. In addition, Google successfully implemented a federated learning solution for Gboard’s emoji prediction using the LSTM architecture in <cite class="ltx_cite ltx_citemacro_citep">(Ramaswamy et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>. However, the results in this paper were only shown in English with 100 emoji classes while assuming that all clients have clean data, i.e clean scenarios. Additionally, the findings presented in <cite class="ltx_cite ltx_citemacro_citep">(Weller et al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> indicate that multilingual federated learning can be achieved without significant performance degradation compared to centralized learning. However, the study did not evaluate the performance of their federated learning models in the case of unseen languages or poisoned data from some clients (i.e., attack scenarios). Notably, the study did not include sparsely activated MoE transformers <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, which showed success in NLP tasks. Although previous work by <cite class="ltx_cite ltx_citemacro_citep">(Gandhi et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> achieved good results in predicting emojis in Hindi tweets using both centralized and federated learning approaches, our research aims to expand upon this approach by applying it to a multilingual context. It is worth noting that their study did not explore the effects of sparsely activated MoE transformers or evaluate the performance of their federated models on unseen languages or under poisoned data attack scenarios.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In a parallel research line, there were studies that investigated FL in an attack scenario, and proposed techniques to defend against label-flipping attacks and backdoor attacks <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Rodríguez-Barroso et al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>; Blanchard et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Wang et al., <a href="#bib.bib36" title="" class="ltx_ref">2020a</a>; Fung et al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Jebreel et al., <a href="#bib.bib17" title="" class="ltx_ref">2022</a>; Manoel et al., <a href="#bib.bib24" title="" class="ltx_ref">2022</a>; Ma et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>. However, their experiments were not carried out on the task of multilingual emoji prediction with multiple classes, and the results were not compared to the centralized setting.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Demo and Source code of this paper on GitHub <a target="_blank" href="https://github.com/kareemgamalmahmoud/FEDERATED-LEARNING-BASED-MULTILINGUAL-EMOJI-PREDICTION-IN-CLEAN-AND-ATTACK-SCENARIOS" title="" class="ltx_ref ltx_href">( FEDERATED-LEARNING-BASED-MULTILINGUAL )</a></span></span></span> Our two million training and testing examples are acquired from both Twitter and the standard SemEval emoji dataset <a target="_blank" href="https://competitions.codalab.org/competitions/17344" title="" class="ltx_ref ltx_href">(SemEval Data)</a>. For multilingual emoji prediction, we train publicly available pre-trained models of different sizes of dense and sparsely activated transformers, namely, Multilingual-MiniLM (M-MiniLM) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib37" title="" class="ltx_ref">2020b</a>)</cite>, Twitter-twihin-Bert-base (Bert-Base) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>, Twitter-XLM-Roberta (XLM-R) <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>, and switch-MoE with 8 experts <cite class="ltx_cite ltx_citemacro_citep">(Fedus et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. To simulate attack scenarios in FL, we apply the label-flipping data poisoning attack to some clients and utilize different FL aggregation schemes to reverse this attack. Our experimental results led to the following findings:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">In either centralized or FL experiments, we achieved emoji prediction accuracy better than the teams reported in the SemEval emoji prediction shared task <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib6" title="" class="ltx_ref">2018b</a>)</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">FL training on emoji data achieves similar accuracy performance to traditional centralized setup. This FL accuracy performance is confirmed in seen or unseen languages with IID and Non-IID data distributions in both unilingual and multilingual settings.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">When some clients’ emoji data is attacked via label flipping, FL’s K-representative unweighted median (Krum) aggregation scheme can restore the accuracy of the clean setting.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The rest of the paper is organized as follows: Section <a href="#S2" title="2 Methodology ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the data, models, training algorithm, and label-flipping attack for FL. This is followed by explaining experiments carried out in this paper. Last but not least, Section <a href="#S4" title="4 Conclusion ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Acquisition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We used the Twitter API to crawl over 2 million tweets that contain only one emoji as well as 500k training and test data from SemEval. The Twitter set encompasses 3 languages namely, Spanish, Italian, and French, while SemEval includes English. This data is filtered to remove stop words, hyperlinks, and duplicate special characters resulting in 1.3 million examples. Given an input sentence, we focus on predicting the 20 most popular emojis, which are the same emojis as the SemEval paper <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib6" title="" class="ltx_ref">2018b</a>)</cite> shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Data Acquisition ‣ 2 Methodology ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2304.01005/assets/figures/emoji_set.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="65" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The 20 most frequent emojis.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Client Partitioning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Three different training setups were carried out in this paper: traditional centralized training with no FL<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In practical scenarios, publicly available datasets such as Twitter data or datasets similar to the private chat data can be used to train the server model for FL. This is particularly useful in chat problems where users may be hesitant to share their private chat data with the application.</span></span></span>, FL with IID data where each client has a random subset of all data, and FL with Non-IID data where each client includes data from one language. The number of clients for FL experiments is four. We have selected four clients for our FL experiments based on previous experiments and to maintain consistency with the Non-IID setup of our problem. This decision is also influenced by the fact that we initially started our work with four languages. In either FL IID or Non-IID, we carry out an experiment while assuming that all clients have clean data (clean scenario) and another experiment while assuming that 25% and 50% of the clients have label-flipped data (attack scenario). For centralized training with no FL, we merge the clients’ datasets into one pool and use label flipping to attack the same set of samples as FL to compare the results.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Attack Scenario: Label flipping Procedure</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We flipped the first 10 emoji classes in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Data Acquisition ‣ 2 Methodology ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> into the last 10 emoji. Based on this flipping, we created two attack scenarios. The first scenario is to make 1 out of 4 FL clients toxic (i.e, 25%), while the second scenario is to make 2 out of 4 of FL clients toxic (i.e 50%). We then apply these datasets to federated training (IID setup, and Non-IID setup) and centralized training as described in Section <a href="#S2.SS2" title="2.2 Client Partitioning ‣ 2 Methodology ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To elaborate on the IID and Non-IID setup, we divided the data into four parts for the IID setup, with each client taking one part. For the 25% FL clients toxic attack scenario, we flipped the first 10 emoji classes to create a toxic dataset for one of the four clients.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">For the Non-IID setup, since we had four languages for training, we divided the data per language and assigned each client one language to work on. In this scenario, we made the English data toxic for the 25% FL clients attack scenario, while for the 50% FL clients attack scenario, we made both the English and French languages toxic. This approach allowed us to simulate a more realistic attack scenario, where the toxic data distribution is not uniform across all clients.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Federated Learning Methods</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">FederatedAveraging (FedAVG) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> is an aggregation scheme used in federated learning, where each client trains a model on its local data using stochastic gradient descent (SGD), and the server aggregates the client weights by taking their average. This process is repeated for a fixed number of rounds, with each round consisting of clients training their models and the server aggregating their weights. On the other hand, Krum <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite> is a robust federated learning algorithm that selects a subset of model updates, excluding those from malicious or incorrect clients, and computes the K-representative unweighted median of the selected updates. The selected update is the one that is closest to the center of the other updates while ignoring the updates that are far away from the center. The parameter K is specified by the user.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Krum (Krumble) is more computationally demanding than FedAvg (Federated Averaging) because it involves additional steps of distance calculation to select the best model updates.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">In Krum, the server selects the best model updates from a subset of the participating devices (clients) based on the distances between the updates. The distances are calculated based on the number of disagreements between the updates and the other updates in the subset. This process requires more computation than FedAvg, where the server simply averages the model updates received from the clients.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">Thus, the additional distance calculation step in Krum makes it more computationally demanding than FedAvg. However, Krum may provide better performance in certain scenarios where the participating clients may be potentially malicious or have poor quality updates.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Although, Krum can produce a more precise final model in attack scenarios where FedAVG may not be effective.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Models</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Publicly available models with different numbers of model parameters are trained for the task of emoji prediction. In particular, we utilized 4 models from hugging face: (1) M-MiniLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib37" title="" class="ltx_ref">2020b</a>)</cite> is a 21M parameter transformer model pre-trained on 16 languages and distilled from Bert Base; (2) Bert-Base is a multi-lingual tweet language model <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite> that is trained on 7 billion Tweets from over 100 distinct languages and has 280M parameters; (3) XLM-R <cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite> is trained on  198M multilingual tweets, has 278M parameters, and pre-trained in more than 30 Languages; (4) switch-MoE is an 8-expert MoE model trained on Masked Language Modeling (MLM) task. The model architecture is similar to the classic T5, but with the Feed Forward layers replaced by the Sparse MLP layers containing "experts" MLP. It has 619M parameters and it’s pre-trained in the English-only language. In addition to the publicly available models, we also built an LSTM model from scratch for the emoji prediction task. The model has 18M parameters and consists of a 1D convolutional layer and 3 LSTM layers.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Training Description</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">We utilized the Flower framework for both federated training and evaluation, given its user-friendly interface and active community <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>. Additionally, we employed Hugging Face’s transformers library <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite> to load pre-trained models, and PyTorch as the underlying differentiation framework <cite class="ltx_cite ltx_citemacro_citep">(Paszke et al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. We conducted training for each sparsely and densely activated transformer model for 30 epochs with the AdamW optimizer and (1e-3, 1e-4) learning rates. Our experiment runs in about 15-20 hours for dense transformers, while it takes a day for MoE training. For FL, we assigned four clients and conducted five rounds of training, with each client training for one epoch per round for both the Non-IID and IID setups. All these experiments are carried out on 256 GB of RAM and two NVIDIA A40 GPUs, an Intel(R) Xeon(R) Gold 6338 machine.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our experimental design consists of three stages:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Train the models under test in a centralized setting
on the task of emoji prediction. This setting is <span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Baseline</span>, which is done to ensure that the models under test are well-equipped for the task of emoji prediction before federated and centralized comparisons.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Take the trained models and train them in FL with a new dataset distributed to clients. In this setup, we mainly carry out an experiment while assuming that all clients have clean data (clean scenario) and another experiment while assuming that some clients have data that has been label-flipped (attack scenario). In both scenarios, we carry out experiments for both <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">IID</span> and <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">Non-IID</span> FL to simulate real scenarios. For further clarification, see Figure <a href="#S3.F2" title="Figure 2 ‣ item 2 ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2304.01005/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="230" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Second stage of our experimental design. (A) shows how the server pushes the baseline model to the clients for training on their local dataset. (B) demonstrates how the clients push back the tuned model to the server. Finally, (C) represents the process by which the server combines all models from the clients to build a new global model.</figcaption>
</figure>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Take the trained models from the first stage and fine-tune these models in a centralized setting on the combination of all the distributed data used to train the FL. The third stage is the <span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Finetuned</span> setting, which is done to compare the performance of the federated learning distributed learning approach versus the traditional centralized learning approach in the task of emoji prediction. Our training set is divided into two halves. The first half of the data is used for the Baseline stage, while the second is used in either the FL stage (both IID and non-IID) or the Finetuned stage. We carry out all these experiments in unilingual and multilingual setups while measuring the corresponding Macro-F1 score for the 20-class emojis.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Macro-F1 and Micro-F1 are two evaluation metrics used in multi-class classification problems. The Micro-F1 score gives equal weight to each individual instance in the dataset, while the Macro-F1 score gives each class an equal weight. In cases where we have unbalanced classes, the Macro-F1 score is often used as it gives equal importance to each class, regardless of its frequency in the dataset.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimenting with the Number of Clients in Federated Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We present the results of our initial experiments to determine the optimal number of clients in federated learning for our problem. We used an LSTM IID multilingual model and evaluated its performance on: Micro-F1 and Macro-F1 scores.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Metrics</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Baseline</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IID</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Finetuned</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Accuracy</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.4%</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.1%</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.8%</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Macro-F1</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">27.6%</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">29.2%</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">29.1%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Multilingual Centralized and Federated with 4 clients Accuracy and Macro-F1 Scores for SemEval test dataset.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2304.01005/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>SemEval Test Results of LSTM IID Multilingual experi-
ments with varying clients.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Experimenting with the Number of Clients in Federated Learning ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that FL shows consistent performance across different number of clients in terms of Accuracy and Macro-F1 scores. There are slight drops in Macro-F1 because we use a fixed dataset distributed into multiple clients leading to a smaller data share per client. Smaller data may yield lower scores, which was also observed in the literature <cite class="ltx_cite ltx_citemacro_citep">(Caldarola et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">So our initial experiments suggest that federated learning can improve the performance of our model, and a smaller number of clients might be more appropriate for our problem.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">However, we can obtain higher scores by using transformer models, which is what we will do in the upcoming experiments.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Unilingual and Multilingual Macro-F1 results in clean scenario</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Unilingual and Multilingual Macro-F1 results in clean scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the results of the unilingual models for the SemEval English test dataset. As can be seen, The Finetuned models generally perform better than their baseline and IID counterparts. For example, the Bert-Base model has a baseline score of 36.9%, while the IID score is 37.4%, and the Finetuned score is 38.1%. This shows that while the Finetuned process improves the model’s performance, the IID training approach is also effective and can achieve a score close to the Finetuned model.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Baseline</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IID</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Finetuned</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Switch-Base-8</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.2%</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.3%</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.6%</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Bert-Base</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">36.9%</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">37.4%</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">38.1%</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">XLM-R</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">35.9%</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">36.7%</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">37.6%</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">M-MiniLM</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">33.3%</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">33.9%</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">35.9%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Unilingual Centralized and Federated Macro-F1 Accuracy Scores for SemEval test dataset.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Turning to <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">multilingual</span>, Table <a href="#S3.T3" title="Table 3 ‣ 3.2 Unilingual and Multilingual Macro-F1 results in clean scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the Macro-F1 accuracy scores for centralized and FL multilingual models on the SemEval English dataset. The baseline accuracy for all models is improved by applying federated learning with the IID, which achieves slightly higher accuracy than the Non-IID setting.
This suggests that data distribution has an impact on model performance. Additionally, finetuning the models further improves the Baseline performance with only small differences observed between federated and Finetuned results.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Baseline</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IID</th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Non-IID</th>
<th id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Finetuned</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<td id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Bert-Base</td>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.3%</td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.8%</td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.3%</td>
<td id="S3.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.7%</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<td id="S3.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">XLM-R</td>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">33.4%</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">34.9%</td>
<td id="S3.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">34.9%</td>
<td id="S3.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">34.1%</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<td id="S3.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">M-MiniLM</td>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">31.4%</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.7%</td>
<td id="S3.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.03%</td>
<td id="S3.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.9%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Multilingual Centralized and Federated Macro-F1 Accuracy Scores for SemEval English Dataset.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">When comparing the unilingual and multilingual models via SemEval, we can see a drop in the Macro-F1 scores due to the mixed languages in the multilingual dataset. However, the drop was not significant, indicating that the models can handle multiple languages to a certain extent. Overall, the federated approach has shown to be a viable alternative to the centralized approach in terms of performance.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Using the Twitter multilingual dataset, Table <a href="#S3.T4" title="Table 4 ‣ 3.2 Unilingual and Multilingual Macro-F1 results in clean scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> outlines the trained <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">multilingual</span> Macro-F1 scores in centralized and FL setups. Similar observations to the case of SemEval are seen.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4">Average Test Results for Twitter Dataset</td>
</tr>
<tr id="S3.T4.1.2.2" class="ltx_tr">
<th id="S3.T4.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S3.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Baseline</td>
<td id="S3.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IID</td>
<td id="S3.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Non-IID</td>
<td id="S3.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Finetuned</td>
</tr>
<tr id="S3.T4.1.3.3" class="ltx_tr">
<th id="S3.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<td id="S3.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.2%</td>
<td id="S3.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.6%</td>
<td id="S3.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.7%</td>
<td id="S3.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.2%</td>
</tr>
<tr id="S3.T4.1.4.4" class="ltx_tr">
<th id="S3.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<td id="S3.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">26.5%</td>
<td id="S3.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">28.2%</td>
<td id="S3.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">28.02%</td>
<td id="S3.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">28.3%</td>
</tr>
<tr id="S3.T4.1.5.5" class="ltx_tr">
<th id="S3.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">M-MiniLM</th>
<td id="S3.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">24.2%</td>
<td id="S3.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25.7%</td>
<td id="S3.T4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">26.1%</td>
<td id="S3.T4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25.9%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Centralized and Federated Learning average results for the Twitter Multilingual (e.g Spanish, French, Italian) Dataset.</figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">In Table <a href="#S3.T5" title="Table 5 ‣ 3.2 Unilingual and Multilingual Macro-F1 results in clean scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we show the Macro-F1 score per language of the best-performing multilingual model Bert-Base in the centralized and federated setups. As shown, the accuracy does not significantly vary per language, which shows the effectiveness of our trained models.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Data</td>
<td id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Baseline</td>
<td id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IID</td>
<td id="S3.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Non-IID</td>
<td id="S3.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Finetuned</td>
</tr>
<tr id="S3.T5.1.2.2" class="ltx_tr">
<td id="S3.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Spanish</td>
<td id="S3.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.7%</td>
<td id="S3.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.5%</td>
<td id="S3.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.8%</td>
<td id="S3.T5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.3%</td>
</tr>
<tr id="S3.T5.1.3.3" class="ltx_tr">
<td id="S3.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">French</td>
<td id="S3.T5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.6%</td>
<td id="S3.T5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.3%</td>
<td id="S3.T5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.1%</td>
<td id="S3.T5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.9%</td>
</tr>
<tr id="S3.T5.1.4.4" class="ltx_tr">
<td id="S3.T5.1.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Italian</td>
<td id="S3.T5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">30.2%</td>
<td id="S3.T5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32.2%</td>
<td id="S3.T5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32.3%</td>
<td id="S3.T5.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32.4%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Centralized and Federated Learning Results for the Twitter multilingual dataset for Bert-Base model.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Comparison between our models and the literature</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Looking at the results in Table <a href="#S3.T6" title="Table 6 ‣ 3.3 Comparison between our models and the literature ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we can observe that most of our multilingual models perform better than the majority of models that were also trained on the SemEval training dataset from the literature in terms of both Micro-F1 and Macro-F1. Specifically, our multilingual models in most cases achieved more than 36% Macro-F1 or more than 49% Micro-F1, whereas the best-performing model from the literature, BERT(Twitter) <cite class="ltx_cite ltx_citemacro_citep">(Edwards et al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>, achieved 40% Micro-F1. Furthermore, our unilingual model with Bert-Base achieved 38.1% Macro-F1, which is comparable to the performance of BERT(Twitter), which achieved 38% Macro-F1. Moreover, our federated models achieved more than 50% Micro-F1, which is better than the performance of BERT(Twitter). Following the Large Language Models interest, we carried out an experiment using the Davinci-003 model <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> on the SemEval set in zero-shot. Davinci-003 achieved a Macro-F1 score of 16%, which also shows the promise of our trained FL models.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<table id="S3.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<td id="S3.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S3.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Micro-F1</td>
<td id="S3.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Macro-F1</td>
</tr>
<tr id="S3.T6.1.2.2" class="ltx_tr">
<td id="S3.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">BiLSTM</td>
<td id="S3.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.6%</td>
<td id="S3.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.3%</td>
</tr>
<tr id="S3.T6.1.3.3" class="ltx_tr">
<td id="S3.T6.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Venkit et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite></td>
<td id="S3.T6.1.3.3.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.3.3.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.4.4" class="ltx_tr">
<td id="S3.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed LSTM IID</td>
<td id="S3.T6.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.1%</td>
<td id="S3.T6.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.2%</td>
</tr>
<tr id="S3.T6.1.5.5" class="ltx_tr">
<td id="S3.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Multilingual</td>
<td id="S3.T6.1.5.5.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.5.5.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.6.6" class="ltx_tr">
<td id="S3.T6.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">XLM-Tw</td>
<td id="S3.T6.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.9%</td>
</tr>
<tr id="S3.T6.1.7.7" class="ltx_tr">
<td id="S3.T6.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T6.1.7.7.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.7.7.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.8.8" class="ltx_tr">
<td id="S3.T6.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">TweetNLP</td>
<td id="S3.T6.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T6.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.0%</td>
</tr>
<tr id="S3.T6.1.9.9" class="ltx_tr">
<td id="S3.T6.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Camacho-Collados et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S3.T6.1.9.9.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.9.9.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.10.10" class="ltx_tr">
<td id="S3.T6.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SemEval first team</td>
<td id="S3.T6.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.1%</td>
<td id="S3.T6.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.9%</td>
</tr>
<tr id="S3.T6.1.11.11" class="ltx_tr">
<td id="S3.T6.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Barbieri et al., <a href="#bib.bib5" title="" class="ltx_ref">2018a</a>)</cite></td>
<td id="S3.T6.1.11.11.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.11.11.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.12.12" class="ltx_tr">
<td id="S3.T6.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">BERT (Twitter)</td>
<td id="S3.T6.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.0%</td>
<td id="S3.T6.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.0%</td>
</tr>
<tr id="S3.T6.1.13.13" class="ltx_tr">
<td id="S3.T6.1.13.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Edwards et al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite></td>
<td id="S3.T6.1.13.13.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.13.13.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.14.14" class="ltx_tr">
<td id="S3.T6.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Bert-Base Fintuned</td>
<td id="S3.T6.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4%</td>
<td id="S3.T6.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.7%</td>
</tr>
<tr id="S3.T6.1.15.15" class="ltx_tr">
<td id="S3.T6.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Multilingual</td>
<td id="S3.T6.1.15.15.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.15.15.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.16.16" class="ltx_tr">
<td id="S3.T6.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Bert-Base FL Non-IID</td>
<td id="S3.T6.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.5%</td>
<td id="S3.T6.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.3%</td>
</tr>
<tr id="S3.T6.1.17.17" class="ltx_tr">
<td id="S3.T6.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Multilingual</td>
<td id="S3.T6.1.17.17.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.17.17.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.18.18" class="ltx_tr">
<td id="S3.T6.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Bert-Base FL IID</td>
<td id="S3.T6.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.3%</td>
<td id="S3.T6.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.8%</td>
</tr>
<tr id="S3.T6.1.19.19" class="ltx_tr">
<td id="S3.T6.1.19.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Multilingual</td>
<td id="S3.T6.1.19.19.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.19.19.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.20.20" class="ltx_tr">
<td id="S3.T6.1.20.20.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Bert-Base FL IID</td>
<td id="S3.T6.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.9%</td>
<td id="S3.T6.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.4%</td>
</tr>
<tr id="S3.T6.1.21.21" class="ltx_tr">
<td id="S3.T6.1.21.21.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Unilingual</td>
<td id="S3.T6.1.21.21.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T6.1.21.21.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T6.1.22.22" class="ltx_tr">
<td id="S3.T6.1.22.22.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Proposed Bert-Base Fintuned</td>
<td id="S3.T6.1.22.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.1%</td>
<td id="S3.T6.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.1%</td>
</tr>
<tr id="S3.T6.1.23.23" class="ltx_tr">
<td id="S3.T6.1.23.23.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Unilingual</td>
<td id="S3.T6.1.23.23.2" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S3.T6.1.23.23.3" class="ltx_td ltx_border_b ltx_border_r"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison between our models’ performance and models from the literature on emoji prediction task using the Micro-F1 and Macro-F1 metrics.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multilingual Macro-F1 Results on unseen language</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To investigate the trained models’ performance on unseen languages (i.e, zero-shot), we run inference on an unseen German dataset in Baseline, FL, and Finetune settings. Table <a href="#S3.T7" title="Table 7 ‣ 3.4 Multilingual Macro-F1 Results on unseen language ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that there is some drop in performance due to the zero-shot setting. However, this experiment still shows that FL performs at least similarly to centralized settings even in unseen languages.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<table id="S3.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T7.1.1.1" class="ltx_tr">
<th id="S3.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Baseline</th>
<th id="S3.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IID</th>
<th id="S3.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Non-IID</th>
<th id="S3.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Finetuned</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T7.1.2.1" class="ltx_tr">
<td id="S3.T7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Bert-Base</td>
<td id="S3.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.9%</td>
<td id="S3.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.1%</td>
<td id="S3.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.1%</td>
<td id="S3.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.5%</td>
</tr>
<tr id="S3.T7.1.3.2" class="ltx_tr">
<td id="S3.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">XLM-R</td>
<td id="S3.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">20.04%</td>
<td id="S3.T7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">20.9%</td>
<td id="S3.T7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">21.07%</td>
<td id="S3.T7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">19.2%</td>
</tr>
<tr id="S3.T7.1.4.3" class="ltx_tr">
<td id="S3.T7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">M-MiniLM</td>
<td id="S3.T7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">15.4%</td>
<td id="S3.T7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">16.5%</td>
<td id="S3.T7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">16.4%</td>
<td id="S3.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">17.1%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>The zero-shot inference results for Centralized and Federated Learning </figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Multilingual Macro-F1 Results in Label-flipping Attack Scenario</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Table <a href="#S3.T8" title="Table 8 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#S3.T9" title="Table 9 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> depict the results of label flipping experiments when 50% clients are attacked (i.e 50% of the data is attacked) using the SemEval and Twitter datasets, respectively. The tables compare the centralized and FL results for FedAVG and Krum. The results demonstrate that Krum performed better than FedAVG in both datasets, with higher accuracy rates. Krum was able to handle the label-flipping attack scenario and produced scores that were very close to the results obtained with the Finetune setting.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Table <a href="#S3.T8" title="Table 8 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> presents the experiment results for three different models, namely Bert-Base, XLM-R, and M-MiniLM. However, we will focus on the results of the Bert-Base model in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where Bert achieved 36.8% Macro-F1 in the clean IID scenario but dropped to 26.2% under FedAVG with Fed-IID due to label-flipping attacks. Traditional training and FL with FedAVG had low Macro-F1 of 24.4%, while Krum aggregation function achieved 36.5% Macro-F1, showing superior handling of label-flipping attacks and improving FL model performance. Appendix <a href="#A1" title="Appendix A Multilingual Macro-F1 Results in Label-flipping Attack Scenario Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> shows similar results for 25% Toxic clients experiments.</p>
</div>
<figure id="S3.T8" class="ltx_table">
<table id="S3.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T8.1.1.1" class="ltx_tr">
<th id="S3.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="S3.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T8.1.2.2" class="ltx_tr">
<th id="S3.T8.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T8.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="S3.T8.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.2%</td>
<td id="S3.T8.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.5%</td>
</tr>
<tr id="S3.T8.1.3.3" class="ltx_tr">
<th id="S3.T8.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="S3.T8.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T8.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">28.1%</td>
<td id="S3.T8.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">35.2%</td>
</tr>
<tr id="S3.T8.1.4.4" class="ltx_tr">
<th id="S3.T8.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T8.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T8.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">24.4%</td>
</tr>
<tr id="S3.T8.1.5.5" class="ltx_tr">
<th id="S3.T8.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T8.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T8.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T8.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T8.1.6.6" class="ltx_tr">
<th id="S3.T8.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T8.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T8.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.6%</td>
<td id="S3.T8.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.8%</td>
</tr>
<tr id="S3.T8.1.7.7" class="ltx_tr">
<th id="S3.T8.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="S3.T8.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T8.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">27.5%</td>
<td id="S3.T8.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">32.9%</td>
</tr>
<tr id="S3.T8.1.8.8" class="ltx_tr">
<th id="S3.T8.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T8.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T8.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">23.3%</td>
</tr>
<tr id="S3.T8.1.9.9" class="ltx_tr">
<th id="S3.T8.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T8.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T8.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T8.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T8.1.10.10" class="ltx_tr">
<th id="S3.T8.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T8.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T8.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.3%</td>
<td id="S3.T8.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.6%</td>
</tr>
<tr id="S3.T8.1.11.11" class="ltx_tr">
<th id="S3.T8.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="S3.T8.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T8.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">26.7%</td>
<td id="S3.T8.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">30.5%</td>
</tr>
<tr id="S3.T8.1.12.12" class="ltx_tr">
<th id="S3.T8.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="S3.T8.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="S3.T8.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">23.9%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the SemEval English test dataset.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><svg id="S3.F4.pic1" class="ltx_picture" height="237.84" overflow="visible" version="1.1" width="249.91"><g transform="translate(0,237.84) matrix(1 0 0 -1 0 0) translate(33.83,0) translate(0,36.58) matrix(0.8 0.0 0.0 0.8 -33.83 -36.58)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(54.56,0) translate(0,55.91)"><g stroke-width="0.4pt" fill="#BFBFBF" stroke="#BFBFBF" color="#BFBFBF"><path d="M 0 -10.19 L 0 213.97 M 122.64 -10.19 L 122.64 213.97 M 245.29 -10.19 L 245.29 213.97" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 -16.1 L 0 -10.19 M 122.64 -16.1 L 122.64 -10.19 M 245.29 -16.1 L 245.29 -10.19 M 0 219.88 L 0 213.97 M 122.64 219.88 L 122.64 213.97 M 245.29 219.88 L 245.29 213.97" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -12.27 20.04 L -6.36 20.04 M -12.27 75.72 L -6.36 75.72 M -12.27 131.4 L -6.36 131.4 M -12.27 187.08 L -6.36 187.08 M 257.55 20.04 L 251.65 20.04 M 257.55 75.72 L 251.65 75.72 M 257.55 131.4 L 251.65 131.4 M 257.55 187.08 L 251.65 187.08" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -12.27 -10.19 L -12.27 213.97 L 257.55 213.97 L 257.55 -10.19 L -12.27 -10.19 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 37.68 -30.59)" fill="#000000" stroke="#000000"><foreignObject width="46.89" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.5.5.5.5.5.1.1" class="ltx_text">Fed-IID</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 145.53 -30.59)" fill="#000000" stroke="#000000"><foreignObject width="76.49" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.6.6.6.6.6.1.1" class="ltx_text">Fed-Non-IID</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -30.99 15.59)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -30.99 71.26)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">25</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -30.99 126.94)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">30</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -30.99 182.62)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="35" display="inline"><semantics id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">35</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M -12.27 -10.19 L 257.55 -10.19 L 257.55 213.97 L -12.27 213.97 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke="#0000FF" fill="#B3B3FF" color="#0000FF"><path d="M 9.2 -10.19 h 42.93 v 99.27 h -42.93 Z M 131.84 -10.19 h 42.93 v 120.43 h -42.93 Z"></path></g><g></g><g stroke="#FF0000" fill="#FFB3B3" color="#FF0000"><path d="M 70.52 -10.19 h 42.93 v 213.97 h -42.93 Z M 193.16 -10.19 h 42.93 v 199.49 h -42.93 Z"></path></g><g></g><g stroke="#000000" fill="#000000" stroke-width="1.0pt" color="#000000"><path d="M 0 69.04 L 245.29 69.04" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 91.99 74.34)" fill="#000000" stroke="#000000"><foreignObject width="61.31" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.7.7.7.7.7.1.1.1.1" class="ltx_text">Finetuned</span></foreignObject></g></g><g></g></g><g transform="matrix(0.0 1.0 -1.0 0.0 -40.49 72.56)" fill="#000000" stroke="#000000"><foreignObject width="58.65" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.8.8.8.8.8.1.1" class="ltx_text">Macro-F1</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 71.66 227.16)" fill="#000000" stroke="#000000"><foreignObject width="101.97" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.9.9.9.9.9.1.1" class="ltx_text">Bert-Base Model</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000"><path d="M 56.48 -55.64 h 132.33 v 22.75 h -132.33 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 60.63 -52.87)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0.76 0) translate(-0.76,0)" fill="#B3B3FF" stroke="#0000FF" color="#0000FF"><path d="M 1.04 -2.77 h 4.84 v 11.07 h -4.84 Z M 7.96 -2.77 h 4.84 v 8.3 h -4.84 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 13.08 0) translate(28.47,0) matrix(1.0 0.0 0.0 1.0 -25.7 -3.77)" fill="#000000" stroke="#000000"><foreignObject width="52.95" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.10.10.10.10.10.1.1.1.1.1" class="ltx_text">FedAVG</span></foreignObject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 70.78 0) translate(-0.76,0)" fill="#FFB3B3" stroke="#FF0000" color="#FF0000"><path d="M 1.04 -2.77 h 4.84 v 11.07 h -4.84 Z M 7.96 -2.77 h 4.84 v 8.3 h -4.84 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 83.1 0) translate(20.47,0) matrix(1.0 0.0 0.0 1.0 -17.7 -3.69)" fill="#000000" stroke="#000000"><foreignObject width="35.4" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F4.pic1.11.11.11.11.11.2.2.2.1.1" class="ltx_text">Krum</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the SemEval English test dataset for Bert-Base Model.</figcaption>
</figure>
<figure id="S3.T9" class="ltx_table">
<table id="S3.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T9.1.1.1" class="ltx_tr">
<th id="S3.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="S3.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T9.1.2.2" class="ltx_tr">
<th id="S3.T9.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T9.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="S3.T9.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.8%</td>
<td id="S3.T9.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.1%</td>
</tr>
<tr id="S3.T9.1.3.3" class="ltx_tr">
<th id="S3.T9.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="S3.T9.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T9.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">23.6%</td>
<td id="S3.T9.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">29.4%</td>
</tr>
<tr id="S3.T9.1.4.4" class="ltx_tr">
<th id="S3.T9.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T9.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T9.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">21.8%</td>
</tr>
<tr id="S3.T9.1.5.5" class="ltx_tr">
<th id="S3.T9.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T9.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T9.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T9.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T9.1.6.6" class="ltx_tr">
<th id="S3.T9.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T9.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T9.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.4%</td>
<td id="S3.T9.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.3%</td>
</tr>
<tr id="S3.T9.1.7.7" class="ltx_tr">
<th id="S3.T9.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="S3.T9.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T9.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">21.5%</td>
<td id="S3.T9.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">26.6%</td>
</tr>
<tr id="S3.T9.1.8.8" class="ltx_tr">
<th id="S3.T9.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T9.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T9.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">20.8%</td>
</tr>
<tr id="S3.T9.1.9.9" class="ltx_tr">
<th id="S3.T9.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T9.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T9.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T9.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T9.1.10.10" class="ltx_tr">
<th id="S3.T9.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T9.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T9.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.02%</td>
<td id="S3.T9.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.9%</td>
</tr>
<tr id="S3.T9.1.11.11" class="ltx_tr">
<th id="S3.T9.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="S3.T9.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T9.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">20.9%</td>
<td id="S3.T9.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">23.9%</td>
</tr>
<tr id="S3.T9.1.12.12" class="ltx_tr">
<th id="S3.T9.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="S3.T9.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="S3.T9.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">20.3%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the average results for the Twitter multilingual dataset.</figcaption>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Similar to the experiment in Table <a href="#S3.T8" title="Table 8 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Table <a href="#S3.T9" title="Table 9 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows that Krum outperforms FedAVG in both the Non-IID and IID settings as well. Also in Table <a href="#S3.T10" title="Table 10 ‣ 3.5 Multilingual Macro-F1 Results in Label-flipping Attack Scenario ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> reports the results of applying label flipping to the German Zero-shot scenario,</p>
</div>
<figure id="S3.T10" class="ltx_table">
<table id="S3.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T10.1.1.1" class="ltx_tr">
<th id="S3.T10.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S3.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="S3.T10.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T10.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T10.1.2.2" class="ltx_tr">
<th id="S3.T10.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T10.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="S3.T10.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.3%</td>
<td id="S3.T10.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.5%</td>
</tr>
<tr id="S3.T10.1.3.3" class="ltx_tr">
<th id="S3.T10.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="S3.T10.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T10.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">15.7%</td>
<td id="S3.T10.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">21.3%</td>
</tr>
<tr id="S3.T10.1.4.4" class="ltx_tr">
<th id="S3.T10.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T10.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T10.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">16.3%</td>
</tr>
<tr id="S3.T10.1.5.5" class="ltx_tr">
<th id="S3.T10.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T10.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T10.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T10.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T10.1.6.6" class="ltx_tr">
<th id="S3.T10.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T10.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T10.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.1%</td>
<td id="S3.T10.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.9%</td>
</tr>
<tr id="S3.T10.1.7.7" class="ltx_tr">
<th id="S3.T10.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="S3.T10.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T10.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">14.04%</td>
<td id="S3.T10.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">19.8%</td>
</tr>
<tr id="S3.T10.1.8.8" class="ltx_tr">
<th id="S3.T10.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T10.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="S3.T10.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">14.3%</td>
</tr>
<tr id="S3.T10.1.9.9" class="ltx_tr">
<th id="S3.T10.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T10.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T10.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="S3.T10.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="S3.T10.1.10.10" class="ltx_tr">
<th id="S3.T10.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T10.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T10.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.3%</td>
<td id="S3.T10.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.2%</td>
</tr>
<tr id="S3.T10.1.11.11" class="ltx_tr">
<th id="S3.T10.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="S3.T10.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T10.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">11.9%</td>
<td id="S3.T10.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">14.3%</td>
</tr>
<tr id="S3.T10.1.12.12" class="ltx_tr">
<th id="S3.T10.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="S3.T10.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="S3.T10.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">11.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the German Zero-shot.</figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Comparing Results of Clean and Attack Scenarios using Various Aggregation Functions.</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Table <a href="#S3.T11" title="Table 11 ‣ 3.6 Comparing Results of Clean and Attack Scenarios using Various Aggregation Functions. ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> provides valuable insights into the performance of a Bert-Base model in a federated multilingual setting under both clean and attack scenarios. The results highlight the impact of toxic clients on the federated setting, showing a decrease in performance for FedAVG, Fed-IID, and Fed-Non-IID. This decrease in performance is particularly evident when the toxic data constitutes 50% of the overall data. However, the use of the Krum aggregation function can mitigate this drop in performance, as previously observed in literature.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Overall, this table serves to provide a clear comparison of the performance under different scenarios, which can be useful for identifying the most suitable aggregation function for a given federated learning scenario.</p>
</div>
<figure id="S3.T11" class="ltx_table">
<table id="S3.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T11.1.1.1" class="ltx_tr">
<th id="S3.T11.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="2">Setting</th>
<td id="S3.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">Results</td>
</tr>
<tr id="S3.T11.1.2.2" class="ltx_tr">
<th id="S3.T11.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Cleaned</th>
<th id="S3.T11.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="S3.T11.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">36.8</td>
</tr>
<tr id="S3.T11.1.3.3" class="ltx_tr">
<th id="S3.T11.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">FedAVG</th>
<th id="S3.T11.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T11.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r" colspan="2">36.3</td>
</tr>
<tr id="S3.T11.1.4.4" class="ltx_tr">
<th id="S3.T11.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T11.1.4.4.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T11.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25% Toxic</td>
<td id="S3.T11.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50% Toxic</td>
</tr>
<tr id="S3.T11.1.5.5" class="ltx_tr">
<th id="S3.T11.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">FedAVG</th>
<th id="S3.T11.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T11.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.3</td>
<td id="S3.T11.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.2</td>
</tr>
<tr id="S3.T11.1.6.6" class="ltx_tr">
<th id="S3.T11.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S3.T11.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="S3.T11.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">33.5</td>
<td id="S3.T11.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">28.1</td>
</tr>
<tr id="S3.T11.1.7.7" class="ltx_tr">
<th id="S3.T11.1.7.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S3.T11.1.7.7.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S3.T11.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25% Toxic</td>
<td id="S3.T11.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50% Toxic</td>
</tr>
<tr id="S3.T11.1.8.8" class="ltx_tr">
<th id="S3.T11.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Krum</th>
<th id="S3.T11.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="S3.T11.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.5</td>
<td id="S3.T11.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.5</td>
</tr>
<tr id="S3.T11.1.9.9" class="ltx_tr">
<th id="S3.T11.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="S3.T11.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Fed-Non-IID</th>
<td id="S3.T11.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">35.3</td>
<td id="S3.T11.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">35.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Macro-F1 Results of Bert-Base Model in Federated Multilingual Setting under Clean and Attack Scenarios for the SemEval English test dataset.</figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Computational Overhead Comparison between Krum and FedAVG</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">The exact computational overhead of the Krum algorithm compared to FedAvg or other aggregation algorithms depends on several factors, such as the number of participating clients, the size of the models, and the specific implementation of the algorithms. In some cases, the computational overhead of the Krum algorithm may be similar to that of FedAvg or even lower, depending on the specific scenario. Table<a href="#S3.T12" title="Table 12 ‣ 3.7 Computational Overhead Comparison between Krum and FedAVG ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows that the Fed-Non-IID algorithm takes longer than the Fed-IID algorithm due to the varying sizes of language datasets among clients. The Krum algorithm takes <math id="S3.SS7.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS7.p1.1.m1.1a"><mo id="S3.SS7.p1.1.m1.1.1" xref="S3.SS7.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS7.p1.1.m1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.1.m1.1c">\sim</annotation></semantics></math>10% more time than FedAVG in our experiments.</p>
</div>
<figure id="S3.T12" class="ltx_table">
<table id="S3.T12.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T12.4.5.1" class="ltx_tr">
<th id="S3.T12.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Setting</th>
<th id="S3.T12.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Time (Hours)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T12.1.1" class="ltx_tr">
<td id="S3.T12.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T12.1.1.2.1" class="ltx_text">FedAVG</span></td>
<td id="S3.T12.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Fed-IID</td>
<td id="S3.T12.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S3.T12.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T12.1.1.1.m1.1a"><mo id="S3.T12.1.1.1.m1.1.1" xref="S3.T12.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T12.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T12.1.1.1.m1.1.1.cmml" xref="S3.T12.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T12.1.1.1.m1.1c">\sim</annotation></semantics></math>18</td>
</tr>
<tr id="S3.T12.2.2" class="ltx_tr">
<td id="S3.T12.2.2.2" class="ltx_td ltx_align_center ltx_border_r">Fed-Non-IID</td>
<td id="S3.T12.2.2.1" class="ltx_td ltx_align_center ltx_border_r">
<math id="S3.T12.2.2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T12.2.2.1.m1.1a"><mo id="S3.T12.2.2.1.m1.1.1" xref="S3.T12.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T12.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T12.2.2.1.m1.1.1.cmml" xref="S3.T12.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T12.2.2.1.m1.1c">\sim</annotation></semantics></math>22</td>
</tr>
<tr id="S3.T12.3.3" class="ltx_tr">
<td id="S3.T12.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T12.3.3.2.1" class="ltx_text">Krum</span></td>
<td id="S3.T12.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Fed-IID</td>
<td id="S3.T12.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S3.T12.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T12.3.3.1.m1.1a"><mo id="S3.T12.3.3.1.m1.1.1" xref="S3.T12.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T12.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T12.3.3.1.m1.1.1.cmml" xref="S3.T12.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T12.3.3.1.m1.1c">\sim</annotation></semantics></math>20</td>
</tr>
<tr id="S3.T12.4.4" class="ltx_tr">
<td id="S3.T12.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Fed-Non-IID</td>
<td id="S3.T12.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S3.T12.4.4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T12.4.4.1.m1.1a"><mo id="S3.T12.4.4.1.m1.1.1" xref="S3.T12.4.4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T12.4.4.1.m1.1b"><csymbol cd="latexml" id="S3.T12.4.4.1.m1.1.1.cmml" xref="S3.T12.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T12.4.4.1.m1.1c">\sim</annotation></semantics></math>24</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Comparison of computational overhead between FedAVG and Krum algorithms in terms of training time for each federated learning experiment.</figcaption>
</figure>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">To estimate the communicated payload for each client in our federated learning model, the total size of model parameters needs to be calculated. Assuming that the local model is the M-MiniLM model of size 0.47 GB, and there are 4 clients participating in each round, then each client needs to transmit 0.47 GB of data to the server during each round of training. Since there are 5 rounds in total, the total amount of data transmitted per client for one epoch would be 2.35 GB (0.47 GB x 5). This is an approximate estimate and does not take into account factors such as compression techniques or network latency. The actual resource constraints may vary depending on these factors.</p>
</div>
<div id="S3.SS7.p3" class="ltx_para">
<p id="S3.SS7.p3.1" class="ltx_p">Table <a href="#S3.T13" title="Table 13 ‣ 3.7 Computational Overhead Comparison between Krum and FedAVG ‣ 3 Experimental Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the estimated amount of data transmitted during one epoch of federated learning using different models. For the M-MiniLM model, the estimated amount of data transmitted per client for one epoch would be 2.35 GB, which is within the resource constraints for modern devices.</p>
</div>
<figure id="S3.T13" class="ltx_table">
<table id="S3.T13.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T13.1.1.1" class="ltx_tr">
<th id="S3.T13.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model Name</th>
<th id="S3.T13.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Model Size</th>
<th id="S3.T13.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data Transmitted</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T13.1.2.1" class="ltx_tr">
<td id="S3.T13.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Switch-Base-8</td>
<td id="S3.T13.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.24 GB</td>
<td id="S3.T13.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.24*5 = 6.2 GB</td>
</tr>
<tr id="S3.T13.1.3.2" class="ltx_tr">
<td id="S3.T13.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Bert-Base</td>
<td id="S3.T13.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">1.12 GB</td>
<td id="S3.T13.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">1.12*5 = 5.6 GB</td>
</tr>
<tr id="S3.T13.1.4.3" class="ltx_tr">
<td id="S3.T13.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">XLM-R</td>
<td id="S3.T13.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">1.11 GB</td>
<td id="S3.T13.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">1.11*5 = 5.55 GB</td>
</tr>
<tr id="S3.T13.1.5.4" class="ltx_tr">
<td id="S3.T13.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">M-MiniLM</td>
<td id="S3.T13.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.47 GB</td>
<td id="S3.T13.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.47*5 = 2.35 GB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Estimated amount of data transmitted per client during one round of FL.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This paper proposes federated learning-based multilingual emoji prediction in clean and attack scenarios. Different transformer models with varying sizes are trained in centralized and federated for which we compare their corresponding emoji prediction accuracy. Our experiments were carried out in seen and unseen languages using different data sources and distributions. Due to federated performance, federated learning can act as a substitute for centralized settings to gain privacy and access to multiple data sources benefits. In addition, we showed that our federated learning performance is competitive with the SemEval shared task on multilingual emoji prediction. In the future, we wish to explore how to achieve similar accuracy performance while considering the communication efficiency of federated learning <cite class="ltx_cite ltx_citemacro_citep">(Passban et al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. We also believe that more active research in federated learning user personalization <cite class="ltx_cite ltx_citemacro_citep">(Arivazhagan et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> given the subjectivity of emojis can be investigated.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We sincerely thank our invaluable contributors for their unwavering support during the entire project. First and foremost, we thank Dr. Mohamed Afify, Principal Applied Scientist at Microsoft Advanced Technology Lab, for his continuous guidance and encouragement. Dr. Mona Farouk for her diligent supervision and contributions to the DEBI program. We extend our thanks to Dr. Yuanzhu Chen, Professor at Queen’s University, for generously providing us with his workstation. In addition, we would like to thank Dr. Muhammad Jabreel for his valuable suggestions on attack scenarios. Moreover, we express our appreciation to Abdelrahman ElHamoly for his invaluable assistance in the initial phase. Finally, we would like to extend our thanks to Orion Weller for his assistance and prompt responses to our inquiries about federated learning.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agnew (2017)</span>
<span class="ltx_bibblock">
P Agnew. 2017.

</span>
<span class="ltx_bibblock">Emoji report.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al. (2019)</span>
<span class="ltx_bibblock">
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav
Choudhary. 2019.

</span>
<span class="ltx_bibblock">Federated learning with personalization layers.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.00818</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2022)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022.

</span>
<span class="ltx_bibblock">Xlm-t: Multilingual language models in twitter for sentiment analysis
and beyond.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirteenth Language Resources and
Evaluation Conference</em>, pages 258–266.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2020)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Jose Camacho-Collados, Leonardo Neves, and Luis
Espinosa-Anke. 2020.

</span>
<span class="ltx_bibblock">Tweeteval: Unified benchmark and comparative evaluation for tweet
classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.12421</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2018a)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis Espinosa
Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio Saggion.
2018a.

</span>
<span class="ltx_bibblock">Semeval 2018 task 2: Multilingual emoji prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 12th International Workshop on Semantic
Evaluation</em>, pages 24–33.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2018b)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Jose Camacho-Collados, Francesco Ronzano, Luis
Espinosa-Anke, Miguel Ballesteros, Valerio Basile, Viviana Patti, and Horacio
Saggion. 2018b.

</span>
<span class="ltx_bibblock">SemEval-2018 Task 2: Multilingual Emoji Prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Workshop on Semantic
Evaluation (SemEval-2018)</em>, New Orleans, LA, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2018c)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Luís Marujo, Pradeep Karuturi, and William Brendel.
2018c.

</span>
<span class="ltx_bibblock">Multi-task emoji learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Wijeratne S, Kiciman E, Saggion H, Sheth A, editors.
Proceedings of the 1st International Workshop on Emoji Understanding and
Applications in Social Media (Emoji 2018) co-located with the 12th
International AAAI Conference on Web and Social Media (ICWSM 2018); 2018 Jun
25; Stanford, CA.[Aachen]: CEUR; 2018.</em> CEUR Workshop Proceedings.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. (2020)</span>
<span class="ltx_bibblock">
Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier
Fernandez-Marques, Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet,
Pedro Porto Buarque de Gusmão, and Nicholas D. Lane. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2007.14390" title="" class="ltx_ref ltx_href">Flower: A friendly
federated learning research framework</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al. (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldarola et al. (2022)</span>
<span class="ltx_bibblock">
Debora Caldarola, Barbara Caputo, and Marco Ciccone. 2022.

</span>
<span class="ltx_bibblock">Improving generalization in federated learning by seeking flat
minima.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIII</em>, pages 654–672.
Springer.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Camacho-Collados et al. (2022)</span>
<span class="ltx_bibblock">
Jose Camacho-Collados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel
Loureiro, Dimosthenis Antypas, Joanne Boisson, Luis Espinosa-Anke, Fangyu
Liu, Eugenio Martínez-Cámara, et al. 2022.

</span>
<span class="ltx_bibblock">Tweetnlp: Cutting-edge natural language processing for social media.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.14774</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edwards et al. (2020)</span>
<span class="ltx_bibblock">
Aleksandra Edwards, Jose Camacho-Collados, Hélène De Ribaupierre, and
Alun Preece. 2020.

</span>
<span class="ltx_bibblock">Go simple and pre-train on domain-specific corpora: On the role of
training data for text classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th international conference on
computational linguistics</em>, pages 5522–5529.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et al. (2021)</span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer. 2021.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple
and efficient sparsity.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res</em>, 23:1–40.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung et al. (2020)</span>
<span class="ltx_bibblock">
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020.

</span>
<span class="ltx_bibblock">The limitations of federated learning in sybil settings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">RAID</em>, pages 301–316.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et al. (2022)</span>
<span class="ltx_bibblock">
Deep Gandhi, Jash Mehta, Nirali Parekh, Karan Waghela, Lynette D’Mello, and
Zeerak Talat. 2022.

</span>
<span class="ltx_bibblock">A federated approach to predicting emojis in hindi tweets.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.06401</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jebreel et al. (2022)</span>
<span class="ltx_bibblock">
Najeeb Moharram Jebreel, Josep Domingo-Ferrer, David Sánchez, and Alberto
Blanco-Justicia. 2022.

</span>
<span class="ltx_bibblock">Defending against the label-flipping attack in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.01982</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2021)</span>
<span class="ltx_bibblock">
Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas,
Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan
Awadalla. 2021.

</span>
<span class="ltx_bibblock">Scalable and efficient moe training for multitask multilingual
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.10465</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
SangEun Lee, Dahye Jeong, and Eunil Park. 2022.

</span>
<span class="ltx_bibblock">Multiemo: Multi-task framework for emoji prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 242:108437.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE signal processing magazine</em>, 37(3):50–60.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loureiro et al. (2022)</span>
<span class="ltx_bibblock">
Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and
Jose Camacho-Collados. 2022.

</span>
<span class="ltx_bibblock">Timelms: Diachronic language models from twitter.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.03829</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, and Qiang Yang. 2020.

</span>
<span class="ltx_bibblock">Threats to federated learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.02133</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2020)</span>
<span class="ltx_bibblock">
Chuan Ma, Jun Li, Ming Ding, Howard H Yang, Feng Shu, Tony QS Quek, and
H Vincent Poor. 2020.

</span>
<span class="ltx_bibblock">On safeguarding privacy and security in the framework of federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE network</em>, 34(4):242–248.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manoel et al. (2022)</span>
<span class="ltx_bibblock">
Andre Manoel, Mirian Hipolito Garcia, Tal Baumel, Shize Su, Jialei Chen, Dan
Miller, Danny Karmon, Robert Sim, and Dimitrios Dimitriadis. 2022.

</span>
<span class="ltx_bibblock">Federated multilingual models for medical transcript analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09722</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pages 1273–1282.
PMLR.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McShane et al. (2021)</span>
<span class="ltx_bibblock">
Lindsay McShane, Ethan Pancer, Maxwell Poole, and Qi Deng. 2021.

</span>
<span class="ltx_bibblock">Emoji, playfulness, and brand engagement on twitter.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Journal of Interactive Marketing</em>, 53(1):96–110.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passban et al. (2022)</span>
<span class="ltx_bibblock">
Peyman Passban, Tanya Roosta, Rahul Gupta, Ankit Chadha, and Clement Chung.
2022.

</span>
<span class="ltx_bibblock">Training mixed-domain translation models via federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01557</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng and Zhao (2021)</span>
<span class="ltx_bibblock">
Dunlu Peng and Huimin Zhao. 2021.

</span>
<span class="ltx_bibblock">Seq2emoji: A hybrid sequence generation model for short text emoji
prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 214:106727.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramaswamy et al. (2019)</span>
<span class="ltx_bibblock">
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays.
2019.

</span>
<span class="ltx_bibblock">Federated learning for emoji prediction in a mobile keyboard.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.04329</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Regulation (2018)</span>
<span class="ltx_bibblock">
General Data Protection Regulation. 2018.

</span>
<span class="ltx_bibblock">General data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Intersoft Consulting, Accessed in October</em>, 24(1).

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodríguez-Barroso et al. (2022)</span>
<span class="ltx_bibblock">
Nuria Rodríguez-Barroso, Eugenio Martínez-Cámara, M Victoria
Luzón, and Francisco Herrera. 2022.

</span>
<span class="ltx_bibblock">Dynamic defense against byzantine poisoning attacks in federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, 133:1–9.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2022)</span>
<span class="ltx_bibblock">
Pushpa Singh, Murari Kumar Singh, Rajnesh Singh, and Narendra Singh. 2022.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Federated Learning for IoT Applications</em>, pages 199–214.
Springer.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomihira et al. (2020)</span>
<span class="ltx_bibblock">
Toshiki Tomihira, Atsushi Otsuka, Akihiro Yamashita, and Tetsuji Satoh. 2020.

</span>
<span class="ltx_bibblock">Multilingual emoji prediction using bert for sentiment analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Journal of Web Information Systems</em>,
16(3):265–280.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venkit et al. (2021)</span>
<span class="ltx_bibblock">
Pranav Venkit, Zeba Karishma, Chi-Yang Hsu, Rahul Katiki, Kenneth Huang, Shomir
Wilson, and Patrick Dudas. 2021.

</span>
<span class="ltx_bibblock">Asourceful’twist: Emoji prediction based on sentiment, hashtags and
application source.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.07833</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020a)</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.
2020a.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:16070–16084.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020b)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
2020b.

</span>
<span class="ltx_bibblock">Minilm: Deep self-attention distillation for task-agnostic
compression of pre-trained transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:5776–5788.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller et al. (2022)</span>
<span class="ltx_bibblock">
Orion Weller, Marc Marone, Vladimir Braverman, Dawn Lawrie, and Benjamin
Van Durme. 2022.

</span>
<span class="ltx_bibblock">Pretrained models for multilingual federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.02291</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays. 2018.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei
Han, and Ahmed El-Kishky. 2022.

</span>
<span class="ltx_bibblock">Twhin-bert: A socially-enriched pre-trained language model for
multilingual tweet representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07562</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Multilingual Macro-F1 Results in Label-flipping Attack Scenario Results</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Tables <a href="#A1.T14" title="Table 14 ‣ Appendix A Multilingual Macro-F1 Results in Label-flipping Attack Scenario Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> and <a href="#A1.T15" title="Table 15 ‣ Appendix A Multilingual Macro-F1 Results in Label-flipping Attack Scenario Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> present a comparison of centralized and federated learning for different multilingual models in a label-flipping attack scenarios with an Attack Ratio of 25%. The models’ performance was evaluated using two different settings, Fed-IID and Fed-Non-IID, and two federated learning algorithms, FedAVG and Krum. Both tables show that the models’ performance decreases in the label-flipping attack scenario, and is worse for the Fed-Non-IID setting than for the Fed-IID setting. Furthermore, Krum outperforms the FedAVG aggregator in all cases, and the Bert-Base model generally performs better than the other models.</p>
</div>
<figure id="A1.T14" class="ltx_table">
<table id="A1.T14.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T14.1.1.1" class="ltx_tr">
<th id="A1.T14.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="A1.T14.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="A1.T14.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T14.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T14.1.2.2" class="ltx_tr">
<th id="A1.T14.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T14.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="A1.T14.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.3%</td>
<td id="A1.T14.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.5%</td>
</tr>
<tr id="A1.T14.1.3.3" class="ltx_tr">
<th id="A1.T14.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="A1.T14.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T14.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">33.5%</td>
<td id="A1.T14.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">35.3%</td>
</tr>
<tr id="A1.T14.1.4.4" class="ltx_tr">
<th id="A1.T14.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T14.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T14.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">35.6%</td>
</tr>
<tr id="A1.T14.1.5.5" class="ltx_tr">
<th id="A1.T14.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T14.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T14.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T14.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T14.1.6.6" class="ltx_tr">
<th id="A1.T14.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T14.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T14.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.7%</td>
<td id="A1.T14.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.9%</td>
</tr>
<tr id="A1.T14.1.7.7" class="ltx_tr">
<th id="A1.T14.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="A1.T14.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T14.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">32.7%</td>
<td id="A1.T14.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">33.4%</td>
</tr>
<tr id="A1.T14.1.8.8" class="ltx_tr">
<th id="A1.T14.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T14.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T14.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">33.9%</td>
</tr>
<tr id="A1.T14.1.9.9" class="ltx_tr">
<th id="A1.T14.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T14.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T14.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T14.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T14.1.10.10" class="ltx_tr">
<th id="A1.T14.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T14.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T14.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.3%</td>
<td id="A1.T14.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.2%</td>
</tr>
<tr id="A1.T14.1.11.11" class="ltx_tr">
<th id="A1.T14.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="A1.T14.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T14.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">30.4%</td>
<td id="A1.T14.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">30.7%</td>
</tr>
<tr id="A1.T14.1.12.12" class="ltx_tr">
<th id="A1.T14.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="A1.T14.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="A1.T14.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">32.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the SemEval English test dataset.</figcaption>
</figure>
<figure id="A1.T15" class="ltx_table">
<table id="A1.T15.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T15.1.1.1" class="ltx_tr">
<th id="A1.T15.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="A1.T15.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="A1.T15.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T15.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T15.1.2.2" class="ltx_tr">
<th id="A1.T15.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T15.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="A1.T15.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.2%</td>
<td id="A1.T15.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.1%</td>
</tr>
<tr id="A1.T15.1.3.3" class="ltx_tr">
<th id="A1.T15.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="A1.T15.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T15.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">30.5%</td>
<td id="A1.T15.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">29.7%</td>
</tr>
<tr id="A1.T15.1.4.4" class="ltx_tr">
<th id="A1.T15.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T15.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T15.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">28.9%</td>
</tr>
<tr id="A1.T15.1.5.5" class="ltx_tr">
<th id="A1.T15.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T15.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T15.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T15.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T15.1.6.6" class="ltx_tr">
<th id="A1.T15.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T15.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T15.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.1%</td>
<td id="A1.T15.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.7%</td>
</tr>
<tr id="A1.T15.1.7.7" class="ltx_tr">
<th id="A1.T15.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="A1.T15.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T15.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">27.1%</td>
<td id="A1.T15.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">27.3%</td>
</tr>
<tr id="A1.T15.1.8.8" class="ltx_tr">
<th id="A1.T15.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T15.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T15.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">26.7%</td>
</tr>
<tr id="A1.T15.1.9.9" class="ltx_tr">
<th id="A1.T15.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T15.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T15.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T15.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T15.1.10.10" class="ltx_tr">
<th id="A1.T15.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T15.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T15.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.9%</td>
<td id="A1.T15.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.3%</td>
</tr>
<tr id="A1.T15.1.11.11" class="ltx_tr">
<th id="A1.T15.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="A1.T15.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T15.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">24.8%</td>
<td id="A1.T15.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">24.6%</td>
</tr>
<tr id="A1.T15.1.12.12" class="ltx_tr">
<th id="A1.T15.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="A1.T15.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="A1.T15.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">24.6%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 15: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the average results for the Twitter multilingual dataset.</figcaption>
</figure>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">Table <a href="#A1.T16" title="Table 16 ‣ Appendix A Multilingual Macro-F1 Results in Label-flipping Attack Scenario Results ‣ FEDERATED LEARNING BASED MULTILINGUAL EMOJI PREDICTION IN CLEAN AND ATTACK SCENARIOS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> presents a comparison between centralized and federated learning approaches using FedAVG and Krum aggregation functions in a label-flipping attack scenario for three different models. We observed that Krum outperforms FedAVG in all cases, indicating that the choice of aggregation function has a significant impact on the model’s performance. However, we also found that the model’s performance is heavily dependent on the architecture and the type of aggregation function used.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">The results of our experiments highlight the vulnerability of federated learning to label-flipping attacks. This vulnerability emphasizes the importance of carefully selecting the federated learning algorithm and aggregation function to mitigate such attacks. Additionally, our experiments revealed that the Fed-IID setting is less vulnerable to label-flipping attacks, which suggests that data distribution plays a critical role in the performance of federated learning models.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">To further investigate the impact of label-flipping attacks on federated learning models, future experiments can explore the impact of different attack ratios and other attack scenarios. Moreover, it would be interesting to study the impact of other factors, such as the heterogeneity of data sources and the distribution of data samples, on the vulnerability of federated learning models to attacks. By gaining a better understanding of the vulnerabilities of federated learning, we can develop more robust and secure models that are better suited for real-world applications.</p>
</div>
<figure id="A1.T16" class="ltx_table">
<table id="A1.T16.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T16.1.1.1" class="ltx_tr">
<th id="A1.T16.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="A1.T16.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Setting</th>
<td id="A1.T16.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T16.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T16.1.2.2" class="ltx_tr">
<th id="A1.T16.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T16.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Fed-IID</th>
<td id="A1.T16.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.67%</td>
<td id="A1.T16.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.48%</td>
</tr>
<tr id="A1.T16.1.3.3" class="ltx_tr">
<th id="A1.T16.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Bert-Base</th>
<th id="A1.T16.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T16.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">20.60%</td>
<td id="A1.T16.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">21.62%</td>
</tr>
<tr id="A1.T16.1.4.4" class="ltx_tr">
<th id="A1.T16.1.4.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T16.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T16.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">21.6%</td>
</tr>
<tr id="A1.T16.1.5.5" class="ltx_tr">
<th id="A1.T16.1.5.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T16.1.5.5.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T16.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T16.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T16.1.6.6" class="ltx_tr">
<th id="A1.T16.1.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T16.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T16.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.60%</td>
<td id="A1.T16.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.25%</td>
</tr>
<tr id="A1.T16.1.7.7" class="ltx_tr">
<th id="A1.T16.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">XLM-R</th>
<th id="A1.T16.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T16.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">18.81%</td>
<td id="A1.T16.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">19.93%</td>
</tr>
<tr id="A1.T16.1.8.8" class="ltx_tr">
<th id="A1.T16.1.8.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T16.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Finetuned</th>
<td id="A1.T16.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">18.2%</td>
</tr>
<tr id="A1.T16.1.9.9" class="ltx_tr">
<th id="A1.T16.1.9.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="A1.T16.1.9.9.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="A1.T16.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FedAVG</td>
<td id="A1.T16.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Krum</td>
</tr>
<tr id="A1.T16.1.10.10" class="ltx_tr">
<th id="A1.T16.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="A1.T16.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-IID</th>
<td id="A1.T16.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.78%</td>
<td id="A1.T16.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.47%</td>
</tr>
<tr id="A1.T16.1.11.11" class="ltx_tr">
<th id="A1.T16.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">M-MiniLM</th>
<th id="A1.T16.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Fed-Non-IID</th>
<td id="A1.T16.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">14.62%</td>
<td id="A1.T16.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">15.38%</td>
</tr>
<tr id="A1.T16.1.12.12" class="ltx_tr">
<th id="A1.T16.1.12.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"></th>
<th id="A1.T16.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Finetuned</th>
<td id="A1.T16.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="2">14.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 16: </span>Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the German Zero-shot.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.01004" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.01005" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.01005">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.01005" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.01008" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 20:36:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
