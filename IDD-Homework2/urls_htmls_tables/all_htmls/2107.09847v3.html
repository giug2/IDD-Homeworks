<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.09847] CogME: A Novel Evaluation Metric for Video Understanding Intelligence</title><meta property="og:description" content="Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning.
Recentl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CogME: A Novel Evaluation Metric for Video Understanding Intelligence">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CogME: A Novel Evaluation Metric for Video Understanding Intelligence">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.09847">

<!--Generated on Fri Mar  8 14:56:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\correspondance</span>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\extraAuth</span>
<p id="p2.2" class="ltx_p">Jeh-Kwang Ryu 
<br class="ltx_break">ryujk@dongguk.edu</p>
</div>
<h1 class="ltx_title ltx_title_document">CogME: A Novel Evaluation Metric for Video Understanding Intelligence</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minjung Shin <sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Jeonghoon Kim <sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Seongho Choi <sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">3</span></sup>, Yu-Jung Heo <sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">3</span></sup>, Donghyun Kim <sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">1,4</span></sup>, Minsu Lee <sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic">3,5</span></sup>, Byoung-Tak Zhang <sup id="id15.15.id7" class="ltx_sup"><span id="id15.15.id7.1" class="ltx_text ltx_font_italic">3,5,∗</span></sup> and Jeh-Kwang Ryu <sup id="id16.16.id8" class="ltx_sup"><span id="id16.16.id8.1" class="ltx_text ltx_font_italic">1,4,∗</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span></h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning.
Recently, substantial attempts have been made on several video datasets with associated question answering (QA) on a large scale. However, existing evaluation metrics for video question answering (VideoQA) do not provide meaningful analysis.
To make progress, we argue that a well-made framework, established on the way humans understand, is required to explain and evaluate the performance of understanding in detail. Then we propose a top-down evaluation system for VideoQA, based on the cognitive process of humans and story elements: Cognitive Modules for Evaluation (CogME). CogME is composed of three cognitive modules: targets, contents, and thinking. The interaction among the modules in the understanding procedure can be expressed in one sentence as follows: <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">“I understand the CONTENT of the TARGET through a way of THINKING.”</span>
Each module has sub-components derived from the story elements.
We can specify the required aspects of understanding by annotating the sub-components to individual questions.
CogME thus provides a framework for an elaborated specification of VideoQA datasets.
To examine the suitability of a VideoQA dataset for validating video understanding intelligence, we evaluated the baseline model of the DramaQA dataset by applying CogME.
The evaluation reveals that story elements are unevenly reflected in the existing dataset, and the model based on the dataset may cause biased predictions.
Although this study has only been able to grasp a narrow range of stories, we expect that it offers the first step in considering the cognitive process of humans on the video understanding intelligence of humans and AI.</p>
</div>
<div id="S1.p2" class="ltx_para">
<span id="S1.p2.1" class="ltx_ERROR undefined">\helveticabold</span>
</div>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:50%;">
<span class="ltx_tag ltx_tag_section">2 </span>Keywords:</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:50%;">artificial intelligence(AI), video understanding intelligence, VideoQA, cognitive modules, story elements, evaluation</span></p>
</div>
</section>
</section>
</div>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Introduction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">How can we embody the ability to understand in machines as artificial intelligence(AI) and how can we evaluate it? Then, what is understanding?
In the perspective of humans, to understand or comprehend something means that someone gets to know the importance or meaning of something.
It is a specific cognitive state of the subject who understands.
Understanding and comprehension include the capacity for rational thought, inference, or discrimination, and even empathy.
It is possible only when incorporating all those sub-activities of cognition, distinct from simply identifying or classifying input information.
Although there has been a discussion about ‘understanding machines’ for decades <cite class="ltx_cite ltx_citemacro_citep">(Schank and Abelson, <a href="#bib.bib29" title="" class="ltx_ref">1975</a>; Hirschman et al., <a href="#bib.bib18" title="" class="ltx_ref">1999</a>)</cite>, it is still difficult to accomplish all these characteristics of understanding of human-level.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Developing video understanding AI is also challenging to enable an all-inclusive process containing images, scripts, sound with temporal dependencies, as well as natural language.
Moreover, it requires various levels of reasoning <cite class="ltx_cite ltx_citemacro_citep">(Bebensee and Zhang, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>.
Recently, the development of video understanding AI is centered around a large-scale video dataset.
Also an open-ended or multiple-choice question answering(QA) are commonly adopted <cite class="ltx_cite ltx_citemacro_citep">(Patel et al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>, and several metrics based on natural language processing are being used for the evaluation of the AI <cite class="ltx_cite ltx_citemacro_citep">(Aafaq et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In the development of AI, whatever the target we try to deal with, it is crucial to establish sustainable evaluation system.
Obviously, this should start with figuring out the nature of the target performance and constructing a decent framework.
However, there was insufficient consideration in the existing approaches to define understanding or structure the underlying mechanisms.
Therefore, in this paper, we argue that meaningful evaluation of AI about video understanding performance also should be established on the way people understand stories in video because it is a unique experience of human beings to constantly create and enjoy stories.
To establish such an evaluation, we attempted to elucidate the understanding ability of humans and define cognitive modules involved in the understanding process.
Also, we proposed a novel evaluation framework for video story understanding, called <span id="S3.p3.1.1" class="ltx_text ltx_font_bold">CogME</span>, based on story elements as the sub-components of the cognitive modules.
The story element refers to components of a literary work, basically including character, setting, plot, etc., demonstrated in Figure <a href="#Sx5.F1" title="Figure 1 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(A).
The sub-components are capable of a flexible application according to the form of the story.
To the best of our knowledge, this is the first trial that applies the thinking structure of humans to evaluate video understanding AI.
This approach is described in Section 3.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Then, we applied the evaluation metric to examine an existing VideoQA dataset, DramaQA <cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>, which is massive video clips of TV series with annotations including QA.
The story elements were assigned on the questions to identify the information and knowledge required by each question.
At the same time, the correct answer rate for each story element was calculated by combining results from the predictions of AI.
This evaluation process is fully explained in Section 4.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">As discussed in Section 5, the evaluation revealed remarkably uneven performance for each story elements.
Through this analysis, we have identified the elements that exhibit a low performance.
Also, we raised the possibility that it was due to a lack of data containing several elements in learning stage , and prepared a plan to improve the performance by putting the deficient elements in the re-learning stage.
Then we claim that this novel evaluation metric, CogME, is effective as a practical evaluation system for various agents (humans or AIs) who answer the VideoQA and would serve as proper guidance to develop AI.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">In Section 2, before the in-depth discussion of meaningful evaluation for VideoQA, we would like to review recent works in two aspects: techniques and contents.
At first, in terms of techniques, we would review current evaluation metrics for VideoQA.
Subsequently, in terms of the contents, we would introduce studies on machine reading comprehension(MRC), related to which more research is conducted on story understanding of humans, and discuss the applicability in the video story understanding.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Existing Evaluation Metrics in VideoQA</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Evaluation of artificial intelligence on prediction about VideoQA can be divided into automatic and human evaluation.
Two prevailing automatic evaluation criteria, accuracy and Wu-Palmer Similarity(WUPS) score <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite>, are mainly used to evaluate the values predicted by the model in VideoQA.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Automatic Evaluation Metric</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p"><span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Accuracy</span> is commonly used for evaluating multiple-choice question answering. Accuracy is mainly an arithmetic accuracy to see the consistency between generated answers and ground-truth. <span id="S4.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_bold">WUPS score</span> is a metric that measures the quality of an answer and accounts for word-level ambiguities in the answers <cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite>. Hand-chosen threshold decide whether to adjust WUPS score. WUPS score is used as a metric for evaluating models in COCO-QA<cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib28" title="" class="ltx_ref">2015</a>)</cite> and DAQUAR<cite class="ltx_cite ltx_citemacro_citep">(Malinowski and Fritz, <a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite> Dataset, etc. WUPS score is used primarily alongside accuracy as a metric for evaluating the model’s predictions.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p"><span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">BLEU</span> is a precision-based evaluation metric <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib26" title="" class="ltx_ref">2002</a>)</cite> that considers exact n-gram matches developed for evaluating machine translation <cite class="ltx_cite ltx_citemacro_citep">(Nema and Khapra, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>. BLEU scores compute the number of n-grams of references and candidates <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. The higher the BLEU score, the better the performance. BLEU has several advantages that are applicable to multiple languages and demonstrate relatively fast computation.
<span id="S4.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_bold">ROUGE</span> is a F-measure metric designed for evaluating translation and summarization <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a href="#bib.bib21" title="" class="ltx_ref">2004</a>)</cite>. ROUGE-N is an indicator of overlapping n-grams between sentences, such as unigram, bigram, and trigram. ROUGE-L is computed based on the longest common subsequence (LCS), which searches for the longest co-occurring set of tokens that are common to both reference and candidate <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. ROUGE-L has the advantage of no need to redefine n-gram size.
<span id="S4.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_bold">METEOR</span> is a F-measure metric developed for evaluating machine translation <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Lavie, <a href="#bib.bib2" title="" class="ltx_ref">2005</a>)</cite> which operates on unigrams <cite class="ltx_cite ltx_citemacro_citep">(Aafaq et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. METEOR has been proposed to overcome the shortcomings of BLEU. Instead of exact matching required by BLEU, METEOR considers semantic matching, such as derived words, synonyms, and paraphrases. The final score is the harmonic mean of precision and recall.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Human Evaluation Metric</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">As the complexity of the QA dataset and the necessity of open-ended QA increase, it is crucial to grasp whether the current metrics for QA adequately evaluate the model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>.
Human evaluations, conducted by human annotation on the model’s predictions, were used to compare the performance of several metrics.
After training the model on three datasets(i.e. NarrativeQA<cite class="ltx_cite ltx_citemacro_citep">(Kočiskỳ et al., <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, ROPES<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, and SemEval<cite class="ltx_cite ltx_citemacro_citep">(Ostermann et al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>), they extracted the data points that match the gold answers.
And the two annotators evaluated how close the prediction of AI to the gold answer.
The scale of the annotation is from 1 to 5 and the two annotations were averaged.
Then the Spearman and Kendall correlation of the human representative annotation for the scores allocated by the automatic metric were calculated.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">In addition to comparing different automatic metrics, there was also an effort to compare performances between people groups with different characteristics so that reflect the result to the machine learning.
The most representative case is KnowIT VQA <cite class="ltx_cite ltx_citemacro_citep">(Garcia et al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>.
The researchers divided groups based on their background knowledge of TV shows, and then investigated the correlation of inter-group responses.
In human evaluation, the accuracy increased more after exposure than before, verifying the relevance of background knowledge to the question.
With these results, they proposed combining video information with background knowledge.
As shown in the two examples above, human evaluation is beneficial in assessing the performance of AI.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Limitations of Existing Automatic Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">There has always been criticism about n-gram based similarity metrics for performance evaluation including BLEU, etc., since current automatic evaluation metrics based on n-gram similarity are not always well associated with human judgments of question answerability <cite class="ltx_cite ltx_citemacro_citep">(Nema and Khapra, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite>.
In particular, it is essential to ensure that such automatic evaluation metrics focus on questions and answers that contain all relationship of relevant information, common sense, etc.
Another problem is that automatic metrics use paradigms to compute given reference and candidate sentences. Depending on “explained content” (content selection) and “how it was explained” (realization), there may be many different ways to explain the same video at the same time <cite class="ltx_cite ltx_citemacro_citep">(Aafaq et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>.
Although accuracy and F1 scores are mainly used in automatic metrics, they can produce false results on unbalanced datasets because they fail to consider the ratio between positive and negative elements <cite class="ltx_cite ltx_citemacro_citep">(Chicco and Jurman, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Therefore, these existing automatic metrics cannot be expected to evaluate the models comprehensively.
While human evaluation is shown to be useful for studying the limitations of current video modeling , it is necessary to collect more data and comprehensive human annotations on datasets to reflect human evaluations in improving the performance of automatic metrics.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Furthermore, simply collating AI’s QA accuracy with human-level does not indicate what AI understands.
Therefore, it is necessary to analyze the performance of video understanding intelligence according to the nature of understanding.
To develop this aspect of discussion, it would be helpful for us to investigate MRC, which has been studied relatively more.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Story Understanding in Reading Comprehension in Human and AI</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Research into story comprehension in both machines and humans has focused on text-based stories, probably because the history of video is relatively short.
As stated in <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite>, for both humans and machines, reading comprehension is the ability to read text, process it, and understand its meaning, which ultimately generates complex reasoning.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Cognitive science and psychology investigated to ‘what humans pay attention for story comprehension’ and revealed the fact that human subjects tend to focus on the central or structural characteristics of the story rather than specific content <cite class="ltx_cite ltx_citemacro_citep">(Thorndyke, <a href="#bib.bib32" title="" class="ltx_ref">1977</a>)</cite>.
They also showed humans make more reasoning when reading narratives than explanatory texts <cite class="ltx_cite ltx_citemacro_citep">(Graesser et al., <a href="#bib.bib16" title="" class="ltx_ref">1994</a>)</cite>.
More detailed, human’s understanding is divided into five independent dimensions in reading: time, space, character, causality, and motivation in narrative context <cite class="ltx_cite ltx_citemacro_citep">(Zwaan et al., <a href="#bib.bib33" title="" class="ltx_ref">1995</a>)</cite>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Meanwhile, in machinery comprehension, MRC research has also been active as a valuable benchmark for natural language processing over the past decade <cite class="ltx_cite ltx_citemacro_citep">(Hirschman et al., <a href="#bib.bib18" title="" class="ltx_ref">1999</a>)</cite>.
And multiple-choice questions have been widely adopted to test the text understanding of AIs <cite class="ltx_cite ltx_citemacro_citep">(Burges, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>; Baradaran et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>.
However, efforts to apply principles of human reading comprehension to MRC have been made recently.
<cite class="ltx_cite ltx_citemacro_citet">Dunietz et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> employed the results from human studies to set four elements that the machine should understand in reading comprehension: place, time, causality, and motivation.
They pointed out that existing MRCs have a problem, unclearness of what they examine because they did not focus on structuring the tested text.
And, they claimed that the upcoming MRCs should be approached to structure the content of the text by applying a decent template derived from the definition of comprehension <cite class="ltx_cite ltx_citemacro_citep">(Dunietz et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.
When verifying the machine’s understanding ability in QA format, it is also necessary to structure the questions so that derive the appropriate answers by finding the associated elements in the structured text.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Video story understanding would share the basic framework with reading comprehension.
However, implementing or evaluating video understanding intelligence requires quite different templates from conventional reading comprehension.
The most significant difference between understanding video stories and reading is that video stories provide information in a direct and realistic way as optic flow and sound <cite class="ltx_cite ltx_citemacro_citep">(Gibson, <a href="#bib.bib15" title="" class="ltx_ref">1979</a>)</cite>, while the primary information source in reading is natural language text.
In case of the appearance or behavior of the characters, readers depend on the imagery in mental representation, while watchers observe the object directly.
On the contrary, text-based stories often explain a person’s intentions or causal relationships directly by describing their psychology, whereas video-based stories often require to infer the intangible elements based on video scenes <cite class="ltx_cite ltx_citemacro_citep">(Hochberg and Brooks, <a href="#bib.bib19" title="" class="ltx_ref">1996</a>)</cite>.
Therefore, we propose a novel approach to develop and evaluate video understanding intelligence by reflecting the characteristics of video stories and human cognitive processes.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>New evaluation paradigm based on the cognitive process of human</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Cognitive Processes Related to Understanding Video Story</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As mentioned earlier, the video directly provides multi-modal information, such as a sequence of images and sounds as the story progresses over time, and requires a heavy process of data from language-associated domains.
In conventional cognitive psychology, in-depth research has been conducted on information processing through each perceptual module (e.g., visual, auditory, language, etc.). However, when analyzing story comprehension capability, this bottom-up approach does not explain comprehensive experiences gained from the story on the screen <cite class="ltx_cite ltx_citemacro_citep">(Hochberg and Brooks, <a href="#bib.bib19" title="" class="ltx_ref">1996</a>)</cite>.
Due to the broad gap between the perception of information and narrative comprehension in human, it was difficult to reflect humans’ cognitive process in implementation of understanding intelligence in machine learning.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Accordingly, it is desirable to classify the components of the understanding process based on story elements, as Dunizet et al. showed in the studies of MRC.
In other words, the understanding process is appropriate to be structured through narrative schemata as a top-down strategy <cite class="ltx_cite ltx_citemacro_citep">(Brewer, <a href="#bib.bib5" title="" class="ltx_ref">1985</a>)</cite>.
For example, the bottom-up approach describes how the perception of a specific character’s face and voice is processed and integrated through separate sensory channels and pathways.
On the other hand, in the top-down manner, each information such as the face, voice, and name of a specific character is processed as a character’s identity as a whole, although the input channels are different.
Thus, here we argue that it is appropriate to use top-down criteria to validate the understanding process.
To make this concrete, we try to solve this problem by applying the DIKW hierarchy and the Fodorian concept of modularity.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The DIKW (Data, Information, Knowledge, and Wisdom) hierarchy is widely accepted as a representative framework demonstrating different levels of what we see and know<cite class="ltx_cite ltx_citemacro_citep">(Schumaker, <a href="#bib.bib30" title="" class="ltx_ref">2011</a>)</cite>.
To build the structure of the story element, we assumed that ‘data’ is provided from the video, necessary ‘information’ and ‘knowledge’ are determined by the questions, and ‘wisdom’ is required to find the correct answers.
Fodorian concept of modularity initially began with Jerry Fodor’s suggestion that perception and cognition take features of modular systems <cite class="ltx_cite ltx_citemacro_citep">(Fodor, <a href="#bib.bib13" title="" class="ltx_ref">1983</a>)</cite>.
Because this concept provides a convenient and reasonable template for describing complex cognitive functions, cognitive scientists commonly refer to their cognition models as modular <cite class="ltx_cite ltx_citemacro_citep">(Coltheart, <a href="#bib.bib11" title="" class="ltx_ref">1999</a>)</cite>.
We defined that cognitive functions involved in the understanding process are three interacting modules, which have sub-components of story elements.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Cognitive Modules and Their Sub-components for Understanding Video Stories </h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">According to the results of the previous studies and the theory of cognitive science, we established a novel metric - Cognitive Modules for Evaluation (CogME). CogME is a evaluation metric based on the cognitive process of humans, which is composed of three modules: targets, contents, and thinking. The interaction among three modules in the understanding procedure is expressed in one sentence as follows. <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">“I understand the CONTENT of the TARGET through a way of THINKING.”</span>
Thus, CogME evaluates the understanding ability in detail by combining two aspects.
The one is the thinking procedures based on the various information entered through the video, and the other is answering performance for given questions.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The ‘target’ of the understanding is defined as the information perceived by watching the image. The ‘content’ of the understanding is the knowledge obtained through the target information. And ‘thinking’ means a method for deriving knowledge. Inclusively, the understanding process is explained to structured information processing that occurs in the interaction among these three modules (Figure <a href="#Sx5.F1" title="Figure 1 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Each module has sub-components derived from story elements or thinking strategy, that can vary according to the type of story.
The sub-components of cognitive modules are summarized in Table <a href="#Sx5.T1" title="Table 1 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to Table <a href="#Sx5.T3" title="Table 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
In this study, we applied this evaluation method to an existing VideoQA dataset by tagging the elements of CogME to each question.
Then, a comparison was made about each element’s accuracy based on the problem-solving results of AI.
In this way, we tried to examine whether CogME can evaluate the video understanding intelligence from the perspective of cognitive science.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Material and Methods</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Application to VideoQA Dataset: DramaQA</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">This study evaluated the DramaQA dataset which includes ∼ 16K human-generated QA pairs closely centered around the narrative and characters of a TV drama along with character-level annotations <cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Bebensee and Zhang, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>.
It was created from the English dubbed version of the TV series, <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">Another Miss Oh,</span> broadcasted on Korean TV channels in 2016.
<span id="S6.SS1.p1.1.2" class="ltx_text ltx_font_italic">Another Miss Oh</span> consists of 18 episodes with continuity and features several ordinary families, couples, and workplaces.
Character-centered annotations and QA pairs in DramaQA were manufactured by a small number of trained annotators, about two to five people, according to the same manual for all 18 episodes.
This dataset designed to reflect various narrative elements in the stage of making questions, seeking a systematic approach from simple information provided by video to complex reasoning about stories <cite class="ltx_cite ltx_citemacro_citep">(Heo et al., <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>.
The QA set was collected for whole 18 episodes, and the baseline model of DramaQA was trained with the 1st-12th episodes and validated with the 13th-15th episodes.
The remaining 16th-18th episodes will be available for the test going forward.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">We analyzed the questions in the 13th-15th episodes used to validate.
By annotating the story elements of CogME to each question, we obtained a correct rate by the elements to profile the strengths and weaknesses of the baseline model.
In addition, we wanted to find factors that could explain the accuracy of each element.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Annotating and Weighting the Story Elements</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">To classify the information and knowledge required by each question, we manually assigned 1 point to the story elements included in each question.
To present the importance of multiple elements, the most critical target element - the most direct object about which the question was asked - was given a weight of ⁢2. And in the thinking module, a weight of ⁢1.5 was assigned to recognition and ⁢2 to reasoning considering logical complexity.
Examples of annotation on questions are demonstrated in Figure <a href="#Sx5.F2" title="Figure 2 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Figure <a href="#Sx5.F2" title="Figure 2 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(A) displays the cases of annotating story elements to the questions for shots, which are brief video clips with no scene transition.
Among the three rows, the top one is a question that can be answered by simply recalling a single clue. It is sufficient to retrieve information related to a single person appearing in the shot.
The second row shows a question that requires a choice among multiple clues.
It is necessary to recognize and choose between the two people facing Dokyong (in black shirt) - Kyungsu (in gray shirt) and Deogi (in white shirt) - who is looking at Dokyong.
Since the character’s identity must be answered, weight is given to the character element among the multiple targets.
And the bottom one is a question that requires reasoning based on clues in the shot.
It can be answered by inferring Haeyoung1’s feeling through the shot in which she is crying.
Since the feature of her feeling is the key element of the question, the emotion is weighted among multiple targets.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Figure <a href="#Sx5.F2" title="Figure 2 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(B) describes the cases of annotating story elements to the questions in scenes, which are video clips containing several events in a single location.
The top panel shows a question that requires recognition of changes occurring within a scene in sequential order. Since the specific event in the sequence must be answered, weight is given to the event among the multiple targets.
The bottom one is a question that requires reasoning based on the dialogue between two characters. Since the context in their talk is the critical element of the question, the conversation is weighted among multiple targets.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Scoring Criteria to the Elements in QA</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.6" class="ltx_p">Allotting score to each question is derived by <span id="S6.SS3.p1.6.1" class="ltx_text ltx_font_italic">Number of Tagged Target Elements * Weighted Thinking Points</span>, which reflected the degree of difficulty as follows:</p>
<table id="S6.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E1.m1.1" class="ltx_Math" alttext="SC=NT*W_{R}" display="block"><semantics id="S6.E1.m1.1a"><mrow id="S6.E1.m1.1.1" xref="S6.E1.m1.1.1.cmml"><mrow id="S6.E1.m1.1.1.2" xref="S6.E1.m1.1.1.2.cmml"><mi id="S6.E1.m1.1.1.2.2" xref="S6.E1.m1.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.2.1" xref="S6.E1.m1.1.1.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.2.3" xref="S6.E1.m1.1.1.2.3.cmml">C</mi></mrow><mo id="S6.E1.m1.1.1.1" xref="S6.E1.m1.1.1.1.cmml">=</mo><mrow id="S6.E1.m1.1.1.3" xref="S6.E1.m1.1.1.3.cmml"><mrow id="S6.E1.m1.1.1.3.2" xref="S6.E1.m1.1.1.3.2.cmml"><mi id="S6.E1.m1.1.1.3.2.2" xref="S6.E1.m1.1.1.3.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.E1.m1.1.1.3.2.1" xref="S6.E1.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.E1.m1.1.1.3.2.3" xref="S6.E1.m1.1.1.3.2.3.cmml">T</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S6.E1.m1.1.1.3.1" xref="S6.E1.m1.1.1.3.1.cmml">∗</mo><msub id="S6.E1.m1.1.1.3.3" xref="S6.E1.m1.1.1.3.3.cmml"><mi id="S6.E1.m1.1.1.3.3.2" xref="S6.E1.m1.1.1.3.3.2.cmml">W</mi><mi id="S6.E1.m1.1.1.3.3.3" xref="S6.E1.m1.1.1.3.3.3.cmml">R</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.1b"><apply id="S6.E1.m1.1.1.cmml" xref="S6.E1.m1.1.1"><eq id="S6.E1.m1.1.1.1.cmml" xref="S6.E1.m1.1.1.1"></eq><apply id="S6.E1.m1.1.1.2.cmml" xref="S6.E1.m1.1.1.2"><times id="S6.E1.m1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.2.1"></times><ci id="S6.E1.m1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.2.2">𝑆</ci><ci id="S6.E1.m1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.2.3">𝐶</ci></apply><apply id="S6.E1.m1.1.1.3.cmml" xref="S6.E1.m1.1.1.3"><times id="S6.E1.m1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.3.1"></times><apply id="S6.E1.m1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.3.2"><times id="S6.E1.m1.1.1.3.2.1.cmml" xref="S6.E1.m1.1.1.3.2.1"></times><ci id="S6.E1.m1.1.1.3.2.2.cmml" xref="S6.E1.m1.1.1.3.2.2">𝑁</ci><ci id="S6.E1.m1.1.1.3.2.3.cmml" xref="S6.E1.m1.1.1.3.2.3">𝑇</ci></apply><apply id="S6.E1.m1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.3.3.1.cmml" xref="S6.E1.m1.1.1.3.3">subscript</csymbol><ci id="S6.E1.m1.1.1.3.3.2.cmml" xref="S6.E1.m1.1.1.3.3.2">𝑊</ci><ci id="S6.E1.m1.1.1.3.3.3.cmml" xref="S6.E1.m1.1.1.3.3.3">𝑅</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.1c">SC=NT*W_{R}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S6.SS3.p1.5" class="ltx_p">In equation(1), <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="SC" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mrow id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><apply id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"><times id="S6.SS3.p1.1.m1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1"></times><ci id="S6.SS3.p1.1.m1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.2">𝑆</ci><ci id="S6.SS3.p1.1.m1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">SC</annotation></semantics></math> is the allotted score to a question, <math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="NT" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml"><mi id="S6.SS3.p1.2.m2.1.1.2" xref="S6.SS3.p1.2.m2.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.2.m2.1.1.1" xref="S6.SS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS3.p1.2.m2.1.1.3" xref="S6.SS3.p1.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><apply id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"><times id="S6.SS3.p1.2.m2.1.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1.1"></times><ci id="S6.SS3.p1.2.m2.1.1.2.cmml" xref="S6.SS3.p1.2.m2.1.1.2">𝑁</ci><ci id="S6.SS3.p1.2.m2.1.1.3.cmml" xref="S6.SS3.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">NT</annotation></semantics></math> is the number of tagged target elements, and <math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="W_{R}" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><msub id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml"><mi id="S6.SS3.p1.3.m3.1.1.2" xref="S6.SS3.p1.3.m3.1.1.2.cmml">W</mi><mi id="S6.SS3.p1.3.m3.1.1.3" xref="S6.SS3.p1.3.m3.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><apply id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.2">𝑊</ci><ci id="S6.SS3.p1.3.m3.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">W_{R}</annotation></semantics></math> is weighted thinking points.
For example, suppose a question that is assigned to three target elements and reasoning in the thinking module.
It is calculated as 3 * 2 and scored 6 points because given 3 points from the number of targets(<math id="S6.SS3.p1.4.m4.1" class="ltx_Math" alttext="NT" display="inline"><semantics id="S6.SS3.p1.4.m4.1a"><mrow id="S6.SS3.p1.4.m4.1.1" xref="S6.SS3.p1.4.m4.1.1.cmml"><mi id="S6.SS3.p1.4.m4.1.1.2" xref="S6.SS3.p1.4.m4.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.4.m4.1.1.1" xref="S6.SS3.p1.4.m4.1.1.1.cmml">​</mo><mi id="S6.SS3.p1.4.m4.1.1.3" xref="S6.SS3.p1.4.m4.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.4.m4.1b"><apply id="S6.SS3.p1.4.m4.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1"><times id="S6.SS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1.1"></times><ci id="S6.SS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.p1.4.m4.1.1.2">𝑁</ci><ci id="S6.SS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.4.m4.1c">NT</annotation></semantics></math>) and 2 points from the weighted thinking module(<math id="S6.SS3.p1.5.m5.1" class="ltx_Math" alttext="W_{R}" display="inline"><semantics id="S6.SS3.p1.5.m5.1a"><msub id="S6.SS3.p1.5.m5.1.1" xref="S6.SS3.p1.5.m5.1.1.cmml"><mi id="S6.SS3.p1.5.m5.1.1.2" xref="S6.SS3.p1.5.m5.1.1.2.cmml">W</mi><mi id="S6.SS3.p1.5.m5.1.1.3" xref="S6.SS3.p1.5.m5.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.5.m5.1b"><apply id="S6.SS3.p1.5.m5.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.5.m5.1.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S6.SS3.p1.5.m5.1.1.2.cmml" xref="S6.SS3.p1.5.m5.1.1.2">𝑊</ci><ci id="S6.SS3.p1.5.m5.1.1.3.cmml" xref="S6.SS3.p1.5.m5.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.5.m5.1c">W_{R}</annotation></semantics></math>).</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.5" class="ltx_p">And the way of assigning a score to each target is as follows:</p>
<table id="S6.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E2.m1.1" class="ltx_Math" alttext="T_{sum}=NT+1" display="block"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><msub id="S6.E2.m1.1.1.2" xref="S6.E2.m1.1.1.2.cmml"><mi id="S6.E2.m1.1.1.2.2" xref="S6.E2.m1.1.1.2.2.cmml">T</mi><mrow id="S6.E2.m1.1.1.2.3" xref="S6.E2.m1.1.1.2.3.cmml"><mi id="S6.E2.m1.1.1.2.3.2" xref="S6.E2.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.2.3.1" xref="S6.E2.m1.1.1.2.3.1.cmml">​</mo><mi id="S6.E2.m1.1.1.2.3.3" xref="S6.E2.m1.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.2.3.1a" xref="S6.E2.m1.1.1.2.3.1.cmml">​</mo><mi id="S6.E2.m1.1.1.2.3.4" xref="S6.E2.m1.1.1.2.3.4.cmml">m</mi></mrow></msub><mo id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.cmml">=</mo><mrow id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml"><mrow id="S6.E2.m1.1.1.3.2" xref="S6.E2.m1.1.1.3.2.cmml"><mi id="S6.E2.m1.1.1.3.2.2" xref="S6.E2.m1.1.1.3.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.E2.m1.1.1.3.2.1" xref="S6.E2.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.E2.m1.1.1.3.2.3" xref="S6.E2.m1.1.1.3.2.3.cmml">T</mi></mrow><mo id="S6.E2.m1.1.1.3.1" xref="S6.E2.m1.1.1.3.1.cmml">+</mo><mn id="S6.E2.m1.1.1.3.3" xref="S6.E2.m1.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><eq id="S6.E2.m1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"></eq><apply id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.2">subscript</csymbol><ci id="S6.E2.m1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.2.2">𝑇</ci><apply id="S6.E2.m1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.2.3"><times id="S6.E2.m1.1.1.2.3.1.cmml" xref="S6.E2.m1.1.1.2.3.1"></times><ci id="S6.E2.m1.1.1.2.3.2.cmml" xref="S6.E2.m1.1.1.2.3.2">𝑠</ci><ci id="S6.E2.m1.1.1.2.3.3.cmml" xref="S6.E2.m1.1.1.2.3.3">𝑢</ci><ci id="S6.E2.m1.1.1.2.3.4.cmml" xref="S6.E2.m1.1.1.2.3.4">𝑚</ci></apply></apply><apply id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3"><plus id="S6.E2.m1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.3.1"></plus><apply id="S6.E2.m1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.3.2"><times id="S6.E2.m1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.3.2.1"></times><ci id="S6.E2.m1.1.1.3.2.2.cmml" xref="S6.E2.m1.1.1.3.2.2">𝑁</ci><ci id="S6.E2.m1.1.1.3.2.3.cmml" xref="S6.E2.m1.1.1.3.2.3">𝑇</ci></apply><cn type="integer" id="S6.E2.m1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">T_{sum}=NT+1</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S6.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S6.E3.m1.1" class="ltx_Math" alttext="T_{i}=SC*\frac{W_{T}}{T_{sum}}" display="block"><semantics id="S6.E3.m1.1a"><mrow id="S6.E3.m1.1.1" xref="S6.E3.m1.1.1.cmml"><msub id="S6.E3.m1.1.1.2" xref="S6.E3.m1.1.1.2.cmml"><mi id="S6.E3.m1.1.1.2.2" xref="S6.E3.m1.1.1.2.2.cmml">T</mi><mi id="S6.E3.m1.1.1.2.3" xref="S6.E3.m1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.1.1.1" xref="S6.E3.m1.1.1.1.cmml">=</mo><mrow id="S6.E3.m1.1.1.3" xref="S6.E3.m1.1.1.3.cmml"><mrow id="S6.E3.m1.1.1.3.2" xref="S6.E3.m1.1.1.3.2.cmml"><mi id="S6.E3.m1.1.1.3.2.2" xref="S6.E3.m1.1.1.3.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.3.2.1" xref="S6.E3.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.E3.m1.1.1.3.2.3" xref="S6.E3.m1.1.1.3.2.3.cmml">C</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S6.E3.m1.1.1.3.1" xref="S6.E3.m1.1.1.3.1.cmml">∗</mo><mfrac id="S6.E3.m1.1.1.3.3" xref="S6.E3.m1.1.1.3.3.cmml"><msub id="S6.E3.m1.1.1.3.3.2" xref="S6.E3.m1.1.1.3.3.2.cmml"><mi id="S6.E3.m1.1.1.3.3.2.2" xref="S6.E3.m1.1.1.3.3.2.2.cmml">W</mi><mi id="S6.E3.m1.1.1.3.3.2.3" xref="S6.E3.m1.1.1.3.3.2.3.cmml">T</mi></msub><msub id="S6.E3.m1.1.1.3.3.3" xref="S6.E3.m1.1.1.3.3.3.cmml"><mi id="S6.E3.m1.1.1.3.3.3.2" xref="S6.E3.m1.1.1.3.3.3.2.cmml">T</mi><mrow id="S6.E3.m1.1.1.3.3.3.3" xref="S6.E3.m1.1.1.3.3.3.3.cmml"><mi id="S6.E3.m1.1.1.3.3.3.3.2" xref="S6.E3.m1.1.1.3.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.3.3.3.3.1" xref="S6.E3.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.3.3.3.3.3" xref="S6.E3.m1.1.1.3.3.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.E3.m1.1.1.3.3.3.3.1a" xref="S6.E3.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S6.E3.m1.1.1.3.3.3.3.4" xref="S6.E3.m1.1.1.3.3.3.3.4.cmml">m</mi></mrow></msub></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.1b"><apply id="S6.E3.m1.1.1.cmml" xref="S6.E3.m1.1.1"><eq id="S6.E3.m1.1.1.1.cmml" xref="S6.E3.m1.1.1.1"></eq><apply id="S6.E3.m1.1.1.2.cmml" xref="S6.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.2.2">𝑇</ci><ci id="S6.E3.m1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.1.1.3.cmml" xref="S6.E3.m1.1.1.3"><times id="S6.E3.m1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.3.1"></times><apply id="S6.E3.m1.1.1.3.2.cmml" xref="S6.E3.m1.1.1.3.2"><times id="S6.E3.m1.1.1.3.2.1.cmml" xref="S6.E3.m1.1.1.3.2.1"></times><ci id="S6.E3.m1.1.1.3.2.2.cmml" xref="S6.E3.m1.1.1.3.2.2">𝑆</ci><ci id="S6.E3.m1.1.1.3.2.3.cmml" xref="S6.E3.m1.1.1.3.2.3">𝐶</ci></apply><apply id="S6.E3.m1.1.1.3.3.cmml" xref="S6.E3.m1.1.1.3.3"><divide id="S6.E3.m1.1.1.3.3.1.cmml" xref="S6.E3.m1.1.1.3.3"></divide><apply id="S6.E3.m1.1.1.3.3.2.cmml" xref="S6.E3.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.2.1.cmml" xref="S6.E3.m1.1.1.3.3.2">subscript</csymbol><ci id="S6.E3.m1.1.1.3.3.2.2.cmml" xref="S6.E3.m1.1.1.3.3.2.2">𝑊</ci><ci id="S6.E3.m1.1.1.3.3.2.3.cmml" xref="S6.E3.m1.1.1.3.3.2.3">𝑇</ci></apply><apply id="S6.E3.m1.1.1.3.3.3.cmml" xref="S6.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.3.3.3.1.cmml" xref="S6.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S6.E3.m1.1.1.3.3.3.2.cmml" xref="S6.E3.m1.1.1.3.3.3.2">𝑇</ci><apply id="S6.E3.m1.1.1.3.3.3.3.cmml" xref="S6.E3.m1.1.1.3.3.3.3"><times id="S6.E3.m1.1.1.3.3.3.3.1.cmml" xref="S6.E3.m1.1.1.3.3.3.3.1"></times><ci id="S6.E3.m1.1.1.3.3.3.3.2.cmml" xref="S6.E3.m1.1.1.3.3.3.3.2">𝑠</ci><ci id="S6.E3.m1.1.1.3.3.3.3.3.cmml" xref="S6.E3.m1.1.1.3.3.3.3.3">𝑢</ci><ci id="S6.E3.m1.1.1.3.3.3.3.4.cmml" xref="S6.E3.m1.1.1.3.3.3.3.4">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.1c">T_{i}=SC*\frac{W_{T}}{T_{sum}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S6.SS3.p2.4" class="ltx_p">In equation (2), <math id="S6.SS3.p2.1.m1.1" class="ltx_Math" alttext="T_{sum}" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><msub id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml"><mi id="S6.SS3.p2.1.m1.1.1.2" xref="S6.SS3.p2.1.m1.1.1.2.cmml">T</mi><mrow id="S6.SS3.p2.1.m1.1.1.3" xref="S6.SS3.p2.1.m1.1.1.3.cmml"><mi id="S6.SS3.p2.1.m1.1.1.3.2" xref="S6.SS3.p2.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.1.m1.1.1.3.1" xref="S6.SS3.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p2.1.m1.1.1.3.3" xref="S6.SS3.p2.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.1.m1.1.1.3.1a" xref="S6.SS3.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p2.1.m1.1.1.3.4" xref="S6.SS3.p2.1.m1.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><apply id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.1.m1.1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS3.p2.1.m1.1.1.2.cmml" xref="S6.SS3.p2.1.m1.1.1.2">𝑇</ci><apply id="S6.SS3.p2.1.m1.1.1.3.cmml" xref="S6.SS3.p2.1.m1.1.1.3"><times id="S6.SS3.p2.1.m1.1.1.3.1.cmml" xref="S6.SS3.p2.1.m1.1.1.3.1"></times><ci id="S6.SS3.p2.1.m1.1.1.3.2.cmml" xref="S6.SS3.p2.1.m1.1.1.3.2">𝑠</ci><ci id="S6.SS3.p2.1.m1.1.1.3.3.cmml" xref="S6.SS3.p2.1.m1.1.1.3.3">𝑢</ci><ci id="S6.SS3.p2.1.m1.1.1.3.4.cmml" xref="S6.SS3.p2.1.m1.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">T_{sum}</annotation></semantics></math> <math id="S6.SS3.p2.2.m2.1" class="ltx_Math" alttext="(i\leq NT)" display="inline"><semantics id="S6.SS3.p2.2.m2.1a"><mrow id="S6.SS3.p2.2.m2.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S6.SS3.p2.2.m2.1.1.1.2" xref="S6.SS3.p2.2.m2.1.1.1.1.cmml">(</mo><mrow id="S6.SS3.p2.2.m2.1.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.1.cmml"><mi id="S6.SS3.p2.2.m2.1.1.1.1.2" xref="S6.SS3.p2.2.m2.1.1.1.1.2.cmml">i</mi><mo id="S6.SS3.p2.2.m2.1.1.1.1.1" xref="S6.SS3.p2.2.m2.1.1.1.1.1.cmml">≤</mo><mrow id="S6.SS3.p2.2.m2.1.1.1.1.3" xref="S6.SS3.p2.2.m2.1.1.1.1.3.cmml"><mi id="S6.SS3.p2.2.m2.1.1.1.1.3.2" xref="S6.SS3.p2.2.m2.1.1.1.1.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p2.2.m2.1.1.1.1.3.1" xref="S6.SS3.p2.2.m2.1.1.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p2.2.m2.1.1.1.1.3.3" xref="S6.SS3.p2.2.m2.1.1.1.1.3.3.cmml">T</mi></mrow></mrow><mo stretchy="false" id="S6.SS3.p2.2.m2.1.1.1.3" xref="S6.SS3.p2.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.2.m2.1b"><apply id="S6.SS3.p2.2.m2.1.1.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1.1"><leq id="S6.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.1"></leq><ci id="S6.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.2">𝑖</ci><apply id="S6.SS3.p2.2.m2.1.1.1.1.3.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.3"><times id="S6.SS3.p2.2.m2.1.1.1.1.3.1.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.3.1"></times><ci id="S6.SS3.p2.2.m2.1.1.1.1.3.2.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.3.2">𝑁</ci><ci id="S6.SS3.p2.2.m2.1.1.1.1.3.3.cmml" xref="S6.SS3.p2.2.m2.1.1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">(i\leq NT)</annotation></semantics></math> means total points of tagged target elements from each question, including weight.
Since only one component is given the weight, that is 2 points; total points are the same as the number of tagged elements plus one.
<math id="S6.SS3.p2.3.m3.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S6.SS3.p2.3.m3.1a"><msub id="S6.SS3.p2.3.m3.1.1" xref="S6.SS3.p2.3.m3.1.1.cmml"><mi id="S6.SS3.p2.3.m3.1.1.2" xref="S6.SS3.p2.3.m3.1.1.2.cmml">T</mi><mi id="S6.SS3.p2.3.m3.1.1.3" xref="S6.SS3.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.3.m3.1b"><apply id="S6.SS3.p2.3.m3.1.1.cmml" xref="S6.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.3.m3.1.1.1.cmml" xref="S6.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S6.SS3.p2.3.m3.1.1.2.cmml" xref="S6.SS3.p2.3.m3.1.1.2">𝑇</ci><ci id="S6.SS3.p2.3.m3.1.1.3.cmml" xref="S6.SS3.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.3.m3.1c">T_{i}</annotation></semantics></math> in equation (3) is the determining score given to individual elements, which distributes the total points considering the weight<math id="S6.SS3.p2.4.m4.1" class="ltx_Math" alttext="(W_{T})" display="inline"><semantics id="S6.SS3.p2.4.m4.1a"><mrow id="S6.SS3.p2.4.m4.1.1.1" xref="S6.SS3.p2.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S6.SS3.p2.4.m4.1.1.1.2" xref="S6.SS3.p2.4.m4.1.1.1.1.cmml">(</mo><msub id="S6.SS3.p2.4.m4.1.1.1.1" xref="S6.SS3.p2.4.m4.1.1.1.1.cmml"><mi id="S6.SS3.p2.4.m4.1.1.1.1.2" xref="S6.SS3.p2.4.m4.1.1.1.1.2.cmml">W</mi><mi id="S6.SS3.p2.4.m4.1.1.1.1.3" xref="S6.SS3.p2.4.m4.1.1.1.1.3.cmml">T</mi></msub><mo stretchy="false" id="S6.SS3.p2.4.m4.1.1.1.3" xref="S6.SS3.p2.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.4.m4.1b"><apply id="S6.SS3.p2.4.m4.1.1.1.1.cmml" xref="S6.SS3.p2.4.m4.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S6.SS3.p2.4.m4.1.1.1">subscript</csymbol><ci id="S6.SS3.p2.4.m4.1.1.1.1.2.cmml" xref="S6.SS3.p2.4.m4.1.1.1.1.2">𝑊</ci><ci id="S6.SS3.p2.4.m4.1.1.1.1.3.cmml" xref="S6.SS3.p2.4.m4.1.1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.4.m4.1c">(W_{T})</annotation></semantics></math>.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">After assigning associated elements to each question, the overall performance score of the agent (human or AI) is determined by multiplying the correct prediction by 1 and the wrong one by 0.
To create a profile, the percentage of correct predictions compared to the total score was calculated.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Results and Discussion</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Frequency and Accuracy by Story Elements in DramaQA</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">We evaluated the performance of an AI using the baseline model of the DramaQA dataset proposed in <cite class="ltx_cite ltx_citemacro_citet">Choi et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.
To make the CogME profile, we analyzed 4,385 questions that are used to validate the understanding capability of the model.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">The distribution of story elements included in the question and the accuracy profiles for each module, obtained from the application of CogME to the dataset, were summarized Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(A) shows the frequency of story elements which is the characteristics of the dataset.
And the accuracy profiles, shown in Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(B)-(D), display elaborated evaluation that the total score cannot offer.
These analysis results reveal that the distribution of story elements is generally uneven(Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(A)), and the performance of predicting each component is unbalanced(Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(B)-(D)).</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">Especially, the elements with the correct answer rate of less than 50% were commonsense (35.4%) in the target module, and relationship (34.8%), means (40.2%), causality (45.2%) in the content module.
Among the elements, commonsense, relationship, and causality almost require reasoning so that the high logical complexity may be the cause of row correct rate.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p">However, those factors have low values not only in correct answer rate but also in frequency (Figure <a href="#Sx5.F3" title="Figure 3 ‣ Figure Captions ‣ CogME: A Novel Evaluation Metric for Video Understanding Intelligence" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(A)).
On the other hand, in case of emotion or motivation elements, the performance is relatively high (emotion, 64.6%; motivation, 61.3%), even though most of the questions also require reasoning.
Through the result, we assume that a learning deficiency is another decisive factor aggravating QA performance.
It can be pointed out that this result does not reflect the lack of those factors in the training session, as it is analyzed on only a part of the validation session, i.e., the 13th-15th of entire 18 episodes.
However, it is reasonable to assume that the training QA set also has similar distribution with the analyzed one since the small trained annotator group constructed the entire QA set according to the same manual.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>General Implications of CogME</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Understanding is a high-level cognitive function in which the DIKW hierarchy functions comprehensively.
Especially, understanding narrative media requires a lot of reasoning.
Since stories are generated and expended by humans, it is acceptable to set the criteria on humans’ performance to evaluate the understanding capability of narratives.
Even though we are in the midst of a breakthrough in AI development, there is still a significant gap in the reasoning capability between humans and AI regarding fluency and naturality.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">As a first step towards narrowing the disparity, we set up an evaluation metric based on the cognitive modules involved in understanding.
More specifically, we established detailed sub-components of the modules from the story elements as a top-down strategy.
Then, we demonstrated a quantified profile of an AI that determines the strong and weak parts in comprehension of the AI by employing this metric to an existing dataset.
As a result, a thorough assessment of the understanding ability of AI was made by conducting performance measurements.
Moreover, this assessment is expected to be applicable regardless of whether the agent is machines or humans, and eventually provide comprehensive quantification of the features of the agents’ level of understanding.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">Furthermore, when planning establishment of the dataset, it can be used as a theoretical foundation to prevent specific elements from being excluded from learning and ensure that all aspects of the story are included.
Besides, this approach allows comparing the video understanding performance of AI with human level, especially by reflecting human characteristics such as age and gender, through an analytical process that the existing metrics did not consider.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Limitations of the Study</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">However, this study has two aspects of limitations regarding the capacity of AI and the design of the dataset.
The first one was that this analysis dealt with a much lower level of story information than people commonly encounter.
Usually, story understanding requires abstract thinking, such as identifying conflicts, themes, and the tone in the story, and people demonstrate such abilities by nature.
Although the AI analyzed in this study demonstrates one of the world’s leading performances, it is still challenging to compare directly with the ability of humans to understand narratives. First, It is due to that AI can handle only a little video capacity in less than a few minutes. And AI shows the lack of ability to make a complex inference by naturally combining various story elements, as for now.
Therefore, to find a complement, it is worth providing structured information to AI for reflecting various story elements with an analytical evaluation rather than a holistic approach with the total score of answering the question.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">Another obstacle in respect to the design of the dataset was revealed when agents chose a wrong answer. It was impossible to distinguish for now which element underwent improper process in most cases due to the dataset’s trait, which is multiple-choice questions with one correct answer.
Indeed, the correct ratio of the ‘Who’ question was close to 95%, but the character element’s total correct rate was 76.9%, similar to the overall answer rate(72.8%).
‘Who’ question requires recognizing and choosing a particular person’s identity, so the high answer rate for ‘Who’ questions implies AI has excellent character recognition capabilities. It is due to the character annotations on the videos.
However, the ‘Who’ question is only part of the entire dataset; the character element was tagged in almost every question.
Although the AI handled the character’s identity very well in most questions, the character element gained 0 points when a wrong prediction occurred due to other factors.
This problem can be improved by changing the form of the question; for example, each option of multiple choices reflects a different factor.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">Nevertheless, this evaluation analysis is still helpful because it provides detailed profiles about the story elements that the overall correct answer rate cannot explain.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Concluding Remarks and Future Studies</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Stories, especially narrative ones, are inherently embodied and enjoyed by people.
And there has been a discussion about machines that can understand or produce stories for decades.
Among the machines that operate the understanding process, AI that can understand videos is challenging to implement, as it should handle massive inputs from complex structures.
Most of the attempts that have been made recently were large annotated video and VideoQA datasets, but existing metrics do not provide meaningful analysis.
To make progress in video-understandable AI, a well-made framework is required to explain the procedure and evaluate the performance of understanding in detail.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">We proposed a top-down evaluation system for VideoQA, CogME, based on the cognitive module of humans and story elements.
CogME is composed of three cognitive modules: targets, contents, and thinking, and each module has sub-components derived from the story elements. Then we specified what aspects of understanding each question tests by tagging the sub-components to individual questions within the dataset.
Also, we evaluated the baseline model of the DramaQA dataset by applying CogME.
The profile created through the analysis showed an unbalanced shape, particularly lacking in the accuracy of commonsense, relationship, means, and causality factors.
Besides, the evaluation revealed that story elements are unevenly reflected in existing datasets and may cause low performance.
Though this study covered only a narrow range of stories with short-form videos, it is worthwhile as the first attempt to consider human cognitive processes in identifying understanding intelligence of humans and AI.
We expect that our proposed approach can serve as proper guidance to develop the video-understanding AI and suggest the following extended studies.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">We would try to examine the improvement of the performance of AI by retraining with QA set to which low-frequency story elements (e.g., common sense, relationship, means, causality) are added.
Simply by exposing AI to various content elements at the learning stage, we can expect improved overall performance, which means close to ”understanding like humans” by referring to the human’s cognitive process.
Besides, the procedure of specifying the content elements contained in the question is conducted according to a set of rules so that we can consider automating the tagging process. Automatized tagging would grant efficiency to analyzing large-scale datasets.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">In Addition, evaluations utilizing story elements are not limited to multiple-choice QAs. Simply choosing the most appropriate one among the multiple-choice questions is only a part of comprehension. Thus, this work can be extended to other performances of determining whether the agent understood the story, such as a open-ended or cloze tests, summary, and rewriting, which may also use story elements.
And regardless of the form of the question, we can compare performances in detail between different agents(e.g., AI vs. human).
These comparisons can be accomplished in an analytic way, such as applying the same QA set to AI and humans, or in a holistic way such as the Turing test.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">Although we have established a framework that reflects human cognitive processes, many parts of the understanding of video remain unknown due to its relatively short history.
At the current level, cognitive science research related to video needs to be revised and supplemented over and over again.
Nowadays, research is actively conducted to measure and analyze humans’ psychological responses to the video media such as movies <cite class="ltx_cite ltx_citemacro_citep">(Tan, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite>, so it is expected that more sophisticated frameworks can be created by actively combining updated knowledge with artificial intelligence development.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">MS developed the idea and scheme of CogME and performed the analysis of the DramaQA dataset.
JK reviewed and the VideoQA related evaluation metrics.
SC provided experimental results of baseline model for DramaQA dataset and reviewed the analyses.
YH and ML shared related previous works, reviewed the progress of the research, and revised the manuscript critically.
DK embodied the concepts and set the range of individual sub-components within cognitive modules.
BZ and JR offered substantial contributions to the initial design for this work and supervised all the processes of this work. All authors discussed the results and commented on the manuscript.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Funding</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors received financial support for the research, authorship, and/or publication of this article: This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government(MSIT) (No. IITP-2017-0-01772-VTT, No. IITP-2019-0-00050-AICRC)</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data Availability Statement</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">The datasets generated for this study are available on request to the corresponding author.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aafaq et al. (2019)</span>
<span class="ltx_bibblock">
Aafaq, N., Mian, A., Liu, W., Gilani, S. Z., and Shah, M. (2019).

</span>
<span class="ltx_bibblock">Video description: A survey of methods, datasets, and evaluation
metrics.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em> 52, 1–37


</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Banerjee, S. and Lavie, A. (2005).

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the acl workshop on intrinsic and extrinsic
evaluation measures for machine translation and/or summarization</em>. 65–72


</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baradaran et al. (2020)</span>
<span class="ltx_bibblock">
Baradaran, R., Ghiasi, R., and Amirkhani, H. (2020).

</span>
<span class="ltx_bibblock">A survey on machine reading comprehension systems


</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bebensee and Zhang (2021)</span>
<span class="ltx_bibblock">
Bebensee, B. and Zhang, B.-T. (2021).

</span>
<span class="ltx_bibblock">Co-attentional transformers for story-based video understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>. 4005–4009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICASSP39728.2021.9413868" title="" class="ltx_ref">10.1109/ICASSP39728.2021.9413868</a>


</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brewer (1985)</span>
<span class="ltx_bibblock">
Brewer, W. F. (1985).

</span>
<span class="ltx_bibblock">The story schema: Universal and culture-specific properties.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Literacy, language, and learning</em> , 167–194


</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burges (2013)</span>
<span class="ltx_bibblock">
Burges, C. J. (2013).

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Towards the Machine Comprehension of Text: An Essay</em>.

</span>
<span class="ltx_bibblock">Tech. Rep. MSR-TR-2013-125


</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Chen, A., Stanovsky, G., Singh, S., and Gardner, M. (2019).

</span>
<span class="ltx_bibblock">Evaluating question answering evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Machine Reading for
Question Answering</em>. 119–124


</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2016)</span>
<span class="ltx_bibblock">
Chen, D., Bolton, J., and Manning, C. D. (2016).

</span>
<span class="ltx_bibblock">A thorough examination of the CNN/Daily Mail reading
comprehension task.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em> (Berlin, Germany:
Association for Computational Linguistics), 2358–2367.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/P16-1223" title="" class="ltx_ref">10.18653/v1/P16-1223</a>


</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chicco and Jurman (2020)</span>
<span class="ltx_bibblock">
Chicco, D. and Jurman, G. (2020).

</span>
<span class="ltx_bibblock">The advantages of the matthews correlation coefficient (mcc) over f1
score and accuracy in binary classification evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">BMC genomics</em> 21, 1–13


</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2021)</span>
<span class="ltx_bibblock">
Choi, S., On, K.-W., Heo, Y.-J., Seo, A., Jang, Y., Lee, M., et al. (2021).

</span>
<span class="ltx_bibblock">Dramaqa: Character-centered video story understanding with
hierarchical qa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>. vol. 35, 1166–1174


</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coltheart (1999)</span>
<span class="ltx_bibblock">
Coltheart, M. (1999).

</span>
<span class="ltx_bibblock">Modularity and cognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Trends in Cognitive Sciences</em> 3, 115–120


</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunietz et al. (2020)</span>
<span class="ltx_bibblock">
Dunietz, J., Burnham, G., Bharadwaj, A., Rambow, O., Chu-Carroll, J., and
Ferrucci, D. (2020).

</span>
<span class="ltx_bibblock">To test machine comprehension, start by defining comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em> (Online: Association for Computational
Linguistics), 7839–7859.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.18653/v1/2020.acl-main.701" title="" class="ltx_ref">10.18653/v1/2020.acl-main.701</a>


</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fodor (1983)</span>
<span class="ltx_bibblock">
Fodor, J. A. (1983).

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The Modularity of Mind: An Essay on Faculty Psychology</em> (MIT
Press)


</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et al. (2020)</span>
<span class="ltx_bibblock">
Garcia, N., Otani, M., Chu, C., and Nakashima, Y. (2020).

</span>
<span class="ltx_bibblock">Knowit vqa: Answering knowledge-based questions about videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>. vol. 34, 10826–10834


</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibson (1979)</span>
<span class="ltx_bibblock">
Gibson, J. J. (1979).

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">The ecological approach to visual perception.</em> (Houghton,
Mifflin and Company)


</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graesser et al. (1994)</span>
<span class="ltx_bibblock">
Graesser, A., Singer, M., and Trabasso, T. (1994).

</span>
<span class="ltx_bibblock">Constructing inferences during narrative text comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Psychological review</em> 101 3, 371–95


</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heo et al. (2019)</span>
<span class="ltx_bibblock">
Heo, Y.-J., On, K.-W., Choi, S., Lim, J., Kim, J., Ryu, J.-K., et al. (2019).

</span>
<span class="ltx_bibblock">Constructing hierarchical q&amp;a datasets for video story
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.00623</em>


</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirschman et al. (1999)</span>
<span class="ltx_bibblock">
Hirschman, L., Light, M., Breck, E., and Burger, J. D. (1999).

</span>
<span class="ltx_bibblock">Deep read: A reading comprehension system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th annual meeting of the Association
for Computational Linguistics</em>. 325–332


</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochberg and Brooks (1996)</span>
<span class="ltx_bibblock">
Hochberg, J. and Brooks, V. (1996).

</span>
<span class="ltx_bibblock">The perception of motion pictures.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Cognitive ecology</em> , 205–292


</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiskỳ et al. (2018)</span>
<span class="ltx_bibblock">
Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M.,
Melis, G., et al. (2018).

</span>
<span class="ltx_bibblock">The narrativeqa reading comprehension challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>
6, 317–328


</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Lin, C.-Y. (2004).

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Text summarization branches out</em>. 74–81


</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2019)</span>
<span class="ltx_bibblock">
Lin, K., Tafjord, O., Clark, P., and Gardner, M. (2019).

</span>
<span class="ltx_bibblock">Reasoning over paragraph effects in situations.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.05852</em>


</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Malinowski, M. and Fritz, M. (2014).

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1410.0210</em>


</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nema and Khapra (2018)</span>
<span class="ltx_bibblock">
Nema, P. and Khapra, M. M. (2018).

</span>
<span class="ltx_bibblock">Towards a better metric for evaluating question generation systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.10192</em>


</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostermann et al. (2018)</span>
<span class="ltx_bibblock">
Ostermann, S., Roth, M., Modi, A., Thater, S., and Pinkal, M. (2018).

</span>
<span class="ltx_bibblock">Semeval-2018 task 11: Machine comprehension using commonsense
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Workshop on semantic
evaluation</em>. 747–757


</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>. 311–318


</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al. (2021)</span>
<span class="ltx_bibblock">
Patel, D., Parikh, R., and Shastri, Y. (2021).

</span>
<span class="ltx_bibblock">Recent advances in video question answering: A review of datasets and
methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em> ,
339–356<a target="_blank" href="https:/doi.org/10.1007/978-3-030-68790-8_27" title="" class="ltx_ref">10.1007/978-3-030-68790-8_27</a>


</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., and Zemel, R. (2015).

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 28,
2953–2961


</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schank and Abelson (1975)</span>
<span class="ltx_bibblock">
Schank, R. C. and Abelson, R. P. (1975).

</span>
<span class="ltx_bibblock">Scripts, plans, and knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th International Joint Conference on
Artificial Intelligence - Volume 1</em> (San Francisco, CA, USA: Morgan Kaufmann
Publishers Inc.), IJCAI’75, 151–157


</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schumaker (2011)</span>
<span class="ltx_bibblock">
Schumaker, R. P. (2011).

</span>
<span class="ltx_bibblock">From data to wisdom: the progression of computational learning in
text mining.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Communications of the IIMA</em> 11, 4


</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan (2018)</span>
<span class="ltx_bibblock">
Tan, E. S. (2018).

</span>
<span class="ltx_bibblock">A psychology of the film.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Palgrave Communications</em> 4, 1–20


</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorndyke (1977)</span>
<span class="ltx_bibblock">
Thorndyke, P. W. (1977).

</span>
<span class="ltx_bibblock">Cognitive structures in comprehension and memory of narrative
discourse.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Cognitive Psychology</em> 9, 77–110.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/0010-0285(77)90005-6" title="" class="ltx_ref">https://doi.org/10.1016/0010-0285(77)90005-6</a>


</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zwaan et al. (1995)</span>
<span class="ltx_bibblock">
Zwaan, R. A., Langston, M. C., and Graesser, A. C. (1995).

</span>
<span class="ltx_bibblock">The construction of situation models in narrative comprehension: An
event-indexing model.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Psychological Science</em> 6, 292–297.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1111/j.1467-9280.1995.tb00513.x" title="" class="ltx_ref">10.1111/j.1467-9280.1995.tb00513.x</a>


</span>
</li>
</ul>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Figure Captions</h2>

<figure id="Sx5.F1" class="ltx_figure"><img src="/html/2107.09847/assets/Figures/figure1.png" id="Sx5.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="426" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx5.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="Sx5.F1.4.2" class="ltx_text" style="font-size:90%;">Scope of the information provided by the short-form video and the scheme of CogME. (A) The range of story elements included in the short format video used in this study is limited to settings, characters, and plots. (Setting-Environmental information(e.g., time, place); Character-Personified individuals; Plot-Progress of sequential events) (B) The interactive cognitive modules- target, thinking, and content- necessary for story understanding. The sub-components of each module are shown in each circle. (-) time and humor elements were excluded from this analysis. </span></figcaption>
</figure>
<figure id="Sx5.F2" class="ltx_figure"><img src="/html/2107.09847/assets/Figures/figure2.png" id="Sx5.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="719" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx5.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="Sx5.F2.4.2" class="ltx_text" style="font-size:90%;">Examples of tagging on the question. (A) Cases of tagging story elements to the questions for shots, which are brief video clips with no scene transition. (B) Cases of tagging story elements to the questions in scenes, which are video clips containing several events in a single location.</span></figcaption>
</figure>
<figure id="Sx5.F3" class="ltx_figure"><img src="/html/2107.09847/assets/Figures/figure3.png" id="Sx5.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="401" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx5.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="Sx5.F3.4.2" class="ltx_text" style="font-size:90%;">The distribution of story elements included in questions of the dataset and the accuracy profiles for each module. (A) Frequencies by story elements of validation QA set. Each bar shows the number of questions by the story elements, and the two colors in the bar represent the number of correct predictions (blue) and the number of missed predictions (orange), respectively. (B)-(D) Accuracy profile by story elements. (B) Correct ratio by elements of the target module, (C) Content module, (D) Process module.</span></figcaption>
</figure>
<figure id="Sx5.T1" class="ltx_table">
<table id="Sx5.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx5.T1.2.1.1" class="ltx_tr">
<th id="Sx5.T1.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="Sx5.T1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Elements</span></span>
</span>
</th>
<th id="Sx5.T1.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.1.1.2.1.1" class="ltx_p" style="width:151.8pt;"><span id="Sx5.T1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</th>
<th id="Sx5.T1.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.1.1.3.1.1" class="ltx_p" style="width:173.4pt;"><span id="Sx5.T1.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Application to this dataset</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx5.T1.2.2.1" class="ltx_tr">
<td id="Sx5.T1.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">Character</span>
</span>
</td>
<td id="Sx5.T1.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.2.1.2.1.1" class="ltx_p" style="width:151.8pt;">Information about individuals featuring in the video. It applies not only to humans but also to personified animals, plants, and objects.</span>
</span>
</td>
<td id="Sx5.T1.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.2.1.3.1.1" class="ltx_p" style="width:173.4pt;">All the characters in the story were human, with 20 main characters.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.3.2" class="ltx_tr">
<td id="Sx5.T1.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Object</span>
</span>
</td>
<td id="Sx5.T1.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.3.2.2.1.1" class="ltx_p" style="width:151.8pt;">Items and body parts featured in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.3.2.3.1.1" class="ltx_p" style="width:173.4pt;">All the objects in the story were realistic.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.4.3" class="ltx_tr">
<td id="Sx5.T1.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">Place</span>
</span>
</td>
<td id="Sx5.T1.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.4.3.2.1.1" class="ltx_p" style="width:151.8pt;">Spatial information of the story in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.4.3.3.1.1" class="ltx_p" style="width:173.4pt;">Space information such as house, park, office was targeted rather than a specific local area.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.5.4" class="ltx_tr">
<td id="Sx5.T1.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.5.4.1.1.1" class="ltx_p" style="width:65.0pt;">Time <sup id="Sx5.T1.2.5.4.1.1.1.1" class="ltx_sup">(-)</sup></span>
</span>
</td>
<td id="Sx5.T1.2.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.5.4.2.1.1" class="ltx_p" style="width:151.8pt;">Temporal information of the story in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.5.4.3.1.1" class="ltx_p" style="width:173.4pt;">Excluded from this dataset because this story had difficulties to specify the time in nature.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.6.5" class="ltx_tr">
<td id="Sx5.T1.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.6.5.1.1.1" class="ltx_p" style="width:65.0pt;">Conversation</span>
</span>
</td>
<td id="Sx5.T1.2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.6.5.2.1.1" class="ltx_p" style="width:151.8pt;">Conveying information between the characters, including dialogues, monologues, speech sounds, and text messages.</span>
</span>
</td>
<td id="Sx5.T1.2.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.6.5.3.1.1" class="ltx_p" style="width:173.4pt;">Most of the conversations were dubbed dialogues or monologues.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.7.6" class="ltx_tr">
<td id="Sx5.T1.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.7.6.1.1.1" class="ltx_p" style="width:65.0pt;">Behavior</span>
</span>
</td>
<td id="Sx5.T1.2.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.7.6.2.1.1" class="ltx_p" style="width:151.8pt;">Movement and action of the subject in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.7.6.3.1.1" class="ltx_p" style="width:173.4pt;">Most of the behavior was daily movements.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.8.7" class="ltx_tr">
<td id="Sx5.T1.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.8.7.1.1.1" class="ltx_p" style="width:65.0pt;">Event</span>
</span>
</td>
<td id="Sx5.T1.2.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.8.7.2.1.1" class="ltx_p" style="width:151.8pt;">Information about what happened in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.8.7.3.1.1" class="ltx_p" style="width:173.4pt;">There was a fantasy related to time as a whole, but most of the events were routine when grasping the contents in short-cut video units.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.9.8" class="ltx_tr">
<td id="Sx5.T1.2.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.9.8.1.1.1" class="ltx_p" style="width:65.0pt;">Emotion</span>
</span>
</td>
<td id="Sx5.T1.2.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.9.8.2.1.1" class="ltx_p" style="width:151.8pt;">The feeling expressed by the subject in the video.</span>
</span>
</td>
<td id="Sx5.T1.2.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.9.8.3.1.1" class="ltx_p" style="width:173.4pt;">Most feelings were not morbid and belonged to a standard category.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.10.9" class="ltx_tr">
<td id="Sx5.T1.2.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.10.9.1.1.1" class="ltx_p" style="width:65.0pt;">Humor <sup id="Sx5.T1.2.10.9.1.1.1.1" class="ltx_sup">(-)</sup></span>
</span>
</td>
<td id="Sx5.T1.2.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.10.9.2.1.1" class="ltx_p" style="width:151.8pt;">A funny message given through jokes, facial expressions, gestures, situations, etc.</span>
</span>
</td>
<td id="Sx5.T1.2.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.10.9.3.1.1" class="ltx_p" style="width:173.4pt;">Excluded from this dataset because it was difficult to express the humorous elements in the dubbed story.</span>
</span>
</td>
</tr>
<tr id="Sx5.T1.2.11.10" class="ltx_tr">
<td id="Sx5.T1.2.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.11.10.1.1.1" class="ltx_p" style="width:65.0pt;">Commonsense</span>
</span>
</td>
<td id="Sx5.T1.2.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.11.10.2.1.1" class="ltx_p" style="width:151.8pt;">Concepts and knowledge which people universally accept in a given culture.</span>
</span>
</td>
<td id="Sx5.T1.2.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T1.2.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T1.2.11.10.3.1.1" class="ltx_p" style="width:173.4pt;">Applied to knowledge-based questions that required an understanding of social knowledge or rules of nature or dealing with answers given as synonyms.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx5.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="Sx5.T1.4.2" class="ltx_text" style="font-size:90%;">Definitions and applications of the story elements as sub-components within the Target module</span></figcaption>
</figure>
<figure id="Sx5.T2" class="ltx_table">
<table id="Sx5.T2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx5.T2.2.1.1" class="ltx_tr">
<th id="Sx5.T2.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="Sx5.T2.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Elements</span></span>
</span>
</th>
<th id="Sx5.T2.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.1.1.2.1.1" class="ltx_p" style="width:151.8pt;"><span id="Sx5.T2.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</th>
<th id="Sx5.T2.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.1.1.3.1.1" class="ltx_p" style="width:173.4pt;"><span id="Sx5.T2.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Application to this dataset</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx5.T2.2.2.1" class="ltx_tr">
<td id="Sx5.T2.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">Identity</span>
</span>
</td>
<td id="Sx5.T2.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.2.1.2.1.1" class="ltx_p" style="width:151.8pt;">Personal information of subjects or name of objects in the story. Description of behaviors or events in the video.</span>
</span>
</td>
<td id="Sx5.T2.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.2.1.3.1.1" class="ltx_p" style="width:173.4pt;">All targets were within the scope of reality and everyday life.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.3.2" class="ltx_tr">
<td id="Sx5.T2.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Feature</span>
</span>
</td>
<td id="Sx5.T2.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.3.2.2.1.1" class="ltx_p" style="width:151.8pt;">Characteristics, traits, or atmosphere of subjects and/or objects.</span>
</span>
</td>
<td id="Sx5.T2.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.3.2.3.1.1" class="ltx_p" style="width:173.4pt;">Mainly focused on the character’s emotions, the relationship between the characters, and objects’ characteristics.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.4.3" class="ltx_tr">
<td id="Sx5.T2.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">Relationship</span>
</span>
</td>
<td id="Sx5.T2.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.4.3.2.1.1" class="ltx_p" style="width:151.8pt;">The relationship between two or more targets: the relationship between characters and characters, and between characters and objects.</span>
</span>
</td>
<td id="Sx5.T2.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.4.3.3.1.1" class="ltx_p" style="width:173.4pt;">Mainly focused on the information or features of the relationship between characters and ownership of objects.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.5.4" class="ltx_tr">
<td id="Sx5.T2.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.5.4.1.1.1" class="ltx_p" style="width:65.0pt;">Means</span>
</span>
</td>
<td id="Sx5.T2.2.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.5.4.2.1.1" class="ltx_p" style="width:151.8pt;">Instruments or methods used to achieve a particular purpose</span>
</span>
</td>
<td id="Sx5.T2.2.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.5.4.3.1.1" class="ltx_p" style="width:173.4pt;">Focused on actions or tools used as a method. Applied only when both the purpose and the means were revealed in the scene.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.6.5" class="ltx_tr">
<td id="Sx5.T2.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.6.5.1.1.1" class="ltx_p" style="width:65.0pt;">Context</span>
</span>
</td>
<td id="Sx5.T2.2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.6.5.2.1.1" class="ltx_p" style="width:151.8pt;">Storyline indirectly revealed through the interaction of characters, especially the meaning of the conversation.</span>
</span>
</td>
<td id="Sx5.T2.2.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.6.5.3.1.1" class="ltx_p" style="width:173.4pt;">Mainly focused on revealing the meaning of conversations between characters.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.7.6" class="ltx_tr">
<td id="Sx5.T2.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.7.6.1.1.1" class="ltx_p" style="width:65.0pt;">Sequence</span>
</span>
</td>
<td id="Sx5.T2.2.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.7.6.2.1.1" class="ltx_p" style="width:151.8pt;">Related events with time series and the changes before and after</span>
</span>
</td>
<td id="Sx5.T2.2.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.7.6.3.1.1" class="ltx_p" style="width:173.4pt;">Mainly focused on events happened in order or occurred simultaneously.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.8.7" class="ltx_tr">
<td id="Sx5.T2.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.8.7.1.1.1" class="ltx_p" style="width:65.0pt;">Causality</span>
</span>
</td>
<td id="Sx5.T2.2.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.8.7.2.1.1" class="ltx_p" style="width:151.8pt;">Causes and consequences of a particular change: natural or mechanical causality</span>
</span>
</td>
<td id="Sx5.T2.2.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.8.7.3.1.1" class="ltx_p" style="width:173.4pt;">Applied to unintentional behaviors, emotional changes, and natural mechanical causality.</span>
</span>
</td>
</tr>
<tr id="Sx5.T2.2.9.8" class="ltx_tr">
<td id="Sx5.T2.2.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.9.8.1.1.1" class="ltx_p" style="width:65.0pt;">Motivation</span>
</span>
</td>
<td id="Sx5.T2.2.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.9.8.2.1.1" class="ltx_p" style="width:151.8pt;">Change resulting from the action in which a person’s preference or intention is involved</span>
</span>
</td>
<td id="Sx5.T2.2.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T2.2.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T2.2.9.8.3.1.1" class="ltx_p" style="width:173.4pt;">Applied to conversations or actions in which intentions can be inferred from the video.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx5.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="Sx5.T2.4.2" class="ltx_text" style="font-size:90%;">Definitions and applications of the story elements as sub-components within the Content module</span></figcaption>
</figure>
<figure id="Sx5.T3" class="ltx_table">
<table id="Sx5.T3.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx5.T3.2.1.1" class="ltx_tr">
<th id="Sx5.T3.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="Sx5.T3.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Elements</span></span>
</span>
</th>
<th id="Sx5.T3.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.1.1.2.1.1" class="ltx_p" style="width:151.8pt;"><span id="Sx5.T3.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</th>
<th id="Sx5.T3.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.1.1.3.1.1" class="ltx_p" style="width:173.4pt;"><span id="Sx5.T3.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Application to this dataset</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx5.T3.2.2.1" class="ltx_tr">
<td id="Sx5.T3.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">Recall</span>
</span>
</td>
<td id="Sx5.T3.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.2.1.2.1.1" class="ltx_p" style="width:151.8pt;">Retrieving or recollecting the facts in the scene or fixed information that is not subject to time changes.</span>
</span>
</td>
<td id="Sx5.T3.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.2.1.3.1.1" class="ltx_p" style="width:173.4pt;">Applied to retrieval of simple information. We only used ‘shot’ videos with low memory level (= short video clips without scene transitions) in this dataset.</span>
</span>
</td>
</tr>
<tr id="Sx5.T3.2.3.2" class="ltx_tr">
<td id="Sx5.T3.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Recognition</span>
</span>
</td>
<td id="Sx5.T3.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.3.2.2.1.1" class="ltx_p" style="width:151.8pt;">Perception of the elements that changed temporally and spatially in the scene.</span>
</span>
</td>
<td id="Sx5.T3.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.3.2.3.1.1" class="ltx_p" style="width:173.4pt;">Applied to asking dialogue or serial event-related questions, or having distinguished within multiple clues given at the same time.</span>
</span>
</td>
</tr>
<tr id="Sx5.T3.2.4.3" class="ltx_tr">
<td id="Sx5.T3.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">Reasoning</span>
</span>
</td>
<td id="Sx5.T3.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.4.3.2.1.1" class="ltx_p" style="width:151.8pt;">Concluding or making a logical judgment based on circumstantial evidence rather than on direct observation.</span>
</span>
</td>
<td id="Sx5.T3.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="Sx5.T3.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Sx5.T3.2.4.3.3.1.1" class="ltx_p" style="width:173.4pt;">All ‘why’ questions and feelings or relationships that need to be inferred, and knowledge-based QA.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx5.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="Sx5.T3.4.2" class="ltx_text" style="font-size:90%;">Definitions and applications of the story elements as sub-components within the Thinking module</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.09846" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.09847" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.09847">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.09847" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.09848" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 14:56:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
