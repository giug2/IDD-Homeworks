<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.09317] Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection</title><meta property="og:description" content="Quality control is a crucial activity performed by manufacturing companies to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.09317">

<!--Generated on Fri Mar  1 11:08:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jo≈æe M. Ro≈æanec
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patrik Zajec
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Spyros Theodoropoulos
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Erik Koehorst
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bla≈æ Fortuna
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dunja Mladeniƒá
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Jo≈æef Stefan Institute, Ljubljana, Slovenia, (e-mail: joze.rozanec@ijs.si, patrik.zajec@ijs.si)
</span>
<span class="ltx_contact ltx_role_address">Jo≈æef Stefan International Postgraduate School, Ljubljana, Slovenia, (e-mail: joze.rozanec@ijs.si, patrik.zajec@ijs.si)
</span>
<span class="ltx_contact ltx_role_address">School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece (e-mail: stheodoropoulos@mail.ntua.gr).
</span>
<span class="ltx_contact ltx_role_address">Philips Consumer Lifestyle BV, Drachten, The Neatherlands (e-mail: erik.koehorst@philips.com)
</span>
<span class="ltx_contact ltx_role_address">Qlector d.o.o., Ljubljana, Slovenia (e-mail: blaz.fortuna@qlector.com)
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Quality control is a crucial activity performed by manufacturing companies to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers‚Äô discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can be used to reduce the labeling workload by more than 50%. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898, even when increasing the dataset imbalance by leaving only 25% of the images denoting defective products. We performed the research with real-world data provided by <span id="id1.id1.1" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Manufacturing plant control; Intelligent manufacturing systems; Advanced manufacturing; Industry 4.0; Smart Manufacturing; Visual Inspection; Quality Inspection; Data Augmentation

</div>
<span id="id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>This work was supported by the Slovenian Research Agency and the European Union‚Äôs Horizon 2020 program project STAR under grant agreement number H2020-956573.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The increasing digitization of the manufacturing sector has enabled greater communication capabilities and tasks automation, producing an increasingly digital value chain <cite class="ltx_cite ltx_citemacro_cite">Benbarrad et¬†al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>); Papadopoulos et¬†al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, which is one of the objectives of the Industry 4.0 paradigm. Quality control is considered one of the main areas where digitalization and the use of Artificial Intelligence provide new value to the manufacturing industry <cite class="ltx_cite ltx_citemacro_cite">Petrakieva et¬†al. (<a href="#bib.bib21" title="" class="ltx_ref">2014</a>); Chouchene et¬†al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, and has been applied in multiple scenarios <cite class="ltx_cite ltx_citemacro_cite">Beltr√°n-Gonz√°lez et¬†al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>); Napoletano et¬†al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Obregon et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>. Artificial Intelligence promises to amend multiple issues associated with manual inspection, such as workers‚Äô fatigue, operator-to-operator inconsistency, and quality dependence on the employees‚Äô experience and well-being, among others <cite class="ltx_cite ltx_citemacro_cite">See (<a href="#bib.bib27" title="" class="ltx_ref">2012</a>)</cite>. In addition, Artificial Intelligence enables great scalability by reducing manual work, increasing the speed of the inspection process, and allowing the execution of the inspection process in a continuous manner <cite class="ltx_cite ltx_citemacro_cite">Garvey (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>); Escobar and Morales-Menendez (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>); Chouchene et¬†al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>. The resulting increase in product quality directly impacts the whole production chain and the business, allowing to trace defect root causes, reducing rework in the production process, and avoiding costly disruptions in the supply chain.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Among the challenges posed by the development of Artificial Intelligence models for visual inspection, we find data gathering <cite class="ltx_cite ltx_citemacro_cite">Ren et¬†al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>, data labeling (in the case of supervised models), increased class imbalance over time (according to the increasing products‚Äô quality), and models‚Äô explainability <cite class="ltx_cite ltx_citemacro_cite">Meister et¬†al. (<a href="#bib.bib15" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We frame the visual inspection problem as a supervised learning problem. This paper addresses the problems of class imbalance and data scarcity regarding defective pieces. These are critical visual inspection problems in manufacturing since increasing products‚Äô quality only increases the class imbalance over time and makes it harder to improve the models‚Äô classification performance. It is thus imperative to devise strategies that can be used to mitigate such an issue. To that end, we conduct a series of experiments to (a) compare our classification models to a State-Of-The-Art (SOTA) unsupervised defect detection method, (b) understand how using trivial techniques to balance the dataset influences the classifier‚Äôs performance, (c) we devote particular attention to the use of Generative Adversarial Network (GAN) models to generate synthetic images of defective parts and evaluate how such a balanced dataset influences the classifier‚Äôs discrimination performance, and (d) simulate more severe class imbalances, to understand how would the classifiers could behave in the future, as we expect a higher scarcity of defective parts when products‚Äô quality increases. We developed the machine learning models with images provided by the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span> corporation. The dataset comprises shaver images classified into three categories based on the defects observed regarding logo printing: good, double print, and interrupted print.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The rest of this paper is structured as follows: Section¬†<a href="#S2" title="2 Related Work ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents related work, Section¬†<a href="#S3" title="3 Use Case ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the use case on which we conducted the research, Section¬†<a href="#S4" title="4 Methodology and Experiments ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the methodology we followed, and the experiments we performed, and Section¬†<a href="#S5" title="5 Results ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the results we obtained, and their implications. Finally, in Section¬†<a href="#S6" title="6 Conclusions ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we provide our conclusions and outline future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Automated visual inspection is an automatic form of non-destructive testing in quality control, achieved through one or multiple cameras that provide visual input and visual processing techniques that allow for detecting defects (<cite class="ltx_cite ltx_citemacro_cite">Czimmermann et¬†al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>). The visual inspection aims to identify functional and cosmetic defects (<cite class="ltx_cite ltx_citemacro_cite">Chin and Harlow (<a href="#bib.bib3" title="" class="ltx_ref">1982</a>)</cite>). Automated visual inspection techniques can be either supervised (requiring labeled data) or unsupervised (requiring no labeling). SOTA image processing techniques involve the use of deep learning (<cite class="ltx_cite ltx_citemacro_cite">Pouyanfar et¬†al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>). Authors frequently use pre-trained models, either as feature extractors (to avoid costly feature engineering) or for end-to-end learning, either by training the model from scratch or fine-tuning a pre-trained one (<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a href="#bib.bib29" title="" class="ltx_ref">2018a</a>)</cite>).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The decreased cost of sensors and their implementation in production lines in the context of Industry 4.0 produces greater amounts of digital data (<cite class="ltx_cite ltx_citemacro_cite">Benbarrad et¬†al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>), which can be leveraged for automated visual inspection (<cite class="ltx_cite ltx_citemacro_cite">Peres et¬†al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>). Given that the reliability of manufactured products is of utmost importance, inspecting all parts is desired. Such inspection translates into great costs, which can be significantly decreased by automating the visual inspection process. The main benefits of such automation are the savings related to human labor costs, the ability to match production and inspection speeds, and error traceability, which provides relevant information to guide management decisions (<cite class="ltx_cite ltx_citemacro_cite">Chin and Harlow (<a href="#bib.bib3" title="" class="ltx_ref">1982</a>)</cite>).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Successful automated visual inspection implementations report using Artificial Intelligence models to ensure the quality of printing (<cite class="ltx_cite ltx_citemacro_cite">Villalba-Diez et¬†al. (<a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>), steel surface defects (<cite class="ltx_cite ltx_citemacro_cite">Jia et¬†al. (<a href="#bib.bib9" title="" class="ltx_ref">2004</a>)</cite>), or assess the quality of manufactured vehicle parts (<cite class="ltx_cite ltx_citemacro_cite">Chouchene et¬†al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>). The most commonly used algorithms are the Support Vector Machines, k-Nearest Neighbours, Multi-Layer Perceptrons (MLPs), and Convolutional Neural Networks (CNNs) (<cite class="ltx_cite ltx_citemacro_cite">Ren et¬†al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Labeling data availability is critical to train and to enhance supervised artificial intelligence models. While the acquisition of data can be cheap, the labeling process is expensive in terms of time and costs since it requires human persons to review the images and provide labels for them. In addition, when inspecting data related to products‚Äô quality, it is to expect a strong class imbalance: most of the data instances correspond to good products, and just a minor proportion of them refer to defective pieces. If the class imbalance is not reduced, (a) most of the labeling time will be devoted to images of non-defective products, which provide little information to the model, (b) and operators will be prone to skip cases that correspond to defective pieces, confusing such an instance with the many ones that correspond to non-defective products. Given the products‚Äô quality improves over time, such class imbalance will be more pronounced. An alternative to expensive data labeling could be the generation of synthetic images corresponding to defective parts. Such a solution would require an initial dataset but enable an unlimited supply of synthetic images, which could be used to enrich the dataset and eventually avoid most of the labeling procedure. Furthermore, such an approach could also be effective in settings where different cameras are installed on each production line, thus producing images with different characteristics (e.g., different color scales). Such a setting could likely prevent using the images of different production lines in a single dataset but require different datasets for images of different characteristics. Using synthetic images could alleviate much of the manual workload required to gather the data over time.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Datasets built for the supervised defect detection use case are usually small and imbalanced, given the cost required to obtain and label data instances from a dataset. It is thus important to choose such data augmentation methods that can deliver value regardless of the size of the dataset. While classical data augmentation techniques, such as SMOTE, ADASYN, or more recent variants, such as DeepSMOTE are frequently used, GANs can learn a function that can approximate model distribution close to a true distribution, generating high fidelity (<cite class="ltx_cite ltx_citemacro_cite">Sampath et¬†al. (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>). While the dataset size could prevent training autoencoders or GANs from scratch, transfer learning can leverage pre-trained neural networks and fine-tune them to comply with the task. There has been extensive research on applying transfer learning to GANs, but many methods relying on traditional weight fine-tuning still fall short when faced with small datasets (<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a href="#bib.bib30" title="" class="ltx_ref">2018b</a>)</cite>). A solution successfully applied to BigGAN and SN-GAN is to focus only on fine-tuning the batch normalization layers, thus reducing the number of adjustable weights (<cite class="ltx_cite ltx_citemacro_cite">Noguchi and Harada (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>). The latter is intuitively equivalent to selecting those features relevant to the current dataset. Some preliminary results using the above method on a pre-trained BigGAN, have shown a significant increase in classifier accuracy after augmenting the original dataset. While imperfect, the images produced seem to retain some of the features needed for classification. Combining generated and original images to increase quality is possible, as done by <cite class="ltx_cite ltx_citemacro_cite">Satoshi¬†Tsutsui (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">GANs have been successfully used in various industrial scenarios to tackle class imbalance in defect detection problems. For example, <cite class="ltx_cite ltx_citemacro_cite">Jain et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> compare three GANs (DC-GAN, Auxiliary Classifier GAN (AC-GAN), and Information-theoretic GAN (InfoGAN)) applied for steel streep defect detection. In their research, GANs enable an improved classification sensitivity from the baseline (consisting only of graphical image transformations), with DC-GAN performing the best. DC-GAN was combined with geometric transformations in a fiber layup inspection setting, providing high-quality synthetic images for datasets with less than fifty representative original images per class <cite class="ltx_cite ltx_citemacro_cite">Meister et¬†al. (<a href="#bib.bib14" title="" class="ltx_ref">2021a</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, on the other hand, applied DC-GANs to generate images regarding solar cell defects, achieving substantial improvements only after combining real and synthetic images using a randomized boolean mosaic approach.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">This research uses classical data augmentation techniques and the Lightweight GAN (<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>) to augment the dataset with synthetic data instances. Classical data augmentation techniques are applied to the feature vectors that result from the feature extraction process. At the same time, the Lightweight GAN is used to create synthetic images, which are added to the regular dataset and then processed to extract the required features.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Use Case</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This research considered the visual inspection task performed on shavers manufactured by <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span>. This non-destructive quality control testing focuses on a cosmetic aspect of the product quality: whether the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span> company logo was printed correctly and eventually detects defective printings and determines the type of defective printing. To perform such an inspection, the company uses different pad-printing setups. Countless products are manufactured daily, and they are handled and inspected. Defective products are removed from the production line. An automated visual quality inspection system could strongly reduce manual work and decrease the inspection time by up to 40%.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For this research, a labeled dataset of 3.518 images was provided. The effort required to label such a dataset is close to eight man-hours and could rise to twenty-four man-hours if some triaging labeling strategy is introduced to increase the quality of the labeling. Images were labeled into three categories: ‚Äùgood‚Äù (no defect was detected), ‚Äùdouble printing‚Äù, and ‚Äùinterrupted printing‚Äù. We tackle the problem as a binary classification and a multiclass classification task.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology and Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We framed the automated defect detection as a binary classification problem (defective vs. non-defective) and a multiclass classification problem (classifying the images into one of three possible categories: good, double print, or interrupted print). To measure the models‚Äô discriminative power, we computed the AUC ROC metric. In the case of multiclass classification, we adopted the ‚Äùone-vs-rest‚Äù heuristic. The heuristic splits the dataset to compute the AUC ROC metric for each class. Then a final value is obtained by calculating the weighted average, weighting the AUC ROC scores of each class by the proportion of true instances of that class in the dataset.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To evaluate the models, we divided the dataset using a stratified ten-fold cross-validation (<cite class="ltx_cite ltx_citemacro_cite">Zeng and Martinez (<a href="#bib.bib34" title="" class="ltx_ref">2000</a>); Kuhn et¬†al. (<a href="#bib.bib11" title="" class="ltx_ref">2013</a>)</cite>). When performing data augmentation to balance the dataset, we considered the instances in the training set of the current k-fold cross-validation instance. While this meant computing ten times the instances for data augmentation, where necessary, at the same time ensured the instances were as close as possible to the data distribution observed in the training set when training the model. We used the Lightweight GAN to perform GAN-based data augmentation. To assess the quality of the synthetic images when compared to the original ones, we measured the Fr√©chet Inception Distance, which is defined as the squared Wasserstein metric between two multidimensional Gaussian distributions: the distribution that corresponds to features from the real-world images, and the one that corresponds to features from the synthetic images.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We used a pre-trained ResNet-18 model to extract average pool layer features. This allowed us to obtain a 512 values long vector per image, which we then reduced to a final feature vector of <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">K</span> features considering <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="K=\sqrt{N}" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">K</mi><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">=</mo><msqrt id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">N</mi></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></eq><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">ùêæ</ci><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><root id="S4.p3.1.m1.1.1.3a.cmml" xref="S4.p3.1.m1.1.1.3"></root><ci id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">K=\sqrt{N}</annotation></semantics></math>, with N equal to the number of data instances in the train set. Finally, feature selection was made by selecting the top <span id="S4.p3.1.2" class="ltx_text ltx_font_italic">K</span> features when ranking them by their mutual information score.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Throughout the experiments, we used a Multi-layer Perceptron classifier <cite class="ltx_cite ltx_citemacro_cite">Rosenblatt (<a href="#bib.bib24" title="" class="ltx_ref">1958</a>)</cite> as a baseline and compared it in different settings and against other models. All experiments were performed with four variations of the dataset: (a) considering all data available and reducing the number of images corresponding to defective parts so that only (b) 75%, (c) 50%, and (d) 25% are left. Doing so enabled us to understand how the models perform under the existing circumstances and how their performance can change over time when the products‚Äô quality improves, and the proportion of defective products decreases. Furthermore, a good classifier‚Äôs performance under heavier imbalance would also inform data collection and labeling efforts. For example, the need for fewer defective product samples could shorten the data collection time and reduce the number of images to label, with the consequent resource gains (time, people availability, and money).</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">In the following subsections, we describe the experiments performed and their rationale. We made the code available in a publicly accessible repository to promote research reproducibility <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The repository URL will be provided upon paper acceptance. The dataset will remain confidential, as requested by <span id="footnote1.1" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span>.</span></span></span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment 1: compare supervised and SOTA unsupervised models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The first experiment we performed was to compare three models: the baseline supervised model (MLP), a Gradient Boosted Tree classifier <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For this research, we use the Catboost implementation.</span></span></span> (which are frequently cited in the literature (<cite class="ltx_cite ltx_citemacro_cite">Yorulmu≈ü et¬†al. (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>); ≈Ωidek et¬†al. (<a href="#bib.bib35" title="" class="ltx_ref">2016</a>)</cite>)), and DRAEM (<cite class="ltx_cite ltx_citemacro_cite">Zavrtanik et¬†al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>), which is considered a SOTA model for unsupervised anomaly detection. DRAEM trains an autoencoder to reconstruct images to look like non-defective pieces. The original and reconstructed images are then fed to a discriminative sub-network, which identifies the anomalous regions to create an anomaly map. However, given that this model can only inform if a defect is present, it cannot be compared against a supervised model in a multiclass setting. We used the following model parameters: for the baseline model, the MLP was built with two dense layers (with 512 and 100 features), with an intermediate ReLU activation between both dense layers and a softmax activation at the output. The GBT model was instantiated with a maximum tree depth of ten, the multiclass loss function, and trained over sixty iterations.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">This experiment aimed to determine whether and how much better the supervised models are compared to unsupervised SOTA models.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment 2: effect of classic oversampling on supervised models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">After validating that the supervised models outperformed the unsupervised SOTA model (the results are presented in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.4 Experiment 4: enhancing classification performance with a custom Neural Network model ‚Ä£ 4 Methodology and Experiments ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in Section <a href="#S5" title="5 Results ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we explored if their performance could be enhanced by enlarging the dataset with synthetic data following three classic oversampling strategies: RANDOM (oversampling the minority classes by picking samples at random with replacement), ADASYN, and SMOTE.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experiment 3: effect of GAN-based oversampling on supervised models</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As described in Section <a href="#S2" title="2 Related Work ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, a growing body of research is focused on leveraging GANs to create synthetic data to mitigate data scarcity. A strong advantage of the GANs is that they learn to produce realistic data instances based on the feedback provided by a discriminator, which attempts to identify which images were artificially generated and which ones correspond to the original dataset.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">After corroborating the positive outcome of oversampling on the supervised models, we explored if synthetic images generated with GANs could further enhance the classifiers‚Äô performance. Our premise was that the similarity between the original and synthetic image distributions and the greater amount of data would boost the classifiers‚Äô performance. We thus explore whether the inclusion of synthetic images generated with a Lightweight GAN model enhances the models‚Äô performance and how additional class imbalance affects the GANs and the classifiers‚Äô performance. Given the poor performance of the Gradient Boosted Trees model, we experimented with the baseline model only.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">We trained the Lightweight GAN models <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We trained the Lightweight GAN based on the implementation available in the following repository: (the repository will be provided upon paper acceptance).</span></span></span> on a Tesla V100 32GB GPU, considering 20.000 iterations, investing nearly two hours per model trained. While we also explored other GANs, such as the StyleGAN, we found that they required many more resources without achieving a higher image quality. E.g., training a StyleGAN took us nearly six times the time required to train a Lightweight GAN model.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experiment 4: enhancing classification performance with a custom Neural Network model</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Finally, given the excellent results obtained in the experiments above with the baseline MLP model, we explored multiple neural network architectures to find some simple architectures that could achieve a better classification performance. We assumed that while using embeddings from a pre-trained ResNet18 model provided excellent features for a classifier, better features could be obtained by a simple CNN model trained directly on our dataset, given the visual simplicity of the images at hand. Furthermore, we trained the model on the original dataset (without data oversampling), considering two cases: training with an unweighted and a weighted loss. The weighted loss is a class imbalance mitigation technique that allows weighting the loss differently for different samples, considering whether they belong to the majority or a minority class. These two settings make the CNN comparable to the supervised models in <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">Experiment 1</span> (for the unweighted loss), and <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">Experiment 2</span> and <span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_italic">Experiment 3</span> when using the weighted loss.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The final architecture was trained using the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite>, the categorical cross-entropy, and using softmax as the output activation function.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Experiment</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">ROC AUC (binary)</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">ROC AUC (multiclass)</span></th>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">100%</span></th>
<th id="S4.T1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">75%</span></th>
<th id="S4.T1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">50%</span></th>
<th id="S4.T1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">25%</span></th>
<th id="S4.T1.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">100%</span></th>
<th id="S4.T1.1.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.6.1" class="ltx_text ltx_font_bold">75%</span></th>
<th id="S4.T1.1.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.7.1" class="ltx_text ltx_font_bold">50%</span></th>
<th id="S4.T1.1.2.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.8.1" class="ltx_text ltx_font_bold">25%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T1.1.3.1.1.1" class="ltx_text ltx_font_bold">1</span></th>
<th id="S4.T1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.3.1.2.1" class="ltx_text ltx_font_bold">Baseline</span></th>
<td id="S4.T1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9894</td>
<td id="S4.T1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9889</td>
<td id="S4.T1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9880</td>
<td id="S4.T1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9835</td>
<td id="S4.T1.1.3.1.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9897</td>
<td id="S4.T1.1.3.1.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9882</td>
<td id="S4.T1.1.3.1.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9881</td>
<td id="S4.T1.1.3.1.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9837</td>
</tr>
<tr id="S4.T1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.4.2.1.1" class="ltx_text ltx_font_bold">GBT</span></th>
<td id="S4.T1.1.4.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9697</td>
<td id="S4.T1.1.4.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9703</td>
<td id="S4.T1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9634</td>
<td id="S4.T1.1.4.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9516</td>
<td id="S4.T1.1.4.2.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9696</td>
<td id="S4.T1.1.4.2.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9698</td>
<td id="S4.T1.1.4.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9619</td>
<td id="S4.T1.1.4.2.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9470</td>
</tr>
<tr id="S4.T1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.5.3.1.1" class="ltx_text ltx_font_bold">DRAEM</span></th>
<td id="S4.T1.1.5.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,8624</td>
<td id="S4.T1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,8624</td>
<td id="S4.T1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,8624</td>
<td id="S4.T1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,8624</td>
<td id="S4.T1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.1.5.3.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.1.5.3.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.1.5.3.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T1.1.6.4" class="ltx_tr">
<th id="S4.T1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T1.1.6.4.1.1" class="ltx_text ltx_font_bold">2</span></th>
<th id="S4.T1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.6.4.2.1" class="ltx_text ltx_font_bold">OS - RANDOM(baseline)</span></th>
<td id="S4.T1.1.6.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9910</td>
<td id="S4.T1.1.6.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9903</td>
<td id="S4.T1.1.6.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9903</td>
<td id="S4.T1.1.6.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9850</td>
<td id="S4.T1.1.6.4.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9912</td>
<td id="S4.T1.1.6.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9905</td>
<td id="S4.T1.1.6.4.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.6.4.9.1" class="ltx_text ltx_font_italic">0,9905</span></td>
<td id="S4.T1.1.6.4.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9850</td>
</tr>
<tr id="S4.T1.1.7.5" class="ltx_tr">
<th id="S4.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.7.5.1.1" class="ltx_text ltx_font_bold">OS - RANDOM(GBT)</span></th>
<td id="S4.T1.1.7.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9728</td>
<td id="S4.T1.1.7.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9710</td>
<td id="S4.T1.1.7.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9650</td>
<td id="S4.T1.1.7.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9512</td>
<td id="S4.T1.1.7.5.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9732</td>
<td id="S4.T1.1.7.5.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9714</td>
<td id="S4.T1.1.7.5.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9651</td>
<td id="S4.T1.1.7.5.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9505</td>
</tr>
<tr id="S4.T1.1.8.6" class="ltx_tr">
<th id="S4.T1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.8.6.1.1" class="ltx_text ltx_font_bold">OS - ADASYN(baseline)</span></th>
<td id="S4.T1.1.8.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9910</td>
<td id="S4.T1.1.8.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9906</td>
<td id="S4.T1.1.8.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9899</td>
<td id="S4.T1.1.8.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9853</td>
<td id="S4.T1.1.8.6.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9912</td>
<td id="S4.T1.1.8.6.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9907</td>
<td id="S4.T1.1.8.6.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9900</td>
<td id="S4.T1.1.8.6.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.8.6.9.1" class="ltx_text ltx_font_italic">0,9851</span></td>
</tr>
<tr id="S4.T1.1.9.7" class="ltx_tr">
<th id="S4.T1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.9.7.1.1" class="ltx_text ltx_font_bold">OS - ADASYN(GBT)</span></th>
<td id="S4.T1.1.9.7.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9760</td>
<td id="S4.T1.1.9.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9727</td>
<td id="S4.T1.1.9.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9693</td>
<td id="S4.T1.1.9.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9575</td>
<td id="S4.T1.1.9.7.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9762</td>
<td id="S4.T1.1.9.7.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9731</td>
<td id="S4.T1.1.9.7.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9693</td>
<td id="S4.T1.1.9.7.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9572</td>
</tr>
<tr id="S4.T1.1.10.8" class="ltx_tr">
<th id="S4.T1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.10.8.1.1" class="ltx_text ltx_font_bold">OS - SMOTE(baseline)</span></th>
<td id="S4.T1.1.10.8.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9902</td>
<td id="S4.T1.1.10.8.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9910</td>
<td id="S4.T1.1.10.8.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9899</td>
<td id="S4.T1.1.10.8.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9854</td>
<td id="S4.T1.1.10.8.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9903</td>
<td id="S4.T1.1.10.8.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9911</td>
<td id="S4.T1.1.10.8.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9900</td>
<td id="S4.T1.1.10.8.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9847</td>
</tr>
<tr id="S4.T1.1.11.9" class="ltx_tr">
<th id="S4.T1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.11.9.1.1" class="ltx_text ltx_font_bold">OS - SMOTE(GBT)</span></th>
<td id="S4.T1.1.11.9.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9727</td>
<td id="S4.T1.1.11.9.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9686</td>
<td id="S4.T1.1.11.9.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9645</td>
<td id="S4.T1.1.11.9.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9551</td>
<td id="S4.T1.1.11.9.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9726</td>
<td id="S4.T1.1.11.9.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9684</td>
<td id="S4.T1.1.11.9.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9640</td>
<td id="S4.T1.1.11.9.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9534</td>
</tr>
<tr id="S4.T1.1.12.10" class="ltx_tr">
<th id="S4.T1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.1.1" class="ltx_text ltx_font_bold">3</span></th>
<th id="S4.T1.1.12.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.2.1" class="ltx_text ltx_font_bold">OS - GAN</span></th>
<td id="S4.T1.1.12.10.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.3.1" class="ltx_text ltx_font_bold">0,9965</span></td>
<td id="S4.T1.1.12.10.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.4.1" class="ltx_text ltx_font_bold">0,9954</span></td>
<td id="S4.T1.1.12.10.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.5.1" class="ltx_text ltx_font_bold">0,9948</span></td>
<td id="S4.T1.1.12.10.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.6.1" class="ltx_text ltx_font_bold">0,9898</span></td>
<td id="S4.T1.1.12.10.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.7.1" class="ltx_text ltx_font_bold">0,9966</span></td>
<td id="S4.T1.1.12.10.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.8.1" class="ltx_text ltx_font_bold">0,9955</span></td>
<td id="S4.T1.1.12.10.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.9.1" class="ltx_text ltx_font_bold">0,9955</span></td>
<td id="S4.T1.1.12.10.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.12.10.10.1" class="ltx_text ltx_font_bold">0,9899</span></td>
</tr>
<tr id="S4.T1.1.13.11" class="ltx_tr">
<th id="S4.T1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.1.13.11.1.1" class="ltx_text ltx_font_bold">4</span></th>
<th id="S4.T1.1.13.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.2.1" class="ltx_text ltx_font_bold">CNN</span></th>
<td id="S4.T1.1.13.11.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.3.1" class="ltx_text ltx_font_italic">0,9934</span></td>
<td id="S4.T1.1.13.11.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.4.1" class="ltx_text ltx_font_italic">0,9931</span></td>
<td id="S4.T1.1.13.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.5.1" class="ltx_text ltx_font_italic">0,9907</span></td>
<td id="S4.T1.1.13.11.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.6.1" class="ltx_text ltx_font_italic">0,9859</span></td>
<td id="S4.T1.1.13.11.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.7.1" class="ltx_text ltx_font_italic">0,9928</span></td>
<td id="S4.T1.1.13.11.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.8.1" class="ltx_text ltx_font_italic">0,9918</span></td>
<td id="S4.T1.1.13.11.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,9900</td>
<td id="S4.T1.1.13.11.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T1.1.13.11.10.1" class="ltx_text ltx_font_italic">0,9851</span></td>
</tr>
<tr id="S4.T1.1.14.12" class="ltx_tr">
<th id="S4.T1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.14.12.1.1" class="ltx_text ltx_font_bold">CNN + loss weighting</span></th>
<td id="S4.T1.1.14.12.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9919</td>
<td id="S4.T1.1.14.12.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9896</td>
<td id="S4.T1.1.14.12.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9896</td>
<td id="S4.T1.1.14.12.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9854</td>
<td id="S4.T1.1.14.12.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9916</td>
<td id="S4.T1.1.14.12.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9892</td>
<td id="S4.T1.1.14.12.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9892</td>
<td id="S4.T1.1.14.12.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0,9845</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>ROC AUC (binary and multiclass) obtained when conducting the experiments. 100%, 75%, 50%, and 25% denote the percentage of images with defective parts from the original training set, effectively left in the training set, to simulate higher class imbalances. The best results are bolded, second-best results are highlighted in italics.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present the results regarding the classifiers‚Äô discriminative power measured with the AUC ROC in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.4 Experiment 4: enhancing classification performance with a custom Neural Network model ‚Ä£ 4 Methodology and Experiments ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In addition, we detail the Fr√©chet Inception Distance obtained when generating images with the Light-weight GAN in Table <a href="#S5.T2" title="Table 2 ‚Ä£ 5 Results ‚Ä£ Synthetic Data Augmentation Using GAN For Improved Automated Visual Inspection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. When creating synthetic instances with a GAN, we found that the best quality was achieved for the images that correspond to ‚Äùinterrupted prints‚Äù. In contrast, surprisingly, the worst quality was achieved for the ‚Äùgood‚Äù images. Furthermore, we observed that the classifier frequently confused ‚Äùgood‚Äù and ‚Äùinterrupted print‚Äù images. After carefully analyzing those instances, we concluded that errors were likely made when labeling data instances to both categories, though the errors were not confirmed. Nevertheless, given how hard it is to identify interrupted prints in some instances, we consider this a reasonable explanation.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our main findings regarding the classification models are that (a) supervised models have a very good performance and strongly surpass the unsupervised models; (b) data augmentation techniques always improve the discriminative power of the classifier in binary and multiclass settings. Nevertheless, (c) the CNN model achieved the second-best results. We detail the experiment results and their implications in the following subsections.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MODEL</th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Good</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">Double print</td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center">Interrupted print</td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Light-weight GAN</th>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.80</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.49</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">29.68</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Fr√©chet Inception Distance score for each class.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experiment 1: compare supervised and SOTA unsupervised models</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">When comparing the supervised models against DRAEM, we found that the supervised models always surpassed DRAEM in the binary classification setting. In the worst case, they outperformed DRAEM by 0,0892 when measuring AUC ROC. The baseline model outperformed the GBT in all cases among the supervised models. Since DRAEM does not output specific classes, it was excluded from the comparison when computing ROC AUC in the multiclass setting.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Given the results presented above, we concluded that while unsupervised models have the advantage of working on unlabeled data, supervised models can be preferred to ensure better quality inspection. Nevertheless, unsupervised defect detection methods can be valuable for the labeling procedure. They can filter images and prioritize those that contain defects over those that do not. Such an approach could automate the labeling of good images (saving more than 50% of manual work) and let the labeling people concentrate on the defective parts. Furthermore, following <cite class="ltx_cite ltx_citemacro_cite">Lehr et¬†al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, means to cluster defective parts can be explored, to automate further or ease manual labeling of images concerning defective parts.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experiment 2: effect of classic oversampling on supervised models</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">When applying RANDOM, ADASYN, and SMOTE oversampling techniques, we observed that the supervised models almost always improved their performance. However, three exceptions were found, all of them for the GBT model: (a) for binary ROC AUC, when using SMOTE and preserving 75% of images regarding defective parts, (b) for binary ROC AUC, when using RANDOM oversampling and preserving 25% of images regarding defective parts, and (c) for multiclass ROC AUC, when using SMOTE and preserving 75% of images regarding defective parts. Our baseline model outperformed the GBT model in all cases, which led to the decision to continue the rest of the experiments considering only the baseline model.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Experiment 3: effect of GAN-based oversampling on supervised models</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Given the positive outcomes in <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">Experiment 2</span>, we explored the effect of GAN-oversampling on the baseline model. We found that GAN oversampling led to the best ROC AUC results, without exception, with metric values ranging between 0,9898 and 0,9966. In addition, GAN oversampling enabled the model to improve between 0,0063 and 0,0074 points compared to the same model trained without oversampling.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Experiment 4: enhancing classification performance with a custom Neural Network model</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">When comparing the baseline MLP to the CNN model in the two settings (weighted and unweighted loss), we found that the CNN model trained with the unweighted loss achieved the second-best performance in all cases. However, contrary to the performance increase observed in Experiments 2 and 3 with data augmentation, loss weighting negatively affected the model. Though it achieved better results than models in Experiment 2 for the original dataset, it could not match their performance when a stronger imbalance was introduced by preserving 75%, 50%, or 25% of the images of defective pieces.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research compared supervised and unsupervised classification models and explored data augmentation techniques to avoid class imbalance in the automated visual inspection use case. We confirmed that supervised models outperform SOTA unsupervised defect detection models. Furthermore, the best classifier discrimination performance was achieved with GAN-based data augmentation and an MLP classifier, using a pre-trained ResNet18 as a feature extractor. We used the Lightweight GAN implementation to generate images regarding defective pieces. The GAN achieved good quality images while consuming few resources. From the results obtained, we consider (a) unsupervised models can be of great utility to identify images corresponding to non-defective pieces and automatically label them, thus reducing labeling work for at least 50%; (b) GANs can be used to reduce further the effort required to gather (e.g., by reducing the waiting time to get a defective piece) and label images of defective pieces, by generating synthetic images; and (c) supervised classification models achieve the best performance, and thus are the best choice when deploying them to production, to automate the visual inspection. Future work will focus on designing a pipeline that implements (a) and (b), along with explainable artificial intelligence techniques, to enhance labeling and manual revision. In particular, the pipeline will reduce the workload, provide hints to the users to ease the labeling effort, and monitor users‚Äô attention to ensure the best labeling quality.</p>
</div>
<div id="S6.p2" class="ltx_para">
<span id="S6.p2.1" class="ltx_ERROR undefined">{ack}</span>
<p id="S6.p2.2" class="ltx_p">The authors acknowledge the valuable input and help Yvo van Vegten from <span id="S6.p2.2.1" class="ltx_text ltx_font_italic">Philips Consumer Lifestyle BV</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltr√°n-Gonz√°lez et¬†al. (2020)</span>
<span class="ltx_bibblock">
Beltr√°n-Gonz√°lez, C., Bustreo, M., and Del¬†Bue, A. (2020).

</span>
<span class="ltx_bibblock">External and internal quality inspection of aerospace components.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 7th International Workshop on Metrology for
AeroSpace (MetroAeroSpace)</em>, 351‚Äì355. IEEE.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benbarrad et¬†al. (2021)</span>
<span class="ltx_bibblock">
Benbarrad, T., Salhaoui, M., Kenitar, S.B., and Arioua, M. (2021).

</span>
<span class="ltx_bibblock">Intelligent machine vision model for defective product inspection
based on machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Journal of Sensor and Actuator Networks</em>, 10(1), 7.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chin and Harlow (1982)</span>
<span class="ltx_bibblock">
Chin, R.T. and Harlow, C.A. (1982).

</span>
<span class="ltx_bibblock">Automated visual inspection: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, (6), 557‚Äì573.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chouchene et¬†al. (2020)</span>
<span class="ltx_bibblock">
Chouchene, A., Carvalho, A., Lima, T.M., Charrua-Santos, F., Os√≥rio, G.J.,
and Barhoumi, W. (2020).

</span>
<span class="ltx_bibblock">Artificial intelligence for product quality inspection toward smart
industries: quality control of vehicle non-conformities.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2020 9th international conference on industrial technology
and management (ICITM)</em>, 127‚Äì131. IEEE.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Czimmermann et¬†al. (2020)</span>
<span class="ltx_bibblock">
Czimmermann, T., Ciuti, G., Milazzo, M., Chiurazzi, M., Roccella, S., Oddo,
C.M., and Dario, P. (2020).

</span>
<span class="ltx_bibblock">Visual-based defect detection and classification approaches for
industrial applications‚Äîa survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 20(5), 1459.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Escobar and Morales-Menendez (2018)</span>
<span class="ltx_bibblock">
Escobar, C.A. and Morales-Menendez, R. (2018).

</span>
<span class="ltx_bibblock">Machine learning techniques for quality control in high conformance
manufacturing environment.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Mechanical Engineering</em>, 10(2), 1687814018755519.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garvey (2018)</span>
<span class="ltx_bibblock">
Garvey, C. (2018).

</span>
<span class="ltx_bibblock">A framework for evaluating barriers to the democratization of
artificial intelligence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Thirty-Second AAAI Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et¬†al. (2020)</span>
<span class="ltx_bibblock">
Jain, S., Seth, G., Paruthi, A., Soni, U., and Kumar, G. (2020).

</span>
<span class="ltx_bibblock">Synthetic data augmentation for surface defect detection and
classification using deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent Manufacturing</em>, 1‚Äì14.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et¬†al. (2004)</span>
<span class="ltx_bibblock">
Jia, H., Murphey, Y.L., Shi, J., and Chang, T.S. (2004).

</span>
<span class="ltx_bibblock">An intelligent real-time vision system for surface defect detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Conference on Pattern
Recognition, 2004. ICPR 2004.</em>, volume¬†3, 239‚Äì242. IEEE.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Kingma, D.P. and Ba, J. (2014).

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn et¬†al. (2013)</span>
<span class="ltx_bibblock">
Kuhn, M., Johnson, K., et¬†al. (2013).

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Applied predictive modeling</em>, volume¬†26.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehr et¬†al. (2020)</span>
<span class="ltx_bibblock">
Lehr, J., Sargsyan, A., Pape, M., Philipps, J., and Kr√ºger, J. (2020).

</span>
<span class="ltx_bibblock">Automated optical inspection using anomaly detection and unsupervised
defect clustering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2020 25th IEEE International Conference on Emerging
Technologies and Factory Automation (ETFA)</em>, volume¬†1, 1235‚Äì1238. IEEE.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et¬†al. (2020)</span>
<span class="ltx_bibblock">
Liu, B., Zhu, Y., Song, K., and Elgammal, A. (2020).

</span>
<span class="ltx_bibblock">Towards faster and stabilized gan training for high-fidelity few-shot
image synthesis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meister et¬†al. (2021a)</span>
<span class="ltx_bibblock">
Meister, S., Mueller, N., Stoeve, J., and Groves, R. (2021a).

</span>
<span class="ltx_bibblock">Synthetic image data augmentation for fibre layup inspection
processes: Techniques to enhance the data set.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent Manufacturing</em>, 32.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/s10845-021-01738-7" title="" class="ltx_ref">10.1007/s10845-021-01738-7</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meister et¬†al. (2021b)</span>
<span class="ltx_bibblock">
Meister, S., Wermes, M.A., St√ºve, J., and Groves, R.M.
(2021b).

</span>
<span class="ltx_bibblock">Explainability of deep learning classifier decisions for optical
detection of manufacturing defects in the automated fiber placement process.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Automated Visual Inspection and Machine Vision IV</em>, volume
11787, 1178705. International Society for Optics and Photonics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Napoletano et¬†al. (2021)</span>
<span class="ltx_bibblock">
Napoletano, P., Piccoli, F., and Schettini, R. (2021).

</span>
<span class="ltx_bibblock">Semi-supervised anomaly detection for visual quality inspection.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, 115275.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noguchi and Harada (2019)</span>
<span class="ltx_bibblock">
Noguchi, A. and Harada, T. (2019).

</span>
<span class="ltx_bibblock">Image generation from small datasets via batch statistics adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision
(ICCV)</em>, 2750‚Äì2758.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Obregon et¬†al. (2021)</span>
<span class="ltx_bibblock">
Obregon, J., Hong, J., and Jung, J.Y. (2021).

</span>
<span class="ltx_bibblock">Rule-based explanations based on ensemble machine learning for
detecting sink mark defects in the injection moulding process.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of Manufacturing Systems</em>, 60, 392‚Äì405.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadopoulos et¬†al. (2021)</span>
<span class="ltx_bibblock">
Papadopoulos, T., Singh, S.P., Spanaki, K., Gunasekaran, A., and Dubey, R.
(2021).

</span>
<span class="ltx_bibblock">Towards the next generation of manufacturing: implications of big
data and digitalization in the context of industry 4.0.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peres et¬†al. (2020)</span>
<span class="ltx_bibblock">
Peres, R.S., Jia, X., Lee, J., Sun, K., Colombo, A.W., and Barata, J. (2020).

</span>
<span class="ltx_bibblock">Industrial artificial intelligence in industry 4.0-systematic review,
challenges and outlook.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8, 220121‚Äì220139.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrakieva et¬†al. (2014)</span>
<span class="ltx_bibblock">
Petrakieva, S., Garasym, O., and Taralova, I. (2014).

</span>
<span class="ltx_bibblock">http://ieeexplore. ieee. org/stamp/stamp. jsp? tp= &amp;arnumber=
7038771.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pouyanfar et¬†al. (2018)</span>
<span class="ltx_bibblock">
Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M.P., Shyu, M.L.,
Chen, S.C., and Iyengar, S.S. (2018).

</span>
<span class="ltx_bibblock">A survey on deep learning: Algorithms, techniques, and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 51(5), 1‚Äì36.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et¬†al. (2021)</span>
<span class="ltx_bibblock">
Ren, Z., Fang, F., Yan, N., and Wu, Y. (2021).

</span>
<span class="ltx_bibblock">State of the art in defect detection based on machine vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Journal of Precision Engineering and
Manufacturing-Green Technology</em>, 1‚Äì31.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenblatt (1958)</span>
<span class="ltx_bibblock">
Rosenblatt, F. (1958).

</span>
<span class="ltx_bibblock">The perceptron: a probabilistic model for information storage and
organization in the brain.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Psychological review</em>, 65 6, 386‚Äì408.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sampath et¬†al. (2021)</span>
<span class="ltx_bibblock">
Sampath, V., Maurtua, I., Aguilar¬†Mart√≠n, J.J., and Gutierrez, A. (2021).

</span>
<span class="ltx_bibblock">A survey on generative adversarial networks for imbalance problems in
computer vision tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of Big Data</em>, 8(1), 27.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1186/s40537-021-00414-0" title="" class="ltx_ref">10.1186/s40537-021-00414-0</a>.

</span>
<span class="ltx_bibblock">URL <span id="bib.bib25.2.1" class="ltx_text ltx_font_typewriter">https://doi.org/10.1186/s40537-021-00414-0</span>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Satoshi¬†Tsutsui (2019)</span>
<span class="ltx_bibblock">
Satoshi¬†Tsutsui, Yanwei¬†Fu, D.C. (2019).

</span>
<span class="ltx_bibblock">Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual
Recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See (2012)</span>
<span class="ltx_bibblock">
See, J.E. (2012).

</span>
<span class="ltx_bibblock">Visual inspection: a review of the literature.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Sandia Report SAND2012-8590, Sandia National Laboratories,
Albuquerque, New Mexico</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalba-Diez et¬†al. (2019)</span>
<span class="ltx_bibblock">
Villalba-Diez, J., Schmidt, D., Gevers, R., Ordieres-Mer√©, J., Buchwitz,
M., and Wellbrock, W. (2019).

</span>
<span class="ltx_bibblock">Deep learning for industrial computer vision quality control in the
printing industry 4.0.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 19(18), 3987.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2018a)</span>
<span class="ltx_bibblock">
Wang, T., Chen, Y., Qiao, M., and Snoussi, H. (2018a).

</span>
<span class="ltx_bibblock">A fast and robust convolutional neural network-based defect detection
model in product quality control.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">The International Journal of Advanced Manufacturing
Technology</em>, 94(9), 3465‚Äì3471.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2018b)</span>
<span class="ltx_bibblock">
Wang, Y., Wu, C., Herranz, L., van¬†de Weijer, J., Gonzalez-Garcia, A., and
Raducanu, B. (2018b).

</span>
<span class="ltx_bibblock">Transferring gans: generating images from limited data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2021)</span>
<span class="ltx_bibblock">
Wang, Y., Luo, S., and Wu, H. (2021).

</span>
<span class="ltx_bibblock">Defect detection of solar cell based on data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Journal of Physics: Conference Series</em>, 1952(2), 022010.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1088/1742-6596/1952/2/022010" title="" class="ltx_ref">10.1088/1742-6596/1952/2/022010</a>.

</span>
<span class="ltx_bibblock">URL <span id="bib.bib31.2.1" class="ltx_text ltx_font_typewriter">https://doi.org/10.1088/1742-6596/1952/2/022010</span>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yorulmu≈ü et¬†al. (2021)</span>
<span class="ltx_bibblock">
Yorulmu≈ü, M.H., Bolat, H.B., and Bahadƒ±r, √á. (2021).

</span>
<span class="ltx_bibblock">Predictive quality defect detection using machine learning
algorithms: A case study from automobile industry.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Intelligent and Fuzzy Systems</em>,
263‚Äì270. Springer.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zavrtanik et¬†al. (2021)</span>
<span class="ltx_bibblock">
Zavrtanik, V., Kristan, M., and Skoƒçaj, D. (2021).

</span>
<span class="ltx_bibblock">Draem-a discriminatively trained reconstruction embedding for surface
anomaly detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, 8330‚Äì8339.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng and Martinez (2000)</span>
<span class="ltx_bibblock">
Zeng, X. and Martinez, T.R. (2000).

</span>
<span class="ltx_bibblock">Distribution-balanced stratified cross-validation for accuracy
estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Journal of Experimental &amp; Theoretical Artificial
Intelligence</em>, 12(1), 1‚Äì12.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">≈Ωidek et¬†al. (2016)</span>
<span class="ltx_bibblock">
≈Ωidek, K., Ho≈°ovsk·ª≥, A., and Dubj√°k, J. (2016).

</span>
<span class="ltx_bibblock">Diagnostics of surface errors by embedded vision system and its
classification by machine learning algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Key Engineering Materials</em>, volume 669, 459‚Äì466. Trans Tech
Publ.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.09316" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.09317" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.09317">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.09317" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.09319" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 11:08:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
