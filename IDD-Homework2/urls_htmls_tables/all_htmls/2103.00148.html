<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2103.00148] Scalable federated machine learning with FEDn</title><meta property="og:description" content="Federated machine learning promises to overcome the input privacy challenge in machine learning. By iteratively updating a model on private clients and aggregating these local model updates into a global federated mode…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scalable federated machine learning with FEDn">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Scalable federated machine learning with FEDn">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2103.00148">

<!--Generated on Thu Mar  7 01:40:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Scalable federated machine learning with FEDn</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Morgan Ekmefjord
<br class="ltx_break">Scaleout Systems
<br class="ltx_break">Uppsala, Sweden
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">morgan@scaleoutsystems.com</span>
&amp;Addi Ait-Mlouk
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">addi.ait-mlouk@it.uu.se</span>
&amp;Sadi Alawadi
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">sadi.alawadi@it.uu.se</span>
&amp;Mattias Åkesson
<br class="ltx_break">Scaleout Systems
<br class="ltx_break">Uppsala, Sweden
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">mattias@scaleoutsystems.com</span>
&amp;Prashant Singh
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">prashant.singh@it.uu.se</span>
&amp;Ola Spjuth
<br class="ltx_break">Department of Pharmaceutical Biosciences
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break">Scaleout Systems, Uppsala, Sweden
<br class="ltx_break"><span id="id6.6.id6" class="ltx_text ltx_font_typewriter">ola.spjuth@farmbio.uu.se</span>
&amp;Salman Toor
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break">Scaleout Systems, Uppsala, Sweden
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">salman.toor@it.uu.se</span>
&amp;Andreas Hellander
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Uppsala University, Uppsala, Sweden
<br class="ltx_break">Scaleout Systems, Uppsala, Sweden
<br class="ltx_break"><span id="id8.8.id8" class="ltx_text ltx_font_typewriter">andreas.hellander@it.uu.se </span>
</span><span class="ltx_author_notes"><span id="id9.9.id1" class="ltx_text ltx_font_typewriter">Corresponding author.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Federated machine learning promises to overcome the input privacy challenge in machine learning. By iteratively updating a model on private clients and aggregating these local model updates into a global federated model, private data is incorporated in the federated model without needing to share and expose that data. Several open software projects for federated learning have appeared. Most of them focuses on supporting flexible experimentation with different model aggregation schemes and with different privacy-enhancing technologies. However, there is a lack of open frameworks that focuses on critical distributed computing aspects of the problem such as scalability and resilience. It is a big step to take for a data scientist to go from an experimental sandbox to testing their federated schemes at scale in real-world geographically distributed settings. To bridge this gap we have designed and developed a production-grade hierarchical federated learning framework, FEDn. The framework is specifically designed to make it easy to go from local development in pseudo-distributed mode to horizontally scalable distributed deployments. FEDn both aims to be production grade for industrial applications and a flexible research tool to explore real-world performance of novel federated algorithms and the framework has been used in number of industrial and academic R&amp;D projects. In this paper we present the architecture and implementation of FEDn. We demonstrate the framework’s scalability and efficiency in evaluations based on two case-studies representative for a cross-silo and a cross-device use-case respectively.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.2" class="ltx_p"><em id="p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Keywords</em> Federated machine learning  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
artificial intelligence  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
privacy.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated machine learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a promising solution to the input data privacy problem in machine learning. In FL, multiple parties, or clients, jointly train a machine learning model while keeping all training data local and private. Instead of moving data to a central storage system,
computation is brought to data at each local client site, and incremental model updates are computed and then combined into a global model according to some aggregation scheme. In this way, only model parameters are shared. The majority of work has dealt with artificial neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> but there is also research done on federated versions of other statistical learning methods such as random forests <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">FL is structurally similar to distributed optimization for statistical learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> but differs in a number of important ways. Foremost, in FL there is no control over how the data is distributed over the participating computational nodes. Hence, data cannot be assumed to be balanced or IID across nodes. This is in stark contrast to distributed learning where we are in full control of the data partitioning and are this able to devise optimal partitions for convergence and load balancing. Moreover, whereas distributed learning typically occurs on reliable cluster infrastructure with high network performance and low failure rate, in FL clients can become unavailable at any time, and communication between client and server takes place over a high-latency network (the internet). Consequently, how to optimally balance local training iterations with global synchronization to avoid poor convergence and to minimize communication rounds are central research questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Benchmark suits targeted at the exploration of federated learning algorithms have also been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Real-world federated learning software needs to be robust, resilient, and highly scalable distributed systems capable of handling both scaling to a large number of edge-clients, and scaling to large model sizes. The design and implementation of such a framework is the main contribution of this paper. Due to a tiered architecture inspired by the MapReduce programming model, FEDn users can implement a wide range of federated learning schemes by adhering to a structured and familiar design pattern, with the framework then assuring that the schemes will be highly resilient and horizontally scalable. FEDn aims to be easy-to-use and agnostic when it comes to the ML-framework used by clients while supporting high-performance federated training in real distributed settings. It is thus well-suited as a tool for research that bridges the machine learning aspects of the problem and the systems aspect of the problem, as well as for running federated learning deployments in production.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">FEDn has been used to implement a number of federated learning projects including a Federated Electra by the Swedish National Library, Federated Object Detection for Baltic seabirds by AI Sweden and Zenseact researchers, and fully distributed deployments of FEDn is available for wide use by researchers and Swedish industry partners in the strategic edge computing testbed AI Sweden EdgeLab. Links to these examples and others are collected and at the Scaleout organization on GitHub. FEDn is also the core enabling framework for federated machine learning in the SESAR project AICHAIN.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We make the following main contributions:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a tiered system architecture based on hierarchical FL and inspired by the MapReduce programming model. In this way we provide a programming pattern for FL applications that ensures highly scalable and resilient federated learning for cross-silo and cross-device scenarios.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We provide a highly efficient open source framework, FEDn, implementing the proposed architecture. FEDn lets a user go from local testing and development to production-grade geographically distributed deployments with no code change.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide performance benchmarks based on thousands (cross-device) of geographically distributed clients and for machine learning model sizes ranging from a few kB to 1GB (cross-silo). Systematic and geographically distributed benchmarks of live federated training at this scale has, to the best of our knowledge, not been reported previously.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">The remainder of this paper is organized as follows. Section <a href="#S2" title="2 Background ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives a background and surveys related work. Section <a href="#S3" title="3 A flexible and horizontally scalable architecture for federated learning ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> explains the architecture and implementation of FEDn. In Section <a href="#S4" title="4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we demonstrate the performance and scalability of the framework through experiments on both cross-silo and cross-device use-cases. Finally, Section <a href="#S5" title="5 Conclusion ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the work and outlines future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Horizontal and vertical federated learning</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">It is common to divide federated learning cases in two distinct categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">horizontal FL</em> the data is assumed to be partitioned by example, i.e. the feature space are assumed to be (nearly) the same between different participants (for example different scientific instruments sharing performance metrics but operating in different conditions and owned by different companies). In <em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">vertical FL</em> the data is assumed to be partitioned by features, i.e. different participants might hold different types of information about the same example (for example a bank and an insurance company holding different types of data about the same customer). The present study focuses on horizontal FL applications. The majority of FL solutions in literature fall in the horizontal FL category, often based on Artificial Neural Networks (ANNs), and perform synchronous batch-based training rounds, see e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Recently, Bonawitz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> proposed a high-level design of a scalable cross-device framework based on TensorFlow where the focus is on horizontal FL training. Kewei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> introduced a lossless federated training scheme based on secure XGBoost for vertical FL.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.5" class="ltx_p">Federated averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is the most widely used method for horizontal FL. It is a decentralized version of stochastic gradient descent (or in general any method that relies on a gradient update scheme) where in one <em id="S2.SS1.p2.5.1" class="ltx_emph ltx_font_italic">round</em> of training, a subset of <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">M</annotation></semantics></math> participating clients receive a copy of the latest global model and execute <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">K</annotation></semantics></math> local epochs ( complete passes over data) of training, updating <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="f(w_{k})" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mrow id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">​</mo><mrow id="S2.SS1.p2.3.m3.1.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.1.2" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p2.3.m3.1.1.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.1.1.1.2" xref="S2.SS1.p2.3.m3.1.1.1.1.1.2.cmml">w</mi><mi id="S2.SS1.p2.3.m3.1.1.1.1.1.3" xref="S2.SS1.p2.3.m3.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.1.3" xref="S2.SS1.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><times id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2"></times><ci id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3">𝑓</ci><apply id="S2.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1.1.2">𝑤</ci><ci id="S2.SS1.p2.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">f(w_{k})</annotation></semantics></math> locally on their own private datasets <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><msub id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">D</mi><mi id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2">𝐷</ci><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">D_{k}</annotation></semantics></math>. They then send the updated weights <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="w_{k}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><msub id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.2" xref="S2.SS1.p2.5.m5.1.1.2.cmml">w</mi><mi id="S2.SS1.p2.5.m5.1.1.3" xref="S2.SS1.p2.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.2">𝑤</ci><ci id="S2.SS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.p2.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">w_{k}</annotation></semantics></math> to the server which averages those weights:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\displaystyle w^{(i)}=\sum_{k=1}^{M}\frac{n_{k}}{n}w_{k}^{(i)}." display="inline"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><msup id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml">w</mi><mrow id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></msup><mo id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><mstyle displaystyle="true" id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml"><munderover id="S2.E1.m1.3.3.1.1.3.1a" xref="S2.E1.m1.3.3.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.3.1.2.2" xref="S2.E1.m1.3.3.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.E1.m1.3.3.1.1.3.1.2.3" xref="S2.E1.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.1.2.3.2" xref="S2.E1.m1.3.3.1.1.3.1.2.3.2.cmml">k</mi><mo id="S2.E1.m1.3.3.1.1.3.1.2.3.1" xref="S2.E1.m1.3.3.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E1.m1.3.3.1.1.3.1.2.3.3" xref="S2.E1.m1.3.3.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.3.3.1.1.3.1.3" xref="S2.E1.m1.3.3.1.1.3.1.3.cmml">M</mi></munderover></mstyle><mrow id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml"><mstyle displaystyle="true" id="S2.E1.m1.3.3.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.cmml"><mfrac id="S2.E1.m1.3.3.1.1.3.2.2a" xref="S2.E1.m1.3.3.1.1.3.2.2.cmml"><msub id="S2.E1.m1.3.3.1.1.3.2.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.2.2.2" xref="S2.E1.m1.3.3.1.1.3.2.2.2.2.cmml">n</mi><mi id="S2.E1.m1.3.3.1.1.3.2.2.2.3" xref="S2.E1.m1.3.3.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S2.E1.m1.3.3.1.1.3.2.2.3" xref="S2.E1.m1.3.3.1.1.3.2.2.3.cmml">n</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.3.2.1" xref="S2.E1.m1.3.3.1.1.3.2.1.cmml">​</mo><msubsup id="S2.E1.m1.3.3.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2.3.2.2" xref="S2.E1.m1.3.3.1.1.3.2.3.2.2.cmml">w</mi><mi id="S2.E1.m1.3.3.1.1.3.2.3.2.3" xref="S2.E1.m1.3.3.1.1.3.2.3.2.3.cmml">k</mi><mrow id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.3.1" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml">(</mo><mi id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.2.2.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"></eq><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2">superscript</csymbol><ci id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2">𝑤</ci><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><apply id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.1.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.3.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.2"></sum><apply id="S2.E1.m1.3.3.1.1.3.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3"><eq id="S2.E1.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.1"></eq><ci id="S2.E1.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.3.1.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.1.1.3.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3.1.3">𝑀</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2"><times id="S2.E1.m1.3.3.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.1"></times><apply id="S2.E1.m1.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2"><divide id="S2.E1.m1.3.3.1.1.3.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2"></divide><apply id="S2.E1.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2.2">𝑛</ci><ci id="S2.E1.m1.3.3.1.1.3.2.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="S2.E1.m1.3.3.1.1.3.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.2.3">𝑛</ci></apply><apply id="S2.E1.m1.3.3.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">superscript</csymbol><apply id="S2.E1.m1.3.3.1.1.3.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.3.2.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.3.2.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3.2.2">𝑤</ci><ci id="S2.E1.m1.3.3.1.1.3.2.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.3.2.3.2.3">𝑘</ci></apply><ci id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1.1">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\displaystyle w^{(i)}=\sum_{k=1}^{M}\frac{n_{k}}{n}w_{k}^{(i)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">These averaged weights are then communicated to all clients and this concludes one <em id="S2.SS1.p4.1.1" class="ltx_emph ltx_font_italic">round</em> of training. The currently supported scheme in FEDn is based on a hierarchical implementation of FedAvg. The choice of hyperparameters, in particular the number of local epochs <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">K</annotation></semantics></math>, critically influences the convergence rate and overall training efficiency - a too large value risks local overfitting and slow convergence of the global model, while a small value increases the number of communication rounds. Optimal values will depend both on the model, the number of clients and the data distribution over clients. In a recent study, Pang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> investigated the impact of the data heterogeneity problem in FL. Another study proposed a self-balancing federated learning framework Astraea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to deal with imbalanced clients’ data. Lim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> presented a number of different strategies to address communication overhead and unreliable network speed. In this paper we do not aim to select optimal hyperparameters for our use-cases, but rather focus on the cost of executing each round as a function of number of clients and model size (system performance rather than model performance).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Cross-device vs cross-silo use cases</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">We can distinguish between two major settings based on the nature of the use-case and participant/client type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">cross device learning</em> the target is typically a very large amount of stateless, low-powered clients engaging in relatively cheap, short bursts of computation. Clients can for example be cell phones, IoT devices or vehicles, and data is partitioned horizontally.
For <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">cross-silo</em> use-cases, clients are larger entities with more local computational power and storage capability. Data can be partitioned horizontally or vertically. In cross-device learning, communication overhead and handling failed connections are key challenges, whereas cross-silo learning can be both computationally and communication constrained and models are relatively large (MBs to GBs). FEDn aims to support the requirements across both these axes of federated learning though a horizontally scalable architecture.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Privacy-enhancing technologies</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">The core contribution of FL in the wider context of privacy-preserving machine learning is to enhance input privacy by allowing training data to remain withing the full control of the data owner. Compared to alternatives such as homomorphic encryption (HE) which allows computation directly on encrypted data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, FL accommodates a wider range of algorithms. <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Output privacy</em> deals with the inverse problem of what can be learnt about the input data from making predictions with the resulting global model. Differential privacy (DP) can be used to add controlled noise to carefully chosen stages of the model construction pipeline in order to make it harder to reverse engineer input data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. It is common to combine FL with HE and DP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, at least with synchronous model update primitives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Multiparty computation (MPC) can also be used for secure aggregation of models to minimize risk for information leakage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">A comprehensive overview of the current state-of-the art in FL along with a survey of privacy-enhancing technologies and security is provided by Kairouz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">In this paper we focus on efficient implementation of the core primitives of FL, but we note that privacy-enhancing technologies, in particular differential privacy and multiparty computation, could be accommodated in the framework with no or limited architectural modifications.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Related work</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">Several open-source frameworks for FL have been developed, including the following ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i1.p1.1" class="ltx_p">TensorFlow Federated (TFF).<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/tensorflow/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tensorflow/federated</a></span></span></span> TFF comprises two layers: Federated Learning and Federated Core. The former is a high-level interface that supports users to apply FL without the need for implementing FL algorithms personally. The latter enables users to implement and experiment new or customized FL algorithms. TFF supports the simulation of the distributed training of FL models but executes only on a single machine.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i2.p1.1" class="ltx_p">PySyft<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/OpenMined/PySyft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenMined/PySyft</a></span></span></span> leverages deep learning, secure multiparty computation, and differential privacy to train privacy-preserving FL models in untrusted environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The framework is based on PyTorch and provide a native Torch interface.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i3.p1.1" class="ltx_p">FATE <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/FederatedAI/FATE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FederatedAI/FATE</a></span></span></span> is an open-source framework developed by Webank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. It exploits secure computation protocols to train FL models using different algorithms, such as decision trees, logistic regression, transfer learning, and deep learning.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i4.p1.1" class="ltx_p">FedML <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://fedml.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fedml.ai/</a></span></span></span> is an open-source library developed to facilitate the development and benchmarking of FL algorithms. FedML supports on-device training, distributed computing, and single-machine simulations. Further, it provides generic API design and reference baseline implementations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i5.p1.1" class="ltx_p">PaddleFL <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/PaddleFL</a></span></span></span> is an open-source framework that supports the replication and comparison of FL algorithms as well as the deployment of FL systems in distributed clusters.</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S2.I1.i6.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i6.p1.1" class="ltx_p">TiFL is a Tier-based Federated Learning System that groups clients into tiers based on their training performance to mitigate the straggler problem caused by the heterogeneity of clients’ capabilities or data quantity.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S2.I1.i7.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i7.p1.1" class="ltx_p">FLOWER<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/adap/flower" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/adap/flower</a></span></span></span> is a new open-source framework-agnostic by design that promotes various aggregation algorithms and deep learning frameworks (e.g. Tensorflow, MXNet,TFLite and PyTorch). Moreover, Flower supports training and evaluation on heterogeneous real-edge devices and multi-cluster nodes.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">In addition to the frameworks mentioned above, OpenFL proposed by Intel specializes in healthcare use-cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. NVIDIA has recently open sourced a standalone python library called NVFlare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para ltx_noindent">
<p id="S2.SS4.p4.1" class="ltx_p">Unlike the above mentioned frameworks, the main aim of FEDn is to provide a production-grade and framework-agnostic distributed implementation with strong scalability and resilience features supporting both cross-silo and cross-device scenarios. To this effect, FEDn implements a two-tiered hierarchical federated learning architecture with a framework based loosely on the well-known MapReduce programming pattern. The work most closely related to ours is the architecture proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, where the authors seek to provide load-balancing capabilities through replication of local servers (with a similar role as combiners in our terminology, see the following sections). However, to the best of our knowledge no open source implementation associated with the work is available, and our work goes beyond that work in the size and scale for large model updates, and in terms of resiliency features.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>A flexible and horizontally scalable architecture for federated learning</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Architecture overview ‣ 3 A flexible and horizontally scalable architecture for federated learning ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the logical architecture of FEDn. Inspired by the map-reduce paradigm, a well-known scalable distributed systems design, the system consists of three logical tiers. The first tier consists of geographically distributed <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">clients</em>. A client is responsible for local model updates and interfaces with the local data source. Clients are the only entity that touches the private datasets.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2103.00148/assets/figures/FEDn-arch.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the computational model and logical architecture of FEDn. FL model training is organized in three logical layers, each populated by components with clearly defined roles. By scaling the number of Combiners we are able to meet communication demands imposed by a growing number of clients.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">The second tier is composed of one or many <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Combiners</em> and a <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Reducer</em>. Combiners are stateless servers with responsibility for coordinating updates from their own subset of clients. Together, the combiners and reducer make up the <em id="S3.SS1.p2.1.3" class="ltx_emph ltx_font_italic">FEDn network</em>. The number of combiners needed in such a network depends on the number of clients and the size of models. Each combiner is responsible for producing a partial model update in a global round of federated training. The partial models are aggregations of model updates by the combiner’s associated clients.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">The key components of the third tier is the <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Controller</em> and the <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Discovery service</em>. The controller is responsible for coordinating the overall computation done in a global training round, for maintaining a trail of global models (model checkpointing), and to handle global state. The Discovery service component is responsible for receiving client connection requests and assigning clients to combiners in the network.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">More details on the architecture and implementation can be found in the the online documentation at https://scaleoutsystems.github.io/fedn/.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Design pattern for implementing FL schemes in FEDn</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">All algorithms supported by FEDn should be able to scale horizontally to meet increasing load on the aggregation servers. For this reason, a user wanting to implement a custom FL scheme in FEDn does this by a) implementing the combiner lever aggregate function (aggregate partial models), b) implementing the Reducer level aggregation function (reduce combiner level models into a global model) and c) implement the global round controller. For each of these, abstract classes define the interface. The default scheme in FEDn employs FedAvg on both the combiner and reducer level resuling in hierarchical FedAvg. While this model may seem restrictive, in practice it enables a large space of possible schemes (for example by simply grouping combiner models on the reducer level, a wide range of ensemble strategies could be achieved).</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation of the FEDn network</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2103.00148/assets/figures/Communication.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="332" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Implementation of FEDn. FEDn provides functionality for status logging, monitoring and visualization of training progress. The current implementation uses MongoDB to store logs and state, and S3/Minio to store the global model trail, however the API allows for different storage backends to be added.
</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.3 Implementation of the FEDn network ‣ 3 A flexible and horizontally scalable architecture for federated learning ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a schematic of the key components of the software. The central communication protocol between clients and combiners is built using Google Protocol buffers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and gRPC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, a popular framework for remote procedure call (RPC). The main reason for this technology choice, apart from high-performance, is that it allows for a great deal of flexibility in the language choice when it comes to implementation of clients. This is important since clients may need to run in resource constrained systems, or on edge devices with special hardware and software environments.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Combiner</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">The Combiner (CB) is a stateless gRPC server whose main role is to coordinate client updates and to aggregate model updates from a given subset of clients. During a model update round, each combiner operates completely independently of all other combiners in the network. At any given point in time, the combiner works on one and only one partial model update. The fact that the combiner is stateless makes the network highly fault-tolerant – a combiner that becomes unresponsive can easily be replaced, and in the processes only model updates from its associated clients for that particular round is lost. It also results in a horizontally scalable network since there is no communication between combiners. The work done by the combiner scales linearly with the number of attached clients, and the total number of clients that can be supported by the network is directly proportional to the number of combiners.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Reducer</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The reducer is responsible for implementing and executing a reducer protocol, combining all partial model updates computed by combiners into a single global model update in each round. FEDn supports use of multiple reducers running in active-passive mode. At any given time, one reduce is responsible for preparing the global model. In the current implementation, the reduce operation is implemented as a separate service that pulls partial model updates from the combiners via a gRPC stream and incrementally aggregates these updates. However, in general the reducer protocol can be extended to execute e.g. hierarchical reduce directly using combiners, or with a secure multiparty computation protocol (but this would affect scalability properties). The work done by the reducer is scales with the number of combiners (in a protocol-dependent manner, linearly in the current implementation) and is independent of the number of clients.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Clients</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">A client (CL) is the main worker in the system. It is an actor that runs on a local site or device and is able to directly access local private data. Clients join a network by asking the discovery service for a combiner assignment. It then connects to its combiner and receives training and validation requests, downloads the global model from the combiner, executes model updates and validations, and streams the results back to the combiner. At the initialization of a federated model, a <em id="S3.SS3.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">compute package</em> is prepared by the model initiator and uploaded to the Controller. When a client sends a connection request to the discovery service, it receives this compute package from the controller and stages it in its local execution environment. Two design objectives related to clients have been central when developing FEDn: 1) <em id="S3.SS3.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">ML framework and language agnostic.</em> To support a wide range of applications, we use a black-box execution model. Each invocation of either a model update or model validation task executes as a single-input single-output (SISO) program. The only requirement from the framework’s perspective is that it knows how to serialize and deserialize the input and output. This allows for a ML-framework agnostic implementation and we currently support both Keras/Tensorflow and PyTorch out of the box (examples using both frameworks are provided in the Supplementary Manuscript). It also means that we cannot use any knowledge about the internals of the network for optimizations or fine-grained parallelism since the unit of computation is the complete model.
2) <em id="S3.SS3.SSS3.p1.1.3" class="ltx_emph ltx_font_italic">No incoming connections allowed.</em> It should not be necessary to have any open ingress ports on the client host. This is a firm requirement in many production scenarios. A client is an gRPC client (Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.3 Implementation of the FEDn network ‣ 3 A flexible and horizontally scalable architecture for federated learning ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>C), which listens for messages on a unidirectional streaming RPC service endpoint on the combiner. Model updates are downloaded and uploaded to the combiner using a dedicated gRPC Model service (Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.3 Implementation of the FEDn network ‣ 3 A flexible and horizontally scalable architecture for federated learning ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>D). These services are responsible for chunking the binary large objects (model updates) and for data transfer using server streaming RPC and client streaming RPC respectively.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Controller Round Protocol</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">The controller implements a configurable round protocol. During each global round of training, the following sequence of events take place in the default implementation:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Ask all combiners if they can participate in the round using the <em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">round participation policy</em>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Set a deadline and ask all participating combiners to coordinate a partial model update.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Wait until all combiners report a completed update, or the round times out.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Check if the round should be considered valid by evaluating a <em id="S3.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">round validity policy</em>.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">If the round is deemed valid, ask the reducer to aggregate all combiner partial models into the global model.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Ask combiners to coordinate <em id="S3.I1.i6.p1.1.1" class="ltx_emph ltx_font_italic">model validations</em> (optional).</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S3.I1.i7.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i7.p1.1" class="ltx_p">Commit the global model to the model trail.</p>
</div>
</li>
</ol>
<p id="S3.SS4.p1.2" class="ltx_p">Global rounds are repeated as needed for model convergence. Several steps in this processes can be configured, hence altering the detailed behavior in a round. For example, the round validity policy can be used to enforce certain requirements on how many combiners need to be successful in a round for the global model to be updated (the default is one).</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Resilience</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">The training process is able to recover robustly in the events of failures in the FEDn network. Resilience is vital for production grade federated machine learning. The strategy in FEDn is horizontal component replication at each tier.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p">The clients in the first tier can join and leave the framework at any time. If a client drops out during a round, its controlling combiner waits for a configurable amount of time (model update deadline), then proceeds with its next set of instructions. A client can simply rejoin at a later time. In practice the deadline is a parameter that should be chosen according to the training cost and balance risk of delays vs risk of missing model updates in a round. Future work will provide additional guidelines and support for choosing this system parameter.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<p id="S3.SS5.p3.1" class="ltx_p">For each client, discovery of combiners is dynamic. The available discovery service assign a combiner to each client. If a combiner becomes unavailable or have some issues during run time, clients get automatically reassigned to another available combiner.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para ltx_noindent">
<p id="S3.SS5.p4.1" class="ltx_p">The reducer component is responsible for the generation of the global model. In classical federated machine learning settings, a reducer can be viewed as a single point-of-failure. However, the FEDn architecture offers a horizontally scalable reducer setup with active-passive settings. At any given time, one reducer is responsible for the global model preparation to ensure consistency. In case of network disruption or other failures, a passive reducer becomes active and offer services. Here it is important to note that if a failure occurs during an active training round, the training process stops to avoid the risk of inconsistency in the models. The user can simply resume training from the last completed round. Supplementary videos demonstrating the resiliency of the framework can be found on the Scaleout YouTube channel <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://www.youtube.com/channel/UCZVv30LFXMQUOswNDKuQpNA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.youtube.com/channel/UCZVv30LFXMQUOswNDKuQpNA</a></span></span></span>.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para ltx_noindent">
<p id="S3.SS5.p5.1" class="ltx_p">Finally, all base services needed by FEDn also follow the same strategy of replication. The backend database, MongoDB can be deployed with high availability settings. Other services, discovery and monitors are all stateless units that can scale horizontally. All the replicated units can have a signal entry point using HAProxy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">To evaluate the performance and scalability of FEDn, we consider two use-cases: 1) a cross-silo natural language processing use-case and 2) a cross-device use-case based on digital human activity recognition. In the cross-silo case, we focus on the cost of a global round of federated training as a function of model size focusing on horizontal scalability and throughput in a geographically distributed setting. In the cross-device case, we benchmark single combiner performance with lightweight clients in the range of 100-1000 clients demonstrating framework implementation efficiency focusing on resource utilization. Taking together, our experiments highlight that the proposed architecture and implementation performs well for both these application classes. We stress that the goal of our evaluations is not to study the accuracy of the federated models as such, but rather to investigate how the cost of each global round depends on the problem class characteristic. With that said, it is important to ensure the correctness of the implementation. A convergence study and comparisons to a centrally trained model for the common CIFAR-10 benchmark dataset with a VGG-16 model and a PyTorch client is available at: https://github.com/scaleoutsystems/FEDn-client-cifar10-pytorch.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Horizontal scalability for a geographically distributed cross-silo use-case</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">A central task in natural language processing (NLP) is the generation of word embeddings (representation models), i.e. representations of every word in a vector space, allowing words with similar meaning to have a similar representation.
Large scale representation models such as Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, Glove <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, fasText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Elmo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, have significantly advanced NLP in recent years.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Private companies or government agencies (e.g., banks, hospitals, and institutes), each own unique large corpora with specific information. These corpora might not be complete enough to individually train high-quality NLP tasks. These organizations may want to collaborate to build a unified model, but without sharing the contents of their corpora.
FL thus holds large potential to advance NLP. However, the size of such models poses a challenge for FL systems due to the large data transfer requirements.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model update size and number of parameters</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:226.7pt;height:30.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.0pt,2.7pt) scale(0.85,0.85) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">10 MB</span></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">50MB</span></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">100MB</span></th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">200MB</span></th>
<th id="S4.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.1.5.1" class="ltx_text ltx_font_smallcaps">1GB</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">2638895</span></td>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.2.1.2.1" class="ltx_text ltx_font_smallcaps">13624335</span></td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.2.1.3.1" class="ltx_text ltx_font_smallcaps">27124335</span></td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.2.1.4.1" class="ltx_text ltx_font_smallcaps">54664335</span></td>
<td id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.1.2.1.5.1" class="ltx_text ltx_font_smallcaps">273329135</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">The purpose of this example is to demonstrate how the FEDn architecture enables large cross-silo use-cases by scaling the combiner network.
We conducted experiments using the IMDB dataset with different model sizes (10MB, 50MB, 100MB, 200MB, 1GB). The model is generated using: 1) Embedding layer (max_features = 20000, maxlen = 100, embedding_size = 128), 2) Convolution (kernel_size = 5, filters = 64, pool_size = 4), 3) LSTM (lstm_output_size = 70) and the local training parameters per round are: 1 epoch, batch size 32 and ADAM as the optimizer. The different model sizes were generated by tuning the Embedding layer of the seed model, see the Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For all experiments we used cloud computing resources from two clouds; SNIC Science Cloud, a Swedish National OpenStack cloud, and Amazon Elastic Compute Cloud (AWS EC2). All VMs are based on Ubuntu 20.04 Server LTS.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Validation of FEDn training accuracy</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">As mentioned earlier, factors affecting convergence rate and thus the number of global rounds is an active research area out of the scope of this study. However, to demonstrate both the correctness of the FEDn implementation, we partitioned the dataset in 10 equal chunks, so that each client has <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS1.SSS1.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">10\%</annotation></semantics></math> of the total dataset. We then compare four federated scenarios to centralized model training: 1) 2 clients for 50 rounds (1CB-2CL) 2), 5 clients (1CB-5CL) and 3) 10 clients (1CB-10CL).</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p2.2" class="ltx_p">The result in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1.1 Validation of FEDn training accuracy ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that FL in FEDn reaches the baseline centralized performance (horizontal line) as more clients (and hence more of the data) are used. In particular, federated learning achieves 87<math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mo id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\%</annotation></semantics></math> accuracy for 10 clients (baseline accuracy 86<math id="S4.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mo id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">\%</annotation></semantics></math> in this experiment).</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2103.00148/assets/figures/accuracy.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="250" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Convergence comparison on IMDB dataset in three different scenarios: 1) 1 combiner, 2 clients and 50 rounds 2) 1 combiner, 5 clients and 50 rounds, 3) 1 combiner, 10 clients and 50 rounds.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>The combiner round time scales linearly with model size</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We first consider a FEDn network consisting of a single, high-powered combiner (8 core, 16GB RAM) in SSC with 48 connected clients, 12 8VCPU,16GB instances in SSC and 12<em id="S4.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">c5.2xlarge</em> instances in EC2 in the <em id="S4.SS1.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">eu-north</em> region and measure the average round time over 5 global rounds. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1.2 The combiner round time scales linearly with model size ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows how the round time at the combiner is affected by increasing model size (orange bars). Since the model size affects both the training time at clients and the cost for data transfer and model aggregation, we show the mean training time for reference (blue bars).</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2103.00148/assets/figures/model_size.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Combiner round times for increasingly large models (NLP downstream tasks).</figcaption>
</figure>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">As can be seen, for the smallest model (10MB) the round time is marginally higher than the client training time and the single combiner can comfortably support the 48 clients. As we scale up the model size, as expected we see an almost linear increase in round time due to the increased communication cost of transferring updates. At 200MB updates the overhead is almost 300s per round. To demonstrate the load balancing capabilities, we next deployed additional combiners for a total of 2 combiners (<em id="S4.SS1.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">200_2CB</em>) and 4 combiners (<em id="S4.SS1.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">200_4CB</em>). As can be seen, this scaling of the FEDn network is an efficient way to bring round time down by balancing work over more aggregation servers.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2103.00148/assets/figures/FEDn-cross-silo.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Geographically distributed horizontal scaling with 1GB model updates. Combiners are spread over 4 regions in AWS EC2 (A). This FEDn network accommodates 40 clients in this experiment, coordinating model updates of in total 40GB in each round (B).</figcaption>
</figure>
<div id="S4.SS1.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">To conclude, Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1.2 The combiner round time scales linearly with model size ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the mean combiner round time is to a large extent governed by the size of the model updates for a fixed number of clients, and that scaling the combiner network horizontally is an efficient way to reduce round time in this scenario.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Multi-region horizontal scaling for large model updates</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">Next we conducted a large-scale geographically distributed experiment using the 1GB version of the model. We deployed a FEDn network with in total 4 combiners, each on a large server (c5.4xlarge, 16VCPU,32GB RAM flavor) in four regions of AWS EC2, eu-north-1 (Stockholm), eu-west-1 (Ireland), us-east-1 (N. Virgina) and eu-central-1 (Frankfurt). The combiners are illustrated with blue dots in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.1.2 The combiner round time scales linearly with model size ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>A and clients with red dots (one dot per region, many clients per dot). The reducer is deployed on a c5.4xlarge instance in the Stockholm region.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">We then conducted a weak scaling experiment, doubling both the number of clients and the number of combiners in three successive experiments by attaching 6 c5.2xlarge clients to each combiner, and measured the round time, see Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.1.2 The combiner round time scales linearly with model size ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>B 1CB_6CL, 2CB_12CL and 4CB_24CL. As can be seen, the system is capable of accommodating 1GB model update transfers with growing number of clients at a near constant combiner network round time. However, as expected, the cost of the reducer, and hence the total round time, grows with the number of combiners. With this network configuration, a 4x increase in the number of clients led to a 2.0x increase in round time. This is in itself a good property, but also highlights an interesting aspect of sizing a FEDn network. In the final experiment we kept all 4 combiners fixed and allowed 16 more clients, 5 from eu-west-1 and 11 from eu-north-1, for a total of 40 clients, to connect to the combiners randomly, i.e. there is no guarantee that they will connect to a geographically close combiner (4CB_40CL). As can be seen, there was capacity in the combiner network, and while the mean round time on the combiner level increased from 260s to 322s, a 1.6x increase in the number of clients only leads to a 10% increase in round time. This illustrates the nature of the map-reduce like architecture - it is important for overall throughput to optimize the ratio of clients per combiner and total number of combiners.</p>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">Zooming in on the detailed workload distribution, we see that the majority of time spent at clients are related to the actual model training, with roughly 11% overhead from downloading and sending models (Supplementary Material). Combiners spend on average only 3% of time aggregating weight updates (Supplementary Material), the rest is spent waiting for model updates and receiving and loading weights. The reducer on the other hand spends a comparatively large fraction of time downloading and loading models from the combiners (Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.1.3 Multi-region horizontal scaling for large model updates ‣ 4.1 Horizontal scalability for a geographically distributed cross-silo use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The reason for this is the fact that we fetch models sequentially after all combiners finish, while the combiner implementation overlaps data transfer with computation. We plan to improve this part of the reducer in future work.</p>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS3.p4.1" class="ltx_p">Finally we note that in each round in our final experiment, we marshall (serialization, reading and writing to disk) and transport 40GB of data between clients and the FEDn network, at an average total throughput of 465MB/s. Taken together, our experiments demonstrate that the framework is capable of sustaining the workloads that can be expected for large-scale cross-silo applications.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2103.00148/assets/figures/Reducer_breakdown1.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Workload distribution for the reducer, numbers in percentages of total round time.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Resource utilization for a cross-device use-case</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Daily human activity recognition (DHAR) is a challenging research areas with impact on different sectors such as the health care system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, e.g. medical diagnosis, rehabilitation, and elderly care, and energy efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.
DHAR leverages data collected from various IoT devices (sensors, mobiles, smartwatches, etc.) that are distributed in a smart home for machine learning, with the goal to understand and learn the person’s daily behaviors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. However, it can be privacy-invasive to share data about different users’ behaviors with third-party stakeholders, hence it is a promising use-case for federated learning.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.6" class="ltx_p">We use the CASA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and a Long Short Term Memory (LSTM) model for human activity recognition to evaluate FEDn in a cross-device setting. The data has been collected from 30 different homes over a two month period from continuous ambient and PIR sensors distributed throughout the home, which contains <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="13956534" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">13956534</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">13956534</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">13956534</annotation></semantics></math> patterns, each pattern comprises a set of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="37" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">37</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">37</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">37</annotation></semantics></math> features linked to different sensors, that represents the daily human activity (sleep, eat, read, watch tv,etc.) for one volunteer in each. The model architecture comprises of one LSTM input layer, four dense layers, and one output layer, with the aim to classify the output to 10 different user daily activities. The model has in total 68,884 trainable parameters and the size of the model is <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="254KB" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">254</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1a" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.4" xref="S4.SS2.p2.3.m3.1.1.4.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">254</cn><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝐾</ci><ci id="S4.SS2.p2.3.m3.1.1.4.cmml" xref="S4.SS2.p2.3.m3.1.1.4">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">254KB</annotation></semantics></math>. The local training parameters setting per round are, <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mn id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><cn type="integer" id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">1</annotation></semantics></math> epoch, <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mn id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><cn type="integer" id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">32</annotation></semantics></math> batch size and <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mn id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><cn type="float" id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">0.01</annotation></semantics></math> learning rate with ADAM as optimizer. The same LSTM model has been used for all experiments presented in this section.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2103.00148/assets/figures/combinerCapacity.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Round time as a function of number of clients for a single large combiner. A 5x increase in number of clients only leads to a 2x increase in round time.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.2 Resource utilization for a cross-device use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that a single large combiner can comfortably handle up to <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">1000</annotation></semantics></math> client connections in our experiments. We studied the round time as a function of the number of clients and observed a 2x increase in round time when the number of clients grow from 200 to 1000. This illustrates the communication efficiency of the combiners.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Resource-constrained combiners</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.2" class="ltx_p">In a fog/edge scenario it is not always possible to rely on the type of high-powered resources we used in the previous section. Here we conducted a vertical scaling study to study the overall resource cost-benefit trade-off for hosting the framework. We conducted experiments where 600 clients were allowed to connect to different combiner configurations with varying resource constraints. Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2.1 Resource-constrained combiners ‣ 4.2 Resource utilization for a cross-device use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows FEDn workload distribution over both combiner and clients while training. From the left barplot, we see that the FEDn core components do not require high-end specialized resources for operational tasks. In fact, while the percentage combiner workload using one large combiner (29.20 <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mo id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">\%</annotation></semantics></math>) is lower than with one small combiner (38.70<math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mo id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\%</annotation></semantics></math>), the FEDn core components still work reliably also in the case of a resource-constrained environment. Moreover, the figure illustrates that by replicating small combiners, the overall workload is subdivided resulting in improved performance and efficiency over a single large combiner. The experiments also illustrate that by adding more constrained-resources (small combiner), the framework performs better and better, together with increased robustness from replicated services. In fact, three small combiners perform better than a single large combiner. This result again helps to better estimate the deployment budget and also highlights the flexibility in terms of deployment strategies and effective utilization of resources of the framework.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2103.00148/assets/figures/IoT_scalability.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The left panel shows the FEDn workload distribution over combiner and clients and the communication between them for different combiner flavours and configuration. The right panel shows client training time distribution and the fraction of time used by the clients to download, train, and upload, the model respectively.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p2.2" class="ltx_p">The pie chart in Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.2.1 Resource-constrained combiners ‣ 4.2 Resource utilization for a cross-device use-case ‣ 4 Evaluation ‣ Scalable federated machine learning with FEDn" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the combiner’s overall workload distribution during the federated training process. The blue color in the pie chart represents the time taken by the clients during the course of the federated training (85.4 <math id="S4.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mo id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">\%</annotation></semantics></math> of the total execution time required to complete <math id="S4.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.SSS1.p2.2.m2.1a"><mn id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.1b"><cn type="integer" id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.1c">20</annotation></semantics></math> rounds). As expected, the client-side consumed most of the time in the local training and model sharing processes, highlighting the relatively low overhead from the framework federation, even using low-cost hardware for combiners. Here it is important to note that, this distribution of the workload is highly dependent on the model size, model parameters, dataset type, and data size used to train local models at each client’s side (as we saw in the previous cross-silo experiments).</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p3.2" class="ltx_p">The histogram chart shows the training time distribution amongst clients. We can see that a high number of clients accomplish the training process between <math id="S4.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="8-27" display="inline"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mrow id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p3.1.m1.1.1.2" xref="S4.SS2.SSS1.p3.1.m1.1.1.2.cmml">8</mn><mo id="S4.SS2.SSS1.p3.1.m1.1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS2.SSS1.p3.1.m1.1.1.3" xref="S4.SS2.SSS1.p3.1.m1.1.1.3.cmml">27</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><apply id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1"><minus id="S4.SS2.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS2.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1.2">8</cn><cn type="integer" id="S4.SS2.SSS1.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1.3">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">8-27</annotation></semantics></math> seconds and a small fraction falls between <math id="S4.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="40-45" display="inline"><semantics id="S4.SS2.SSS1.p3.2.m2.1a"><mrow id="S4.SS2.SSS1.p3.2.m2.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.cmml"><mn id="S4.SS2.SSS1.p3.2.m2.1.1.2" xref="S4.SS2.SSS1.p3.2.m2.1.1.2.cmml">40</mn><mo id="S4.SS2.SSS1.p3.2.m2.1.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS2.SSS1.p3.2.m2.1.1.3" xref="S4.SS2.SSS1.p3.2.m2.1.1.3.cmml">45</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.2.m2.1b"><apply id="S4.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1"><minus id="S4.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.1"></minus><cn type="integer" id="S4.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.2">40</cn><cn type="integer" id="S4.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.3">45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.2.m2.1c">40-45</annotation></semantics></math> seconds, which means that we observe some stragglers or communication delays during the execution. This illustrates that federated model training using the FEDn framework is highly efficient in its communication strategy and at the same time robust enough to manage unexpected behaviors that may happen during the training process. These features are essential to offer production-grade services in geographically distributed settings.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p">Cross-device federated training requires a framework that can manage high-throughput communication based on a very large number of devices, efficient processing to prepare partial and global models, and robust mechanisms to manage devices with constrained-resources (low processing or communication capabilities). The presented experiments clearly illustrate that the framework is well-suited for cross-device federated learning.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this paper, we have designed and developed FEDn, a lightweight, scalable, robust, and efficient framework that is vendor-agnostic both in terms of the underlying machine learning libraries and also the distributed infrastructure technologies. In this article, we present an in-depth performance analysis based on both large numbers of clients (cross-device) and large models (cross-silo), training federated models in real, heterogeneous distributed environments. Taken together, our results highlight that FEDn can sustain federated machine learning applications from many different application areas in production environments. Furthermore, the framework is based on a structured computational model, and it is our hope that it will be a useful framework also for researchers developing new federated schemes, allowing them to rapidly test ideas in real complex distributed and heterogeneous infrastructure environments.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Availability</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">FEDn is publicly available under the Apache2 license at <a target="_blank" href="https://github.com/scaleoutsystems/fedn" title="" class="ltx_ref ltx_href">https://github.com/scaleoutsystems/fedn</a>. The models and examples in this benchmark are available at <a target="_blank" href="https://github.com/scaleoutsystems/examples" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/scaleoutsystems/examples</a>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgement</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We acknowledge valuable discussions on the design with Daniel Zakrisson, Jens Frid and Stefan Hellander, and code contributions from Li Ju, Fredrik Wrede and Desislava Stoyanova. Funding has been provided by the eSSENCE strategic collaboration on eScience (Alawadi, Ait-Mlouk, Toor, and Hellander) and the Swedish Innovation Agency Vinnova grant. no. 2019-02819 (awarded to Scaleout Systems AB).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, 2017, pp.
1273–1282.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. [2019]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1912.04977</em>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al. [2016]</span>
<span class="ltx_bibblock">
J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">NIPS Workshop on Private Multi-Party Machine
Learning</em>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2019]</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards
federated learning at scale: System design,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1902.01046</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheller et al. [2018]</span>
<span class="ltx_bibblock">
M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas,
“Multi-institutional deep learning modeling without sharing patient data: A
feasibility study on brain tumor segmentation,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International
MICCAI Brainlesion Workshop</em>.   Springer, 2018, pp. 92–104.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Y. Liu, Z. Ma, X. Liu, Z. Wang, S. Ma, and K. Ren, “Revocable federated
learning: A benchmark of federated forest,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1911.03242</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boyd et al. [2011]</span>
<span class="ltx_bibblock">
S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Distributed optimization and statistical learning via the alternating
direction method of multipliers,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Foundations and
Trends® in Machine learning</em>, vol. 3, no. 1, pp. 1–122,
2011.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al. [2016]</span>
<span class="ltx_bibblock">
E. P. Xing, Q. Ho, P. Xie, and D. Wei, “Strategies and principles of
distributed machine learning on big data,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Engineering</em>, vol. 2,
no. 2, pp. 179–195, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2014]</span>
<span class="ltx_bibblock">
M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
J. Long, E. J. Shekita, and B.-Y. Su, “Scaling distributed machine learning
with the parameter server,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">11th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 14)</em>, Broomfield, CO, 2014, pp.
583–598.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence of fedavg
on non-iid data,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.02189</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. [2018]</span>
<span class="ltx_bibblock">
S. Caldas, P. Wu, T. Li, J. Konečnỳ, H. B. McMahan, V. Smith, and
A. Talwalkar, “Leaf: A benchmark for federated settings,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1812.01097</em>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em>, vol. 10, no. 2, pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2020]</span>
<span class="ltx_bibblock">
Y. Wu, S. Cai, X. Xiao, G. Chen, and B. C. Ooi, “Privacy preserving vertical
federated learning for tree-based models,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2008.06170</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. [2018]</span>
<span class="ltx_bibblock">
S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, “Don’t decay the
learning rate, increase the batch size,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference
on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2019]</span>
<span class="ltx_bibblock">
K. Cheng, T. Fan, Y. Jin, Y. Liu, T. Chen, and Q. Yang, “Secureboost: A
lossless federated learning framework,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1901.08755,
2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al. [2020]</span>
<span class="ltx_bibblock">
J. Pang, Y. Huang, Z. Xie, Q. Han, and Z. Cai, “Realizing the heterogeneity: A
self-organized federated learning framework for iot,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of
Things Journal</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. [2019]</span>
<span class="ltx_bibblock">
M. Duan, D. Liu, X. Chen, Y. Tan, J. Ren, L. Qiao, and L. Liang, “ASTRAEA::
Self-balancing federated learning for improving classification accuracy of
mobile deep learning applications,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2019 IEEE 37th International
Conference on Computer Design (ICCD)</em>.   IEEE, 2019, pp. 246–254.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al. [2020]</span>
<span class="ltx_bibblock">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halevi and Shoup [2014]</span>
<span class="ltx_bibblock">
S. Halevi and V. Shoup, “Algorithms in helib,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Annual Cryptology
Conference</em>.   Springer, 2014, pp.
554–571.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. [2016]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>,
2016, pp. 308–318.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. [2018]</span>
<span class="ltx_bibblock">
T. Ryffel, A. Trask, M. Dahl, B. Wagner, J. Mancuso, D. Rueckert, and
J. Passerat-Palmbach, “A generic framework for privacy preserving deep
learning,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WeBank [2018]</span>
<span class="ltx_bibblock">
WeBank. (2018) Fate: An industrial grade federated learning framework.
[Online]. Available: <a target="_blank" href="https://fate.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://fate.fedai.org/</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh,
H. Qiu, L. Shen <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Fedml: A research library and benchmark for
federated machine learning,” <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al. [2020]</span>
<span class="ltx_bibblock">
Z. Chai, A. Ali, S. Zawad, S. Truex, A. Anwar, N. Baracaldo, Y. Zhou,
H. Ludwig, F. Yan, and Y. Cheng, “Tifl: A tier-based federated learning
system,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.09249</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. [2020]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, P. P. de Gusmão,
and N. D. Lane, “Flower: A friendly federated learning research framework,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.14390</em>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reina et al. [2021]</span>
<span class="ltx_bibblock">
G. A. Reina, A. Gruzdev, P. Foley, O. Perepelkina, M. Sharma, I. Davidyuk,
I. Trushkin, M. Radionov, A. Mokrov, D. Agapov <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Openfl: An
open-source framework for federated learning,” <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2105.06413</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA [2021]</span>
<span class="ltx_bibblock">
NVIDIA. (2021) Nvidia federated learning application runtime environment.
[Online]. Available: <a target="_blank" href="https://github.com/NVIDIA/NVFlare" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NVFlare</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google [2008]</span>
<span class="ltx_bibblock">
Google. (2008) Google protocol buffers. [Online]. Available:
<a target="_blank" href="https://github.com/protocolbuffers/protobuf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/protocolbuffers/protobuf</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">google [2015]</span>
<span class="ltx_bibblock">
google. (2015) grpc protocol. [Online]. Available: <a target="_blank" href="https://grpc.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://grpc.io/</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HAProxy [2013]</span>
<span class="ltx_bibblock">
HAProxy. (2013) Haproxy. [Online]. Available: <a target="_blank" href="http://www.haproxy.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.haproxy.org/</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. [2013]</span>
<span class="ltx_bibblock">
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” in
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>.   Curran Associates, Inc., 2013, pp. 3111–3119.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. [2014]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word
representation,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">In EMNLP</em>, 2014.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al. [2017]</span>
<span class="ltx_bibblock">
P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors
with subword information,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for
Computational Linguistics</em>, vol. 5, pp. 135–146, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al. [2018]</span>
<span class="ltx_bibblock">
M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
L. Zettlemoyer, “Deep contextualized word representations,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc.
of NAACL</em>, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2019]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding.”   Minneapolis, Minnesota: Association for Computational
Linguistics, Jun. 2019, pp. 4171–4186.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2018]</span>
<span class="ltx_bibblock">
J. Qi, P. Yang, A. Waraich, Z. Deng, Y. Zhao, and Y. Yang, “Examining
sensor-based physical activity recognition and monitoring for healthcare
using internet of things: A systematic review,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Journal of biomedical
informatics</em>, vol. 87, pp. 138–153, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadi-Karvigh et al. [2018]</span>
<span class="ltx_bibblock">
S. Ahmadi-Karvigh, A. Ghahramani, B. Becerik-Gerber, and L. Soibelman,
“Real-time activity recognition for energy efficiency in buildings,”
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Applied energy</em>, vol. 211, pp. 146–160, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alkhabbas et al. [2020]</span>
<span class="ltx_bibblock">
F. Alkhabbas, S. Alawadi, R. Spalazzese, and P. Davidsson, “Activity
recognition and user preference learning for automated configuration of iot
environments,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on
the Internet of Things</em>, 2020, pp. 1–8.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook [2010]</span>
<span class="ltx_bibblock">
D. J. Cook, “Learning setting-generalized activity models for smart spaces,”
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE intelligent systems</em>, vol. 2010, no. 99, p. 1, 2010.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Morgan Ekmefjord,Addi Ait-Mlouk,Sadi Alawadi, Mattias Åkesson, Desislava Stoyanova,Ola Spjuth,Salman Toor,Andreas Hellander"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Federated machine learning , artificial intelligence,privacy"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="q-bio.NC, q-bio.QM"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="Scalable federated machine learning with FEDn"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2103.00147" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2103.00148" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2103.00148">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2103.00148" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2103.00149" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 01:40:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
