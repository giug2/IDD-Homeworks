<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models</title>
<!--Generated on Fri Aug 16 15:44:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Foundation Models,  Retrieval-augmented,  Few-shot Segmentation,  Medical Image.
" lang="en" name="keywords"/>
<base href="/html/2408.08813v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S1" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S2" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S2.SS1" title="In II Related Works ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Foundation Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S2.SS2" title="In II Related Works ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Retrieval-augmented Techniques</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S2.SS3" title="In II Related Works ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Few-shot Medical Image Segmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS1" title="In III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Preliminaries of SAM 2 Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS2" title="In III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Retrieval Module</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS3" title="In III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Few-shot Segmentation Framework</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS1" title="In IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS1.SSS1" title="In IV-A Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>ACDC Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS1.SSS2" title="In IV-A Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>CMR T1-Map Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS1.SSS3" title="In IV-A Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Fluoroscopy Image Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS2" title="In IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS3" title="In IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Segmentation Results on Three Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS4" title="In IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Comparison with Supervised Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.SS5" title="In IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Ablation Studies</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S5" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S5.SS1" title="In V Discussion ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Annotation Tools</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S5.SS2" title="In V Discussion ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Limitations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S6" title="In Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lin Zhao, Xiao Chen, Eric Z. Chen, Yikang Liu, Terrence Chen, Shanhui Sun
</span><span class="ltx_author_notes">L. Zhao, Y. Liu, X. Chen, E. Chen, T. Chen and S. Sun are with United Imaging Intelligence, 65 Blue Sky Drive, Burlington, MA 01803, USA. (e-mail: shanhui.sun@uii-ai.com)</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Medical image segmentation is crucial for clinical decision-making, but the scarcity of annotated data presents significant challenges. Few-shot segmentation (FSS) methods show promise but often require retraining on the target domain and struggle to generalize across different modalities. Similarly, adapting foundation models like the Segment Anything Model (SAM) for medical imaging has limitations, including the need for finetuning and domain-specific adaptation. To address these issues, we propose a novel method that adapts DINOv2 and Segment Anything Model 2 (SAM 2) for retrieval-augmented few-shot medical image segmentation. Our approach uses DINOv2’s feature as query to retrieve similar samples from limited annotated data, which are then encoded as memories and stored in memory bank. With the memory attention mechanism of SAM 2, the model leverages these memories as conditions to generate accurate segmentation of the target image. We evaluated our framework on three medical image segmentation tasks, demonstrating superior performance and generalizability across various modalities without the need for any retraining or finetuning. Overall, this method offers a practical and effective solution for few-shot medical image segmentation and holds significant potential as a valuable annotation tool in clinical applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Foundation Models, Retrieval-augmented, Few-shot Segmentation, Medical Image.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Medical image segmentation plays a critical role in diagnosis, treatment planning, and the quantification of tissue volumes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib2" title="">2</a>]</cite>. Accurate segmentation enables clinicians to precisely delineate anatomical structures and abnormalities within medical images, which is essential for making informed clinical decisions. However, unlike natural image segmentation, medical image segmentation faces unique challenges due to the limited availability of data and annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib5" title="">5</a>]</cite>. Obtaining precise annotations requires the expertise of trained medical professionals and is time-consuming, making it difficult to gather enough labeled samples for supervised training. In practice, while acquiring a large number of annotations is often impractical, obtaining a smaller set, such as a few dozen, is more feasible. Maximizing the utility of these limited samples to create reliable segmentation masks without compromising the accuracy is a pressing challenge in the field.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="568" id="S1.F1.1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) Illustration of how human annotators learn to segment medical images: By studying a few annotated examples, the human annotator can effectively apply the learned knowledge to segment a new, unseen case.(b) Representation of our proposed model’s process: Retrieving the contextual and anatomical information from similar annotated example to guide the foundation models to perform the segmentation for new case without any retraining or finetuning.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Few-shot segmentation (FSS) offers promising potential to address the aforementioned challenges in the medical imaging domain, particularly the scarcity of labeled data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib6" title="">6</a>]</cite>. Various approaches have been developed to enable models to learn from a limited number of annotated samples, such as ALPNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib7" title="">7</a>]</cite>, ADNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib8" title="">8</a>]</cite>,Q-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib9" title="">9</a>]</cite>, and CAT-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib10" title="">10</a>]</cite>. Despite their effectiveness and success, these methods have certain limitations. They often require retraining on a target domain with known classes before being applied to new classes within the same domain, limiting their ability to generalize across different medical image modalities. Additionally, most of these methods employ a 1-way 1-shot setting, where neighboring slices are used as support images for FSS. While the 1-shot setting can be valuable in research, acquiring a small number of annotated samples is feasible and aligns better with clinical practices. Furthermore, reliance on neighboring slices can constrain the model’s flexibility and effectiveness, especially when annotations for neighboring slices are unavailable for new subjects or when there is significant anatomical variability between slices. These limitations restrict the practical application and effectiveness of these models in diverse clinical scenarios.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Foundation models, pretrained on large-scale datasets across various domains, have demonstrated strong few-shot and zero-shot capabilities. In the realm of image segmentation, the Segment-Anything Model (SAM) has shown remarkable zero-shot generalization when driven by prompts, suggesting its potential for FSS in medical images. However, SAM was pretrained on natural images and underperforms in specialized areas like medical imaging, leading to various adaptations. These adaptations include constructing large-scale medical image datasets for fine-tuning SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib11" title="">11</a>]</cite>, using adapters to tailor SAM for domain-specific needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib12" title="">12</a>]</cite>, and extending SAM from 2D to 3D medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib14" title="">14</a>]</cite>. While these methods have improved performance compared to fully supervised models like U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib16" title="">16</a>]</cite>, they still require fine-tuning and retraining on specific domains or image modalities. Given a small amount of annotated data, fine-tuning and retraining may be ineffective. Intuitively, we could leverage these small annotated samples as prompts to guide the model in segmentation tasks, similar to how prompts guide large language models (LLMs). However, current methods still rely on prompts like points, boxes, or masks, which do not fully exploit the potential of the available annotated data.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recently, the Segment Anything Model 2 (SAM 2) introduced enhanced capabilities for image segmentation and extended promptable segmentation to video applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib17" title="">17</a>]</cite>. A key modification in SAM 2 for video segmentation is the memory-driven approach which leverages temporal information by encoding segmentations from previous video frames as memories and condition on those memories to produce the segmentation of subsequent frames. Such ability naturally adapts to few-shot medical image segmentation: instead of integrating information from previous frames, we can retrieval the contextual and anatomical information from similar cases to guide the segmentation process, as the Retrieval-Augmented Generation (RAG) approach used in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib19" title="">19</a>]</cite>. This mirrors how humans learn to perform segmentation tasks: even without prior knowledge, they can observe and study a few examples, quickly understand the process, and then apply it effectively (<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S1.F1" title="In I Introduction ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Building on this intuition, we propose a novel framework that adapts DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib20" title="">20</a>]</cite> and SAM 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib17" title="">17</a>]</cite> for few-shot medical image segmentation. The framework features a retrieval module that uses DINOv2 to generate embeddings and retrieve similar samples from limited annotated data. The segmentation network, adapted from SAM 2, encodes these retrieved images and masks as memories stored in a memory bank. The memory attention mechanism then integrates relevant features and anatomical structures from these stored memories, enhancing the input image features for the decoder which generates the final segmentation mask with greater precision and accuracy. Notably, our framework directly utilizes the structure and pre-trained weights without any retraining or finetuning in the target domain. When applied to a new domain, only the samples in the retrieval database need to be updated. We evaluate the proposed framework on three medical image segmentation datasets, including ACDC, CMR T1-Map, and Fluoroscopy Image datasets. Building on these foundation models, our method demonstrates superior performance and generalizes well across different modalities and contrasts without any retraining or finetuning. Our contributions in this work are summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce a novel framework that integrates retrieval augmentation and memory-driven approach, leveraging foundation models like DINOv2 and SAM 2 for accurate medical image segmentation with limited annotated data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We present an effective method to adapt retrieval-augmented techniques for querying relevant annotated samples in medical image segmentation. To the best of our knowledge, this is the first work to apply retrieval-augmented techniques to image segmentation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our framework demonstrates exceptional adaptability across various imaging modalities and contrasts without requiring retraining, highlighting its potential as a valuable annotation tool in diverse clinical scenarios.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Foundation Models</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Large-scale language, vision, and multimodal foundation models such as GPTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib23" title="">23</a>]</cite>, DINO (self-Distillation with No labels) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib20" title="">20</a>]</cite>, Contrastive Language-Image Pretraining (CLIP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib25" title="">25</a>]</cite>, and Large Language and Vision Assistant (LLaVA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib26" title="">26</a>]</cite> have dramatically transformed the artificial intelligence landscape. These models significantly enhance zero-shot and few-shot capabilities across natural language processing, image recognition, and multimodal applications. In image domain, DINOv2 has demonstrated exceptional proficiency in learning high-level semantic information, facilitating a variety of downstream tasks such as image classification, semantic segmentation, image retrieval, video tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib27" title="">27</a>]</cite>, and feature matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib28" title="">28</a>]</cite>. Another notable development is the Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib29" title="">29</a>]</cite>, a foundation model pretrained on a large-scale segmentation dataset, has shown outstanding zero-shot generalization capabilities in promptable segmentation. However, SAM underperformed in some specific domains such as medical imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib30" title="">30</a>]</cite> so various attempts have been made in fintuning and adapting it <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib11" title="">11</a>]</cite>. Recently, Segment Anything Model 2 (SAM 2) has brought enhanced capabilities for segmenting images and extending promptable segmentation to video applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib17" title="">17</a>]</cite>. In this paper, we leverage the semantic learning of DINOv2 for retrieval augmentation and perform the few-shot segmentation by adapting the video segmentation capabilities of SAM 2. This integration allows us to retrieval similar images with masks as prompts to guide segmentation for medical images without further training/finetuning as in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Retrieval-augmented Techniques</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Retrieval-Augmented Generation (RAG) in natural language processing combines the strengths of retrieval-based systems and generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib19" title="">19</a>]</cite>. RAG begins by retrieving relevant information from an external knowledge source based on the input query, providing contextual information that enhances the accuracy, informativeness, and contextual grounding of responses generated by large language models (LLMs). In the image domain, retrieval-augmented techniques have also been explored in various areas. For example, the Retrieval-Augmented Diffusion Model (RDM) was proposed for image synthesis, where the generative model is conditioned on informative samples retrieved from a database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib31" title="">31</a>]</cite>. Similarly, Retrieval-Augmented Classification (RAC) integrates an explicit retrieval module into standard image classification pipelines to improve long-tail visual recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib32" title="">32</a>]</cite>. Additionally, REtrieval-Augmented CusTomization (REACT) leverages relevant image-text pairs from a vast web-scale database to customize visual models for specific target domains, achieving significant performance across tasks while minimizing the need for extensive retraining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib33" title="">33</a>]</cite>. In this work, we extend the retrieval strategy to the image segmentation task, designing a retrieval module that queries an external database for contextual and anatomical information, which is then used to guide the SAM 2 model in few-shot medical image segmentation without requiring any retraining.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Few-shot Medical Image Segmentation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Few-shot segmentation (FSS) has been widely explored in the medical imaging field, where data and annotations are usually scarce. Typically, FSS frameworks follow the Prototype Alignment Network (PANet) approach, which learns class-specific prototype representations from a few support images and then performs segmentation on the query images by matching each pixel to the learned prototypes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib6" title="">6</a>]</cite>. For the FSS in medical imaging domain, ALPNet enhances PANet by introducing superpixel-based self-supervised learning, eliminating the need for manual annotations, and by incorporating an adaptive local prototype pooling module to address the foreground-background imbalance in medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib7" title="">7</a>]</cite>. The Anomaly Detection-inspired Network (ADNet) takes a different approach by relying on a single foreground prototype to compute anomaly scores for all query pixels, with segmentation achieved by thresholding these scores using a learned threshold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib8" title="">8</a>]</cite>. Q-Net improves the ADNet by adding a query-informed threshold adaptation module and a query-informed prototype refinement module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib9" title="">9</a>]</cite>.
CAT-Net, which leverages a cross-masked attention transformer, enhances the FSS by mining the correlations between the support and query images and restricts the focus to relevant foreground information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib10" title="">10</a>]</cite>. Despite the success of these FSS methods, they still require further retraining on the target domain and are typically trained and applied within the same domain for few-shot segmentation of new classes. This limitation significantly restricts their practicality in real-world applications. Our approach leverage the foundation models to perform the FSS without retraining in new domain, enabling more flexible and efficient segmentation across diverse medical imaging applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods</span>
</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="365" id="S3.F2.1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of Retrieval-Augmented Few-Shot Medical Image Segmentation Workflow: (a) The main segmentation pipeline starts with input images processed through DINOv2 for dino embedding, followed by querying similar images and corresponding masks which are encoded and stored in a memory bank. The memory attention mechanism integrates the information from memory bank to assist the mask decoder in generating the final segmentation mask. (b) The process of indexing limited annotated data using DINOv2 and Faiss, enabling efficient retrieval of relevant images to enhance segmentation accuracy.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Preliminaries of SAM 2 Model</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">SAM 2 (Segment Anything Model 2) is designed for both image and video segmentation, featuring several key components. It utilizes a hierarchical image encoder, pre-trained with MAE and Hiera, which captures multiscale image features for robust image representation. The memory attention mechanism in SAM 2 dynamically integrates information from past frames, leveraging self-attention and cross-attention within transformer blocks to condition current frame features on past data. This mechanism works in tandem with a memory bank that stores encoded memories of previous frames, allowing SAM 2 to recall and utilize temporal context effectively. Additionally, SAM 2’s mask decoder and prompt encoder enhance segmentation precision by incorporating high-resolution details and handling ambiguous prompts through predictive modeling.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The memory-driven approach of SAM 2 for video segmentation is both inspiring and intuitive for our medical image segmentation task. Its ability to recall and integrate relevant past information naturally adapts to few-shot segmentation scenarios. Instead of integrating information from previous frames, we can retrieve the contextual and anatomical information from similar cases to guide the segmentation process. This mirrors how humans learn to perform segmentation tasks: even without prior knowledge, by observing and understanding a few examples, they can quickly grasp the task and execute it accurately. Our framework builds on these intuitions; in <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS2" title="III-B Retrieval Module ‣ III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, we detail the retrieval module that retrieval the similar cases for supporting the segmenation process, and in <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS3" title="III-C Few-shot Segmentation Framework ‣ III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, we introduce our few-shot segmentation network which is adapted from the SAM 2 model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Retrieval Module</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The Retrieval Module is a critical component of our segmentation framework, designed to augment segmentation by efficiently retrieving relevant annotated examples from a database built from limited samples. The module begins by utilizing DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib20" title="">20</a>]</cite> to extract high-dimensional embeddings from a set of annotated images. These embeddings capture the semantic features of the images and are then indexed using FAISS (Facebook AI Similarity Search) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib34" title="">34</a>]</cite> (<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.F2" title="In III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>(b)) which is optimized for efficient similarity search of dense vectors.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Once the FAISS index of annotated images is constructed, the Retrieval Module leverages DINOv2-generated embeddings of the input image to query the index and identify the most similar images. These queried images, along with their associated segmentation masks, are then passed to the next stage of the pipeline, where they are encoded and stored in a memory bank. This process ensures that the model can access relevant contextual and anatomical information during segmentation, guiding the model to make more accurate and informed segmentation decisions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Few-shot Segmentation Framework</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our few-shot segmentation framework, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.F2" title="In III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> (a), is designed to perform medical image segmentation with limited annotated data by leveraging a retrieval-augmented system. This framework is adapted from the Segment Anything Model 2 (SAM 2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib17" title="">17</a>]</cite> and operates without requiring additional training, making it highly adaptable and efficient for various medical imaging tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Given an input image, it is first processed through DINOv2 to generate embedding that capture its semantic information. This embedding is then fed into the Retrieval Module, as discussed in <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S3.SS2" title="III-B Retrieval Module ‣ III Methods ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, to retrieve the most similar images from a pre-built FAISS index. The retrieved images are passed through a pre-trained hierarchical image encoder to obtain the image embeddings, specifically a MAE-pretrained Hiera model which is designed to handle multiscale features for robust image representation. This hierarchical structure allows the framework to extract detailed and context-rich embeddings at multiple scales, which is crucial for the segmentation task. Once the image embeddings of the retrieved images are obtained, they are input into the memory encoder along with their associated segmentation masks. The memory encoder downsamples the segmentation masks and fuses them with the image embeddings obtained from the image encoder, generating compact and efficient memory representations. These encoded memories are then stored in a memory bank for the memory attention operation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">During the segmentation process, the input image is processed through the image encoder to obtain its embeddings. The model then utilizes the memory attention mechanism from SAM 2, which dynamically integrates information from the memory bank. This memory attention module consists of stacked transformer blocks that first apply self-attention to the current image features, followed by cross-attention to the stored memories. The enriched features generated by this process are then fed into the mask decoder. The mask decoder in SAM 2 is similar to the one in the original SAM, but with a key enhancement: skip connections from the hierarchical image encoder bypass the memory attention module. These skip connections incorporate high-resolution information directly into the mask decoding process, enhancing the accuracy and detail of the final segmentation output. This use of skip connections is particularly important in medical image segmentation, as it mirrors the architecture of U-Net, which is well-suited for capturing fine-grained details in medical images. More details refer to  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">It is important to note that our framework directly adapts the image encoder, memory encoder, memory attention, and mask decoder from SAM 2 without requiring any additional retraining or pretraining. During segmentation, our framework operates without the need for external prompts for the mask decoder. Instead, the segmentation is guided by the rich contextual information stored in the memory bank, which includes encoded features from similar images. This memory-driven approach allows the framework to accurately segment target structures by leveraging past experiences, making it highly efficient and effective even with limited annotated data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Datasets</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We employ 3 different medical image segmentation datasets to validate the proposed method.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>ACDC Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Automated Cardiac Diagnosis Challenge (ACDC) dataset is a benchmark in cardiac MRI segmentation, containing images from 100 patients across five clinical conditions. It includes 2D short-axis cine-MRI sequences at end-diastolic (ED) and end-systolic (ES) phases, with manual segmentations of the left ventricle (LV), myocardium (Myo), and right ventricle(RV). To construct the image retrieval database, we randomly selected 3 subjects from the 100 patients and extracted a total of 50 axial slices from both the ED and ES phases. The same procedure was applied to create the test dataset by randomly selecting another 3 subjects. For comparison, the supervised training on the ACDC dataset uses the same test dataset but includes a training set with 1808 axial slices from 70 subjects.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>CMR T1-Map Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The CMR T1-Map dataset is dedicated to segmenting the myocardium (Myo) in T1 mapping CMR images, acquired under varying scanning protocols. To create a representative dataset, we randomly selected 3 subjects from a pool of 163 patients, extracting a total of 50 axial slices for training. Similarly, an additional 3 subjects with 50 slices were selected for the test dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.4.1.1">IV-A</span>3 </span>Fluoroscopy Image Dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">The Fluoroscopy Image dataset is designed for the segmentation of the left coronary artery (LCA). It includes data from 9 subjects with annotations, with 5 subjects used to construct the retrieval database and 4 subjects designated for the test dataset. For each subject, we selected 10 consecutive frames from two different positional angles, resulting in a total of 20 frames per subject. This setup yields 100 frames for the retrieval database and 80 frames for the test dataset.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The DINOv2 model we utilize for produce embeddings for retrieval is DINOv2 ViT-14-small with registers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib35" title="">35</a>]</cite>, which yield a 384-dimensional vector for each image. For the SAM 2 model, we utilize the one build on Hiera-large <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib36" title="">36</a>]</cite>. The retrieval module is implemented with Faiss library (<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ai.meta.com/tools/faiss/</span>) with squared Euclidean (L2) distance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib34" title="">34</a>]</cite>. In addition, we normalize the DINOv2 embeddings both when constructing the index and during retrieval.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Segmentation Results on Three Datasets</span>
</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Segmentation performance comparison using Dice similarity coefficient (DSC) on ACDC, CMR T1-MAP, and Fluoroscopy Image datasets. Abbreviations: RV (right ventricle), Myo (myocardium), LV (left ventricle).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T1.1.1.1.2">ACDC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3">CMR T1-MAP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.4.1">Fluoroscopy Image</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.1">RV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.2">Myo</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.3">LV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.2.2.4">Myo</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.3.1.1">SAM 2 (1 Pos Point)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.2">0.4146</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.3">0.4565</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.4">0.6612</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.5">0.4483</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1.6">0.3761</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.2.1">SAM 2 (2 Pos Points)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.2">0.4268</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.3">0.4604</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.4">0.6610</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.5">0.4436</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2.6">0.3849</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.3.1">SAM 2 (1 Pos&amp;Neg Points)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.2">0.4441</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.3">0.4794</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.4">0.6952</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.5">0.4215</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3.6">0.3844</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.4.1">SAM 2 (2 Pos&amp;Neg Points)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.2">0.4133</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.3">0.5004</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.4">0.6801</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.5">0.4357</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4.6">0.4858</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.7.5.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.5.2">0.6729</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.5.3">0.7757</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.5.4">0.8472</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.5.5">0.8238</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.7.5.6">0.8431</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this subsection, we present a comparative analysis of our model and the SAM 2 model using different point prompts. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.T1" title="In IV-C Segmentation Results on Three Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a> demonstrate the segmentation performance across three datasets: ACDC, CMR T1-MAP, and Fluoroscopy Image, evaluated using the Dice similarity coefficient (DSC). Our approach consistently outperforms the SAM 2 model across all datasets and point prompt configurations. For the ACDC dataset, our method achieves DSC scores of 0.6729, 0.7757, and 0.8472 for the right ventricle (RV), myocardium (Myo), and left ventricle (LV), respectively, demonstrating a remarkble improvement, particularly in the Myo and LV classes. In the CMR T1-MAP and Fluoroscopy Image datasets, our model also shows superior performance with DSC scores of 0.8238 and 0.8431, respectively. This consistent improvements in segmentation accuracy suggest the robustness and effectiveness of our method for few-shot medical image segmentation, especially in cases where the SAM 2 model with prompts falls short.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S4.F3.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Qualitative comparison of segmentation performance on ACDC, CMR T1-MAP, and Fluoroscopy Image datasets from a randomly selected sample for each dataset, respectively.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.F3" title="In IV-C Segmentation Results on Three Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> presents a qualitative comparison of segmentation performance across three datasets. In the ACDC dataset, our method closely matches the ground truth, accurately segmenting both the myocardium and ventricles. In contrast, the SAM 2 model with point prompts tends to over-segment the RV, Myo, and LV, particularly the ring-shaped myocardium. Similarly, in the CMR T1-MAP dataset, our method delivers more precise segmentation, while the SAM 2 variants often over-segment the myocardial region, incorrectly including parts of the LV and RV. For the Fluoroscopy Image dataset, our method achieves more refined and continuous vessel segmentation, whereas the SAM 2 model produces incomplete and less accurate results, missing several main and side branches. Overall, this figure clearly demonstrates the superiority of our method in achieving more accurate and reliable segmentations across diverse medical imaging modalities.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Comparison with Supervised Methods</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this subsection, we compare the segmentation performance of our method with fully-supervised methods on the ACDC dataset, as presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.T2" title="In IV-D Comparison with Supervised Methods ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a>. The table highlights the results of traditional fully-supervised models like U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib15" title="">15</a>]</cite> and SwinUNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#bib.bib37" title="">37</a>]</cite> under both full and limited data settings. When trained with the full dataset of over 1800 samples, U-Net and SwinUNETR achieve high dice coefficients. However, in the limited sample setting with only 50 samples, the performance of these fully-supervised models drops significantly, with U-Net and SwinUNETR showing markedly lower DSCs, especially for the right ventricle (RV) and myocardium (Myo).</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of segmentation performance on the ACDC dataset between fully-supervised methods and our approach.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.1.1.1.1">Setting</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T2.1.1.1.2.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.1.3">ACDC</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.1">RV</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.2">Myo</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2.3">LV</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.3.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.3.3.1.1">Full Data (1808 slices)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.2">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.3">0.8743</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.4">0.8785</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3.5">0.9473</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.1">SwinUNETR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.2">0.7623</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.3">0.8346</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4.4">0.9198</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.1.5.5.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.5.5.1.1">Limited Data (50 slices)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.5.2">U-Net</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.5.3">0.1996</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.5.4">0.4794</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.5.5">0.5938</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.1">SwinUNETR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.2">0.2792</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3">0.4202</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.4">0.4542</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.7.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.2.1">0.6729</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.3.1">0.7757</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.4.1">0.8472</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">In contrast, our method, which leverages foundation models to effectively utilize limited data, achieves much higher dice coefficients
than supervised model. Remarkably, even when compared to the supervised methods trained on the full dataset, our method’s performance is only about 0.1 DSC behind, suggesting its robustness and adaptability in scenarios with limited annotated data. This close performance gap demonstrates the effectiveness of our approach, particularly in medical image segmentation where acquiring large annotated datasets can be challenging.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.4.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.5.2">Ablation Studies</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In this subsection, we conduct ablation studies to evaluate the impact of different retrieval strategies and the number of queried images on segmentation performance.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.T3" title="In IV-E Ablation Studies ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a> compares the segmentation performance on the ACDC dataset using two retrieval strategies: Random and DINOv2, with two different numbers of queried images, #8 and #16.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of segmentation performance on the ACDC dataset using different retrieval strategies (Random vs. DINOv2) and varying numbers of queried images (#8 vs. #16).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.1.1">No. Queried Imgs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.2.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.1.3">ACDC</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.1">RV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2">Myo</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.3">LV</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.3.1.1.1">#8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.3">0.5801</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.4">0.7046</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.5">0.7796</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.1">DINOv2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2">0.6529</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.3">0.7538</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.4">0.8183</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.5.3.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.5.3.1.1">#16</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.2">Random</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.3">0.6684</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.4">0.7774</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.5">0.8402</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.1">DINOv2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.2">0.6729</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.3">0.7757</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.4">0.8472</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">In the case of retrieving 8 images (#8), the DINOv2-based retrieval significantly outperforms the Random retrieval across all three classes. This indicates that the semantic information provided by the DINOv2 model is effective in selecting more relevant and similar support images, leading to better segmentation results. When the number of queried images is increased to 16 (#16), both methods show improved performance, but the gap between the Random and DINOv2 methods narrows. This convergence can be attributed to the limited database size of 50 images. As the number of queried images increases, even randomly selected samples are likely to include some relevant examples, reducing the advantage of the DINOv2-based retrieval.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="481" id="S4.F4.1.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Segmentation performance on the ACDC dataset with different numbers of queried images, measured by Dice similarity coefficient (DSC) for the right ventricle (RV), myocardium (Myo), and left ventricle (LV).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">We also explore the impact of the number of queried images on the segmentation performance for the ACDC dataset. The results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.F4" title="In IV-E Ablation Studies ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. It is observed that as the number of queried images increases from 2 to 32, there is a consistent improvement in segmentation performance across all three classes. Initially, with only 2 queried images, the DSCs are relatively low for all classes, particularly for the RV. However, as the number of queried images increases to 4 and 8, there is a notable improvement in DSCs, with the performance curve showing a steep ascent, highlighting the importance of having a sufficient number of relevant support images to enhance segmentation accuracy. Beyond 8 queried images, the rate of improvement starts to plateau. This trend indicates that while increasing the number of queried images contributes to better segmentation performance, there is a diminishing return after a certain point, where additional images provide marginal gains.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Annotation Tools</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Our approach shows significant promise as an annotation tool for medical image segmentation, particularly in real-world practices where segmentation is often performed slice by slice. Traditional supervised methods often falter with limited annotated samples, making them less effective in such scenarios. However, our model can accurately segment challenging structures, like vessels which is time-consuming for human annotators, with limited data. Given just a few dozen samples, our model can quickly segment hundreds or thousands of images, producing the labels that can be used to train more robust supervised models. This efficiency can significantly accelerate the annotation process and enhance the overall quality of medical image segmentation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Our model’s adaptability across different imaging modalities is a significant advantage, especially given the wide variation in medical images, such as differences in modality and contrast. Traditional models often struggle with this diversity, requiring extensive retraining for each new scenario. In contrast, our approach easily adapts to different imaging modalities by simply providing sample images and corresponding masks. This capability allows our model to effectively handle the heterogeneity of medical images, making it a versatile annotation tool for medical image segmentation across various clinical applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Limitations</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S5.F5" title="In V-B Limitations ‣ V Discussion ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> illustrates the segmentation performance of our method on challenging basal and apical slices from the ACDC dataset. Despite our method’s overall effectiveness demonstrated in previous sections, the segmentation results for these particular slices are suboptimal. This is a common challenge even for supervised methods, as the basal and apical slices often present more anatomical variations and smaller regions of interest, making accurate segmentation difficult. The queried images visualized indicate that the retrieved samples, while semantically similar, may not provide adequate support due to the small size of the target regions and the inherent heterogeneity in medical images. This suggests that the retrieval process, which uses whole image features, might not always capture the fine details needed for precise segmentation of small objects. Furthermore, the limited size and variability of the database mean that sufficiently similar images to support accurate segmentation might not be available. These factors underscore the ongoing challenges in medical image segmentation, particularly for small and variable anatomical structures.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S5.F5.1.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Segmentation results on challenging basal and apical slices from the ACDC dataset, with corresponding top three similar queried images retrieved using the DINOv2.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For the vessel segmentation task, we also observed that the model struggles when the image contrast is low, leading to suboptimal performance. This limitation stems from the inherent capabilities of the SAM 2 model, which our method builds upon. SAM 2 tends to underperform when the contrast between foreground/background is insufficient, especially for segmenting small branches in our cases (<a class="ltx_ref" href="https://arxiv.org/html/2408.08813v1#S4.F3" title="In IV-C Segmentation Results on Three Datasets ‣ IV Experiments ‣ Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>). These low-contrast vessels are particularly challenging to delineate accurately. One potential solution to this problem could be fine-tuning SAM 2 on a large-scale medical dataset, which might enhance its ability to handle low-contrast regions and capture the fine details necessary for accurate segmentation in these challenging scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced a retrieval-augmented few-shot segmentation framework for medical image segmentation, leveraging the advanced capabilities of foundation models like SAM 2 and DINOv2. Our approach consistently outperforms SAM 2 with point prompts, demonstrating improvements in segmentation accuracy and efficiency. Additionally, it shows remarkable adaptability across various medical imaging modalities, effectively handling the heterogeneity of medical data without the need for further retraining or finetuning. This method not only serves as a powerful tool for segmentation tasks but also holds great promise as a valuable annotation tool in clinical applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Rogowska, “Overview and fundamentals of medical image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Handbook of medical imaging, processing and analysis</em>, pp. 69–85, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
N. Siddique, S. Paheding, C. P. Elkin, and V. Devabhaktuni, “U-net and its variants for medical image segmentation: A review of theory and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE access</em>, vol. 9, pp. 82 031–82 057, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W. Chi, L. Ma, J. Wu, M. Chen, W. Lu, and X. Gu, “Deep learning-based medical image segmentation with limited labels,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Physics in Medicine &amp; Biology</em>, vol. 65, no. 23, p. 235001, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Cui, D. Wei, K. Ma, S. Gu, and Y. Zheng, “A unified framework for generalized low-shot medical image segmentation with scarce data,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Medical Imaging</em>, vol. 40, no. 10, pp. 2656–2671, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Peng and Y. Wang, “Medical image segmentation with limited supervision: a review of deep network models,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Access</em>, vol. 9, pp. 36 827–36 851, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. Wang, J. H. Liew, Y. Zou, D. Zhou, and J. Feng, “Panet: Few-shot image semantic segmentation with prototype alignment,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 9197–9206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Ouyang, C. Biffi, C. Chen, T. Kart, H. Qiu, and D. Rueckert, “Self-supervision with superpixels: Training few-shot medical image segmentation without annotation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIX 16</em>.   Springer, 2020, pp. 762–780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Hansen, S. Gautam, R. Jenssen, and M. Kampffmeyer, “Anomaly detection-inspired few-shot medical image segmentation through self-supervision with supervoxels,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Medical Image Analysis</em>, vol. 78, p. 102385, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Q. Shen, Y. Li, J. Jin, and B. Liu, “Q-net: Query-informed few-shot medical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of SAI Intelligent Systems Conference</em>.   Springer, 2023, pp. 610–628.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Lin, Y. Chen, K.-T. Cheng, and H. Chen, “Few shot medical image segmentation with cross attention transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>.   Springer, 2023, pp. 233–243.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Ma, Y. He, F. Li, L. Han, C. You, and B. Wang, “Segment anything in medical images,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Nature Communications</em>, vol. 15, no. 1, p. 654, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Wu, W. Ji, Y. Liu, H. Fu, M. Xu, Y. Xu, and Y. Jin, “Medical sam adapter: Adapting segment anything model for medical image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2304.12620</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W. Lei, X. Wei, X. Zhang, K. Li, and S. Zhang, “Medlsam: Localize and segment anything model for 3d medical images,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2306.14752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
N.-T. Bui, D.-H. Hoang, M.-T. Tran, and N. Le, “Sam3d: Segment anything model in volumetric medical images,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2309.03493</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, “nnu-net: a self-configuring method for deep learning-based biomedical image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Nature methods</em>, vol. 18, no. 2, pp. 203–211, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">et al.</em>, “Sam 2: Segment anything in images and videos,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">arXiv preprint arXiv:2408.00714</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">et al.</em>, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, “From local to global: A graph rag approach to query-focused summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2404.16130</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et al.</em>, “Dinov2: Learning robust visual features without supervision,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">arXiv preprint arXiv:2304.07193</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et al.</em>, “Gpt-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “The llama 3 herd of models,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, “Emerging properties in self-supervised vision transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et al.</em>, “Learning transferable visual models from natural language supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">International conference on machine learning</em>.   PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in neural information processing systems</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
N. Tumanyan, A. Singer, S. Bagon, and T. Dekel, “Dino-tracker: Taming dino for self-supervised point tracking in a single video,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.14548</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Jiang, A. Karpur, B. Cao, Q. Huang, and A. Araujo, “Omniglue: Generalizable feature matching with foundation model guidance,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 19 865–19 875.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et al.</em>, “Segment anything,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 4015–4026.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. A. Mazurowski, H. Dong, H. Gu, J. Yang, N. Konz, and Y. Zhang, “Segment anything model for medical image analysis: an experimental study,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Medical Image Analysis</em>, vol. 89, p. 102918, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Blattmann, R. Rombach, K. Oktay, J. Müller, and B. Ommer, “Retrieval-augmented diffusion models,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 15 309–15 324, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Long, W. Yin, T. Ajanthan, V. Nguyen, P. Purkait, R. Garg, A. Blair, C. Shen, and A. van den Hengel, “Retrieval augmented classification for long-tail visual recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 6959–6969.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Liu, K. Son, J. Yang, C. Liu, J. Gao, Y. J. Lee, and C. Li, “Learning customized visual models with retrieval-augmented knowledge,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 15 148–15 158.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazaré, M. Lomeli, L. Hosseini, and H. Jégou, “The faiss library,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski, “Vision transformers need registers,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
C. Ryali, Y.-T. Hu, D. Bolya, C. Wei, H. Fan, P.-Y. Huang, V. Aggarwal, A. Chowdhury, O. Poursaeed, J. Hoffman <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Hiera: A hierarchical vision transformer without the bells-and-whistles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 29 441–29 454.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. R. Roth, and D. Xu, “Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">International MICCAI brainlesion workshop</em>.   Springer, 2021, pp. 272–284.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 16 15:44:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
