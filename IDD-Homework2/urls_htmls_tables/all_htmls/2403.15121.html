<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.15121] SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation</title><meta property="og:description" content="Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders that have a significant impact on individuals and society. Early identification of risk markers for these diseases is crucial for understanding t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.15121">

<!--Generated on Fri Apr  5 15:29:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vladyslav Zalevskyi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Kristoffer Hougaard Madsen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Danish Research Centre for Magnetic Resonance (DRCMR), Hvidovre, Denmark
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders that have a significant impact on individuals and society. Early identification of risk markers for these diseases is crucial for understanding their progression and enabling preventive interventions. The Danish High Risk and Resilience Study (VIA) is a longitudinal cohort study that aims to gain insights into the early disease processes of SZ and BD, particularly in children with familial high risk (FHR). Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions. The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing. Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group. However, CS segmentation presents challenges due to its high morphological variability and complex shape, which are especially apparent in the adolescent cohort. This study explores two novel approaches for training robust and adaptable CS segmentation models that address these challenges. Firstly, we utilize synthetic data generation to model the morphological variability of the CS, adapting SynthSeg’s generative model to our problem. Secondly, we employ self-supervised pre-training and multi-task learning to adjust the segmentation models to new subject cohorts by learning relevant feature representations of the cortex shape. These approaches aim to overcome limited data availability and enable reliable CS segmentation performance on diverse populations, removing the need for extensive and error-prone post- and pre-processing steps. By leveraging synthetic data and self-supervised learning, this research demonstrates how recent advancements in training robust and generalizable deep learning models can help overcome problems hindering the deployment of DL medical imaging solutions. Although our evaluation showed only a moderate improvement in performance metrics, we emphasize the significant potential of the methods explored to advance CS segmentation and their importance in facilitating early detection and intervention strategies for SZ and BD.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
segmentation <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>central sulcus <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>synthetic data <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>SynthSeg <span id="id5.id4" class="ltx_ERROR undefined">\sep</span>self-supervised training <span id="id6.id5" class="ltx_ERROR undefined">\sep</span>SimCLR <span id="id7.id6" class="ltx_ERROR undefined">\sep</span>U-Net <span id="id8.id7" class="ltx_ERROR undefined">\sep</span>multi-task learning

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>MAIA Master</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Background</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders that impact approximately 0.7% and 1.0% of the population respectively <cite class="ltx_cite ltx_citemacro_citep">(Robinson and Bergen, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>. These conditions impose a significant burden on both individuals and society, resulting in substantial economic, mental, and societal costs <cite class="ltx_cite ltx_citemacro_citep">(Millier et al., <a href="#bib.bib40" title="" class="ltx_ref">2014</a>; Ferrari et al., <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>. SZ and BD are believed to be neurodevelopmental disorders influenced by both genetic and environmental factors <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib52" title="" class="ltx_ref">2015</a>)</cite>. Identifying early risk markers for these diseases can enhance our understanding of their progression and lay the groundwork for primary preventive interventions.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">SZ and BD typically manifest in late teenage years or early 20s, while children at familial high risk may exhibit symptoms even earlier, often before the age of 12 <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib52" title="" class="ltx_ref">2015</a>; Robinson and Bergen, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>)</cite>. Having a family history of BD or SZ is the strongest risk factor for developing these disorders and, according to a meta-analysis, approximately 55% of children at familial high risk will encounter mental illness in early adulthood, with around one-third experiencing severe mental illness (SMI) <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">The Danish High Risk and Resilience Study (VIA) is a longitudinal cohort study of 520 7-year-old children born to parents with schizophrenia, bipolar disorder, or no mental disorders <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib52" title="" class="ltx_ref">2015</a>)</cite>. Its main objectives are to gain insights into the early disease processes of schizophrenia and bipolar disorder, investigate the developmental trajectory of children with familial high risk across various domains (neurocognition, psychopathology, social cognition, motor function) and examine the influence of genetic and environmental factors on the progression of these disorders. The study seeks to explore symptom formation, cognitive impairments, differences in brain structure and activation patterns <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">According to the VIA7 study results, children born to parents diagnosed with SZ and BD already demonstrate higher rates of psychiatric diagnosis, cognitive deficits (particularly in FHR SZ), and motor difficulties by age 7 <cite class="ltx_cite ltx_citemacro_citep">(Burton et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>. When compared to controls, children with FHR of SZ show persistent developmental deficits in manual dexterity and balance. While no observable motor development differences are found among children with FHR of BD as a group, children with definite motor problems across all groups had a higher likelihood of experiencing psychosis, suggesting a connection between childhood motor impairment and neurodevelopmental susceptibility to psychosis <cite class="ltx_cite ltx_citemacro_citep">(Burton et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. Studying structural brain changes related to these impairments during early disease formation could provide critical information on differences in neurodevelopment between individuals with and without familial risk as well as their causes.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">The central sulcus (CS) is an important landmark for examining structural brain differences in individuals with motor and sensory deficits. It is a prominent anatomical feature of the brain that separates the frontal lobe from the parietal lobe and is symmetrically located in both hemispheres of the brain. It is one of the major sulci (grooves) found in the cerebral cortex. Research has shown that alterations in the shape and size of the central sulcus, which separates the primary motor and somatosensory areas, can impact fine motor control and sensory processing in individuals <cite class="ltx_cite ltx_citemacro_citep">(Jensen, <a href="#bib.bib27" title="" class="ltx_ref">2016</a>)</cite>. Therefore, analysis of the shape and morphology of the CS can contribute to a better understanding of the observed neurodevelopmental abnormalities in the FHR group.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">The first step in CS analysis is its detection and segmentation, commonly based on structural magnetic resonance (MR) images. Although the central sulcus is one of the most stable and prominent folds of the human brain, its size and shape vary substantially across individuals and between hemispheres <cite class="ltx_cite ltx_citemacro_citep">(Caulo et al., <a href="#bib.bib11" title="" class="ltx_ref">2007</a>)</cite>. For example, one of the most prominent sections of the central sulcus is the so-called hand knob region, which has significant anatomical variations illustrated in Figure <a href="#S1.F1.1" title="Figure 1 ‣ 1.1 Background ‣ 1 Introduction ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.15121/assets/figures/hand_knob_variability.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic representation of the different morphological variants of the hand motor cortex observed in humans. Omega, medially asymmetric epsilon, laterally asymmetric epsilon, and null variants were observed in 88.3%, 2.9%, 7.0%, and 1.8% of the hemispheres, respectively with statistically significant sex differences. The epsilon variant was twice as frequent in men, and an interhemispheric concordance for morphologic variants was observed only for women. Courtesy of <cite class="ltx_cite ltx_citemacro_citep">(Caulo et al., <a href="#bib.bib11" title="" class="ltx_ref">2007</a>)</cite>.</figcaption>
</figure>
<div id="S1.SS1.p7" class="ltx_para">
<p id="S1.SS1.p7.1" class="ltx_p">Furthermore, the CS morphology depends highly on the gyrification of the cortex, which measures the degree of cortical folding <cite class="ltx_cite ltx_citemacro_citep">(White et al., <a href="#bib.bib54" title="" class="ltx_ref">2010</a>)</cite> . Increased gyrification characterized by numerous and complex gyri and sulci may lead to more intricate and convoluted sulci patterns with more twists and turns while decreased gyrification, observed with ageing may result in shallower and less complex gyri and wider sulci <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>)</cite>. This decrease in gyrification is caused by systematic cortical thinning during normal ageing and is related to neuronal pruning, life-long reshaping and neurodegenerative processes <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>)</cite>. This in fact means that the intricate pattern of gyri and sulci will vary considerably among different age groups, particularly in children and adults as we know that the peak of gyrification happens in early childhood after which it steadily decreases over time <cite class="ltx_cite ltx_citemacro_citep">(Klein et al., <a href="#bib.bib29" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Project proposal</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Segmenting the central sulcus poses a significant challenge due to its intrinsically high morphological variability, which is further influenced by the gyrification changes that occur with age. Successfully addressing these challenges requires sophisticated models and, crucially, large and diverse datasets that encompass the full range of CS morphological variations. Unfortunately, the only currently available dataset with manual sulci segmentations, to the best of our knowledge, is limited in terms of subject count and represents a specific cohort, making robust and precise CS segmentation on diverse populations a difficult task <cite class="ltx_cite ltx_citemacro_citep">(Brainvisa, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">In light of these challenges, the primary objective of this research is to develop and investigate approaches for constructing robust and adaptable CS segmentation models. Our experiments aim to address the issue of limited data availability and provide pipelines that can train CS segmentation models that demonstrate reliable performance on unseen and diverse populations of subjects. To achieve them, we investigate two novel ideas in the field of CS segmentation, namely how synthetic data generation can be used to model morphological variability of the CS while self-supervised pre-training and multi-task learning can be utilized for adjusting the model to new subject cohorts by learning pertinent feature representations of the cortex shape.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>State of the art</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Since the development of high-resolution brain MR imaging, the recognition of cerebral sulci and their morphology analysis has been of significant interest to researchers studying structural abnormality patterns related to the diseases affecting the neocortex <cite class="ltx_cite ltx_citemacro_citep">(Mangin et al., <a href="#bib.bib38" title="" class="ltx_ref">1995</a>; Huntgeburth and Petrides, <a href="#bib.bib24" title="" class="ltx_ref">2012</a>)</cite>. This led to the development of several classes of approaches for automatic sulci detection.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The first type of approaches relies on feature-based elastic registration of a labelled template atlas with segmented sulci to the subject’s imaging data. This method propagates labels and identifies anatomical structures of interest by matching surface features between the subject and the pre-labelled template <cite class="ltx_cite ltx_citemacro_citep">(Desikan et al., <a href="#bib.bib17" title="" class="ltx_ref">2006</a>; Behnke et al., <a href="#bib.bib2" title="" class="ltx_ref">2003</a>)</cite>. While these approaches have been successful in identifying some major sulci, the high inter-subject variability of the cortical folding patterns makes it challenging to achieve an exact match between a subject and a template. Moreover, the existence of such a match is uncertain which further complicates the use of these methods for a precise sulci shape analysis. <cite class="ltx_cite ltx_citemacro_citep">(Yang and Kruggel, <a href="#bib.bib56" title="" class="ltx_ref">2008</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Another set of approaches explored by <cite class="ltx_cite ltx_citemacro_citet">Kao et al. (<a href="#bib.bib28" title="" class="ltx_ref">2007</a>); Vivodtzev et al. (<a href="#bib.bib53" title="" class="ltx_ref">2003</a>); Shi et al. (<a href="#bib.bib50" title="" class="ltx_ref">2007</a>)</cite> consider curvature and geodesic depth properties of the cerebral folds. They use depth thresholding and deformable models to differentiate sulci and gyri using cortical surface meshes created from 3D MR images, relying on the assumption that sulci are concave and gyri are convex. However, these approaches highly depend on the ad-hoc handcrafted rules, thresholds and parameters describing the elasticity of the deformable model or depth and curvature thresholds as well as the quality of meshing, which can limit their generalizability and performance.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Recent advancements in image processing, computational methods, and deep learning approaches have led to substantial progress in automatic cortical sulci segmentation <cite class="ltx_cite ltx_citemacro_citep">(Borne et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. These advancements increased the accuracy of segmentation as well as expanded on the types and variety of supported sulci, enabling more precise investigations into complex folding patterns and their relationship with brain structure and function <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. In this section, we provide an overview of recent developments in the field, which encompass the most popular pipelines for automatic sulci segmentation and outline the motivations behind the methods explored in this study.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Spherical CNNs</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the past decade, deep learning models have gained significant traction in biomedical research due to their exceptional ability for feature extraction and outstanding performance <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>. While there have been previous efforts to apply traditional convolutional neural networks (CNNs) to segment the sulci, such as demonstrated in <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, the unique characteristics of the convoluted cerebral cortex have led to the proposal to use a spherical variant of CNNs <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Standard 2D or 3D CNN architectures are ill-suited for handling the curved geometry of the convoluted cerebral cortex. Most CNN models are designed to optimally work in Euclidean image grids, which restricts their ability to effectively encode cortical surface data. Due to the intricate shape and high curvature of the cortex, it is possible for two points situated on the cortex to have a small Euclidean distance. However, in terms of the manifold distance through the cerebral cortex, these points could be significantly far apart representing distinct and separate regions of the brain. The complex geometry of the cortex introduces a non-linear mapping between Euclidean and manifold distances, meaning that proximity in Euclidean space does not necessarily imply proximity on the cortical surface.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">These limitations have prompted the increasing popularity of spherical CNNs as they offer a more suitable framework for processing and analyzing the cortical surface <cite class="ltx_cite ltx_citemacro_citep">(Willbrand et al., <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite> . However, for them to work, the cortical surface first needs to be represented as a 2D spherical manifold. This process typically involves segmenting the white matter (WM) and grey matter (GM) tissues based on structural brain images, constructing a cerebral cortex surface mesh through tesselation, and applying post-processing steps to address topological inconsistencies, holes, gaps, and optimize surface geometry <cite class="ltx_cite ltx_citemacro_citep">(McConnell, <a href="#bib.bib39" title="" class="ltx_ref">1995</a>)</cite>. Finally, the surface mesh is inflated while preserving its metric properties, resulting in an expanded, spherical representation of the cortex <cite class="ltx_cite ltx_citemacro_citep">(Fischl et al., <a href="#bib.bib20" title="" class="ltx_ref">1999</a>)</cite>.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lyu et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> further improves the performance of spherical CNNs in sulci segmentation tasks by applying surface data augmentation and context-aware training in a pipeline schematically depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Spherical CNNs ‣ 2 State of the art ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Given the small size of the dataset used (60 and 36 in two explored cohorts), the authors emphasized a crucial need for data augmentation. However, the augmentation approaches for spherical surfaces have not been extensively explored compared to regular 2D/3D data. The authors proposed a novel approach that utilizes surface registration to augment training samples. The augmentations are achieved by applying spherical harmonics to decompose the spherical deformation needed to register every training image to all others and reconstruct intermediate deformations by controlling the basis functions. By doing so, the suggested approach bridges the gap between moving and target samples in the feature space along their deformation trajectory. This method enhances the training data by generating additional variations that improve the performance of models trained on limited samples. In their context-aware learning phase, hierarchical training is employed. The model is first trained to recognize the deeper and more stable primary sulci, and then the predicted information about their location is used as an additional input channel to guide the segmentation of shallower and more variable tertiary sulci.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.15121/assets/figures/spherCNN_pipeline.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A schematic representation of the framework proposed by <cite class="ltx_cite ltx_citemacro_citet">Lyu et al. (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> for the training of spherical CNNs for sulci segmentation. Two main contributions are the data augmentation approach (blue box), which augments training samples by deforming them through surface registration to every possible pair of other training samples while reconstructing all intermediate deformations and using them as additional samples and the context-aware training method (green box) in which spatial information of primary/secondary sulci is extrapolated to guide the segmentation of smaller and shallower tertiary sulci. Courtesy of <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">While the use of spherical CNNs to capture cerebral surface topology is a promising idea, the numerous pre- and post-processing steps required to segment the tissues and generate cortical meshes and spherical surfaces present drawbacks. The performance of the separate models used in these steps can significantly impact the resulting surface representations of the cortex, leading to missed or wrongly detected sulci regions. The data augmentation technique proposed by the authors although presents a novel augmentation scheme for spherical data is nevertheless limited in its variability to sulci patterns presented in the training data. The limited amount of data augmentation techniques for spherical surfaces and the general lack of research in the field of spherical CNNs can impede the development of robust segmentation algorithms.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Brainvisa</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The BrainVISA software package is widely recognized and utilized in the literature for sulci segmentation <cite class="ltx_cite ltx_citemacro_citep">(Leroy et al., <a href="#bib.bib32" title="" class="ltx_ref">2015</a>; Ochiai et al., <a href="#bib.bib41" title="" class="ltx_ref">2004</a>; Roell et al., <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>; Perrot et al., <a href="#bib.bib43" title="" class="ltx_ref">2011</a>; Kochunov et al., <a href="#bib.bib31" title="" class="ltx_ref">2011</a>)</cite>. It offers the capability to segment more than 120 different sulci of the brain and compute morphological features based on the segmentation. In its latest version, as described in <cite class="ltx_cite ltx_citemacro_citet">Borne et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, BrainVISA introduces several approaches for sulci labelling, consolidating decades of research in developing automatic pipelines for sulci segmentation. Although these approaches follow different directions for segmentation, they all share the same data preparation steps and begin with sulci detection. BrainVISA’s pipeline encompasses multiple pre-processing steps and as shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2 Brainvisa ‣ 2 State of the art ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, as it starts from a high-quality structural T1-weighted image used to first detect and then label the sulci.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2403.15121/assets/figures/BvisaPipeline.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>BrainVISA pre-processing pipeline. (a) T1w structural image; (b) Skull stripping; (c) Hemisphere segmentation; (d) GM and WM segmentation; (e) CSF skeleton labelling; (f) Cerebral cortex surface reconstruction; (g) Sulci detection; (h) Sulci parcellation. Based on <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Pre-processing</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The pre-processing steps applied by BrainVISA aim to transform the structural MR image into a binary CSF skeleton image, where non-zero voxels define the skeleton of the CSF that corresponds to the detected sulci <cite class="ltx_cite ltx_citemacro_citep">(Borne et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. To achieve this, several key steps are carried out. The pre-processing pipeline starts with bias field correction to mitigate low-frequency intensity variations in the MRI image. Afterwards, brain and cerebellum identification is performed, followed by the removal of non-brain tissues using a technique based on 3D erosion and template-based 3D region growth. The cortical grey matter ribbon is then obtained after which spherical meshes for the pial and GM/WM interfaces are extracted. Next, based on curvature estimation a crevasse detector reconstructs sulcal structures as medial surfaces between the two opposing gyral banks spanning from the most internal point of the sulcal fold to the cortex’s convex hull. Following that, the skeleton of the CSF is fragmented into elementary folds, ensuring adherence to topological and geometric constraints specific to the sulci definition <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>. Finally, the CSF skeleton image is parcellated into distinct sulci using one of the following methods.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Multi-atlas parcellation</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Multi-atlas segmentation (MAS) methods, originally presented by <cite class="ltx_cite ltx_citemacro_citet">Rohlfing et al. (<a href="#bib.bib47" title="" class="ltx_ref">2004</a>)</cite>, leverage manually segmented images as atlases, wherein each atlas is adjusted to fit the image being segmented, and the best matches are selected to participate in the segmentation process. This approach enables a more accurate representation of anatomical variability by avoiding the use of an average template atlas to model the segmentation problem. Instead, MAS techniques incorporate atlases that better capture the inter-subject variability present in the data. It is worth noting, however, that the registration of atlases to the target images can be computationally demanding.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">In BrainVISA, the MAS technique involves creating patches extracted as cubical slices from the training images that encompass the elementary sulci detected in the preceding steps <cite class="ltx_cite ltx_citemacro_citep">(Borne et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. These patches are then registered to the target image, where the folds skeleton has been extracted, and the best matches are determined. The patch labels are subsequently propagated onto the target image, utilizing the distance between patches to perform a robust weighted average of the labels. Finally, the propagated labels are utilized to calculate the label score maps.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>CNN parcellation</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Similarly to approaches based on spherical CNNs, BrainVISA’s deep learning models do not rely on the original intensity image but instead utilize a pre-processed version of it. They employ a binary 3D image that represents the skeleton of CSF. BrainVISA authors experimented with a 3D U-Net convolutional neural network (CNN) based on the architecture proposed by <cite class="ltx_cite ltx_citemacro_cite">Çiçek et al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite>, examining both patch-based and whole-image-based models, concluding that the U-Net model processing the entire image outperformed the patch-based one. The superiority of the whole-image approach was attributed to its ability to capture comprehensive sulcal patterns more efficiently. This was achieved by having a larger field of view and the capability to observe the complete folding pattern of the brain, enabling a better understanding of the overall structure <cite class="ltx_cite ltx_citemacro_citep">(Borne et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">During the training process, the CSF skeleton image was used as input to the DL model, which was trained to produce a parcellation of the skeleton by assigning a specific sulci label to each non-zero voxel. During training, only the classification error of the voxels belonging to the skeleton contributed to the loss, based on the assumption that the sulci detection step was executed accurately. Such design choice reduced the complexity of the learning task, as the model solely had to learn the labelling of the skeleton voxels without considering the background. Moreover, due to the heavy reliance on pre-processed skeleton images, the researchers employed only a simple random rotation-based augmentation during training, since the binary nature of the images limited the application of complex data augmentation techniques.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Limitations of the current methods</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">While the methods utilizing spherical CNNs and BrainVISA pipelines for automated sulci segmentation demonstrate significant advancements in the field, they are not without their limitations.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">First of all, both of them have significant pre- and post-processing pipelines. The performance of individual models employed in them can substantially impact the resulting spherical surface or CSF skeleton cortex representations. Both of them heavily rely on the quality of the WM/GM/CSF segmentations that are used to build cortex meshes which could be a complicated task, especially in low-resolution images corrupted by artefacts, introducing potential inaccuracies or errors. Furthermore, in a population of children or adolescents, for example, higher cortical gyrification can lead to narrower sulci gaps which make proper differentiation between opposing gyral banks challenging due to the partial volume effect <cite class="ltx_cite ltx_citemacro_citep">(Kochunov et al., <a href="#bib.bib30" title="" class="ltx_ref">2005</a>)</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Secondly, both methods employ a narrow range of data augmentations, which has limited effectiveness in enhancing the diversity of cortex morphologies represented in the training set. These augmentations might fail to adequately simulate the variability of sulci that could be absent in the original data or image variation and bias induced by differences in acquisition schemes or scanners.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Finally, both approaches are trained and evaluated on small-scale in-house datasets consisting of only a few dozen images, typically representing a specific cohort. This limitation arises from the lack of comprehensive, diverse, and standardized datasets available for evaluating sulci segmentation techniques. Consequently, the generalizability and robustness of these approaches across different cohorts are called into question.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">These challenges and limitations motivate us to investigate alternative approaches in this study. Our focus is to develope models that could effectively handle variations in image quality and contrast, which would simplify the segmentation pipeline avoiding multiple pre-processing steps that can lead to the accumulation of errors. We are interested in exploring approaches that can efficiently utilise little available data as well as adapt the model to diverse cohorts with previously unseen sulcal patterns. In the subsequent section, we detail the specific methodologies employed to achieve these objectives.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Material and methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we discuss the datasets used for training and evaluation. It is important to note that the primary objective of this study is to investigate the training of robust and generalizable segmentation models. Therefore, we are exclusively using the BrainVISA dataset that has high-quality curated and labelled CS segmentations to train or fine-tune models for the CS segmentations task, while the VIA11 dataset is used solely for evaluation or self-supervised pre-training that assumes that the CS ground truths do not exist. Such a split allows us to assess the performance degradation of models trained on one dataset and evaluated on another, analysing how inherent disparities in population demographics and acquisition parameters between the datasets affect the model’s performance.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To ensure uniformity in the input data for the models, we apply the same pre-processing steps for both datasets, which include only skull-stripping and registration to the common MNI template <cite class="ltx_cite ltx_citemacro_citep">(Collins et al., <a href="#bib.bib16" title="" class="ltx_ref">1994</a>)</cite>. Furthermore, the images were cropped to content and resampled to a consistent resolution of 256x256x124 using the Python implementation of SimpleITK by <cite class="ltx_cite ltx_citemacro_citet">Yaniv et al. (<a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite>, thereby ensuring identical embedding dimensionality for VIA11 and BrainVISA images.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>BrainVISA</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Along with presenting the latest sulci segmentation approaches of BrainVISA, <cite class="ltx_cite ltx_citemacro_citet">Borne et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> have also released the dataset used to train them. Although it represents a significant contribution in terms of data availability, providing the first to our knowledge high-quality manually segmented dataset with multiple sulci labels for multiple subjects, it has strong limitations in terms of cohort representation.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">The dataset contains images from 62 healthy subjects selected from various databases. The subjects are predominantly right-handed men aged between 25 and 35 years. For each subject, a panel of experts produced segmentations for 63 sulci in the right hemisphere and 64 sulci in the left hemisphere through an iterative process involving consensus-based labelling, where agreement among all experts was required for the final segmentation. Although precise, such a labelling scheme excludes the possibility of estimating inter-rater reliability. The dataset includes skull-stripped T1-weighted images, CSF skeleton images, sulci segmentations and brain masks for each subject.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">The dataset was randomly split into the train (70%) and validation (30%) sets, enabling performance evaluation on the BrainVISA data as well. Only the training portion of the dataset was used for CS segmentation learning, synthetic data generation and self-supervised pre-training as described in the following sections.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>VIA11</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The VIA11 study is the second phase of the longitudinal VIA project, which focuses on assessing participants in their 11th year of life <cite class="ltx_cite ltx_citemacro_citep">(Thorup et al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>. In contrast to the initial examination conducted at age 7 (VIA7), VIA11 study protocol incorporates several neuroimaging techniques, with our analysis focusing on structural T1-weighted (MP2RAGE) images.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">For our study, we included 303 subjects who participated in the VIA11 study and had structural MR images of sufficient quality. The cohort’s average age is 12.1 ± 0.28 years. It has a balanced gender distribution (49% male, 51% female) and includes predominantly right-handed individuals (258 right-handed, 26 left-handed, 19 ambidextrous).</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Central sulcus labels for this dataset were obtained using a semi-automatic approach. First, the BrainVISA Morphologist pipeline <cite class="ltx_cite ltx_citemacro_citep">(Borne et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> was employed for the initial sulci segmentation of all subjects. Then, manual quality control was performed to estimate their correctness which resulted in 125 subjects having sufficiently good segmentations, 165 subjects having notable errors that would require manual correction of the BrainVISA segmentations, and 13 subjects having incorrect orientation or other errors that prevented manual quality control. In our work, we used 125 subjects’ images for which the initial automatic tissue and sulci segmentation procedures were deemed of sufficient quality to perform self-supervised learning as well as estimate the model’s performance and compare it among our approaches. The remaining 165 subjects, for which manually corrected segmentations were not available until the very end of the project were never used for any supervised or unsupervised training. These 165 images were only used as a hold-out test set for comparison between BrainVISA and our approaches. It is worth noticing nevertheless, that those manual corrections were performed based on the BrainVISA initial segmentations, and mostly consisted of removing/adding voxels to the BrainVISA’s output, which in fact means that these segmentations are highly biased towards BrainVISA’s output. We also note that the only pre-processing step applied to VIA11 images was skull-stripping and registration to the MNI space using the BrainVISA software, thereby replicating the same pipeline used to generate the BrainVISA dataset. This ensures uniform data representation for both the training and evaluation phases.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Methods</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Data augmentation has emerged as a popular technique for training deep learning models in scenarios with limited training data, particularly in the medical imaging domain <cite class="ltx_cite ltx_citemacro_citep">(Chlap et al., <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>. In this section, we explain our rationale for employing synthetic data generation based on the work by <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> and provide specific implementation details.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Moreover, to address the issue of limited and constrained diversity in the datasets, we investigate the use of a contrastive self-supervised framework, SimCLR, developed by <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, in conjunction with our synthetic data to learn cortex representation through self-supervised and multi-task training. We demonstrate how this approach can facilitate model adaptation to new datasets without labelled data, thereby aiding in performing segmentation tasks on dissimilar populations.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Given that our primary focus is exploring training and data generation techniques, we have opted to utilize a simple yet effective 3D CNN U-Net segmentation model designed by <cite class="ltx_cite ltx_citemacro_citet">Çiçek et al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite>. 3D U-Nets are among the most commonly employed architectures for 3D medical image segmentation, demonstrating effectiveness, relative computational efficiency, and robustness in the medical domain <cite class="ltx_cite ltx_citemacro_citep">(Hesamian et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>. U-Net models typically consist of symmetric encoder and decoder parts, that have skipped connections between them. The encoder part of the U-Net is responsible for extracting hierarchical and abstract feature representations from the input image, which is passed to the decoder responsible for upsampling the encoded feature maps to the original input image dimensions and generating the final segmentation map. Specifically, in all our experiments, we employed a 5-level 3D U-Net with an implementation from MONAI, <cite class="ltx_cite ltx_citemacro_citet">Cardoso et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, featuring 16, 32, 64, 128, and 256 channels per layer. This choice allowed us to work with a model of comparable size and complexity to that used by <cite class="ltx_cite ltx_citemacro_citet">Borne et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, while considering the limitations of our computational resources.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Synthetic data generation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">SynthSeg, introduced in the work by <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite>, is a segmentation model that leverages a generative approach to create synthetic images for network training. By dynamically generating training images with fully randomized parameters, the SynthSeg model learns contrast, intensity, scale, resolution, morphology, artefacts, and noise invariant features, leading to superior segmentation performance, particularly on low-quality images <cite class="ltx_cite ltx_citemacro_citep">(Billot et al., <a href="#bib.bib5" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Iglesias et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>. We adapted the SynthSeg’s data generator for our specific problem, utilizing its powerful generation capabilities to create a diverse image dataset based on the limited available labelled images.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2.1 Synthetic data generation ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the general pipeline used to create our synthetic dataset. It starts from a segmentation (containing labels of tissues to synthesize, such as WM, GM, CSF, skull bone, and fat) that is passed as input to the SynthSeg data generator. We obtain these segmentations for our datasets from two different sources.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2403.15121/assets/figures/SyntheticDataGeneration.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Synthetic Data Generation Pipeline. First, we create a segmentation map, that contains both the tissue and sulci labels. Then we pass it through the SynthSeg generative model, which applies a series of transformations to the segmentation and creates the artificial image by sampling tissue-specific intensity values based on the tissue priors. Finally, the output of the model is the synthetic image and transformed segmentations that contains sulci and tissue labels.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">For the VIA11 we utilize FreeSurfer’s Samseg tool <cite class="ltx_cite ltx_citemacro_citep">(Puonti et al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>)</cite> to obtain preliminary segmentations. These segmentations are then manually quality-checked by a skilled neuroscientist. This quality control ensures that the resulting brain segmentations are anatomically correct and could be used for subsequent image synthesis. From the 125 subjects originally reserved for SST, we select 101 that pass this quality control and only their segmentations are used for the synthetic image generation based on VIA11 data. <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> show that with as little as 20 segmentation maps they can reach the top performance therefore we believe that our choice of using only the 101 highest-quality segmentations will not impede the performance of the models. Generated synthetic images from the VIA11 segmentations are then used only for self-supervised pretraining described later.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">For the BrainVISA dataset, we obtain the tissue segmentations by utilizing an implementation of an expectation-maximization-based algorithm described by <cite class="ltx_cite ltx_citemacro_citet">Schindler and Dellaert (<a href="#bib.bib49" title="" class="ltx_ref">2004</a>)</cite>, that classifies image voxels between WM and GM based on the estimated parameters of the intensities distribution of tissues. Since BrainVISA images contain voxels belonging only to either WM or GM and skull stripping of those images is manually corrected, we opt to use this method for its simplicity and speed. For the BrainVISA segmentations, we additionally combine the central sulcus labels with the tissue labels to create a single segmentation that includes both the tissue information required for generating synthetic images and the sulci labels needed to train the CS segmentation model.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">After obtaining the final segmentations we employ SynthSeg’s data generator while incorporating the following adjustments:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">We use T1w tissue priors provided by <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> to generate images with a contrast similar to T1w by sampling the intensity values for each tissue based on its Gaussian mixture model parameters. Although the original paper demonstrated that the same approach could be used to train a model invariant to any specific contrast by sampling random intensities that do not rely on any priors, we decided to use T1w-based intensities to simplify and speed up the training process, since our goal is evaluating the models’ performance on the VIA11 dataset, which also consists of T1w images. Furthermore, we believe that restricting the power of possible augmentations can lead to faster convergence and the ability to learn from fewer data which is favourable in the current setting due to the limited amount of generated images and computational resources</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">We preserve the original image dimensions when generating the output, excluding the random resampling and cropping transformations employed by the SynthSeg model. Preserving sufficient spatial resolution is crucial for accurate sulci segmentation, and reducing resolution or cropping the images may result in the loss of important information. Additionally, both the VIA11 and BrainVISA datasets contain isotropic images with a spatial resolution close to 1x1x1mm, eliminating the need to learn resolution-agnostic features for our experiments.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">We utilize the complete set of original spatial transformations, including random affine and elastic transformations of the segmentation map, Gaussian blurring, and bias field corruption applied to the generated image.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">As we use skull-stripped images, no transformations related to random drops of segmentation labels related to the skull are performed.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Sulci labels are not considered during the image generation step. The model uses corresponding sulci voxels labels of background, WM, or GM for synthesizing intensities under the sulci labels, ensuring the integrity of the image and allowing potential overlap of sulci labels with WM or GM voxels, if present in the original images.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS1.p6" class="ltx_para">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p">After the generation process, we obtain a pair of synthetic intensity images and the corresponding segmentation, which includes labels for the tissues and the sulci (only for BrainVISA). This dataset obtained through the generative model is referred to as the synthetic dataset.</p>
</div>
<div id="S3.SS2.SSS1.p7" class="ltx_para">
<p id="S3.SS2.SSS1.p7.1" class="ltx_p">By applying rigid and non-rigid spatial transformations, random intensity sampling, artefact generation, bias field corruption, and blurring, we simulate high variability in image appearances as well as cortex morphology while preserving crucial information necessary for CS detection and segmentation. <cite class="ltx_cite ltx_citemacro_citet">Clarisse et al. (<a href="#bib.bib15" title="" class="ltx_ref">1997</a>)</cite> demonstrated that consistent and accurate identification of the CS relies on several key criteria, including its relative location to other stable and distinct folds, specific shape patterns, as well as its symmetrical location and position on the cortical surface, all of which remain relatively invariant with our transformations. Therefore, by distorting the images in ways that maintain these criteria, we hypothesize that the model will learn a more robust representation of the CS location and shape, invariant to potential distortions that can occur in different datasets, caused by the changes induced by gyrification or brain volume differences, leading to better recognition performance and increased robustness on the morphologically diverse datasets. This hypothesis is supported by the findings of <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023b</a>)</cite>, who showed the effectiveness of such synthetic data generation for tissue segmentation tasks.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>SimCLR</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Self-supervised learning (SSL) is a popular method for training DL models in the absence of labelled data that has been especially popular in the medical imaging field, where the cost of labelling is extremely high <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>. Contrastive training is one of the popular SSL approaches used to learn meaningful representations of input data by maximizing the similarity between different views of the same input and minimizing the similarity between views of different data <cite class="ltx_cite ltx_citemacro_citep">(Jaiswal et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> is a successful implementation of contrastive learning, particularly in medical image classification and segmentation <cite class="ltx_cite ltx_citemacro_citep">(Azizi et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>; Dominic et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Zeng et al., <a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>. The general structure of SimCLR is illustrated in Figure <a href="#S3.F5.1" title="Figure 5 ‣ 3.2.2 SimCLR ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Our objective in utilizing SSL and SimCLR is to integrate knowledge about cortex morphology, including sulci position and shape, into the model weights during the pre-training phase. This integration is expected to be beneficial during the subsequent fine-tuning phase, where the model will focus on learning central sulcus segmentation.</p>
</div>
<figure id="S3.F5.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.15121/assets/figures/SelfSupervisedTrainingFlow.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_square" width="598" height="616" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>SimCLR framework architecture. First, two image views are generated for each segmentation present in the batch using a synthetic data generator. These synthetic images are then passed through a U-Net encoder, which calculates a dense image representation which is further projected into a space where contrastive loss is computed using an MLP. The loss function encourages the embeddings of images from the same segmentation to be close together in the embedding space while pushing apart the embeddings of images from different segmentation maps. </figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The first step of the SimCLR framework involves generating multiple views of the same input image, which is a crucial step aimed at preserving relevant semantic information while introducing image variability. Random cropping, colour distortion, and Gaussian blur proved to be effective transformations for generating views in natural image classification <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>. However, in our case of 3D grayscale volumes in which cropping might erase important information about the cortex morphology we apply a different approach. Instead, we leverage the synthetic dataset generated from a single tissue segmentation and treat it as a dataset containing multiple views of the same input. We hypothesize that the diverse synthetic images derived from the same segmentation capture essential and identical information about the same cortex morphology while the unique transformations applied to each image introduce the necessary variability. This view generation process can be thought of as creating multiple distortions of the same cortical morphological fingerprint by stretching, scaling, changing its colour or elastically deforming it that would nonetheless preserve the unique pattern present in it.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">After generating the different image views, they are sequentially passed through the base model and a non-linear transformation unit based on a Multi-Layer Perceptron (MLP). The base model serves as a robust feature extractor, producing a dense representation of the image that captures key features. Inspired by recent experiments <cite class="ltx_cite ltx_citemacro_citep">(Dominic et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Zeng et al., <a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>, we choose the U-Net encoder as the base model. We utilize the first five layers from the downsampling path of the U-Net and flatten the output of the last down convolution layer after max pooling to introduce it as input to the MLP. The MLP projects the feature embeddings from the base model space into a space where contrastive loss is calculated. This helps filter out specific features preferred by the contrastive loss optimization and allows the base model to learn a more robust image representation. In our experiments, we used a 3-layer MLP with a final embedding dimension of 128, as deeper MLPs have shown better results <cite class="ltx_cite ltx_citemacro_citep">(Azizi et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.3" class="ltx_p">The final step involves calculating the embeddings’ similarity and optimizing the total contrastive loss shown in Equation <a href="#S3.E1" title="In 3.2.2 SimCLR ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which is based on the Normalized Temperature-scaled Cross Entropy Loss (NT-Xent) derived from the work of <cite class="ltx_cite ltx_citemacro_citet">Oord et al. (<a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite> and displayed in Equation <a href="#S3.E2" title="In 3.2.2 SimCLR ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The SimCLR framework aims to maximize the similarity between the embeddings of two augmented versions of the same image (i.e., <math id="S3.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="S3.SS2.SSS2.p4.1.m1.1a"><msub id="S3.SS2.SSS2.p4.1.m1.1.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p4.1.m1.1.1.2" xref="S3.SS2.SSS2.p4.1.m1.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS2.p4.1.m1.1.1.3" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.1.m1.1b"><apply id="S3.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.2">𝑧</ci><ci id="S3.SS2.SSS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.1.m1.1c">z_{i}</annotation></semantics></math> and <math id="S3.SS2.SSS2.p4.2.m2.1" class="ltx_Math" alttext="z_{j}" display="inline"><semantics id="S3.SS2.SSS2.p4.2.m2.1a"><msub id="S3.SS2.SSS2.p4.2.m2.1.1" xref="S3.SS2.SSS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p4.2.m2.1.1.2" xref="S3.SS2.SSS2.p4.2.m2.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS2.p4.2.m2.1.1.3" xref="S3.SS2.SSS2.p4.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.2.m2.1b"><apply id="S3.SS2.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1.2">𝑧</ci><ci id="S3.SS2.SSS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.2.m2.1c">z_{j}</annotation></semantics></math>) while minimizing it between views of different images <math id="S3.SS2.SSS2.p4.3.m3.1" class="ltx_Math" alttext="z_{k}" display="inline"><semantics id="S3.SS2.SSS2.p4.3.m3.1a"><msub id="S3.SS2.SSS2.p4.3.m3.1.1" xref="S3.SS2.SSS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p4.3.m3.1.1.2" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS2.p4.3.m3.1.1.3" xref="S3.SS2.SSS2.p4.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.3.m3.1b"><apply id="S3.SS2.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2">𝑧</ci><ci id="S3.SS2.SSS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.3.m3.1c">z_{k}</annotation></semantics></math>. The similarity between embeddings is estimated using cosine similarity defined in Equation <a href="#S3.E3" title="In 3.2.2 SimCLR ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{L_{\text{contrastive}}}=\frac{1}{2N}\sum_{k=1}^{N}[{\ell(2k-1,2k))+\ell({2k,2k-1})}]" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1b"><msub id="S3.E1.m1.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.2">ℒ</mi><mtext id="S3.E1.m1.1.1.3">contrastive</mtext></msub><mo id="S3.E1.m1.1.2">=</mo><mfrac id="S3.E1.m1.1.3"><mn id="S3.E1.m1.1.3.2">1</mn><mrow id="S3.E1.m1.1.3.3"><mn id="S3.E1.m1.1.3.3.2">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.3.3.1">​</mo><mi id="S3.E1.m1.1.3.3.3">N</mi></mrow></mfrac><munderover id="S3.E1.m1.1.4"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.1.4.2.2">∑</mo><mrow id="S3.E1.m1.1.4.2.3"><mi id="S3.E1.m1.1.4.2.3.2">k</mi><mo id="S3.E1.m1.1.4.2.3.1">=</mo><mn id="S3.E1.m1.1.4.2.3.3">1</mn></mrow><mi id="S3.E1.m1.1.4.3">N</mi></munderover><mrow id="S3.E1.m1.1.5"><mo stretchy="false" id="S3.E1.m1.1.5.1">[</mo><mi mathvariant="normal" id="S3.E1.m1.1.5.2">ℓ</mi><mrow id="S3.E1.m1.1.5.3"><mo stretchy="false" id="S3.E1.m1.1.5.3.1">(</mo><mn id="S3.E1.m1.1.5.3.2">2</mn><mi id="S3.E1.m1.1.5.3.3">k</mi><mo id="S3.E1.m1.1.5.3.4">−</mo><mn id="S3.E1.m1.1.5.3.5">1</mn><mo id="S3.E1.m1.1.5.3.6">,</mo><mn id="S3.E1.m1.1.5.3.7">2</mn><mi id="S3.E1.m1.1.5.3.8">k</mi><mo stretchy="false" id="S3.E1.m1.1.5.3.9">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.1.5.4">)</mo></mrow><mo id="S3.E1.m1.1.6">+</mo><mi mathvariant="normal" id="S3.E1.m1.1.7">ℓ</mi><mrow id="S3.E1.m1.1.8"><mo stretchy="false" id="S3.E1.m1.1.8.1">(</mo><mn id="S3.E1.m1.1.8.2">2</mn><mi id="S3.E1.m1.1.8.3">k</mi><mo id="S3.E1.m1.1.8.4">,</mo><mn id="S3.E1.m1.1.8.5">2</mn><mi id="S3.E1.m1.1.8.6">k</mi><mo id="S3.E1.m1.1.8.7">−</mo><mn id="S3.E1.m1.1.8.8">1</mn><mo stretchy="false" id="S3.E1.m1.1.8.9">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.1.9">]</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L_{\text{contrastive}}}=\frac{1}{2N}\sum_{k=1}^{N}[{\ell(2k-1,2k))+\ell({2k,2k-1})}]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.7" class="ltx_Math" alttext="\ell_{i,j}=-\log\frac{\exp(\text{sim}(z_{i},z_{j})/\tau)}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\text{sim}(z_{i},z_{k})/\tau)}" display="block"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.8" xref="S3.E2.m1.7.8.cmml"><msub id="S3.E2.m1.7.8.2" xref="S3.E2.m1.7.8.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.7.8.2.2" xref="S3.E2.m1.7.8.2.2.cmml">ℓ</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">i</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.E2.m1.7.8.1" xref="S3.E2.m1.7.8.1.cmml">=</mo><mrow id="S3.E2.m1.7.8.3" xref="S3.E2.m1.7.8.3.cmml"><mo rspace="0.167em" id="S3.E2.m1.7.8.3a" xref="S3.E2.m1.7.8.3.cmml">−</mo><mrow id="S3.E2.m1.7.8.3.2" xref="S3.E2.m1.7.8.3.2.cmml"><mi id="S3.E2.m1.7.8.3.2.1" xref="S3.E2.m1.7.8.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.7.8.3.2a" xref="S3.E2.m1.7.8.3.2.cmml">⁡</mo><mfrac id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml"><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">exp</mi><mo id="S3.E2.m1.4.4.2.2a" xref="S3.E2.m1.4.4.2.3.cmml">⁡</mo><mrow id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.2" xref="S3.E2.m1.4.4.2.3.cmml">(</mo><mrow id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.cmml"><mrow id="S3.E2.m1.4.4.2.2.1.1.2" xref="S3.E2.m1.4.4.2.2.1.1.2.cmml"><mtext id="S3.E2.m1.4.4.2.2.1.1.2.4" xref="S3.E2.m1.4.4.2.2.1.1.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1.2.3" xref="S3.E2.m1.4.4.2.2.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.1.1.2.2.2" xref="S3.E2.m1.4.4.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.2.2.2.3" xref="S3.E2.m1.4.4.2.2.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.4.4.2.2.1.1.2.2.2.4" xref="S3.E2.m1.4.4.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.2.cmml">z</mi><mi id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.1.2.2.2.5" xref="S3.E2.m1.4.4.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.2.2.1.1.3" xref="S3.E2.m1.4.4.2.2.1.1.3.cmml">/</mo><mi id="S3.E2.m1.4.4.2.2.1.1.4" xref="S3.E2.m1.4.4.2.2.1.1.4.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E2.m1.4.4.2.2.1.3" xref="S3.E2.m1.4.4.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E2.m1.7.7.5" xref="S3.E2.m1.7.7.5.cmml"><msubsup id="S3.E2.m1.7.7.5.4" xref="S3.E2.m1.7.7.5.4.cmml"><mo id="S3.E2.m1.7.7.5.4.2.2" xref="S3.E2.m1.7.7.5.4.2.2.cmml">∑</mo><mrow id="S3.E2.m1.7.7.5.4.2.3" xref="S3.E2.m1.7.7.5.4.2.3.cmml"><mi id="S3.E2.m1.7.7.5.4.2.3.2" xref="S3.E2.m1.7.7.5.4.2.3.2.cmml">k</mi><mo id="S3.E2.m1.7.7.5.4.2.3.1" xref="S3.E2.m1.7.7.5.4.2.3.1.cmml">=</mo><mn id="S3.E2.m1.7.7.5.4.2.3.3" xref="S3.E2.m1.7.7.5.4.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.7.7.5.4.3" xref="S3.E2.m1.7.7.5.4.3.cmml"><mn id="S3.E2.m1.7.7.5.4.3.2" xref="S3.E2.m1.7.7.5.4.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.5.4.3.1" xref="S3.E2.m1.7.7.5.4.3.1.cmml">​</mo><mi id="S3.E2.m1.7.7.5.4.3.3" xref="S3.E2.m1.7.7.5.4.3.3.cmml">N</mi></mrow></msubsup><mrow id="S3.E2.m1.7.7.5.3" xref="S3.E2.m1.7.7.5.3.cmml"><msub id="S3.E2.m1.7.7.5.3.3" xref="S3.E2.m1.7.7.5.3.3.cmml"><mn id="S3.E2.m1.7.7.5.3.3.2" xref="S3.E2.m1.7.7.5.3.3.2.cmml">𝟙</mn><mrow id="S3.E2.m1.5.5.3.1.1.1" xref="S3.E2.m1.5.5.3.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.3.1.1.1.2" xref="S3.E2.m1.5.5.3.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.5.5.3.1.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.3.1.1.1.1.2" xref="S3.E2.m1.5.5.3.1.1.1.1.2.cmml">k</mi><mo id="S3.E2.m1.5.5.3.1.1.1.1.1" xref="S3.E2.m1.5.5.3.1.1.1.1.1.cmml">≠</mo><mi id="S3.E2.m1.5.5.3.1.1.1.1.3" xref="S3.E2.m1.5.5.3.1.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S3.E2.m1.5.5.3.1.1.1.3" xref="S3.E2.m1.5.5.3.1.1.2.1.cmml">]</mo></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.7.7.5.3.2" xref="S3.E2.m1.7.7.5.3.2.cmml">​</mo><mrow id="S3.E2.m1.7.7.5.3.1.1" xref="S3.E2.m1.7.7.5.3.1.2.cmml"><mi id="S3.E2.m1.6.6.4.2" xref="S3.E2.m1.6.6.4.2.cmml">exp</mi><mo id="S3.E2.m1.7.7.5.3.1.1a" xref="S3.E2.m1.7.7.5.3.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.7.7.5.3.1.1.1" xref="S3.E2.m1.7.7.5.3.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.5.3.1.1.1.2" xref="S3.E2.m1.7.7.5.3.1.2.cmml">(</mo><mrow id="S3.E2.m1.7.7.5.3.1.1.1.1" xref="S3.E2.m1.7.7.5.3.1.1.1.1.cmml"><mrow id="S3.E2.m1.7.7.5.3.1.1.1.1.2" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.cmml"><mtext id="S3.E2.m1.7.7.5.3.1.1.1.1.2.4" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.5.3.1.1.1.1.2.3" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.3" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.4" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.2.cmml">z</mi><mi id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.5" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.7.7.5.3.1.1.1.1.3" xref="S3.E2.m1.7.7.5.3.1.1.1.1.3.cmml">/</mo><mi id="S3.E2.m1.7.7.5.3.1.1.1.1.4" xref="S3.E2.m1.7.7.5.3.1.1.1.1.4.cmml">τ</mi></mrow><mo stretchy="false" id="S3.E2.m1.7.7.5.3.1.1.1.3" xref="S3.E2.m1.7.7.5.3.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.8.cmml" xref="S3.E2.m1.7.8"><eq id="S3.E2.m1.7.8.1.cmml" xref="S3.E2.m1.7.8.1"></eq><apply id="S3.E2.m1.7.8.2.cmml" xref="S3.E2.m1.7.8.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.8.2.1.cmml" xref="S3.E2.m1.7.8.2">subscript</csymbol><ci id="S3.E2.m1.7.8.2.2.cmml" xref="S3.E2.m1.7.8.2.2">ℓ</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑖</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S3.E2.m1.7.8.3.cmml" xref="S3.E2.m1.7.8.3"><minus id="S3.E2.m1.7.8.3.1.cmml" xref="S3.E2.m1.7.8.3"></minus><apply id="S3.E2.m1.7.8.3.2.cmml" xref="S3.E2.m1.7.8.3.2"><log id="S3.E2.m1.7.8.3.2.1.cmml" xref="S3.E2.m1.7.8.3.2.1"></log><apply id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7"><divide id="S3.E2.m1.7.7.6.cmml" xref="S3.E2.m1.7.7"></divide><apply id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2"><exp id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"></exp><apply id="S3.E2.m1.4.4.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"><divide id="S3.E2.m1.4.4.2.2.1.1.3.cmml" xref="S3.E2.m1.4.4.2.2.1.1.3"></divide><apply id="S3.E2.m1.4.4.2.2.1.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2"><times id="S3.E2.m1.4.4.2.2.1.1.2.3.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.3"></times><ci id="S3.E2.m1.4.4.2.2.1.1.2.4a.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.4"><mtext id="S3.E2.m1.4.4.2.2.1.1.2.4.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.4">sim</mtext></ci><interval closure="open" id="S3.E2.m1.4.4.2.2.1.1.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2"><apply id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.2">𝑧</ci><ci id="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply><ci id="S3.E2.m1.4.4.2.2.1.1.4.cmml" xref="S3.E2.m1.4.4.2.2.1.1.4">𝜏</ci></apply></apply><apply id="S3.E2.m1.7.7.5.cmml" xref="S3.E2.m1.7.7.5"><apply id="S3.E2.m1.7.7.5.4.cmml" xref="S3.E2.m1.7.7.5.4"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.4.1.cmml" xref="S3.E2.m1.7.7.5.4">superscript</csymbol><apply id="S3.E2.m1.7.7.5.4.2.cmml" xref="S3.E2.m1.7.7.5.4"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.4.2.1.cmml" xref="S3.E2.m1.7.7.5.4">subscript</csymbol><sum id="S3.E2.m1.7.7.5.4.2.2.cmml" xref="S3.E2.m1.7.7.5.4.2.2"></sum><apply id="S3.E2.m1.7.7.5.4.2.3.cmml" xref="S3.E2.m1.7.7.5.4.2.3"><eq id="S3.E2.m1.7.7.5.4.2.3.1.cmml" xref="S3.E2.m1.7.7.5.4.2.3.1"></eq><ci id="S3.E2.m1.7.7.5.4.2.3.2.cmml" xref="S3.E2.m1.7.7.5.4.2.3.2">𝑘</ci><cn type="integer" id="S3.E2.m1.7.7.5.4.2.3.3.cmml" xref="S3.E2.m1.7.7.5.4.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.7.7.5.4.3.cmml" xref="S3.E2.m1.7.7.5.4.3"><times id="S3.E2.m1.7.7.5.4.3.1.cmml" xref="S3.E2.m1.7.7.5.4.3.1"></times><cn type="integer" id="S3.E2.m1.7.7.5.4.3.2.cmml" xref="S3.E2.m1.7.7.5.4.3.2">2</cn><ci id="S3.E2.m1.7.7.5.4.3.3.cmml" xref="S3.E2.m1.7.7.5.4.3.3">𝑁</ci></apply></apply><apply id="S3.E2.m1.7.7.5.3.cmml" xref="S3.E2.m1.7.7.5.3"><times id="S3.E2.m1.7.7.5.3.2.cmml" xref="S3.E2.m1.7.7.5.3.2"></times><apply id="S3.E2.m1.7.7.5.3.3.cmml" xref="S3.E2.m1.7.7.5.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.3.3.1.cmml" xref="S3.E2.m1.7.7.5.3.3">subscript</csymbol><cn type="integer" id="S3.E2.m1.7.7.5.3.3.2.cmml" xref="S3.E2.m1.7.7.5.3.3.2">1</cn><apply id="S3.E2.m1.5.5.3.1.1.2.cmml" xref="S3.E2.m1.5.5.3.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.3.1.1.2.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.5.5.3.1.1.1.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1"><neq id="S3.E2.m1.5.5.3.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.1"></neq><ci id="S3.E2.m1.5.5.3.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.2">𝑘</ci><ci id="S3.E2.m1.5.5.3.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.3.1.1.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.E2.m1.7.7.5.3.1.2.cmml" xref="S3.E2.m1.7.7.5.3.1.1"><exp id="S3.E2.m1.6.6.4.2.cmml" xref="S3.E2.m1.6.6.4.2"></exp><apply id="S3.E2.m1.7.7.5.3.1.1.1.1.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1"><divide id="S3.E2.m1.7.7.5.3.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.3"></divide><apply id="S3.E2.m1.7.7.5.3.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2"><times id="S3.E2.m1.7.7.5.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.3"></times><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.2.4a.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.4"><mtext id="S3.E2.m1.7.7.5.3.1.1.1.1.2.4.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.4">sim</mtext></ci><interval closure="open" id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2"><apply id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.2">𝑧</ci><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.2.2.2.2.3">𝑘</ci></apply></interval></apply><ci id="S3.E2.m1.7.7.5.3.1.1.1.1.4.cmml" xref="S3.E2.m1.7.7.5.3.1.1.1.1.4">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">\ell_{i,j}=-\log\frac{\exp(\text{sim}(z_{i},z_{j})/\tau)}{\sum_{k=1}^{2N}\mathbb{1}_{[k\neq i]}\exp(\text{sim}(z_{i},z_{k})/\tau)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\text{sim}(z_{i},z_{j})=\frac{z_{i}^{\top}\cdot z_{j}}{\|z_{i}\|\cdot\|z_{j}\|}" display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml"><mtext id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.3" xref="S3.E3.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.3" xref="S3.E3.m1.4.4.2.2.3.cmml">(</mo><msub id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">z</mi><mi id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.4.4.2.2.2.4" xref="S3.E3.m1.4.4.2.2.3.cmml">,</mo><msub id="S3.E3.m1.4.4.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.2.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.2.cmml">z</mi><mi id="S3.E3.m1.4.4.2.2.2.2.3" xref="S3.E3.m1.4.4.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.5" xref="S3.E3.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml">=</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><msubsup id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml"><mi id="S3.E3.m1.2.2.4.2.2.2" xref="S3.E3.m1.2.2.4.2.2.2.cmml">z</mi><mi id="S3.E3.m1.2.2.4.2.2.3" xref="S3.E3.m1.2.2.4.2.2.3.cmml">i</mi><mo id="S3.E3.m1.2.2.4.2.3" xref="S3.E3.m1.2.2.4.2.3.cmml">⊤</mo></msubsup><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.2.4.1" xref="S3.E3.m1.2.2.4.1.cmml">⋅</mo><msub id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml"><mi id="S3.E3.m1.2.2.4.3.2" xref="S3.E3.m1.2.2.4.3.2.cmml">z</mi><mi id="S3.E3.m1.2.2.4.3.3" xref="S3.E3.m1.2.2.4.3.3.cmml">j</mi></msub></mrow><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.cmml">‖</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mo rspace="0.222em" id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">⋅</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.2.1.cmml">‖</mo><msub id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">z</mi><mi id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"></eq><apply id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"><times id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.3"></times><ci id="S3.E3.m1.4.4.2.4a.cmml" xref="S3.E3.m1.4.4.2.4"><mtext id="S3.E3.m1.4.4.2.4.cmml" xref="S3.E3.m1.4.4.2.4">sim</mtext></ci><interval closure="open" id="S3.E3.m1.4.4.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2"><apply id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">𝑧</ci><ci id="S3.E3.m1.3.3.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.4.4.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2.2">𝑧</ci><ci id="S3.E3.m1.4.4.2.2.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><ci id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4.1">⋅</ci><apply id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.2.1.cmml" xref="S3.E3.m1.2.2.4.2">superscript</csymbol><apply id="S3.E3.m1.2.2.4.2.2.cmml" xref="S3.E3.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.2.2.1.cmml" xref="S3.E3.m1.2.2.4.2">subscript</csymbol><ci id="S3.E3.m1.2.2.4.2.2.2.cmml" xref="S3.E3.m1.2.2.4.2.2.2">𝑧</ci><ci id="S3.E3.m1.2.2.4.2.2.3.cmml" xref="S3.E3.m1.2.2.4.2.2.3">𝑖</ci></apply><csymbol cd="latexml" id="S3.E3.m1.2.2.4.2.3.cmml" xref="S3.E3.m1.2.2.4.2.3">top</csymbol></apply><apply id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.3.1.cmml" xref="S3.E3.m1.2.2.4.3">subscript</csymbol><ci id="S3.E3.m1.2.2.4.3.2.cmml" xref="S3.E3.m1.2.2.4.3.2">𝑧</ci><ci id="S3.E3.m1.2.2.4.3.3.cmml" xref="S3.E3.m1.2.2.4.3.3">𝑗</ci></apply></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><ci id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3">⋅</ci><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2">norm</csymbol><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2">𝑧</ci><ci id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\text{sim}(z_{i},z_{j})=\frac{z_{i}^{\top}\cdot z_{j}}{\|z_{i}\|\cdot\|z_{j}\|}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS2.p7" class="ltx_para">
<p id="S3.SS2.SSS2.p7.1" class="ltx_p">By optimizing this loss function, we aim to pre-train the model on a task similar to the downstream task, but without actual labels. The pre-training with SimCLR on synthetic data encourages the U-Net encoder to learn robust and comprehensive feature representations of the cortex morphology, as it is the only consistent and distinctive feature across different image views. Following pre-training, the model undergoes fine-tuning on the downstream task.</p>
</div>
<div id="S3.SS2.SSS2.p8" class="ltx_para">
<p id="S3.SS2.SSS2.p8.1" class="ltx_p">Our hypothesis is that by initializing the weights of the U-Net encoder with those learned during pre-training, we can transfer information about the anatomical variability of the cortical folds from a bigger and more diverse dataset and then leverage that knowledge for segmentation through fine-tuning. Specifically, we focus on pre-training with the VIA11 dataset and subsequent fine-tuning on the BrainVISA dataset to assess if the model can capture cohort-specific sulci properties that may be absent in the limited labelled data as we are especially interested in improving the performance on the VIA11 that we use for the final evaluation.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Multi-task learning</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">In our previous approach, we utilized the SimCLR framework for pre-training the U-Net encoder. However, we also investigate the pre-training of the decoder component in the U-Net architecture. Drawing inspiration from recent advancements in multi-task learning <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>; Zhou et al., <a href="#bib.bib61" title="" class="ltx_ref">2021</a>)</cite>, we propose a novel pre-training framework that combines contrastive self-supervised learning with segmentation learning to pre-train the entire U-Net model.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Illustrated in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2.3 Multi-task learning ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our multi-task SSL pipeline consists of two parts. The first part follows the contrastive pre-training structure described before, calculating the contrastive loss and updating the weights of the U-Net encoder. The second part employs the same encoder model, combined with a symmetrical decoder which is simultaneously trained in a joint optimization procedure for brain tissue segmentation. We utilize the same labels used to create synthetic images to train the U-Net decoder to segment GM tissue based on the intensity images, effectively replicating a part of the SynthSeg training pipeline. We choose to train the model for only single-class GM segmentation to make it compatible with the downstream single-class task of CS segmentation, avoiding the need to adjust internal embedding and kernel dimensions. Furthermore, GM segmentations were already available as a prerequisite for synthetic data generation and segmenting GM requires an understanding of the cortex morphology from the model at the surface detection level, albeit based on intensity contrast.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2403.15121/assets/figures/MultiTaskFlow.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Multi-task self-supervised training scheme. Combined contrastive and segmentation loss allows pre-training of both the encoder (on contrastive and GM segmentation tasks) and decoder (only on GM segmentation task) of the U-Net.</figcaption>
</figure>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">The final loss function optimized in this pipeline, shown in Equation <a href="#S3.E4" title="In 3.2.3 Multi-task learning ‣ 3.2 Methods ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, is a combination of the segmentation loss and the contrastive loss discussed earlier. We employ the soft dice loss implementation from MONAI <cite class="ltx_cite ltx_citemacro_citep">(Cardoso et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> for the segmentation loss. This training scheme enables us to target the full U-Net model during the pre-training phase, learning improved weights initialization for both the encoder and decoder. Moreover, it encourages the encoder to learn representative features not only for the contrastive task but also for the segmentation task directly. By employing the multi-task loss, we aim to leverage the information learned during the pre-training phase more effectively in the downstream phase.</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\mathcal{L_{\text{multi-task}}}=\mathcal{L}_{\text{segmentation}}+\mathcal{L}_{\text{contrastive}}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3a.cmml">multi-task</mtext></msub><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3a.cmml">segmentation</mtext></msub><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">ℒ</mi><mtext id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3a.cmml">contrastive</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">ℒ</ci><ci id="S3.E4.m1.1.1.2.3a.cmml" xref="S3.E4.m1.1.1.2.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">multi-task</mtext></ci></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2">ℒ</ci><ci id="S3.E4.m1.1.1.3.2.3a.cmml" xref="S3.E4.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3">segmentation</mtext></ci></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">ℒ</ci><ci id="S3.E4.m1.1.1.3.3.3a.cmml" xref="S3.E4.m1.1.1.3.3.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">contrastive</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathcal{L_{\text{multi-task}}}=\mathcal{L}_{\text{segmentation}}+\mathcal{L}_{\text{contrastive}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training and Validation Strategy</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">This section presents the technical implementation details of the tested models, training and validation strategies employed. These details aim to clarify the rationale behind our parameter choices and facilitate the replication of our results.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Unlike the approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite>, which utilizes online data generation which creates synthetic images on the fly and directly feeds them into the segmentation model, we employ an offline generation approach. This choice is driven by computational limitations that prevent us from simultaneously running both the generative and segmentation models on the same GPU. Due to the storage constraints, we generate 100 synthetic images for each subject from both the BrainVISA and VIA11 datasets, resulting in 6,200 synthetic images for BrainVISA and 10,100 synthetic images for VIA11 datasets.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To ensure consistency in training parameters, we train all U-Net models for a maximum of 200 epochs during the central sulcus (CS) segmentation learning process. We employ an early stopping criterion, wherein training is halted if the validation loss fails to improve for the last 10 epochs. For CS segmentation, we adopt the Tversky Loss introduced by <cite class="ltx_cite ltx_citemacro_citet">Salehi et al. (<a href="#bib.bib48" title="" class="ltx_ref">2017</a>)</cite> as our learning criterion. This loss function has demonstrated superior performance in highly imbalanced segmentation problems, which is important in our case as CS voxels occupy on average around 0.02% of all image voxels. Although we train the model on both synthetic and original images in some experiments, validation is always performed using the original images from the corresponding validation splits. We employ a batch size of 1, as it is the maximum that can fit within our available GPUs (NVIDIA GeForce RTX 3090 with 24GB of video RAM) and an initial learning rate (LR) of 0.001.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In the final evaluation stage, we incorporate a post-processing step in our workflow to facilitate a meaningful comparison between the segmentations and meshes generated by our models and BrainVISA’s pipeline. Given that our segmentations do not rely on CSF skeleton images and do not impose anatomical correctness requirements as part of their design, it is essential to ensure their compatibility with the meshing algorithm. To achieve this, we have opted to employ the same meshing algorithm utilized by BrainVISA in their pipeline for generating meshes from segmentations. By adopting the same tool, we minimize additional variability introduced by different meshing techniques, enabling a more accurate comparison of mesh properties.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">The meshing process involves intricate calculations to create a surface based on a point cloud <cite class="ltx_cite ltx_citemacro_citep">(McConnell, <a href="#bib.bib39" title="" class="ltx_ref">1995</a>)</cite>. However, during this process, errors such as gaps, holes, and excessive tessellation can arise, particularly in the presence of noise points. To address these issues and achieve appropriate tessellation in our generated segmentations, we apply a straightforward post-processing approach. We begin by performing a morphological binary dilation on the obtained segmentation, connecting sulcus segments that are close to each other but separated spatially. Subsequently, connected component labelling is applied to the dilated image. In the final segmentation, we retain only the voxels from the original segmentation that belong to the two largest connected components calculated from the dilated image. This step ensures that only the central sulcus segments from the left and right hemispheres are retained before the meshing stage, reducing errors in the resulting sulcus mesh and enhancing its quality. We apply this post-processing step only for the final comparison between BrainVISA’s pipeline and our approaches as it is needed specifically for correct meshing and fair comparison with the full BrainVISA pipeline.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Quantitative Analysis</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To evaluate the quality of our segmentations, we employ two widely used metrics: the Dice similarity coefficient (DSC) and the Hausdorff distance (HD).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.2" class="ltx_p">The DSC quantifies the voxel-wise overlap between two segmentations, denoted as <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">X</annotation></semantics></math> and
<math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">Y</annotation></semantics></math>. Its values range from 0 to 1, where 0 represents no overlap and 1 indicates complete agreement. The DSC is computed using the following formula:</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.3" class="ltx_Math" alttext="DSC=\frac{{2\times|X\cap Y|}}{{|X|+|Y|}}" display="block"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.4" xref="S3.Ex1.m1.3.4.cmml"><mrow id="S3.Ex1.m1.3.4.2" xref="S3.Ex1.m1.3.4.2.cmml"><mi id="S3.Ex1.m1.3.4.2.2" xref="S3.Ex1.m1.3.4.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.4.2.1" xref="S3.Ex1.m1.3.4.2.1.cmml">​</mo><mi id="S3.Ex1.m1.3.4.2.3" xref="S3.Ex1.m1.3.4.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.4.2.1a" xref="S3.Ex1.m1.3.4.2.1.cmml">​</mo><mi id="S3.Ex1.m1.3.4.2.4" xref="S3.Ex1.m1.3.4.2.4.cmml">C</mi></mrow><mo id="S3.Ex1.m1.3.4.1" xref="S3.Ex1.m1.3.4.1.cmml">=</mo><mfrac id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mn id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.2.cmml">×</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">X</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml">∩</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><mrow id="S3.Ex1.m1.3.3.3" xref="S3.Ex1.m1.3.3.3.cmml"><mrow id="S3.Ex1.m1.3.3.3.4.2" xref="S3.Ex1.m1.3.3.3.4.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.3.4.2.1" xref="S3.Ex1.m1.3.3.3.4.1.1.cmml">|</mo><mi id="S3.Ex1.m1.2.2.2.1" xref="S3.Ex1.m1.2.2.2.1.cmml">X</mi><mo stretchy="false" id="S3.Ex1.m1.3.3.3.4.2.2" xref="S3.Ex1.m1.3.3.3.4.1.1.cmml">|</mo></mrow><mo id="S3.Ex1.m1.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.cmml">+</mo><mrow id="S3.Ex1.m1.3.3.3.5.2" xref="S3.Ex1.m1.3.3.3.5.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.3.5.2.1" xref="S3.Ex1.m1.3.3.3.5.1.1.cmml">|</mo><mi id="S3.Ex1.m1.3.3.3.2" xref="S3.Ex1.m1.3.3.3.2.cmml">Y</mi><mo stretchy="false" id="S3.Ex1.m1.3.3.3.5.2.2" xref="S3.Ex1.m1.3.3.3.5.1.1.cmml">|</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.4.cmml" xref="S3.Ex1.m1.3.4"><eq id="S3.Ex1.m1.3.4.1.cmml" xref="S3.Ex1.m1.3.4.1"></eq><apply id="S3.Ex1.m1.3.4.2.cmml" xref="S3.Ex1.m1.3.4.2"><times id="S3.Ex1.m1.3.4.2.1.cmml" xref="S3.Ex1.m1.3.4.2.1"></times><ci id="S3.Ex1.m1.3.4.2.2.cmml" xref="S3.Ex1.m1.3.4.2.2">𝐷</ci><ci id="S3.Ex1.m1.3.4.2.3.cmml" xref="S3.Ex1.m1.3.4.2.3">𝑆</ci><ci id="S3.Ex1.m1.3.4.2.4.cmml" xref="S3.Ex1.m1.3.4.2.4">𝐶</ci></apply><apply id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3"><divide id="S3.Ex1.m1.3.3.4.cmml" xref="S3.Ex1.m1.3.3"></divide><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.2"></times><cn type="integer" id="S3.Ex1.m1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.3">2</cn><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><abs id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></abs><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><intersect id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"></intersect><ci id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">𝑌</ci></apply></apply></apply><apply id="S3.Ex1.m1.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3"><plus id="S3.Ex1.m1.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3"></plus><apply id="S3.Ex1.m1.3.3.3.4.1.cmml" xref="S3.Ex1.m1.3.3.3.4.2"><abs id="S3.Ex1.m1.3.3.3.4.1.1.cmml" xref="S3.Ex1.m1.3.3.3.4.2.1"></abs><ci id="S3.Ex1.m1.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.1">𝑋</ci></apply><apply id="S3.Ex1.m1.3.3.3.5.1.cmml" xref="S3.Ex1.m1.3.3.3.5.2"><abs id="S3.Ex1.m1.3.3.3.5.1.1.cmml" xref="S3.Ex1.m1.3.3.3.5.2.1"></abs><ci id="S3.Ex1.m1.3.3.3.2.cmml" xref="S3.Ex1.m1.3.3.3.2">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">DSC=\frac{{2\times|X\cap Y|}}{{|X|+|Y|}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.1" class="ltx_p">Another important metric we employ is the Hausdorff distance, defined as:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.9" class="ltx_Math" alttext="H(X,Y)=\max\left\{\sup_{x\in X}\inf_{y\in Y}\rho(x,y),\sup_{y\in Y}\inf_{x\in X}\rho(x,y)\right\}" display="block"><semantics id="S3.Ex2.m1.9a"><mrow id="S3.Ex2.m1.9.9" xref="S3.Ex2.m1.9.9.cmml"><mrow id="S3.Ex2.m1.9.9.4" xref="S3.Ex2.m1.9.9.4.cmml"><mi id="S3.Ex2.m1.9.9.4.2" xref="S3.Ex2.m1.9.9.4.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.9.9.4.1" xref="S3.Ex2.m1.9.9.4.1.cmml">​</mo><mrow id="S3.Ex2.m1.9.9.4.3.2" xref="S3.Ex2.m1.9.9.4.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.9.9.4.3.2.1" xref="S3.Ex2.m1.9.9.4.3.1.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">X</mi><mo id="S3.Ex2.m1.9.9.4.3.2.2" xref="S3.Ex2.m1.9.9.4.3.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">Y</mi><mo stretchy="false" id="S3.Ex2.m1.9.9.4.3.2.3" xref="S3.Ex2.m1.9.9.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.9.9.3" xref="S3.Ex2.m1.9.9.3.cmml">=</mo><mrow id="S3.Ex2.m1.9.9.2.2" xref="S3.Ex2.m1.9.9.2.3.cmml"><mi id="S3.Ex2.m1.7.7" xref="S3.Ex2.m1.7.7.cmml">max</mi><mo id="S3.Ex2.m1.9.9.2.2a" xref="S3.Ex2.m1.9.9.2.3.cmml">⁡</mo><mrow id="S3.Ex2.m1.9.9.2.2.2" xref="S3.Ex2.m1.9.9.2.3.cmml"><mo id="S3.Ex2.m1.9.9.2.2.2.3" xref="S3.Ex2.m1.9.9.2.3.cmml">{</mo><mrow id="S3.Ex2.m1.8.8.1.1.1.1" xref="S3.Ex2.m1.8.8.1.1.1.1.cmml"><munder id="S3.Ex2.m1.8.8.1.1.1.1.1" xref="S3.Ex2.m1.8.8.1.1.1.1.1.cmml"><mo lspace="0em" movablelimits="false" rspace="0.0835em" id="S3.Ex2.m1.8.8.1.1.1.1.1.2" xref="S3.Ex2.m1.8.8.1.1.1.1.1.2.cmml">sup</mo><mrow id="S3.Ex2.m1.8.8.1.1.1.1.1.3" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.8.8.1.1.1.1.1.3.2" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.Ex2.m1.8.8.1.1.1.1.1.3.1" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.8.8.1.1.1.1.1.3.3" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.3.cmml">X</mi></mrow></munder><mrow id="S3.Ex2.m1.8.8.1.1.1.1.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.cmml"><munder id="S3.Ex2.m1.8.8.1.1.1.1.2.1" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.cmml"><mo lspace="0.0835em" movablelimits="false" rspace="0.167em" id="S3.Ex2.m1.8.8.1.1.1.1.2.1.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.2.cmml">inf</mo><mrow id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.cmml"><mi id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.2.cmml">y</mi><mo id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.1" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.3" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.3.cmml">Y</mi></mrow></munder><mrow id="S3.Ex2.m1.8.8.1.1.1.1.2.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.cmml"><mi id="S3.Ex2.m1.8.8.1.1.1.1.2.2.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.2.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.8.8.1.1.1.1.2.2.1" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.2.1" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.1.cmml">(</mo><mi id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml">x</mi><mo id="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.2.2" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.1.cmml">,</mo><mi id="S3.Ex2.m1.4.4" xref="S3.Ex2.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.2.3" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.9.9.2.2.2.4" xref="S3.Ex2.m1.9.9.2.3.cmml">,</mo><mrow id="S3.Ex2.m1.9.9.2.2.2.2" xref="S3.Ex2.m1.9.9.2.2.2.2.cmml"><munder id="S3.Ex2.m1.9.9.2.2.2.2.1" xref="S3.Ex2.m1.9.9.2.2.2.2.1.cmml"><mo lspace="0em" movablelimits="false" rspace="0.0835em" id="S3.Ex2.m1.9.9.2.2.2.2.1.2" xref="S3.Ex2.m1.9.9.2.2.2.2.1.2.cmml">sup</mo><mrow id="S3.Ex2.m1.9.9.2.2.2.2.1.3" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.cmml"><mi id="S3.Ex2.m1.9.9.2.2.2.2.1.3.2" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.2.cmml">y</mi><mo id="S3.Ex2.m1.9.9.2.2.2.2.1.3.1" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.9.9.2.2.2.2.1.3.3" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.3.cmml">Y</mi></mrow></munder><mrow id="S3.Ex2.m1.9.9.2.2.2.2.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.cmml"><munder id="S3.Ex2.m1.9.9.2.2.2.2.2.1" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.cmml"><mo lspace="0.0835em" movablelimits="false" rspace="0.167em" id="S3.Ex2.m1.9.9.2.2.2.2.2.1.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.2.cmml">inf</mo><mrow id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.cmml"><mi id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.2.cmml">x</mi><mo id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.1" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.1.cmml">∈</mo><mi id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.3" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.3.cmml">X</mi></mrow></munder><mrow id="S3.Ex2.m1.9.9.2.2.2.2.2.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.cmml"><mi id="S3.Ex2.m1.9.9.2.2.2.2.2.2.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.2.cmml">ρ</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.9.9.2.2.2.2.2.2.1" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.2.1" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.1.cmml">(</mo><mi id="S3.Ex2.m1.5.5" xref="S3.Ex2.m1.5.5.cmml">x</mi><mo id="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.2.2" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.Ex2.m1.6.6" xref="S3.Ex2.m1.6.6.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.2.3" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.9.9.2.2.2.5" xref="S3.Ex2.m1.9.9.2.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.9b"><apply id="S3.Ex2.m1.9.9.cmml" xref="S3.Ex2.m1.9.9"><eq id="S3.Ex2.m1.9.9.3.cmml" xref="S3.Ex2.m1.9.9.3"></eq><apply id="S3.Ex2.m1.9.9.4.cmml" xref="S3.Ex2.m1.9.9.4"><times id="S3.Ex2.m1.9.9.4.1.cmml" xref="S3.Ex2.m1.9.9.4.1"></times><ci id="S3.Ex2.m1.9.9.4.2.cmml" xref="S3.Ex2.m1.9.9.4.2">𝐻</ci><interval closure="open" id="S3.Ex2.m1.9.9.4.3.1.cmml" xref="S3.Ex2.m1.9.9.4.3.2"><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝑋</ci><ci id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">𝑌</ci></interval></apply><apply id="S3.Ex2.m1.9.9.2.3.cmml" xref="S3.Ex2.m1.9.9.2.2"><max id="S3.Ex2.m1.7.7.cmml" xref="S3.Ex2.m1.7.7"></max><apply id="S3.Ex2.m1.8.8.1.1.1.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1"><apply id="S3.Ex2.m1.8.8.1.1.1.1.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1">subscript</csymbol><csymbol cd="latexml" id="S3.Ex2.m1.8.8.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1.2">supremum</csymbol><apply id="S3.Ex2.m1.8.8.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3"><in id="S3.Ex2.m1.8.8.1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.1"></in><ci id="S3.Ex2.m1.8.8.1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.Ex2.m1.8.8.1.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.1.3.3">𝑋</ci></apply></apply><apply id="S3.Ex2.m1.8.8.1.1.1.1.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2"><apply id="S3.Ex2.m1.8.8.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.8.8.1.1.1.1.2.1.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1">subscript</csymbol><csymbol cd="latexml" id="S3.Ex2.m1.8.8.1.1.1.1.2.1.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.2">infimum</csymbol><apply id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3"><in id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.1"></in><ci id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.2">𝑦</ci><ci id="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.3.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.1.3.3">𝑌</ci></apply></apply><apply id="S3.Ex2.m1.8.8.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2"><times id="S3.Ex2.m1.8.8.1.1.1.1.2.2.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.1"></times><ci id="S3.Ex2.m1.8.8.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.2">𝜌</ci><interval closure="open" id="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.8.8.1.1.1.1.2.2.3.2"><ci id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3">𝑥</ci><ci id="S3.Ex2.m1.4.4.cmml" xref="S3.Ex2.m1.4.4">𝑦</ci></interval></apply></apply></apply><apply id="S3.Ex2.m1.9.9.2.2.2.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2"><apply id="S3.Ex2.m1.9.9.2.2.2.2.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.9.9.2.2.2.2.1.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1">subscript</csymbol><csymbol cd="latexml" id="S3.Ex2.m1.9.9.2.2.2.2.1.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1.2">supremum</csymbol><apply id="S3.Ex2.m1.9.9.2.2.2.2.1.3.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3"><in id="S3.Ex2.m1.9.9.2.2.2.2.1.3.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.1"></in><ci id="S3.Ex2.m1.9.9.2.2.2.2.1.3.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.2">𝑦</ci><ci id="S3.Ex2.m1.9.9.2.2.2.2.1.3.3.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.1.3.3">𝑌</ci></apply></apply><apply id="S3.Ex2.m1.9.9.2.2.2.2.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2"><apply id="S3.Ex2.m1.9.9.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.9.9.2.2.2.2.2.1.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1">subscript</csymbol><csymbol cd="latexml" id="S3.Ex2.m1.9.9.2.2.2.2.2.1.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.2">infimum</csymbol><apply id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3"><in id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.1"></in><ci id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.2">𝑥</ci><ci id="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.3.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.1.3.3">𝑋</ci></apply></apply><apply id="S3.Ex2.m1.9.9.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2"><times id="S3.Ex2.m1.9.9.2.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.1"></times><ci id="S3.Ex2.m1.9.9.2.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.2">𝜌</ci><interval closure="open" id="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.1.cmml" xref="S3.Ex2.m1.9.9.2.2.2.2.2.2.3.2"><ci id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5">𝑥</ci><ci id="S3.Ex2.m1.6.6.cmml" xref="S3.Ex2.m1.6.6">𝑦</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.9c">H(X,Y)=\max\left\{\sup_{x\in X}\inf_{y\in Y}\rho(x,y),\sup_{y\in Y}\inf_{x\in X}\rho(x,y)\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.2" class="ltx_p">It measures the mutual proximity of two segmentations and provides insight into their spatial dissimilarity. HD reflects the maximum distance of the two closest points in the segmentations and it takes positive values with smaller ones reflecting higher proximity and 0 corresponding to the complete overlap of two segmentations. Given the nature of the segmentation task, we argue that the Hausdorff distance is a crucial measure of segmentation quality that should be considered. Sulci localization is a complicated task and the precise placement of the sulci ribbon in the gap between two gyri is often ambiguous as shown in Figure <a href="#S3.F7.1" title="Figure 7 ‣ 3.4 Quantitative Analysis ‣ 3 Material and methods ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S3.F7.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.15121/assets/figures/ambig_segm2.png" id="S3.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Ambiguity in CS segmentation. a) Brain image; b) Brain image with overlapped sulci segmentations: ground truth (red) and manually drawn alternatives (blue and green). Notice how the alternative segmentations closely follow the ground truth in terms of shape and correct anatomical position, despite having zero overlap with the ground truth and yielding a DSC of 0. The precise localization of the CS ribbon within the sulcal gap, which often spans multiple voxels in width, is inherently ambiguous. Therefore, a metric that accounts for the distance between segmentations provides a more robust measure, which makes it crucial to consider.</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Implementation Details</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We used Python programming language with several frameworks for this project. Pytorch and Pytorch Lightning were used to implement and train the SSL and DL models while Tensorflow was used to adapt and run the synthetic data generation pipeline. Additionally, libraries like SimpleITK-SimpleElastix and nibabel were used for image registration and spatial transformations and ITK-Snap with 3D Slicer were used for visualization purposes.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Project code as well as other hyperparameters values and corresponding documentation can be found at: https://github.com/Vivikar/central-sulcus-analysis.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present the results of our experiments, both qualitatively and quantitatively, following the same order as in the previous section.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Synthetic data generation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We begin by examining the impact of synthetic data on the model’s generalizability. To assess this, we compare the performance of the model trained on the synthetic BrainVISA dataset with the model trained on the original BrainVISA dataset. Figure <a href="#S4.F8" title="Figure 8 ‣ 4.1 Synthetic data generation ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> displays the quantitative results comparing the performance of these two models on two evaluation datasets: one composed of the original BrainVISA images from the validation split and the other consisting of original 125 images from the VIA11 dataset, for which we have used BrainVISA’s segmentations as ground truth since they passed the quality control.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/SynthVSOrig.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="313" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Box plot showing DSC and HD scores for the models trained on the synthetic and original BrainVISA datasets and evaluated on the original images from the BrainVISA validation split and VIA11 dataset. A statistically significant decrease (p-value <math id="S4.F8.2.m1.1" class="ltx_Math" alttext="&lt;0.000001" display="inline"><semantics id="S4.F8.2.m1.1b"><mrow id="S4.F8.2.m1.1.1" xref="S4.F8.2.m1.1.1.cmml"><mi id="S4.F8.2.m1.1.1.2" xref="S4.F8.2.m1.1.1.2.cmml"></mi><mo id="S4.F8.2.m1.1.1.1" xref="S4.F8.2.m1.1.1.1.cmml">&lt;</mo><mn id="S4.F8.2.m1.1.1.3" xref="S4.F8.2.m1.1.1.3.cmml">0.000001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F8.2.m1.1c"><apply id="S4.F8.2.m1.1.1.cmml" xref="S4.F8.2.m1.1.1"><lt id="S4.F8.2.m1.1.1.1.cmml" xref="S4.F8.2.m1.1.1.1"></lt><csymbol cd="latexml" id="S4.F8.2.m1.1.1.2.cmml" xref="S4.F8.2.m1.1.1.2">absent</csymbol><cn type="float" id="S4.F8.2.m1.1.1.3.cmml" xref="S4.F8.2.m1.1.1.3">0.000001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F8.2.m1.1d">&lt;0.000001</annotation></semantics></math> based on a two-sided t-test) of HD scores between the model trained on synthetic and original data is observed for the VIA11 dataset.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">On both datasets, we observe a decrease in the Dice similarity coefficient for the models trained on synthetic data. This finding aligns with the studies conducted by <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Billot et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023b</a>)</cite>, which demonstrate that while synthetic data yields significant improvements for images affected by artefacts, low quality, or low resolution, models trained on synthetic data tend to under-perform compared to state-of-the-art models on high-quality and high-resolution images. However, the model trained on synthetic data exhibits a substantial decrease in Hausdorff distance scores on the VIA11 dataset, which arguably provides a more sensible evaluation of performance in this setting (see Figure <a href="#S4.F9.1" title="Figure 9 ‣ 4.1 Synthetic data generation ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Figure <a href="#S4.F9.1" title="Figure 9 ‣ 4.1 Synthetic data generation ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> presents qualitative results that help explain these outcomes. It illustrates how the model trained solely on the original data misclassifies the region unrelated to the central sulcus (CS), while the model trained on synthetic data does not. It is important to note that the misclassified region corresponds to the neck and represents a skull-stripping error, as it should not be present in the skull-stripped image. However, the model trained on synthetic data makes errors in mistakenly segmenting sulci neighbouring to CS as illustrated in image c) of Figure <a href="#S4.F9.1" title="Figure 9 ‣ 4.1 Synthetic data generation ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Although these errors lead to significantly lower Hausdorff distance scores as they are closer to ground truth, they are of great concern as they still have a substantial impact on the meshing of the CS segmentation and the subsequent estimation of its morphological features.</p>
</div>
<figure id="S4.F9.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.15121/assets/figures/synth_errors.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Sample segmentations for VIA11 subjects for the a) model trained on original BrainVISA data, and b) and c) model trained on synthetic BrainVISA data. The yellow arrows indicate the regions where some of the modes made mistakes.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">In our following experiments, we use the synthetic dataset for learning CS segmentation in the fine-tuning stages of the SSL as it demonstrates superior performance.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>SimCLR</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To test how SSL can aid in adjusting the model to new datasets we apply the synthetic data generation approach described earlier to the 101 VIA11 images to create a synthetic VIA11 dataset, which we utilize for self-supervision in conjunction with the synthetic BrainVISA dataset.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>SSL pre-training</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> use several methods to validate the performance of their self-supervised pre-training. However, since our downstream task is not related to classification and the used images do not represent distinct categories of objects, we have chosen to validate the quality of learned image representations through the dimensionality reduction approach. Figure <a href="#S4.F10.1" title="Figure 10 ‣ 4.2.1 SSL pre-training ‣ 4.2 SimCLR ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the projection of the embeddings outputted by the MLP from 128D space to 2D space using T-SNE <cite class="ltx_cite ltx_citemacro_citep">(van der Maaten and Hinton, <a href="#bib.bib37" title="" class="ltx_ref">2008</a>)</cite>. We select random four validation VIA images that were not included in the 101 images used for self-supervised training, and we generate 100 synthetic images based on the segmentations of these four. Despite the model never encountering images generated from these four segmentations during training, we observe a clear separation between the projected embeddings corresponding to each segmentation which shows that during SSL the U-Net encoder was able to learn distinct features separating these images.</p>
</div>
<figure id="S4.F10.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:216.8pt;"><img src="/html/2403.15121/assets/figures/results/TSNE_Res.png" id="S4.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="388" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Visualization of embeddings of 100 different images projected onto a 2D plane with T-SNE from 4 different validation segmentations. Each coloured dot represents a synthetic image generated from a distinct segmentation.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Full U-Net fine-tuning</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Figure <a href="#S4.F11" title="Figure 11 ‣ 4.2.2 Full U-Net fine-tuning ‣ 4.2 SimCLR ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> presents the metrics of the U-Net models with different SSL approaches, fine-tuned on the synthetic BrainVISA dataset with no frozen layers (i.e., all encoder weights were initialized with those calculated during SSL and then updated during downstream training). We observe a statistically significant increase in the Dice score metrics with SSL, with the best values for the VIA11 dataset obtained when pre-trained on VIA11 compared to no SSL (p-value=0.000799). However, we find no statistically significant difference in HD when comparing No SSL and VIA11 SSL for the VIA11 dataset. It appears that SSL with a small and homogeneous dataset like BrainVISA does not contribute to the increase of the model’s generalizability and robustness, while SSL with a bigger and more diverse VIA11 dataset leads to an increase in the Dice scores without degrading HD. Therefore, in our following experiments, we consider the model pre-trained with VIA11 SSL and fine-tuned with synthetic BrainVISA data as our best-performing model.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/SST_FullFineTUne.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="313" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>DSC and HD for models with SSL based on different datasets with subsequent fine-tuning of the full encoder and decoder on both original datasets. A statistically significant difference (p-value <math id="S4.F11.2.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.F11.2.m1.1b"><mo id="S4.F11.2.m1.1.1" xref="S4.F11.2.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.F11.2.m1.1c"><lt id="S4.F11.2.m1.1.1.cmml" xref="S4.F11.2.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.2.m1.1d">&lt;</annotation></semantics></math> 0.005) was observed when comparing No SST with the VIA SST in terms of DSC on the VIA11 dataset.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>U-Net fine-tuning with the frozen encoder</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Azizi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> highlights the importance of careful fine-tuning when learning downstream tasks to preserve the information learned during self-supervision. We investigate the impact of freezing the encoder model after SSL and thus completely preserving the SSL features for learning the CS segmentation task. Figure <a href="#S4.F12" title="Figure 12 ‣ 4.2.3 U-Net fine-tuning with the frozen encoder ‣ 4.2 SimCLR ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> compares the models with and without SSL, evaluating whether freezing the encoder during the downstream task benefits the CS segmentation performance. The results show a significant decrease in performance for both datasets and on both evaluation metrics when the encoder is frozen.</p>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/SST_FrozenNotComp.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="277" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>DSC and HD for models without SSL and SSL on the VIA11 dataset with frozen and not frozen encoder on both datasets.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Multi-task SSL</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As we can see in Figure <a href="#S4.F13" title="Figure 13 ‣ 4.3 Multi-task SSL ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> there are no statistically significant improvements in any of the metrics for the multi-task learning scenario. Due to computational limitations, we conducted experiments with SSL and evaluation solely on the VIA11 data, focusing primarily on the adaptability of our model to diverse and unseen datasets. Although no improvements were observed, we note that this strategy did not substantially degrade our results therefore such an outcome could be a result of a poor hyper-parameters selection.</p>
</div>
<figure id="S4.F13" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/SST_Segm.png" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>DSC and HD for models trained without SSL, with VIA SSL and with multi-task VIA11 SSL and tested on the VIA11 dataset.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with BrainVISA</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To evaluate the effectiveness of our approach and compare it with the state-of-the-art BrainVISA’s pipeline, we conducted several experiments using 165 VIA11 images from the held-out test set. These images were not utilized during any stage of the pre-training or data synthesis. We have obtained manual ground truth CS segmentations by correcting the initial BrainVISA’s pipeline output for them, as it was deemed necessary during our initial quality assessment of the BrainVISA’s results.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Figure <a href="#S4.F14" title="Figure 14 ‣ 4.4 Comparison with BrainVISA ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> presents a comparison between our best model that is based on VIA11 SST with the further fine-tuning on the synthetic BrainVISA data. We see that BrainVISA’s segmentations have a much higher Dice score.</p>
</div>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/Bvisa_SSTMT_comparison_quant.png" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>DSC and HD scores of the BrainVISA segmentations and our U-Net model trained with VIA11 SSL. Our model shows a statistically significant decrease in the HD scores with a p-value of 0.0379 based on a two-sample T-test.</figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">This substantial difference in DSC can be attributed to two main factors. First, the manual segmentations are essentially modifications of BrainVISA results, and in many cases, the initial BrainVISA estimate, if sufficiently accurate, was left unchanged. Second, the nature of the segmentations generated by our algorithm is characterized by thicker segmentation ribbons as can be seen in Figure <a href="#S4.F15" title="Figure 15 ‣ 4.4 Comparison with BrainVISA ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, which are anatomically and morphologically correct but receive lower Dice scores due to its voxel-wise intersection evaluation. However, our approach shows a statistically significant improvement in the HD (BrainVISA’s mean 8.315 vs our model’s 7.37 with p-value <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><lt id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">&lt;</annotation></semantics></math> 0.005).</p>
</div>
<figure id="S4.F15" class="ltx_figure"><img src="/html/2403.15121/assets/figures/my_bvisa_qual.png" id="S4.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Segmentation examples. a) produced by BrainVISA software, b) produced by our VIA11 SST U-Net model, c) manually corrected ground truth. Yellow arrows indicate gaps in the segmentation ribbon that can lead to holes in the resulting mesh interfering with morphological features calculation.</figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Considering our ultimate goal of evaluating morphological features of the CS, an essential step in their analysis is meshing and subsequent extraction of shape features. The BrainVISA software provides built-in tools for meshing sulci segmentations. We utilized these tools to compute meshes for the segmentations obtained from the BrainVISA pipeline, manually corrected segmentations, and segmentations produced by our best model. Figure <a href="#S4.F16" title="Figure 16 ‣ 4.4 Comparison with BrainVISA ‣ 4 Results ‣ SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> displays correlation plots between the volume and surface area calculated from these three meshes. It is immediately apparent that the morphological features of volume and surface area calculated from our segmentations exhibit a close correlation with those calculated from the manual segmentations.</p>
</div>
<figure id="S4.F16" class="ltx_figure"><img src="/html/2403.15121/assets/figures/results/VolAreaCorrelSquare.png" id="S4.F16.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Volume and surface area of the meshes calculated based on the manually corrected segmentation, BrainVISA’s and ours (predicted by the VIA11 SST U-Net) plotted against each other.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we have presented and evaluated various approaches for training deep learning models to perform central sulcus segmentation. Our review of the current state-of-the-art approaches revealed important limitations in models trained on small and restricted labelled datasets, which fail to account for the neuroanatomical variability of cortical morphology. We have emphasized the need for robust and automatic segmentation models and proposed novel frameworks to address this challenge. Our frameworks focus on two key ideas: efficient utilization of limited labelled data through artificial simulation of cortical variability in synthetic images and the creation of a pipeline for adapting the model to new subject populations through self-supervised learning of cortex morphology features.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Discussion of results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Firstly, we explored the use of synthetic data to train more robust models. Our findings indicate that models trained with synthetic data exhibit significantly lower Hausdorff distance (HD) scores on the VIA11 dataset, despite never being exposed to it during training. This dataset consists of a different subject population, with different image contrasts, and quality compared to the training dataset. This promising result highlights the potential of using synthetic data to simulate the morphological variability of cortex present in diverse subject populations that is beneficial for sulci localization. This approach helps address the limitations posed by small datasets, which have historically hindered progress in sulci segmentation research.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We tested the effectiveness of a self-supervised learning framework based on SimCLR combined with synthetic data for learning unique and distinct representations of cortex morphology. Our experiments demonstrate that our pre-training strategy for the U-Net encoder leads to improved DSC for the VIA11 dataset. This shows that with this pipeline we can adapt our segmentation model to new datasets without requiring any labels for them, effectively transferring information about cortex shape variability from the new dataset to our model. Consequently, this approach shows that we can improve the segmentation results for new cohorts by performing SSL on just the intensity images, paving the way to utilizing abundant unlabelled datasets that are openly available for the training of foundation models that better capture real-world anatomical variability of the cortex.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Although the multi-task SSL approach did not substantially improve our results, we believe that more careful pre-training of both the encoder and decoder models could yield better performance. We did not extensively experiment with hyper-parameter tuning for the multi-task framework and subsequent fine-tuning, which could explain the lack of improvement. Additionally, the substantial difference between GM segmentation and CS segmentation may have hindered the adaptability of the SSL loss for CS segmentation.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Lastly, we compared our best model with the current state-of-the-art pipeline from BrainVISA and demonstrated comparable performance with an improvement in the HD metric. To validate the correctness of the morphological structures represented by these segmentations, we constructed meshes based on them. The meshes built from segmentations produced by our models have highly correlated surface area and volume measures to those obtained from manual ground truths, indicating that the developed approach can be effectively utilized for CS segmentation that can be further used for analyzing the shape properties of the central sulcus.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Limitations and future work</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">The objective of this study was to explore and establish a proof of concept for training robust CS segmentation models directly from intensity images, without the need for extensive pre- or post-processing steps. We aimed to address the challenges commonly encountered in the medical imaging domain, including the limited availability of labelled data and the high morphological variability of the target structure.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">However, it is important to acknowledge the limitations of our work. We conducted evaluations on only one external dataset (VIA11) in addition to the training dataset (BrainVISA). To obtain a more comprehensive understanding of the pipeline’s performance and robustness, further evaluations on additional datasets with diverse population cohorts are necessary. For instance, testing the model on datasets consisting of elderly individuals with neurodegenerative processes resulting in substantial brain atrophy or infants and young children with either under-developed or over-developed cortical gyrification could provide valuable insights.</p>
</div>
<div id="S5.SS1.SSS1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.p3.1" class="ltx_p">In our synthetic data generation process, we utilized a limited set of artificial images due to storage and computational constraints. To enhance the diversity of the synthetic dataset, implementing an online generation procedure with unlimited and unique images for each generation could be explored. Additionally, we were unable to extensively experiment and determine an optimal set of transformation parameters for the SynthSeg Generative model. We believe that exhaustive tuning of spatial and intensity parameters applied to the images can further increase the dataset’s diversity. Moreover, incorporating brain images without skull stripping could improve the model’s robustness to potential artefacts that can occur if it is performed with errors as well as potentially eliminate the need for this pre-processing step.</p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.p4.1" class="ltx_p">In the SSL training stage, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> demonstrated that large batch size leads to better performance. Although longer training can partially mitigate the effects of smaller batch sizes, we did not specifically study how batch size affects our SSL stage. Moreover, in the multi-task SSL setting, we did not explore weighting schemes for the contrastive and segmentation losses due to time and GPU constraints, despite studies suggesting potential benefits <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib33" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="S5.SS1.SSS1.p5" class="ltx_para">
<p id="S5.SS1.SSS1.p5.1" class="ltx_p">Finally, we have not experimented with different DL architectures for our base segmentation model, although there are many new architectures based on Transformers that show promising results in the medical image segmentation field <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> as well as U-Net variations. We have chosen to use a simple and lightweight U-Net model that made possible an extensive exploration of the proposed solutions given our computational constraints.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Conclusions</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Synthetic data generation and self-supervised learning are two powerful tools that can address challenges in the development and deployment of DL models for recognition and segmentation tasks. In this study, we have demonstrated that by employing synthetic data within a self-supervised learning framework that enables the model to learn unique cortical morphology representations, we can achieve results that are comparable to state-of-the-art methods in central sulcus segmentation. These approaches alleviate the need for costly and error-prone pre-processing steps, allowing the training of robust and generalizable DL models that can be adapted to new cohorts without requiring any ground truth labels and work efficiently even with little available training data.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">First and foremost, I would like to express gratitude to my supervisor, Kristoffer Madsen, for his guidance and invaluable support throughout this project. I would also like to express my thanks to my colleagues at DRCMR, Enedino Hernández-Torres and Line K. Johnsen, for their assistance with data access, quality estimations, and manual segmentations of the sulci data. I am also grateful to Melissa Larsen and the centre’s director Hartwig R. Siebner, for helping me better understand the neurobiological basis of this project as well as for providing me with a great opportunity to work at DRCMR.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et al. (2021)</span>
<span class="ltx_bibblock">
Azizi, S., Mustafa, B.,
Ryan, F., Beaver, Z.,
Freyberg, J., Deaton, J.,
Loh, A., Karthikesalingam, A.,
Kornblith, S., Chen, T., et al.,
2021.

</span>
<span class="ltx_bibblock">Big self-supervised models advance medical image
classification, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp.
3478–3488.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Behnke et al. (2003)</span>
<span class="ltx_bibblock">
Behnke, K.J., Rettmann, M.E.,
Pham, D.L., Shen, D.,
Resnick, S.M., Davatzikos, C.,
Prince, J.L., 2003.

</span>
<span class="ltx_bibblock">Automatic classification of sulcal regions of the
human brain cortex using pattern recognition, in:
Medical imaging 2003: Image processing,
SPIE. pp. 1499–1510.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Billot et al. (2020)</span>
<span class="ltx_bibblock">
Billot, B., Greve, D.,
Van Leemput, K., Fischl, B.,
Iglesias, J.E., Dalca, A.V.,
2020.

</span>
<span class="ltx_bibblock">A learning strategy for contrast-agnostic mri
segmentation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2003.01995 .

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Billot et al. (2023a)</span>
<span class="ltx_bibblock">
Billot, B., Greve, D.N.,
Puonti, O., Thielscher, A.,
Van Leemput, K., Fischl, B.,
Dalca, A.V., Iglesias, J.E.,
2023a.

</span>
<span class="ltx_bibblock">Synthseg: Segmentation of brain MRI scans of any
contrast and resolution without retraining.

</span>
<span class="ltx_bibblock">Medical Image Analysis 86,
102789.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/j.media.2023.102789" title="" class="ltx_ref">10.1016/j.media.2023.102789</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Billot et al. (2023b)</span>
<span class="ltx_bibblock">
Billot, B., Magdamo, C.,
Cheng, Y., Arnold, S.E.,
Das, S., Iglesias, J.E.,
2023b.

</span>
<span class="ltx_bibblock">Robust machine learning segmentation for large-scale
analysis of heterogeneous clinical brain mri datasets.

</span>
<span class="ltx_bibblock">Proceedings of the National Academy of Sciences
120, e2216399120.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1073/pnas.2216399120" title="" class="ltx_ref">10.1073/pnas.2216399120</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borne et al. (2020)</span>
<span class="ltx_bibblock">
Borne, L., Rivière, D.,
Mancip, M., Mangin, J.F.,
2020.

</span>
<span class="ltx_bibblock">Automatic labeling of cortical sulci using patch- or
CNN-based segmentation techniques combined with bottom-up geometric
constraints.

</span>
<span class="ltx_bibblock">Medical Image Analysis 62,
101651.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/j.media.2020.101651" title="" class="ltx_ref">10.1016/j.media.2020.101651</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brainvisa (2019)</span>
<span class="ltx_bibblock">
Brainvisa, 2019.

</span>
<span class="ltx_bibblock">Sulci database.

</span>
<span class="ltx_bibblock">Online.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burton et al. (2023)</span>
<span class="ltx_bibblock">
Burton, B.K., Krantz, M.F.,
Skovgaard, L.T., Brandt, J.M.,
Gregersen, M., Søndergaard, A.,
Knudsen, C.B., Andreassen, A.K.,
Veddum, L., Rohd, S.B.,
Wilms, M., Tjott, C.,
Hjorthøj, C., Ohland, J.,
Greve, A., Hemager, N.,
Bliksted, V.F., Mors, O.,
Plessen, K.J., Thorup, A.A.E.,
Nordentoft, M., 2023.

</span>
<span class="ltx_bibblock">Impaired motor development in children with familial
high risk of schizophrenia or bipolar disorder and the association with
psychotic experiences: a 4-year danish observational follow-up study.

</span>
<span class="ltx_bibblock">The Lancet Psychiatry 10,
108–118.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/s2215-0366(22)00402-3" title="" class="ltx_ref">10.1016/s2215-0366(22)00402-3</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burton et al. (2017)</span>
<span class="ltx_bibblock">
Burton, B.K., Thorup, A.A.E.,
Jepsen, J.R., Poulsen, G.,
Ellersgaard, D., Spang, K.S.,
Christiani, C.J., Hemager, N.,
Gantriis, D., Greve, A.,
Mors, O., Nordentoft, M.,
Plessen, K.J., 2017.

</span>
<span class="ltx_bibblock">Impairments of motor function among children with a
familial risk of schizophrenia or bipolar disorder at 7 years old in denmark:
an observational cohort study.

</span>
<span class="ltx_bibblock">The Lancet Psychiatry 4,
400–408.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/s2215-0366(17)30103-7" title="" class="ltx_ref">10.1016/s2215-0366(17)30103-7</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cardoso et al. (2022)</span>
<span class="ltx_bibblock">
Cardoso, M.J., Li, W.,
Brown, R., Ma, N.,
Kerfoot, E., Wang, Y.,
Murrey, B., Myronenko, A.,
Zhao, C., Yang, D.,
Nath, V., He, Y., Xu,
Z., Hatamizadeh, A., Myronenko, A.,
Zhu, W., Liu, Y., Zheng,
M., Tang, Y., Yang, I.,
Zephyr, M., Hashemian, B.,
Alle, S., Darestani, M.Z.,
Budd, C., Modat, M.,
Vercauteren, T., Wang, G.,
Li, Y., Hu, Y., Fu, Y.,
Gorman, B., Johnson, H.,
Genereaux, B., Erdal, B.S.,
Gupta, V., Diaz-Pinto, A.,
Dourson, A., Maier-Hein, L.,
Jaeger, P.F., Baumgartner, M.,
Kalpathy-Cramer, J., Flores, M.,
Kirby, J., Cooper, L.A.D.,
Roth, H.R., Xu, D.,
Bericat, D., Floca, R.,
Zhou, S.K., Shuaib, H.,
Farahani, K., Maier-Hein, K.H.,
Aylward, S., Dogra, P.,
Ourselin, S., Feng, A.,
2022.

</span>
<span class="ltx_bibblock">Monai: An open-source framework for deep learning in
healthcare.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caulo et al. (2007)</span>
<span class="ltx_bibblock">
Caulo, M., Briganti, C.,
Mattei, P., Perfetti, B.,
Ferretti, A., Romani, G.,
Tartaro, A., Colosimo, C.,
2007.

</span>
<span class="ltx_bibblock">New morphologic variants of the hand motor cortex as
seen with MR imaging in a large study population.

</span>
<span class="ltx_bibblock">American Journal of Neuroradiology
28, 1480–1485.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3174/ajnr.a0597" title="" class="ltx_ref">10.3174/ajnr.a0597</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen, T., Kornblith, S.,
Norouzi, M., Hinton, G.,
2020.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual
representations, in: International conference on machine
learning, PMLR. pp. 1597–1607.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chlap et al. (2021)</span>
<span class="ltx_bibblock">
Chlap, P., Min, H.,
Vandenberg, N., Dowling, J.,
Holloway, L., Haworth, A.,
2021.

</span>
<span class="ltx_bibblock">A review of medical image data augmentation
techniques for deep learning applications.

</span>
<span class="ltx_bibblock">Journal of Medical Imaging and Radiation Oncology
65, 545–563.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1111/1754-9485.13261" title="" class="ltx_ref">https://doi.org/10.1111/1754-9485.13261</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Çiçek et al. (2016)</span>
<span class="ltx_bibblock">
Çiçek, Ö., Abdulkadir, A.,
Lienkamp, S.S., Brox, T.,
Ronneberger, O., 2016.

</span>
<span class="ltx_bibblock">3d u-net: learning dense volumetric segmentation from
sparse annotation, in: Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2016: 19th International Conference,
Athens, Greece, October 17-21, 2016, Proceedings, Part II 19,
Springer. pp. 424–432.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clarisse et al. (1997)</span>
<span class="ltx_bibblock">
Clarisse, J., Pertuzon, B.,
Ayachi, M., Francke, J., et al.,
1997.

</span>
<span class="ltx_bibblock">Identification of the central sulcus using the
scanner and mri.

</span>
<span class="ltx_bibblock">Journal of Neuroradiology= Journal de
Neuroradiologie 24, 187–204.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collins et al. (1994)</span>
<span class="ltx_bibblock">
Collins, D.L., Neelin, P.,
Peters, T.M., Evans, A.C.,
1994.

</span>
<span class="ltx_bibblock">Automatic 3d intersubject registration of mr
volumetric data in standardized talairach space.

</span>
<span class="ltx_bibblock">Journal of computer assisted tomography
18, 192–205.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desikan et al. (2006)</span>
<span class="ltx_bibblock">
Desikan, R.S., Ségonne, F.,
Fischl, B., Quinn, B.T.,
Dickerson, B.C., Blacker, D.,
Buckner, R.L., Dale, A.M.,
Maguire, R.P., Hyman, B.T.,
Albert, M.S., Killiany, R.J.,
2006.

</span>
<span class="ltx_bibblock">An automated labeling system for subdividing the
human cerebral cortex on MRI scans into gyral based regions of interest.

</span>
<span class="ltx_bibblock">NeuroImage 31,
968–980.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/j.neuroimage.2006.01.021" title="" class="ltx_ref">10.1016/j.neuroimage.2006.01.021</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dominic et al. (2023)</span>
<span class="ltx_bibblock">
Dominic, J., Bhaskhar, N.,
Desai, A.D., Schmidt, A.,
Rubin, E., Gunel, B.,
Gold, G.E., Hargreaves, B.A.,
Lenchik, L., Boutin, R.,
Chaudhari, A.S., 2023.

</span>
<span class="ltx_bibblock">Improving data-efficiency and robustness of medical
imaging segmentation using inpainting-based self-supervised learning.

</span>
<span class="ltx_bibblock">Bioengineering 10.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3390/bioengineering10020207" title="" class="ltx_ref">10.3390/bioengineering10020207</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrari et al. (2016)</span>
<span class="ltx_bibblock">
Ferrari, A.J., Stockings, E.,
Khoo, J.P., Erskine, H.E.,
Degenhardt, L., Vos, T.,
Whiteford, H.A., 2016.

</span>
<span class="ltx_bibblock">The prevalence and burden of bipolar disorder:
findings from the global burden of disease study 2013.

</span>
<span class="ltx_bibblock">Bipolar Disorders 18,
440–450.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1111/bdi.12423" title="" class="ltx_ref">10.1111/bdi.12423</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fischl et al. (1999)</span>
<span class="ltx_bibblock">
Fischl, B., Sereno, M.I.,
Dale, A., 1999.

</span>
<span class="ltx_bibblock">Cortical surface-based analysis: Ii: Inflation,
flattening, and a surface-based coordinate system.

</span>
<span class="ltx_bibblock">NeuroImage 9,
195 – 207.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Gao, F., Yoon, H., Wu,
T., Chu, X., 2020.

</span>
<span class="ltx_bibblock">A feature transfer enabled multi-task deep learning
model on medical imaging.

</span>
<span class="ltx_bibblock">Expert Systems with Applications
143, 112957.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.eswa.2019.112957" title="" class="ltx_ref">https://doi.org/10.1016/j.eswa.2019.112957</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hesamian et al. (2019)</span>
<span class="ltx_bibblock">
Hesamian, M.H., Jia, W.,
He, X., Kennedy, P.,
2019.

</span>
<span class="ltx_bibblock">Deep learning techniques for medical image
segmentation: Achievements and challenges.

</span>
<span class="ltx_bibblock">Journal of Digital Imaging 32,
582–596.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1007/s10278-019-00227-x" title="" class="ltx_ref">10.1007/s10278-019-00227-x</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Huang, S.C., Pareek, A.,
Jensen, M., Lungren, M.P.,
Yeung, S., Chaudhari, A.S.,
2023.

</span>
<span class="ltx_bibblock">Self-supervised learning for medical image
classification: a systematic review and implementation guidelines.

</span>
<span class="ltx_bibblock">npj Digital Medicine 6.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1038/s41746-023-00811-0" title="" class="ltx_ref">10.1038/s41746-023-00811-0</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huntgeburth and Petrides (2012)</span>
<span class="ltx_bibblock">
Huntgeburth, S.C., Petrides, M.,
2012.

</span>
<span class="ltx_bibblock">Morphological patterns of the collateral sulcus in
the human brain.

</span>
<span class="ltx_bibblock">European Journal of Neuroscience
35, 1295–1311.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1111/j.1460-9568.2012.08031.x" title="" class="ltx_ref">https://doi.org/10.1111/j.1460-9568.2012.08031.x</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iglesias et al. (2021)</span>
<span class="ltx_bibblock">
Iglesias, J.E., Billot, B.,
Balbastre, Y., Tabari, A.,
Conklin, J., González, R.G.,
Alexander, D.C., Golland, P.,
Edlow, B.L., Fischl, B., et al.,
2021.

</span>
<span class="ltx_bibblock">Joint super-resolution and synthesis of 1 mm
isotropic mp-rage volumes from clinical mri exams with scans of different
orientation, resolution and contrast.

</span>
<span class="ltx_bibblock">Neuroimage 237,
118206.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaiswal et al. (2020)</span>
<span class="ltx_bibblock">
Jaiswal, A., Babu, A.R.,
Zadeh, M.Z., Banerjee, D.,
Makedon, F., 2020.

</span>
<span class="ltx_bibblock">A survey on contrastive self-supervised learning.

</span>
<span class="ltx_bibblock">Technologies 9,
2.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jensen (2016)</span>
<span class="ltx_bibblock">
Jensen, B., 2016.

</span>
<span class="ltx_bibblock">Influence of Maturation, Pathology and Functional
Lateralization on 3D Sulcal Morphology using MRI.

</span>
<span class="ltx_bibblock">Ph.D. thesis. Technical University of Denmark, DTU Compute.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kao et al. (2007)</span>
<span class="ltx_bibblock">
Kao, C.Y., Hofer, M.,
Sapiro, G., Stern, J.,
Rehm, K., Rottenberg, D.A.,
2007.

</span>
<span class="ltx_bibblock">A geometric method for automatic extraction of sulcal
fundi.

</span>
<span class="ltx_bibblock">IEEE transactions on medical imaging
26, 530–540.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein et al. (2014)</span>
<span class="ltx_bibblock">
Klein, D., Rotarska-Jagiela, A.,
Genc, E., Sritharan, S.,
Mohr, H., Roux, F., Han,
C.E., Kaiser, M., Singer, W.,
Uhlhaas, P.J., 2014.

</span>
<span class="ltx_bibblock">Adolescent brain maturation and cortical folding:
Evidence for reductions in gyrification.

</span>
<span class="ltx_bibblock">PLoS ONE 9,
e84914.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1371/journal.pone.0084914" title="" class="ltx_ref">10.1371/journal.pone.0084914</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kochunov et al. (2005)</span>
<span class="ltx_bibblock">
Kochunov, P., Mangin, J.F.,
Coyle, T., Lancaster, J.,
Thompson, P., Rivière, D.,
Cointepas, Y., Régis, J.,
Schlosser, A., Royall, D.R.,
Zilles, K., Mazziotta, J.,
Toga, A., Fox, P.T.,
2005.

</span>
<span class="ltx_bibblock">Age-related morphology trends of cortical sulci.

</span>
<span class="ltx_bibblock">Human Brain Mapping 26,
210–220.

</span>
<span class="ltx_bibblock">URL: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1002/hbm.20198</span>,
doi:<a target="_blank" href="https:/doi.org/10.1002/hbm.20198" title="" class="ltx_ref">10.1002/hbm.20198</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kochunov et al. (2011)</span>
<span class="ltx_bibblock">
Kochunov, P., Rogers, W.,
Mangin, J.F., Lancaster, J.,
2011.

</span>
<span class="ltx_bibblock">A library of cortical morphology analysis tools to
study development, aging and genetics of cerebral cortex.

</span>
<span class="ltx_bibblock">Neuroinformatics 10,
81–96.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1007/s12021-011-9127-9" title="" class="ltx_ref">10.1007/s12021-011-9127-9</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al. (2015)</span>
<span class="ltx_bibblock">
Leroy, F., Cai, Q.,
Bogart, S.L., Dubois, J.,
Coulon, O., Monzalvo, K.,
Fischer, C., Glasel, H.,
der Haegen, L.V., Bénézit, A.,
Lin, C.P., Kennedy, D.N.,
Ihara, A.S., Hertz-Pannier, L.,
Moutard, M.L., Poupon, C.,
Brysbaert, M., Roberts, N.,
Hopkins, W.D., Mangin, J.F.,
Dehaene-Lambertz, G., 2015.

</span>
<span class="ltx_bibblock">New human-specific brain landmark: The depth
asymmetry of superior temporal sulcus.

</span>
<span class="ltx_bibblock">Proceedings of the National Academy of Sciences
112, 1208–1213.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1073/pnas.1412389112" title="" class="ltx_ref">10.1073/pnas.1412389112</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021a)</span>
<span class="ltx_bibblock">
Lin, B., Ye, F., Zhang,
Y., 2021a.

</span>
<span class="ltx_bibblock">A closer look at loss weighting in multi-task
learning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2111.10603 .

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021b)</span>
<span class="ltx_bibblock">
Lin, H.Y., Huang, C.C.,
Chou, K.H., Yang, A.C.,
Lo, C.Y.Z., Tsai, S.J.,
Lin, C.P., 2021b.

</span>
<span class="ltx_bibblock">Differential patterns of gyral and sulcal
morphological changes during normal aging process.

</span>
<span class="ltx_bibblock">Frontiers in Aging Neuroscience
13.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3389/fnagi.2021.625931" title="" class="ltx_ref">10.3389/fnagi.2021.625931</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Liu, X., Song, L., Liu,
S., Zhang, Y., 2021.

</span>
<span class="ltx_bibblock">A review of deep-learning-based medical image
segmentation methods.

</span>
<span class="ltx_bibblock">Sustainability 13,
1224.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3390/su13031224" title="" class="ltx_ref">10.3390/su13031224</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2021)</span>
<span class="ltx_bibblock">
Lyu, I., Bao, S., Hao,
L., Yao, J., Miller, J.A.,
Voorhies, W., Taylor, W.D.,
Bunge, S.A., Weiner, K.S.,
Landman, B.A., 2021.

</span>
<span class="ltx_bibblock">Labeling lateral prefrontal sulci using spherical
data augmentation and context-aware training.

</span>
<span class="ltx_bibblock">NeuroImage 229,
117758.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.neuroimage.2021.117758" title="" class="ltx_ref">https://doi.org/10.1016/j.neuroimage.2021.117758</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
van der Maaten, L., Hinton, G.,
2008.

</span>
<span class="ltx_bibblock">Visualizing data using t-sne.

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research
9, 2579–2605.

</span>
<span class="ltx_bibblock">URL: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://jmlr.org/papers/v9/vandermaaten08a.html</span>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangin et al. (1995)</span>
<span class="ltx_bibblock">
Mangin, J.F., Frouin, V.,
Bloch, I., Rogis, J.,
Lopez-Krahe, J., 1995.

</span>
<span class="ltx_bibblock">From 3d magnetic resonance images to structural
representations of the cortex topography using topology preserving
deformations.

</span>
<span class="ltx_bibblock">Journal of Mathematical Imaging and Vision
5, 297–318.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1007/bf01250286" title="" class="ltx_ref">10.1007/bf01250286</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McConnell (1995)</span>
<span class="ltx_bibblock">
McConnell, S.K., 1995.

</span>
<span class="ltx_bibblock">Constructing the cerebral cortex: neurogenesis and
fate determination.

</span>
<span class="ltx_bibblock">Neuron 15,
761–768.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Millier et al. (2014)</span>
<span class="ltx_bibblock">
Millier, A., Schmidt, U.,
Angermeyer, M., Chauhan, D.,
Murthy, V., Toumi, M.,
Cadi-Soussi, N., 2014.

</span>
<span class="ltx_bibblock">Humanistic burden in schizophrenia: A literature
review.

</span>
<span class="ltx_bibblock">Journal of Psychiatric Research
54, 85–93.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/j.jpsychires.2014.03.021" title="" class="ltx_ref">10.1016/j.jpsychires.2014.03.021</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ochiai et al. (2004)</span>
<span class="ltx_bibblock">
Ochiai, T., Grimault, S.,
Scavarda, D., Roch, G.,
Hori, T., Rivière, D.,
Mangin, J.F., Régis, J.,
2004.

</span>
<span class="ltx_bibblock">Sulcal pattern and morphology of the superior
temporal sulcus.

</span>
<span class="ltx_bibblock">NeuroImage 22,
706–719.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.neuroimage.2004.01.023" title="" class="ltx_ref">https://doi.org/10.1016/j.neuroimage.2004.01.023</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord et al. (2018)</span>
<span class="ltx_bibblock">
Oord, A.v.d., Li, Y.,
Vinyals, O., 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive
coding.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1807.03748 .

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perrot et al. (2011)</span>
<span class="ltx_bibblock">
Perrot, M., Rivière, D.,
Mangin, J.F., 2011.

</span>
<span class="ltx_bibblock">Cortical sulci recognition and spatial
normalization.

</span>
<span class="ltx_bibblock">Medical Image Analysis 15,
529–550.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.media.2011.02.008" title="" class="ltx_ref">https://doi.org/10.1016/j.media.2011.02.008</a>.
special section on IPMI 2009.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puonti et al. (2016)</span>
<span class="ltx_bibblock">
Puonti, O., Iglesias, J.E.,
Van Leemput, K., 2016.

</span>
<span class="ltx_bibblock">Fast and sequence-adaptive whole-brain segmentation
using parametric bayesian modeling.

</span>
<span class="ltx_bibblock">NeuroImage 143,
235–249.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.neuroimage.2016.09.011" title="" class="ltx_ref">https://doi.org/10.1016/j.neuroimage.2016.09.011</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson and Bergen (2021)</span>
<span class="ltx_bibblock">
Robinson, N., Bergen, S.E.,
2021.

</span>
<span class="ltx_bibblock">Environmental risk factors for schizophrenia and
bipolar disorder and their relationship to genetic risk: Current knowledge
and future directions.

</span>
<span class="ltx_bibblock">Frontiers in Genetics 12.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3389/fgene.2021.686666" title="" class="ltx_ref">10.3389/fgene.2021.686666</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roell et al. (2021)</span>
<span class="ltx_bibblock">
Roell, M., Cachia, A.,
Matejko, A., Houdé, O.,
Ansari, D., Borst, G.,
2021.

</span>
<span class="ltx_bibblock">Sulcation of the intraparietal sulcus is related to
symbolic but not non-symbolic number skills.

</span>
<span class="ltx_bibblock">Developmental Cognitive Neuroscience
51, 100998.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.dcn.2021.100998" title="" class="ltx_ref">https://doi.org/10.1016/j.dcn.2021.100998</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohlfing et al. (2004)</span>
<span class="ltx_bibblock">
Rohlfing, T., Brandt, R.,
Menzel, R., Maurer Jr, C.R.,
2004.

</span>
<span class="ltx_bibblock">Evaluation of atlas selection strategies for
atlas-based image segmentation with application to confocal microscopy images
of bee brains.

</span>
<span class="ltx_bibblock">NeuroImage 21,
1428–1442.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salehi et al. (2017)</span>
<span class="ltx_bibblock">
Salehi, S.S.M., Erdogmus, D.,
Gholipour, A., 2017.

</span>
<span class="ltx_bibblock">Tversky loss function for image segmentation using 3d
fully convolutional deep networks, in: Wang, Q.,
Shi, Y., Suk, H.I.,
Suzuki, K. (Eds.), Machine Learning in
Medical Imaging, Springer International Publishing,
Cham. pp. 379–387.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schindler and Dellaert (2004)</span>
<span class="ltx_bibblock">
Schindler, G., Dellaert, F.,
2004.

</span>
<span class="ltx_bibblock">Atlanta world: An expectation maximization framework
for simultaneous low-level edge grouping and camera calibration in complex
man-made environments, in: Proceedings of the 2004 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, 2004.
CVPR 2004., IEEE. pp. I–I.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2007)</span>
<span class="ltx_bibblock">
Shi, Y., Tu, Z., Reiss,
A.L., Dutton, R.A., Lee, A.D.,
Galaburda, A.M., Dinov, I.,
Thompson, P.M., Toga, A.W.,
2007.

</span>
<span class="ltx_bibblock">Joint sulci detection using graphical models and
boosted priors, in: IPMI, pp. 98–109.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorup et al. (2018)</span>
<span class="ltx_bibblock">
Thorup, A.A.E., Hemager, N.,
Søndergaard, A., Gregersen, M.,
Prøsch, Å.K., Krantz, M.F.,
Brandt, J.M., Carmichael, L.,
Melau, M., Ellersgaard, D.V.,
Burton, B.K., Greve, A.N.,
Uddin, M.J., Ohland, J.,
Nejad, A.B., Johnsen, L.K.,
van Themaat, A.H.V.L., Andreassen, A.K.,
Vedum, L., Knudsen, C.B.,
Stadsgaard, H., Jepsen, J.R.M.,
Siebner, H.R., Østergaard, L.,
Bliksted, V.F., Plessen, K.J.,
Mors, O., Nordentoft, M.,
2018.

</span>
<span class="ltx_bibblock">The danish high risk and resilience
study—VIA 11: Study protocol for the first follow-up of the
VIA 7 cohort -522 children born to parents with schizophrenia spectrum
disorders or bipolar disorder and controls being re-examined for the first
time at age 11.

</span>
<span class="ltx_bibblock">Frontiers in Psychiatry 9.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.3389/fpsyt.2018.00661" title="" class="ltx_ref">10.3389/fpsyt.2018.00661</a>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorup et al. (2015)</span>
<span class="ltx_bibblock">
Thorup, A.A.E., Jepsen, J.R.,
Ellersgaard, D.V., Burton, B.K.,
Christiani, C.J., Hemager, N.,
Skjærbæk, M., Ranning, A.,
Spang, K.S., Gantriis, D.L.,
Greve, A.N., Zahle, K.K.,
Mors, O., Plessen, K.J.,
Nordentoft, M., 2015.

</span>
<span class="ltx_bibblock">The danish high risk and resilience study
– VIA 7 - a cohort study of 520 7-year-old children born of
parents diagnosed with either schizophrenia, bipolar disorder or neither of
these two mental disorders.

</span>
<span class="ltx_bibblock">BMC Psychiatry 15.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1186/s12888-015-0616-5" title="" class="ltx_ref">10.1186/s12888-015-0616-5</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vivodtzev et al. (2003)</span>
<span class="ltx_bibblock">
Vivodtzev, F., Linsen, L.,
Bonneau, G.P., Hamann, B.,
Joy, K., Olshausen, B.A.,
2003.

</span>
<span class="ltx_bibblock">Hierachical isosurface segmentation based on discrete
curvature.

</span>
<span class="ltx_bibblock">UC Davis: Institute for Data Analysis and
Visualization .

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al. (2010)</span>
<span class="ltx_bibblock">
White, T., Su, S.,
Schmidt, M., Kao, C.Y.,
Sapiro, G., 2010.

</span>
<span class="ltx_bibblock">The development of gyrification in childhood and
adolescence.

</span>
<span class="ltx_bibblock">Brain and Cognition 72,
36–45.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1016/j.bandc.2009.10.009" title="" class="ltx_ref">10.1016/j.bandc.2009.10.009</a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Willbrand et al. (2022)</span>
<span class="ltx_bibblock">
Willbrand, E.H., Parker, B.J.,
Voorhies, W.I., Miller, J.A.,
Lyu, I., Hallock, T.,
Aponik-Gremillion, L., Koslov, S.R.,
Bunge, S.A., Foster, B.L.,
and, K.S.W., 2022.

</span>
<span class="ltx_bibblock">Uncovering a tripartite landmark in posterior
cingulate cortex.

</span>
<span class="ltx_bibblock">Science Advances 8.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1126/sciadv.abn9516" title="" class="ltx_ref">10.1126/sciadv.abn9516</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Kruggel (2008)</span>
<span class="ltx_bibblock">
Yang, F., Kruggel, F.,
2008.

</span>
<span class="ltx_bibblock">Automatic segmentation of human brain sulci.

</span>
<span class="ltx_bibblock">Medical Image Analysis 12,
442–451.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.media.2008.01.003" title="" class="ltx_ref">https://doi.org/10.1016/j.media.2008.01.003</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Yang, J., Wang, D.,
Rollins, C., Leming, M.,
Liò, P., Suckling, J.,
Murray, G., Garrison, J.,
Cachia, A., 2019.

</span>
<span class="ltx_bibblock">Volumetric segmentation and characterisation of the
paracingulate sulcus on MRI scans.

</span>
<span class="ltx_bibblock">bioRxiv doi:<a target="_blank" href="https:/doi.org/10.1101/859496" title="" class="ltx_ref">10.1101/859496</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yaniv et al. (2017)</span>
<span class="ltx_bibblock">
Yaniv, Z., Lowekamp, B.C.,
Johnson, H.J., Beare, R.,
2017.

</span>
<span class="ltx_bibblock">SimpleITK image-analysis notebooks: a collaborative
environment for education and reproducible research.

</span>
<span class="ltx_bibblock">Journal of Digital Imaging 31,
290–303.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1007/s10278-017-0037-8" title="" class="ltx_ref">10.1007/s10278-017-0037-8</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2021)</span>
<span class="ltx_bibblock">
Zeng, D., Kheir, J.N.,
Zeng, P., Shi, Y., 2021.

</span>
<span class="ltx_bibblock">Contrastive learning with temporal correlated medical
images: A case study using lung segmentation in chest x-rays (invited
paper), in: 2021 IEEE/ACM International Conference On
Computer Aided Design (ICCAD), pp. 1–7.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Zhang, Z., Wang, Y., Gao,
Y., Li, Z., Zhang, S.,
Lin, X., Hou, Z., Yu,
Q., Wang, X., Liu, S.,
2020.

</span>
<span class="ltx_bibblock">Morphological changes in the central sulcus of
children with isolated growth hormone deficiency versus idiopathic short
stature.

</span>
<span class="ltx_bibblock">Developmental Neurobiology 81,
36–46.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/10.1002/dneu.22797" title="" class="ltx_ref">10.1002/dneu.22797</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2021)</span>
<span class="ltx_bibblock">
Zhou, Y., Chen, H., Li,
Y., Liu, Q., Xu, X.,
Wang, S., Yap, P.T.,
Shen, D., 2021.

</span>
<span class="ltx_bibblock">Multi-task learning for segmentation and
classification of tumors in 3d automated breast ultrasound images.

</span>
<span class="ltx_bibblock">Medical Image Analysis 70,
101918.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.media.2020.101918" title="" class="ltx_ref">https://doi.org/10.1016/j.media.2020.101918</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.15120" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.15121" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.15121">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.15121" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.15122" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:29:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
