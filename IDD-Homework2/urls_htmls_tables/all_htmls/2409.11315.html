<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.11315] fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction</title><meta property="og:description" content="Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work [1], is of significant interest to both cognitive neuroscience and computer vision. To …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.11315">

<!--Generated on Sat Oct  5 20:33:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">
fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu<sup id="id9.3.id1" class="ltx_sup">†</sup>
</span><span class="ltx_author_notes"><sup id="id10.4.id1" class="ltx_sup">†</sup>: Corresponding author.
Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng and Yanwei Fu are with Fudan University. Yanwei Fu is also with Fudan ISTBI—ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang Normal University, Jinhua, China.
E-mail: jxgao22@m.fudan.edu.cn, yanweifu@fudan.edu.cn.
Dr. Yuqian Fu is now with ETH Zürich and INSAIT.
Dr. Xuelin Qian is now with Northwestern Polytechnical University.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p"><span id="id11.id1.1" class="ltx_text">Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as <span id="id11.id1.1.1" class="ltx_text ltx_font_bold">Recon3DMind</span> in our conference work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the <span id="id11.id1.1.2" class="ltx_text ltx_font_bold">fMRI-3D</span> dataset, which includes data from 15 participants and showcases a total of 4,768 3D objects. The dataset comprises two components: <span id="id11.id1.1.3" class="ltx_text ltx_font_bold">fMRI-Shape</span>, previously introduced and accessible at <a target="_blank" href="https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape</a>, and <span id="id11.id1.1.4" class="ltx_text ltx_font_bold">fMRI-Objaverse</span>, proposed in this paper and available at <a target="_blank" href="https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse</a>. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3,142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset.
Additionally, we propose <span id="id11.id1.1.5" class="ltx_text ltx_font_bold">MinD-3D</span>, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model’s effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: <a target="_blank" href="https://jianxgao.github.io/MinD-3D" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jianxgao.github.io/MinD-3D</a>.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id12.id1" class="ltx_text">
FMRI decoding, 3D vision, Dataset, Diffusion model.
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Functional Magnetic Resonance Imaging (fMRI), a kind of signal that can be obtained in a non-invasive way, could capture blood changes in the human brain induced by neuronal activity. Due to its relatively easy accessibility, fMRI has been commonly used to reflect visual activities.
Some recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have successfully reconstructed high-quality images from fMRI signals by utilizing powerful generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
These approaches focus on extracting semantic features from fMRI signals, often requiring only semantic features to generate relevant high-quality images.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.11315/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Overview of Recon3DMind task, showcasing the fMRI-3D dataset collection process with 15 participants observing 360-degree view videos of 3D objects, and MinD-3D framework for reconstructing 3D objects from fMRI signals.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing methods primarily focus on reconstructing 2D visual information, but the human visual system extends far beyond merely processing flat images. It possesses the extraordinary ability to transform 2D projections into rich 3D representations. This complex mechanism allows us to perceive the world in depth, recognizing attributes like size, distance, and spatial depth.
In contrast to previous studies, our research centers on modeling the brain’s 3D visual capabilities. We introduce a new task, called <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Recon3DMind</span> (<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">Recon</span>structing <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">3D</span> Objects from <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">Mind</span>), which leverages advanced computer vision techniques to decode and reconstruct the 3D visual information perceived by the brain from fMRI signals. This task goes beyond merely extracting semantic features, incorporating spatial and structural dimensions that are essential for a comprehensive understanding of 3D vision.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Several studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have demonstrated that the brain’s mechanisms for 3D visual perception are significantly more intricate than those for 2D perception. This complexity is reflected in the distinct activation of brain regions during 3D visualization tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. As a result, relying solely on semantic features is insufficient to fully model the brain’s capacity for 3D spatial perception.
Effectively describing 3D objects requires taking into account not only their semantic features but also their shape and structural properties. For instance, two cars may appear identical when viewed head-on, yet differ greatly in length when viewed from the side. This example underscores the importance of capturing the full range of spatial and structural features to authentically represent 3D objects.
Accordingly, our work seeks to advance the modeling of human 3D perception by developing an enhanced fMRI feature extractor. This extractor is designed to capture semantic elements, spatial structures and other 3D-specific characteristics from fMRI signals. This approach aims to enable a more complete and accurate reconstruction of 3D visual information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In our conference work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we introduced the fMRI-Shape dataset to tackle the significant challenge of the lack of datasets pairing fMRI data with 3D visuals for this complex task. The dataset comprises data from 14 participants and 1,624 3D objects. However, the Core set was limited to just 13 categories of 3D objects. To address this limitation in category diversity and to expand the number of objects, we propose fMRI-Objaverse, which includes data from 5 participants and 3,142 3D objects across 117 categories, accompanied by text captions. Notably, it shares 4 participants with the Core set of fMRI-Shape, significantly enhancing the diversity of fMRI-Shape, as shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We collectively refer to these two datasets as <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">fMRI-3D</span>, aiming to support various experimental setups and further promote research within the community.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">During fMRI data collection, we present 3D objects through 360-degree view videos, providing comprehensive visualizations that stimulate the brain’s perception of 3D objects and facilitate the collection of high-quality data. In our approach, participants watch 360-degree videos of stationary 3D objects from ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, where a rotating camera completes a full orbit around each object, offering a complete view from all angles. This method ensures detailed and accurate capture of fMRI signals, as participants engage with the objects, allowing for the full range of spatial features to be recorded. As shown in Fig.<a href="#S2.F3" title="Figure 3 ‣ 2.3 3D Generation ‣ 2 Related work ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we also analyze the variation in fMRI data across both subjects and objects. Interestingly, the variation across subjects is even greater than that across objects. After careful preprocessing, these recordings are transformed into multi-frame fMRI signals, resulting in a rich dataset for detailed analysis. The complexities and specific features of the fMRI-3D dataset will be discussed further in Sec. <a href="#S3" title="3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2409.11315/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span id="S1.F2.2.1" class="ltx_text ltx_font_bold">Statistical Overview of fMRI-3D.</span> It displays the number of instances for each object category in the Core Set of the fMRI-Shape and fMRI-Objaverse datasets. fMRI-Objaverse significantly complements fMRI-Shape by offering a wider range of categories

</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Leveraging our carefully curated fMRI-3D dataset, we introduce an innovative and efficient three-stage framework called MinD-3D to extract both spatial structure and semantic features from multi-frame fMRI signals and reconstruct the corresponding 3D visual stimuli.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In the first stage, we utilize an encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> pre-trained on NSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> dataset to extract spatial features from the fMRI data. These features are then aggregated across multiple frames using a feature aggregation module. To ensure the biological relevance and effectiveness of these extracted features, we align them with the visual-spatial characteristics of the corresponding objects using contrastive learning loss. This alignment ensures that the extracted features are both accurate and biologically meaningful in relation to the brain’s visual processing mechanisms.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The second stage focuses on decoding the brain’s visual activity from the fMRI data. We achieve this through a transformer-based diffusion model trained in feature space, conditioned on the fMRI-derived features. This conditioning enables us to progressively generate accurate visual representations, translating complex brain activity into understandable visual data.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">In the final stage, we focus on reconstructing the 3D models as they are perceived by the human brain. To do this, we train a latent-adapted Argus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> model, which uses the visual features generated in the second stage along with the initial fMRI features to create 3D objects. This process mimics how the brain’s visual cortex operates, providing a simulated insight into the brain’s perception and processing of 3D objects.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">To measure the effectiveness of our model, we design new benchmarks that evaluate its performance at both the semantic and structural levels. These metrics comprehensively assess the model’s ability to generate 3D representations that are structurally and semantically accurate. We evaluate our model under both standard and Out-of-Distribution settings, and it consistently outperforms baseline models. Additionally, we conduct in-depth analyses of our fMRI-3D dataset and the features extracted by MinD-3D, exploring how the brain perceives different angles, objects, and semantic information within specific ROIs (regions of interest). We further validate the relevance of our model’s features by correlating them with brain regions, confirming that the representations it produces are consistent with the brain’s visual information processing.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">This paper expands upon our preliminary conference work, and we summarize the key contributions as follows:</p>
</div>
<div id="S1.p12" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">fMRI-Objaverse</span>, a large-scale extension of the original dataset. Together with fMRI-Shape, we collectively refer to these datasets as fMRI-3D, designed to support various experimental setups and advance research in the field.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We establish new benchmarks for the task of 3D visual reconstruction from human brain data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct extensive experiments to analyze the contributions of our proposed dataset in decoding fMRI signals, further validating the effectiveness of <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">MinD-3D</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">fMRI Decoding Methods</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Current fMRI decoding methods primarily focus on reconstructing the vision perception in a 2D format, such as the images or videos perceived by humans. This is a challenging task, as it involves extracting relevant features from fMRI signals with precision to recreate accurate 2D representations. Deep learning methods, known for their impressive capabilities, are particularly suited to address this challenge. Initial successes in this area have been demonstrated by earlier methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Subsequent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have shown that generative models are particularly effective for these tasks, leading to the employment of various diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as decoders to reconstruct visual scenes, achieving remarkable results. However, these studies have been limited to 2D visual representations and the related vision ROIs. In this paper, we aim to extend the scope of fMRI visual decoding to 3D representations, involving more vision ROIs. Our goal is to directly reconstruct 3D objects from fMRI signals. To accomplish this, we propose a new framework that employs a transformer-based feature encoder for extraction and aggregation. This framework translates neural space data into visual space and utilizes a powerful 3D decoder to reconstruct the 3D object, leveraging features from the visual space.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Diffusion Models</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> are exceptional generative tools for both pixel and feature generation. As a variant, the latent diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, equipped with an autoencoder, compresses images into lower-dimensional latent features, thereby generating a compressed version of the data rather than directly generating the data itself. Dit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> replaces the backbone of diffusion models with transformers, which will improve the performance and scalability of these models. This approach, operating in the latent space, significantly reduces computational requirements and enables the generation of higher-quality images with enhanced details in the latent space. In this paper, we aim to leverage the potent feature-generation capabilities of diffusion models to generate visual features based on fMRI features. To achieve this, we adapt a transformer-based diffusion model, focusing solely on its latent component. The conditional information driving the model is derived from the fMRI features.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">3D Generation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">3D generation can be accomplished through various methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Some methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> employ 3D Gaussian splatting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> for this purpose.
Other studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> utilize diffusion models to generate multi-view representations of objects, subsequently constructing 3D models. Additionally, traditional and direct approaches leverage autoregressive methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for 3D object generation.
In our study, we adapt Argus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a robust 3D generative model with several transformer layers, as our decoder to generate 3D objects from fMRI data. This approach integrates visual features generated by the preceding diffusion module. These visual features serve as conditional embeddings for Argus. This synergistic integration aims to enhance the model’s ability to accurately reconstruct 3D objects from complex brain activity.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.11315/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span id="S2.F3.2.1" class="ltx_text ltx_font_bold">Individual differences in brain activation patterns within the fMRI-3D dataset.</span> In our dataset fMRI-3D, the variation in brain activity across different participants viewing the same object is greater than the variation when the same participant views different objects. Red and blue regions represent areas with higher values for variation across subjects and variation across objects, respectively.

</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2409.11315/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S2.F4.2.1" class="ltx_text ltx_font_bold">Comparing fMRI-3D with other 2D fMRI datasets.</span> As the first 3D fMRI dataset, fMRI-3D features a larger number of participants and frames, providing ample support for experiments in our proposed novel task and further research.

</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Designs and Curated Dataset</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we detail the procedures for collecting the proposed fMRI-3D dataset, which consists of two components: fMRI-Shape and fMRI-Objaverse. The scale of fMRI-3D is compared to other benchmark datasets, including NSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, BOLD5000 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, GOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and Video-fMRI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, as illustrated in Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.3 3D Generation ‣ 2 Related work ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Specific details about fMRI-Shape and fMRI-Objaverse are provided in Tab. <a href="#S3.T1" title="Table I ‣ 3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. For all experiments, written informed consent was obtained from each participant, and the study was approved by the ethical review board.
To better illustrate brain activation patterns and demonstrate the utility of the fMRI-3D dataset, we analyze and visualize responses to three distinct objects across six subjects, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 3D Generation ‣ 2 Related work ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Note that only voxels with activation levels above the 50th percentile are displayed. We also compute the variation across subjects and objects, with red and blue regions indicating higher activation values, respectively, reflecting areas in the human brain sensitive to the stimuli. This visualization highlights significant individual differences in brain activation across subjects, which are more pronounced than the variations in responses to different objects. These findings emphasize the inherent challenges and underscore the importance of the AP and APAC settings.
All participants had normal or corrected-to-normal vision. The fMRI-3D dataset will be made publicly available to support further research in Recon3DMind.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table I: </span>
<span id="S3.T1.2.1" class="ltx_text ltx_font_bold">Details of fMRI-3D Dataset.</span> The dataset ensures a balanced representation of male and female participants and includes both the fMRI-Shape and fMRI-Objaverse datasets. The fMRI-Shape dataset is composed of three distinct subsets: the Across-Person Set (AP Set), which contains fMRI data from different participants for core testing, and the Across-Person &amp; Across-Class Set (APAC Set), where different participants view new 3D objects, both designed to facilitate model generalization evaluations. The fMRI-Objaverse dataset extends fMRI-Shape, featuring four of the same participants viewing a wider variety of 3D objects from Objaverse, accompanied by text captions. 
</figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<th id="S3.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;"></th>
<th id="S3.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">Participant</th>
<th id="S3.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">Males/Females</th>
<th id="S3.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">Category</th>
<th id="S3.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">Objects</th>
<th id="S3.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">Frames</th>
</tr>
<tr id="S3.T1.3.2.2" class="ltx_tr">
<th id="S3.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">fMRI-Shape</th>
<th id="S3.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">14</th>
<th id="S3.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">7/7</th>
<th id="S3.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">55</th>
<th id="S3.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">1624</th>
<th id="S3.T1.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.8pt;padding-right:12.8pt;">123200</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.3.1" class="ltx_tr">
<th id="S3.T1.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">     Core Set</th>
<th id="S3.T1.3.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">8</th>
<th id="S3.T1.3.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">4/4</th>
<td id="S3.T1.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">13</td>
<td id="S3.T1.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">1404</td>
<td id="S3.T1.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">14040</td>
</tr>
<tr id="S3.T1.3.4.2" class="ltx_tr">
<th id="S3.T1.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">     AP Set</th>
<th id="S3.T1.3.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">2</th>
<th id="S3.T1.3.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">1/1</th>
<td id="S3.T1.3.4.2.4" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">13</td>
<td id="S3.T1.3.4.2.5" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">104</td>
<td id="S3.T1.3.4.2.6" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">1040</td>
</tr>
<tr id="S3.T1.3.5.3" class="ltx_tr">
<th id="S3.T1.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">     APAC Set</th>
<th id="S3.T1.3.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">4</th>
<th id="S3.T1.3.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:12.8pt;padding-right:12.8pt;">2/2</th>
<td id="S3.T1.3.5.3.4" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">55</td>
<td id="S3.T1.3.5.3.5" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">220</td>
<td id="S3.T1.3.5.3.6" class="ltx_td ltx_align_center" style="padding-left:12.8pt;padding-right:12.8pt;">2200</td>
</tr>
<tr id="S3.T1.3.6.4" class="ltx_tr">
<th id="S3.T1.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">fMRI-Objaverse</th>
<th id="S3.T1.3.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">5</th>
<th id="S3.T1.3.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">2/3</th>
<td id="S3.T1.3.6.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">117</td>
<td id="S3.T1.3.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">3142</td>
<td id="S3.T1.3.6.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">125680</td>
</tr>
<tr id="S3.T1.3.7.5" class="ltx_tr">
<th id="S3.T1.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">fMRI-3D (Total)</th>
<th id="S3.T1.3.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">15</th>
<th id="S3.T1.3.7.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">7/8</th>
<td id="S3.T1.3.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">172</td>
<td id="S3.T1.3.7.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">4768</td>
<td id="S3.T1.3.7.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:12.8pt;padding-right:12.8pt;">248880</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">fMRI-Shape</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">FMRI-Shape contains data from 14 participants who were unaware of the objectives of the work.
To ensure diversity in the dataset, the 3D objects were sourced from ShapeNetCore <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which includes 55 object categories. We employed the rendering technique from Zero123 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to render 192 images using Blender and generated 8-second videos at 24 fps for each object.
These videos depict the 3D objects rotating 360 degrees at a 60-degree pitch angle, as illustrated in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.2 fMRI-Objaverse ‣ 3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The dataset is available for download at:
<a target="_blank" href="https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape</a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">1) Core Set:</span>
The core set of fMRI-Shape includes data from 8 participants (4 males and 4 females, aged 21 to 29, Participants No. 1-8). A total of 1,404 objects were selected from 13 commonly used categories in 3D reconstruction literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> within ShapeNetCore. For each category, 100 objects were used for training and 8 for testing, resulting in 108 objects per category. For more details, please refer to our conference version <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">2) Across-Person Set (AP Set):</span>
The AP Set was designed for Out-of-Distribution (OOD) testing and includes fMRI data from 2 participants (1 male aged 24 and 1 female aged 26, Participants No. 9 and 10) who viewed the test objects from the Core set.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">3) Across-Person &amp; Across-Class Set (APAC Set):</span>
The APAC Set presents a more challenging OOD test compared to the AP Set. It includes data from 4 participants (2 males and 2 females, aged 22 to 26, Participants No. 11-14). For this set, we randomly selected 4 objects from each of the 55 categories in ShapeNetCore, distinct from those in the Core set, resulting in a total of 220 objects.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">As illustrated in the middle part of Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.2 fMRI-Objaverse ‣ 3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, individual differences among participants pose significant challenges for generalization. The AP and APAC sets are crucial for OOD testing and will serve as important benchmarks for assessing the generalization capability of 3D decoding models.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">fMRI-Objaverse</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">FMRI-Objaverse, as partially shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 fMRI-Objaverse ‣ 3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, includes data from five participants, all of whom were unfamiliar with the study. Four participants (2 males and 2 females, aged 22 to 26, identified as Nos. 1, 6, 7, and 8) overlap with the core fMRI-Shape dataset, providing an important extension in terms of diversity and scale. Additionally, the fifth participant (No. 15), a 22-year-old female, was included to further expand the dataset.
To enhance our dataset, we selected 3,142 objects from the top 117 object categories in Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, based on a subset filtered by LGM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and enriched with text descriptions from Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Unlike in fMRI-Shape, each 3D object in this dataset was rendered into 384 frames, generating a 6.4-second video at 48 fps using Blender.
Each participant spent approximately 8 hours in experimental sessions, divided into 53 sessions. During each session, participants viewed 60 videos in a randomized order, except for the last session, with 1.6-second rest intervals between each pair of objects. To prevent low-quality data due to visual fatigue, we randomly reversed the rotation direction for 40% of the selected objects. All objects were presented once to each participant. (Note: Our MRI machine samples data every 800ms, so we selected 6.4-second videos with a 1.6-second rest period between them.)
This extension supports further multimodal experiments and applications. The objects in Objaverse contain more detail and a wider variety of categories, and the higher fps videos present stronger visual effects, posing a significant challenge for reconstructing them in fMRI-Objaverse. The dataset is available for download at: <a target="_blank" href="https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse</a>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2409.11315/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span id="S3.F5.2.1" class="ltx_text ltx_font_bold">Individual differences in brain activation patterns within the fMRI-Objaverse dataset.</span> In our extensive fMRI-Objaverse dataset, the variation in brain activity across different participants viewing the same object is highly pronounced. Red regions represent areas with higher levels of variation across subjects.

</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2409.11315/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span id="S3.F6.7.1" class="ltx_text ltx_font_bold">Overview of the fMRI-3D Acquisition Process.</span> Initially, we render each object into an 8-second long video, showcasing a 360-degree view. Subsequent fMRI signal capture is performed in video format, followed by data processing with fMRIPrep to convert signals from 32k_fs_LR surface space into 2D images of dimensions 1023 <math id="S3.F6.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.F6.3.m1.1b"><mo id="S3.F6.3.m1.1.1" xref="S3.F6.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F6.3.m1.1c"><times id="S3.F6.3.m1.1.1.cmml" xref="S3.F6.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.3.m1.1d">\times</annotation></semantics></math> 2514. <span id="S3.F6.8.2" class="ltx_text ltx_font_bold">Individual differences</span> observed in the dataset, as highlighted in the middle part, underscore the challenges in generalizing these findings. On the rights, regions of interest (ROIs) are transformed into 256 <math id="S3.F6.4.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.F6.4.m2.1b"><mo id="S3.F6.4.m2.1.1" xref="S3.F6.4.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.F6.4.m2.1c"><times id="S3.F6.4.m2.1.1.cmml" xref="S3.F6.4.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.4.m2.1d">\times</annotation></semantics></math> 256 image.
</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">Data Acquisition and Preprocessing</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">T1 and fMRI data were acquired in a 3T scanner and a 32-channel RF head coil. T1-weighted data were scanned using MPRAGE sequence (0.8-mm isotropic resolution, TR=2500ms, TE=2.22ms, flip angle <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="8^{\circ}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msup id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">8</mn><mo id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">8</cn><compose id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">8^{\circ}</annotation></semantics></math>). Functional data were scanned using gradient-echo EPI at 2-mm isotropic resolution with whole-brain coverage (TR=800ms, TE=37ms, flip angle <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="52^{\circ}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msup id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">52</mn><mo id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">52</cn><compose id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">52^{\circ}</annotation></semantics></math>, multi-band acceleration factor 8). The sampling frequency of the 3T scanner is 1.25Hz, so each video segment corresponds to a total of 10 frames of task-state fMRI signals.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Stimuli were presented using an LCD screen (<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="8^{\circ}\times 8^{\circ}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><msup id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">8</mn><mo id="S3.SS3.p2.1.m1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.2.3.cmml">∘</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">×</mo><msup id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mn id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">8</mn><mo id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">8</cn><compose id="S3.SS3.p2.1.m1.1.1.2.3.cmml" xref="S3.SS3.p2.1.m1.1.1.2.3"></compose></apply><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">8</cn><compose id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">8^{\circ}\times 8^{\circ}</annotation></semantics></math>) positioned at the head of the scanner bed. Participants viewed the monitor via a mirror mounted on the RF coil and fixated a red central dot (0.4° × 0.4°).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Preprocessing was performed using fMRIPrep <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, the preprocessed functional data in 32k_fs_LR surface space were converted into 2D images and utilized for further analysis. Given the delay of the BOLD signal by 6 seconds, we applied z-scoring to the data points across every vertex within each run, incorporating a 6.4-second lag. These normalized values were then projected onto 1023 × 2514 pixel 2D images using pycortex. For analysis, Regions of Interest (ROIs) were selected from the Human Connectome Project Multi-Modal Parcellation (HCP-MMP) atlas in the 32k_fs_LR space. These ROIs included areas such as “V1, V2, V3, V3A, V3B, V3CD, V4, LO1, LO2, LO3, PIT, V4t, V6, V6A, V7, V8, PH, FFC, IP0, MT, MST, FST, VVC, VMV1, VMV2, VMV3, PHA1, PHA2, PHA3”. Subsequently, the ROIs were converted into a 256 × 256 image, as illustrated in the right part of Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.2 fMRI-Objaverse ‣ 3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Problem Setup</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Recon3DMind tackles a critical challenge in cognitive neuroscience: developing computational models that can accurately interpret and reconstruct the brain’s 3D visual comprehension. This endeavor not only bridges the gap between cognitive neuroscience and computer vision but also has the potential to advance the latter field in unprecedented ways. In this paper, we focus on the specifics of fMRI-based 3D reconstruction, providing detailed definitions and formulas that underpin our approach.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.11" class="ltx_p">We begin with the acquisition of a multi-frame fMRI signal, denoted as <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\{F\}" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.2.2" xref="S4.p2.1.m1.1.2.1.cmml"><mo stretchy="false" id="S4.p2.1.m1.1.2.2.1" xref="S4.p2.1.m1.1.2.1.cmml">{</mo><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">F</mi><mo stretchy="false" id="S4.p2.1.m1.1.2.2.2" xref="S4.p2.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><set id="S4.p2.1.m1.1.2.1.cmml" xref="S4.p2.1.m1.1.2.2"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝐹</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\{F\}</annotation></semantics></math>, where <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="|F|=n" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.2" xref="S4.p2.2.m2.1.2.cmml"><mrow id="S4.p2.2.m2.1.2.2.2" xref="S4.p2.2.m2.1.2.2.1.cmml"><mo stretchy="false" id="S4.p2.2.m2.1.2.2.2.1" xref="S4.p2.2.m2.1.2.2.1.1.cmml">|</mo><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">F</mi><mo stretchy="false" id="S4.p2.2.m2.1.2.2.2.2" xref="S4.p2.2.m2.1.2.2.1.1.cmml">|</mo></mrow><mo id="S4.p2.2.m2.1.2.1" xref="S4.p2.2.m2.1.2.1.cmml">=</mo><mi id="S4.p2.2.m2.1.2.3" xref="S4.p2.2.m2.1.2.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.2.cmml" xref="S4.p2.2.m2.1.2"><eq id="S4.p2.2.m2.1.2.1.cmml" xref="S4.p2.2.m2.1.2.1"></eq><apply id="S4.p2.2.m2.1.2.2.1.cmml" xref="S4.p2.2.m2.1.2.2.2"><abs id="S4.p2.2.m2.1.2.2.1.1.cmml" xref="S4.p2.2.m2.1.2.2.2.1"></abs><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝐹</ci></apply><ci id="S4.p2.2.m2.1.2.3.cmml" xref="S4.p2.2.m2.1.2.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">|F|=n</annotation></semantics></math> (with <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="n=8" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">n</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><eq id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></eq><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝑛</ci><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">n=8</annotation></semantics></math> or <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="n=10" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">n</mi><mo id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><eq id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></eq><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">𝑛</ci><cn type="integer" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">n=10</annotation></semantics></math>). These signals correspond to both a 3D object mesh, <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="\Psi" display="inline"><semantics id="S4.p2.5.m5.1a"><mi mathvariant="normal" id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml">Ψ</mi><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><ci id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1">Ψ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">\Psi</annotation></semantics></math>, and a video, <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="\{V\}" display="inline"><semantics id="S4.p2.6.m6.1a"><mrow id="S4.p2.6.m6.1.2.2" xref="S4.p2.6.m6.1.2.1.cmml"><mo stretchy="false" id="S4.p2.6.m6.1.2.2.1" xref="S4.p2.6.m6.1.2.1.cmml">{</mo><mi id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml">V</mi><mo stretchy="false" id="S4.p2.6.m6.1.2.2.2" xref="S4.p2.6.m6.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><set id="S4.p2.6.m6.1.2.1.cmml" xref="S4.p2.6.m6.1.2.2"><ci id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1">𝑉</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">\{V\}</annotation></semantics></math>, with <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="|V|=k" display="inline"><semantics id="S4.p2.7.m7.1a"><mrow id="S4.p2.7.m7.1.2" xref="S4.p2.7.m7.1.2.cmml"><mrow id="S4.p2.7.m7.1.2.2.2" xref="S4.p2.7.m7.1.2.2.1.cmml"><mo stretchy="false" id="S4.p2.7.m7.1.2.2.2.1" xref="S4.p2.7.m7.1.2.2.1.1.cmml">|</mo><mi id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">V</mi><mo stretchy="false" id="S4.p2.7.m7.1.2.2.2.2" xref="S4.p2.7.m7.1.2.2.1.1.cmml">|</mo></mrow><mo id="S4.p2.7.m7.1.2.1" xref="S4.p2.7.m7.1.2.1.cmml">=</mo><mi id="S4.p2.7.m7.1.2.3" xref="S4.p2.7.m7.1.2.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><apply id="S4.p2.7.m7.1.2.cmml" xref="S4.p2.7.m7.1.2"><eq id="S4.p2.7.m7.1.2.1.cmml" xref="S4.p2.7.m7.1.2.1"></eq><apply id="S4.p2.7.m7.1.2.2.1.cmml" xref="S4.p2.7.m7.1.2.2.2"><abs id="S4.p2.7.m7.1.2.2.1.1.cmml" xref="S4.p2.7.m7.1.2.2.2.1"></abs><ci id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">𝑉</ci></apply><ci id="S4.p2.7.m7.1.2.3.cmml" xref="S4.p2.7.m7.1.2.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">|V|=k</annotation></semantics></math> (where <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="k=192" display="inline"><semantics id="S4.p2.8.m8.1a"><mrow id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml"><mi id="S4.p2.8.m8.1.1.2" xref="S4.p2.8.m8.1.1.2.cmml">k</mi><mo id="S4.p2.8.m8.1.1.1" xref="S4.p2.8.m8.1.1.1.cmml">=</mo><mn id="S4.p2.8.m8.1.1.3" xref="S4.p2.8.m8.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><apply id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1"><eq id="S4.p2.8.m8.1.1.1.cmml" xref="S4.p2.8.m8.1.1.1"></eq><ci id="S4.p2.8.m8.1.1.2.cmml" xref="S4.p2.8.m8.1.1.2">𝑘</ci><cn type="integer" id="S4.p2.8.m8.1.1.3.cmml" xref="S4.p2.8.m8.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">k=192</annotation></semantics></math> or <math id="S4.p2.9.m9.1" class="ltx_Math" alttext="k=384" display="inline"><semantics id="S4.p2.9.m9.1a"><mrow id="S4.p2.9.m9.1.1" xref="S4.p2.9.m9.1.1.cmml"><mi id="S4.p2.9.m9.1.1.2" xref="S4.p2.9.m9.1.1.2.cmml">k</mi><mo id="S4.p2.9.m9.1.1.1" xref="S4.p2.9.m9.1.1.1.cmml">=</mo><mn id="S4.p2.9.m9.1.1.3" xref="S4.p2.9.m9.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.9.m9.1b"><apply id="S4.p2.9.m9.1.1.cmml" xref="S4.p2.9.m9.1.1"><eq id="S4.p2.9.m9.1.1.1.cmml" xref="S4.p2.9.m9.1.1.1"></eq><ci id="S4.p2.9.m9.1.1.2.cmml" xref="S4.p2.9.m9.1.1.2">𝑘</ci><cn type="integer" id="S4.p2.9.m9.1.1.3.cmml" xref="S4.p2.9.m9.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m9.1c">k=384</annotation></semantics></math> frames), which the subject observes. The task requires an efficient encoder, <math id="S4.p2.10.m10.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.p2.10.m10.1a"><mi id="S4.p2.10.m10.1.1" xref="S4.p2.10.m10.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.p2.10.m10.1b"><ci id="S4.p2.10.m10.1.1.cmml" xref="S4.p2.10.m10.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.10.m10.1c">E</annotation></semantics></math>, capable of extracting both spatial structural and semantic features from the fMRI signal. It is important to note that while a single frame of fMRI data is sufficient to extract semantic information for 2D image reconstruction, reconstructing 3D structures requires additional spatial structural features. Therefore, multiple frames of fMRI data are input to capture these comprehensive spatial features from the spatio-temporal signals. Mathematically, this is expressed as: <math id="S4.p2.11.m11.1" class="ltx_Math" alttext="f=E(F)" display="inline"><semantics id="S4.p2.11.m11.1a"><mrow id="S4.p2.11.m11.1.2" xref="S4.p2.11.m11.1.2.cmml"><mi id="S4.p2.11.m11.1.2.2" xref="S4.p2.11.m11.1.2.2.cmml">f</mi><mo id="S4.p2.11.m11.1.2.1" xref="S4.p2.11.m11.1.2.1.cmml">=</mo><mrow id="S4.p2.11.m11.1.2.3" xref="S4.p2.11.m11.1.2.3.cmml"><mi id="S4.p2.11.m11.1.2.3.2" xref="S4.p2.11.m11.1.2.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.p2.11.m11.1.2.3.1" xref="S4.p2.11.m11.1.2.3.1.cmml">​</mo><mrow id="S4.p2.11.m11.1.2.3.3.2" xref="S4.p2.11.m11.1.2.3.cmml"><mo stretchy="false" id="S4.p2.11.m11.1.2.3.3.2.1" xref="S4.p2.11.m11.1.2.3.cmml">(</mo><mi id="S4.p2.11.m11.1.1" xref="S4.p2.11.m11.1.1.cmml">F</mi><mo stretchy="false" id="S4.p2.11.m11.1.2.3.3.2.2" xref="S4.p2.11.m11.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.11.m11.1b"><apply id="S4.p2.11.m11.1.2.cmml" xref="S4.p2.11.m11.1.2"><eq id="S4.p2.11.m11.1.2.1.cmml" xref="S4.p2.11.m11.1.2.1"></eq><ci id="S4.p2.11.m11.1.2.2.cmml" xref="S4.p2.11.m11.1.2.2">𝑓</ci><apply id="S4.p2.11.m11.1.2.3.cmml" xref="S4.p2.11.m11.1.2.3"><times id="S4.p2.11.m11.1.2.3.1.cmml" xref="S4.p2.11.m11.1.2.3.1"></times><ci id="S4.p2.11.m11.1.2.3.2.cmml" xref="S4.p2.11.m11.1.2.3.2">𝐸</ci><ci id="S4.p2.11.m11.1.1.cmml" xref="S4.p2.11.m11.1.1">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.11.m11.1c">f=E(F)</annotation></semantics></math>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.5" class="ltx_p">After feature extraction, a powerful decoder is used to reconstruct the original 3D mesh, <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\Psi" display="inline"><semantics id="S4.p3.1.m1.1a"><mi mathvariant="normal" id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">Ψ</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">Ψ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\Psi</annotation></semantics></math>, based on the extracted feature <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">f</annotation></semantics></math>: <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="\Psi=D(f)" display="inline"><semantics id="S4.p3.3.m3.1a"><mrow id="S4.p3.3.m3.1.2" xref="S4.p3.3.m3.1.2.cmml"><mi mathvariant="normal" id="S4.p3.3.m3.1.2.2" xref="S4.p3.3.m3.1.2.2.cmml">Ψ</mi><mo id="S4.p3.3.m3.1.2.1" xref="S4.p3.3.m3.1.2.1.cmml">=</mo><mrow id="S4.p3.3.m3.1.2.3" xref="S4.p3.3.m3.1.2.3.cmml"><mi id="S4.p3.3.m3.1.2.3.2" xref="S4.p3.3.m3.1.2.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.2.3.1" xref="S4.p3.3.m3.1.2.3.1.cmml">​</mo><mrow id="S4.p3.3.m3.1.2.3.3.2" xref="S4.p3.3.m3.1.2.3.cmml"><mo stretchy="false" id="S4.p3.3.m3.1.2.3.3.2.1" xref="S4.p3.3.m3.1.2.3.cmml">(</mo><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">f</mi><mo stretchy="false" id="S4.p3.3.m3.1.2.3.3.2.2" xref="S4.p3.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.2.cmml" xref="S4.p3.3.m3.1.2"><eq id="S4.p3.3.m3.1.2.1.cmml" xref="S4.p3.3.m3.1.2.1"></eq><ci id="S4.p3.3.m3.1.2.2.cmml" xref="S4.p3.3.m3.1.2.2">Ψ</ci><apply id="S4.p3.3.m3.1.2.3.cmml" xref="S4.p3.3.m3.1.2.3"><times id="S4.p3.3.m3.1.2.3.1.cmml" xref="S4.p3.3.m3.1.2.3.1"></times><ci id="S4.p3.3.m3.1.2.3.2.cmml" xref="S4.p3.3.m3.1.2.3.2">𝐷</ci><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">\Psi=D(f)</annotation></semantics></math>. Thus, our model can be succinctly described as <math id="S4.p3.4.m4.2" class="ltx_Math" alttext="M=\{E,D\}" display="inline"><semantics id="S4.p3.4.m4.2a"><mrow id="S4.p3.4.m4.2.3" xref="S4.p3.4.m4.2.3.cmml"><mi id="S4.p3.4.m4.2.3.2" xref="S4.p3.4.m4.2.3.2.cmml">M</mi><mo id="S4.p3.4.m4.2.3.1" xref="S4.p3.4.m4.2.3.1.cmml">=</mo><mrow id="S4.p3.4.m4.2.3.3.2" xref="S4.p3.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S4.p3.4.m4.2.3.3.2.1" xref="S4.p3.4.m4.2.3.3.1.cmml">{</mo><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">E</mi><mo id="S4.p3.4.m4.2.3.3.2.2" xref="S4.p3.4.m4.2.3.3.1.cmml">,</mo><mi id="S4.p3.4.m4.2.2" xref="S4.p3.4.m4.2.2.cmml">D</mi><mo stretchy="false" id="S4.p3.4.m4.2.3.3.2.3" xref="S4.p3.4.m4.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.2b"><apply id="S4.p3.4.m4.2.3.cmml" xref="S4.p3.4.m4.2.3"><eq id="S4.p3.4.m4.2.3.1.cmml" xref="S4.p3.4.m4.2.3.1"></eq><ci id="S4.p3.4.m4.2.3.2.cmml" xref="S4.p3.4.m4.2.3.2">𝑀</ci><set id="S4.p3.4.m4.2.3.3.1.cmml" xref="S4.p3.4.m4.2.3.3.2"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">𝐸</ci><ci id="S4.p3.4.m4.2.2.cmml" xref="S4.p3.4.m4.2.2">𝐷</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.2c">M=\{E,D\}</annotation></semantics></math>, where the transformation is represented as <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="\Psi=M(F)" display="inline"><semantics id="S4.p3.5.m5.1a"><mrow id="S4.p3.5.m5.1.2" xref="S4.p3.5.m5.1.2.cmml"><mi mathvariant="normal" id="S4.p3.5.m5.1.2.2" xref="S4.p3.5.m5.1.2.2.cmml">Ψ</mi><mo id="S4.p3.5.m5.1.2.1" xref="S4.p3.5.m5.1.2.1.cmml">=</mo><mrow id="S4.p3.5.m5.1.2.3" xref="S4.p3.5.m5.1.2.3.cmml"><mi id="S4.p3.5.m5.1.2.3.2" xref="S4.p3.5.m5.1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.1.2.3.1" xref="S4.p3.5.m5.1.2.3.1.cmml">​</mo><mrow id="S4.p3.5.m5.1.2.3.3.2" xref="S4.p3.5.m5.1.2.3.cmml"><mo stretchy="false" id="S4.p3.5.m5.1.2.3.3.2.1" xref="S4.p3.5.m5.1.2.3.cmml">(</mo><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">F</mi><mo stretchy="false" id="S4.p3.5.m5.1.2.3.3.2.2" xref="S4.p3.5.m5.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><apply id="S4.p3.5.m5.1.2.cmml" xref="S4.p3.5.m5.1.2"><eq id="S4.p3.5.m5.1.2.1.cmml" xref="S4.p3.5.m5.1.2.1"></eq><ci id="S4.p3.5.m5.1.2.2.cmml" xref="S4.p3.5.m5.1.2.2">Ψ</ci><apply id="S4.p3.5.m5.1.2.3.cmml" xref="S4.p3.5.m5.1.2.3"><times id="S4.p3.5.m5.1.2.3.1.cmml" xref="S4.p3.5.m5.1.2.3.1"></times><ci id="S4.p3.5.m5.1.2.3.2.cmml" xref="S4.p3.5.m5.1.2.3.2">𝑀</ci><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">\Psi=M(F)</annotation></semantics></math>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.3" class="ltx_p">To effectively implement this model, it is crucial to leverage both the fMRI signals <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\{F\}" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.2.2" xref="S4.p4.1.m1.1.2.1.cmml"><mo stretchy="false" id="S4.p4.1.m1.1.2.2.1" xref="S4.p4.1.m1.1.2.1.cmml">{</mo><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">F</mi><mo stretchy="false" id="S4.p4.1.m1.1.2.2.2" xref="S4.p4.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><set id="S4.p4.1.m1.1.2.1.cmml" xref="S4.p4.1.m1.1.2.2"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">𝐹</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\{F\}</annotation></semantics></math> and the corresponding video <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\{V\}" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.2.2" xref="S4.p4.2.m2.1.2.1.cmml"><mo stretchy="false" id="S4.p4.2.m2.1.2.2.1" xref="S4.p4.2.m2.1.2.1.cmml">{</mo><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">V</mi><mo stretchy="false" id="S4.p4.2.m2.1.2.2.2" xref="S4.p4.2.m2.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><set id="S4.p4.2.m2.1.2.1.cmml" xref="S4.p4.2.m2.1.2.2"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">𝑉</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\{V\}</annotation></semantics></math> to train the model <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">M</annotation></semantics></math>. For this, we propose a three-stage, innovative, and efficient framework. Each stage is carefully designed to capture different aspects of the fMRI data and the associated visual stimuli, ensuring a comprehensive and accurate 3D reconstruction from the complex neural signals. This process not only pushes the boundaries of current computer vision techniques but also provides valuable insights into how the human brain processes 3D spatial information.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2409.11315/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span id="S4.F7.2.1" class="ltx_text ltx_font_bold">Overview of the MinD-3D Framework.</span> Our approach combines a Neuro-Fusion Encoder for extracting features from fMRI frames, a Feature Bridge Diffusion Model for generating visual features from these fMRI signals, and a Latent Adapted Decoder based on the Argus 3D shape generator for reconstructing 3D objects. This integrated system effectively aligns and translates brain signals into accurate 3D visual representations. Note that the CLIP encoder is only for training the model, while not used for inference.
</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Overview</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As depicted in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4 Problem Setup ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, our MinD-3D model comprises three main components: <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">1) Neuro-Fusion Encoder:</span> This part is responsible for extracting both semantic and structural features from the fMRI frames. <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_bold">2) Feature Bridge Diffusion Model:</span> This module generates visual features based on the features extracted by the Encoder. <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_bold">3) Latent Adapted Decoder:</span> Finally, this component recovers 3D objects, using both the fMRI features and the generated visual features.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">Neuro-Fusion Encoder</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">Drawing from the potent Masked Image Model (MIM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we adopt an auto-encoder structure to extract meaningful fMRI features, trained on the UKB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Our encoder, termed the Neuro-Fusion Encoder (NFE), employs the encoder of LEA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="E_{f}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">E</mi><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝐸</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">E_{f}</annotation></semantics></math>, along with a Feature Aggregation module (<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{FA}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">𝒜</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><times id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></times><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">ℱ</ci><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\mathcal{FA}</annotation></semantics></math>), to transform fMRI frames into embeddings. Inspired by contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, we then align these embeddings with the vision space through CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.13" class="ltx_p">Initially, we process each fMRI frame in parallel to obtain the spatial fMRI embeddings:</p>
<table id="S5.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex1.m1.1" class="ltx_Math" alttext="\mathbf{F}_{emb}=E_{f}(\mathbf{F})" display="block"><semantics id="S5.Ex1.m1.1a"><mrow id="S5.Ex1.m1.1.2" xref="S5.Ex1.m1.1.2.cmml"><msub id="S5.Ex1.m1.1.2.2" xref="S5.Ex1.m1.1.2.2.cmml"><mi id="S5.Ex1.m1.1.2.2.2" xref="S5.Ex1.m1.1.2.2.2.cmml">𝐅</mi><mrow id="S5.Ex1.m1.1.2.2.3" xref="S5.Ex1.m1.1.2.2.3.cmml"><mi id="S5.Ex1.m1.1.2.2.3.2" xref="S5.Ex1.m1.1.2.2.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.2.2.3.1" xref="S5.Ex1.m1.1.2.2.3.1.cmml">​</mo><mi id="S5.Ex1.m1.1.2.2.3.3" xref="S5.Ex1.m1.1.2.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.2.2.3.1a" xref="S5.Ex1.m1.1.2.2.3.1.cmml">​</mo><mi id="S5.Ex1.m1.1.2.2.3.4" xref="S5.Ex1.m1.1.2.2.3.4.cmml">b</mi></mrow></msub><mo id="S5.Ex1.m1.1.2.1" xref="S5.Ex1.m1.1.2.1.cmml">=</mo><mrow id="S5.Ex1.m1.1.2.3" xref="S5.Ex1.m1.1.2.3.cmml"><msub id="S5.Ex1.m1.1.2.3.2" xref="S5.Ex1.m1.1.2.3.2.cmml"><mi id="S5.Ex1.m1.1.2.3.2.2" xref="S5.Ex1.m1.1.2.3.2.2.cmml">E</mi><mi id="S5.Ex1.m1.1.2.3.2.3" xref="S5.Ex1.m1.1.2.3.2.3.cmml">f</mi></msub><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.2.3.1" xref="S5.Ex1.m1.1.2.3.1.cmml">​</mo><mrow id="S5.Ex1.m1.1.2.3.3.2" xref="S5.Ex1.m1.1.2.3.cmml"><mo stretchy="false" id="S5.Ex1.m1.1.2.3.3.2.1" xref="S5.Ex1.m1.1.2.3.cmml">(</mo><mi id="S5.Ex1.m1.1.1" xref="S5.Ex1.m1.1.1.cmml">𝐅</mi><mo stretchy="false" id="S5.Ex1.m1.1.2.3.3.2.2" xref="S5.Ex1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.1b"><apply id="S5.Ex1.m1.1.2.cmml" xref="S5.Ex1.m1.1.2"><eq id="S5.Ex1.m1.1.2.1.cmml" xref="S5.Ex1.m1.1.2.1"></eq><apply id="S5.Ex1.m1.1.2.2.cmml" xref="S5.Ex1.m1.1.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.2.2.1.cmml" xref="S5.Ex1.m1.1.2.2">subscript</csymbol><ci id="S5.Ex1.m1.1.2.2.2.cmml" xref="S5.Ex1.m1.1.2.2.2">𝐅</ci><apply id="S5.Ex1.m1.1.2.2.3.cmml" xref="S5.Ex1.m1.1.2.2.3"><times id="S5.Ex1.m1.1.2.2.3.1.cmml" xref="S5.Ex1.m1.1.2.2.3.1"></times><ci id="S5.Ex1.m1.1.2.2.3.2.cmml" xref="S5.Ex1.m1.1.2.2.3.2">𝑒</ci><ci id="S5.Ex1.m1.1.2.2.3.3.cmml" xref="S5.Ex1.m1.1.2.2.3.3">𝑚</ci><ci id="S5.Ex1.m1.1.2.2.3.4.cmml" xref="S5.Ex1.m1.1.2.2.3.4">𝑏</ci></apply></apply><apply id="S5.Ex1.m1.1.2.3.cmml" xref="S5.Ex1.m1.1.2.3"><times id="S5.Ex1.m1.1.2.3.1.cmml" xref="S5.Ex1.m1.1.2.3.1"></times><apply id="S5.Ex1.m1.1.2.3.2.cmml" xref="S5.Ex1.m1.1.2.3.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.2.3.2.1.cmml" xref="S5.Ex1.m1.1.2.3.2">subscript</csymbol><ci id="S5.Ex1.m1.1.2.3.2.2.cmml" xref="S5.Ex1.m1.1.2.3.2.2">𝐸</ci><ci id="S5.Ex1.m1.1.2.3.2.3.cmml" xref="S5.Ex1.m1.1.2.3.2.3">𝑓</ci></apply><ci id="S5.Ex1.m1.1.1.cmml" xref="S5.Ex1.m1.1.1">𝐅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.1c">\mathbf{F}_{emb}=E_{f}(\mathbf{F})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.1" class="ltx_p">Then, we use the Feature Aggregation module to aggregate the fMRI embeddings into fMRI latent feature <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝐜</ci><ci id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\mathbf{c}_{f}</annotation></semantics></math>:</p>
<table id="S5.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex2.m1.1" class="ltx_Math" alttext="\mathbf{c}_{f}=\mathcal{FA}(\mathbf{F}_{emb})" display="block"><semantics id="S5.Ex2.m1.1a"><mrow id="S5.Ex2.m1.1.1" xref="S5.Ex2.m1.1.1.cmml"><msub id="S5.Ex2.m1.1.1.3" xref="S5.Ex2.m1.1.1.3.cmml"><mi id="S5.Ex2.m1.1.1.3.2" xref="S5.Ex2.m1.1.1.3.2.cmml">𝐜</mi><mi id="S5.Ex2.m1.1.1.3.3" xref="S5.Ex2.m1.1.1.3.3.cmml">f</mi></msub><mo id="S5.Ex2.m1.1.1.2" xref="S5.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex2.m1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.3.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.2.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S5.Ex2.m1.1.1.1.4" xref="S5.Ex2.m1.1.1.1.4.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.2a" xref="S5.Ex2.m1.1.1.1.2.cmml">​</mo><mrow id="S5.Ex2.m1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.Ex2.m1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.cmml">𝐅</mi><mrow id="S5.Ex2.m1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.3.2" xref="S5.Ex2.m1.1.1.1.1.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.1.1.1.3.1" xref="S5.Ex2.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.3.3" xref="S5.Ex2.m1.1.1.1.1.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.1.1.1.1.3.1a" xref="S5.Ex2.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.3.4" xref="S5.Ex2.m1.1.1.1.1.1.1.3.4.cmml">b</mi></mrow></msub><mo stretchy="false" id="S5.Ex2.m1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1b"><apply id="S5.Ex2.m1.1.1.cmml" xref="S5.Ex2.m1.1.1"><eq id="S5.Ex2.m1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.2"></eq><apply id="S5.Ex2.m1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.3">subscript</csymbol><ci id="S5.Ex2.m1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.3.2">𝐜</ci><ci id="S5.Ex2.m1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.3.3">𝑓</ci></apply><apply id="S5.Ex2.m1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1"><times id="S5.Ex2.m1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.2"></times><ci id="S5.Ex2.m1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.3">ℱ</ci><ci id="S5.Ex2.m1.1.1.1.4.cmml" xref="S5.Ex2.m1.1.1.1.4">𝒜</ci><apply id="S5.Ex2.m1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2">𝐅</ci><apply id="S5.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3"><times id="S5.Ex2.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3.1"></times><ci id="S5.Ex2.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3.2">𝑒</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3.3">𝑚</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.3.4.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3.4">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1c">\mathbf{c}_{f}=\mathcal{FA}(\mathbf{F}_{emb})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.5" class="ltx_p">To align the fMRI latent feature with the vision space, we employ the ViT-B/32 CLIP vision encoder <math id="S5.SS2.p2.2.m1.1" class="ltx_Math" alttext="E_{v}" display="inline"><semantics id="S5.SS2.p2.2.m1.1a"><msub id="S5.SS2.p2.2.m1.1.1" xref="S5.SS2.p2.2.m1.1.1.cmml"><mi id="S5.SS2.p2.2.m1.1.1.2" xref="S5.SS2.p2.2.m1.1.1.2.cmml">E</mi><mi id="S5.SS2.p2.2.m1.1.1.3" xref="S5.SS2.p2.2.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m1.1b"><apply id="S5.SS2.p2.2.m1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m1.1.1.1.cmml" xref="S5.SS2.p2.2.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m1.1.1.2.cmml" xref="S5.SS2.p2.2.m1.1.1.2">𝐸</ci><ci id="S5.SS2.p2.2.m1.1.1.3.cmml" xref="S5.SS2.p2.2.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m1.1c">E_{v}</annotation></semantics></math> and here to encode video frames into vision embeddings. During training, we randomly select n frames as <math id="S5.SS2.p2.3.m2.1" class="ltx_Math" alttext="\{\mathbf{V}\}" display="inline"><semantics id="S5.SS2.p2.3.m2.1a"><mrow id="S5.SS2.p2.3.m2.1.2.2" xref="S5.SS2.p2.3.m2.1.2.1.cmml"><mo stretchy="false" id="S5.SS2.p2.3.m2.1.2.2.1" xref="S5.SS2.p2.3.m2.1.2.1.cmml">{</mo><mi id="S5.SS2.p2.3.m2.1.1" xref="S5.SS2.p2.3.m2.1.1.cmml">𝐕</mi><mo stretchy="false" id="S5.SS2.p2.3.m2.1.2.2.2" xref="S5.SS2.p2.3.m2.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m2.1b"><set id="S5.SS2.p2.3.m2.1.2.1.cmml" xref="S5.SS2.p2.3.m2.1.2.2"><ci id="S5.SS2.p2.3.m2.1.1.cmml" xref="S5.SS2.p2.3.m2.1.1">𝐕</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m2.1c">\{\mathbf{V}\}</annotation></semantics></math> and compute the average vision embeddings <math id="S5.SS2.p2.4.m3.1" class="ltx_Math" alttext="V_{emb}" display="inline"><semantics id="S5.SS2.p2.4.m3.1a"><msub id="S5.SS2.p2.4.m3.1.1" xref="S5.SS2.p2.4.m3.1.1.cmml"><mi id="S5.SS2.p2.4.m3.1.1.2" xref="S5.SS2.p2.4.m3.1.1.2.cmml">V</mi><mrow id="S5.SS2.p2.4.m3.1.1.3" xref="S5.SS2.p2.4.m3.1.1.3.cmml"><mi id="S5.SS2.p2.4.m3.1.1.3.2" xref="S5.SS2.p2.4.m3.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.4.m3.1.1.3.1" xref="S5.SS2.p2.4.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.4.m3.1.1.3.3" xref="S5.SS2.p2.4.m3.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.4.m3.1.1.3.1a" xref="S5.SS2.p2.4.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.4.m3.1.1.3.4" xref="S5.SS2.p2.4.m3.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m3.1b"><apply id="S5.SS2.p2.4.m3.1.1.cmml" xref="S5.SS2.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.4.m3.1.1.1.cmml" xref="S5.SS2.p2.4.m3.1.1">subscript</csymbol><ci id="S5.SS2.p2.4.m3.1.1.2.cmml" xref="S5.SS2.p2.4.m3.1.1.2">𝑉</ci><apply id="S5.SS2.p2.4.m3.1.1.3.cmml" xref="S5.SS2.p2.4.m3.1.1.3"><times id="S5.SS2.p2.4.m3.1.1.3.1.cmml" xref="S5.SS2.p2.4.m3.1.1.3.1"></times><ci id="S5.SS2.p2.4.m3.1.1.3.2.cmml" xref="S5.SS2.p2.4.m3.1.1.3.2">𝑒</ci><ci id="S5.SS2.p2.4.m3.1.1.3.3.cmml" xref="S5.SS2.p2.4.m3.1.1.3.3">𝑚</ci><ci id="S5.SS2.p2.4.m3.1.1.3.4.cmml" xref="S5.SS2.p2.4.m3.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m3.1c">V_{emb}</annotation></semantics></math> as the visual latent feature <math id="S5.SS2.p2.5.m4.1" class="ltx_Math" alttext="\mathbf{c}_{v}" display="inline"><semantics id="S5.SS2.p2.5.m4.1a"><msub id="S5.SS2.p2.5.m4.1.1" xref="S5.SS2.p2.5.m4.1.1.cmml"><mi id="S5.SS2.p2.5.m4.1.1.2" xref="S5.SS2.p2.5.m4.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.5.m4.1.1.3" xref="S5.SS2.p2.5.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m4.1b"><apply id="S5.SS2.p2.5.m4.1.1.cmml" xref="S5.SS2.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m4.1.1.1.cmml" xref="S5.SS2.p2.5.m4.1.1">subscript</csymbol><ci id="S5.SS2.p2.5.m4.1.1.2.cmml" xref="S5.SS2.p2.5.m4.1.1.2">𝐜</ci><ci id="S5.SS2.p2.5.m4.1.1.3.cmml" xref="S5.SS2.p2.5.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m4.1c">\mathbf{c}_{v}</annotation></semantics></math>:</p>
<table id="S5.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex3.m1.1" class="ltx_Math" alttext="\mathbf{c}_{v}=\frac{\sum^{n}_{i=1}E_{v}(\mathbf{V_{i}})}{n}" display="block"><semantics id="S5.Ex3.m1.1a"><mrow id="S5.Ex3.m1.1.2" xref="S5.Ex3.m1.1.2.cmml"><msub id="S5.Ex3.m1.1.2.2" xref="S5.Ex3.m1.1.2.2.cmml"><mi id="S5.Ex3.m1.1.2.2.2" xref="S5.Ex3.m1.1.2.2.2.cmml">𝐜</mi><mi id="S5.Ex3.m1.1.2.2.3" xref="S5.Ex3.m1.1.2.2.3.cmml">v</mi></msub><mo id="S5.Ex3.m1.1.2.1" xref="S5.Ex3.m1.1.2.1.cmml">=</mo><mfrac id="S5.Ex3.m1.1.1" xref="S5.Ex3.m1.1.1.cmml"><mrow id="S5.Ex3.m1.1.1.1" xref="S5.Ex3.m1.1.1.1.cmml"><msubsup id="S5.Ex3.m1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.2.cmml"><mo id="S5.Ex3.m1.1.1.1.2.2.2" xref="S5.Ex3.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.Ex3.m1.1.1.1.2.3" xref="S5.Ex3.m1.1.1.1.2.3.cmml"><mi id="S5.Ex3.m1.1.1.1.2.3.2" xref="S5.Ex3.m1.1.1.1.2.3.2.cmml">i</mi><mo id="S5.Ex3.m1.1.1.1.2.3.1" xref="S5.Ex3.m1.1.1.1.2.3.1.cmml">=</mo><mn id="S5.Ex3.m1.1.1.1.2.3.3" xref="S5.Ex3.m1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S5.Ex3.m1.1.1.1.2.2.3" xref="S5.Ex3.m1.1.1.1.2.2.3.cmml">n</mi></msubsup><mrow id="S5.Ex3.m1.1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.cmml"><msub id="S5.Ex3.m1.1.1.1.1.3" xref="S5.Ex3.m1.1.1.1.1.3.cmml"><mi id="S5.Ex3.m1.1.1.1.1.3.2" xref="S5.Ex3.m1.1.1.1.1.3.2.cmml">E</mi><mi id="S5.Ex3.m1.1.1.1.1.3.3" xref="S5.Ex3.m1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S5.Ex3.m1.1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.1.2.cmml">​</mo><mrow id="S5.Ex3.m1.1.1.1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.Ex3.m1.1.1.1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.Ex3.m1.1.1.1.1.1.1.1" xref="S5.Ex3.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.Ex3.m1.1.1.1.1.1.1.1.2" xref="S5.Ex3.m1.1.1.1.1.1.1.1.2.cmml">𝐕</mi><mi id="S5.Ex3.m1.1.1.1.1.1.1.1.3" xref="S5.Ex3.m1.1.1.1.1.1.1.1.3.cmml">𝐢</mi></msub><mo stretchy="false" id="S5.Ex3.m1.1.1.1.1.1.1.3" xref="S5.Ex3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mi id="S5.Ex3.m1.1.1.3" xref="S5.Ex3.m1.1.1.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex3.m1.1b"><apply id="S5.Ex3.m1.1.2.cmml" xref="S5.Ex3.m1.1.2"><eq id="S5.Ex3.m1.1.2.1.cmml" xref="S5.Ex3.m1.1.2.1"></eq><apply id="S5.Ex3.m1.1.2.2.cmml" xref="S5.Ex3.m1.1.2.2"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.2.2.1.cmml" xref="S5.Ex3.m1.1.2.2">subscript</csymbol><ci id="S5.Ex3.m1.1.2.2.2.cmml" xref="S5.Ex3.m1.1.2.2.2">𝐜</ci><ci id="S5.Ex3.m1.1.2.2.3.cmml" xref="S5.Ex3.m1.1.2.2.3">𝑣</ci></apply><apply id="S5.Ex3.m1.1.1.cmml" xref="S5.Ex3.m1.1.1"><divide id="S5.Ex3.m1.1.1.2.cmml" xref="S5.Ex3.m1.1.1"></divide><apply id="S5.Ex3.m1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1"><apply id="S5.Ex3.m1.1.1.1.2.cmml" xref="S5.Ex3.m1.1.1.1.2"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.2.1.cmml" xref="S5.Ex3.m1.1.1.1.2">subscript</csymbol><apply id="S5.Ex3.m1.1.1.1.2.2.cmml" xref="S5.Ex3.m1.1.1.1.2"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.2.2.1.cmml" xref="S5.Ex3.m1.1.1.1.2">superscript</csymbol><sum id="S5.Ex3.m1.1.1.1.2.2.2.cmml" xref="S5.Ex3.m1.1.1.1.2.2.2"></sum><ci id="S5.Ex3.m1.1.1.1.2.2.3.cmml" xref="S5.Ex3.m1.1.1.1.2.2.3">𝑛</ci></apply><apply id="S5.Ex3.m1.1.1.1.2.3.cmml" xref="S5.Ex3.m1.1.1.1.2.3"><eq id="S5.Ex3.m1.1.1.1.2.3.1.cmml" xref="S5.Ex3.m1.1.1.1.2.3.1"></eq><ci id="S5.Ex3.m1.1.1.1.2.3.2.cmml" xref="S5.Ex3.m1.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S5.Ex3.m1.1.1.1.2.3.3.cmml" xref="S5.Ex3.m1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S5.Ex3.m1.1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1.1"><times id="S5.Ex3.m1.1.1.1.1.2.cmml" xref="S5.Ex3.m1.1.1.1.1.2"></times><apply id="S5.Ex3.m1.1.1.1.1.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.1.3.1.cmml" xref="S5.Ex3.m1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex3.m1.1.1.1.1.3.2.cmml" xref="S5.Ex3.m1.1.1.1.1.3.2">𝐸</ci><ci id="S5.Ex3.m1.1.1.1.1.3.3.cmml" xref="S5.Ex3.m1.1.1.1.1.3.3">𝑣</ci></apply><apply id="S5.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex3.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex3.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex3.m1.1.1.1.1.1.1.1.2">𝐕</ci><ci id="S5.Ex3.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex3.m1.1.1.1.1.1.1.1.3">𝐢</ci></apply></apply></apply><ci id="S5.Ex3.m1.1.1.3.cmml" xref="S5.Ex3.m1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex3.m1.1c">\mathbf{c}_{v}=\frac{\sum^{n}_{i=1}E_{v}(\mathbf{V_{i}})}{n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.7" class="ltx_p">where n is the number of selected video frames. Then we utilize the CLIP loss to ensure the alignment between fMRI features <math id="S5.SS2.p2.6.m1.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS2.p2.6.m1.1a"><msub id="S5.SS2.p2.6.m1.1.1" xref="S5.SS2.p2.6.m1.1.1.cmml"><mi id="S5.SS2.p2.6.m1.1.1.2" xref="S5.SS2.p2.6.m1.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.6.m1.1.1.3" xref="S5.SS2.p2.6.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m1.1b"><apply id="S5.SS2.p2.6.m1.1.1.cmml" xref="S5.SS2.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m1.1.1.1.cmml" xref="S5.SS2.p2.6.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.6.m1.1.1.2.cmml" xref="S5.SS2.p2.6.m1.1.1.2">𝐜</ci><ci id="S5.SS2.p2.6.m1.1.1.3.cmml" xref="S5.SS2.p2.6.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m1.1c">\mathbf{c}_{f}</annotation></semantics></math> and visual features <math id="S5.SS2.p2.7.m2.1" class="ltx_Math" alttext="\mathbf{c}_{v}" display="inline"><semantics id="S5.SS2.p2.7.m2.1a"><msub id="S5.SS2.p2.7.m2.1.1" xref="S5.SS2.p2.7.m2.1.1.cmml"><mi id="S5.SS2.p2.7.m2.1.1.2" xref="S5.SS2.p2.7.m2.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.7.m2.1.1.3" xref="S5.SS2.p2.7.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m2.1b"><apply id="S5.SS2.p2.7.m2.1.1.cmml" xref="S5.SS2.p2.7.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.7.m2.1.1.1.cmml" xref="S5.SS2.p2.7.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.7.m2.1.1.2.cmml" xref="S5.SS2.p2.7.m2.1.1.2">𝐜</ci><ci id="S5.SS2.p2.7.m2.1.1.3.cmml" xref="S5.SS2.p2.7.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m2.1c">\mathbf{c}_{v}</annotation></semantics></math>:</p>
<table id="S5.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex4.m1.2" class="ltx_Math" alttext="\mathcal{L}_{c}=\mathcal{L}_{clip}(\mathbf{c}_{f},\mathbf{c}_{v})" display="block"><semantics id="S5.Ex4.m1.2a"><mrow id="S5.Ex4.m1.2.2" xref="S5.Ex4.m1.2.2.cmml"><msub id="S5.Ex4.m1.2.2.4" xref="S5.Ex4.m1.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex4.m1.2.2.4.2" xref="S5.Ex4.m1.2.2.4.2.cmml">ℒ</mi><mi id="S5.Ex4.m1.2.2.4.3" xref="S5.Ex4.m1.2.2.4.3.cmml">c</mi></msub><mo id="S5.Ex4.m1.2.2.3" xref="S5.Ex4.m1.2.2.3.cmml">=</mo><mrow id="S5.Ex4.m1.2.2.2" xref="S5.Ex4.m1.2.2.2.cmml"><msub id="S5.Ex4.m1.2.2.2.4" xref="S5.Ex4.m1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex4.m1.2.2.2.4.2" xref="S5.Ex4.m1.2.2.2.4.2.cmml">ℒ</mi><mrow id="S5.Ex4.m1.2.2.2.4.3" xref="S5.Ex4.m1.2.2.2.4.3.cmml"><mi id="S5.Ex4.m1.2.2.2.4.3.2" xref="S5.Ex4.m1.2.2.2.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.Ex4.m1.2.2.2.4.3.1" xref="S5.Ex4.m1.2.2.2.4.3.1.cmml">​</mo><mi id="S5.Ex4.m1.2.2.2.4.3.3" xref="S5.Ex4.m1.2.2.2.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.Ex4.m1.2.2.2.4.3.1a" xref="S5.Ex4.m1.2.2.2.4.3.1.cmml">​</mo><mi id="S5.Ex4.m1.2.2.2.4.3.4" xref="S5.Ex4.m1.2.2.2.4.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.Ex4.m1.2.2.2.4.3.1b" xref="S5.Ex4.m1.2.2.2.4.3.1.cmml">​</mo><mi id="S5.Ex4.m1.2.2.2.4.3.5" xref="S5.Ex4.m1.2.2.2.4.3.5.cmml">p</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.Ex4.m1.2.2.2.3" xref="S5.Ex4.m1.2.2.2.3.cmml">​</mo><mrow id="S5.Ex4.m1.2.2.2.2.2" xref="S5.Ex4.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S5.Ex4.m1.2.2.2.2.2.3" xref="S5.Ex4.m1.2.2.2.2.3.cmml">(</mo><msub id="S5.Ex4.m1.1.1.1.1.1.1" xref="S5.Ex4.m1.1.1.1.1.1.1.cmml"><mi id="S5.Ex4.m1.1.1.1.1.1.1.2" xref="S5.Ex4.m1.1.1.1.1.1.1.2.cmml">𝐜</mi><mi id="S5.Ex4.m1.1.1.1.1.1.1.3" xref="S5.Ex4.m1.1.1.1.1.1.1.3.cmml">f</mi></msub><mo id="S5.Ex4.m1.2.2.2.2.2.4" xref="S5.Ex4.m1.2.2.2.2.3.cmml">,</mo><msub id="S5.Ex4.m1.2.2.2.2.2.2" xref="S5.Ex4.m1.2.2.2.2.2.2.cmml"><mi id="S5.Ex4.m1.2.2.2.2.2.2.2" xref="S5.Ex4.m1.2.2.2.2.2.2.2.cmml">𝐜</mi><mi id="S5.Ex4.m1.2.2.2.2.2.2.3" xref="S5.Ex4.m1.2.2.2.2.2.2.3.cmml">v</mi></msub><mo stretchy="false" id="S5.Ex4.m1.2.2.2.2.2.5" xref="S5.Ex4.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex4.m1.2b"><apply id="S5.Ex4.m1.2.2.cmml" xref="S5.Ex4.m1.2.2"><eq id="S5.Ex4.m1.2.2.3.cmml" xref="S5.Ex4.m1.2.2.3"></eq><apply id="S5.Ex4.m1.2.2.4.cmml" xref="S5.Ex4.m1.2.2.4"><csymbol cd="ambiguous" id="S5.Ex4.m1.2.2.4.1.cmml" xref="S5.Ex4.m1.2.2.4">subscript</csymbol><ci id="S5.Ex4.m1.2.2.4.2.cmml" xref="S5.Ex4.m1.2.2.4.2">ℒ</ci><ci id="S5.Ex4.m1.2.2.4.3.cmml" xref="S5.Ex4.m1.2.2.4.3">𝑐</ci></apply><apply id="S5.Ex4.m1.2.2.2.cmml" xref="S5.Ex4.m1.2.2.2"><times id="S5.Ex4.m1.2.2.2.3.cmml" xref="S5.Ex4.m1.2.2.2.3"></times><apply id="S5.Ex4.m1.2.2.2.4.cmml" xref="S5.Ex4.m1.2.2.2.4"><csymbol cd="ambiguous" id="S5.Ex4.m1.2.2.2.4.1.cmml" xref="S5.Ex4.m1.2.2.2.4">subscript</csymbol><ci id="S5.Ex4.m1.2.2.2.4.2.cmml" xref="S5.Ex4.m1.2.2.2.4.2">ℒ</ci><apply id="S5.Ex4.m1.2.2.2.4.3.cmml" xref="S5.Ex4.m1.2.2.2.4.3"><times id="S5.Ex4.m1.2.2.2.4.3.1.cmml" xref="S5.Ex4.m1.2.2.2.4.3.1"></times><ci id="S5.Ex4.m1.2.2.2.4.3.2.cmml" xref="S5.Ex4.m1.2.2.2.4.3.2">𝑐</ci><ci id="S5.Ex4.m1.2.2.2.4.3.3.cmml" xref="S5.Ex4.m1.2.2.2.4.3.3">𝑙</ci><ci id="S5.Ex4.m1.2.2.2.4.3.4.cmml" xref="S5.Ex4.m1.2.2.2.4.3.4">𝑖</ci><ci id="S5.Ex4.m1.2.2.2.4.3.5.cmml" xref="S5.Ex4.m1.2.2.2.4.3.5">𝑝</ci></apply></apply><interval closure="open" id="S5.Ex4.m1.2.2.2.2.3.cmml" xref="S5.Ex4.m1.2.2.2.2.2"><apply id="S5.Ex4.m1.1.1.1.1.1.1.cmml" xref="S5.Ex4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex4.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex4.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex4.m1.1.1.1.1.1.1.2">𝐜</ci><ci id="S5.Ex4.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex4.m1.1.1.1.1.1.1.3">𝑓</ci></apply><apply id="S5.Ex4.m1.2.2.2.2.2.2.cmml" xref="S5.Ex4.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.Ex4.m1.2.2.2.2.2.2.1.cmml" xref="S5.Ex4.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S5.Ex4.m1.2.2.2.2.2.2.2.cmml" xref="S5.Ex4.m1.2.2.2.2.2.2.2">𝐜</ci><ci id="S5.Ex4.m1.2.2.2.2.2.2.3.cmml" xref="S5.Ex4.m1.2.2.2.2.2.2.3">𝑣</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex4.m1.2c">\mathcal{L}_{c}=\mathcal{L}_{clip}(\mathbf{c}_{f},\mathbf{c}_{v})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.12" class="ltx_p">During training, we optimize the neuro-fusion encoder initialized with pre-trained weights, while the CLIP vision encoder remains frozen. It’s worth noting that the CLIP vision encoder <math id="S5.SS2.p2.8.m1.1" class="ltx_Math" alttext="E_{v}" display="inline"><semantics id="S5.SS2.p2.8.m1.1a"><msub id="S5.SS2.p2.8.m1.1.1" xref="S5.SS2.p2.8.m1.1.1.cmml"><mi id="S5.SS2.p2.8.m1.1.1.2" xref="S5.SS2.p2.8.m1.1.1.2.cmml">E</mi><mi id="S5.SS2.p2.8.m1.1.1.3" xref="S5.SS2.p2.8.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m1.1b"><apply id="S5.SS2.p2.8.m1.1.1.cmml" xref="S5.SS2.p2.8.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.8.m1.1.1.1.cmml" xref="S5.SS2.p2.8.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.8.m1.1.1.2.cmml" xref="S5.SS2.p2.8.m1.1.1.2">𝐸</ci><ci id="S5.SS2.p2.8.m1.1.1.3.cmml" xref="S5.SS2.p2.8.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m1.1c">E_{v}</annotation></semantics></math> and images <math id="S5.SS2.p2.9.m2.1" class="ltx_Math" alttext="\{\mathbf{V}\}" display="inline"><semantics id="S5.SS2.p2.9.m2.1a"><mrow id="S5.SS2.p2.9.m2.1.2.2" xref="S5.SS2.p2.9.m2.1.2.1.cmml"><mo stretchy="false" id="S5.SS2.p2.9.m2.1.2.2.1" xref="S5.SS2.p2.9.m2.1.2.1.cmml">{</mo><mi id="S5.SS2.p2.9.m2.1.1" xref="S5.SS2.p2.9.m2.1.1.cmml">𝐕</mi><mo stretchy="false" id="S5.SS2.p2.9.m2.1.2.2.2" xref="S5.SS2.p2.9.m2.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m2.1b"><set id="S5.SS2.p2.9.m2.1.2.1.cmml" xref="S5.SS2.p2.9.m2.1.2.2"><ci id="S5.SS2.p2.9.m2.1.1.cmml" xref="S5.SS2.p2.9.m2.1.1">𝐕</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m2.1c">\{\mathbf{V}\}</annotation></semantics></math> are just used for training, we will drop it during inference. By adopting this approach, we will get the fMRI features <math id="S5.SS2.p2.10.m3.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS2.p2.10.m3.1a"><msub id="S5.SS2.p2.10.m3.1.1" xref="S5.SS2.p2.10.m3.1.1.cmml"><mi id="S5.SS2.p2.10.m3.1.1.2" xref="S5.SS2.p2.10.m3.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.10.m3.1.1.3" xref="S5.SS2.p2.10.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m3.1b"><apply id="S5.SS2.p2.10.m3.1.1.cmml" xref="S5.SS2.p2.10.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.10.m3.1.1.1.cmml" xref="S5.SS2.p2.10.m3.1.1">subscript</csymbol><ci id="S5.SS2.p2.10.m3.1.1.2.cmml" xref="S5.SS2.p2.10.m3.1.1.2">𝐜</ci><ci id="S5.SS2.p2.10.m3.1.1.3.cmml" xref="S5.SS2.p2.10.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m3.1c">\mathbf{c}_{f}</annotation></semantics></math> and embedding <math id="S5.SS2.p2.11.m4.1" class="ltx_Math" alttext="\mathbf{F}_{emb}" display="inline"><semantics id="S5.SS2.p2.11.m4.1a"><msub id="S5.SS2.p2.11.m4.1.1" xref="S5.SS2.p2.11.m4.1.1.cmml"><mi id="S5.SS2.p2.11.m4.1.1.2" xref="S5.SS2.p2.11.m4.1.1.2.cmml">𝐅</mi><mrow id="S5.SS2.p2.11.m4.1.1.3" xref="S5.SS2.p2.11.m4.1.1.3.cmml"><mi id="S5.SS2.p2.11.m4.1.1.3.2" xref="S5.SS2.p2.11.m4.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.11.m4.1.1.3.1" xref="S5.SS2.p2.11.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.11.m4.1.1.3.3" xref="S5.SS2.p2.11.m4.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.11.m4.1.1.3.1a" xref="S5.SS2.p2.11.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.11.m4.1.1.3.4" xref="S5.SS2.p2.11.m4.1.1.3.4.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.11.m4.1b"><apply id="S5.SS2.p2.11.m4.1.1.cmml" xref="S5.SS2.p2.11.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.11.m4.1.1.1.cmml" xref="S5.SS2.p2.11.m4.1.1">subscript</csymbol><ci id="S5.SS2.p2.11.m4.1.1.2.cmml" xref="S5.SS2.p2.11.m4.1.1.2">𝐅</ci><apply id="S5.SS2.p2.11.m4.1.1.3.cmml" xref="S5.SS2.p2.11.m4.1.1.3"><times id="S5.SS2.p2.11.m4.1.1.3.1.cmml" xref="S5.SS2.p2.11.m4.1.1.3.1"></times><ci id="S5.SS2.p2.11.m4.1.1.3.2.cmml" xref="S5.SS2.p2.11.m4.1.1.3.2">𝑒</ci><ci id="S5.SS2.p2.11.m4.1.1.3.3.cmml" xref="S5.SS2.p2.11.m4.1.1.3.3">𝑚</ci><ci id="S5.SS2.p2.11.m4.1.1.3.4.cmml" xref="S5.SS2.p2.11.m4.1.1.3.4">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.11.m4.1c">\mathbf{F}_{emb}</annotation></semantics></math> aligned with the visual space. The fMRI features <math id="S5.SS2.p2.12.m5.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS2.p2.12.m5.1a"><msub id="S5.SS2.p2.12.m5.1.1" xref="S5.SS2.p2.12.m5.1.1.cmml"><mi id="S5.SS2.p2.12.m5.1.1.2" xref="S5.SS2.p2.12.m5.1.1.2.cmml">𝐜</mi><mi id="S5.SS2.p2.12.m5.1.1.3" xref="S5.SS2.p2.12.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.12.m5.1b"><apply id="S5.SS2.p2.12.m5.1.1.cmml" xref="S5.SS2.p2.12.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.12.m5.1.1.1.cmml" xref="S5.SS2.p2.12.m5.1.1">subscript</csymbol><ci id="S5.SS2.p2.12.m5.1.1.2.cmml" xref="S5.SS2.p2.12.m5.1.1.2">𝐜</ci><ci id="S5.SS2.p2.12.m5.1.1.3.cmml" xref="S5.SS2.p2.12.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.12.m5.1c">\mathbf{c}_{f}</annotation></semantics></math> will be used as the conditional information for the feature bridge diffusion model.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Feature Bridge Diffusion Model</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.4" class="ltx_p">This section describes the Neural-Visual Synthesis process via a transformer-based diffusion model, herein referred to as the Feature Bridge Diffusion Model (FBDM), inspired by the hierarchical design proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. The FBDM’s primary role is to bridge fMRI latent features <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><msub id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝐜</ci><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\mathbf{c}_{f}</annotation></semantics></math> with their visual counterparts <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="\hat{\mathbf{c}_{v}}" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mover accent="true" id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><msub id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2.2" xref="S5.SS3.p1.2.m2.1.1.2.2.cmml">𝐜</mi><mi id="S5.SS3.p1.2.m2.1.1.2.3" xref="S5.SS3.p1.2.m2.1.1.2.3.cmml">v</mi></msub><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><ci id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">^</ci><apply id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.2.1.cmml" xref="S5.SS3.p1.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS3.p1.2.m2.1.1.2.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2.2">𝐜</ci><ci id="S5.SS3.p1.2.m2.1.1.2.3.cmml" xref="S5.SS3.p1.2.m2.1.1.2.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\hat{\mathbf{c}_{v}}</annotation></semantics></math>, effectively transforming neurological signals into visual representations.
In the forward diffusion process, we treat the visual latent feature <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{c}_{v}" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><msub id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">𝐜</ci><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\mathbf{c}_{v}</annotation></semantics></math> as <math id="S5.SS3.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{x}_{0}" display="inline"><semantics id="S5.SS3.p1.4.m4.1a"><msub id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml"><mi id="S5.SS3.p1.4.m4.1.1.2" xref="S5.SS3.p1.4.m4.1.1.2.cmml">𝐱</mi><mn id="S5.SS3.p1.4.m4.1.1.3" xref="S5.SS3.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><apply id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.4.m4.1.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p1.4.m4.1.1.2.cmml" xref="S5.SS3.p1.4.m4.1.1.2">𝐱</ci><cn type="integer" id="S5.SS3.p1.4.m4.1.1.3.cmml" xref="S5.SS3.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">\mathbf{x}_{0}</annotation></semantics></math> and introduce Gaussian noise into it across 100 timesteps. This approach modifies the standard 1000-timestep model to align with the settings in Mind-eye <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>:</p>
<table id="S5.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex5.m1.4" class="ltx_Math" alttext="\mathbf{x}_{t}=\sqrt{\alpha_{t}}\mathbf{x}_{t-1}+\sqrt{1-\alpha_{t}}\mathbf{\epsilon},\quad\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})" display="block"><semantics id="S5.Ex5.m1.4a"><mrow id="S5.Ex5.m1.4.4.2" xref="S5.Ex5.m1.4.4.3.cmml"><mrow id="S5.Ex5.m1.3.3.1.1" xref="S5.Ex5.m1.3.3.1.1.cmml"><msub id="S5.Ex5.m1.3.3.1.1.2" xref="S5.Ex5.m1.3.3.1.1.2.cmml"><mi id="S5.Ex5.m1.3.3.1.1.2.2" xref="S5.Ex5.m1.3.3.1.1.2.2.cmml">𝐱</mi><mi id="S5.Ex5.m1.3.3.1.1.2.3" xref="S5.Ex5.m1.3.3.1.1.2.3.cmml">t</mi></msub><mo id="S5.Ex5.m1.3.3.1.1.1" xref="S5.Ex5.m1.3.3.1.1.1.cmml">=</mo><mrow id="S5.Ex5.m1.3.3.1.1.3" xref="S5.Ex5.m1.3.3.1.1.3.cmml"><mrow id="S5.Ex5.m1.3.3.1.1.3.2" xref="S5.Ex5.m1.3.3.1.1.3.2.cmml"><msqrt id="S5.Ex5.m1.3.3.1.1.3.2.2" xref="S5.Ex5.m1.3.3.1.1.3.2.2.cmml"><msub id="S5.Ex5.m1.3.3.1.1.3.2.2.2" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2.cmml"><mi id="S5.Ex5.m1.3.3.1.1.3.2.2.2.2" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2.2.cmml">α</mi><mi id="S5.Ex5.m1.3.3.1.1.3.2.2.2.3" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2.3.cmml">t</mi></msub></msqrt><mo lspace="0em" rspace="0em" id="S5.Ex5.m1.3.3.1.1.3.2.1" xref="S5.Ex5.m1.3.3.1.1.3.2.1.cmml">​</mo><msub id="S5.Ex5.m1.3.3.1.1.3.2.3" xref="S5.Ex5.m1.3.3.1.1.3.2.3.cmml"><mi id="S5.Ex5.m1.3.3.1.1.3.2.3.2" xref="S5.Ex5.m1.3.3.1.1.3.2.3.2.cmml">𝐱</mi><mrow id="S5.Ex5.m1.3.3.1.1.3.2.3.3" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.cmml"><mi id="S5.Ex5.m1.3.3.1.1.3.2.3.3.2" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.2.cmml">t</mi><mo id="S5.Ex5.m1.3.3.1.1.3.2.3.3.1" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.1.cmml">−</mo><mn id="S5.Ex5.m1.3.3.1.1.3.2.3.3.3" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S5.Ex5.m1.3.3.1.1.3.1" xref="S5.Ex5.m1.3.3.1.1.3.1.cmml">+</mo><mrow id="S5.Ex5.m1.3.3.1.1.3.3" xref="S5.Ex5.m1.3.3.1.1.3.3.cmml"><msqrt id="S5.Ex5.m1.3.3.1.1.3.3.2" xref="S5.Ex5.m1.3.3.1.1.3.3.2.cmml"><mrow id="S5.Ex5.m1.3.3.1.1.3.3.2.2" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.cmml"><mn id="S5.Ex5.m1.3.3.1.1.3.3.2.2.2" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.2.cmml">1</mn><mo id="S5.Ex5.m1.3.3.1.1.3.3.2.2.1" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.1.cmml">−</mo><msub id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.cmml"><mi id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.2" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.2.cmml">α</mi><mi id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.3" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.3.cmml">t</mi></msub></mrow></msqrt><mo lspace="0em" rspace="0em" id="S5.Ex5.m1.3.3.1.1.3.3.1" xref="S5.Ex5.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S5.Ex5.m1.3.3.1.1.3.3.3" xref="S5.Ex5.m1.3.3.1.1.3.3.3.cmml">ϵ</mi></mrow></mrow></mrow><mo rspace="1.167em" id="S5.Ex5.m1.4.4.2.3" xref="S5.Ex5.m1.4.4.3a.cmml">,</mo><mrow id="S5.Ex5.m1.4.4.2.2" xref="S5.Ex5.m1.4.4.2.2.cmml"><mi id="S5.Ex5.m1.4.4.2.2.2" xref="S5.Ex5.m1.4.4.2.2.2.cmml">ϵ</mi><mo id="S5.Ex5.m1.4.4.2.2.1" xref="S5.Ex5.m1.4.4.2.2.1.cmml">∼</mo><mrow id="S5.Ex5.m1.4.4.2.2.3" xref="S5.Ex5.m1.4.4.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex5.m1.4.4.2.2.3.2" xref="S5.Ex5.m1.4.4.2.2.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S5.Ex5.m1.4.4.2.2.3.1" xref="S5.Ex5.m1.4.4.2.2.3.1.cmml">​</mo><mrow id="S5.Ex5.m1.4.4.2.2.3.3.2" xref="S5.Ex5.m1.4.4.2.2.3.3.1.cmml"><mo stretchy="false" id="S5.Ex5.m1.4.4.2.2.3.3.2.1" xref="S5.Ex5.m1.4.4.2.2.3.3.1.cmml">(</mo><mn id="S5.Ex5.m1.1.1" xref="S5.Ex5.m1.1.1.cmml">0</mn><mo id="S5.Ex5.m1.4.4.2.2.3.3.2.2" xref="S5.Ex5.m1.4.4.2.2.3.3.1.cmml">,</mo><mi id="S5.Ex5.m1.2.2" xref="S5.Ex5.m1.2.2.cmml">𝐈</mi><mo stretchy="false" id="S5.Ex5.m1.4.4.2.2.3.3.2.3" xref="S5.Ex5.m1.4.4.2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex5.m1.4b"><apply id="S5.Ex5.m1.4.4.3.cmml" xref="S5.Ex5.m1.4.4.2"><csymbol cd="ambiguous" id="S5.Ex5.m1.4.4.3a.cmml" xref="S5.Ex5.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S5.Ex5.m1.3.3.1.1.cmml" xref="S5.Ex5.m1.3.3.1.1"><eq id="S5.Ex5.m1.3.3.1.1.1.cmml" xref="S5.Ex5.m1.3.3.1.1.1"></eq><apply id="S5.Ex5.m1.3.3.1.1.2.cmml" xref="S5.Ex5.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S5.Ex5.m1.3.3.1.1.2.1.cmml" xref="S5.Ex5.m1.3.3.1.1.2">subscript</csymbol><ci id="S5.Ex5.m1.3.3.1.1.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.2.2">𝐱</ci><ci id="S5.Ex5.m1.3.3.1.1.2.3.cmml" xref="S5.Ex5.m1.3.3.1.1.2.3">𝑡</ci></apply><apply id="S5.Ex5.m1.3.3.1.1.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3"><plus id="S5.Ex5.m1.3.3.1.1.3.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.1"></plus><apply id="S5.Ex5.m1.3.3.1.1.3.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2"><times id="S5.Ex5.m1.3.3.1.1.3.2.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.1"></times><apply id="S5.Ex5.m1.3.3.1.1.3.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2"><root id="S5.Ex5.m1.3.3.1.1.3.2.2a.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2"></root><apply id="S5.Ex5.m1.3.3.1.1.3.2.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S5.Ex5.m1.3.3.1.1.3.2.2.2.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2">subscript</csymbol><ci id="S5.Ex5.m1.3.3.1.1.3.2.2.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2.2">𝛼</ci><ci id="S5.Ex5.m1.3.3.1.1.3.2.2.2.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.2.2.3">𝑡</ci></apply></apply><apply id="S5.Ex5.m1.3.3.1.1.3.2.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3"><csymbol cd="ambiguous" id="S5.Ex5.m1.3.3.1.1.3.2.3.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3">subscript</csymbol><ci id="S5.Ex5.m1.3.3.1.1.3.2.3.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3.2">𝐱</ci><apply id="S5.Ex5.m1.3.3.1.1.3.2.3.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3"><minus id="S5.Ex5.m1.3.3.1.1.3.2.3.3.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.1"></minus><ci id="S5.Ex5.m1.3.3.1.1.3.2.3.3.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.2">𝑡</ci><cn type="integer" id="S5.Ex5.m1.3.3.1.1.3.2.3.3.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.2.3.3.3">1</cn></apply></apply></apply><apply id="S5.Ex5.m1.3.3.1.1.3.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3"><times id="S5.Ex5.m1.3.3.1.1.3.3.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.1"></times><apply id="S5.Ex5.m1.3.3.1.1.3.3.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2"><root id="S5.Ex5.m1.3.3.1.1.3.3.2a.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2"></root><apply id="S5.Ex5.m1.3.3.1.1.3.3.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2"><minus id="S5.Ex5.m1.3.3.1.1.3.3.2.2.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.1"></minus><cn type="integer" id="S5.Ex5.m1.3.3.1.1.3.3.2.2.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.2">1</cn><apply id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.1.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3">subscript</csymbol><ci id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.2.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.2">𝛼</ci><ci id="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.2.2.3.3">𝑡</ci></apply></apply></apply><ci id="S5.Ex5.m1.3.3.1.1.3.3.3.cmml" xref="S5.Ex5.m1.3.3.1.1.3.3.3">italic-ϵ</ci></apply></apply></apply><apply id="S5.Ex5.m1.4.4.2.2.cmml" xref="S5.Ex5.m1.4.4.2.2"><csymbol cd="latexml" id="S5.Ex5.m1.4.4.2.2.1.cmml" xref="S5.Ex5.m1.4.4.2.2.1">similar-to</csymbol><ci id="S5.Ex5.m1.4.4.2.2.2.cmml" xref="S5.Ex5.m1.4.4.2.2.2">italic-ϵ</ci><apply id="S5.Ex5.m1.4.4.2.2.3.cmml" xref="S5.Ex5.m1.4.4.2.2.3"><times id="S5.Ex5.m1.4.4.2.2.3.1.cmml" xref="S5.Ex5.m1.4.4.2.2.3.1"></times><ci id="S5.Ex5.m1.4.4.2.2.3.2.cmml" xref="S5.Ex5.m1.4.4.2.2.3.2">𝒩</ci><interval closure="open" id="S5.Ex5.m1.4.4.2.2.3.3.1.cmml" xref="S5.Ex5.m1.4.4.2.2.3.3.2"><cn type="integer" id="S5.Ex5.m1.1.1.cmml" xref="S5.Ex5.m1.1.1">0</cn><ci id="S5.Ex5.m1.2.2.cmml" xref="S5.Ex5.m1.2.2">𝐈</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex5.m1.4c">\mathbf{x}_{t}=\sqrt{\alpha_{t}}\mathbf{x}_{t-1}+\sqrt{1-\alpha_{t}}\mathbf{\epsilon},\quad\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS3.p1.6" class="ltx_p">During the reverse diffusion process, FBDM is tasked with predicting and reversing the introduced noise to reconstruct the latent visual feature. This is achieved by estimating the noise <math id="S5.SS3.p1.5.m1.1" class="ltx_Math" alttext="\hat{\mathbf{\epsilon}}_{t}" display="inline"><semantics id="S5.SS3.p1.5.m1.1a"><msub id="S5.SS3.p1.5.m1.1.1" xref="S5.SS3.p1.5.m1.1.1.cmml"><mover accent="true" id="S5.SS3.p1.5.m1.1.1.2" xref="S5.SS3.p1.5.m1.1.1.2.cmml"><mi id="S5.SS3.p1.5.m1.1.1.2.2" xref="S5.SS3.p1.5.m1.1.1.2.2.cmml">ϵ</mi><mo id="S5.SS3.p1.5.m1.1.1.2.1" xref="S5.SS3.p1.5.m1.1.1.2.1.cmml">^</mo></mover><mi id="S5.SS3.p1.5.m1.1.1.3" xref="S5.SS3.p1.5.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m1.1b"><apply id="S5.SS3.p1.5.m1.1.1.cmml" xref="S5.SS3.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.5.m1.1.1.1.cmml" xref="S5.SS3.p1.5.m1.1.1">subscript</csymbol><apply id="S5.SS3.p1.5.m1.1.1.2.cmml" xref="S5.SS3.p1.5.m1.1.1.2"><ci id="S5.SS3.p1.5.m1.1.1.2.1.cmml" xref="S5.SS3.p1.5.m1.1.1.2.1">^</ci><ci id="S5.SS3.p1.5.m1.1.1.2.2.cmml" xref="S5.SS3.p1.5.m1.1.1.2.2">italic-ϵ</ci></apply><ci id="S5.SS3.p1.5.m1.1.1.3.cmml" xref="S5.SS3.p1.5.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m1.1c">\hat{\mathbf{\epsilon}}_{t}</annotation></semantics></math> at each timestep, conditioned on the fMRI latent feature <math id="S5.SS3.p1.6.m2.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS3.p1.6.m2.1a"><msub id="S5.SS3.p1.6.m2.1.1" xref="S5.SS3.p1.6.m2.1.1.cmml"><mi id="S5.SS3.p1.6.m2.1.1.2" xref="S5.SS3.p1.6.m2.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.6.m2.1.1.3" xref="S5.SS3.p1.6.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m2.1b"><apply id="S5.SS3.p1.6.m2.1.1.cmml" xref="S5.SS3.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.6.m2.1.1.1.cmml" xref="S5.SS3.p1.6.m2.1.1">subscript</csymbol><ci id="S5.SS3.p1.6.m2.1.1.2.cmml" xref="S5.SS3.p1.6.m2.1.1.2">𝐜</ci><ci id="S5.SS3.p1.6.m2.1.1.3.cmml" xref="S5.SS3.p1.6.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m2.1c">\mathbf{c}_{f}</annotation></semantics></math>:</p>
<table id="S5.Ex6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex6.m1.3" class="ltx_Math" alttext="\hat{\mathbf{\epsilon}}_{t}=FBDM(\mathbf{x}_{t},t,\mathbf{c}_{f})" display="block"><semantics id="S5.Ex6.m1.3a"><mrow id="S5.Ex6.m1.3.3" xref="S5.Ex6.m1.3.3.cmml"><msub id="S5.Ex6.m1.3.3.4" xref="S5.Ex6.m1.3.3.4.cmml"><mover accent="true" id="S5.Ex6.m1.3.3.4.2" xref="S5.Ex6.m1.3.3.4.2.cmml"><mi id="S5.Ex6.m1.3.3.4.2.2" xref="S5.Ex6.m1.3.3.4.2.2.cmml">ϵ</mi><mo id="S5.Ex6.m1.3.3.4.2.1" xref="S5.Ex6.m1.3.3.4.2.1.cmml">^</mo></mover><mi id="S5.Ex6.m1.3.3.4.3" xref="S5.Ex6.m1.3.3.4.3.cmml">t</mi></msub><mo id="S5.Ex6.m1.3.3.3" xref="S5.Ex6.m1.3.3.3.cmml">=</mo><mrow id="S5.Ex6.m1.3.3.2" xref="S5.Ex6.m1.3.3.2.cmml"><mi id="S5.Ex6.m1.3.3.2.4" xref="S5.Ex6.m1.3.3.2.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.Ex6.m1.3.3.2.3" xref="S5.Ex6.m1.3.3.2.3.cmml">​</mo><mi id="S5.Ex6.m1.3.3.2.5" xref="S5.Ex6.m1.3.3.2.5.cmml">B</mi><mo lspace="0em" rspace="0em" id="S5.Ex6.m1.3.3.2.3a" xref="S5.Ex6.m1.3.3.2.3.cmml">​</mo><mi id="S5.Ex6.m1.3.3.2.6" xref="S5.Ex6.m1.3.3.2.6.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.Ex6.m1.3.3.2.3b" xref="S5.Ex6.m1.3.3.2.3.cmml">​</mo><mi id="S5.Ex6.m1.3.3.2.7" xref="S5.Ex6.m1.3.3.2.7.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.Ex6.m1.3.3.2.3c" xref="S5.Ex6.m1.3.3.2.3.cmml">​</mo><mrow id="S5.Ex6.m1.3.3.2.2.2" xref="S5.Ex6.m1.3.3.2.2.3.cmml"><mo stretchy="false" id="S5.Ex6.m1.3.3.2.2.2.3" xref="S5.Ex6.m1.3.3.2.2.3.cmml">(</mo><msub id="S5.Ex6.m1.2.2.1.1.1.1" xref="S5.Ex6.m1.2.2.1.1.1.1.cmml"><mi id="S5.Ex6.m1.2.2.1.1.1.1.2" xref="S5.Ex6.m1.2.2.1.1.1.1.2.cmml">𝐱</mi><mi id="S5.Ex6.m1.2.2.1.1.1.1.3" xref="S5.Ex6.m1.2.2.1.1.1.1.3.cmml">t</mi></msub><mo id="S5.Ex6.m1.3.3.2.2.2.4" xref="S5.Ex6.m1.3.3.2.2.3.cmml">,</mo><mi id="S5.Ex6.m1.1.1" xref="S5.Ex6.m1.1.1.cmml">t</mi><mo id="S5.Ex6.m1.3.3.2.2.2.5" xref="S5.Ex6.m1.3.3.2.2.3.cmml">,</mo><msub id="S5.Ex6.m1.3.3.2.2.2.2" xref="S5.Ex6.m1.3.3.2.2.2.2.cmml"><mi id="S5.Ex6.m1.3.3.2.2.2.2.2" xref="S5.Ex6.m1.3.3.2.2.2.2.2.cmml">𝐜</mi><mi id="S5.Ex6.m1.3.3.2.2.2.2.3" xref="S5.Ex6.m1.3.3.2.2.2.2.3.cmml">f</mi></msub><mo stretchy="false" id="S5.Ex6.m1.3.3.2.2.2.6" xref="S5.Ex6.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex6.m1.3b"><apply id="S5.Ex6.m1.3.3.cmml" xref="S5.Ex6.m1.3.3"><eq id="S5.Ex6.m1.3.3.3.cmml" xref="S5.Ex6.m1.3.3.3"></eq><apply id="S5.Ex6.m1.3.3.4.cmml" xref="S5.Ex6.m1.3.3.4"><csymbol cd="ambiguous" id="S5.Ex6.m1.3.3.4.1.cmml" xref="S5.Ex6.m1.3.3.4">subscript</csymbol><apply id="S5.Ex6.m1.3.3.4.2.cmml" xref="S5.Ex6.m1.3.3.4.2"><ci id="S5.Ex6.m1.3.3.4.2.1.cmml" xref="S5.Ex6.m1.3.3.4.2.1">^</ci><ci id="S5.Ex6.m1.3.3.4.2.2.cmml" xref="S5.Ex6.m1.3.3.4.2.2">italic-ϵ</ci></apply><ci id="S5.Ex6.m1.3.3.4.3.cmml" xref="S5.Ex6.m1.3.3.4.3">𝑡</ci></apply><apply id="S5.Ex6.m1.3.3.2.cmml" xref="S5.Ex6.m1.3.3.2"><times id="S5.Ex6.m1.3.3.2.3.cmml" xref="S5.Ex6.m1.3.3.2.3"></times><ci id="S5.Ex6.m1.3.3.2.4.cmml" xref="S5.Ex6.m1.3.3.2.4">𝐹</ci><ci id="S5.Ex6.m1.3.3.2.5.cmml" xref="S5.Ex6.m1.3.3.2.5">𝐵</ci><ci id="S5.Ex6.m1.3.3.2.6.cmml" xref="S5.Ex6.m1.3.3.2.6">𝐷</ci><ci id="S5.Ex6.m1.3.3.2.7.cmml" xref="S5.Ex6.m1.3.3.2.7">𝑀</ci><vector id="S5.Ex6.m1.3.3.2.2.3.cmml" xref="S5.Ex6.m1.3.3.2.2.2"><apply id="S5.Ex6.m1.2.2.1.1.1.1.cmml" xref="S5.Ex6.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex6.m1.2.2.1.1.1.1.1.cmml" xref="S5.Ex6.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S5.Ex6.m1.2.2.1.1.1.1.2.cmml" xref="S5.Ex6.m1.2.2.1.1.1.1.2">𝐱</ci><ci id="S5.Ex6.m1.2.2.1.1.1.1.3.cmml" xref="S5.Ex6.m1.2.2.1.1.1.1.3">𝑡</ci></apply><ci id="S5.Ex6.m1.1.1.cmml" xref="S5.Ex6.m1.1.1">𝑡</ci><apply id="S5.Ex6.m1.3.3.2.2.2.2.cmml" xref="S5.Ex6.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S5.Ex6.m1.3.3.2.2.2.2.1.cmml" xref="S5.Ex6.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S5.Ex6.m1.3.3.2.2.2.2.2.cmml" xref="S5.Ex6.m1.3.3.2.2.2.2.2">𝐜</ci><ci id="S5.Ex6.m1.3.3.2.2.2.2.3.cmml" xref="S5.Ex6.m1.3.3.2.2.2.2.3">𝑓</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex6.m1.3c">\hat{\mathbf{\epsilon}}_{t}=FBDM(\mathbf{x}_{t},t,\mathbf{c}_{f})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS3.p1.10" class="ltx_p">Differing from U-Net based models, our approach utilizes a transformer to leverage the self-attention mechanism for integrating the conditional information <math id="S5.SS3.p1.7.m1.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS3.p1.7.m1.1a"><msub id="S5.SS3.p1.7.m1.1.1" xref="S5.SS3.p1.7.m1.1.1.cmml"><mi id="S5.SS3.p1.7.m1.1.1.2" xref="S5.SS3.p1.7.m1.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.7.m1.1.1.3" xref="S5.SS3.p1.7.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m1.1b"><apply id="S5.SS3.p1.7.m1.1.1.cmml" xref="S5.SS3.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.7.m1.1.1.1.cmml" xref="S5.SS3.p1.7.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.7.m1.1.1.2.cmml" xref="S5.SS3.p1.7.m1.1.1.2">𝐜</ci><ci id="S5.SS3.p1.7.m1.1.1.3.cmml" xref="S5.SS3.p1.7.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m1.1c">\mathbf{c}_{f}</annotation></semantics></math>. This is done by embedding the timestep <math id="S5.SS3.p1.8.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S5.SS3.p1.8.m2.1a"><mi id="S5.SS3.p1.8.m2.1.1" xref="S5.SS3.p1.8.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.8.m2.1b"><ci id="S5.SS3.p1.8.m2.1.1.cmml" xref="S5.SS3.p1.8.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.8.m2.1c">t</annotation></semantics></math>, the noise-perturbed feature <math id="S5.SS3.p1.9.m3.1" class="ltx_Math" alttext="\mathbf{x}_{t}" display="inline"><semantics id="S5.SS3.p1.9.m3.1a"><msub id="S5.SS3.p1.9.m3.1.1" xref="S5.SS3.p1.9.m3.1.1.cmml"><mi id="S5.SS3.p1.9.m3.1.1.2" xref="S5.SS3.p1.9.m3.1.1.2.cmml">𝐱</mi><mi id="S5.SS3.p1.9.m3.1.1.3" xref="S5.SS3.p1.9.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.9.m3.1b"><apply id="S5.SS3.p1.9.m3.1.1.cmml" xref="S5.SS3.p1.9.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.9.m3.1.1.1.cmml" xref="S5.SS3.p1.9.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.9.m3.1.1.2.cmml" xref="S5.SS3.p1.9.m3.1.1.2">𝐱</ci><ci id="S5.SS3.p1.9.m3.1.1.3.cmml" xref="S5.SS3.p1.9.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.9.m3.1c">\mathbf{x}_{t}</annotation></semantics></math>, and the conditional fMRI feature <math id="S5.SS3.p1.10.m4.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS3.p1.10.m4.1a"><msub id="S5.SS3.p1.10.m4.1.1" xref="S5.SS3.p1.10.m4.1.1.cmml"><mi id="S5.SS3.p1.10.m4.1.1.2" xref="S5.SS3.p1.10.m4.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.10.m4.1.1.3" xref="S5.SS3.p1.10.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.10.m4.1b"><apply id="S5.SS3.p1.10.m4.1.1.cmml" xref="S5.SS3.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.10.m4.1.1.1.cmml" xref="S5.SS3.p1.10.m4.1.1">subscript</csymbol><ci id="S5.SS3.p1.10.m4.1.1.2.cmml" xref="S5.SS3.p1.10.m4.1.1.2">𝐜</ci><ci id="S5.SS3.p1.10.m4.1.1.3.cmml" xref="S5.SS3.p1.10.m4.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.10.m4.1c">\mathbf{c}_{f}</annotation></semantics></math> into a unified representation, further augmented with a learnable token to capture the target vision features accurately.
The objective of FBDM training is to minimize the discrepancy between the actual noise and its prediction:</p>
<table id="S5.Ex7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex7.m1.4" class="ltx_Math" alttext="\mathcal{L}_{FBDM}=\mathbb{E}_{\mathbf{x},\mathbf{\epsilon},t}\left[\|\mathbf{\epsilon}-\hat{\mathbf{\epsilon}}_{t}\|^{2}\right]" display="block"><semantics id="S5.Ex7.m1.4a"><mrow id="S5.Ex7.m1.4.4" xref="S5.Ex7.m1.4.4.cmml"><msub id="S5.Ex7.m1.4.4.3" xref="S5.Ex7.m1.4.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex7.m1.4.4.3.2" xref="S5.Ex7.m1.4.4.3.2.cmml">ℒ</mi><mrow id="S5.Ex7.m1.4.4.3.3" xref="S5.Ex7.m1.4.4.3.3.cmml"><mi id="S5.Ex7.m1.4.4.3.3.2" xref="S5.Ex7.m1.4.4.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.Ex7.m1.4.4.3.3.1" xref="S5.Ex7.m1.4.4.3.3.1.cmml">​</mo><mi id="S5.Ex7.m1.4.4.3.3.3" xref="S5.Ex7.m1.4.4.3.3.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S5.Ex7.m1.4.4.3.3.1a" xref="S5.Ex7.m1.4.4.3.3.1.cmml">​</mo><mi id="S5.Ex7.m1.4.4.3.3.4" xref="S5.Ex7.m1.4.4.3.3.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.Ex7.m1.4.4.3.3.1b" xref="S5.Ex7.m1.4.4.3.3.1.cmml">​</mo><mi id="S5.Ex7.m1.4.4.3.3.5" xref="S5.Ex7.m1.4.4.3.3.5.cmml">M</mi></mrow></msub><mo id="S5.Ex7.m1.4.4.2" xref="S5.Ex7.m1.4.4.2.cmml">=</mo><mrow id="S5.Ex7.m1.4.4.1" xref="S5.Ex7.m1.4.4.1.cmml"><msub id="S5.Ex7.m1.4.4.1.3" xref="S5.Ex7.m1.4.4.1.3.cmml"><mi id="S5.Ex7.m1.4.4.1.3.2" xref="S5.Ex7.m1.4.4.1.3.2.cmml">𝔼</mi><mrow id="S5.Ex7.m1.3.3.3.5" xref="S5.Ex7.m1.3.3.3.4.cmml"><mi id="S5.Ex7.m1.1.1.1.1" xref="S5.Ex7.m1.1.1.1.1.cmml">𝐱</mi><mo id="S5.Ex7.m1.3.3.3.5.1" xref="S5.Ex7.m1.3.3.3.4.cmml">,</mo><mi id="S5.Ex7.m1.2.2.2.2" xref="S5.Ex7.m1.2.2.2.2.cmml">ϵ</mi><mo id="S5.Ex7.m1.3.3.3.5.2" xref="S5.Ex7.m1.3.3.3.4.cmml">,</mo><mi id="S5.Ex7.m1.3.3.3.3" xref="S5.Ex7.m1.3.3.3.3.cmml">t</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.Ex7.m1.4.4.1.2" xref="S5.Ex7.m1.4.4.1.2.cmml">​</mo><mrow id="S5.Ex7.m1.4.4.1.1.1" xref="S5.Ex7.m1.4.4.1.1.2.cmml"><mo id="S5.Ex7.m1.4.4.1.1.1.2" xref="S5.Ex7.m1.4.4.1.1.2.1.cmml">[</mo><msup id="S5.Ex7.m1.4.4.1.1.1.1" xref="S5.Ex7.m1.4.4.1.1.1.1.cmml"><mrow id="S5.Ex7.m1.4.4.1.1.1.1.1.1" xref="S5.Ex7.m1.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.Ex7.m1.4.4.1.1.1.1.1.1.2" xref="S5.Ex7.m1.4.4.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.2" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.2.cmml">ϵ</mi><mo id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.1" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.cmml"><mi id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.2" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.2.cmml">ϵ</mi><mo id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.1" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.3" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S5.Ex7.m1.4.4.1.1.1.1.1.1.3" xref="S5.Ex7.m1.4.4.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S5.Ex7.m1.4.4.1.1.1.1.3" xref="S5.Ex7.m1.4.4.1.1.1.1.3.cmml">2</mn></msup><mo id="S5.Ex7.m1.4.4.1.1.1.3" xref="S5.Ex7.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex7.m1.4b"><apply id="S5.Ex7.m1.4.4.cmml" xref="S5.Ex7.m1.4.4"><eq id="S5.Ex7.m1.4.4.2.cmml" xref="S5.Ex7.m1.4.4.2"></eq><apply id="S5.Ex7.m1.4.4.3.cmml" xref="S5.Ex7.m1.4.4.3"><csymbol cd="ambiguous" id="S5.Ex7.m1.4.4.3.1.cmml" xref="S5.Ex7.m1.4.4.3">subscript</csymbol><ci id="S5.Ex7.m1.4.4.3.2.cmml" xref="S5.Ex7.m1.4.4.3.2">ℒ</ci><apply id="S5.Ex7.m1.4.4.3.3.cmml" xref="S5.Ex7.m1.4.4.3.3"><times id="S5.Ex7.m1.4.4.3.3.1.cmml" xref="S5.Ex7.m1.4.4.3.3.1"></times><ci id="S5.Ex7.m1.4.4.3.3.2.cmml" xref="S5.Ex7.m1.4.4.3.3.2">𝐹</ci><ci id="S5.Ex7.m1.4.4.3.3.3.cmml" xref="S5.Ex7.m1.4.4.3.3.3">𝐵</ci><ci id="S5.Ex7.m1.4.4.3.3.4.cmml" xref="S5.Ex7.m1.4.4.3.3.4">𝐷</ci><ci id="S5.Ex7.m1.4.4.3.3.5.cmml" xref="S5.Ex7.m1.4.4.3.3.5">𝑀</ci></apply></apply><apply id="S5.Ex7.m1.4.4.1.cmml" xref="S5.Ex7.m1.4.4.1"><times id="S5.Ex7.m1.4.4.1.2.cmml" xref="S5.Ex7.m1.4.4.1.2"></times><apply id="S5.Ex7.m1.4.4.1.3.cmml" xref="S5.Ex7.m1.4.4.1.3"><csymbol cd="ambiguous" id="S5.Ex7.m1.4.4.1.3.1.cmml" xref="S5.Ex7.m1.4.4.1.3">subscript</csymbol><ci id="S5.Ex7.m1.4.4.1.3.2.cmml" xref="S5.Ex7.m1.4.4.1.3.2">𝔼</ci><list id="S5.Ex7.m1.3.3.3.4.cmml" xref="S5.Ex7.m1.3.3.3.5"><ci id="S5.Ex7.m1.1.1.1.1.cmml" xref="S5.Ex7.m1.1.1.1.1">𝐱</ci><ci id="S5.Ex7.m1.2.2.2.2.cmml" xref="S5.Ex7.m1.2.2.2.2">italic-ϵ</ci><ci id="S5.Ex7.m1.3.3.3.3.cmml" xref="S5.Ex7.m1.3.3.3.3">𝑡</ci></list></apply><apply id="S5.Ex7.m1.4.4.1.1.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1"><csymbol cd="latexml" id="S5.Ex7.m1.4.4.1.1.2.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S5.Ex7.m1.4.4.1.1.1.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex7.m1.4.4.1.1.1.1.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1">superscript</csymbol><apply id="S5.Ex7.m1.4.4.1.1.1.1.1.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.Ex7.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.2">norm</csymbol><apply id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1"><minus id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.1"></minus><ci id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.2">italic-ϵ</ci><apply id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2"><ci id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.2.2">italic-ϵ</ci></apply><ci id="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><cn type="integer" id="S5.Ex7.m1.4.4.1.1.1.1.3.cmml" xref="S5.Ex7.m1.4.4.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex7.m1.4c">\mathcal{L}_{FBDM}=\mathbb{E}_{\mathbf{x},\mathbf{\epsilon},t}\left[\|\mathbf{\epsilon}-\hat{\mathbf{\epsilon}}_{t}\|^{2}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS3.p1.13" class="ltx_p">Upon training completion, the model discards the initial visual latent feature <math id="S5.SS3.p1.11.m1.1" class="ltx_Math" alttext="\mathbf{c}_{v}" display="inline"><semantics id="S5.SS3.p1.11.m1.1a"><msub id="S5.SS3.p1.11.m1.1.1" xref="S5.SS3.p1.11.m1.1.1.cmml"><mi id="S5.SS3.p1.11.m1.1.1.2" xref="S5.SS3.p1.11.m1.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.11.m1.1.1.3" xref="S5.SS3.p1.11.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.11.m1.1b"><apply id="S5.SS3.p1.11.m1.1.1.cmml" xref="S5.SS3.p1.11.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.11.m1.1.1.1.cmml" xref="S5.SS3.p1.11.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.11.m1.1.1.2.cmml" xref="S5.SS3.p1.11.m1.1.1.2">𝐜</ci><ci id="S5.SS3.p1.11.m1.1.1.3.cmml" xref="S5.SS3.p1.11.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.11.m1.1c">\mathbf{c}_{v}</annotation></semantics></math> and enters a generative phase. It begins with pure noise and progressively refines this through the reverse diffusion process to generate the predicted visual latent feature <math id="S5.SS3.p1.12.m2.1" class="ltx_Math" alttext="\hat{\mathbf{c}_{v}}" display="inline"><semantics id="S5.SS3.p1.12.m2.1a"><mover accent="true" id="S5.SS3.p1.12.m2.1.1" xref="S5.SS3.p1.12.m2.1.1.cmml"><msub id="S5.SS3.p1.12.m2.1.1.2" xref="S5.SS3.p1.12.m2.1.1.2.cmml"><mi id="S5.SS3.p1.12.m2.1.1.2.2" xref="S5.SS3.p1.12.m2.1.1.2.2.cmml">𝐜</mi><mi id="S5.SS3.p1.12.m2.1.1.2.3" xref="S5.SS3.p1.12.m2.1.1.2.3.cmml">v</mi></msub><mo id="S5.SS3.p1.12.m2.1.1.1" xref="S5.SS3.p1.12.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.12.m2.1b"><apply id="S5.SS3.p1.12.m2.1.1.cmml" xref="S5.SS3.p1.12.m2.1.1"><ci id="S5.SS3.p1.12.m2.1.1.1.cmml" xref="S5.SS3.p1.12.m2.1.1.1">^</ci><apply id="S5.SS3.p1.12.m2.1.1.2.cmml" xref="S5.SS3.p1.12.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p1.12.m2.1.1.2.1.cmml" xref="S5.SS3.p1.12.m2.1.1.2">subscript</csymbol><ci id="S5.SS3.p1.12.m2.1.1.2.2.cmml" xref="S5.SS3.p1.12.m2.1.1.2.2">𝐜</ci><ci id="S5.SS3.p1.12.m2.1.1.2.3.cmml" xref="S5.SS3.p1.12.m2.1.1.2.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.12.m2.1c">\hat{\mathbf{c}_{v}}</annotation></semantics></math>. This process effectively translates the aligned fMRI latent features <math id="S5.SS3.p1.13.m3.1" class="ltx_Math" alttext="\mathbf{c}_{f}" display="inline"><semantics id="S5.SS3.p1.13.m3.1a"><msub id="S5.SS3.p1.13.m3.1.1" xref="S5.SS3.p1.13.m3.1.1.cmml"><mi id="S5.SS3.p1.13.m3.1.1.2" xref="S5.SS3.p1.13.m3.1.1.2.cmml">𝐜</mi><mi id="S5.SS3.p1.13.m3.1.1.3" xref="S5.SS3.p1.13.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.13.m3.1b"><apply id="S5.SS3.p1.13.m3.1.1.cmml" xref="S5.SS3.p1.13.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.13.m3.1.1.1.cmml" xref="S5.SS3.p1.13.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.13.m3.1.1.2.cmml" xref="S5.SS3.p1.13.m3.1.1.2">𝐜</ci><ci id="S5.SS3.p1.13.m3.1.1.3.cmml" xref="S5.SS3.p1.13.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.13.m3.1c">\mathbf{c}_{f}</annotation></semantics></math> into visual representations, serving as essential conditional information for the subsequent latent-adapted decoding phase.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2409.11315/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>

The qualitative results generated by LEA-3D, fMRI-PTE-3D, and our method are presented. GT indicates the ground-truth 3D objects. All the objects have been rendered into a 2D format.
</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span id="S5.SS4.1.1" class="ltx_text ltx_font_italic">Latent Adapted Decoder</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The Latent Adapted Decoder (LAD) leverages Argus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a state-of-the-art 3D mesh generation method, as its backbone. This component is tasked with generating the Vector Quantized (VQ) latent representation of target 3D objects from conditional information processed through a generative auto-regressive transformer. The resulting 3D objects are then reconstructed using the VQ decoder.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">To tailor Argus for processing fMRI input and to retain its generative efficiency, we employ adapter tuning during training. This involves integrating the adapter layers within the transformer block. Following classic methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, we apply two feed-forward linear layers to effectively adapt the self-attention outputs.
The adapter layer is systematically inserted at every <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mi id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><ci id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">T</annotation></semantics></math> transformer blocks, ensuring that the model remains adaptable and efficient. The decoder formulates the VQ latent vector for the target object as follows:</p>
<table id="S5.Ex8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex8.m1.3" class="ltx_Math" alttext="p(\mathbf{z})=\prod_{i=1}^{m}p\left(\mathbf{z}_{i}\mid\hat{\mathbf{c}_{v}},\mathbf{z}_{&lt;i}\right)" display="block"><semantics id="S5.Ex8.m1.3a"><mrow id="S5.Ex8.m1.3.3" xref="S5.Ex8.m1.3.3.cmml"><mrow id="S5.Ex8.m1.3.3.3" xref="S5.Ex8.m1.3.3.3.cmml"><mi id="S5.Ex8.m1.3.3.3.2" xref="S5.Ex8.m1.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.Ex8.m1.3.3.3.1" xref="S5.Ex8.m1.3.3.3.1.cmml">​</mo><mrow id="S5.Ex8.m1.3.3.3.3.2" xref="S5.Ex8.m1.3.3.3.cmml"><mo stretchy="false" id="S5.Ex8.m1.3.3.3.3.2.1" xref="S5.Ex8.m1.3.3.3.cmml">(</mo><mi id="S5.Ex8.m1.1.1" xref="S5.Ex8.m1.1.1.cmml">𝐳</mi><mo stretchy="false" id="S5.Ex8.m1.3.3.3.3.2.2" xref="S5.Ex8.m1.3.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S5.Ex8.m1.3.3.2" xref="S5.Ex8.m1.3.3.2.cmml">=</mo><mrow id="S5.Ex8.m1.3.3.1" xref="S5.Ex8.m1.3.3.1.cmml"><munderover id="S5.Ex8.m1.3.3.1.2" xref="S5.Ex8.m1.3.3.1.2.cmml"><mo movablelimits="false" id="S5.Ex8.m1.3.3.1.2.2.2" xref="S5.Ex8.m1.3.3.1.2.2.2.cmml">∏</mo><mrow id="S5.Ex8.m1.3.3.1.2.2.3" xref="S5.Ex8.m1.3.3.1.2.2.3.cmml"><mi id="S5.Ex8.m1.3.3.1.2.2.3.2" xref="S5.Ex8.m1.3.3.1.2.2.3.2.cmml">i</mi><mo id="S5.Ex8.m1.3.3.1.2.2.3.1" xref="S5.Ex8.m1.3.3.1.2.2.3.1.cmml">=</mo><mn id="S5.Ex8.m1.3.3.1.2.2.3.3" xref="S5.Ex8.m1.3.3.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.Ex8.m1.3.3.1.2.3" xref="S5.Ex8.m1.3.3.1.2.3.cmml">m</mi></munderover><mrow id="S5.Ex8.m1.3.3.1.1" xref="S5.Ex8.m1.3.3.1.1.cmml"><mi id="S5.Ex8.m1.3.3.1.1.3" xref="S5.Ex8.m1.3.3.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.Ex8.m1.3.3.1.1.2" xref="S5.Ex8.m1.3.3.1.1.2.cmml">​</mo><mrow id="S5.Ex8.m1.3.3.1.1.1.1" xref="S5.Ex8.m1.3.3.1.1.1.1.1.cmml"><mo id="S5.Ex8.m1.3.3.1.1.1.1.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex8.m1.3.3.1.1.1.1.1" xref="S5.Ex8.m1.3.3.1.1.1.1.1.cmml"><msub id="S5.Ex8.m1.3.3.1.1.1.1.1.3" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S5.Ex8.m1.3.3.1.1.1.1.1.3.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3.2.cmml">𝐳</mi><mi id="S5.Ex8.m1.3.3.1.1.1.1.1.3.3" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S5.Ex8.m1.3.3.1.1.1.1.1.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.2.cmml">∣</mo><mrow id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S5.Ex8.m1.2.2" xref="S5.Ex8.m1.2.2.cmml"><msub id="S5.Ex8.m1.2.2.2" xref="S5.Ex8.m1.2.2.2.cmml"><mi id="S5.Ex8.m1.2.2.2.2" xref="S5.Ex8.m1.2.2.2.2.cmml">𝐜</mi><mi id="S5.Ex8.m1.2.2.2.3" xref="S5.Ex8.m1.2.2.2.3.cmml">v</mi></msub><mo id="S5.Ex8.m1.2.2.1" xref="S5.Ex8.m1.2.2.1.cmml">^</mo></mover><mo id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.2.cmml">,</mo><msub id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">𝐳</mi><mrow id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.1" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo id="S5.Ex8.m1.3.3.1.1.1.1.3" xref="S5.Ex8.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex8.m1.3b"><apply id="S5.Ex8.m1.3.3.cmml" xref="S5.Ex8.m1.3.3"><eq id="S5.Ex8.m1.3.3.2.cmml" xref="S5.Ex8.m1.3.3.2"></eq><apply id="S5.Ex8.m1.3.3.3.cmml" xref="S5.Ex8.m1.3.3.3"><times id="S5.Ex8.m1.3.3.3.1.cmml" xref="S5.Ex8.m1.3.3.3.1"></times><ci id="S5.Ex8.m1.3.3.3.2.cmml" xref="S5.Ex8.m1.3.3.3.2">𝑝</ci><ci id="S5.Ex8.m1.1.1.cmml" xref="S5.Ex8.m1.1.1">𝐳</ci></apply><apply id="S5.Ex8.m1.3.3.1.cmml" xref="S5.Ex8.m1.3.3.1"><apply id="S5.Ex8.m1.3.3.1.2.cmml" xref="S5.Ex8.m1.3.3.1.2"><csymbol cd="ambiguous" id="S5.Ex8.m1.3.3.1.2.1.cmml" xref="S5.Ex8.m1.3.3.1.2">superscript</csymbol><apply id="S5.Ex8.m1.3.3.1.2.2.cmml" xref="S5.Ex8.m1.3.3.1.2"><csymbol cd="ambiguous" id="S5.Ex8.m1.3.3.1.2.2.1.cmml" xref="S5.Ex8.m1.3.3.1.2">subscript</csymbol><csymbol cd="latexml" id="S5.Ex8.m1.3.3.1.2.2.2.cmml" xref="S5.Ex8.m1.3.3.1.2.2.2">product</csymbol><apply id="S5.Ex8.m1.3.3.1.2.2.3.cmml" xref="S5.Ex8.m1.3.3.1.2.2.3"><eq id="S5.Ex8.m1.3.3.1.2.2.3.1.cmml" xref="S5.Ex8.m1.3.3.1.2.2.3.1"></eq><ci id="S5.Ex8.m1.3.3.1.2.2.3.2.cmml" xref="S5.Ex8.m1.3.3.1.2.2.3.2">𝑖</ci><cn type="integer" id="S5.Ex8.m1.3.3.1.2.2.3.3.cmml" xref="S5.Ex8.m1.3.3.1.2.2.3.3">1</cn></apply></apply><ci id="S5.Ex8.m1.3.3.1.2.3.cmml" xref="S5.Ex8.m1.3.3.1.2.3">𝑚</ci></apply><apply id="S5.Ex8.m1.3.3.1.1.cmml" xref="S5.Ex8.m1.3.3.1.1"><times id="S5.Ex8.m1.3.3.1.1.2.cmml" xref="S5.Ex8.m1.3.3.1.1.2"></times><ci id="S5.Ex8.m1.3.3.1.1.3.cmml" xref="S5.Ex8.m1.3.3.1.1.3">𝑝</ci><apply id="S5.Ex8.m1.3.3.1.1.1.1.1.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S5.Ex8.m1.3.3.1.1.1.1.1.2.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.2">conditional</csymbol><apply id="S5.Ex8.m1.3.3.1.1.1.1.1.3.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.Ex8.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S5.Ex8.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3.2">𝐳</ci><ci id="S5.Ex8.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S5.Ex8.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1"><apply id="S5.Ex8.m1.2.2.cmml" xref="S5.Ex8.m1.2.2"><ci id="S5.Ex8.m1.2.2.1.cmml" xref="S5.Ex8.m1.2.2.1">^</ci><apply id="S5.Ex8.m1.2.2.2.cmml" xref="S5.Ex8.m1.2.2.2"><csymbol cd="ambiguous" id="S5.Ex8.m1.2.2.2.1.cmml" xref="S5.Ex8.m1.2.2.2">subscript</csymbol><ci id="S5.Ex8.m1.2.2.2.2.cmml" xref="S5.Ex8.m1.2.2.2.2">𝐜</ci><ci id="S5.Ex8.m1.2.2.2.3.cmml" xref="S5.Ex8.m1.2.2.2.3">𝑣</ci></apply></apply><apply id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.2">𝐳</ci><apply id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3"><lt id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.Ex8.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex8.m1.3c">p(\mathbf{z})=\prod_{i=1}^{m}p\left(\mathbf{z}_{i}\mid\hat{\mathbf{c}_{v}},\mathbf{z}_{&lt;i}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS4.p2.3" class="ltx_p">Subsequently, the 3D VQ-decoder decodes this latent vector <math id="S5.SS4.p2.2.m1.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S5.SS4.p2.2.m1.1a"><mi id="S5.SS4.p2.2.m1.1.1" xref="S5.SS4.p2.2.m1.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m1.1b"><ci id="S5.SS4.p2.2.m1.1.1.cmml" xref="S5.SS4.p2.2.m1.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m1.1c">\mathbf{z}</annotation></semantics></math> into the final 3D mesh representation <math id="S5.SS4.p2.3.m2.1" class="ltx_Math" alttext="\Psi" display="inline"><semantics id="S5.SS4.p2.3.m2.1a"><mi mathvariant="normal" id="S5.SS4.p2.3.m2.1.1" xref="S5.SS4.p2.3.m2.1.1.cmml">Ψ</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m2.1b"><ci id="S5.SS4.p2.3.m2.1.1.cmml" xref="S5.SS4.p2.3.m2.1.1">Ψ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m2.1c">\Psi</annotation></semantics></math>:</p>
<table id="S5.Ex9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex9.m1.1" class="ltx_Math" alttext="\Psi=D(\mathbf{z})" display="block"><semantics id="S5.Ex9.m1.1a"><mrow id="S5.Ex9.m1.1.2" xref="S5.Ex9.m1.1.2.cmml"><mi mathvariant="normal" id="S5.Ex9.m1.1.2.2" xref="S5.Ex9.m1.1.2.2.cmml">Ψ</mi><mo id="S5.Ex9.m1.1.2.1" xref="S5.Ex9.m1.1.2.1.cmml">=</mo><mrow id="S5.Ex9.m1.1.2.3" xref="S5.Ex9.m1.1.2.3.cmml"><mi id="S5.Ex9.m1.1.2.3.2" xref="S5.Ex9.m1.1.2.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.Ex9.m1.1.2.3.1" xref="S5.Ex9.m1.1.2.3.1.cmml">​</mo><mrow id="S5.Ex9.m1.1.2.3.3.2" xref="S5.Ex9.m1.1.2.3.cmml"><mo stretchy="false" id="S5.Ex9.m1.1.2.3.3.2.1" xref="S5.Ex9.m1.1.2.3.cmml">(</mo><mi id="S5.Ex9.m1.1.1" xref="S5.Ex9.m1.1.1.cmml">𝐳</mi><mo stretchy="false" id="S5.Ex9.m1.1.2.3.3.2.2" xref="S5.Ex9.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex9.m1.1b"><apply id="S5.Ex9.m1.1.2.cmml" xref="S5.Ex9.m1.1.2"><eq id="S5.Ex9.m1.1.2.1.cmml" xref="S5.Ex9.m1.1.2.1"></eq><ci id="S5.Ex9.m1.1.2.2.cmml" xref="S5.Ex9.m1.1.2.2">Ψ</ci><apply id="S5.Ex9.m1.1.2.3.cmml" xref="S5.Ex9.m1.1.2.3"><times id="S5.Ex9.m1.1.2.3.1.cmml" xref="S5.Ex9.m1.1.2.3.1"></times><ci id="S5.Ex9.m1.1.2.3.2.cmml" xref="S5.Ex9.m1.1.2.3.2">𝐷</ci><ci id="S5.Ex9.m1.1.1.cmml" xref="S5.Ex9.m1.1.1">𝐳</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex9.m1.1c">\Psi=D(\mathbf{z})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS4.p2.4" class="ltx_p">The decoder aims to minimize the negative log-likelihood loss during optimization:</p>
<table id="S5.Ex10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex10.m1.3" class="ltx_Math" alttext="\mathcal{L}_{nll}=\mathbb{E}_{\mathbf{x}\sim p(\mathbf{x})}\left[-\log p(\mathbf{z})\right]" display="block"><semantics id="S5.Ex10.m1.3a"><mrow id="S5.Ex10.m1.3.3" xref="S5.Ex10.m1.3.3.cmml"><msub id="S5.Ex10.m1.3.3.3" xref="S5.Ex10.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex10.m1.3.3.3.2" xref="S5.Ex10.m1.3.3.3.2.cmml">ℒ</mi><mrow id="S5.Ex10.m1.3.3.3.3" xref="S5.Ex10.m1.3.3.3.3.cmml"><mi id="S5.Ex10.m1.3.3.3.3.2" xref="S5.Ex10.m1.3.3.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.Ex10.m1.3.3.3.3.1" xref="S5.Ex10.m1.3.3.3.3.1.cmml">​</mo><mi id="S5.Ex10.m1.3.3.3.3.3" xref="S5.Ex10.m1.3.3.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.Ex10.m1.3.3.3.3.1a" xref="S5.Ex10.m1.3.3.3.3.1.cmml">​</mo><mi id="S5.Ex10.m1.3.3.3.3.4" xref="S5.Ex10.m1.3.3.3.3.4.cmml">l</mi></mrow></msub><mo id="S5.Ex10.m1.3.3.2" xref="S5.Ex10.m1.3.3.2.cmml">=</mo><mrow id="S5.Ex10.m1.3.3.1" xref="S5.Ex10.m1.3.3.1.cmml"><msub id="S5.Ex10.m1.3.3.1.3" xref="S5.Ex10.m1.3.3.1.3.cmml"><mi id="S5.Ex10.m1.3.3.1.3.2" xref="S5.Ex10.m1.3.3.1.3.2.cmml">𝔼</mi><mrow id="S5.Ex10.m1.1.1.1" xref="S5.Ex10.m1.1.1.1.cmml"><mi id="S5.Ex10.m1.1.1.1.3" xref="S5.Ex10.m1.1.1.1.3.cmml">𝐱</mi><mo id="S5.Ex10.m1.1.1.1.2" xref="S5.Ex10.m1.1.1.1.2.cmml">∼</mo><mrow id="S5.Ex10.m1.1.1.1.4" xref="S5.Ex10.m1.1.1.1.4.cmml"><mi id="S5.Ex10.m1.1.1.1.4.2" xref="S5.Ex10.m1.1.1.1.4.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.Ex10.m1.1.1.1.4.1" xref="S5.Ex10.m1.1.1.1.4.1.cmml">​</mo><mrow id="S5.Ex10.m1.1.1.1.4.3.2" xref="S5.Ex10.m1.1.1.1.4.cmml"><mo stretchy="false" id="S5.Ex10.m1.1.1.1.4.3.2.1" xref="S5.Ex10.m1.1.1.1.4.cmml">(</mo><mi id="S5.Ex10.m1.1.1.1.1" xref="S5.Ex10.m1.1.1.1.1.cmml">𝐱</mi><mo stretchy="false" id="S5.Ex10.m1.1.1.1.4.3.2.2" xref="S5.Ex10.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S5.Ex10.m1.3.3.1.2" xref="S5.Ex10.m1.3.3.1.2.cmml">​</mo><mrow id="S5.Ex10.m1.3.3.1.1.1" xref="S5.Ex10.m1.3.3.1.1.2.cmml"><mo id="S5.Ex10.m1.3.3.1.1.1.2" xref="S5.Ex10.m1.3.3.1.1.2.1.cmml">[</mo><mrow id="S5.Ex10.m1.3.3.1.1.1.1" xref="S5.Ex10.m1.3.3.1.1.1.1.cmml"><mo rspace="0.167em" id="S5.Ex10.m1.3.3.1.1.1.1a" xref="S5.Ex10.m1.3.3.1.1.1.1.cmml">−</mo><mrow id="S5.Ex10.m1.3.3.1.1.1.1.2" xref="S5.Ex10.m1.3.3.1.1.1.1.2.cmml"><mrow id="S5.Ex10.m1.3.3.1.1.1.1.2.2" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S5.Ex10.m1.3.3.1.1.1.1.2.2.1" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S5.Ex10.m1.3.3.1.1.1.1.2.2a" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.cmml">⁡</mo><mi id="S5.Ex10.m1.3.3.1.1.1.1.2.2.2" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S5.Ex10.m1.3.3.1.1.1.1.2.1" xref="S5.Ex10.m1.3.3.1.1.1.1.2.1.cmml">​</mo><mrow id="S5.Ex10.m1.3.3.1.1.1.1.2.3.2" xref="S5.Ex10.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.Ex10.m1.3.3.1.1.1.1.2.3.2.1" xref="S5.Ex10.m1.3.3.1.1.1.1.2.cmml">(</mo><mi id="S5.Ex10.m1.2.2" xref="S5.Ex10.m1.2.2.cmml">𝐳</mi><mo stretchy="false" id="S5.Ex10.m1.3.3.1.1.1.1.2.3.2.2" xref="S5.Ex10.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S5.Ex10.m1.3.3.1.1.1.3" xref="S5.Ex10.m1.3.3.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex10.m1.3b"><apply id="S5.Ex10.m1.3.3.cmml" xref="S5.Ex10.m1.3.3"><eq id="S5.Ex10.m1.3.3.2.cmml" xref="S5.Ex10.m1.3.3.2"></eq><apply id="S5.Ex10.m1.3.3.3.cmml" xref="S5.Ex10.m1.3.3.3"><csymbol cd="ambiguous" id="S5.Ex10.m1.3.3.3.1.cmml" xref="S5.Ex10.m1.3.3.3">subscript</csymbol><ci id="S5.Ex10.m1.3.3.3.2.cmml" xref="S5.Ex10.m1.3.3.3.2">ℒ</ci><apply id="S5.Ex10.m1.3.3.3.3.cmml" xref="S5.Ex10.m1.3.3.3.3"><times id="S5.Ex10.m1.3.3.3.3.1.cmml" xref="S5.Ex10.m1.3.3.3.3.1"></times><ci id="S5.Ex10.m1.3.3.3.3.2.cmml" xref="S5.Ex10.m1.3.3.3.3.2">𝑛</ci><ci id="S5.Ex10.m1.3.3.3.3.3.cmml" xref="S5.Ex10.m1.3.3.3.3.3">𝑙</ci><ci id="S5.Ex10.m1.3.3.3.3.4.cmml" xref="S5.Ex10.m1.3.3.3.3.4">𝑙</ci></apply></apply><apply id="S5.Ex10.m1.3.3.1.cmml" xref="S5.Ex10.m1.3.3.1"><times id="S5.Ex10.m1.3.3.1.2.cmml" xref="S5.Ex10.m1.3.3.1.2"></times><apply id="S5.Ex10.m1.3.3.1.3.cmml" xref="S5.Ex10.m1.3.3.1.3"><csymbol cd="ambiguous" id="S5.Ex10.m1.3.3.1.3.1.cmml" xref="S5.Ex10.m1.3.3.1.3">subscript</csymbol><ci id="S5.Ex10.m1.3.3.1.3.2.cmml" xref="S5.Ex10.m1.3.3.1.3.2">𝔼</ci><apply id="S5.Ex10.m1.1.1.1.cmml" xref="S5.Ex10.m1.1.1.1"><csymbol cd="latexml" id="S5.Ex10.m1.1.1.1.2.cmml" xref="S5.Ex10.m1.1.1.1.2">similar-to</csymbol><ci id="S5.Ex10.m1.1.1.1.3.cmml" xref="S5.Ex10.m1.1.1.1.3">𝐱</ci><apply id="S5.Ex10.m1.1.1.1.4.cmml" xref="S5.Ex10.m1.1.1.1.4"><times id="S5.Ex10.m1.1.1.1.4.1.cmml" xref="S5.Ex10.m1.1.1.1.4.1"></times><ci id="S5.Ex10.m1.1.1.1.4.2.cmml" xref="S5.Ex10.m1.1.1.1.4.2">𝑝</ci><ci id="S5.Ex10.m1.1.1.1.1.cmml" xref="S5.Ex10.m1.1.1.1.1">𝐱</ci></apply></apply></apply><apply id="S5.Ex10.m1.3.3.1.1.2.cmml" xref="S5.Ex10.m1.3.3.1.1.1"><csymbol cd="latexml" id="S5.Ex10.m1.3.3.1.1.2.1.cmml" xref="S5.Ex10.m1.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S5.Ex10.m1.3.3.1.1.1.1.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1"><minus id="S5.Ex10.m1.3.3.1.1.1.1.1.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1"></minus><apply id="S5.Ex10.m1.3.3.1.1.1.1.2.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1.2"><times id="S5.Ex10.m1.3.3.1.1.1.1.2.1.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1.2.1"></times><apply id="S5.Ex10.m1.3.3.1.1.1.1.2.2.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2"><log id="S5.Ex10.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.1"></log><ci id="S5.Ex10.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S5.Ex10.m1.3.3.1.1.1.1.2.2.2">𝑝</ci></apply><ci id="S5.Ex10.m1.2.2.cmml" xref="S5.Ex10.m1.2.2">𝐳</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex10.m1.3c">\mathcal{L}_{nll}=\mathbb{E}_{\mathbf{x}\sim p(\mathbf{x})}\left[-\log p(\mathbf{z})\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.SS4.p2.5" class="ltx_p">For training, the decoder is fine-tuned from pre-trained weights, with the trained neuro-fusion encoder and the feature bridge diffusion model. This strategy preserves Argus’s generative capabilities while adapting to new inputs and minimizing the computational overhead associated with training.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table II: </span>
<span id="S5.T2.9.1" class="ltx_text ltx_font_bold">Performance Comparison on fMRI-Shape.</span> We report the average metrics for each subject trained and tested on their own data in the core part. LEA-3D and fMRI-PTE-3D represent variants of LEA and fMRI-PTE, respectively. The following three baselines are ablation studies for MinD-3D.

</figcaption>
<table id="S5.T2.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.7.8.1" class="ltx_tr">
<th id="S5.T2.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;" rowspan="2"><span id="S5.T2.7.8.1.1.1" class="ltx_text ltx_font_smallcaps">Methods</span></th>
<th id="S5.T2.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;" colspan="3">Semantic-Level</th>
<th id="S5.T2.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;" colspan="4">Structure-Level</th>
</tr>
<tr id="S5.T2.7.7" class="ltx_tr">
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">2-way<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">10-way<math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S5.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">LPIPS<math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mo stretchy="false" id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">SSIM<math id="S5.T2.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.4.4.4.m1.1a"><mo stretchy="false" id="S5.T2.4.4.4.m1.1.1" xref="S5.T2.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S5.T2.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">FPD<math id="S5.T2.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.5.5.5.m1.1a"><mo stretchy="false" id="S5.T2.5.5.5.m1.1.1" xref="S5.T2.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.m1.1b"><ci id="S5.T2.5.5.5.m1.1.1.cmml" xref="S5.T2.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T2.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">CD<math id="S5.T2.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.6.6.6.m1.1a"><mo stretchy="false" id="S5.T2.6.6.6.m1.1.1" xref="S5.T2.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.m1.1b"><ci id="S5.T2.6.6.6.m1.1.1.cmml" xref="S5.T2.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T2.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:11.4pt;padding-right:11.4pt;">EMD<math id="S5.T2.7.7.7.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.7.7.7.m1.1a"><mo stretchy="false" id="S5.T2.7.7.7.m1.1.1" xref="S5.T2.7.7.7.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.m1.1b"><ci id="S5.T2.7.7.7.m1.1.1.cmml" xref="S5.T2.7.7.7.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.7.9.1" class="ltx_tr">
<td id="S5.T2.7.9.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">LEA-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S5.T2.7.9.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.787</td>
<td id="S5.T2.7.9.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.371</td>
<td id="S5.T2.7.9.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.527</td>
<td id="S5.T2.7.9.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">0.562</td>
<td id="S5.T2.7.9.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">4.229</td>
<td id="S5.T2.7.9.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">2.291</td>
<td id="S5.T2.7.9.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:11.4pt;padding-right:11.4pt;">5.347</td>
</tr>
<tr id="S5.T2.7.10.2" class="ltx_tr">
<td id="S5.T2.7.10.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">fMRI-PTE-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S5.T2.7.10.2.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.815</td>
<td id="S5.T2.7.10.2.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.392</td>
<td id="S5.T2.7.10.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">0.433</td>
<td id="S5.T2.7.10.2.5" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.694</td>
<td id="S5.T2.7.10.2.6" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">3.571</td>
<td id="S5.T2.7.10.2.7" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">1.992</td>
<td id="S5.T2.7.10.2.8" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">4.621</td>
</tr>
<tr id="S5.T2.7.11.3" class="ltx_tr">
<td id="S5.T2.7.11.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">w/o Both</td>
<td id="S5.T2.7.11.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">0.789</td>
<td id="S5.T2.7.11.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">0.367</td>
<td id="S5.T2.7.11.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">0.479</td>
<td id="S5.T2.7.11.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">0.616</td>
<td id="S5.T2.7.11.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">3.694</td>
<td id="S5.T2.7.11.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">2.205</td>
<td id="S5.T2.7.11.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:11.4pt;padding-right:11.4pt;">5.073</td>
</tr>
<tr id="S5.T2.7.12.4" class="ltx_tr">
<td id="S5.T2.7.12.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">w/o Diffusion</td>
<td id="S5.T2.7.12.4.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.801</td>
<td id="S5.T2.7.12.4.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.385</td>
<td id="S5.T2.7.12.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">0.423</td>
<td id="S5.T2.7.12.4.5" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.669</td>
<td id="S5.T2.7.12.4.6" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">3.526</td>
<td id="S5.T2.7.12.4.7" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">2.071</td>
<td id="S5.T2.7.12.4.8" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">4.625</td>
</tr>
<tr id="S5.T2.7.13.5" class="ltx_tr">
<td id="S5.T2.7.13.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">w/o Contrastive</td>
<td id="S5.T2.7.13.5.2" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.823</td>
<td id="S5.T2.7.13.5.3" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.419</td>
<td id="S5.T2.7.13.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">0.319</td>
<td id="S5.T2.7.13.5.5" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">0.701</td>
<td id="S5.T2.7.13.5.6" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">3.315</td>
<td id="S5.T2.7.13.5.7" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">1.826</td>
<td id="S5.T2.7.13.5.8" class="ltx_td ltx_align_center" style="padding-left:11.4pt;padding-right:11.4pt;">4.027</td>
</tr>
<tr id="S5.T2.7.14.6" class="ltx_tr">
<td id="S5.T2.7.14.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;">MinD-3D (full)</td>
<td id="S5.T2.7.14.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.2.1" class="ltx_text ltx_font_bold">0.839</span></td>
<td id="S5.T2.7.14.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.3.1" class="ltx_text ltx_font_bold">0.432</span></td>
<td id="S5.T2.7.14.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.4.1" class="ltx_text ltx_font_bold">0.230</span></td>
<td id="S5.T2.7.14.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.5.1" class="ltx_text ltx_font_bold">0.734</span></td>
<td id="S5.T2.7.14.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.6.1" class="ltx_text ltx_font_bold">3.157</span></td>
<td id="S5.T2.7.14.6.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.7.1" class="ltx_text ltx_font_bold">1.742</span></td>
<td id="S5.T2.7.14.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:11.4pt;padding-right:11.4pt;"><span id="S5.T2.7.14.6.8.1" class="ltx_text ltx_font_bold">3.833</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Benchmark</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">To establish new benchmarks for 3D visual decoding, we conduct experiments in both standard and Out-of-Distribution (OOD) settings. In this section, we introduce the metrics and provide details of the experiments.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span id="S6.SS1.1.1" class="ltx_text ltx_font_italic">Metrics</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">To effectively evaluate the performance of our models on this novel task, we employ metrics across two key dimensions: semantic-level and structure-level.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">Semantic Level.</span> To assess the semantic quality of our model, we adopt standard metrics commonly used in prior 2D fMRI studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, specifically N-way top-K accuracy. We report 2-way-top-1 and 10-way-top-1 accuracy, as shown in Table <a href="#S5.T2" title="Table II ‣ 5.4 Latent Adapted Decoder ‣ 5 Method ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Additionally, we calculate the Learned Perceptual Image Patch Similarity (LPIPS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to evaluate the perceptual quality of the semantic information. These metrics are computed by comparing the reconstructed images with the ground truth (GT) images. Both the reconstructed and GT objects are rendered into images at every 60-degree rotation, and the metrics are calculated for each frame. The final score is derived by averaging the values across frames.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.3" class="ltx_p"><span id="S6.SS1.p3.3.1" class="ltx_text ltx_font_bold">Structure Level.</span>
In addition to semantic evaluation, it is essential to measure how well our model captures the geometrical structure of objects. We utilize common 3D reconstruction metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>: Fréchet Point Cloud Distance (FPD, scaled by <math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times 10^{-1}" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mrow id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml"><mi id="S6.SS1.p3.1.m1.1.1.2" xref="S6.SS1.p3.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.p3.1.m1.1.1.1" xref="S6.SS1.p3.1.m1.1.1.1.cmml">×</mo><msup id="S6.SS1.p3.1.m1.1.1.3" xref="S6.SS1.p3.1.m1.1.1.3.cmml"><mn id="S6.SS1.p3.1.m1.1.1.3.2" xref="S6.SS1.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.SS1.p3.1.m1.1.1.3.3" xref="S6.SS1.p3.1.m1.1.1.3.3.cmml"><mo id="S6.SS1.p3.1.m1.1.1.3.3a" xref="S6.SS1.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S6.SS1.p3.1.m1.1.1.3.3.2" xref="S6.SS1.p3.1.m1.1.1.3.3.2.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><apply id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"><times id="S6.SS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1.1"></times><csymbol cd="latexml" id="S6.SS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.p3.1.m1.1.1.2">absent</csymbol><apply id="S6.SS1.p3.1.m1.1.1.3.cmml" xref="S6.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.p3.1.m1.1.1.3.1.cmml" xref="S6.SS1.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S6.SS1.p3.1.m1.1.1.3.2.cmml" xref="S6.SS1.p3.1.m1.1.1.3.2">10</cn><apply id="S6.SS1.p3.1.m1.1.1.3.3.cmml" xref="S6.SS1.p3.1.m1.1.1.3.3"><minus id="S6.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S6.SS1.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S6.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S6.SS1.p3.1.m1.1.1.3.3.2">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times 10^{-1}</annotation></semantics></math>), Chamfer Distance (CD, scaled by <math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times 10^{2}" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mrow id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mi id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.p3.2.m2.1.1.1" xref="S6.SS1.p3.2.m2.1.1.1.cmml">×</mo><msup id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml"><mn id="S6.SS1.p3.2.m2.1.1.3.2" xref="S6.SS1.p3.2.m2.1.1.3.2.cmml">10</mn><mn id="S6.SS1.p3.2.m2.1.1.3.3" xref="S6.SS1.p3.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><times id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1.1"></times><csymbol cd="latexml" id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">absent</csymbol><apply id="S6.SS1.p3.2.m2.1.1.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.p3.2.m2.1.1.3.1.cmml" xref="S6.SS1.p3.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S6.SS1.p3.2.m2.1.1.3.2.cmml" xref="S6.SS1.p3.2.m2.1.1.3.2">10</cn><cn type="integer" id="S6.SS1.p3.2.m2.1.1.3.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\times 10^{2}</annotation></semantics></math>), and Earth Mover’s Distance (EMD, scaled by <math id="S6.SS1.p3.3.m3.1" class="ltx_Math" alttext="\times 10^{2}" display="inline"><semantics id="S6.SS1.p3.3.m3.1a"><mrow id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml"><mi id="S6.SS1.p3.3.m3.1.1.2" xref="S6.SS1.p3.3.m3.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.p3.3.m3.1.1.1" xref="S6.SS1.p3.3.m3.1.1.1.cmml">×</mo><msup id="S6.SS1.p3.3.m3.1.1.3" xref="S6.SS1.p3.3.m3.1.1.3.cmml"><mn id="S6.SS1.p3.3.m3.1.1.3.2" xref="S6.SS1.p3.3.m3.1.1.3.2.cmml">10</mn><mn id="S6.SS1.p3.3.m3.1.1.3.3" xref="S6.SS1.p3.3.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><apply id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"><times id="S6.SS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1.1"></times><csymbol cd="latexml" id="S6.SS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.p3.3.m3.1.1.2">absent</csymbol><apply id="S6.SS1.p3.3.m3.1.1.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.p3.3.m3.1.1.3.1.cmml" xref="S6.SS1.p3.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S6.SS1.p3.3.m3.1.1.3.2.cmml" xref="S6.SS1.p3.3.m3.1.1.3.2">10</cn><cn type="integer" id="S6.SS1.p3.3.m3.1.1.3.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">\times 10^{2}</annotation></semantics></math>). These metrics are computed by sampling point clouds from the GT and reconstructed meshes. Additionally, we calculate the frame-averaged Structural Similarity Index (SSIM) using the same rendering process to evaluate similarity from various viewpoints.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span id="S6.SS2.1.1" class="ltx_text ltx_font_italic">Implementation Details</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">As detailed in Sec. <a href="#S3" title="3 Experimental Designs and Curated Dataset ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, each data pair in the fMRI-Shape dataset consists of 10 fMRI frames and 192 corresponding images. For the experiment for fMRI-Shape, to maximize dataset usage and incorporate data augmentation, we randomly select 6 fMRI frames for training. The original 1023 × 2514 2D fMRI images are resized to 256 × 256 for processing. For the image data, 8 frames are randomly selected for training, with each image resized to 224 × 224. During inference, only the middle 6 fMRI frames are used, without additional images.
Regarding the model architecture, we set <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="T=4" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mi id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">T</mi><mo id="S6.SS2.p1.1.m1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><eq id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"></eq><ci id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">T=4</annotation></semantics></math> to incorporate adapter layers.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">The model training process is divided into two stages. In the first stage, we optimize the neuro-fusion encoder and the feature bridge diffusion model. The neuro-fusion encoder is initialized with pre-trained weights, while the feature bridge diffusion model is trained from scratch, which takes approximately 2.5 days on a single A100 GPU. In the second stage, all parameters, including those of the adapters, are optimized, while the parameters in Argus are frozen. This stage is completed in roughly one day on a single A100 GPU.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span id="S6.SS3.1.1" class="ltx_text ltx_font_italic">Standard Experiment</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">As the first attempt to model 3D imaging within the human brain, we define the standard experiment setting as training and testing on the pre-split, person-specific Core set in fMRI-Shape, using the defined metrics to evaluate our model’s performance. Given the complexity of this task, which involves various brain regions, direct comparisons with existing models are not feasible. To reduce training costs, we adapted the LEA and fMRI-PTE models, which were trained on the same vision ROIs and have strong performance. These models utilize the same 3D decoder as MinD-3D. These adaptations serve as baselines in this study, enabling a more contextual and fair comparison within this novel domain.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">Tab. <a href="#S5.T2" title="Table II ‣ 5.4 Latent Adapted Decoder ‣ 5 Method ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reports the averaged metrics at both the structural and semantic levels for all subjects. MinD-3D demonstrates superior performance across all metrics, achieving approximately 83.9% accuracy in 2-way-top-1, 43.2% accuracy in 10-way-top-1, and scores of 0.734, 0.230, 3.157, 1.742, and 3.833 in other metrics. These results indicate that our model not only excels in generating objects with high semantic accuracy but also performs exceptionally well in preserving structural similarity, outperforming baseline methods. Tab. <a href="#S5.T2" title="Table II ‣ 5.4 Latent Adapted Decoder ‣ 5 Method ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> also highlights the effectiveness of both the diffusion model and the contrastive learning module, which play critical roles in the success of MinD-3D.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Qualitative results from LEA-3D, fMRI-PTE-3D, and our method are presented in Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.3 Feature Bridge Diffusion Model ‣ 5 Method ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. MinD-3D consistently produces 3D objects that are structurally similar to their real counterparts, while maintaining semantic integrity in most cases. This highlights the robustness of our model in handling this challenging task and its ability to generate faithful reconstructions.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">Furthermore, given the individual differences highlighted in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 3D Generation ‣ 2 Related work ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we also examine our model’s performance on each subject to assess its generalization ability. Fig. <a href="#S6.F9" title="Figure 9 ‣ 6.3 Standard Experiment ‣ 6 Experiments and Benchmark ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows several examples from 5 different subjects, demonstrating the effectiveness of the MinD-3D model. Due to varying brain responses among subjects to the same objects, some differences are observed in the results. However, overall, the model successfully reconstructs both the category and characteristics of the objects. This validates the feasibility of the proposed task, Recon3DMind, and the associated fMRI-Shape dataset developed for this study.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2409.11315/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="385" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
<span id="S6.F9.2.1" class="ltx_text ltx_font_bold">Samples from different subjects.</span> To demonstrate the effectiveness of our MinD-3D model, we present results from different subjects, with the model trained specifically on each individual’s data.

</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span><span id="S6.SS4.1.1" class="ltx_text ltx_font_italic">Out-Of-Distribution Experiments</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">To effectively utilize a subset of the fMRI-Shape dataset and further assess the generalization capabilities of our proposed MinD-3D model, we conduct two Out-Of-Distribution (OOD) experiments under challenging settings:
<span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_bold">1) Across-Person Testing (APT):</span> In APT, we evaluate our model, which was trained only on Subject 1, using the data from Subject 9. We compare the results with the baselines and report the metrics in Tab. <a href="#S6.T3" title="Table III ‣ 6.4 Out-Of-Distribution Experiments ‣ 6 Experiments and Benchmark ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
<span id="S6.SS4.p1.1.2" class="ltx_text ltx_font_bold">2) Across-Person &amp; Across-Class Testing (APACT):</span> In APACT, we similarly evaluate our model, trained solely on Subject 1, with the data from Subject 11. We also compare with the baselines and report the metrics in Tab. <a href="#S6.T3" title="Table III ‣ 6.4 Out-Of-Distribution Experiments ‣ 6 Experiments and Benchmark ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">We present the reconstructed objects from AP &amp; APAC testing in Fig. <a href="#S6.F10" title="Figure 10 ‣ 6.4 Out-Of-Distribution Experiments ‣ 6 Experiments and Benchmark ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. As shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 3D Generation ‣ 2 Related work ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, individual differences significantly impact the results, which our AP &amp; APAC tests empirically confirm. Despite the high difficulty, MinD-3D successfully recovers the basic shapes of the objects, providing a strong baseline for the community. While performance in these OOD scenarios does not reach In-Distribution (ID) levels—an expected outcome given the task’s complexity and the substantial individual differences and domain gaps—our method still surpasses existing baselines. This demonstrates the robustness of MinD-3D and establishes a new benchmark for future work.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table III: </span>Quantitative results of APT and APACT. We use the model trained on Subject 1 and compare metrics for Subjects 9 and 11 separately.</figcaption>
<table id="S6.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.6.7.1" class="ltx_tr">
<th id="S6.T3.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S6.T3.6.7.1.1.1" class="ltx_text ltx_font_smallcaps">Methods</span></th>
<th id="S6.T3.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3">APT</th>
<th id="S6.T3.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">APACT</th>
</tr>
<tr id="S6.T3.6.6" class="ltx_tr">
<th id="S6.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">FPD<math id="S6.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.1.1.1.m1.1a"><mo stretchy="false" id="S6.T3.1.1.1.m1.1.1" xref="S6.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S6.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">CD<math id="S6.T3.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.2.2.2.m1.1a"><mo stretchy="false" id="S6.T3.2.2.2.m1.1.1" xref="S6.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.m1.1b"><ci id="S6.T3.2.2.2.m1.1.1.cmml" xref="S6.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S6.T3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">EMD<math id="S6.T3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.3.3.3.m1.1a"><mo stretchy="false" id="S6.T3.3.3.3.m1.1.1" xref="S6.T3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.3.m1.1b"><ci id="S6.T3.3.3.3.m1.1.1.cmml" xref="S6.T3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S6.T3.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">FPD<math id="S6.T3.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.4.4.4.m1.1a"><mo stretchy="false" id="S6.T3.4.4.4.m1.1.1" xref="S6.T3.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.4.m1.1b"><ci id="S6.T3.4.4.4.m1.1.1.cmml" xref="S6.T3.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S6.T3.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">CD<math id="S6.T3.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.5.5.5.m1.1a"><mo stretchy="false" id="S6.T3.5.5.5.m1.1.1" xref="S6.T3.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.5.5.5.m1.1b"><ci id="S6.T3.5.5.5.m1.1.1.cmml" xref="S6.T3.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S6.T3.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">EMD<math id="S6.T3.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.6.6.6.m1.1a"><mo stretchy="false" id="S6.T3.6.6.6.m1.1.1" xref="S6.T3.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.6.6.6.m1.1b"><ci id="S6.T3.6.6.6.m1.1.1.cmml" xref="S6.T3.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.6.8.1" class="ltx_tr">
<th id="S6.T3.6.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">LEA-3D</th>
<td id="S6.T3.6.8.1.2" class="ltx_td ltx_align_center ltx_border_tt">5.362</td>
<td id="S6.T3.6.8.1.3" class="ltx_td ltx_align_center ltx_border_tt">3.627</td>
<td id="S6.T3.6.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6.174</td>
<td id="S6.T3.6.8.1.5" class="ltx_td ltx_align_center ltx_border_tt">6.958</td>
<td id="S6.T3.6.8.1.6" class="ltx_td ltx_align_center ltx_border_tt">4.944</td>
<td id="S6.T3.6.8.1.7" class="ltx_td ltx_align_center ltx_border_tt">8.107</td>
</tr>
<tr id="S6.T3.6.9.2" class="ltx_tr">
<th id="S6.T3.6.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">fMRI-PTE-3D</th>
<td id="S6.T3.6.9.2.2" class="ltx_td ltx_align_center">4.501</td>
<td id="S6.T3.6.9.2.3" class="ltx_td ltx_align_center">2.956</td>
<td id="S6.T3.6.9.2.4" class="ltx_td ltx_align_center ltx_border_r">5.772</td>
<td id="S6.T3.6.9.2.5" class="ltx_td ltx_align_center">6.261</td>
<td id="S6.T3.6.9.2.6" class="ltx_td ltx_align_center">4.570</td>
<td id="S6.T3.6.9.2.7" class="ltx_td ltx_align_center">7.843</td>
</tr>
<tr id="S6.T3.6.10.3" class="ltx_tr">
<th id="S6.T3.6.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">MinD-3D</th>
<td id="S6.T3.6.10.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.6.10.3.2.1" class="ltx_text ltx_font_bold">3.838</span></td>
<td id="S6.T3.6.10.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.6.10.3.3.1" class="ltx_text ltx_font_bold">2.415</span></td>
<td id="S6.T3.6.10.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S6.T3.6.10.3.4.1" class="ltx_text ltx_font_bold">5.117</span></td>
<td id="S6.T3.6.10.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.6.10.3.5.1" class="ltx_text ltx_font_bold">5.689</span></td>
<td id="S6.T3.6.10.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.6.10.3.6.1" class="ltx_text ltx_font_bold">4.181</span></td>
<td id="S6.T3.6.10.3.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.6.10.3.7.1" class="ltx_text ltx_font_bold">7.194</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2409.11315/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
<span id="S6.F10.2.1" class="ltx_text ltx_font_bold">Visualization of AP &amp; APAC testing.</span> We show some results of AP and APAC testing separately. AP testing trains on Subject 1 and tests on Subject 9, while APAC testing trains on Subject 1 and tests on Subject 11.

</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Analysis</span>
</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span><span id="S7.SS1.1.1" class="ltx_text ltx_font_italic">Voxel Importance Analysis for Object Angle Variations</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In this section, we explore the pattern when the angle of the object changes. Most directly, we perform linear regression on the whole-brain fMRI signal data for classification. The weight of each voxel in the brain serves as its importance score. Specifically, we performed logistic regression on the whole-brain fMRI signals using all voxels from subject 1 while viewing objects from two different angles (the 3rd and 9th frames of fMRI). We randomly selected 70% of the data for training and used the remaining 30% for testing. The accuracy of the linear classifier is 91.34%. To evaluate the importance of each voxel, we visualized the absolute values of the classifier weights in Fig. <a href="#S7.F11" title="Figure 11 ‣ 7.1 Voxel Importance Analysis for Object Angle Variations ‣ 7 Analysis ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. Red regions indicate voxels with higher coefficients, while others represent lower coefficients. These ROIs include “2, AIP, FFC, FST, IPS1, LIPv, LO3, MIP, MST, MT, PH, PFt, PGp, PHT, TPOJ2, V1, V2, V3, V3A, V3B, V3CD, V4, V4t, V7, V8, VIP, 7PC, and 7PL.” These ROIs involve the parietal and occipital lobe, corresponding to spatial information processing and visual processing in the human brain, respectively.</p>
</div>
<figure id="S7.F11" class="ltx_figure"><img src="/html/2409.11315/assets/x11.png" id="S7.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>
<span id="S7.F11.2.1" class="ltx_text ltx_font_bold">Visualization of Weights for Object Angles.</span> The absolute values of the linear classifier weights are shown for voxels across the whole brain to assess the importance of each ROI in distinguishing objects at different angles. Red regions indicate voxels with higher coefficients.

</figcaption>
</figure>
<figure id="S7.F12" class="ltx_figure"><img src="/html/2409.11315/assets/x12.png" id="S7.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>
<span id="S7.F12.2.1" class="ltx_text ltx_font_bold">Visualization of Weights for Object Types.</span> The absolute values of the linear classifier weights are shown for voxels across the brain to explore the importance of each ROI in classifying different object types (cars and rifles). Red regions indicate voxels with higher coefficients.

</figcaption>
</figure>
<figure id="S7.F13" class="ltx_figure"><img src="/html/2409.11315/assets/x13.png" id="S7.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>
<span id="S7.F13.2.1" class="ltx_text ltx_font_bold">Differentiation between objects within the same category.</span> We show the differentiation between two cars and two planes to illustrate how the brain distinguishes objects within the same category. Deep blue indicates voxels with higher values.

</figcaption>
</figure>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span><span id="S7.SS2.1.1" class="ltx_text ltx_font_italic">Analysis of Different Objects</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">In this section, we explore the brain activity patterns associated with viewing different objects. The analysis is divided into two parts:</p>
</div>
<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Between Different Types of Objects</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.1" class="ltx_p">Building on the previous angle-based experiment, we perform logistic regression on the whole-brain fMRI signals of subject 1 to differentiate between cars and rifles. The linear classifier achieved an accuracy of 79.54%. We also visualize the absolute values of the classifier weights in Fig. <a href="#S7.F12" title="Figure 12 ‣ 7.1 Voxel Importance Analysis for Object Angle Variations ‣ 7 Analysis ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, where red regions indicate voxels with higher coefficients and other colors represent lower coefficients. The regions of interest (ROIs) include “25, 47s, A5, AIP, PreS, STGa, STSda, TGd, V1, V2, V3, V3A, V4, V4t, V6, V7, and V8,” primarily located in the temporal and occipital lobes, corresponding to high-level visual functions and visual processing, respectively.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Between Objects Within the Same Category</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.1" class="ltx_p">In this case, classification experiments are more challenging due to the lack of specific labels for supervision. However, we visualize the absolute difference between two objects (a car and a plane) in Fig. <a href="#S7.F13" title="Figure 13 ‣ 7.1 Voxel Importance Analysis for Object Angle Variations ‣ 7 Analysis ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> for analysis. In the figure, deep blue indicates voxels with higher values, while lighter colors represent lower values. Red rectangles highlight regions with higher values. Notably, these regions are predominantly located in the parietal and occipital lobes, which are known to play key roles in object differentiation.</p>
</div>
</section>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span><span id="S7.SS3.1.1" class="ltx_text ltx_font_italic">Explore how our brain understand semantic information</span>
</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Our proposed MinD-3D successfully reconstructs 3D objects both semantically and structurally, so we aim to explore how the brain processes semantic information of different objects. Our model mainly focuses on the visual regions and does not include a specific branch for semantic information. To achieve this, we use Class Activation Mapping (CAM) methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> to analyze the importance of each part of the input fMRI frame. We
We present CAMs for three objects in Fig. <a href="#S7.F14" title="Figure 14 ‣ 7.3 Explore how our brain understand semantic information ‣ 7 Analysis ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. From this, we identify the following ROIs that may be involved in processing visual semantic information: FFC, FST, IP0, MT, MST, PHA1, PHA2, PHA3, PH, PIT, V1, V2, V3, V3A, V4, V4t, V6A, V7, V8, VMV1, VMV2, VMV3, and VVC.</p>
</div>
<figure id="S7.F14" class="ltx_figure"><img src="/html/2409.11315/assets/x14.png" id="S7.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>
<span id="S7.F14.2.1" class="ltx_text ltx_font_bold">CAM for Different Objects.</span> We use CAM to visualize the importance of each ROI in the visual regions for different objects.

</figcaption>
</figure>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span><span id="S7.SS4.1.1" class="ltx_text ltx_font_italic">Features Analysis</span>
</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.9" class="ltx_p">To understand the biological relevance of our feature extractor, in line with previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, we employ a linear encoding model to project the extracted fMRI features <math id="S7.SS4.p1.1.m1.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.1.m1.1a"><msub id="S7.SS4.p1.1.m1.1.1" xref="S7.SS4.p1.1.m1.1.1.cmml"><mi id="S7.SS4.p1.1.m1.1.1.2" xref="S7.SS4.p1.1.m1.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.1.m1.1.1.3" xref="S7.SS4.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.1.m1.1b"><apply id="S7.SS4.p1.1.m1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.1.m1.1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S7.SS4.p1.1.m1.1.1.2.cmml" xref="S7.SS4.p1.1.m1.1.1.2">𝑐</ci><ci id="S7.SS4.p1.1.m1.1.1.3.cmml" xref="S7.SS4.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.1.m1.1c">c_{f}</annotation></semantics></math> onto brain activity patterns, using subject 1 as a representative case for analysis. Specifically, we utilize ridge regression to establish the mapping between <math id="S7.SS4.p1.2.m2.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.2.m2.1a"><msub id="S7.SS4.p1.2.m2.1.1" xref="S7.SS4.p1.2.m2.1.1.cmml"><mi id="S7.SS4.p1.2.m2.1.1.2" xref="S7.SS4.p1.2.m2.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.2.m2.1.1.3" xref="S7.SS4.p1.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.2.m2.1b"><apply id="S7.SS4.p1.2.m2.1.1.cmml" xref="S7.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.2.m2.1.1.1.cmml" xref="S7.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S7.SS4.p1.2.m2.1.1.2.cmml" xref="S7.SS4.p1.2.m2.1.1.2">𝑐</ci><ci id="S7.SS4.p1.2.m2.1.1.3.cmml" xref="S7.SS4.p1.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.2.m2.1c">c_{f}</annotation></semantics></math> and the fMRI signals within the training dataset. We then project <math id="S7.SS4.p1.3.m3.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.3.m3.1a"><msub id="S7.SS4.p1.3.m3.1.1" xref="S7.SS4.p1.3.m3.1.1.cmml"><mi id="S7.SS4.p1.3.m3.1.1.2" xref="S7.SS4.p1.3.m3.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.3.m3.1.1.3" xref="S7.SS4.p1.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.3.m3.1b"><apply id="S7.SS4.p1.3.m3.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.3.m3.1.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S7.SS4.p1.3.m3.1.1.2.cmml" xref="S7.SS4.p1.3.m3.1.1.2">𝑐</ci><ci id="S7.SS4.p1.3.m3.1.1.3.cmml" xref="S7.SS4.p1.3.m3.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.3.m3.1c">c_{f}</annotation></semantics></math> from the test set into the fMRI space and compute the Pearson correlation coefficient to assess the relationship. As illustrated in Fig. <a href="#S7.F15" title="Figure 15 ‣ 7.4 Features Analysis ‣ 7 Analysis ‣ fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, areas positively correlated with visual Regions of Interest (ROIs) are marked in red, while other regions showing positive correlations are highlighted in blue. The results demonstrate a strong positive correlation between <math id="S7.SS4.p1.4.m4.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.4.m4.1a"><msub id="S7.SS4.p1.4.m4.1.1" xref="S7.SS4.p1.4.m4.1.1.cmml"><mi id="S7.SS4.p1.4.m4.1.1.2" xref="S7.SS4.p1.4.m4.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.4.m4.1.1.3" xref="S7.SS4.p1.4.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.4.m4.1b"><apply id="S7.SS4.p1.4.m4.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.4.m4.1.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S7.SS4.p1.4.m4.1.1.2.cmml" xref="S7.SS4.p1.4.m4.1.1.2">𝑐</ci><ci id="S7.SS4.p1.4.m4.1.1.3.cmml" xref="S7.SS4.p1.4.m4.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.4.m4.1c">c_{f}</annotation></semantics></math> and visual ROIs, confirming the effective extraction of relevant features. Additionally, we perform a similar correlation analysis with the visual latent feature <math id="S7.SS4.p1.5.m5.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S7.SS4.p1.5.m5.1a"><msub id="S7.SS4.p1.5.m5.1.1" xref="S7.SS4.p1.5.m5.1.1.cmml"><mi id="S7.SS4.p1.5.m5.1.1.2" xref="S7.SS4.p1.5.m5.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.5.m5.1.1.3" xref="S7.SS4.p1.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.5.m5.1b"><apply id="S7.SS4.p1.5.m5.1.1.cmml" xref="S7.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.5.m5.1.1.1.cmml" xref="S7.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S7.SS4.p1.5.m5.1.1.2.cmml" xref="S7.SS4.p1.5.m5.1.1.2">𝑐</ci><ci id="S7.SS4.p1.5.m5.1.1.3.cmml" xref="S7.SS4.p1.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.5.m5.1c">c_{v}</annotation></semantics></math>, which also shows positive correlations. Notably, the correlation strength of <math id="S7.SS4.p1.6.m6.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.6.m6.1a"><msub id="S7.SS4.p1.6.m6.1.1" xref="S7.SS4.p1.6.m6.1.1.cmml"><mi id="S7.SS4.p1.6.m6.1.1.2" xref="S7.SS4.p1.6.m6.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.6.m6.1.1.3" xref="S7.SS4.p1.6.m6.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.6.m6.1b"><apply id="S7.SS4.p1.6.m6.1.1.cmml" xref="S7.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.6.m6.1.1.1.cmml" xref="S7.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S7.SS4.p1.6.m6.1.1.2.cmml" xref="S7.SS4.p1.6.m6.1.1.2">𝑐</ci><ci id="S7.SS4.p1.6.m6.1.1.3.cmml" xref="S7.SS4.p1.6.m6.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.6.m6.1c">c_{f}</annotation></semantics></math> surpasses that of <math id="S7.SS4.p1.7.m7.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S7.SS4.p1.7.m7.1a"><msub id="S7.SS4.p1.7.m7.1.1" xref="S7.SS4.p1.7.m7.1.1.cmml"><mi id="S7.SS4.p1.7.m7.1.1.2" xref="S7.SS4.p1.7.m7.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.7.m7.1.1.3" xref="S7.SS4.p1.7.m7.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.7.m7.1b"><apply id="S7.SS4.p1.7.m7.1.1.cmml" xref="S7.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.7.m7.1.1.1.cmml" xref="S7.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S7.SS4.p1.7.m7.1.1.2.cmml" xref="S7.SS4.p1.7.m7.1.1.2">𝑐</ci><ci id="S7.SS4.p1.7.m7.1.1.3.cmml" xref="S7.SS4.p1.7.m7.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.7.m7.1c">c_{v}</annotation></semantics></math>, indicating its superior relevance to brain activity. However, it is important to note that in some measures, the correlation coefficient of <math id="S7.SS4.p1.8.m8.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S7.SS4.p1.8.m8.1a"><msub id="S7.SS4.p1.8.m8.1.1" xref="S7.SS4.p1.8.m8.1.1.cmml"><mi id="S7.SS4.p1.8.m8.1.1.2" xref="S7.SS4.p1.8.m8.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.8.m8.1.1.3" xref="S7.SS4.p1.8.m8.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.8.m8.1b"><apply id="S7.SS4.p1.8.m8.1.1.cmml" xref="S7.SS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.8.m8.1.1.1.cmml" xref="S7.SS4.p1.8.m8.1.1">subscript</csymbol><ci id="S7.SS4.p1.8.m8.1.1.2.cmml" xref="S7.SS4.p1.8.m8.1.1.2">𝑐</ci><ci id="S7.SS4.p1.8.m8.1.1.3.cmml" xref="S7.SS4.p1.8.m8.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.8.m8.1c">c_{v}</annotation></semantics></math> is higher than that of <math id="S7.SS4.p1.9.m9.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.SS4.p1.9.m9.1a"><msub id="S7.SS4.p1.9.m9.1.1" xref="S7.SS4.p1.9.m9.1.1.cmml"><mi id="S7.SS4.p1.9.m9.1.1.2" xref="S7.SS4.p1.9.m9.1.1.2.cmml">c</mi><mi id="S7.SS4.p1.9.m9.1.1.3" xref="S7.SS4.p1.9.m9.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.9.m9.1b"><apply id="S7.SS4.p1.9.m9.1.1.cmml" xref="S7.SS4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.9.m9.1.1.1.cmml" xref="S7.SS4.p1.9.m9.1.1">subscript</csymbol><ci id="S7.SS4.p1.9.m9.1.1.2.cmml" xref="S7.SS4.p1.9.m9.1.1.2">𝑐</ci><ci id="S7.SS4.p1.9.m9.1.1.3.cmml" xref="S7.SS4.p1.9.m9.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.9.m9.1c">c_{f}</annotation></semantics></math>, which highlights the complexity of the task and underscores the critical role of bridging features. Furthermore, our results indicate that our generated visual latent features achieve activation levels comparable to the ground truth (GT), emphasizing the effectiveness of our comparative learning approach and demonstrating well-aligned fMRI feature and image spaces.</p>
</div>
<figure id="S7.F15" class="ltx_figure"><img src="/html/2409.11315/assets/x15.png" id="S7.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>
<span id="S7.F15.8.1" class="ltx_text ltx_font_bold">Quality of the extracted features.</span>
We map the features to brain regions via ridge regression modeling using <math id="S7.F15.4.m1.1" class="ltx_Math" alttext="c_{f}" display="inline"><semantics id="S7.F15.4.m1.1b"><msub id="S7.F15.4.m1.1.1" xref="S7.F15.4.m1.1.1.cmml"><mi id="S7.F15.4.m1.1.1.2" xref="S7.F15.4.m1.1.1.2.cmml">c</mi><mi id="S7.F15.4.m1.1.1.3" xref="S7.F15.4.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F15.4.m1.1c"><apply id="S7.F15.4.m1.1.1.cmml" xref="S7.F15.4.m1.1.1"><csymbol cd="ambiguous" id="S7.F15.4.m1.1.1.1.cmml" xref="S7.F15.4.m1.1.1">subscript</csymbol><ci id="S7.F15.4.m1.1.1.2.cmml" xref="S7.F15.4.m1.1.1.2">𝑐</ci><ci id="S7.F15.4.m1.1.1.3.cmml" xref="S7.F15.4.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F15.4.m1.1d">c_{f}</annotation></semantics></math>, <math id="S7.F15.5.m2.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S7.F15.5.m2.1b"><msub id="S7.F15.5.m2.1.1" xref="S7.F15.5.m2.1.1.cmml"><mi id="S7.F15.5.m2.1.1.2" xref="S7.F15.5.m2.1.1.2.cmml">c</mi><mi id="S7.F15.5.m2.1.1.3" xref="S7.F15.5.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S7.F15.5.m2.1c"><apply id="S7.F15.5.m2.1.1.cmml" xref="S7.F15.5.m2.1.1"><csymbol cd="ambiguous" id="S7.F15.5.m2.1.1.1.cmml" xref="S7.F15.5.m2.1.1">subscript</csymbol><ci id="S7.F15.5.m2.1.1.2.cmml" xref="S7.F15.5.m2.1.1.2">𝑐</ci><ci id="S7.F15.5.m2.1.1.3.cmml" xref="S7.F15.5.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F15.5.m2.1d">c_{v}</annotation></semantics></math> and <math id="S7.F15.6.m3.1" class="ltx_Math" alttext="\hat{c_{v}}" display="inline"><semantics id="S7.F15.6.m3.1b"><mover accent="true" id="S7.F15.6.m3.1.1" xref="S7.F15.6.m3.1.1.cmml"><msub id="S7.F15.6.m3.1.1.2" xref="S7.F15.6.m3.1.1.2.cmml"><mi id="S7.F15.6.m3.1.1.2.2" xref="S7.F15.6.m3.1.1.2.2.cmml">c</mi><mi id="S7.F15.6.m3.1.1.2.3" xref="S7.F15.6.m3.1.1.2.3.cmml">v</mi></msub><mo id="S7.F15.6.m3.1.1.1" xref="S7.F15.6.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S7.F15.6.m3.1c"><apply id="S7.F15.6.m3.1.1.cmml" xref="S7.F15.6.m3.1.1"><ci id="S7.F15.6.m3.1.1.1.cmml" xref="S7.F15.6.m3.1.1.1">^</ci><apply id="S7.F15.6.m3.1.1.2.cmml" xref="S7.F15.6.m3.1.1.2"><csymbol cd="ambiguous" id="S7.F15.6.m3.1.1.2.1.cmml" xref="S7.F15.6.m3.1.1.2">subscript</csymbol><ci id="S7.F15.6.m3.1.1.2.2.cmml" xref="S7.F15.6.m3.1.1.2.2">𝑐</ci><ci id="S7.F15.6.m3.1.1.2.3.cmml" xref="S7.F15.6.m3.1.1.2.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F15.6.m3.1d">\hat{c_{v}}</annotation></semantics></math>, with voxel-wise Pearson’s correlation coefficients computed between them. The visual function ROI is highlighted in red. In contrast, other regions are depicted in blue.

</figcaption>
</figure>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we introduce the innovative task of Recon3DMind and its accompanying large-scale dataset, fMRI-3D, across various settings for the first time. Technologically, we develop a novel three-stage framework that involves multiple brain regions, including those associated with human 3D vision, specifically designed for this task. This approach establishes new benchmarks in the field and demonstrates the feasibility of the task. Initially, our model proficiently extracts features from fMRI frames. In the second stage, it employs a diffusion module to transition these fMRI features into the vision domain. Finally, in the third stage, the vision features are transformed into 3D models using an advanced 3D generation model. Our comprehensive experimental results and analyses affirm the model’s effectiveness in accurately extracting fMRI features and converting them into their corresponding 3D objects. We also conduct an in-depth analysis of our proposed fMRI-3D dataset and the features extracted by MinD-3D, further validating both the quality of fMRI-3D and the effectiveness of MinD-3D. This pioneering work not only opens up a new avenue in neuroimaging and 3D reconstruction but also paves the way for future research aimed at a deeper understanding and visualization of neural representations in 3D vision.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Gao, Y. Fu, Y. Wang, X. Qian, J. Feng, and Y. Fu, “Mind-3d: Reconstruct high-quality 3d objects in human brain,” 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Z. Chen, J. Qing, T. Xiang, W. L. Yue, and J. H. Zhou, “Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 22 710–22 720.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Chen, J. Qing, and J. H. Zhou, “Cinematic mindscapes: High-quality video reconstruction from brain activity,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11675</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. S. Scotti, A. Banerjee, J. Goode, S. Shabalin, A. Nguyen, E. Cohen, A. J. Dempster, N. Verlinde, E. Yundler, D. Weisberg <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Reconstructing the mind’s eye: fmri-to-image with contrastive learning and diffusion priors,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18274</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
X. Qian, Y. Wang, J. Huo, J. Feng, and Y. Fu, “fmri-pte: A large-scale fmri pretrained transformer encoder for multi-subject brain activity decoding,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.00342</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Qian, Y. Wang, Y. Fu, X. Xue, and J. Feng, “Semantic neural decoding via cross-modal generation,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14730</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, “Maskgit: Masked generative image transformer,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 11 315–11 325.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
I. I. Groen and C. I. Baker, “Scenes in the human brain: Comparing 2d versus 3d representations,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Neuron</em>, vol. 101, no. 1, pp. 8–10, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Grossberg, “370How We See the World in Depth: From 3D vision to how 2D pictures induce 3D percepts,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Conscious Mind, Resonant Brain: How Each Brain Makes a Mind</em>.   Oxford University Press, 06 2021. [Online]. Available: <a target="_blank" href="https://doi.org/10.1093/oso/9780190070557.003.0011" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1093/oso/9780190070557.003.0011</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. Linton, “Minimal theory of 3d vision: new approach to visual scale and visual shape,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Philosophical Transactions of the Royal Society B</em>, vol. 378, no. 1869, p. 20210455, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Georgieva, R. Peeters, H. Kolster, J. T. Todd, and G. A. Orban, “The processing of three-dimensional shape from disparity in the human brain,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of Neuroscience</em>, vol. 29, no. 3, pp. 727–742, 2009.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Jerath, M. W. Crawford, and V. A. Barnes, “Functional representation of vision within the mind: A visual consciousness model based in 3d default space,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of Medical Hypotheses and Ideas</em>, vol. 9, no. 1, pp. 45–56, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Shapenet: An information-rich 3d model repository,” <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.03012</em>, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, “Objaverse: A universe of annotated 3d objects,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08051</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. J. Allen, G. St-Yves, Y. Wu, J. L. Breedlove, J. S. Prince, L. T. Dowdle, M. Nau, B. Caron, F. Pestilli, I. Charest <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence,” <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">Nature neuroscience</em>, vol. 25, no. 1, pp. 116–126, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Qian, Y. Wang, S. Luo, Y. Zhang, Y. Tai, Z. Zhang, C. Wang, X. Xue, B. Zhao, T. Huang <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Pushing auto-regressive models for 3d shape generation at capacity and scalability,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.12225</em>, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. Horikawa and Y. Kamitani, “Generic decoding of seen and imagined objects using hierarchical visual features,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Nature communications</em>, vol. 8, no. 1, p. 15037, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Wen, J. Shi, Y. Zhang, K.-H. Lu, J. Cao, and Z. Liu, “Neural encoding and decoding with deep learning for dynamic natural vision,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Cerebral cortex</em>, vol. 28, no. 12, pp. 4136–4160, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
G. Shen, T. Horikawa, K. Majima, and Y. Kamitani, “Deep image reconstruction from human brain activity,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">PLoS computational biology</em>, vol. 15, no. 1, p. e1006633, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G. Shen, K. Dwivedi, K. Majima, T. Horikawa, and Y. Kamitani, “End-to-end deep image reconstruction from human brain activity,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Frontiers in computational neuroscience</em>, vol. 13, p. 21, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Du, J. Li, L. Huang, and H. He, “Brain encoding and decoding in fmri with bidirectional deep generative models,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Engineering</em>, vol. 5, no. 5, pp. 948–953, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Sun, M. Li, Z. Chen, Y. Zhang, S. Wang, and M.-F. Moens, “Contrast, attend and diffuse to decode high-resolution images from brain activities,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 6840–6851, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.02502</em>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
W. Peebles and S. Xie, “Scalable diffusion models with transformers,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 4195–4205.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm: Large multi-view gaussian model for high-resolution 3d content creation,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05054</em>, 2024.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Liu, X. Tang, F. Cheng, R. Yang, Z. Li, J. Liu, Y. Huang, J. Lin, S. Liu, X. Wu <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Mirrorgaussian: Reflecting 3d gaussians for reconstructing mirror reflections,” <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.11921</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian: Generative gaussian splatting for efficient 3d content creation,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16653</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis, “3d gaussian splatting for real-time radiance field rendering,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics</em>, vol. 42, no. 4, July 2023. [Online]. Available: <a target="_blank" href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
P. Wang and Y. Shi, “Imagedream: Image-prompt multi-view diffusion for 3d generation,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.02201</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Ye, F. Liu, Q. Li, Z. Wang, Y. Wang, X. Wang, Y. Duan, and J. Zhu, “Dreamreward: Text-to-3d generation with human preference,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.14613</em>, 2024.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A.-C. Cheng, X. Li, S. Liu, M. Sun, and M.-H. Yang, “Autoregressive 3d shape generation via canonical mapping,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2022, pp. 89–104.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Ibing, G. Kobsik, and L. Kobbelt, “Octree transformer: Autoregressive 3d shape generation on hierarchically structured sequences,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 2698–2707.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
N. Chang, J. A. Pyles, A. Marcus, A. Gupta, M. J. Tarr, and E. M. Aminoff, “Bold5000, a public fmri dataset while viewing 5000 visual images,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Scientific data</em>, vol. 6, no. 1, p. 49, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick, “Zero-1-to-3: Zero-shot one image to 3d object,” 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Chen and H. Zhang, “Learning implicit fields for generative shape modeling,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 5939–5948.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, “Pointgrow: Autoregressively learned point cloud generation with self-attention,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2020, pp. 61–70.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. Ibing, I. Lim, and L. Kobbelt, “3d shape generation with grid-based implicit functions,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 13 559–13 568.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T. Luo, C. Rockwell, H. Lee, and J. Johnson, “Scalable 3d captioning with pretrained models,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.07279</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
O. Esteban, C. Markiewicz, R. W. Blair, C. Moodie, A. I. Isik, A. Erramuzpe Aliaga, J. Kent, M. Goncalves, E. DuPre, M. Snyder, H. Oya, S. Ghosh, J. Wright, J. Durnez, R. Poldrack, and K. J. Gorgolewski, “fMRIPrep: a robust preprocessing pipeline for functional MRI,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Nature Methods</em>, vol. 16, pp. 111–116, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
O. Esteban, R. Blair, C. J. Markiewicz, S. L. Berleant, C. Moodie, F. Ma, A. I. Isik, A. Erramuzpe, M. Kent, James D. andGoncalves, E. DuPre, K. R. Sitek, D. E. P. Gomez, D. J. Lurie, Z. Ye, R. A. Poldrack, and K. J. Gorgolewski, “fmriprep,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Software</em>, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 16 000–16 009.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Gao, X. Qian, Y. Wang, T. Xiao, T. He, Z. Zhang, and Y. Fu, “Coarse-to-fine amodal segmentation with shape prior,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 1262–1271.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
K. L. Miller, F. Alfaro-Almagro, N. K. Bangerter, D. L. Thomas, E. Yacoub, J. Xu, A. J. Bartsch, S. Jbabdi, S. N. Sotiropoulos, J. L. Andersson <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Multimodal population brain imaging in the uk biobank prospective epidemiological study,” <em id="bib.bib45.2.2" class="ltx_emph ltx_font_italic">Nature neuroscience</em>, vol. 19, no. 11, pp. 1523–1536, 2016.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning transferable visual models from natural language supervision,” in <em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
M. Cherti, R. Beaumont, R. Wightman, M. Wortsman, G. Ilharco, C. Gordon, C. Schuhmann, L. Schmidt, and J. Jitsev, “Reproducible scaling laws for contrastive language-image learning,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 2818–2829.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-conditional image generation with clip latents,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06125</em>, vol. 1, no. 2, p. 3, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer learning for nlp,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2019, pp. 2790–2799.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
T. Yang, Y. Zhu, Y. Xie, A. Zhang, C. Chen, and M. Li, “Aim: Adapting image models for efficient video action recognition,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.03024</em>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
F. Ozcelik, B. Choksi, M. Mozafari, L. Reddy, and R. VanRullen, “Reconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2022 International Joint Conference on Neural Networks (IJCNN)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
W. Mai and Z. Zhang, “Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.07428</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable effectiveness of deep features as a perceptual metric,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Wang, X. Qi, and C.-W. Fu, “Towards implicit text-guided 3d shape generation,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 17 896–17 906.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann, “Disn: Deep implicit surface network for high-quality single-view 3d reconstruction,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning deep features for discriminative localization,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition</em>, 2016.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Y. Takagi and S. Nishimoto, “High-resolution image reconstruction with latent diffusion models from human brain activity,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 14 453–14 463.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
F. Ozcelik and R. VanRullen, “Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.05334</em>, 2023.

</span>
</li>
</ul>
</section>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/bio/gjx.jpg" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jianxiong Gao</span>  received the B.S. degree in Statistics from Shandong University in 2022.
He is currently pursuing a Ph.D. degree in Biomedical Engineering at the Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, under the supervision of Dr. Yanwei Fu and Dr. Jianfeng Feng. His research interests include amodal segmentation and neural decoding.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/bio/fyq.jpg" id="id4.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id4.1.1.2" class="ltx_td">
<span id="id4.1.1.2.1" class="ltx_inline-block">
<span id="id4.1.1.2.1.1" class="ltx_p"><span id="id4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Yuqian Fu</span>  is currently a postdoc researcher at INSAIT, Bulgaria. Previously, she worked as a postdoc researcher at Computer Vision Lab (CVL), ETH Zürich, Switzerland. She received her Ph.D. degree from the School of Computer Science, Fudan University, China, in June 2023. Her research topics are vision and deep learning, especially transfer learning, domain adaptation, and multimodal learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id5" class="ltx_float biography">
<table id="id5.1" class="ltx_tabular">
<tr id="id5.1.1" class="ltx_tr">
<td id="id5.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/bio/wy.jpg" id="id5.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="id5.1.1.2" class="ltx_td">
<span id="id5.1.1.2.1" class="ltx_inline-block">
<span id="id5.1.1.2.1.1" class="ltx_p"><span id="id5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Yun Wang</span>  received the B.S. and M.S. degrees in electrical engineering from Wuhan University, Hubei Province, China, in 2011 and is currently pursuing a Ph.D. degree in the Institute of Science and Technology for Brain-Inspired Intelligence at Fudan University. From 2011 to 2017, he was a Research Engineer in State Grid Electric Power Research Institute. His current research interests include computational neuroscience and brain-inspired intelligence.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id6" class="ltx_float biography">
<table id="id6.1" class="ltx_tabular">
<tr id="id6.1.1" class="ltx_tr">
<td id="id6.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/bio/qxl.jpg" id="id6.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="97" height="125" alt="[Uncaptioned image]"></td>
<td id="id6.1.1.2" class="ltx_td">
<span id="id6.1.1.2.1" class="ltx_inline-block">
<span id="id6.1.1.2.1.1" class="ltx_p"><span id="id6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Xuelin Qian</span>  (Member, IEEE) is an Associate Professor in the School of Automation, Northwestern Polytechnical University (NWPU). Before that, he held a post-doctoral position with Fudan University from 2022 to 2024. He received the Ph.D. degree from Fudan University in 2021, and the B.S. degree from Xidian University in 2015. He has published over 15 papers in top-tier conferences and journals, and served as a reviewer for CVPR, ICCV, TPAMI, IJCV <span id="id6.1.1.2.1.1.2" class="ltx_text ltx_font_italic">etc</span>. His research interests are image retrieval, multi-modal generation, and medical image analysis.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id7" class="ltx_float biography">
<table id="id7.1" class="ltx_tabular">
<tr id="id7.1.1" class="ltx_tr">
<td id="id7.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/bio/fjf.jpg" id="id7.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="100" height="125" alt="[Uncaptioned image]"></td>
<td id="id7.1.1.2" class="ltx_td">
<span id="id7.1.1.2.1" class="ltx_inline-block">
<span id="id7.1.1.2.1.1" class="ltx_p"><span id="id7.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jianfeng Feng</span>  (Senior Member, IEEE) received the
BS, MS, and PhD degrees from the Department of Probability and Statistics, Peking University, China. He is the chair professor with the Shanghai National Centre for Mathematic Sciences and the dean with the Brain-Inspired AI Institute, Fudan University. He leads the DTB project. He has been developing new mathematical, statistical, and computational theories and methods to meet the challenges raised in neuroscience and mental health research.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id8" class="ltx_float biography">
<table id="id8.1" class="ltx_tabular">
<tr id="id8.1.1" class="ltx_tr">
<td id="id8.1.1.1" class="ltx_td"><img src="/html/2409.11315/assets/x16.png" id="id8.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="77" height="96" alt="[Uncaptioned image]"></td>
<td id="id8.1.1.2" class="ltx_td">
<span id="id8.1.1.2.1" class="ltx_inline-block">
<span id="id8.1.1.2.1.1" class="ltx_p"><span id="id8.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Yanwei Fu</span>  received the MEng degree from the Department of Computer Science and Technology, Nanjing University, China, in 2011, and the PhD degree from the Queen Mary University of London, in 2014. He held a post-doctoral position at Disney Research, Pittsburgh, PA, from 2015 to 2016. He is currently a tenure-track professor at Fudan University.
He was appointed as the Professor of Special Appointment
(Eastern Scholar) at Shanghai Institutions
of Higher Learning in 2017, and awarded
the 1000 Young talent scholar in 2018.
His work has led to many awards, including the IEEE ICME 2019 best paper.
He published more than 110 journal/conference papers including IEEE TPAMI, TMM, ECCV, and CVPR. His research interests are one-shot learning, learning-based 3D reconstruction, and learning-based robotic grasping.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.11314" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.11315" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.11315">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.11315" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.11316" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 20:33:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
