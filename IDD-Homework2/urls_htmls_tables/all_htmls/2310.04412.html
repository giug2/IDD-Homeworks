<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.04412] FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning</title><meta property="og:description" content="Federated learning (FL) is an emerging paradigm in machine learning, where a shared model is collaboratively learned using data from multiple devices to mitigate the risk of data leakage. While recent studies posit tha…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.04412">

<!--Generated on Wed Feb 28 02:26:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Peiran Xu<sup id="id2.2.id1" class="ltx_sup">*1</sup>  
Zeyu Wang<sup id="id3.3.id2" class="ltx_sup">*2</sup>  
Jieru Mei<sup id="id4.4.id3" class="ltx_sup">3</sup>  
Liangqiong Qu<sup id="id5.5.id4" class="ltx_sup">4</sup>  
Alan Yuille<sup id="id6.6.id5" class="ltx_sup">3</sup>  
Cihang Xie<sup id="id7.7.id6" class="ltx_sup">2</sup>  
Yuyin Zhou<sup id="id8.8.id7" class="ltx_sup">2</sup>
<br class="ltx_break"><sup id="id9.9.id8" class="ltx_sup"><span id="id9.9.id8.1" class="ltx_text ltx_font_italic">∗</span></sup>equal technical contribution  
<br class="ltx_break"><sup id="id10.10.id9" class="ltx_sup">1</sup>UCLA  <sup id="id11.11.id10" class="ltx_sup">2</sup>UC Santa Cruz  <sup id="id12.12.id11" class="ltx_sup">3</sup>Johns Hopkins University
 <sup id="id13.13.id12" class="ltx_sup">4</sup>The University of Hong Kong
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">Federated learning (FL) is an emerging paradigm in machine learning, where a shared model is collaboratively learned using data from multiple devices to mitigate the risk of data leakage. While recent studies posit that Vision Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in addressing data heterogeneity in FL, the specific architectural components that underpin this advantage have yet to be elucidated. In this paper, we systematically investigate the impact of different architectural elements, such as activation functions and normalization layers, on the performance within heterogeneous FL.
Through rigorous empirical analyses, we are able to offer the first-of-its-kind general guidance on micro-architecture design principles for heterogeneous FL.</p>
<p id="id15.id2" class="ltx_p">Intriguingly, our findings indicate that with strategic architectural modifications, pure CNNs can achieve a level of robustness that either matches or even exceeds that of ViTs when handling heterogeneous data clients in FL.
Additionally, our approach is compatible with existing FL techniques and delivers state-of-the-art solutions across a broad spectrum of FL benchmarks. The code is publicly available at <a target="_blank" href="https://github.com/UCSC-VLAA/FedConv" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/UCSC-VLAA/FedConv</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) is an emerging paradigm that holds significant potential in safeguarding user data privacy in a variety of real-world applications, such as mobile edge computing <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>. Yet, one of the biggest challenges in FL is data heterogeneity, making it difficult to develop a single shared model that can generalize well across all local devices.
While numerous solutions have been proposed to enhance heterogeneous FL from an optimization standpoint <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Hsu et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, the recent work by <cite class="ltx_cite ltx_citemacro_citet">Qu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite> highlights that the selection of neural architectures also plays a crucial role in addressing this challenge. This study delves into the comparative strengths of Vision Transformers (ViTs) vis-à-vis Convolutional Neural Networks (CNNs) within the FL context and posits that the performance disparity between ViTs and CNNs amplifies with increasing data heterogeneity.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">The analysis provided in <cite class="ltx_cite ltx_citemacro_citet">Qu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>, while insightful, primarily operates at a macro level, leaving certain nuances of neural architectures unexplored.
Specifically, the interplay between individual architectural elements in ViT and its robustness to heterogeneous data in FL remains unclear. While recent studies <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> suggest that self-attention-block is the main building block that contributes significantly to ViT’s robustness against out-of-distribution data, its applicability to other settings, particularly heterogeneous FL, is yet to be ascertained. This gap in understanding prompts us to consider the following questions: <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">which architectural elements in ViT underpin its superior performance in heterogeneous FL, and as a step further, can CNNs benefit from incorporating these architectural elements to improve their performance in this scenario?</em></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.04412/assets/imgs/all11.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Performance comparison on three heterogeneous FL datasets. While a vanilla ResNet significantly underperforms Transformers and ConvNeXt when facing data heterogeneity, our enhanced CNN model, named FedConv, consistently achieves superior performance.
</figcaption>
</figure>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To this end, we take a closer analysis of the architectural elements of ViTs, and empirically uncover several key designs for improving model robustness against heterogeneous data in FL. First, we note the design of activation functions matters, concluding that smooth and near-zero-centered activation functions consistently yield substantial improvements. Second, simplifying the architecture by completely removing all normalization layers and retraining a single activation function in each block emerges as a beneficial design. Third, we reveal two key properties — feature extraction from overlapping patches and convolutions only for downsampling — that define a robust stem layer for handling diverse input data.
Lastly, we critically observe that a large enough kernel size (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, 9) is essential in securing model robustness against heterogeneous distributions.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Our experiments extensively verify the consistent improvements brought by these architectural designs in the context of heterogeneous FL. Specifically, by integrating these designs, we are able to build a self-attention-free CNN architecture, dubbed FedConv, that outperforms established models, including ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, Swin-Transformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, and ConvNeXt<cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>. Notably, our FedConv achieves 92.21% accuracy on COVID-FL and 54.19% on iNaturalist, outperforming the next-best solutions by 2.57% and 13.89%, respectively. Moreover, our FedConv can effectively generalize to other datasets with varying numbers of data clients, from as few as 5 to more than 2,000; when combined with existing FL methods, our FedConv consistently registers new performance records across a range of FL benchmarks.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In conclusion, our findings shed light on the pivotal role of architecture configuration in robustifying CNNs for heterogeneous FL. We hope that our insights will inspire the community to further probe the significance of architectural nuances in different FL settings.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Federated Learning.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">FL is a decentralized approach that aims to learn a shared model by aggregating updates or parameters from locally distributed training data <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>. However, one of the key challenges in FL is the presence of data heterogeneity, or varying distribution of training data across clients, which has been shown to cause weight divergence <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib65" title="" class="ltx_ref">2018</a>)</cite> and other optimization issues <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> in FL.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">To address this challenge, FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite> adds a proximal term in the loss function to achieve more stable and accurate convergence; FedAVG-Share <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib65" title="" class="ltx_ref">2018</a>)</cite> keeps a small globally-shared subset amongst devices; SCAFFOLD <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> introduces a variable to both estimate and correct update direction of each client. Beyond these, several other techniques have been explored in heterogeneous FL, including reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib53" title="" class="ltx_ref">2020a</a>)</cite>, hierarchical clustering <cite class="ltx_cite ltx_citemacro_citep">(Briggs et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib67" title="" class="ltx_ref">2021</a>; Li &amp; Wang, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>; Qu et al., <a href="#bib.bib38" title="" class="ltx_ref">2022a</a>)</cite>, and self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib64" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">Recently, a new perspective for improving heterogeneous FL is by designing novel neural architectures. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> points out that updating only non-BatchNorm (BN) layers significantly enhances FedAVG performance, while <cite class="ltx_cite ltx_citemacro_citet">Du et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> suggests that simply replacing BN with LayerNorm (LN) can mitigate external covariate shifts and accelerate convergence. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020b</a>)</cite> shows neuron permutation matters when averaging model weights from different clients. Our research draws inspiration from <cite class="ltx_cite ltx_citemacro_citet">Qu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>, which shows that Transformers are inherently stronger than CNNs in handling data heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>. Yet, we come to a completely different conclusion: with the right architecture designs, CNNs can be comparable to, or even more robust than, Transformers in the context of heterogeneous FL.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Vision Transformer.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">CNNs have been the dominant architecture in visual recognition for nearly a decade due to their superior performance <cite class="ltx_cite ltx_citemacro_citep">(Simonyan &amp; Zisserman, <a href="#bib.bib42" title="" class="ltx_ref">2014</a>; He et al., <a href="#bib.bib15" title="" class="ltx_ref">2016</a>; Szegedy et al., <a href="#bib.bib44" title="" class="ltx_ref">2016a</a>; Howard et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Tan &amp; Le, <a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>. However, the recently emerged ViTs challenge the leading position of CNNs <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib47" title="" class="ltx_ref">2021a</a>; <a href="#bib.bib48" title="" class="ltx_ref">b</a>; Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> — by applying a stack of global self-attention blocks <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> on a sequence of image patches, Transformers can even show stronger performance than CNNs in a range of visual benchmarks, especially when a huge amount of
training data is available <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib47" title="" class="ltx_ref">2021a</a>; Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>; He et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>. Additionally, Transformers are shown to be inherently more robust than CNNs against occlusions, adversarial perturbation, and out-of-distribution corruptions <cite class="ltx_cite ltx_citemacro_citep">(Bhojanapalli et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>; Bai et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Paul &amp; Chen, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Modernized CNN.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Recent works also reignite the discussion on whether CNNs can still be the preferred architecture for visual recognition. <cite class="ltx_cite ltx_citemacro_citet">Wightman et al. (<a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite> find that by simply changing to an advanced training recipe, the classic ResNet-50 achieves a remarkable 4% improvement on ImageNet, a performance that is comparable to its DeiT counterpart <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib47" title="" class="ltx_ref">2021a</a>)</cite>. ConvMixer <cite class="ltx_cite ltx_citemacro_citep">(Trockman &amp; Kolter, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, on the other hand, integrates the patchify stem setup into CNNs, yielding competitive performance with Transformers. Furthermore, ConvNeXt <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> shows that, by aggressively incorporating every applicable architectural design from Transformer, even the pure CNN can attain excessively strong performance across a variety of visual tasks.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">Our work is closely related to ConvNeXt <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>. However, in contrast to their design philosophy which aims to build Transformer-like CNNs, our goal is to pinpoint a core set of architectural elements that can enhance CNNs, particularly in the context of heterogeneous FL.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FedConv</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we conduct a comprehensive analysis of several architectural elements in heterogeneous FL. Our findings suggest that pure CNNs can achieve comparable or even superior performance in heterogeneous FL when incorporating specific architectural designs. Key designs including switching to smooth and near-zero-centered activation functions in Section <a href="#S3.SS2" title="3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, reducing activation and normalization layers in Section <a href="#S3.SS3" title="3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, adopting the stem layer setup in Section <a href="#S3.SS4" title="3.4 Stem layer ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>, and enlarging the kernel size in Section <a href="#S3.SS5" title="3.5 Kernel Size ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>. By combining these designs, we develop a novel CNN architecture, <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">FedConv</span>. As demonstrated in Section <a href="#S3.SS6" title="3.6 Component combination ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>, this architecture emerges as a simple yet effective alternative to handle heterogeneous data in FL.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experiment Setup</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Datasets.</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Our main dataset is COVID-FL <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite>, a real-world medical FL dataset containing 20,055 medical images, sourced from 12 hospitals. Note that clients (<em id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, hospital) in this dataset are characterized by the absence of one or more classes. This absence induces pronounced data heterogeneity in FL, driven both by the limited overlap in client partitions and the imbalanced distribution of labels. To provide a comprehensive assessment of our approach, we report performance in both the centralized training setting and the distributed FL setting.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Federated learning methods.</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">We consider the classic Federated Averaging (FedAVG) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite> as the default FL method, unless specified otherwise. FedAVG operates as follows: 1) the global server model is initially sent to local clients; 2) these clients next engage in multiple rounds of local updates with their local data; 3) once updated, these models are sent back to the server, where they are averaged to create an updated global model.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training recipe.</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">All models are first pre-trained on ImageNet using the recipe provided in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>, and then get finetuned on COVID-FL using FedAVG. Specifically, in fine-tuning, we set the base learning rate to 1.75e-4 with a cosine learning rate scheduler, weight decay to 0.05, batch size to 64, and warmup epoch to 5. Following <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>; Yan et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite>, we apply AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> to each local client and maintain their momentum term locally. For FedAVG, we set the total communication round to 100, with a local training epoch of 1. Note that all 12 clients are included in every communication round.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Computational cost.</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">We hereby use FLOPs as the metric to measure the model scale. To ensure a fair comparison, all models considered in this study are intentionally calibrated to align with the FLOPs scale of ViT-Small <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, unless specified otherwise.</p>
</div>
<div id="S3.SS1.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px4.p2.1" class="ltx_p">To improve the performance-to-cost ratio of our models, we pivot from the conventional convolution operation in ResNet, opting instead for depth-wise convolution. This method, as corroborated by prior research <cite class="ltx_cite ltx_citemacro_citep">(Howard et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Sandler et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>; Howard et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Tan &amp; Le, <a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>, exhibits a better balance between performance and computational cost.
Additionally, following the design philosophy of ResNeXt <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite>, we adjust the base channel configuration across stages, transitioning from (64, 128, 256, 512) to (96, 192, 384, 768); we further calibrate the block number to keep the total FLOPs closely align with the ViT-Small scale.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">An improved ResNet baseline.</h5>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p">Building upon the adoption of depth-wise convolutions, we demonstrate that further incorporating two simple architectural elements from ViT enables us to build a much stronger ResNet baseline. Firstly, we replace BN with LN. This shift is motivated by prior studies which reveal the adverse effects of maintaining the EMA of batch-averaged feature statistics in heterogeneous FL <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Du et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>; instead, they advocate that batch-independent normalization techniques like LN can improve both the performance and convergence speed of the global model <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. Secondly, we replace the traditional ReLU activation function with GELU <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks &amp; Gimpel, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>. As discussed in Section <a href="#S3.SS2" title="3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, this change can substantially increase the classification accuracy in COVID-FL by 4.52%, from 72.92% to 77.44%.</p>
</div>
<div id="S3.SS1.SSS0.Px5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px5.p2.1" class="ltx_p">We refer to the model resulting from these changes as <span id="S3.SS1.SSS0.Px5.p2.1.1" class="ltx_text ltx_font_bold">ResNet-M</span>, which will serve as the default baseline for our subsequent experiments. However, a critical observation is that, despite its enhancements, the accuracy of ResNet-M still lags notably behind its Transformer counterpart in heterogeneous FL, which registers a much higher accuracy at 88.38%.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Activation function</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">We hereby investigate the impact of activation function selection on model performance in the context of heterogeneous FL. Our exploration begins with GELU, the default activation function in Transformers <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib47" title="" class="ltx_ref">2021a</a>; Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>. Previous studies show that GELU consistently outperforms ReLU in terms of both clean accuracy and adversarial robustness <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks &amp; Gimpel, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>; Elfwing et al., <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Clevert et al., <a href="#bib.bib7" title="" class="ltx_ref">2016</a>; Xie et al., <a href="#bib.bib57" title="" class="ltx_ref">2020</a>)</cite>. In our assessment of GELU’s efficacy for heterogeneous FL, we observe a similar conclusion: as shown in Table <a href="#S3.F2" title="Figure 2 ‣ 3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, replacing ReLU with GELU can lead to a significant accuracy improvement of 4.52% in COVID-FL, from 72.92% to 77.44%.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:164.8pt;"><img src="/html/2310.04412/assets/imgs/act8.jpg" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Plots of various activation function curves.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:195.1pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>A study on the effect of various activation functions.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S3.F2.fig1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F2.fig1.1.1.1" class="ltx_tr">
<th id="S3.F2.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Activation</th>
<td id="S3.F2.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Central</td>
<td id="S3.F2.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FL</td>
<td id="S3.F2.fig1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Mean Act</td>
</tr>
<tr id="S3.F2.fig1.1.2.2" class="ltx_tr">
<th id="S3.F2.fig1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ReLU</th>
<td id="S3.F2.fig1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">95.59</td>
<td id="S3.F2.fig1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.92</td>
<td id="S3.F2.fig1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">0.28</td>
</tr>
<tr id="S3.F2.fig1.1.3.3" class="ltx_tr">
<th id="S3.F2.fig1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LReLU</th>
<td id="S3.F2.fig1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">95.54</td>
<td id="S3.F2.fig1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.41</td>
<td id="S3.F2.fig1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.26</td>
</tr>
<tr id="S3.F2.fig1.1.4.4" class="ltx_tr">
<th id="S3.F2.fig1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PReLU</th>
<td id="S3.F2.fig1.1.4.4.2" class="ltx_td ltx_align_center">95.41</td>
<td id="S3.F2.fig1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">74.24</td>
<td id="S3.F2.fig1.1.4.4.4" class="ltx_td ltx_align_center">0.20</td>
</tr>
<tr id="S3.F2.fig1.1.5.5" class="ltx_tr">
<th id="S3.F2.fig1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SoftPlus</th>
<td id="S3.F2.fig1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">95.28</td>
<td id="S3.F2.fig1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.86</td>
<td id="S3.F2.fig1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.70</td>
</tr>
<tr id="S3.F2.fig1.1.6.6" class="ltx_tr">
<th id="S3.F2.fig1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.F2.fig1.1.6.6.1.1" class="ltx_ERROR undefined">\cdashline</span>1-4
GELU</th>
<td id="S3.F2.fig1.1.6.6.2" class="ltx_td ltx_align_center">95.82</td>
<td id="S3.F2.fig1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">77.44</td>
<td id="S3.F2.fig1.1.6.6.4" class="ltx_td ltx_align_center">0.07</td>
</tr>
<tr id="S3.F2.fig1.1.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S3.F2.fig1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.F2.fig1.1.7.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">SiLU</span></th>
<td id="S3.F2.fig1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S3.F2.fig1.1.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">95.81</span></td>
<td id="S3.F2.fig1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.F2.fig1.1.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">79.52</span></td>
<td id="S3.F2.fig1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S3.F2.fig1.1.7.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">0.04</span></td>
</tr>
<tr id="S3.F2.fig1.1.8.8" class="ltx_tr">
<th id="S3.F2.fig1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">ELU</th>
<td id="S3.F2.fig1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b">95.59</td>
<td id="S3.F2.fig1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">78.25</td>
<td id="S3.F2.fig1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b">-0.07</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Building upon the insights from <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a href="#bib.bib57" title="" class="ltx_ref">2020</a>)</cite>, we next explore the generalization of this improvement to smooth activation functions, which are defined as being <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{C}</annotation></semantics></math><sup id="S3.SS2.p2.1.1" class="ltx_sup">1</sup> smooth. Specifically, we assess five activation functions: two that are non-smooth (Parametric Rectified Linear Unit (PReLU) <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib14" title="" class="ltx_ref">2015</a>)</cite> and Leaky ReLU (LReLU)), and three that are smooth (SoftPlus <cite class="ltx_cite ltx_citemacro_citep">(Dugas et al., <a href="#bib.bib11" title="" class="ltx_ref">2000</a>)</cite>, Exponential Linear Unit (ELU) <cite class="ltx_cite ltx_citemacro_citep">(Clevert et al., <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>, and Sigmoid Linear Unit (SiLU) <cite class="ltx_cite ltx_citemacro_citep">(Elfwing et al., <a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>). The curves of these activation functions are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and their performance on COVID-FL is reported in Table <a href="#S3.F2" title="Figure 2 ‣ 3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our results show that smooth activation functions generally outperform their non-smooth counterparts in heterogeneous FL. Notably, both SiLU and ELU achieve an accuracy surpassing 78%, markedly superior to the accuracy of LReLU (73.41%) and PReLU (74.24%). Yet, there is an anomaly in our findings: SoftPlus, when replacing ReLU, fails to show a notable improvement. This suggests that <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">smoothness alone is not sufficient for achieving strong performance in heterogeneous FL</em>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">With a closer look at these smooth activation functions, we note a key difference between SoftPlus and the others: while SoftPlus consistently yields positive outputs, the other three (GELU, SiLU, and ELU) can produce negative values for certain input ranges, potentially facilitating outputs that are more centered around zero. To quantitatively characterize this difference, we calculate the mean activation values of ResNet-M across all layers when presented with COVID-FL data. As noted in the “Mean” column of Table <a href="#S3.F2" title="Figure 2 ‣ 3.2 Activation function ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, GELU, SiLU, and ELU consistently hold mean activation values close to zero, while other activation functions exhibit a much larger deviations from zero. These empirical results lead us to conclude that utilizing a <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">smooth and near-zero-centered</span> activation function is advantageous in heterogeneous FL.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Reducing activation and normalization layers</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Transformer blocks, in contrast to traditional CNN blocks, generally incorporate fewer activation and normalization layers <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib48" title="" class="ltx_ref">2021b</a>; Liu et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>.
Prior works show that CNNs can remain stable or even attain higher performance with fewer activation layers <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> or without normalization layers <cite class="ltx_cite ltx_citemacro_citep">(Brock et al., <a href="#bib.bib5" title="" class="ltx_ref">2021b</a>)</cite>.
In this section, we delve into this design choice in the context of heterogeneous FL.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">Drawing inspiration from ConvNeXt, we evaluate three different block instantiations: 1) <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">Normal Block</em>, which serves as the basic building block of our ResNet-M baseline; 2) <em id="S3.SS3.p2.1.2" class="ltx_emph ltx_font_italic">Invert Block</em>, originally proposed in <cite class="ltx_cite ltx_citemacro_citep">(Sandler et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>, which has a hidden dimension that is four times the size of the input dimension; and 3) <em id="S3.SS3.p2.1.3" class="ltx_emph ltx_font_italic">InvertUp Block</em>, which modifies Invert Block by repositioning the depth-wise convolution to the top of the block. The detailed block configurations is illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ Reducing activation layers. ‣ 3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Notably, regardless of the specific block configuration employed, we always ensure similar FLOPs across models by adjusting the number of blocks.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reducing activation layers.</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">We begin our experiments by aggressively reducing the number of activation layers. Specifically, we retain only one activation layer in each block to sustain non-linearity. As highlighted in Table <a href="#S3.T2.st1" title="In Table 2 ‣ Reducing normalization layers. ‣ 3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, all three block designs showcase at least one configuration that delivers substantial performance gains in heterogeneous FL. For example, the best configurations yield an improvement of 6.89% for Normal Block (from 77.44% to 84.33%), 3.77% for Invert Block (from 80.35% to 84.12%), and 1.48% for InvertUp Block (from 80.96% to 82.44%). More intriguingly, a simple rule-of-thumb design principle emerges for these best configurations: the activation function is most effective when placed subsequent to the channel-expanding convolution layer, whose output channel dimension is larger than its input dimension.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2310.04412/assets/imgs/block4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Block architecture details.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reducing normalization layers.</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">Building upon our experiments above with only one best-positioned activation layer, we further investigate the impact of aggressively removing normalization layers. Similarly, we hereby are interested in keeping only one normalization layer in each block. As presented in Table <a href="#S3.T2.st2" title="In Table 2 ‣ Reducing normalization layers. ‣ 3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, we observe that the effects of removing normalization layers are highly block-dependent in heterogeneous FL: it consistently hurts Normal Block, leads to improvements for the Invert Block (up to +2.07%), and consistently enhances InvertUP block (up to +3.26%).</p>
</div>
<figure id="S3.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>(a) Analysis of the effect of reducing activation functions. “ActX” refers to the block that only keeps the activation layer after the Xth convolution layer. “All” refers to keeping all activation layers within the block.
(b) Analysis of the effect of reducing normalization functions. “NormY” refers to the block that only keeps the normalization layer after the Yth convolution layer. “No Norm” refers to removing all normalization layers within the block.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T2.st1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.st1.1.1.1" class="ltx_tr">
<th id="S3.T2.st1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Block</th>
<th id="S3.T2.st1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Act</th>
<th id="S3.T2.st1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Central</th>
<th id="S3.T2.st1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.st1.1.2.1" class="ltx_tr">
<th id="S3.T2.st1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st1.1.2.1.1.1" class="ltx_text">Normal</span></th>
<th id="S3.T2.st1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">All</th>
<td id="S3.T2.st1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">95.82</td>
<td id="S3.T2.st1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">77.44</td>
</tr>
<tr id="S3.T2.st1.1.3.2" class="ltx_tr">
<th id="S3.T2.st1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st1.1.3.2.1.1" class="ltx_ERROR undefined">\cdashline</span>2-4</th>
<th id="S3.T2.st1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Act1</th>
<td id="S3.T2.st1.1.3.2.3" class="ltx_td ltx_align_center">95.41</td>
<td id="S3.T2.st1.1.3.2.4" class="ltx_td ltx_align_center">79.24</td>
</tr>
<tr id="S3.T2.st1.1.4.3" class="ltx_tr">
<th id="S3.T2.st1.1.4.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S3.T2.st1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Act2</th>
<td id="S3.T2.st1.1.4.3.3" class="ltx_td ltx_align_center">95.41</td>
<td id="S3.T2.st1.1.4.3.4" class="ltx_td ltx_align_center">80.12</td>
</tr>
<tr id="S3.T2.st1.1.5.4" class="ltx_tr">
<th id="S3.T2.st1.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S3.T2.st1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.5.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act3</span></th>
<td id="S3.T2.st1.1.5.4.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.5.4.3.1" class="ltx_text" style="background-color:#E6E6E6;">95.89</span></td>
<td id="S3.T2.st1.1.5.4.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.5.4.4.1" class="ltx_text" style="background-color:#E6E6E6;">84.33</span></td>
</tr>
<tr id="S3.T2.st1.1.6.5" class="ltx_tr">
<th id="S3.T2.st1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st1.1.6.5.1.1" class="ltx_text">Invert</span></th>
<th id="S3.T2.st1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">All</th>
<td id="S3.T2.st1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">95.64</td>
<td id="S3.T2.st1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">80.35</td>
</tr>
<tr id="S3.T2.st1.1.7.6" class="ltx_tr">
<th id="S3.T2.st1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st1.1.7.6.1.1" class="ltx_ERROR undefined">\cdashline</span>2-4</th>
<th id="S3.T2.st1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.7.6.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act1</span></th>
<td id="S3.T2.st1.1.7.6.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.7.6.3.1" class="ltx_text" style="background-color:#E6E6E6;">96.19</span></td>
<td id="S3.T2.st1.1.7.6.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.7.6.4.1" class="ltx_text" style="background-color:#E6E6E6;">84.12</span></td>
</tr>
<tr id="S3.T2.st1.1.8.7" class="ltx_tr">
<th id="S3.T2.st1.1.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S3.T2.st1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Act2</th>
<td id="S3.T2.st1.1.8.7.3" class="ltx_td ltx_align_center">95.24</td>
<td id="S3.T2.st1.1.8.7.4" class="ltx_td ltx_align_center">82.06</td>
</tr>
<tr id="S3.T2.st1.1.9.8" class="ltx_tr">
<th id="S3.T2.st1.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S3.T2.st1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Act3</th>
<td id="S3.T2.st1.1.9.8.3" class="ltx_td ltx_align_center">95.46</td>
<td id="S3.T2.st1.1.9.8.4" class="ltx_td ltx_align_center">78.60</td>
</tr>
<tr id="S3.T2.st1.1.10.9" class="ltx_tr">
<th id="S3.T2.st1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st1.1.10.9.1.1" class="ltx_text">InvertUp</span></th>
<th id="S3.T2.st1.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">All</th>
<td id="S3.T2.st1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t">95.76</td>
<td id="S3.T2.st1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_t">80.96</td>
</tr>
<tr id="S3.T2.st1.1.11.10" class="ltx_tr">
<th id="S3.T2.st1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st1.1.11.10.1.1" class="ltx_ERROR undefined">\cdashline</span>2-4</th>
<th id="S3.T2.st1.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Act1</th>
<td id="S3.T2.st1.1.11.10.3" class="ltx_td ltx_align_center">95.21</td>
<td id="S3.T2.st1.1.11.10.4" class="ltx_td ltx_align_center">76.97</td>
</tr>
<tr id="S3.T2.st1.1.12.11" class="ltx_tr">
<th id="S3.T2.st1.1.12.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S3.T2.st1.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.12.11.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act2</span></th>
<td id="S3.T2.st1.1.12.11.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.12.11.3.1" class="ltx_text" style="background-color:#E6E6E6;">95.71</span></td>
<td id="S3.T2.st1.1.12.11.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st1.1.12.11.4.1" class="ltx_text" style="background-color:#E6E6E6;">82.44</span></td>
</tr>
<tr id="S3.T2.st1.1.13.12" class="ltx_tr">
<th id="S3.T2.st1.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r"></th>
<th id="S3.T2.st1.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Act3</th>
<td id="S3.T2.st1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">95.56</td>
<td id="S3.T2.st1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb">77.46</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T2.st2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.st2.1.1.1" class="ltx_tr">
<th id="S3.T2.st2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Block</th>
<th id="S3.T2.st2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Act</th>
<th id="S3.T2.st2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Norm</th>
<th id="S3.T2.st2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Central</th>
<th id="S3.T2.st2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.st2.1.2.1" class="ltx_tr">
<th id="S3.T2.st2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st2.1.2.1.1.1" class="ltx_text">Normal</span></th>
<td id="S3.T2.st2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Act3</td>
<td id="S3.T2.st2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S3.T2.st2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">95.89</td>
<td id="S3.T2.st2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">84.33</td>
</tr>
<tr id="S3.T2.st2.1.3.2" class="ltx_tr">
<th id="S3.T2.st2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st2.1.3.2.1.1" class="ltx_ERROR undefined">\cdashline</span>2-5</th>
<td id="S3.T2.st2.1.3.2.2" class="ltx_td ltx_align_center">Act3</td>
<td id="S3.T2.st2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">Norm1</td>
<td id="S3.T2.st2.1.3.2.4" class="ltx_td ltx_align_center">95.84</td>
<td id="S3.T2.st2.1.3.2.5" class="ltx_td ltx_align_center">82.06</td>
</tr>
<tr id="S3.T2.st2.1.4.3" class="ltx_tr">
<th id="S3.T2.st2.1.4.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.4.3.2" class="ltx_td ltx_align_center">Act3</td>
<td id="S3.T2.st2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">Norm2</td>
<td id="S3.T2.st2.1.4.3.4" class="ltx_td ltx_align_center">95.36</td>
<td id="S3.T2.st2.1.4.3.5" class="ltx_td ltx_align_center">82.33</td>
</tr>
<tr id="S3.T2.st2.1.5.4" class="ltx_tr">
<th id="S3.T2.st2.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.5.4.2" class="ltx_td ltx_align_center">Act3</td>
<td id="S3.T2.st2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">Norm3</td>
<td id="S3.T2.st2.1.5.4.4" class="ltx_td ltx_align_center">95.34</td>
<td id="S3.T2.st2.1.5.4.5" class="ltx_td ltx_align_center">81.36</td>
</tr>
<tr id="S3.T2.st2.1.6.5" class="ltx_tr">
<th id="S3.T2.st2.1.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.6.5.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.6.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act3</span></td>
<td id="S3.T2.st2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.6.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">No Norm</span></td>
<td id="S3.T2.st2.1.6.5.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.6.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">95.29</span></td>
<td id="S3.T2.st2.1.6.5.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.6.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">82.50</span></td>
</tr>
<tr id="S3.T2.st2.1.7.6" class="ltx_tr">
<th id="S3.T2.st2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st2.1.7.6.1.1" class="ltx_text">Invert</span></th>
<td id="S3.T2.st2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t">Act1</td>
<td id="S3.T2.st2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S3.T2.st2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t">96.19</td>
<td id="S3.T2.st2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t">84.12</td>
</tr>
<tr id="S3.T2.st2.1.8.7" class="ltx_tr">
<th id="S3.T2.st2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st2.1.8.7.1.1" class="ltx_ERROR undefined">\cdashline</span>2-5</th>
<td id="S3.T2.st2.1.8.7.2" class="ltx_td ltx_align_center">Act1</td>
<td id="S3.T2.st2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">Norm1</td>
<td id="S3.T2.st2.1.8.7.4" class="ltx_td ltx_align_center">95.76</td>
<td id="S3.T2.st2.1.8.7.5" class="ltx_td ltx_align_center">82.48</td>
</tr>
<tr id="S3.T2.st2.1.9.8" class="ltx_tr">
<th id="S3.T2.st2.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.9.8.2" class="ltx_td ltx_align_center">Act1</td>
<td id="S3.T2.st2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">Norm2</td>
<td id="S3.T2.st2.1.9.8.4" class="ltx_td ltx_align_center">96.04</td>
<td id="S3.T2.st2.1.9.8.5" class="ltx_td ltx_align_center">83.02</td>
</tr>
<tr id="S3.T2.st2.1.10.9" class="ltx_tr">
<th id="S3.T2.st2.1.10.9.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.10.9.2" class="ltx_td ltx_align_center">Act1</td>
<td id="S3.T2.st2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r">Norm3</td>
<td id="S3.T2.st2.1.10.9.4" class="ltx_td ltx_align_center">95.59</td>
<td id="S3.T2.st2.1.10.9.5" class="ltx_td ltx_align_center">86.19</td>
</tr>
<tr id="S3.T2.st2.1.11.10" class="ltx_tr">
<th id="S3.T2.st2.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.11.10.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.11.10.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act1</span></td>
<td id="S3.T2.st2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.11.10.3.1" class="ltx_text" style="background-color:#E6E6E6;">No Norm</span></td>
<td id="S3.T2.st2.1.11.10.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.11.10.4.1" class="ltx_text" style="background-color:#E6E6E6;">95.94</span></td>
<td id="S3.T2.st2.1.11.10.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.11.10.5.1" class="ltx_text" style="background-color:#E6E6E6;">84.63</span></td>
</tr>
<tr id="S3.T2.st2.1.12.11" class="ltx_tr">
<th id="S3.T2.st2.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.st2.1.12.11.1.1" class="ltx_text">InvertUp</span></th>
<td id="S3.T2.st2.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t">Act2</td>
<td id="S3.T2.st2.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</td>
<td id="S3.T2.st2.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t">95.71</td>
<td id="S3.T2.st2.1.12.11.5" class="ltx_td ltx_align_center ltx_border_t">82.44</td>
</tr>
<tr id="S3.T2.st2.1.13.12" class="ltx_tr">
<th id="S3.T2.st2.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S3.T2.st2.1.13.12.1.1" class="ltx_ERROR undefined">\cdashline</span>2-5</th>
<td id="S3.T2.st2.1.13.12.2" class="ltx_td ltx_align_center">Act2</td>
<td id="S3.T2.st2.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r">Norm1</td>
<td id="S3.T2.st2.1.13.12.4" class="ltx_td ltx_align_center">95.64</td>
<td id="S3.T2.st2.1.13.12.5" class="ltx_td ltx_align_center">82.45</td>
</tr>
<tr id="S3.T2.st2.1.14.13" class="ltx_tr">
<th id="S3.T2.st2.1.14.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.14.13.2" class="ltx_td ltx_align_center">Act2</td>
<td id="S3.T2.st2.1.14.13.3" class="ltx_td ltx_align_center ltx_border_r">Norm2</td>
<td id="S3.T2.st2.1.14.13.4" class="ltx_td ltx_align_center">95.64</td>
<td id="S3.T2.st2.1.14.13.5" class="ltx_td ltx_align_center">83.46</td>
</tr>
<tr id="S3.T2.st2.1.15.14" class="ltx_tr">
<th id="S3.T2.st2.1.15.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.st2.1.15.14.2" class="ltx_td ltx_align_center">Act2</td>
<td id="S3.T2.st2.1.15.14.3" class="ltx_td ltx_align_center ltx_border_r">Norm3</td>
<td id="S3.T2.st2.1.15.14.4" class="ltx_td ltx_align_center">95.71</td>
<td id="S3.T2.st2.1.15.14.5" class="ltx_td ltx_align_center">85.70</td>
</tr>
<tr id="S3.T2.st2.1.16.15" class="ltx_tr">
<th id="S3.T2.st2.1.16.15.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r"></th>
<td id="S3.T2.st2.1.16.15.2" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.16.15.2.1" class="ltx_text" style="background-color:#E6E6E6;">Act2</span></td>
<td id="S3.T2.st2.1.16.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.16.15.3.1" class="ltx_text" style="background-color:#E6E6E6;">No Norm</span></td>
<td id="S3.T2.st2.1.16.15.4" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.16.15.4.1" class="ltx_text" style="background-color:#E6E6E6;">95.74</span></td>
<td id="S3.T2.st2.1.16.15.5" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#E6E6E6;"><span id="S3.T2.st2.1.16.15.5.1" class="ltx_text" style="background-color:#E6E6E6;">85.65</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Normalization-free setup.</h5>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">Another interesting direction to explore is by removing all normalization layers from our models. This is motivated by recent studies that demonstrate high-performance visual recognition with normalization-free CNNs <cite class="ltx_cite ltx_citemacro_citep">(Brock et al., <a href="#bib.bib4" title="" class="ltx_ref">2021a</a>; <a href="#bib.bib5" title="" class="ltx_ref">b</a>)</cite>. To achieve this, we train normalization-free networks using the Adaptive Gradient Clipping (AGC) technique, following <cite class="ltx_cite ltx_citemacro_citep">(Brock et al., <a href="#bib.bib5" title="" class="ltx_ref">2021b</a>)</cite>. The results are presented in Table <a href="#S3.T2.st2" title="In Table 2 ‣ Reducing normalization layers. ‣ 3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. Surprisingly, we observe that these normalization-free variants are able to achieve competitive performance compared to their best counterparts with only one activation layer and one normalization layer, <em id="S3.SS3.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, 82.50% <em id="S3.SS3.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic">vs.</em> 82.33% for Normal Block, 84.63% <em id="S3.SS3.SSS0.Px3.p1.1.3" class="ltx_emph ltx_font_italic">vs.</em> 86.19% for Invert Block, and 85.65% <em id="S3.SS3.SSS0.Px3.p1.1.4" class="ltx_emph ltx_font_italic">vs.</em> 85.70% for InvertUP Block. Additionally, a normalization-free setup offers practical advantages such as faster training speed <cite class="ltx_cite ltx_citemacro_citep">(Singh &amp; Shrivastava, <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> and reduced GPU memory overhead <cite class="ltx_cite ltx_citemacro_citep">(Bulo et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. In our context, compared to the vanilla block instantiations, removing normalization layers leads to a 28.5% acceleration in training, a 38.8% cut in GPU memory, and conveniently bypasses the need to determine the best strategy for reducing normalization layers.</p>
</div>
<div id="S3.SS3.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px3.p2.1" class="ltx_p">Moreover, our proposed normalization-free approach is particularly noteworthy for its superior performance compared to FedBN <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> (see supplementary material), a widely acknowledged normalization technique in FL. While FedBN decentralizes normalization across multiple clients, ours outperforms it by completely eliminating normalization layers. These findings invite a reevaluation of “already extensively discussed” normalization layers in heterogeneous FL.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Stem layer</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">CNNs and Transformers adopt different pipelines to process input data, which is known as the <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">stem</span>. Typically, CNNs employ a stack of convolutions to downsample images into desired-sized feature maps, while Transformers use patchify layers to directly divide images into a set of tokens. To better understand the impact of the stem layer in heterogeneous FL, we comparatively study diverse stem designs, including the default ResNet-stem, Swin-stem, and ConvStem inspired by <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib56" title="" class="ltx_ref">2021</a>)</cite>. A visualization of these stem designs is provided in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Stem layer ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and the empirical results are reported in Table <a href="#S3.F5" title="Figure 5 ‣ 3.4 Stem layer ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We note that 1) both Swin-stem and ConvStem outperform the vanilla ResNet-stem baseline, and 2) ConvStem attains the best performance. Next, we probe potential enhancements to ResNet-stem and Swin-stem by leveraging the “advanced” designs in ConvStem.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2310.04412/assets/imgs/stem11.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of different stem setups, including ResNet-stem, Swin-stem and ConvStem.
‘s’ denotes the stride of convolution.
</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T3" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Analysis of the effect of various stem layers. “(Kernel Size 5)” denotes using a kernel size of 5 in convolution. “(No MaxPool)” denotes removing the max-pooling layer and increasing the stride of the first convolution layer accordingly.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S3.F5.fig1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F5.fig1.1.1.1" class="ltx_tr">
<th id="S3.F5.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Stem</th>
<td id="S3.F5.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Central</td>
<td id="S3.F5.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">FL</td>
</tr>
<tr id="S3.F5.fig1.1.2.2" class="ltx_tr">
<th id="S3.F5.fig1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet-stem</th>
<td id="S3.F5.fig1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">95.82</td>
<td id="S3.F5.fig1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">77.44</td>
</tr>
<tr id="S3.F5.fig1.1.3.3" class="ltx_tr">
<th id="S3.F5.fig1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Swin-stem</th>
<td id="S3.F5.fig1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">95.61</td>
<td id="S3.F5.fig1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">79.44</td>
</tr>
<tr id="S3.F5.fig1.1.4.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S3.F5.fig1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.F5.fig1.1.4.4.1.1" class="ltx_text" style="background-color:#E6E6E6;">ConvStem</span></th>
<td id="S3.F5.fig1.1.4.4.2" class="ltx_td ltx_align_center"><span id="S3.F5.fig1.1.4.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">95.26</span></td>
<td id="S3.F5.fig1.1.4.4.3" class="ltx_td ltx_align_center"><span id="S3.F5.fig1.1.4.4.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">83.01</span></td>
</tr>
<tr id="S3.F5.fig1.1.5.5" class="ltx_tr">
<th id="S3.F5.fig1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Swin-stem (Kernel Size 5)</th>
<td id="S3.F5.fig1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">95.64</td>
<td id="S3.F5.fig1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">82.76</td>
</tr>
<tr id="S3.F5.fig1.1.6.6" class="ltx_tr">
<th id="S3.F5.fig1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">ResNet-stem (No MaxPool)</th>
<td id="S3.F5.fig1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b">95.76</td>
<td id="S3.F5.fig1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b">82.59</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="/html/2310.04412/assets/imgs/1ks.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The study on the effect of different kernel sizes.</figcaption>
</figure>
</div>
</div>
</figure>
<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Overlapping convolution.</h5>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">We first investigate the performance gap between Swin-stem and ConvStem. We posit that the crux of this gap might be attributed to the variation in patch overlapping. Specifically, Swin-stem employs a convolution layer with a stride of 4 and a kernel size of 4, thereby extracting features from non-overlapping patches; while ConvStem resorts to overlapping convolutions, which inherently bring in adjacent gradient consistency and spatial smoothness <cite class="ltx_cite ltx_citemacro_citep">(Graham et al., <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>. To validate our hypothesis, we modify the Swin-stem by increasing the kernel size to 5 while retaining a stride of 4. This seemingly modest alteration yielded a marked performance enhancement of +3.32% (from 79.44% to 82.76%), confirming the pivotal role of overlapping convolutions within stem layers in heterogeneous FL.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Convlutions-only downsampling.</h5>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">ResNet-stem, despite its employment of a 7<math id="S3.SS4.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS4.SSS0.Px2.p1.1.m1.1a"><mo id="S3.SS4.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.p1.1.m1.1b"><times id="S3.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>7 convolution layer with a stride of 2 — thereby extracting features from overlapped patches — remarkably lags behind Swin-stem in performance. A noteworthy distinction lies in the ResNet-stem’s integration of an additional max-pooling layer to facilitate part of its downsampling; while both Swin-stem and ConvStem exclusively rely on convolution layers for this purpose. To understand the role of the max-pooling layer within ResNet-stem, we remove it and adjust the stride of the initial convolution layer from 2 to 4. As shown in Table <a href="#S3.F5" title="Figure 5 ‣ 3.4 Stem layer ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, this modification, dubbed “ResNet-stem (No MaxPool)”, registers an impressive 5.15% absolute accuracy improvement over the vanilla ResNet-stem. This observation suggests that employing convolutions alone (hence no pooling layers) for downsampling is important in heterogeneous FL.</p>
</div>
<div id="S3.SS4.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px2.p2.1" class="ltx_p">In summary, our analysis highlights the significance of the stem layer’s design in securing model performance in heterogeneous FL. Specifically, two key factors are identified, <em id="S3.SS4.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, the stem layer needs to extract features from overlapping patches and employs convolutions only for downsampling.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Kernel Size</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">Global self-attention is generally recognized as a critical factor that contributes to the robustness of ViT across diverse data distributions in FL <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>. Motivated by this, we explore whether augmenting the receptive field of a CNN– by increasing its kernel size — can enhance this robustness. As depicted in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Stem layer ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, increasing the kernel size directly corresponds to significant accuracy improvements in heterogeneous FL. The largest improvement is achieved with a kernel size of 9, elevating accuracy by 6.95% over the baseline model with a kernel size of 3 (<em id="S3.SS5.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, 84.39% <em id="S3.SS5.p1.1.2" class="ltx_emph ltx_font_italic">vs.</em> 77.44%). It is worth noting, however, that pushing the kernel size beyond 9 ceases to yield further performance enhancements and might, in fact, detract from accuracy.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Component combination</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p">We now introduce FedConv, a novel CNN architecture designed to robustly handle heterogeneous clients in FL. Originating from our ResNet-M baseline model, FedConv incorporates five pivotal design elements, including <em id="S3.SS6.p1.1.1" class="ltx_emph ltx_font_italic">SiLU activation function</em>, <em id="S3.SS6.p1.1.2" class="ltx_emph ltx_font_italic">retraining only one activation function per block</em>, <em id="S3.SS6.p1.1.3" class="ltx_emph ltx_font_italic">normalization-free setup</em>, <em id="S3.SS6.p1.1.4" class="ltx_emph ltx_font_italic">ConvStem</em>, and <em id="S3.SS6.p1.1.5" class="ltx_emph ltx_font_italic">a large kernel size of 9</em>. By building upon three distinct instantiations of CNN blocks (as illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ Reducing activation layers. ‣ 3.3 Reducing activation and normalization layers ‣ 3 FedConv ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), we term the resulting models as FedConv-Normal, FedConv-Invert, and FedConv-InvertUp.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.1" class="ltx_p">Our empirical results demonstrate that these seemingly simple architectural designs collectively lead to a significant performance improvement in heterogeneous FL. As shown in Table <a href="#S4.T4" title="Table 4 ‣ 4 Generalization and Practical Implications ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, FedConv models achieve the best performance, surpassing strong competitors such as ViT, Swin-Transformer, and ConvNeXt. The standout performer, FedConv-InvertUp, records the highest accuracy of 92.21%, outperforming the prior art, ConvNeXt, by 2.64%. These outcomes compellingly contest the assertions in <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>, highlighting that a pure CNN architecture can be a competitive alternative to ViT in heterogeneous FL scenarios.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Generalization and Practical Implications</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we assess the generalization ability and communication costs of FedConv, both of which are critical metrics for real-world deployments.
To facilitate our analysis, we choose FedConv-InvertUp,
our top-performing variant, to serve as the default model for our ablation study.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison on COVID-FL. By incorporating archiectural elements such as SiLU activation function, retaining only one activation function, the normalization-free setup, ConvStem, and a large kernel size of 9, our FedConv models consistently outperform other advanced solutions in heterogeneous FL. </figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Model</th>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FLOPs</td>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Central</th>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">FL</td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">ResNet50</th>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.1G</td>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">95.66</th>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">73.61</td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<th id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">ResNet-M</th>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">4.6G</td>
<th id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">95.82</th>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center">77.44</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<th id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Swin-Tiny</th>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.5G</td>
<th id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">95.74</th>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">88.38</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<th id="S4.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">ViT-Small</th>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">4.6G</td>
<th id="S4.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">95.86</th>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_center">84.89</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<th id="S4.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">ConvNeXt-Tiny</th>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">4.5G</td>
<th id="S4.T4.1.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">96.01</th>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_center">89.57</td>
</tr>
<tr id="S4.T4.1.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T4.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.7.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">FedConv-Normal</span></th>
<td id="S4.T4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.1.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">4.6G</span></td>
<th id="S4.T4.1.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">95.84</span></th>
<td id="S4.T4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.7.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">90.61</span></td>
</tr>
<tr id="S4.T4.1.8.8" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T4.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.8.8.1.1" class="ltx_text" style="background-color:#E6E6E6;">FedConv-Invert</span></th>
<td id="S4.T4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.1.8.8.2.1" class="ltx_text" style="background-color:#E6E6E6;">4.6G</span></td>
<th id="S4.T4.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.8.8.3.1" class="ltx_text" style="background-color:#E6E6E6;">96.19</span></th>
<td id="S4.T4.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.8.4.1" class="ltx_text" style="background-color:#E6E6E6;">91.68</span></td>
</tr>
<tr id="S4.T4.1.9.9" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T4.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.1.9.9.1.1" class="ltx_text" style="background-color:#E6E6E6;">FedConv-InvertUp</span></th>
<td id="S4.T4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T4.1.9.9.2.1" class="ltx_text" style="background-color:#E6E6E6;">4.6G</span></td>
<th id="S4.T4.1.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.1.9.9.3.1" class="ltx_text" style="background-color:#E6E6E6;">96.04</span></th>
<td id="S4.T4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.9.9.4.1" class="ltx_text" style="background-color:#E6E6E6;">92.21</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Generalizing to other Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">To assess the generalizability of our model, we evaluate its performance on two additional heterogeneous FL datasets: CIFAR-10 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib25" title="" class="ltx_ref">2009</a>)</cite> and iNaturalist <cite class="ltx_cite ltx_citemacro_citep">(Van Horn et al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CIFAR-10.</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>, we use the original test set as our validation set, and the training set is divided into five parts, with each part representing one client. Leveraging the mean Kolmogorov-Smirnov (KS) statistic to measure distribution variations between pairs of clients, we create three partitions, each representing different levels of label distribution skewness: split-1 (KS=0, representing an IID set), split-2 (KS=0.49), and split-3 (KS=0.57).</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">iNaturalist.</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">iNaturalist is a large-scale fine-grained visual classification dataset, containing natural images taken by citizen scientists <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. For our analysis, we use a federated version, iNature, sourced from FedScale <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. This version includes 193K images from 2295 clients, of which 1901 are in the training set and the remaining 394 are in the validation set.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ iNaturalist. ‣ 4.1 Generalizing to other Datasets ‣ 4 Generalization and Practical Implications ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reports the performance on CIFAR-10 and iNaturalist datasets. As data heterogeneity increases from split1 to split3 on CIFAR-10, while FedConv only experiences a modest accuracy drop of 1.85%, other models drop the accuracy by at least 2.35%. On iNaturalist, FedConv impressively achieves an accuracy of 54.19%, surpassing the runner-up, ViT-Small, by more than 10%. These results confirm the strong generalization ability of FedConv in highly heterogeneous FL settings.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison on CIFAR-10 and iNaturalist. Our FedConv model consistently outperforms other models. Notably, as data heterogeneity increases, FedConv’s strong generalization becomes more evident.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.1.1.1.1.1" class="ltx_text">Model</span></th>
<td id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">CIFAR-10</td>
<td id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">iNaturalist</td>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<td id="S4.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Central</td>
<td id="S4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Split-1</td>
<td id="S4.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Split-2</td>
<td id="S4.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Split-3</td>
<td id="S4.T5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">FL</td>
</tr>
<tr id="S4.T5.1.3.3" class="ltx_tr">
<th id="S4.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</th>
<td id="S4.T5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">97.47</td>
<td id="S4.T5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">96.69</td>
<td id="S4.T5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">95.56</td>
<td id="S4.T5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.43</td>
<td id="S4.T5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">12.61</td>
</tr>
<tr id="S4.T5.1.4.4" class="ltx_tr">
<th id="S4.T5.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Swin-Tiny</th>
<td id="S4.T5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">98.31</td>
<td id="S4.T5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">98.36</td>
<td id="S4.T5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">97.83</td>
<td id="S4.T5.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.22</td>
<td id="S4.T5.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">24.57</td>
</tr>
<tr id="S4.T5.1.5.5" class="ltx_tr">
<th id="S4.T5.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ViT-Small</th>
<td id="S4.T5.1.5.5.2" class="ltx_td ltx_align_center">97.99</td>
<td id="S4.T5.1.5.5.3" class="ltx_td ltx_align_center">98.24</td>
<td id="S4.T5.1.5.5.4" class="ltx_td ltx_align_center">97.84</td>
<td id="S4.T5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">95.64</td>
<td id="S4.T5.1.5.5.6" class="ltx_td ltx_align_center">40.30</td>
</tr>
<tr id="S4.T5.1.6.6" class="ltx_tr">
<th id="S4.T5.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ConvNeXt-Tiny</th>
<td id="S4.T5.1.6.6.2" class="ltx_td ltx_align_center">98.31</td>
<td id="S4.T5.1.6.6.3" class="ltx_td ltx_align_center">98.20</td>
<td id="S4.T5.1.6.6.4" class="ltx_td ltx_align_center">97.67</td>
<td id="S4.T5.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r">95.85</td>
<td id="S4.T5.1.6.6.6" class="ltx_td ltx_align_center">22.53</td>
</tr>
<tr id="S4.T5.1.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T5.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T5.1.7.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">FedConv-InvertUp</span></th>
<td id="S4.T5.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">98.42</span></td>
<td id="S4.T5.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">98.11</span></td>
<td id="S4.T5.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.7.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">97.74</span></td>
<td id="S4.T5.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T5.1.7.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">96.26</span></td>
<td id="S4.T5.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.7.7.6.1" class="ltx_text" style="background-color:#E6E6E6;">54.19</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Generalizing to other FL Methods</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We next evaluate our model with different FL methods, namely FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite>, FedAVG-Share <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib65" title="" class="ltx_ref">2018</a>)</cite>, and FedYogi <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>. FedProx introduces a proximal term to estimate and restrict the impact of the local model on the global model; FedAVG-Share utilizes a globally shared dataset to collect data from each client for local model updating; FedYogi incorporates the adaptive optimization technique Yogi <cite class="ltx_cite ltx_citemacro_citep">(Zaheer et al., <a href="#bib.bib61" title="" class="ltx_ref">2018</a>)</cite> into the FL context.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">The results, as reported in Table <a href="#S4.F6" title="Figure 6 ‣ 4.2 Generalizing to other FL Methods ‣ 4 Generalization and Practical Implications ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, consistently highlight the superior performance of our FedConv model across these diverse FL methods. This observation underscores FedConv’s potential to enhance a wide range of heterogeneous FL methods, enabling seamless integration and suggesting its promise for further performance improvements.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T6" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance comparison with different FL methods on COVID-FL. ’Share’ denotes ’FedAVG-Share’. We note our FedConv consistently shows superior performance.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.F6.fig1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.F6.fig1.1.1.1" class="ltx_tr">
<th id="S4.F6.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S4.F6.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FedProx</th>
<th id="S4.F6.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Share</th>
<th id="S4.F6.fig1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FedYogi</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.F6.fig1.1.2.1" class="ltx_tr">
<th id="S4.F6.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</th>
<td id="S4.F6.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">72.92</td>
<td id="S4.F6.fig1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">92.43</td>
<td id="S4.F6.fig1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">66.01</td>
</tr>
<tr id="S4.F6.fig1.1.3.2" class="ltx_tr">
<th id="S4.F6.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ViT-Small</th>
<td id="S4.F6.fig1.1.3.2.2" class="ltx_td ltx_align_center">87.07</td>
<td id="S4.F6.fig1.1.3.2.3" class="ltx_td ltx_align_center">93.89</td>
<td id="S4.F6.fig1.1.3.2.4" class="ltx_td ltx_align_center">87.69</td>
</tr>
<tr id="S4.F6.fig1.1.4.3" class="ltx_tr">
<th id="S4.F6.fig1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Swin-Tiny</th>
<td id="S4.F6.fig1.1.4.3.2" class="ltx_td ltx_align_center">87.74</td>
<td id="S4.F6.fig1.1.4.3.3" class="ltx_td ltx_align_center">94.02</td>
<td id="S4.F6.fig1.1.4.3.4" class="ltx_td ltx_align_center">91.86</td>
</tr>
<tr id="S4.F6.fig1.1.5.4" class="ltx_tr">
<th id="S4.F6.fig1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ConvNeXt-Tiny</th>
<td id="S4.F6.fig1.1.5.4.2" class="ltx_td ltx_align_center">89.35</td>
<td id="S4.F6.fig1.1.5.4.3" class="ltx_td ltx_align_center">95.11</td>
<td id="S4.F6.fig1.1.5.4.4" class="ltx_td ltx_align_center">92.46</td>
</tr>
<tr id="S4.F6.fig1.1.6.5" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.F6.fig1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.F6.fig1.1.6.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">FedConv-InvertUp</span></th>
<td id="S4.F6.fig1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F6.fig1.1.6.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">92.11</span></td>
<td id="S4.F6.fig1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F6.fig1.1.6.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">95.23</span></td>
<td id="S4.F6.fig1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F6.fig1.1.6.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">93.10</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="/html/2310.04412/assets/imgs/commu4.png" id="S4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="309" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Test accuracy versus communication rounds conducted on the split-3 of CIFAR-10. The black dashed line is the target test accuracy. Our model shows the fastest convergence speed.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Communication Cost</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">In FL, communication can be a major bottleneck due to the inherent complications of coordinating numerous devices. The process of client communication is often more time-consuming than local model updates, thereby emerging as a significant challenge in FL <cite class="ltx_cite ltx_citemacro_citep">(Van Berkel, <a href="#bib.bib50" title="" class="ltx_ref">2009</a>)</cite>. The total number of communication rounds and the size of the messages transmitted during each round are key factors in determining the communication efficiency <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>. To comprehensively evaluate these aspects, we follow the methodology proposed in <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>. Specifically, we record the number of communication rounds required for different models to achieve a preset accuracy threshold. Additionally, we use Transmitted Message Size (TMS), which is calculated by multiplying the number of model parameters with the associated communication rounds, to quantify communication costs.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">As shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Generalizing to other FL Methods ‣ 4 Generalization and Practical Implications ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our FedConv-InvertUp achieves the fastest convergence speed among all models. In CIFAR-10 split3, where high data heterogeneity exists, FedConv-InvertUp only needs 4 communication rounds to achieve the target accuracy of 90%, while ConvNeXt necessitates 7 rounds. This efficiency also translates to a marked reduction in TMS in FL, as reported in Table <a href="#S4.T7" title="Table 7 ‣ 4.3 Communication Cost ‣ 4 Generalization and Practical Implications ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. In contrast, ResNet struggles to converge to the 90% accuracy threshold in the CIFAR-10 split3 setting. These results demonstrate the effectiveness of our proposed FedConv architecture in reducing communication costs and improving the overall FL performance.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison based on TMS. TMS is calculated by multiplying the number of model parameters with the communication rounds needed to attain the target accuracy. We note our FedConv requires the lowest TMS to reach the target accuracy.</figcaption>
<table id="S4.T7.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.5.6.1" class="ltx_tr">
<th id="S4.T7.5.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S4.T7.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ResNet50</th>
<th id="S4.T7.5.6.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Swin-Tiny</th>
<th id="S4.T7.5.6.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ViT-Small</th>
<th id="S4.T7.5.6.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ConvNext-Tiny</th>
<th id="S4.T7.5.6.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FedConv-InvertUp</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.5.5" class="ltx_tr">
<th id="S4.T7.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">TMS</th>
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><math id="S4.T7.1.1.1.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S4.T7.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T7.1.1.1.m1.1.1" xref="S4.T7.1.1.1.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.m1.1b"><infinity id="S4.T7.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.m1.1c">\infty</annotation></semantics></math></td>
<td id="S4.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">27.5M<math id="S4.T7.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.2.2.2.m1.1a"><mo id="S4.T7.2.2.2.m1.1.1" xref="S4.T7.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.m1.1b"><times id="S4.T7.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.m1.1c">\times</annotation></semantics></math>10</td>
<td id="S4.T7.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">21.7M<math id="S4.T7.3.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.3.3.3.m1.1a"><mo id="S4.T7.3.3.3.m1.1.1" xref="S4.T7.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.m1.1b"><times id="S4.T7.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.m1.1c">\times</annotation></semantics></math>11</td>
<td id="S4.T7.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">27.8M<math id="S4.T7.4.4.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.4.4.4.m1.1a"><mo id="S4.T7.4.4.4.m1.1.1" xref="S4.T7.4.4.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.m1.1b"><times id="S4.T7.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.m1.1c">\times</annotation></semantics></math>8</td>
<td id="S4.T7.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">25.6M<math id="S4.T7.5.5.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T7.5.5.5.m1.1a"><mo id="S4.T7.5.5.5.m1.1.1" xref="S4.T7.5.5.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.m1.1b"><times id="S4.T7.5.5.5.m1.1.1.cmml" xref="S4.T7.5.5.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.m1.1c">\times</annotation></semantics></math>5</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this paper, we conduct a comprehensive investigation of several architectural elements in ViT, showing those elements that, when integrated into CNNs, substantially enhance their performance in heterogeneous FL. Moreover, by combining these architectural modifications, we succeed in building a pure CNN architecture that can consistently match or even outperform ViT in a range of heterogeneous FL settings. We hope that our proposed FedConv can serve as a strong baseline, catalyzing further innovations in FL research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This work is supported by a gift from Open Philanthropy, TPU Research Cloud Program, and Google Cloud Research Credits program.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2021)</span>
<span class="ltx_bibblock">
Yutong Bai, Jieru Mei, Alan Yuille, and Cihang Xie.

</span>
<span class="ltx_bibblock">Are transformers more robust than CNNs?

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhojanapalli et al. (2021)</span>
<span class="ltx_bibblock">
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas
Unterthiner, and Andreas Veit.

</span>
<span class="ltx_bibblock">Understanding robustness of transformers for image classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.14586</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briggs et al. (2020)</span>
<span class="ltx_bibblock">
Christopher Briggs, Zhong Fan, and Peter Andras.

</span>
<span class="ltx_bibblock">Federated learning with hierarchical clustering of local updates to
improve training on non-iid data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IJCNN</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brock et al. (2021a)</span>
<span class="ltx_bibblock">
Andrew Brock, Soham De, and Samuel L Smith.

</span>
<span class="ltx_bibblock">Characterizing signal propagation to close the performance gap in
unnormalized resnets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2021a.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brock et al. (2021b)</span>
<span class="ltx_bibblock">
Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan.

</span>
<span class="ltx_bibblock">High-performance large-scale image recognition without normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021b.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulo et al. (2018)</span>
<span class="ltx_bibblock">
Samuel Rota Bulo, Lorenzo Porzi, and Peter Kontschieder.

</span>
<span class="ltx_bibblock">In-place activated batchnorm for memory-optimized training of dnns.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clevert et al. (2016)</span>
<span class="ltx_bibblock">
Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter.

</span>
<span class="ltx_bibblock">Fast and accurate deep network learning by exponential linear units
(ELUs).

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cubuk et al. (2020)</span>
<span class="ltx_bibblock">
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.

</span>
<span class="ltx_bibblock">Randaugment: Practical data augmentation with no separate search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Zhixu Du, Jingwei Sun, Ang Li, Pin-Yu Chen, Jianyi Zhang, Hai Li, Yiran Chen,
et al.

</span>
<span class="ltx_bibblock">Rethinking normalization methods in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.03277</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dugas et al. (2000)</span>
<span class="ltx_bibblock">
Claude Dugas, Yoshua Bengio, François B’elisle, Claude Nadeau, and
Ren’e Garcia.

</span>
<span class="ltx_bibblock">Incorporating second-order functional knowledge for better option
pricing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2000.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elfwing et al. (2018)</span>
<span class="ltx_bibblock">
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.

</span>
<span class="ltx_bibblock">Sigmoid-weighted linear units for neural network function
approximation in reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, 107:3–11, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graham et al. (2021)</span>
<span class="ltx_bibblock">
Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
Hervé Jégou, and Matthijs Douze.

</span>
<span class="ltx_bibblock">Levit: a vision transformer in convnet’s clothing for faster
inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2015)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2022)</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks &amp; Gimpel (2016)</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (gelus).

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.08415</em>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al. (2019)</span>
<span class="ltx_bibblock">
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al.

</span>
<span class="ltx_bibblock">Searching for mobilenetv3.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al. (2017)</span>
<span class="ltx_bibblock">
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">MobileNets: Efficient convolutional neural networks for mobile
vision applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv:1704.04861</em>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al. (2020)</span>
<span class="ltx_bibblock">
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.

</span>
<span class="ltx_bibblock">The non-iid data quagmire of decentralized machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2019)</span>
<span class="ltx_bibblock">
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.

</span>
<span class="ltx_bibblock">Measuring the effects of non-identical data distribution for
federated visual classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06335</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2020)</span>
<span class="ltx_bibblock">
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.

</span>
<span class="ltx_bibblock">Federated visual classification with real-world data distribution.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2016)</span>
<span class="ltx_bibblock">
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger.

</span>
<span class="ltx_bibblock">Deep networks with stochastic depth.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. (2020)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2021)</span>
<span class="ltx_bibblock">
Fan Lai, Yinwei Dai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf
Chowdhury.

</span>
<span class="ltx_bibblock">Fedscale: Benchmarking model and system performance of federated
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ResilientFL</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Wang (2019)</span>
<span class="ltx_bibblock">
Daliang Li and Junpu Wang.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Li Li, Yuxi Fan, Mike Tse, and Kuo-Yi Lin.

</span>
<span class="ltx_bibblock">A review of applications in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Industrial Engineering</em>, 149:106854,
2020a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(3):50–60, 2020b.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020c)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">MLSys</em>, 2020c.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou.

</span>
<span class="ltx_bibblock">Fedbn: Federated learning on non-iid features via local batch
normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.07623</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
and Saining Xie.

</span>
<span class="ltx_bibblock">A convnet for the 2020s.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">AISTATS</em>, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul &amp; Chen (2022)</span>
<span class="ltx_bibblock">
Sayak Paul and Pin-Yu Chen.

</span>
<span class="ltx_bibblock">Vision transformers are robust learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2022a)</span>
<span class="ltx_bibblock">
Liangqiong Qu, Niranjan Balachandar, Miao Zhang, and Daniel Rubin.

</span>
<span class="ltx_bibblock">Handling data heterogeneity with generative replay in collaborative
learning for medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 78:102424,
2022a.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2022b)</span>
<span class="ltx_bibblock">
Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli,
Li Fei-Fei, and Daniel Rubin.

</span>
<span class="ltx_bibblock">Rethinking architecture design for tackling data heterogeneity in
federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022b.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2020)</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.00295</em>, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandler et al. (2018)</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.

</span>
<span class="ltx_bibblock">Mobilenetv2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan &amp; Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh &amp; Shrivastava (2019)</span>
<span class="ltx_bibblock">
Saurabh Singh and Abhinav Shrivastava.

</span>
<span class="ltx_bibblock">Evalnorm: Estimating batch normalization statistics for evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2016a)</span>
<span class="ltx_bibblock">
Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke.

</span>
<span class="ltx_bibblock">Inception-v4, inception-resnet and the impact of residual connections
on learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ICLR Workshop</em>, 2016a.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2016b)</span>
<span class="ltx_bibblock">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
Zbigniew Wojna.

</span>
<span class="ltx_bibblock">Rethinking the inception architecture for computer vision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016b.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan &amp; Le (2019)</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2021a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Training data-efficient image transformers &amp; distillation through
attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021a.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2021b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
Hervé Jégou.

</span>
<span class="ltx_bibblock">Going deeper with image transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021b.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trockman &amp; Kolter (2022)</span>
<span class="ltx_bibblock">
Asher Trockman and J Zico Kolter.

</span>
<span class="ltx_bibblock">Patches are all you need?

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.09792</em>, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Berkel (2009)</span>
<span class="ltx_bibblock">
CH Van Berkel.

</span>
<span class="ltx_bibblock">Multi-core for mobile phones.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">DATE</em>, 2009.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Horn et al. (2018)</span>
<span class="ltx_bibblock">
Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie.

</span>
<span class="ltx_bibblock">The inaturalist species classification and detection dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020a)</span>
<span class="ltx_bibblock">
Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li.

</span>
<span class="ltx_bibblock">Optimizing federated learning on non-iid data with reinforcement
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">INFOCOM</em>, 2020a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020b)</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020b.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wightman et al. (2021)</span>
<span class="ltx_bibblock">
Ross Wightman, Hugo Touvron, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Resnet strikes back: An improved training procedure in timm.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.00476</em>, 2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2021)</span>
<span class="ltx_bibblock">
Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor Darrell, and Ross
Girshick.

</span>
<span class="ltx_bibblock">Early convolutions help transformers see better.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2020)</span>
<span class="ltx_bibblock">
Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le.

</span>
<span class="ltx_bibblock">Smooth adversarial training.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.14536</em>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2017)</span>
<span class="ltx_bibblock">
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.

</span>
<span class="ltx_bibblock">Aggregated residual transformations for deep neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023)</span>
<span class="ltx_bibblock">
Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel
Rubin, Lei Xing, and Yuyin Zhou.

</span>
<span class="ltx_bibblock">Label-efficient self-supervised federated learning for tackling data
heterogeneity in medical imaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yun et al. (2019)</span>
<span class="ltx_bibblock">
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and
Youngjoon Yoo.

</span>
<span class="ltx_bibblock">Cutmix: Regularization strategy to train strong classifiers with
localizable features.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaheer et al. (2018)</span>
<span class="ltx_bibblock">
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.

</span>
<span class="ltx_bibblock">Adaptive methods for nonconvex optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang Zhou,
Zhongang Cai, Haiyu Zhao, Xianglong Liu, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Delving deep into the generalization of vision transformers under
distribution shifts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.

</span>
<span class="ltx_bibblock">mixup: Beyond empirical risk minimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2018.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Wei Zhang, Xiang Li, Hui Ma, Zhong Luo, and Xu Li.

</span>
<span class="ltx_bibblock">Federated learning for machinery fault diagnosis with dynamic
validation and self-supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 213:106679, 2021.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>, 2018.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2020)</span>
<span class="ltx_bibblock">
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.

</span>
<span class="ltx_bibblock">Random erasing data augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou.

</span>
<span class="ltx_bibblock">Data-free knowledge distillation for heterogeneous federated
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Analysis of FedBN</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">We experiment with FedBN<cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, a popular algorithm that also tackles data heterogeneity from the perspective of model architecture. Its key idea is to not average BN layers in FedAVG. We apply this method on the regular ResNet-50, and a ConvNeXt-Tiny model with its normalization layer changed from LN-C to BN. As shown in Table <a href="#A1.T8" title="Table 8 ‣ A.1 Analysis of FedBN ‣ Appendix A Appendix ‣ FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, in split 1 and 2, where data heterogeneity is not so severe, FedBN achieves close performance compared to FedAVG. However, in a more extreme heterogeneous scenario like CIFAR-10 split3, the performance of both ResNet and ConvNeXt-BN trained by FedBN drops sharply. Specifically, from split1 to split 3, ResNet and ConvNeXt-BN shows a drop of 14.60% (96.42% to 81.82%), and 24.12% (97.99% to 73.87%), respectively. By contrast, the original ConvNeXt that chooses LN-C as its normalization layer, shows only a performance drop of 2.35% (98.20% to 95.85%). Also, our proposed FedConv achieves the best accuracy of 96.26% in split3, demonstrating the effectiveness of the normalization-free design.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance comparison on CIFAR-10 dataset. ’ConvNeXt-BN’ denotes ConvNeXt with LN-C changed to BN. ’<math id="A1.T8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.2.m1.1b"><mo id="A1.T8.2.m1.1.1" xref="A1.T8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.2.m1.1c"><csymbol cd="latexml" id="A1.T8.2.m1.1.1.cmml" xref="A1.T8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.m1.1d">\pm</annotation></semantics></math>’ indicates the range of accuracy between clients. </figcaption>
<table id="A1.T8.17" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T8.17.16.1" class="ltx_tr">
<th id="A1.T8.17.16.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="A1.T8.17.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Model</th>
<td id="A1.T8.17.16.1.3" class="ltx_td ltx_align_center ltx_border_t">Split1</td>
<td id="A1.T8.17.16.1.4" class="ltx_td ltx_align_center ltx_border_t">Split2</td>
<td id="A1.T8.17.16.1.5" class="ltx_td ltx_align_center ltx_border_t">Split3</td>
</tr>
<tr id="A1.T8.5.3" class="ltx_tr">
<th id="A1.T8.5.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="A1.T8.5.3.4.1" class="ltx_text">FedBN</span></th>
<th id="A1.T8.5.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</th>
<td id="A1.T8.3.1.1" class="ltx_td ltx_align_center ltx_border_t">96.42<math id="A1.T8.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.3.1.1.m1.1a"><mo id="A1.T8.3.1.1.m1.1.1" xref="A1.T8.3.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.3.1.1.m1.1b"><csymbol cd="latexml" id="A1.T8.3.1.1.m1.1.1.cmml" xref="A1.T8.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.3.1.1.m1.1c">\pm</annotation></semantics></math>0.18</td>
<td id="A1.T8.4.2.2" class="ltx_td ltx_align_center ltx_border_t">93.15<math id="A1.T8.4.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.4.2.2.m1.1a"><mo id="A1.T8.4.2.2.m1.1.1" xref="A1.T8.4.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.4.2.2.m1.1b"><csymbol cd="latexml" id="A1.T8.4.2.2.m1.1.1.cmml" xref="A1.T8.4.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.4.2.2.m1.1c">\pm</annotation></semantics></math>1.35</td>
<td id="A1.T8.5.3.3" class="ltx_td ltx_align_center ltx_border_t">81.82<math id="A1.T8.5.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.5.3.3.m1.1a"><mo id="A1.T8.5.3.3.m1.1.1" xref="A1.T8.5.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.5.3.3.m1.1b"><csymbol cd="latexml" id="A1.T8.5.3.3.m1.1.1.cmml" xref="A1.T8.5.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.5.3.3.m1.1c">\pm</annotation></semantics></math>1.52</td>
</tr>
<tr id="A1.T8.8.6" class="ltx_tr">
<th id="A1.T8.8.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ConvNeXt-BN</th>
<td id="A1.T8.6.4.1" class="ltx_td ltx_align_center">97.99<math id="A1.T8.6.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.6.4.1.m1.1a"><mo id="A1.T8.6.4.1.m1.1.1" xref="A1.T8.6.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.6.4.1.m1.1b"><csymbol cd="latexml" id="A1.T8.6.4.1.m1.1.1.cmml" xref="A1.T8.6.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.6.4.1.m1.1c">\pm</annotation></semantics></math>0.05</td>
<td id="A1.T8.7.5.2" class="ltx_td ltx_align_center">95.76<math id="A1.T8.7.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.7.5.2.m1.1a"><mo id="A1.T8.7.5.2.m1.1.1" xref="A1.T8.7.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.7.5.2.m1.1b"><csymbol cd="latexml" id="A1.T8.7.5.2.m1.1.1.cmml" xref="A1.T8.7.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.7.5.2.m1.1c">\pm</annotation></semantics></math>0.82</td>
<td id="A1.T8.8.6.3" class="ltx_td ltx_align_center">73.87<math id="A1.T8.8.6.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.8.6.3.m1.1a"><mo id="A1.T8.8.6.3.m1.1.1" xref="A1.T8.8.6.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.8.6.3.m1.1b"><csymbol cd="latexml" id="A1.T8.8.6.3.m1.1.1.cmml" xref="A1.T8.8.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.8.6.3.m1.1c">\pm</annotation></semantics></math>12.57</td>
</tr>
<tr id="A1.T8.11.9" class="ltx_tr">
<th id="A1.T8.11.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="A1.T8.11.9.4.1" class="ltx_text">FedAVG</span></th>
<th id="A1.T8.11.9.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet50</th>
<td id="A1.T8.9.7.1" class="ltx_td ltx_align_center ltx_border_t">96.69<math id="A1.T8.9.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.9.7.1.m1.1a"><mo id="A1.T8.9.7.1.m1.1.1" xref="A1.T8.9.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.9.7.1.m1.1b"><csymbol cd="latexml" id="A1.T8.9.7.1.m1.1.1.cmml" xref="A1.T8.9.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.9.7.1.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.10.8.2" class="ltx_td ltx_align_center ltx_border_t">95.56<math id="A1.T8.10.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.10.8.2.m1.1a"><mo id="A1.T8.10.8.2.m1.1.1" xref="A1.T8.10.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.10.8.2.m1.1b"><csymbol cd="latexml" id="A1.T8.10.8.2.m1.1.1.cmml" xref="A1.T8.10.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.10.8.2.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.11.9.3" class="ltx_td ltx_align_center ltx_border_t">87.43 <math id="A1.T8.11.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.11.9.3.m1.1a"><mo id="A1.T8.11.9.3.m1.1.1" xref="A1.T8.11.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.11.9.3.m1.1b"><csymbol cd="latexml" id="A1.T8.11.9.3.m1.1.1.cmml" xref="A1.T8.11.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.11.9.3.m1.1c">\pm</annotation></semantics></math>0.00</td>
</tr>
<tr id="A1.T8.14.12" class="ltx_tr">
<th id="A1.T8.14.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ConvNeXt</th>
<td id="A1.T8.12.10.1" class="ltx_td ltx_align_center">98.20<math id="A1.T8.12.10.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.12.10.1.m1.1a"><mo id="A1.T8.12.10.1.m1.1.1" xref="A1.T8.12.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.12.10.1.m1.1b"><csymbol cd="latexml" id="A1.T8.12.10.1.m1.1.1.cmml" xref="A1.T8.12.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.12.10.1.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.13.11.2" class="ltx_td ltx_align_center">97.67<math id="A1.T8.13.11.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.13.11.2.m1.1a"><mo id="A1.T8.13.11.2.m1.1.1" xref="A1.T8.13.11.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.13.11.2.m1.1b"><csymbol cd="latexml" id="A1.T8.13.11.2.m1.1.1.cmml" xref="A1.T8.13.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.13.11.2.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.14.12.3" class="ltx_td ltx_align_center">95.85<math id="A1.T8.14.12.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.14.12.3.m1.1a"><mo id="A1.T8.14.12.3.m1.1.1" xref="A1.T8.14.12.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.14.12.3.m1.1b"><csymbol cd="latexml" id="A1.T8.14.12.3.m1.1.1.cmml" xref="A1.T8.14.12.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.14.12.3.m1.1c">\pm</annotation></semantics></math>0.00</td>
</tr>
<tr id="A1.T8.17.15" class="ltx_tr">
<th id="A1.T8.17.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">FedConv-InvertUp</th>
<td id="A1.T8.15.13.1" class="ltx_td ltx_align_center ltx_border_b">98.11<math id="A1.T8.15.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.15.13.1.m1.1a"><mo id="A1.T8.15.13.1.m1.1.1" xref="A1.T8.15.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.15.13.1.m1.1b"><csymbol cd="latexml" id="A1.T8.15.13.1.m1.1.1.cmml" xref="A1.T8.15.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.15.13.1.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.16.14.2" class="ltx_td ltx_align_center ltx_border_b">97.74<math id="A1.T8.16.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.16.14.2.m1.1a"><mo id="A1.T8.16.14.2.m1.1.1" xref="A1.T8.16.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.16.14.2.m1.1b"><csymbol cd="latexml" id="A1.T8.16.14.2.m1.1.1.cmml" xref="A1.T8.16.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.16.14.2.m1.1c">\pm</annotation></semantics></math>0.00</td>
<td id="A1.T8.17.15.3" class="ltx_td ltx_align_center ltx_border_b">96.26<math id="A1.T8.17.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="A1.T8.17.15.3.m1.1a"><mo id="A1.T8.17.15.3.m1.1.1" xref="A1.T8.17.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="A1.T8.17.15.3.m1.1b"><csymbol cd="latexml" id="A1.T8.17.15.3.m1.1.1.cmml" xref="A1.T8.17.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.17.15.3.m1.1c">\pm</annotation></semantics></math>0.00</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation Details</h3>

<section id="A1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Pre-training Recipe</h4>

<div id="A1.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>, we pre-train our model for 300 epochs. The learning rate is set to 4e-3 with linear warmup for 20 epochs and cosine decay schedule in subsequent epochs. AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> is adopted with weight decay set to 0.05. For data augmentation, we adopt Mixup <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib63" title="" class="ltx_ref">2018</a>)</cite>, CutMix <cite class="ltx_cite ltx_citemacro_citep">(Yun et al., <a href="#bib.bib60" title="" class="ltx_ref">2019</a>)</cite>, RandAugment
<cite class="ltx_cite ltx_citemacro_citep">(Cubuk et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and Random Erasing <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite>. For regularization, we adopt Stochastic Depth<cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> and Label Smoothing <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al., <a href="#bib.bib45" title="" class="ltx_ref">2016b</a>)</cite>. Layer Scale <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib48" title="" class="ltx_ref">2021b</a>)</cite> of initial value 1e-6 is applied. All layer weights are initialized using
truncated normal distribution.</p>
</div>
</section>
<section id="A1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Fine-tuning Recipe</h4>

<section id="A1.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CIFAR-10.</h5>

<div id="A1.SS2.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.Px1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citet">Qu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022b</a>)</cite>, for CNN structures and ViT, a SGD optimizer without weight decay is adopted. For Swin-Transformer, a AdamW with a weight decay of 0.05 is adopted. The learning rate is set to 0.03 for CNNs and ViT, and 3.125e-5 for Swin-Transformer. We use 5-epoch linear warmup and cosine learning rate decay. Stochastic Depth rate is applied with value set to 0.1, and gradient clipping is set to 1 for all models. For FL settings, we train models for 100 communication rounds, and we choose all 5 clients in each round.</p>
</div>
</section>
<section id="A1.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">iNaturalist.</h5>

<div id="A1.SS2.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.Px2.p1.1" class="ltx_p">We follow the training settings in CIFAR-10. For FL settings, we train models for 2000 communication rounds, and choose 25 clients in each round.</p>
</div>
</section>
</section>
<section id="A1.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span>FL methods</h4>

<div id="A1.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p1.5" class="ltx_p">In FedProx, <math id="A1.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="A1.SS2.SSS3.p1.1.m1.1a"><mi id="A1.SS2.SSS3.p1.1.m1.1.1" xref="A1.SS2.SSS3.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS3.p1.1.m1.1b"><ci id="A1.SS2.SSS3.p1.1.m1.1.1.cmml" xref="A1.SS2.SSS3.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS3.p1.1.m1.1c">\mu</annotation></semantics></math> is set to 5e-5 for Swin, and 5e-4 for other models. We share 5% images from each client in FedAVG-Share. In FedYogi, we set <math id="A1.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="A1.SS2.SSS3.p1.2.m2.1a"><msub id="A1.SS2.SSS3.p1.2.m2.1.1" xref="A1.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="A1.SS2.SSS3.p1.2.m2.1.1.2" xref="A1.SS2.SSS3.p1.2.m2.1.1.2.cmml">β</mi><mn id="A1.SS2.SSS3.p1.2.m2.1.1.3" xref="A1.SS2.SSS3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS3.p1.2.m2.1b"><apply id="A1.SS2.SSS3.p1.2.m2.1.1.cmml" xref="A1.SS2.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="A1.SS2.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="A1.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="A1.SS2.SSS3.p1.2.m2.1.1.2">𝛽</ci><cn type="integer" id="A1.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="A1.SS2.SSS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS3.p1.2.m2.1c">\beta_{1}</annotation></semantics></math>, <math id="A1.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="A1.SS2.SSS3.p1.3.m3.1a"><msub id="A1.SS2.SSS3.p1.3.m3.1.1" xref="A1.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="A1.SS2.SSS3.p1.3.m3.1.1.2" xref="A1.SS2.SSS3.p1.3.m3.1.1.2.cmml">β</mi><mn id="A1.SS2.SSS3.p1.3.m3.1.1.3" xref="A1.SS2.SSS3.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS3.p1.3.m3.1b"><apply id="A1.SS2.SSS3.p1.3.m3.1.1.cmml" xref="A1.SS2.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="A1.SS2.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="A1.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="A1.SS2.SSS3.p1.3.m3.1.1.2">𝛽</ci><cn type="integer" id="A1.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="A1.SS2.SSS3.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS3.p1.3.m3.1c">\beta_{2}</annotation></semantics></math> to 0.9 and 0.99 following <cite class="ltx_cite ltx_citemacro_citet">Reddi et al. (<a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, client learning rate <math id="A1.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="A1.SS2.SSS3.p1.4.m4.1a"><mi id="A1.SS2.SSS3.p1.4.m4.1.1" xref="A1.SS2.SSS3.p1.4.m4.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS3.p1.4.m4.1b"><ci id="A1.SS2.SSS3.p1.4.m4.1.1.cmml" xref="A1.SS2.SSS3.p1.4.m4.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS3.p1.4.m4.1c">\eta</annotation></semantics></math> to 0.01, and adaptivity <math id="A1.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="A1.SS2.SSS3.p1.5.m5.1a"><mi id="A1.SS2.SSS3.p1.5.m5.1.1" xref="A1.SS2.SSS3.p1.5.m5.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS3.p1.5.m5.1b"><ci id="A1.SS2.SSS3.p1.5.m5.1.1.cmml" xref="A1.SS2.SSS3.p1.5.m5.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS3.p1.5.m5.1c">\tau</annotation></semantics></math> to 5e-2 for ResNet50, 4e-3 for other models.</p>
</div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.04411" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.04412" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.04412">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.04412" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.04413" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:26:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
