<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2001.11091] The benefits of synthetic data for action categorization</title><meta property="og:description" content="In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The benefits of synthetic data for action categorization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The benefits of synthetic data for action categorization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2001.11091">

<!--Generated on Fri Mar  8 14:53:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The benefits of synthetic data for action categorization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamad Ballout, Mohammad Tuqan, Daniel Asmar, Elie Shammas, George Sakr
<br class="ltx_break">Department of Mechanical Engineering, VRL Lab
<br class="ltx_break">American University of Beirut, Beirut, Lebanon
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mab73@mail.aub.edu, m.m.tuqan@gmail.com, da20@aub.edu.lb, es34@aub.edu.lb, georges.sakr@usj.edu.lb</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified texture-less and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the drive towards a pervasive Internet-of-Things (IoT) society, machines will have to interact much more with humans, and to do so, they will have to understand human actions and activities. Action categorization is the process of classifying a trimmed video that contains a single action. For example, a four second video of a person biking should be classified as ‘biking’. On the other hand, the process of detecting the time interval of each action in a video and labeling them is called temporal action detection. In this paper, we are dealing with the problem of action categorization.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The state-of-the-art action categorization systems are mostly built on deep network architectures. The major challenge for these networks is the collection and annotation of a sufficient number of videos for training. The deeper the networks are, the more data is required. To give some perspective to the problem, training the 3D ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> on the UCF-101 dataset failed, despite the fact that UCF-101 contains more than thirteen thousand videos of 101 classes. In fact, to successfully train 3D ResNet, a much larger dataset (Kinetic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>) was required, which includes more than 300,000 videos. Manually annotating all these videos is a daunting task, and thus the need for a method that generates annotated videos in a simpler manner (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2001.11091/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="162" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic data can be used to train a network from scratch or augment a pre-trained network to improve its performance. </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One approach to overcome the requirement for large amounts of data is to do unsupervised action localization, which does not require any annotated videos, and aims to automatically group videos of a similar action into one class. Such systems rely on local features that can detect similar actions. Unfortunately, the results of these systems to date <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> are not comparable to supervised systems, with accuracy values as low as 60% on UCF-101.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another approach to mitigate the problem of video collection and annotation, is to rely on simulated data instead of videos of real recorded actions. Such approaches have been attempted on still images generated through graphics simulators. For example, synthetic data was used in object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>; it was also used in segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and also in the evaluation of optical flow solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Creating simulated realistic full scenes with complete background information is difficult, and not many people have attempted it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; however, what is interesting to note is that the difficult part is mostly in the creation of the background. In fact, if one could disregard the background completely, creating actions in simulation would be relatively simple to do.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Training deep network models using synthetic data has shown promising results in multiple computer vision applications, such as object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, optical flow estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Some of these contributions are in the methods used for generating synthetic data. For instance, Dwibedi <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p5.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> suggested an easy yet effective way to generate synthetic images using what they call ‘cut and paste’. Cropped pictures where placed inside any background picture to create realistic training images. The most important point in their contribution is that images are generated quickly, and their system outperforms the existing synthesis detection approaches by 21% when combined with real data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Another important aspect, referred to as cross-domain generalization, is the ability of synthetic data to generalize to real world data. It would not be useful to train a system on synthetic data, and then have the trained network produce worse detection and classification on real world data. To address this problem, Tobin <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p6.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> used domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for object detection; the concept is based on introducing random variations in the simulator in such a manner that the world, after randomization, appears different to the to AI system. In the simulator of Tobin <em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p6.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the parameters for lighting, pose, and textures were set in a random and non-realistic manner. Their idea was to provide enough variability when training in a way that the system would then be able to generalize to the real world during testing. By training only on synthetic data, their proposed network produced compelling results on a benchmark object detection dataset.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The method we are proposing is similar in spirit to that of De Souza <em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p7.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; however, three main differences exist. First, the network we use is an <span id="S1.p7.1.3" class="ltx_text ltx_font_italic">augmented</span> TSN, which includes an additional channel or two for synthetic data compared to their cool-TSN, which feeds the synthetic and the real data together using mini-batches. Second, we propose using a reduced form of synthetic data (hereafter referred to as <span id="S1.p7.1.4" class="ltx_text ltx_font_italic">simplified synthetic data</span>), which leads to a better accuracy than that of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Third, the synthetic dataset we created and used for training is considerably smaller than that of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> (8529 versus 39,982), and yet the improvement we achieve is higher than what they achieve (3.9% versus 1% on HMDB-51). Our experiments are more comprehensive, in which we test our augmented TSN on different inputs, sometimes using synthetic data alone, and others using a mixture of real and synthetic data.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To the best of our knowledge, along with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, only a few papers assess the effect of training a network on synthetic videos alone, and test it on a dataset such UCF-101. Our results revealed that the more synthetic data is used to train a network, the higher the accuracy becomes.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">In this paper, we propose to evaluate the advantage of using simplified synthetic data for training neural networks for action categorization. We test our approach on the Temporal Segment Network (TSN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which learns both spatial and temporal streams separately; the temporal stream is fed with optical flow fields, and is not affected by background. For instance, if the action to be modeled is ‘diving from a cliff’, whether the surrounding environment is simulated in the scene or not, the optical flow frames are not affected as long as the background is static. Thus, our videos generated using a physics engine, such as Unity, are background-free and no complex scene design was required.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">The contributions of this paper include the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">First, we prove the efficiency of using simplified synthetic data for action recognition, in which only optical flow data is considered. Domain randomization is applied by shaking the camera in a random fashion, as well as changing the lighting conditions in the recorded videos.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Second, augmenting the vanilla TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> by including as input an additional stream of synthetic data on top of the real videos. Our proposed augmentation outperforms the vanilla TSN by a significant margin.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Third, training the TSN with only the generated synthetic dataset resulted in 52.7% accuracy when tested on the sub-dataset of UCF-101.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Finally, we release a dataset of synthetic data with all the actions chosen from classes of benchmark datasets, including HMDB-51 and UCF-101. We name the datasets S-HMDB-38 and S-UCF-25, and make them publicly available for testing. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.dropbox.com/s/isz6vsw6fzibbwk/datasets.zip?dl=0</span></span></span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section presents the proposed methodology, including an analysis on the value of appearance data versus optical flow in action recognition, as well as the assessment of what part of synthetic data is most relevant for the sake of action categorization.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The Value of Appearance Data in Action Categorization</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In action recognition networks, it is common to include both spatial and temporal ConvNet streams. In TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, for example, the spatial stream takes RGB frames as input, and the temporal stream processes optical flow. Each stream is trained separately and then their scores are fused at the output layer. When tested on the UCF-101 dataset, after training on the spatial stream alone, TSN scored an accuracy of 84.5%, and when trained on the temporal stream alone, it scored an accuracy of 87.2%. Fusing both streams resulted in an improved accuracy of 92.0%. Looking closer at these results, the relatively moderate improvement attained when adding appearance data questioned its value in action categorization. This result motivated us to use <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">only</em> temporal information in our synthetic data; an idea that agrees with the results presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Moreover, using only temporal information introduces substantial simplifications to the synthesis of videos, as will be seen below.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Optical flow is the pattern of motion in the visual scene reflecting the relative motion between the object and the camera. For the sake of reproducing the action, a question arises regarding the background of the scene and its impact on the optical flow. Under ideal conditions, where the camera is fixed, and the lighting condition is consistent among all the videos within the dataset, the background does not have a significant contribution to the optical flow pattern. However, in the absence of ideal conditions, relative motion can be detected from the pixels in the background of the scene. This happens, for instance, when a camera is held by a person who is moving or walking. This random movement or ‘shake’ in a camera can distort the ideal conditions that are replicated in a gaming engine.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">One approach to tackle this problem is domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is used in object recognition. In domain randomization, an object in a scene is placed out of context in order to train the network to deal with the possible variability in the scene appearance when detecting that object. In our paper, we borrow the idea of randomization from the field object recognition, and apply it to actions by adding random changes in light intensity in the scene, and random camera movements. The variations are applied either within the vicinity of its original position to introduce the shaking effect into the camera, or by tracking the action of interest. Therefore, the synthetic videos we use constitute a combination of scenes under ideal and non-ideal conditions, either by introducing distortion in light intensity, camera position, or a combination of both.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Creating realistic backgrounds is one of the main challenges one faces when generating synthetic data. In an outdoor environment, this challenge is even more difficult, requiring graphics experts capable of reproducing computer models of natural objects, such as trees and rocks. It would seem that in the case where background information is critical, the required computer effort could be better spent collecting videos of real scenes. On the other hand, if one could completely disregard the background and create simple videos of foreground alone, synthetic data creation becomes considerably simpler (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2001.11091/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="367" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The simplified data were generated using Unity without setting up a 3D scene</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We investigate the significance of appearance on network performance for videos of human actions on simulated data we created, as well as on the benchmark datasets HMDB-51 and UCF-101. The synthetic actions were downloaded from Mixamo.com, or from the Unity asset store. To test for the effect of background, various backgrounds and scenes were downloaded from the Unity asset store. For the HMDB-51 classes, 38 were reconstructed; whereas for the 101 UCF, 25 classes were reconstructed. Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the number of videos and classes for actions corresponding to those found in the benchmark datasets. It is worth noting that we only synthesized videos for 38 out of the 51 HMDB classes, and 25 our of the 101 UCF classes, since other classes were not available to download.
<span id="S2.SS2.p2.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of videos created of objects corresponding to those found in benchmark datasets: background videos stands for the videos that were made using a 3D setup.</figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.3.1" class="ltx_tr">
<td id="S2.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></td>
<td id="S2.T1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.3.1.2.1" class="ltx_text"></span><span id="S2.T1.3.1.2.2" class="ltx_text" style="font-size:90%;"> </span><span id="S2.T1.3.1.2.3" class="ltx_text" style="font-size:90%;">
<span id="S2.T1.3.1.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.3.1.2.3.1.1" class="ltx_tr">
<span id="S2.T1.3.1.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Classes</span></span>
<span id="S2.T1.3.1.2.3.1.2" class="ltx_tr">
<span id="S2.T1.3.1.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(#)</span></span>
</span></span><span id="S2.T1.3.1.2.4" class="ltx_text"></span><span id="S2.T1.3.1.2.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="S2.T1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.3.1.3.1" class="ltx_text"></span><span id="S2.T1.3.1.3.2" class="ltx_text" style="font-size:90%;"> </span><span id="S2.T1.3.1.3.3" class="ltx_text" style="font-size:90%;">
<span id="S2.T1.3.1.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.3.1.3.3.1.1" class="ltx_tr">
<span id="S2.T1.3.1.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Background</span></span>
<span id="S2.T1.3.1.3.3.1.2" class="ltx_tr">
<span id="S2.T1.3.1.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Videos (#)</span></span>
</span></span><span id="S2.T1.3.1.3.4" class="ltx_text"></span><span id="S2.T1.3.1.3.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="S2.T1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.3.1.4.1" class="ltx_text"></span><span id="S2.T1.3.1.4.2" class="ltx_text" style="font-size:90%;"> </span><span id="S2.T1.3.1.4.3" class="ltx_text" style="font-size:90%;">
<span id="S2.T1.3.1.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.3.1.4.3.1.1" class="ltx_tr">
<span id="S2.T1.3.1.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Background-less</span></span>
<span id="S2.T1.3.1.4.3.1.2" class="ltx_tr">
<span id="S2.T1.3.1.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Videos (#)</span></span>
</span></span><span id="S2.T1.3.1.4.4" class="ltx_text"></span><span id="S2.T1.3.1.4.5" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="S2.T1.3.2" class="ltx_tr">
<td id="S2.T1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.2.1.1" class="ltx_text" style="font-size:90%;">HMDB-51</span></td>
<td id="S2.T1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.2.1" class="ltx_text" style="font-size:90%;">38</span></td>
<td id="S2.T1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.3.1" class="ltx_text" style="font-size:90%;">3817</span></td>
<td id="S2.T1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.3.2.4.1" class="ltx_text" style="font-size:90%;">8528</span></td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.3.3.1.1" class="ltx_text" style="font-size:90%;">UCF-101</span></td>
<td id="S2.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.3.2.1" class="ltx_text" style="font-size:90%;">25</span></td>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.1" class="ltx_text" style="font-size:90%;">–</span></td>
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.3.3.4.1" class="ltx_text" style="font-size:90%;">5514</span></td>
</tr>
</table>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In each dataset, different characters are used to do the same action; and for each action, there are multiple animations that differ from each other in the way of doing the action. For example, if the action is ‘riding a bike’, there are several ways of riding it: it could be ridden at a quick or slow speed, it could be ridden standing up or sitting down (see Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Also, for further generalization, some of the actions were done under different lighting conditions such as under dark, shadow, or bright lighting conditions.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x3.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x4.png" id="S2.F3.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x5.png" id="S2.F3.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x6.png" id="S2.F3.4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x7.png" id="S2.F3.5.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x8.png" id="S2.F3.6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x9.png" id="S2.F3.7.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2001.11091/assets/x10.png" id="S2.F3.8.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="88" height="66" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of videos created with background (left column) and simplified videos (right column</figcaption>
</figure>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">When removing appearance and relying on optical flow alone, none of these nuances are relevant any longer. Optical flow is not affected by the lighting, nor by the color and texture of the garment of a character. This fact allowed us to use the same character for most of the videos where appearance is disregarded.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">As a result, in this paper we suggest using only the optical flow of synthetic data (which is simple to obtain), and disregard the background scene information (which can be difficult to obtain). For example, in producing the ‘climb stair’ action, it is enough to only reproduce the action of climbing the stairs without the need to place the stairs in the scene. For the ‘pitching’ action, it is sufficient to simulate the act of pitching a ball without having to simulate a ball in the scene, as its contribution to the optical flow is negligible. In other words, the only factor contributing in the synthesis of the scene is the action of the character itself.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Each action includes between 200-250 videos, requiring between 60-75 minutes in total to generate. Similar to real videos, the generated videos are between two to six seconds. Camera acquisition is set to thirty frames per second in all of the generated videos. The aspect ratios of the videos are close to those of the UCF-101 videos (320x240 pixels). To complete the dataset, each action is reproduced from different camera viewpoints and environment conditions (light source intensity and camera position). Note however, that not all the actions are one-person synthetic. Some actions such as ‘salsa spin’ or ‘boxing’, require including the second person in order to correctly interpret the overall meaning of the action (two-people synthetic).</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2001.11091/assets/x11.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="297" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Networks tested, Augmented TSN with 4 and 3 streams, and Optical Flow TSN</figcaption>
</figure>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">The results of this comparison were quite surprising: training using optical flow alone on both the real and synthetic datasets produced classifications results superior to those in which appearance data was also included. As a result, it was decided that in simulations, we would only vary the camera viewpoint while performing the various actions. Also, purposely shaking the camera in simulations resulted in conditions that are close to what is experienced in the real world.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Several experiments were performed to assess the effectiveness of using synthetic data in training of action recognition networks. In what follows, we first present the datasets we used for the experiments, then discuss the proposed networks, and finally discuss the results of our experiments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Five different datasets were used for the proposed experiments, two of which comprise videos of real actions, including UCF-101 and HMDB-51, and three that are synthetic, which we simulated:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">UCF-101: a dataset of human actions from videos in the wild, containing 13320 videos distributed in 101 action categories</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">HMDB-51: a video database for human actions, mostly extracted from movies and web videos. It consists of 6766 videos from 51 different action categories.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Synthetic-appearance (HMDB-38): approximately 4000 synthetic videos were created including background, chosen from 38 classes of real HMDB-51. Examples are shown in the left column of Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Synthetic-simplified (HMDB-38): approximately 8000 synthetic videos were created using no-background, chosen from 38 classes of real HMDB-51. Examples of the videos are shown in the right column of Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Synthetic-simplified (UCF-25): approximately 5000 synthetic videos were created using no-background, from 25 classes of the real UCF-101.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Networks tested</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To investigate the potential of using simplified synthetic data for training, we tested four different networks (Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), including TSN, Augmented TSN (Network-1 and Network-2), and one-flow TSN (Network-3).</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">TSN</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Temporal Segment Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> are an upgraded version of the two stream convolutional network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. TSN uses a deeper network than the two stream network, and it benefits from a 2D-CNN model trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The 2D-CNN could be Inception, 2D-ResNet, or Batch-Normalized-Inception. Another advantage of TSN versus a classic two stream convolution network, is its ability to model long-range temporal structures by extracting sparse snippets of the video, instead of using consecutive frames that are highly redundant.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Augmented-TSN (Network-1).</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">This network is based on TSN, with two additional streams. The first addition is the spatial stream that takes as input synthesized + real appearance data. The second additional stream is the temporal one, which takes as input the optical flow of both the synthetic and the real datasets. Fusing the score was done by giving the real+synthetic flow a weight of 2.0, real flow 1.0, real RGB 1.0, and synthetic+real RGB 0.5.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">S-TSN (Network-2).</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Synthetic-Simplified TSN is based also based on the TSN model, with an addition stream that trains extracted optical flow for both real and simplified synthetic data. Fusing the score was done by giving all of the streams the same weights.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">One-flow-TSN (Network-3).</h5>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">This model uses only the temporal stream of TSN. It takes as an input the extracted optical flow of the Synthetic-UCF-25 dataset and it is tested on real videos from UCF-25. The intent of this experiment is to prove that synthetic data alone is sufficient to train a network that is later tested on real data.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Analysis of the Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">All of the network parameters were tuned to those of the original TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, except for the drop-out ratio, which gave a better accuracy when set to 0.8 for the temporal stream. The network weights were initialized with pre-trained models from ImageNet using BN-Inception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> as the ConvNet architecture. Also, mini-batch stochastic gradient descent was used with a batch-size set to 128 with a momentum of 0.9. The number of iterations was adopted from the TSN for both spatial and temporal streams. The optical flow frames were extracted using the code provided in the TSN framework. Finally, we followed the traditional evaluation on the three training/testing splits for all of the experiments.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>The Effect of Background Removal</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">In this experiment we tested the effect of using quality versus quantity videos. For Network-1, we added 4000 synthetic videos with an appropriate background setup. For Network-2, we added 4000 simplified videos and in another instance, we added 8000 simplified videos. What we observed ( Table <a href="#S3.T2" title="Table 2 ‣ 3.3.1 The Effect of Background Removal ‣ 3.3 Analysis of the Results ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) was that adding 8000 videos to Network-2 produced slightly better results than Network-1. Even though Network-1 includes an additional stream compared to Network-2, the latter preformed better. We concluded from this experiment that a large number of simplified videos can outperform videos with background included.
<span id="S3.SS3.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Network 2 versus Network 1: note that domain randomization is effective in simulating real outdoor data</figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.3.1" class="ltx_tr">
<td id="S3.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td id="S3.T2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.3.1.2.1" class="ltx_text" style="font-size:90%;">HMDB-51</span></td>
</tr>
<tr id="S3.T2.3.2" class="ltx_tr">
<td id="S3.T2.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.3.2.1.1" class="ltx_text" style="font-size:90%;">Network-1 with 4000</span></td>
<td id="S3.T2.3.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T2.3.3.1.1" class="ltx_text" style="font-size:90%;">background synthetic videos</span></td>
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.3.3.2.1" class="ltx_text" style="font-size:90%;">72.3 %</span></td>
</tr>
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.3.4.1.1" class="ltx_text" style="font-size:90%;">Network-2 with 4000</span></td>
<td id="S3.T2.3.4.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.3.5" class="ltx_tr">
<td id="S3.T2.3.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T2.3.5.1.1" class="ltx_text" style="font-size:90%;">simplified synthetic videos</span></td>
<td id="S3.T2.3.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.3.5.2.1" class="ltx_text" style="font-size:90%;">71.8 %</span></td>
</tr>
<tr id="S3.T2.3.6" class="ltx_tr">
<td id="S3.T2.3.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.3.6.1.1" class="ltx_text" style="font-size:90%;">Network-2 with 8000</span></td>
<td id="S3.T2.3.6.2" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T2.3.7" class="ltx_tr">
<td id="S3.T2.3.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T2.3.7.1.1" class="ltx_text" style="font-size:90%;">simplified synthetic videos</span></td>
<td id="S3.T2.3.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.3.7.2.1" class="ltx_text" style="font-size:90%;">72.4 %</span></td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Effect of Adding Synthetic Data to Training</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">This experiment was divided into two parts: the first one performed on a sub-dataset of HMDB-51 and UCF-101. As mentioned in Section <a href="#S3.SS1" title="3.1 Datasets ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we reconstructed (synthetically) 38 classes of the HMDB-51 and 25 classes of the UCF-101. Thus, the training and the testing in this part is performed on 38 classes of HMDB-51 and 25 classes UCF-101. The purpose of this experiment is to study the effect of adding synthetic data to
all of the dataset classes. First, we reproduced the TSN results for those sub-datasets using the original real videos. Next, we added half of the generated synthetic videos (2500 for UCF-25, and 4000 for HMDB-38) to a third stream using Network-2. Finally, we added all of the generated videos (5000 for UCF-25 and 8000 for HMDB-38) to the third stream in Network-2. Table <a href="#S3.T3" title="Table 3 ‣ 3.3.2 Effect of Adding Synthetic Data to Training ‣ 3.3 Analysis of the Results ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> clearly shows that adding simplified synthetic videos improved the performance of TSN. It also shows that the more synthetic videos are added the better the network performs.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">The second part of the experiment is done on the entire dataset, even though some of the classes are not augmented with synthetic data. In this part, we compared the performance of Network-2 versus state-of-the-art methods, on both the HMDB-51 and UCF-101.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>The effect of combining real and simplified synthetic videos on sub-datasets. </figcaption>
<table id="S3.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.3.1" class="ltx_tr">
<td id="S3.T3.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></td>
<td id="S3.T3.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.2.1" class="ltx_text" style="font-size:90%;">HMDB-38</span></td>
<td id="S3.T3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.3.1" class="ltx_text" style="font-size:90%;">UCF-25</span></td>
</tr>
<tr id="S3.T3.3.2" class="ltx_tr">
<td id="S3.T3.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.3.2.1.1" class="ltx_text" style="font-size:90%;">Real only</span></td>
<td id="S3.T3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.2.2.1" class="ltx_text" style="font-size:90%;">71.8</span></td>
<td id="S3.T3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.2.3.1" class="ltx_text" style="font-size:90%;">96.66</span></td>
</tr>
<tr id="S3.T3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.3.3.1.1" class="ltx_text" style="font-size:90%;">Half Synthetic</span></td>
<td id="S3.T3.3.3.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T3.3.4" class="ltx_tr">
<td id="S3.T3.3.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T3.3.4.1.1" class="ltx_text" style="font-size:90%;">Videos+Real</span></td>
<td id="S3.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.3.4.2.1" class="ltx_text" style="font-size:90%;">73.66</span></td>
<td id="S3.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T3.3.4.3.1" class="ltx_text" style="font-size:90%;">97.5</span></td>
</tr>
<tr id="S3.T3.3.5" class="ltx_tr">
<td id="S3.T3.3.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.3.5.1.1" class="ltx_text" style="font-size:90%;">All Synthetic</span></td>
<td id="S3.T3.3.5.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T3.3.5.3" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T3.3.6" class="ltx_tr">
<td id="S3.T3.3.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T3.3.6.1.1" class="ltx_text" style="font-size:90%;">Videos+Real</span></td>
<td id="S3.T3.3.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T3.3.6.2.1" class="ltx_text" style="font-size:90%;">74.62</span></td>
<td id="S3.T3.3.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T3.3.6.3.1" class="ltx_text" style="font-size:90%;">97.8</span></td>
</tr>
</table>
</figure>
<section id="S3.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">HMDB-51 Dataset.</h5>

<div id="S3.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS2.Px1.p1.1" class="ltx_p">First, we compare our Network-2 to Cool-TSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Cool-TSN includes 39,982 appearance videos for 35 action classes, where 21 of them are common with HMDB-51. In our Network-2, we generate only 8000 videos, with actions similar to the 38 classes of HMDB-51. In the third stream, the synthetic videos were added to the real videos in the 38 classes, while the remaining 13 classes had only real videos. Results show that Network-2 outperformed Cool-TSN by 2.9 %. As can be seen in Table <a href="#S3.T4" title="Table 4 ‣ HMDB-51 Dataset. ‣ 3.3.2 Effect of Adding Synthetic Data to Training ‣ 3.3 Analysis of the Results ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the Network-2 also outperformed all of the state of the art systems, except I3D and OFF. However, we must note that I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> had to bfe pre-trained on 300,000 videos from the Kinetics dataset.
<span id="S3.SS3.SSS2.Px1.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Benchmarking Network-2 versus state-of-the-art networks</figcaption>
<table id="S3.T4.1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1.2" class="ltx_tr">
<td id="S3.T4.1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T4.1.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td id="S3.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.2.2.1" class="ltx_text"></span><span id="S3.T4.1.1.2.2.2" class="ltx_text" style="font-size:90%;"> </span><span id="S3.T4.1.1.2.2.3" class="ltx_text" style="font-size:90%;">
<span id="S3.T4.1.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.1.1.2.2.3.1.1" class="ltx_tr">
<span id="S3.T4.1.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">UCF-101</span></span>
<span id="S3.T4.1.1.2.2.3.1.2" class="ltx_tr">
<span id="S3.T4.1.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(%)</span></span>
</span></span><span id="S3.T4.1.1.2.2.4" class="ltx_text"></span><span id="S3.T4.1.1.2.2.5" class="ltx_text" style="font-size:90%;"></span>
</td>
<td id="S3.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.2.3.1" class="ltx_text"></span><span id="S3.T4.1.1.2.3.2" class="ltx_text" style="font-size:90%;"> </span><span id="S3.T4.1.1.2.3.3" class="ltx_text" style="font-size:90%;">
<span id="S3.T4.1.1.2.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T4.1.1.2.3.3.1.1" class="ltx_tr">
<span id="S3.T4.1.1.2.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HMDB-51</span></span>
<span id="S3.T4.1.1.2.3.3.1.2" class="ltx_tr">
<span id="S3.T4.1.1.2.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(%)</span></span>
</span></span><span id="S3.T4.1.1.2.3.4" class="ltx_text"></span><span id="S3.T4.1.1.2.3.5" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="S3.T4.1.1.3" class="ltx_tr">
<td id="S3.T4.1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.3.1.1" class="ltx_text" style="font-size:90%;">iDT+FV </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.T4.1.1.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.3.2.1" class="ltx_text" style="font-size:90%;">84.8</span></td>
<td id="S3.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.3.3.1" class="ltx_text" style="font-size:90%;">57.2</span></td>
</tr>
<tr id="S3.T4.1.1.4" class="ltx_tr">
<td id="S3.T4.1.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.4.1.1" class="ltx_text" style="font-size:90%;">Tow-stream </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.T4.1.1.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.2.1" class="ltx_text" style="font-size:90%;">88.0</span></td>
<td id="S3.T4.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.3.1" class="ltx_text" style="font-size:90%;">59.4</span></td>
</tr>
<tr id="S3.T4.1.1.5" class="ltx_tr">
<td id="S3.T4.1.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.5.1.1" class="ltx_text" style="font-size:90%;">Two-stream LSTM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.T4.1.1.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.5.2.1" class="ltx_text" style="font-size:90%;">88.6</span></td>
<td id="S3.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.5.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">L</span><sup id="S3.T4.1.1.1.1.2" class="ltx_sup"><span id="S3.T4.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">2</span></sup><span id="S3.T4.1.1.1.1.3" class="ltx_text" style="font-size:90%;">STM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S3.T4.1.1.1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.1.2.1" class="ltx_text" style="font-size:90%;">93.6</span></td>
<td id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.1.3.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
</tr>
<tr id="S3.T4.1.1.6" class="ltx_tr">
<td id="S3.T4.1.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.6.1.1" class="ltx_text" style="font-size:90%;">TSN-2M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.T4.1.1.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.6.2.1" class="ltx_text" style="font-size:90%;">94.0</span></td>
<td id="S3.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.6.3.1" class="ltx_text" style="font-size:90%;">68.5</span></td>
</tr>
<tr id="S3.T4.1.1.7" class="ltx_tr">
<td id="S3.T4.1.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.7.1.1" class="ltx_text" style="font-size:90%;">TSN-3M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.T4.1.1.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.7.2.1" class="ltx_text" style="font-size:90%;">94.2</span></td>
<td id="S3.T4.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.7.3.1" class="ltx_text" style="font-size:90%;">69.5</span></td>
</tr>
<tr id="S3.T4.1.1.8" class="ltx_tr">
<td id="S3.T4.1.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.8.1.1" class="ltx_text" style="font-size:90%;">Cool-TSN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S3.T4.1.1.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.8.2.1" class="ltx_text" style="font-size:90%;">94.2</span></td>
<td id="S3.T4.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.8.3.1" class="ltx_text" style="font-size:90%;">69.5</span></td>
</tr>
<tr id="S3.T4.1.1.9" class="ltx_tr">
<td id="S3.T4.1.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.9.1.1" class="ltx_text" style="font-size:90%;">3D-ResNet-101 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S3.T4.1.1.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.9.2.1" class="ltx_text" style="font-size:90%;">94.5</span></td>
<td id="S3.T4.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.9.3.1" class="ltx_text" style="font-size:90%;">70.2</span></td>
</tr>
<tr id="S3.T4.1.1.10" class="ltx_tr">
<td id="S3.T4.1.1.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.10.1.1" class="ltx_text" style="font-size:90%;">Two-stream MiCT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S3.T4.1.1.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.10.2.1" class="ltx_text" style="font-size:90%;">94.7</span></td>
<td id="S3.T4.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.10.3.1" class="ltx_text" style="font-size:90%;">70.5</span></td>
</tr>
<tr id="S3.T4.1.1.11" class="ltx_tr">
<td id="S3.T4.1.1.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.11.1.1" class="ltx_text" style="font-size:90%;">CoViar </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S3.T4.1.1.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.11.2.1" class="ltx_text" style="font-size:90%;">94.9</span></td>
<td id="S3.T4.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.11.3.1" class="ltx_text" style="font-size:90%;">70.2</span></td>
</tr>
<tr id="S3.T4.1.1.12" class="ltx_tr">
<td id="S3.T4.1.1.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.12.1.1" class="ltx_text" style="font-size:90%;">OFF </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S3.T4.1.1.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.12.2.1" class="ltx_text" style="font-size:90%;">96.0</span></td>
<td id="S3.T4.1.1.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.12.3.1" class="ltx_text" style="font-size:90%;">74.2</span></td>
</tr>
<tr id="S3.T4.1.1.13" class="ltx_tr">
<td id="S3.T4.1.1.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T4.1.1.13.1.1" class="ltx_text" style="font-size:90%;">Two-stream I3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T4.1.1.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.T4.1.1.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S3.T4.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.13.2.1" class="ltx_text" style="font-size:90%;">98.0</span></td>
<td id="S3.T4.1.1.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.13.3.1" class="ltx_text" style="font-size:90%;">80.7</span></td>
</tr>
<tr id="S3.T4.1.1.14" class="ltx_tr">
<td id="S3.T4.1.1.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T4.1.1.14.1.1" class="ltx_text" style="font-size:90%;">Network-2</span></td>
<td id="S3.T4.1.1.14.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T4.1.1.14.3" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T4.1.1.15" class="ltx_tr">
<td id="S3.T4.1.1.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T4.1.1.15.1.1" class="ltx_text" style="font-size:90%;">half of the generated</span></td>
<td id="S3.T4.1.1.15.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.1.1.15.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T4.1.1.16" class="ltx_tr">
<td id="S3.T4.1.1.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T4.1.1.16.1.1" class="ltx_text" style="font-size:90%;">videos (Ours)</span></td>
<td id="S3.T4.1.1.16.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.1.16.2.1" class="ltx_text" style="font-size:90%;">94.4</span></td>
<td id="S3.T4.1.1.16.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.1.1.16.3.1" class="ltx_text" style="font-size:90%;">71.8</span></td>
</tr>
<tr id="S3.T4.1.1.17" class="ltx_tr">
<td id="S3.T4.1.1.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T4.1.1.17.1.1" class="ltx_text" style="font-size:90%;">Network-2</span></td>
<td id="S3.T4.1.1.17.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T4.1.1.17.3" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S3.T4.1.1.18" class="ltx_tr">
<td id="S3.T4.1.1.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T4.1.1.18.1.1" class="ltx_text" style="font-size:90%;">all of the generated</span></td>
<td id="S3.T4.1.1.18.2" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.1.1.18.3" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S3.T4.1.1.19" class="ltx_tr">
<td id="S3.T4.1.1.19.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T4.1.1.19.1.1" class="ltx_text" style="font-size:90%;">videos(Ours)</span></td>
<td id="S3.T4.1.1.19.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T4.1.1.19.2.1" class="ltx_text" style="font-size:90%;">94.6</span></td>
<td id="S3.T4.1.1.19.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T4.1.1.19.3.1" class="ltx_text" style="font-size:90%;">72.4</span></td>
</tr>
</table>
</figure>
<div id="S3.SS3.SSS2.Px1.p2" class="ltx_para">
<p id="S3.SS3.SSS2.Px1.p2.1" class="ltx_p">On the other hand, although OFF uses 5 streams while Network-2 uses 3 streams, it outperforms our proposed system by only 1.8 %.</p>
</div>
</section>
<section id="S3.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">UCF-101 dataset.</h5>

<div id="S3.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.Px2.p1.1" class="ltx_p">Similar to HMDB-51, we created a sub-UCF-101 synthetic, including approximately 5000 videos, representing 25 classes of the actions. Using the same procedure as that performed on the HMDB-51, we added the 2500 videos first and then added 5000 videos to 25 classes of UCF-101. Since we reconstructed only 25% (25 out of 101) of the UCF-101 classes compared to 75% ( 38 of 51) of the HMDB-51, here the improvement was lower. Network-2 improved 0.6% above the vanilla TSN when 5000 videos were added to get a final score of 94.6 %.</p>
</div>
</section>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>The Effect of Decreasing the Number of Real Videos</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">In this section, we investigate the ability of simplified synthetic videos to replace real videos. Three tests were performed: (1) reducing the amount of videos by half, (2) keeping only 10 % of the real videos, and (3) having no real videos at all in the training sets.</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">In the first experiment, half of the real videos were randomly removed from the training splits, and the performance of the network was evaluated with and without synthetic data. In the second experiment, only 10 % of the real data was kept and the same evaluation was repeated. Finally, in the third experiment, training was performed on only synthetic data, and testing on real data. The first two experiments were done using Network-2 while the last experiment wass done using Network-3. Network-3 in Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.2 Synthetic Data ‣ 2 Methodology ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> includes only the optical flow stream. Since there were some actions from the real datasets that could not be synthesized, during testing we limited the actions from UCF-101 to those that were produced synthetically. For UCF-101, 25 out of 101 classes were used.
Table <a href="#S3.T5" title="Table 5 ‣ 3.3.3 The Effect of Decreasing the Number of Real Videos ‣ 3.3 Analysis of the Results ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the accuracy of the network did not drop by much when half of the real videos were removed. However, when only 10% of the videos were kept, the accuracy dropped to 77.14 % with real videos, while it stayed relatively high 85.41% with the additional 5000 synthetic videos.
<span id="S3.SS3.SSS3.p2.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>The effect of decreasing the number of real videos on the accuracy of UCF-25 while adding 2500 and 5000 synthetic videos </figcaption>
<table id="S3.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T5.3.1" class="ltx_tr">
<td id="S3.T5.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">UCF-25</span></td>
<td id="S3.T5.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.2.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S3.T5.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.3.1" class="ltx_text" style="font-size:90%;">Real + 2500</span></td>
<td id="S3.T5.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.4.1" class="ltx_text" style="font-size:90%;">Real + 5000</span></td>
</tr>
<tr id="S3.T5.3.2" class="ltx_tr">
<td id="S3.T5.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.2.1.1" class="ltx_text" style="font-size:90%;">100% Real</span></td>
<td id="S3.T5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.2.2.1" class="ltx_text" style="font-size:90%;">96.66</span></td>
<td id="S3.T5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.2.3.1" class="ltx_text" style="font-size:90%;">97.5</span></td>
<td id="S3.T5.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.2.4.1" class="ltx_text" style="font-size:90%;">97.8</span></td>
</tr>
<tr id="S3.T5.3.3" class="ltx_tr">
<td id="S3.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.3.1.1" class="ltx_text" style="font-size:90%;">50%Real</span></td>
<td id="S3.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.3.2.1" class="ltx_text" style="font-size:90%;">96.34</span></td>
<td id="S3.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.3.3.1" class="ltx_text" style="font-size:90%;">97.02</span></td>
<td id="S3.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.3.4.1" class="ltx_text" style="font-size:90%;">97.7</span></td>
</tr>
<tr id="S3.T5.3.4" class="ltx_tr">
<td id="S3.T5.3.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.4.1.1" class="ltx_text" style="font-size:90%;">10%Real</span></td>
<td id="S3.T5.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.4.2.1" class="ltx_text" style="font-size:90%;">77.4</span></td>
<td id="S3.T5.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.4.3.1" class="ltx_text" style="font-size:90%;">81.69</span></td>
<td id="S3.T5.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.4.4.1" class="ltx_text" style="font-size:90%;">85.41</span></td>
</tr>
<tr id="S3.T5.3.5" class="ltx_tr">
<td id="S3.T5.3.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.3.5.1.1" class="ltx_text" style="font-size:90%;">0% Real</span></td>
<td id="S3.T5.3.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.3.5.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S3.T5.3.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.3.5.3.1" class="ltx_text" style="font-size:90%;">30.71</span></td>
<td id="S3.T5.3.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.3.5.4.1" class="ltx_text" style="font-size:90%;">52.7</span></td>
</tr>
</table>
</figure>
<div id="S3.SS3.SSS3.p3" class="ltx_para">
<p id="S3.SS3.SSS3.p3.1" class="ltx_p">Finally, when training with 5000 synthetic videos, the accuracy dropped to 52.7% while training on around 2500 synthetic videos scored 30.71%. These results are comparable to those by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, who got 52.1% when trained only on their generated synthetic data. The results we put forward demonstrate the potential of training action recognition networks using simplified videos, especially that some of the trained classes, such as ‘CleanAndJerk’ achieved an accuracy above 90%, as shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3.3 The Effect of Decreasing the Number of Real Videos ‣ 3.3 Analysis of the Results ‣ 3 Experiments and Results ‣ The benefits of synthetic data for action categorization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. On the other hand, other classes (such as ‘BaseBallPitch’ and ‘Skateboarding’) scored as low as 5%, suggesting the inability of the simulator in accurately re-creating those classes.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2001.11091/assets/x12.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The accuracy for individual classes using synthetic data alone</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we analyzed the effectiveness of simplified synthetic data in the training of deep networks for the sake of action categorization. We validated that optical flow information was sufficient to train a network, and that appearance information could be disregarded. The caveat here is that the proposed actions do not require background interaction to differentiate between two different actions (<em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p1.1.2" class="ltx_text"></span>, swimming vs flying). We also tested using synthesized data for training under two different scenarios: the first using synthesized data to augment TSN with an additional stream, and the second using only synthesized data to train the network from scratch. Both scenarios revealed good results, where in all cases we obtained notable improvements for TSN on both UCF-101 and HMDB-51.
We are currently working on creating a large synthetic dataset that includes over 200 k videos in order to compare its effectiveness to that of a dataset of real actions of comparable size, such as Kinetics.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns retrace the
history of 2d cnns and imagenet,” in </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 18–22.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
F. Viola, T. Green, T. Back, P. Natsev </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, “The kinetics human
action video dataset,” </span><em id="bib.bib2.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1705.06950</em><span id="bib.bib2.5.5" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S. Jones and L. Shao, “Unsupervised spectral dual assignment clustering of
human actions in context,” in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, 2014, pp. 604–611.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
K. Soomro and M. Shah, “Unsupervised action discovery and localization in
videos,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 696–705.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To,
E. Cameracci, S. Boochoon, and S. Birchfield, “Training deep networks with
synthetic data: Bridging the reality gap by domain randomization,”
</span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.06516</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
D. Dwibedi, I. Misra, and M. Hebert, “Cut, paste and learn: Surprisingly easy
synthesis for instance detection,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE international
conference on computer vision (ICCV)</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes,” in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer
vision and pattern recognition</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 3234–3243.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van
Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning optical flow with
convolutional networks,” in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International
Conference on Computer Vision</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 2758–2766.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
C. R. De Souza, A. Gaidon, Y. Cabon, and A. M. L. Peña, “Procedural
generation of videos to train deep action recognition networks.” in
</span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">,
2017, pp. 2594–2604.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
X. Peng, B. Sun, K. Ali, and K. Saenko, “Learning deep object detectors from
3d models,” in </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on
Computer Vision</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 1278–1286.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, “Galileo:
Perceiving physical object properties by integrating a physics engine with
deep learning,” in </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">,
2015, pp. 127–135.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa,
“Learning from synthetic data: Addressing domain shift for semantic
segmentation,” in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla,
“Understanding real world indoor scenes with synthetic data,” in
</span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 4077–4085.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic open
source movie for optical flow evaluation,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on
Computer Vision</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, 2012, pp. 611–625.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
H. Rahmani and A. Mian, “3d action recognition from novel viewpoints,” in
</span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 1506–1515.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
——, “Learning a non-linear knowledge transfer model for cross-view action
recognition,” in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 2458–2466.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
C. Vondrick, H. Pirsiavash, and A. Torralba, “Generating videos with scene
dynamics,” in </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances In Neural Information Processing Systems</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">,
2016, pp. 613–621.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
H. Su, C. R. Qi, Y. Li, and L. J. Guibas, “Render for cnn: Viewpoint
estimation in images using cnns trained with rendered 3d model views,” in
</span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer Vision</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">,
2015, pp. 2686–2694.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
G. Rogez and C. Schmid, “Mocap-guided data augmentation for 3d pose estimation
in the wild,” in </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">,
2016, pp. 3108–3116.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ
International Conference on</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">.   IEEE,
2017, pp. 23–30.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool,
“Temporal segment networks: Towards good practices for deep action
recognition,” in </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">.   Springer, 2016, pp. 20–36.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards
understanding action recognition,” in </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
international conference on computer vision</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, 2013, pp. 3192–3199.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action
recognition in videos,” in </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing
systems</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, 2014, pp. 568–576.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">.   Ieee, 2009, pp. 248–255.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:1502.03167</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model and
the kinetics dataset,” in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition
(CVPR), 2017 IEEE Conference on</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2017, pp. 4724–4733.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
H. Wang and C. Schmid, “Action recognition with improved trajectories,” in
</span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">,
2013, pp. 3551–3558.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
J. Yue-Hei Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and
G. Toderici, “Beyond short snippets: Deep networks for video
classification,” in </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer
vision and pattern recognition</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 4694–4702.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
L. Sun, K. Jia, K. Chen, D.-Y. Yeung, B. E. Shi, and S. Savarese, “Lattice
long short-term memory for human action recognition.” in </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, 2017,
pp. 2166–2175.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhou, X. Sun, Z.-J. Zha, and W. Zeng, “Mict: Mixed 3d/2d convolutional tube
for human action recognition,” in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 449–458.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
C.-Y. Wu, M. Zaheer, H. Hu, R. Manmatha, A. J. Smola, and
P. Krähenbühl, “Compressed video action recognition,” in
</span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 6026–6035.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
S. Sun, Z. Kuang, L. Sheng, W. Ouyang, and W. Zhang, “Optical flow guided
feature: a fast and robust motion representation for video action
recognition,” in </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2001.11090" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2001.11091" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2001.11091">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2001.11091" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2001.11092" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 14:53:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
