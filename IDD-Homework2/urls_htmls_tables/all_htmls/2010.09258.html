<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2010.09258] From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security</title><meta property="og:description" content="[Summary]Federated learning is an improved version of distributed machine learning that further offloads operations which would usually be performed by a central server. The server becomes more like an assistant coordi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2010.09258">

<!--Generated on Sat Mar  9 04:58:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\authormark</span>
<p id="p1.2" class="ltx_p">Sheng Shen <span id="p1.2.1" class="ltx_text ltx_font_smallcaps">et al</span></p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\corres</span>
<p id="p2.2" class="ltx_p">*Tianqing Zhu,</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\presentaddress</span>
<p id="p3.2" class="ltx_p">15 Broadway, Ultimo, NSW 2007, Australia</p>
</div>
<h1 class="ltx_title ltx_title_document">From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Shen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianqing Zhu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Di Wu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wanlei Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address"><span id="id1.1.id1" class="ltx_ERROR undefined">\orgdiv</span>University of Technology Sydney, <span id="id2.2.id2" class="ltx_ERROR undefined">\orgname</span>Center for Cyber Security and Privacy, School of Computer Science, <span id="id3.3.id3" class="ltx_ERROR undefined">\orgaddress</span><span id="id4.4.id4" class="ltx_ERROR undefined">\state</span>NSW, <span id="id5.5.id5" class="ltx_ERROR undefined">\country</span>Australia
</span>
<span class="ltx_contact ltx_role_address"><span id="id6.6.id1" class="ltx_ERROR undefined">\orgdiv</span>University of Technology Sydney, <span id="id7.7.id2" class="ltx_ERROR undefined">\orgname</span>School of Computer Science, <span id="id8.8.id3" class="ltx_ERROR undefined">\orgaddress</span><span id="id9.9.id4" class="ltx_ERROR undefined">\state</span>NSW, <span id="id10.10.id5" class="ltx_ERROR undefined">\country</span>Australia
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:Tianqing.zhu@uts.edu.au">Tianqing.zhu@uts.edu.au</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">[Summary]Federated learning is an improved version of distributed machine learning that further offloads operations which would usually be performed by a central server. The server becomes more like an assistant coordinating clients to work together rather than micro-managing the workforce as in traditional DML. One of the greatest advantages of federated learning is the additional privacy and security guarantees it affords. Federated learning architecture relies on smart devices, such as smartphones and IoT sensors, that collect and process their own data, so sensitive information never has to leave the client device. Rather, clients train a sub-model locally and send an encrypted update to the central server for aggregation into the global model. These strong privacy guarantees make federated learning an attractive choice in a world where data breaches and information theft are common and serious threats. This survey outlines the landscape and latest developments in data privacy and security for federated learning. We identify the different mechanisms used to provide privacy and security, such as differential privacy, secure multi-party computation and secure aggregation. We also survey the current attack models, identifying the areas of vulnerability and the strategies adversaries use to penetrate federated systems. The survey concludes with a discussion on the open challenges and potential directions of future work in this increasingly popular learning paradigm.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>federated learning, data privacy, security, distributed machine learning
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_articletype"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">articletype: </span>Article Type</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Since Google <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">konevcny2016federated</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">konevcny2017federated</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">mcmahan2016communication</span></sup></cite> first proposed the concept, federated learning has become an intriguing topic in privacy-preserving machine learning. Sometimes called collaborative learning, federated learning mobilizes multiple clients, such as computers, processing devices or smart sensors, coordinated by a central server to collaboratively train a machine learning model. Google’s idea was to distribute training sets across across multiple devices and have each contribute to building the model, all while preventing data leaks <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2019federated</span></sup></cite>. Clients train part of the model and upload partial updates to a central server, which then averages those updates and applies the result to the global model. The scheme considers the size of the training data, the computing resources required and the data privacy and security concerns. Each client is an isolated “data island”, and data never leaves the island. Once used to train the model, the only ’ship’ to leave is a model update to the central server. Plus, no island has all the data. Due to the limitation of a data island on the size and the characteristic, federated learning framework ideally benefits more to clients to collaboratively train a machine learning model using their data in security. The result is a more effective model that is insensitive to the raw data of others and, thus, federated learning has proven to be particularly attractive to governments, hospitals and financial institutions.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning is a specific category of distributed machine learning (DML) <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p2.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bonawitz2019towards</span></sup></cite> that further decentralizes operations that would usually be performed by the server. The server becomes more like an assistant that coordinates clients to work together instead of micro-managing schedules as in traditional DML. <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">FIGURE <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span> shows the classic DML framework, which includes a central server, some clients and a data manager. The central server can act as the data manager, or the data can be managed by a third party storage system under the server’s control. Together, the server and the data manager use optimization strategies to partition the training data into many subsets and the model into many parts and then disseminate learning tasks to the clients. Note that a key difference between DML and federated learning is that, in DML, one client may ask other clients to transfer their training data if needed to meet their own learning prerequisites or conditions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In comparison, <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">FIGURE <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span> illustrates a typical federated learning system. First, a central server publishes a machine learning task and selects clients to participate in each epoch of the training process. Then it sends the model and relevant sources to the clients and waits for their training results. Clients train the model with the data on their device and return an update of the model parameters or gradients to the server. The server then aggregates those details and updates the ’master’ model for the next training epoch. There two key advantages with this type of learning scheme: reduced computational and communications overhead and better privacy. The details of DML and federated learning will be introduced in the next section.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;"><img src="/html/2010.09258/assets/x1.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A basic DML system</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:208.1pt;"><img src="/html/2010.09258/assets/x2.png" id="S1.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="236" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A basic federated learning system</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In fact, federated learning can incorporate many privacy preserving and security mechanisms across the entire system – from the collaborative training process to aggregating updates at the server. For instance, differential privacy (DP) <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p4.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dwork2006calibrating</span></sup></cite> and local DP <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p4.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">kairouz2016extremal</span></sup></cite> can guarantee that both the training data and the updates remain private at the numeric level. Secure aggregation protocols on the server side, consisting of secure multi-party computation, secret sharing and homomorphic encryption, can perturb the updates to guarantee model security during transfer and aggregation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">However, although federated learning has made huge improvements to the privacy and security of machine learning models and all their associated processes, it is not a perfect solution. As with many new computing paradigms, federated learning is attracting its share of attention from adversaries with malicious intent. These adversaries might be internal agents participating in the training process who can influence the model updates, or they may be external agents that can only observe the learning and update process but are still able to make inferences that comprise a person’s privacy. Therefore, federated learning is still vulnerable to information leaks and many other types of attacks, such as backdoor attacks, model poisoning and inference attacks. A detailed survey of these adversarial models appears in a later section. Although some comprehensive surveys on federated learning have been published in the past three years, most focus on reviewing the systems and applications of federated learning. Few mention privacy preserving and security <cite class="ltx_cite ltx_citemacro_cite"><sup id="S1.p5.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">baruch2019little</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2019federated</span></sup></cite>, and none go into detail. Hence, the focus of this survey is on the privacy preserving and security aspects of federated learning, including privacy concerns, techniques for protecting privacy and securing assets, adversarial models, the challenges the field faces today, and future directions for advancement.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In the next section, we compare DML and federated learning from the perspective of privacy preservation. Then, in Section 3, we provide an in-depth analysis of the current mechanisms used in federated learning to provide privacy and security guarantees. Section 4 presents some of the most common and effective attack models against federated learning. We demonstrate the ways in which federated learning is still vulnerable to some methods of attack along with some possible defense strategies. Promising fields and applications for federated learning are outlined in Section 5, followed by the conclusion and future directions of research in Section 6.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>From Distributed Machine Learning To Federated Learning</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Distributed Machine Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">DML is a combination of distributed computing and machine learning,. DML has a very fast learning speed, so it is widely used for tasks with large-scale data or model parameters. The principle is to partition the training data and the model and have a parameter server devise and co-ordinate a schedule of multiple clients which learn each partition as a sub-task. All clients learn their allocated sub-task in parallel and, when all clients have completed their work, the parameter server aggregates the sub-models together and generates a complete model through scheduled aggregation algorithms. To train the model more effectively, the sub-models should simultaneously match the sub-tasks in order to train the model more effectively. Obviously, this process relies heavily on good communication between the server and the clients. However, it is important to strike a balance between the learning and communication costs because, with large scale data, resource constraints on storage, bandwidth and time can present real problems. As such, with DML frameworks, proper scheduling is vital to efficient performance.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>The Structure and Data Flows of Distributed Machine Learning</h4>

<figure id="S2.F3" class="ltx_figure"><img src="/html/2010.09258/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="301" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The architecture and data flow of distributed machine learning</figcaption>
</figure>
<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p"><span id="S2.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">FIGURE <a href="#S2.F3" title="Figure 3 ‣ 2.1.1 The Structure and Data Flows of Distributed Machine Learning ‣ 2.1 Distributed Machine Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span> shows the basic architecture of DML. The parameter server is central to the system. It is responsible for: scheduling; partitioning the model and the training data; allocating sub-tasks to clients; and aggregating the sub-models. The data manager could be the server or a third-party storage device obeying the server’s partition strategy. Clients can complete sub-tasks independently but, if a sub-tasks has prerequisite or follow-on tasks, they can also communicate with other clients to get the data they need.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">From the perspective of data flow, the parameter server must obviously have to access to the whole dataset to be able to create the partitions no matter whether the data is managed by the central server or a third-party. Notably, this data flow is one way; the partitioned data does not get sent back to the server; only the sub-model update does.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Privacy and Security Problems and Adversaries in DML</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The biggest, but not the only, vulnerability of DML is the amount of communication needed between the parameter server and the clients. Like the highway robberies of old, privacy protection is at its weakest when data is in transit. Therefore, the more communication, the more opportunities there are for attack. The danger alerts in <span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">FIGURE <a href="#S2.F3" title="Figure 3 ‣ 2.1.1 The Structure and Data Flows of Distributed Machine Learning ‣ 2.1 Distributed Machine Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span> indicate likely points of intrusion by adversaries. They include, the parameter server as the brain of the system, the data manager, the individual clients who may or may not be secure, as well as any time data is transmitted from one device to another. If an attack is successful, the amount of data leaked depends on the location of the attack. Violating a client may only net one or two data partitions but successfully penetrating the parameter or data server may yield the entire database or the entire model.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p"><span id="S2.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">TABLE <a href="#S2.T1" title="Table 1 ‣ 2.1.2 Privacy and Security Problems and Adversaries in DML ‣ 2.1 Distributed Machine Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span> summarizes the types of adversaries and their targets against DML schemes. Spectators can only observe the algorithms, models and training process. These adversaries are most likely curious about the training data and model but cannot affect the learning process. Conversely , participant adversaries can do quite a lot more damage. For instance, a malicious parameter server could wreak havoc because of its strong and centralized power, whereas the damage done by an adversarial clients is more contained. Hence, higher-level devices in the architecture are more attractive to adversaries.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_left">
<span id="S2.T1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:433.6pt;"><img src="/html/2010.09258/assets/x4.png" id="S2.T1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="133" alt="[Uncaptioned image]">
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Type of adversaries and their targets against DML systems</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Federated Learning is a specific type of DML, designed to overcome some of the privacy and security issues with classic DML architecture.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Basic Structures and Data Flow</h4>

<figure id="S2.F4" class="ltx_figure"><img src="/html/2010.09258/assets/x5.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Federate Learning Structure And Data Flow</figcaption>
</figure>
<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The basic architecture of federated learning including its data flows is illustrated in <span id="S2.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">FIGURE <a href="#S2.F4" title="Figure 4 ‣ 2.2.1 Basic Structures and Data Flow ‣ 2.2 Federated Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span>. As shown, there are some similarities and some differences between DML and federated learning. Like traditional DML, there is a central server, which is responsible for the overall control and management of training a global model and some clients who receive training sub-tasks from the central server. There are two key differences however: a) Instead of each client working individually on their own piece of the model, in federated learning, all selected clients work on the same training task in each epoch; and b) The clients in a federated learning system are typically devices like smart phones, tablets, and sensors that are able to capture or collect information as opposed to desktop computers or routers. Therefore, because the training data is gathered, stored and used at the client level, the only information that ever needs to be transmitted is the model updates.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">The learning procedure is relatively straightforward. In each training epoch, the server allocates a training task and computing resources to any client that is ready to learn, then it transmits the current model. The client trains the model with its own local data and sends the updated parameters as encrypted training results back to an aggregator for compilation. As such, there is greater data privacy because there is no need to transmit sensitive information, and encrypting the updated parameters before sending them to aggregators increases security over the models.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">The aggregators, also controlled by the central server, average the parameter updates. There are two types of aggregators: master and temporary. Master aggregators manage the number of training epochs and generate an appropriate number of “temporary” aggregators for each epoch to consolidate the training results. These temporary aggregators do not store any permanent information, and all aggregators follow what is called a “secure aggregation protocol”, which means encrypted data can be processed and compiled without knowing the true data. The master aggregators then fully aggregate the results from the temporary aggregators and deliver the results to the central server that updates the model. The server then schedules the next training task and starts a new training epoch.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Privacy And Security Problems And Adversaries In Federated Learning</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Even though federated learning was designed improve privacy and security, these frameworks still have vulnerabilities and security risks. Again, the danger signs in <span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">FIGURE <a href="#S2.F4" title="Figure 4 ‣ 2.2.1 Basic Structures and Data Flow ‣ 2.2 Federated Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span> identify potential attack targets. First, the raw data on the client devices is an attractive target for adversaries. Even though these data are never transmitted, they are still open to inference attacks without proper privacy guarantees at the device level. Second, the master model is a very valuable prize, which could be targeted in either a master aggregrator or the central server. The different types of adversaries and their potential attack targets are summarized in <span id="S2.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">TABLE <a href="#S2.T2" title="Table 2 ‣ 2.2.2 Privacy And Security Problems And Adversaries In Federated Learning ‣ 2.2 Federated Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_left">
<span id="S2.T2.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:433.6pt;"><img src="/html/2010.09258/assets/x6.png" id="S2.T2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="120" alt="[Uncaptioned image]">
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Type of adversaries and their targets against federated learning systems</figcaption>
</figure>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">As well as adversaries targets, we also cover potential roles of adversaries in <span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">TABLE <a href="#S2.T2" title="Table 2 ‣ 2.2.2 Privacy And Security Problems And Adversaries In Federated Learning ‣ 2.2 Federated Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>. In federated learning, an adversary can be either a spectator or a participant. Malicious spectators of the model’s training process can observe, but they cannot affect model performance, so the vulnerability here is one of an inference attack. The target might be either the model’s parameters or an attempt to glean sensitive information from the data. Malicious participants, however, can both observe and change individual updates, while malicious aggregators can observe the global parameter updates and control the results of averaging.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>From DML to Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Federated learning offers some significant improvements over DML in terms of both security and efficiency. Although there is still work to be done to make federated learning near to a completely secure system, this scheme seems the inevitable future of decentralized learning. What follows is a summary of the major advancements from DML to federated learning to date.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Federated learning uses smartphones, tablets and sensors as clients instead of computers and other non-data gathering devices. This means data is collected, used, and stored autonomously. Data does not need to be transferred around the system and no data manager is required. Less communication means less risk of data leaks and greater data privacy. In fact, data privacy protection is a fundamental property of federated learning. As a comparison between Figures <a href="#S2.F3" title="Figure 3 ‣ 2.1.1 The Structure and Data Flows of Distributed Machine Learning ‣ 2.1 Distributed Machine Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S2.F4" title="Figure 4 ‣ 2.2.1 Basic Structures and Data Flow ‣ 2.2 Federated Learning ‣ 2 From Distributed Machine Learning To Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show, federated learning is not subject to direct privacy leaks during communication.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Unlike DML, which allocates model training to clients in a piecemeal fashion, federated learning operates on the principle of collaboration where selected clients work on the same training task in parallel. Both systems still transmit parameter updates between the client and various servers. However, the collaborative nature of this approach means there is no need for a chain or hierarchy of clients and no reason for any client-to-client transfers. This further reduces the risk of data leaks.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Third, federated learning involves less communication than DML which reduces the opportunity for attack risks. As mentioned, the central server in DML allocates both data and model partitions to clients by communication. There may also be communication between lower-level clients and higher-level clients, to complete follow-on tasks. Hence, guaranteeing a smooth learning process depends heavily on much internal scheduling and task allocation, all of which requires communication. Because of this, the overall risk of information leaks is much higher with DML preventing information leaks over both the training data and the model parameters usually mean an expensive encryption mechanism need to be integrated into the system. By contrast, communication in federated learning is typically very low and highly efficient. The only communication allowed between the server and clients is for the global model transfers and learning updates aggregation, which does not involve any clients’ sensitive data. Therefore, only local data privacy preserving mechanisms are necessary to protect training data as opposed to multiple mechanisms to cover the device and the communications. Further, it is more difficult for and adversary to perform an inference attack on a client’s device than when data is in transit.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Last but not least, federated learning requires dramatically less storage space than DML. DML frameworks involve a data manager that stores the entire training set, ready to be partitioned for parallel clients. The central server may act as this data manager, which substantially adds to the server’s storage load. Alternatively, the data could be managed by a third-party storage server. This option increases the risk of data leaks because it adds another entity into the system that could be malicious, plus there is an extra financial cost for data storage and maintenance. But, depending on the situation, relieving pressure on the central server may outweigh these downsides. Federated learning bypasses all these problems because the raw data is collected and processed by the client’s device, which reduces much of the storage load on the server. Further, the clients generate the training data from the data they collect as opposed to generating data specifically for model training. Hence, the impact of the learning process on the server is also drastically reduced.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Overall, federated learning is considered to be an improved version of DML that provides substantially better privacy preservation and communications security. The vulnerabilities to adversaries are largely reduced to inference attacks as the learning results are sent to the aggregator for averaging or when learning updates are sent to the central server. Also, because the global model is publicly accessible to each participant in federated learning, adversaries can relatively easily reconstruct accurate model parameters from a client’s updates. Further, federated learning’s performance is strongly related to the update aggregation process, where each client’s contribution to the global model is the same. Consequently, just one malicious client can have a huge effect on the system. The lesson is that it is not possible to guarantee privacy and security with a framework alone. Additional privacy preserving and security mechanisms must be filled into the framework to guarantee these protections.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Privacy Preservation and Security in Federated Learning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As mentioned above, privacy is one of the paramount properties in the federated learning framework. As such, many different privacy preserving methods and security models are available to provide a meaningful privacy guarantee. In this section, we survey these methods and models and explore how each protects the various attack points in a federated learning system.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Privacy in Federated Learning</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Differential Privacy</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">DP is a provable privacy concept conceived by Dwork et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dwork2006calibrating</span></sup></cite>. Its premise that the outputs of the queries on neighboring datasets should be statistically similar is one of the strongest standards for a privacy guarantee. Traditional DP is centralized. The formal definition of DP is presented as follows:</p>
</div>
<div id="S3.Thmtheorem1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="S3.Thmtheorem1.2.1.1" class="ltx_text ltx_font_bold">Definition 3.1</span></span><span id="S3.Thmtheorem1.3.2" class="ltx_text ltx_font_bold"> </span>(<math id="S3.Thmtheorem1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.Thmtheorem1.1.m1.1b"><mi id="S3.Thmtheorem1.1.m1.1.1" xref="S3.Thmtheorem1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.1.m1.1c"><ci id="S3.Thmtheorem1.1.m1.1.1.cmml" xref="S3.Thmtheorem1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.1.m1.1d">\epsilon</annotation></semantics></math>-Differential Privacy)<span id="S3.Thmtheorem1.4.3" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="S3.Thmtheorem1.p1" class="ltx_para">
<p id="S3.Thmtheorem1.p1.6" class="ltx_p"><span id="S3.Thmtheorem1.p1.6.6" class="ltx_text ltx_font_italic">A randomized algorithm <math id="S3.Thmtheorem1.p1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.Thmtheorem1.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.Thmtheorem1.p1.1.1.m1.1.1" xref="S3.Thmtheorem1.p1.1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.p1.1.1.m1.1b"><ci id="S3.Thmtheorem1.p1.1.1.m1.1.1.cmml" xref="S3.Thmtheorem1.p1.1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.1.1.m1.1c">\mathcal{M}</annotation></semantics></math> gives <math id="S3.Thmtheorem1.p1.2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.Thmtheorem1.p1.2.2.m2.1a"><mi id="S3.Thmtheorem1.p1.2.2.m2.1.1" xref="S3.Thmtheorem1.p1.2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.p1.2.2.m2.1b"><ci id="S3.Thmtheorem1.p1.2.2.m2.1.1.cmml" xref="S3.Thmtheorem1.p1.2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.2.2.m2.1c">\epsilon</annotation></semantics></math>-differential privacy for any pair of
<em id="S3.Thmtheorem1.p1.6.6.1" class="ltx_emph ltx_font_upright">neighbouring datasets</em> <math id="S3.Thmtheorem1.p1.3.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.Thmtheorem1.p1.3.3.m3.1a"><mi id="S3.Thmtheorem1.p1.3.3.m3.1.1" xref="S3.Thmtheorem1.p1.3.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.p1.3.3.m3.1b"><ci id="S3.Thmtheorem1.p1.3.3.m3.1.1.cmml" xref="S3.Thmtheorem1.p1.3.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.3.3.m3.1c">D</annotation></semantics></math> and <math id="S3.Thmtheorem1.p1.4.4.m4.1" class="ltx_math_unparsed" alttext="D\prime" display="inline"><semantics id="S3.Thmtheorem1.p1.4.4.m4.1a"><mrow id="S3.Thmtheorem1.p1.4.4.m4.1b"><mi id="S3.Thmtheorem1.p1.4.4.m4.1.1">D</mi><mo lspace="0em" id="S3.Thmtheorem1.p1.4.4.m4.1.2">′</mo></mrow><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.4.4.m4.1c">D\prime</annotation></semantics></math>,
and for every set of outcomes <math id="S3.Thmtheorem1.p1.5.5.m5.1" class="ltx_Math" alttext="\Omega" display="inline"><semantics id="S3.Thmtheorem1.p1.5.5.m5.1a"><mi mathvariant="normal" id="S3.Thmtheorem1.p1.5.5.m5.1.1" xref="S3.Thmtheorem1.p1.5.5.m5.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.p1.5.5.m5.1b"><ci id="S3.Thmtheorem1.p1.5.5.m5.1.1.cmml" xref="S3.Thmtheorem1.p1.5.5.m5.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.5.5.m5.1c">\Omega</annotation></semantics></math>,
<math id="S3.Thmtheorem1.p1.6.6.m6.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.Thmtheorem1.p1.6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.Thmtheorem1.p1.6.6.m6.1.1" xref="S3.Thmtheorem1.p1.6.6.m6.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem1.p1.6.6.m6.1b"><ci id="S3.Thmtheorem1.p1.6.6.m6.1.1.cmml" xref="S3.Thmtheorem1.p1.6.6.m6.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem1.p1.6.6.m6.1c">\mathcal{M}</annotation></semantics></math> satisfies:</span></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.3" class="ltx_math_unparsed" alttext="Pr[\mathcal{M}(D)\in\Omega]\leq\exp(\epsilon)\cdot Pr[\mathcal{M}(D\prime)\in\Omega]\ ." display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3b"><mi id="S3.E1.m1.3.4">P</mi><mi id="S3.E1.m1.3.5">r</mi><mrow id="S3.E1.m1.3.6"><mo stretchy="false" id="S3.E1.m1.3.6.1">[</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.6.2">ℳ</mi><mrow id="S3.E1.m1.3.6.3"><mo stretchy="false" id="S3.E1.m1.3.6.3.1">(</mo><mi id="S3.E1.m1.1.1">D</mi><mo stretchy="false" id="S3.E1.m1.3.6.3.2">)</mo></mrow><mo id="S3.E1.m1.3.6.4">∈</mo><mi mathvariant="normal" id="S3.E1.m1.3.6.5">Ω</mi><mo stretchy="false" id="S3.E1.m1.3.6.6">]</mo></mrow><mo id="S3.E1.m1.3.7">≤</mo><mi id="S3.E1.m1.2.2">exp</mi><mrow id="S3.E1.m1.3.8"><mo stretchy="false" id="S3.E1.m1.3.8.1">(</mo><mi id="S3.E1.m1.3.3">ϵ</mi><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.3.8.2">)</mo></mrow><mo rspace="0.222em" id="S3.E1.m1.3.9">⋅</mo><mi id="S3.E1.m1.3.10">P</mi><mi id="S3.E1.m1.3.11">r</mi><mrow id="S3.E1.m1.3.12"><mo stretchy="false" id="S3.E1.m1.3.12.1">[</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.12.2">ℳ</mi><mrow id="S3.E1.m1.3.12.3"><mo stretchy="false" id="S3.E1.m1.3.12.3.1">(</mo><mi id="S3.E1.m1.3.12.3.2">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.12.3.3">′</mo><mo stretchy="false" id="S3.E1.m1.3.12.3.4">)</mo></mrow><mo id="S3.E1.m1.3.12.4">∈</mo><mi mathvariant="normal" id="S3.E1.m1.3.12.5">Ω</mi><mo rspace="0.222em" stretchy="false" id="S3.E1.m1.3.12.6">]</mo></mrow><mo id="S3.E1.m1.3.13">.</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.3c">Pr[\mathcal{M}(D)\in\Omega]\leq\exp(\epsilon)\cdot Pr[\mathcal{M}(D\prime)\in\Omega]\ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.2" class="ltx_p"><math id="S3.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><mi id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><ci id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">\epsilon</annotation></semantics></math> is the privacy parameter, also known as the <em id="S3.SS1.SSS1.p2.2.1" class="ltx_emph ltx_font_italic">privacy budget</em> <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS1.p2.2.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dwork2011firm</span></sup></cite>.
It controls the level of privacy preservation. A smaller <math id="S3.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">\epsilon</annotation></semantics></math> means greater privacy. <em id="S3.SS1.SSS1.p2.2.3" class="ltx_emph ltx_font_italic">Laplace</em> and <em id="S3.SS1.SSS1.p2.2.4" class="ltx_emph ltx_font_italic">Gaussian</em> mechanisms are widely used for numeric outputs of queries. The methods of DP involve adding noise to the data to obscure certain sensitive attributes until others cannot distinguish the exact true answers of quarries.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Ostensibly, DP involves adding noise to the data to obscure certain sensitive attributes until others cannot exactly determine the true answer to any query. However, the privacy guarantee DP offers is based on the assumption of a trustworthy data curator; DP cannot protect sensitive information from malicious data collectors or curators. Hence, to address situations where an individual needs to disclose their personal information to an untrusted curator, global DP was extended into local DP. Local DP is an improved DP model with the added restriction that an adversary is unable to learn too much sensitive information of any individual data contributor in the database. In local DP, only the owner of the data can obtain the original information, which provides strong privacy protection for individuals. A formal definition of local DP follows.</p>
</div>
<div id="S3.Thmtheorem2" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="S3.Thmtheorem2.2.2.1" class="ltx_text ltx_font_bold">Definition 3.2</span></span><span id="S3.Thmtheorem2.3.3" class="ltx_text ltx_font_bold"> </span>(<em id="S3.Thmtheorem2.1.1" class="ltx_emph ltx_font_italic"><math id="S3.Thmtheorem2.1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.Thmtheorem2.1.1.m1.1b"><mi id="S3.Thmtheorem2.1.1.m1.1.1" xref="S3.Thmtheorem2.1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.1.1.m1.1c"><ci id="S3.Thmtheorem2.1.1.m1.1.1.cmml" xref="S3.Thmtheorem2.1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.1.1.m1.1d">\epsilon</annotation></semantics></math>-Local Differential Privacy</em>)<span id="S3.Thmtheorem2.4.4" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="S3.Thmtheorem2.p1" class="ltx_para">
<p id="S3.Thmtheorem2.p1.8" class="ltx_p"><span id="S3.Thmtheorem2.p1.8.8" class="ltx_text ltx_font_italic">A randomized algorithm <math id="S3.Thmtheorem2.p1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.Thmtheorem2.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.Thmtheorem2.p1.1.1.m1.1.1" xref="S3.Thmtheorem2.p1.1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.1.1.m1.1b"><ci id="S3.Thmtheorem2.p1.1.1.m1.1.1.cmml" xref="S3.Thmtheorem2.p1.1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.1.1.m1.1c">\mathcal{M}</annotation></semantics></math> satisfies <math id="S3.Thmtheorem2.p1.2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.Thmtheorem2.p1.2.2.m2.1a"><mi id="S3.Thmtheorem2.p1.2.2.m2.1.1" xref="S3.Thmtheorem2.p1.2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.2.2.m2.1b"><ci id="S3.Thmtheorem2.p1.2.2.m2.1.1.cmml" xref="S3.Thmtheorem2.p1.2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.2.2.m2.1c">\epsilon</annotation></semantics></math>-differential privacy where <math id="S3.Thmtheorem2.p1.3.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.Thmtheorem2.p1.3.3.m3.1a"><mi id="S3.Thmtheorem2.p1.3.3.m3.1.1" xref="S3.Thmtheorem2.p1.3.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.3.3.m3.1b"><ci id="S3.Thmtheorem2.p1.3.3.m3.1.1.cmml" xref="S3.Thmtheorem2.p1.3.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.3.3.m3.1c">\epsilon</annotation></semantics></math> is a privacy parameter and <math id="S3.Thmtheorem2.p1.4.4.m4.1" class="ltx_Math" alttext="\epsilon\geq 0" display="inline"><semantics id="S3.Thmtheorem2.p1.4.4.m4.1a"><mrow id="S3.Thmtheorem2.p1.4.4.m4.1.1" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.cmml"><mi id="S3.Thmtheorem2.p1.4.4.m4.1.1.2" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.2.cmml">ϵ</mi><mo id="S3.Thmtheorem2.p1.4.4.m4.1.1.1" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.1.cmml">≥</mo><mn id="S3.Thmtheorem2.p1.4.4.m4.1.1.3" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.4.4.m4.1b"><apply id="S3.Thmtheorem2.p1.4.4.m4.1.1.cmml" xref="S3.Thmtheorem2.p1.4.4.m4.1.1"><geq id="S3.Thmtheorem2.p1.4.4.m4.1.1.1.cmml" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.1"></geq><ci id="S3.Thmtheorem2.p1.4.4.m4.1.1.2.cmml" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.Thmtheorem2.p1.4.4.m4.1.1.3.cmml" xref="S3.Thmtheorem2.p1.4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.4.4.m4.1c">\epsilon\geq 0</annotation></semantics></math>, if and only if for two inputs <math id="S3.Thmtheorem2.p1.5.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.Thmtheorem2.p1.5.5.m5.1a"><mi id="S3.Thmtheorem2.p1.5.5.m5.1.1" xref="S3.Thmtheorem2.p1.5.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.5.5.m5.1b"><ci id="S3.Thmtheorem2.p1.5.5.m5.1.1.cmml" xref="S3.Thmtheorem2.p1.5.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.5.5.m5.1c">d</annotation></semantics></math> and <math id="S3.Thmtheorem2.p1.6.6.m6.1" class="ltx_Math" alttext="d^{\prime}" display="inline"><semantics id="S3.Thmtheorem2.p1.6.6.m6.1a"><msup id="S3.Thmtheorem2.p1.6.6.m6.1.1" xref="S3.Thmtheorem2.p1.6.6.m6.1.1.cmml"><mi id="S3.Thmtheorem2.p1.6.6.m6.1.1.2" xref="S3.Thmtheorem2.p1.6.6.m6.1.1.2.cmml">d</mi><mo id="S3.Thmtheorem2.p1.6.6.m6.1.1.3" xref="S3.Thmtheorem2.p1.6.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.6.6.m6.1b"><apply id="S3.Thmtheorem2.p1.6.6.m6.1.1.cmml" xref="S3.Thmtheorem2.p1.6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.Thmtheorem2.p1.6.6.m6.1.1.1.cmml" xref="S3.Thmtheorem2.p1.6.6.m6.1.1">superscript</csymbol><ci id="S3.Thmtheorem2.p1.6.6.m6.1.1.2.cmml" xref="S3.Thmtheorem2.p1.6.6.m6.1.1.2">𝑑</ci><ci id="S3.Thmtheorem2.p1.6.6.m6.1.1.3.cmml" xref="S3.Thmtheorem2.p1.6.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.6.6.m6.1c">d^{\prime}</annotation></semantics></math>, and any possible output <math id="S3.Thmtheorem2.p1.7.7.m7.1" class="ltx_Math" alttext="\Omega" display="inline"><semantics id="S3.Thmtheorem2.p1.7.7.m7.1a"><mi mathvariant="normal" id="S3.Thmtheorem2.p1.7.7.m7.1.1" xref="S3.Thmtheorem2.p1.7.7.m7.1.1.cmml">Ω</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.7.7.m7.1b"><ci id="S3.Thmtheorem2.p1.7.7.m7.1.1.cmml" xref="S3.Thmtheorem2.p1.7.7.m7.1.1">Ω</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.7.7.m7.1c">\Omega</annotation></semantics></math> of <math id="S3.Thmtheorem2.p1.8.8.m8.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.Thmtheorem2.p1.8.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.Thmtheorem2.p1.8.8.m8.1.1" xref="S3.Thmtheorem2.p1.8.8.m8.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.Thmtheorem2.p1.8.8.m8.1b"><ci id="S3.Thmtheorem2.p1.8.8.m8.1.1.cmml" xref="S3.Thmtheorem2.p1.8.8.m8.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Thmtheorem2.p1.8.8.m8.1c">\mathcal{M}</annotation></semantics></math>, we have</span></p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.1" class="ltx_math_unparsed" alttext="Pr[\mathcal{M}(x)=\Omega]\leq e^{\epsilon}\cdot Pr[\mathcal{M}(x\prime)=\Omega]\ " display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1b"><mi id="S3.E2.m1.1.2">P</mi><mi id="S3.E2.m1.1.3">r</mi><mrow id="S3.E2.m1.1.4"><mo stretchy="false" id="S3.E2.m1.1.4.1">[</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.4.2">ℳ</mi><mrow id="S3.E2.m1.1.4.3"><mo stretchy="false" id="S3.E2.m1.1.4.3.1">(</mo><mi id="S3.E2.m1.1.1">x</mi><mo stretchy="false" id="S3.E2.m1.1.4.3.2">)</mo></mrow><mo id="S3.E2.m1.1.4.4">=</mo><mi mathvariant="normal" id="S3.E2.m1.1.4.5">Ω</mi><mo stretchy="false" id="S3.E2.m1.1.4.6">]</mo></mrow><mo id="S3.E2.m1.1.5">≤</mo><msup id="S3.E2.m1.1.6"><mi id="S3.E2.m1.1.6.2">e</mi><mi id="S3.E2.m1.1.6.3">ϵ</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.7">⋅</mo><mi id="S3.E2.m1.1.8">P</mi><mi id="S3.E2.m1.1.9">r</mi><mrow id="S3.E2.m1.1.10"><mo stretchy="false" id="S3.E2.m1.1.10.1">[</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.10.2">ℳ</mi><mrow id="S3.E2.m1.1.10.3"><mo stretchy="false" id="S3.E2.m1.1.10.3.1">(</mo><mi id="S3.E2.m1.1.10.3.2">x</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.10.3.3">′</mo><mo stretchy="false" id="S3.E2.m1.1.10.3.4">)</mo></mrow><mo id="S3.E2.m1.1.10.4">=</mo><mi mathvariant="normal" id="S3.E2.m1.1.10.5">Ω</mi><mo stretchy="false" id="S3.E2.m1.1.10.6">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1c">Pr[\mathcal{M}(x)=\Omega]\leq e^{\epsilon}\cdot Pr[\mathcal{M}(x\prime)=\Omega]\ </annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.4" class="ltx_p">The main difference between local and global (traditional) DP is that, in global DP, the randomized noise is from the outputs of an algorithm over all users’ data, whereas with local DP, the randomized noise is over a single user’s data. Further, the data collector can only receive perturbed data <math id="S3.SS1.SSS1.p4.1.m1.1" class="ltx_math_unparsed" alttext="x\prime" display="inline"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mrow id="S3.SS1.SSS1.p4.1.m1.1b"><mi id="S3.SS1.SSS1.p4.1.m1.1.1">x</mi><mo lspace="0em" id="S3.SS1.SSS1.p4.1.m1.1.2">′</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">x\prime</annotation></semantics></math> not the original data <math id="S3.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mi id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><ci id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">x</annotation></semantics></math>, and cannot distinguish the real data <math id="S3.SS1.SSS1.p4.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mi id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><ci id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">x</annotation></semantics></math> and <math id="S3.SS1.SSS1.p4.4.m4.1" class="ltx_math_unparsed" alttext="x\prime" display="inline"><semantics id="S3.SS1.SSS1.p4.4.m4.1a"><mrow id="S3.SS1.SSS1.p4.4.m4.1b"><mi id="S3.SS1.SSS1.p4.4.m4.1.1">x</mi><mo lspace="0em" id="S3.SS1.SSS1.p4.4.m4.1.2">′</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.4.m4.1c">x\prime</annotation></semantics></math> with much confidence, regardless of the background knowledge. Thus, the user is given a privacy guarantee without the need for a trustworthy third party. If multiple DP algorithms are applied to the same dataset, the total privacy level equals the sum of the privacy budget of each algorithm.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Differential Privacy in Federated Learning</h4>

<section id="S3.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Global Differential Privacy</h5>

<div id="S3.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS2.Px1.p1.1" class="ltx_p">DP is a rigorous and easily-implemented privacy model that can guarantee privacy even if an adversary has auxiliary information <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu2020more</span></sup></cite>. DP has some properties that make it particularly useful for protecting privacy in deep learning: simplicity, composition ability, and correlated data guarantee <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">abadi2016deep</span></sup></cite>. Deep learning is often computationally inefficient and, because adding noise does not increase computational complexity, DP is often incorporated into deep learning frameworks as a simple privacy preserving method. Deep learning networks typically have many layers, and the composition ability protects the information in each layer of the network, which ensures that the output from deep learning is private. However, the data used to train the deep learning network may be correlated, which can increase the chances of a privacy leak. Hence, some methods consider these correlations so as to provide a better privacy guarantee.</p>
</div>
<div id="S3.SS1.SSS2.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS2.Px1.p2.1" class="ltx_p">Global DP protects client privacy by adding noise during the aggregation process on the server-side. Clients’ updates are uploaded and stored in the aggregator temporarily. These updates can be treated like a dataset where the aggregation result is the “query”, and every update is one record. The goal with global DP is, therefore, to hide every client update in the aggregation result. McMahan et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px1.p2.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mcmahan2016communication</span></sup></cite> were the first to consider protecting user data in the training set with DP in federated learning. They argued that DP could provide a rigorous worst-case privacy guarantee, even when the adversary had arbitrary side-information, by adding random noise to the model’s training process. However, that guarantee would come with a utility cost. In their later work <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px1.p2.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">konevcny2016federated</span></sup></cite>, they modified a federated learning system’s central algorithm to produce a differentially private model, i.e., a releasable model that protects the privacy of all individuals contributing updates to the trained global model. The one shortcoming of global DP in federated learning is that the sensitivity is hard to be set. Sensitivity has an enormous impact on both the privacy guarantee and the model’s performance. Yet setting the sensitivity during the aggregation process is challenging because aggregators should not be able to distinguish a particular client’s update, which may negatively impact the trade-off between privacy and the model’s utility.</p>
</div>
</section>
<section id="S3.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Local Differential Privacy</h5>

<div id="S3.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.Px2.p1.8" class="ltx_p">In a federated learning setting, local DP is a better solution for protecting privacy from the client’s perspective. Each client protects their own data and unadulterated learning results (i.e., model updates) with a specific randomized algorithm. Noisy updates are then uploaded to the aggregator. Abadi et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px2.p1.8.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">abadi2016deep</span></sup></cite> of Google were the first to proposed deep learning with DP in 2016, which was later followed by local DP to protect the individual training process of each participant in Google’s federated learning framework. These developers created a differentially private stochastic gradient descent algorithm (SGD), a moments accountant and a hyper-parameter tuning process, which, at the time, were are new algorithmic techniques for learning. They also refined the analysis of privacy costs within DP frameworks. More specifically, the algorithm assumes a training model with the parameters <math id="S3.SS1.SSS2.Px2.p1.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.1.m1.1a"><mi id="S3.SS1.SSS2.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.1.m1.1c">\theta</annotation></semantics></math>. The procedure is then to minimize its loss function <math id="S3.SS1.SSS2.Px2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}(\theta)" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.2.m2.1a"><mrow id="S3.SS1.SSS2.Px2.p1.2.m2.1.2" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.2" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.1" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.3.2" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.3.2.1" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.SSS2.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.3.2.2" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2"><times id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.1"></times><ci id="S3.SS1.SSS2.Px2.p1.2.m2.1.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.2.2">ℒ</ci><ci id="S3.SS1.SSS2.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.2.m2.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.2.m2.1c">\mathcal{L}(\theta)</annotation></semantics></math>, compute the gradient <math id="S3.SS1.SSS2.Px2.p1.3.m3.3" class="ltx_Math" alttext="\mathbf{g}_{t}(x_{i})\leftarrow\nabla_{\theta}\mathcal{L}(\theta,x_{i})" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.3.m3.3a"><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.3.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.cmml"><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.cmml"><msub id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.2.cmml">𝐠</mi><mi id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.3.cmml">←</mo><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.cmml"><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.cmml"><msub id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.cmml"><mo rspace="0.167em" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.2.cmml">∇</mo><mi id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.3.cmml">θ</mi></msub><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.2.cmml">ℒ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.2.cmml">(</mo><mi id="S3.SS1.SSS2.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1.cmml">θ</mi><mo id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.2.cmml">,</mo><msub id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.4" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.3.m3.3b"><apply id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3"><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.3">←</ci><apply id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1"><times id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.2"></times><apply id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.2">𝐠</ci><ci id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.3.3">𝑡</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.2.2.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2"><times id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.2"></times><apply id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3"><apply id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.2">∇</ci><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.1.3">𝜃</ci></apply><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.3.2">ℒ</ci></apply><interval closure="open" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1"><ci id="S3.SS1.SSS2.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.1.1">𝜃</ci><apply id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.3.m3.3.3.2.1.1.1.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.3.m3.3c">\mathbf{g}_{t}(x_{i})\leftarrow\nabla_{\theta}\mathcal{L}(\theta,x_{i})</annotation></semantics></math> for a random subset of examples at the each step of the SGD and then clip the <math id="S3.SS1.SSS2.Px2.p1.4.m4.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.4.m4.1a"><msub id="S3.SS1.SSS2.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1.2.cmml">ℓ</mi><mn id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1.2">ℓ</ci><cn type="integer" id="S3.SS1.SSS2.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.4.m4.1c">\ell_{2}</annotation></semantics></math> norm of each gradient <math id="S3.SS1.SSS2.Px2.p1.5.m5.1" class="ltx_math_unparsed" alttext="\bar{\mathbf{g}}_{t}(x_{i})\leftarrow\mathbf{g}_{t}(x_{i})/" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.5.m5.1a"><mrow id="S3.SS1.SSS2.Px2.p1.5.m5.1b"><msub id="S3.SS1.SSS2.Px2.p1.5.m5.1.1"><mover accent="true" id="S3.SS1.SSS2.Px2.p1.5.m5.1.1.2"><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.1.2.2">𝐠</mi><mo id="S3.SS1.SSS2.Px2.p1.5.m5.1.1.2.1">¯</mo></mover><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.1.3">t</mi></msub><mrow id="S3.SS1.SSS2.Px2.p1.5.m5.1.2"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.5.m5.1.2.1">(</mo><msub id="S3.SS1.SSS2.Px2.p1.5.m5.1.2.2"><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.2.2.2">x</mi><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.2.2.3">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.5.m5.1.2.3">)</mo></mrow><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.5.m5.1.3">←</mo><msub id="S3.SS1.SSS2.Px2.p1.5.m5.1.4"><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.4.2">𝐠</mi><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.4.3">t</mi></msub><mrow id="S3.SS1.SSS2.Px2.p1.5.m5.1.5"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.5.m5.1.5.1">(</mo><msub id="S3.SS1.SSS2.Px2.p1.5.m5.1.5.2"><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.5.2.2">x</mi><mi id="S3.SS1.SSS2.Px2.p1.5.m5.1.5.2.3">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.5.m5.1.5.3">)</mo></mrow><mo id="S3.SS1.SSS2.Px2.p1.5.m5.1.6">/</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.5.m5.1c">\bar{\mathbf{g}}_{t}(x_{i})\leftarrow\mathbf{g}_{t}(x_{i})/</annotation></semantics></math> max<math id="S3.SS1.SSS2.Px2.p1.6.m6.2" class="ltx_Math" alttext="(1,\frac{\left\|\mathbf{g}_{t}(x_{i})\right\|_{2}}{C})" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.6.m6.2a"><mrow id="S3.SS1.SSS2.Px2.p1.6.m6.2.3.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.6.m6.2.3.2.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.3.1.cmml">(</mo><mn id="S3.SS1.SSS2.Px2.p1.6.m6.2.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.2.cmml">1</mn><mo id="S3.SS1.SSS2.Px2.p1.6.m6.2.3.2.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.3.1.cmml">,</mo><mfrac id="S3.SS1.SSS2.Px2.p1.6.m6.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.2.cmml"><mo id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.2.cmml">𝐠</mi><mi id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.3.cmml">2</mn></msub><mi id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.3.cmml">C</mi></mfrac><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.6.m6.2.3.2.3" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.6.m6.2b"><interval closure="open" id="S3.SS1.SSS2.Px2.p1.6.m6.2.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.3.2"><cn type="integer" id="S3.SS1.SSS2.Px2.p1.6.m6.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.2.2">1</cn><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1"><divide id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1"></divide><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1">subscript</csymbol><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1"><times id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.2"></times><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.2">𝐠</ci><ci id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.1.3">2</cn></apply><ci id="S3.SS1.SSS2.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.6.m6.1.1.3">𝐶</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.6.m6.2c">(1,\frac{\left\|\mathbf{g}_{t}(x_{i})\right\|_{2}}{C})</annotation></semantics></math> where <math id="S3.SS1.SSS2.Px2.p1.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.7.m7.1a"><mi id="S3.SS1.SSS2.Px2.p1.7.m7.1.1" xref="S3.SS1.SSS2.Px2.p1.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.7.m7.1b"><ci id="S3.SS1.SSS2.Px2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.7.m7.1c">C</annotation></semantics></math> is the clipping threshold. Noise is added while computing the average <math id="S3.SS1.SSS2.Px2.p1.8.m8.2" class="ltx_Math" alttext="\tilde{\mathbf{g}}_{t}\leftarrow\frac{1}{L}(\sum_{i}\bar{\mathbf{g}}_{t}(x_{i})+\mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I}))" display="inline"><semantics id="S3.SS1.SSS2.Px2.p1.8.m8.2a"><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.cmml"><msub id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.cmml"><mover accent="true" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.cmml"><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.2.cmml">𝐠</mi><mo id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.1.cmml">~</mo></mover><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.3.cmml">t</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.2.cmml">←</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.cmml"><mfrac id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.cmml"><mn id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.2.cmml">1</mn><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.3.cmml">L</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.cmml"><mo lspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.3.cmml">i</mi></msub><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.cmml"><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.2.cmml">𝐠</mi><mo id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.1.cmml">¯</mo></mover><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.3.cmml">+</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.2.cmml">(</mo><mn id="S3.SS1.SSS2.Px2.p1.8.m8.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.1.1.cmml">0</mn><mo id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.2.cmml">,</mo><mrow id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.cmml"><msup id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.cmml"><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.2.cmml">σ</mi><mn id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1.cmml">​</mo><msup id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.2.cmml">C</mi><mn id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1a" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.4" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.4.cmml">𝐈</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.4" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.3" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p1.8.m8.2b"><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2"><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.2">←</ci><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3">subscript</csymbol><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2"><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.1">~</ci><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.2.2">𝐠</ci></apply><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.3.3">𝑡</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1"><times id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.2"></times><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3"><divide id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3"></divide><cn type="integer" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.2">1</cn><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.3.3">𝐿</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1"><plus id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.3"></plus><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1"><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.2"></sum><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1"><times id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.2"></times><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2"><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.1">¯</ci><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.2.2">𝐠</ci></apply><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2"><times id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.2"></times><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.3">𝒩</ci><interval closure="open" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1"><cn type="integer" id="S3.SS1.SSS2.Px2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.1.1">0</cn><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1"><times id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.1"></times><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.2">𝜎</ci><cn type="integer" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.2.3">2</cn></apply><apply id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.2">𝐶</ci><cn type="integer" id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.3.3">2</cn></apply><ci id="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.4.cmml" xref="S3.SS1.SSS2.Px2.p1.8.m8.2.2.1.1.1.1.2.1.1.1.4">𝐈</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p1.8.m8.2c">\tilde{\mathbf{g}}_{t}\leftarrow\frac{1}{L}(\sum_{i}\bar{\mathbf{g}}_{t}(x_{i})+\mathcal{N}(0,\sigma^{2}C^{2}\mathbf{I}))</annotation></semantics></math> before taking a step in the opposite direction of this average noisy gradient. The approach offers protection against a strong adversary, even with full knowledge of the training mechanism and access to the model’s parameters.</p>
</div>
<div id="S3.SS1.SSS2.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.Px2.p2.8" class="ltx_p">Geyer et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS1.SSS2.Px2.p2.8.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">geyer2017differentially</span></sup></cite> subsequently proposed an algorithm for client-side DP but still using federated optimization with the aim of hiding the participation and contributions of clients during the training process. Balancing the trade-off between privacy loss and model performance occurs during decentralized training. Instead of protecting a single data point’s contribution from an individual client in the learning model, the algorithm is designed to protect a client’s entire dataset. Altering and approximating the federated averaging process is done with a randomized mechanism consisting of two steps: random sub-sampling and distorting. Random sub-sampling means to randomly sample a subset of clients from the total pool of participating clients to update their optimized training results on in the further calculation in each communication round. The difference between the optimized local model and the global model in this round is referred to as client <math id="S3.SS1.SSS2.Px2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.1.m1.1a"><mi id="S3.SS1.SSS2.Px2.p2.1.m1.1.1" xref="S3.SS1.SSS2.Px2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.1.m1.1b"><ci id="S3.SS1.SSS2.Px2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.1.m1.1c">k</annotation></semantics></math>’s update <math id="S3.SS1.SSS2.Px2.p2.2.m2.1" class="ltx_Math" alttext="\Delta w^{k}=w^{k}-w_{t}" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.2.m2.1a"><mrow id="S3.SS1.SSS2.Px2.p2.2.m2.1.1" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.1" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.1.cmml">​</mo><msup id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.3.cmml">k</mi></msup></mrow><mo id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.1" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.cmml"><msup id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.cmml"><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.3.cmml">k</mi></msup><mo id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.1" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.1.cmml">−</mo><msub id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.2" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.3" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.2.m2.1b"><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1"><eq id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.1"></eq><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2"><times id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.1"></times><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.2">Δ</ci><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.2.3.3">𝑘</ci></apply></apply><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3"><minus id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.1"></minus><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.2.3">𝑘</ci></apply><apply id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.2.m2.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.2.m2.1c">\Delta w^{k}=w^{k}-w_{t}</annotation></semantics></math>. Distorting is the step where Gaussian noise is added to each client’s update. Each client’s update is scaled as <math id="S3.SS1.SSS2.Px2.p2.3.m3.1" class="ltx_math_unparsed" alttext="\Delta w^{k}/" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.3.m3.1a"><mrow id="S3.SS1.SSS2.Px2.p2.3.m3.1b"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p2.3.m3.1.1">Δ</mi><msup id="S3.SS1.SSS2.Px2.p2.3.m3.1.2"><mi id="S3.SS1.SSS2.Px2.p2.3.m3.1.2.2">w</mi><mi id="S3.SS1.SSS2.Px2.p2.3.m3.1.2.3">k</mi></msup><mo id="S3.SS1.SSS2.Px2.p2.3.m3.1.3">/</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.3.m3.1c">\Delta w^{k}/</annotation></semantics></math> max<math id="S3.SS1.SSS2.Px2.p2.4.m4.2" class="ltx_Math" alttext="(1,\frac{\lVert\Delta w^{k}\rVert_{2}}{S})" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.4.m4.2a"><mrow id="S3.SS1.SSS2.Px2.p2.4.m4.2.3.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p2.4.m4.2.3.2.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.3.1.cmml">(</mo><mn id="S3.SS1.SSS2.Px2.p2.4.m4.2.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.2.cmml">1</mn><mo id="S3.SS1.SSS2.Px2.p2.4.m4.2.3.2.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.3.1.cmml">,</mo><mfrac id="S3.SS1.SSS2.Px2.p2.4.m4.1.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.2.1.cmml">∥</mo><mrow id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.3.cmml">k</mi></msup></mrow><mo fence="true" lspace="0em" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.3.cmml">2</mn></msub><mi id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.3.cmml">S</mi></mfrac><mo stretchy="false" id="S3.SS1.SSS2.Px2.p2.4.m4.2.3.2.3" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.4.m4.2b"><interval closure="open" id="S3.SS1.SSS2.Px2.p2.4.m4.2.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.3.2"><cn type="integer" id="S3.SS1.SSS2.Px2.p2.4.m4.2.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.2.2">1</cn><apply id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1"><divide id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1"></divide><apply id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1">subscript</csymbol><apply id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1"><times id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.2">Δ</ci><apply id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply><cn type="integer" id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.1.3">2</cn></apply><ci id="S3.SS1.SSS2.Px2.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.4.m4.1.1.3">𝑆</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.4.m4.2c">(1,\frac{\lVert\Delta w^{k}\rVert_{2}}{S})</annotation></semantics></math> to ensure that the second norm is limited to <math id="S3.SS1.SSS2.Px2.p2.5.m5.1" class="ltx_Math" alttext="\forall k" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.5.m5.1a"><mrow id="S3.SS1.SSS2.Px2.p2.5.m5.1.1" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1.cmml"><mo rspace="0.167em" id="S3.SS1.SSS2.Px2.p2.5.m5.1.1.1" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1.1.cmml">∀</mo><mi id="S3.SS1.SSS2.Px2.p2.5.m5.1.1.2" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1.2.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.5.m5.1b"><apply id="S3.SS1.SSS2.Px2.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p2.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1.1">for-all</csymbol><ci id="S3.SS1.SSS2.Px2.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.5.m5.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.5.m5.1c">\forall k</annotation></semantics></math>, <math id="S3.SS1.SSS2.Px2.p2.6.m6.1" class="ltx_Math" alttext="\lVert\Delta w^{k}\rVert_{2}&lt;S" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.6.m6.1a"><mrow id="S3.SS1.SSS2.Px2.p2.6.m6.1.1" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.cmml"><msub id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.2.1.cmml">∥</mo><mrow id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.3.cmml">k</mi></msup></mrow><mo fence="true" lspace="0em" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.3.cmml">2</mn></msub><mo id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.2" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.2.cmml">&lt;</mo><mi id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.3" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.6.m6.1b"><apply id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1"><lt id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.2"></lt><apply id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1">subscript</csymbol><apply id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1"><times id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.2">Δ</ci><apply id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply><cn type="integer" id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.1.3">2</cn></apply><ci id="S3.SS1.SSS2.Px2.p2.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.6.m6.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.6.m6.1c">\lVert\Delta w^{k}\rVert_{2}&lt;S</annotation></semantics></math>. Originally, the developers set the clipping bound sensitivity to <math id="S3.SS1.SSS2.Px2.p2.7.m7.1" class="ltx_Math" alttext="S=" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.7.m7.1a"><mrow id="S3.SS1.SSS2.Px2.p2.7.m7.1.1" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.2" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.2.cmml">S</mi><mo id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.1" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.1.cmml">=</mo><mi id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.3" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.7.m7.1b"><apply id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1"><eq id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.1"></eq><ci id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.2">𝑆</ci><csymbol cd="latexml" id="S3.SS1.SSS2.Px2.p2.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.7.m7.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.7.m7.1c">S=</annotation></semantics></math> median<math id="S3.SS1.SSS2.Px2.p2.8.m8.1" class="ltx_Math" alttext="\{\Delta w^{k}\}_{k\in Z_{t}}" display="inline"><semantics id="S3.SS1.SSS2.Px2.p2.8.m8.1a"><msub id="S3.SS1.SSS2.Px2.p2.8.m8.1.1" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.cmml"><mrow id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.2.cmml">{</mo><mrow id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.2" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.1" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.2" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.2.cmml">w</mi><mi id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.3.cmml">k</mi></msup></mrow><mo stretchy="false" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.2" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.2.cmml">k</mi><mo id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.1" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.1.cmml">∈</mo><msub id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.cmml"><mi id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.2" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.2.cmml">Z</mi><mi id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.3" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.3.cmml">t</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.Px2.p2.8.m8.1b"><apply id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1">subscript</csymbol><set id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1"><apply id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1"><times id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.2">Δ</ci><apply id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.2">𝑤</ci><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.1.1.1.3.3">𝑘</ci></apply></apply></set><apply id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3"><in id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.1"></in><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.2">𝑘</ci><apply id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.1.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.2.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.2">𝑍</ci><ci id="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.3.cmml" xref="S3.SS1.SSS2.Px2.p2.8.m8.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.Px2.p2.8.m8.1c">\{\Delta w^{k}\}_{k\in Z_{t}}</annotation></semantics></math> without using a randomised mechanism to compute the median. This caused a privacy violation.</p>
</div>
</section>
<section id="S3.SS1.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>

<div id="S3.SS1.SSS2.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS2.Px3.p1.1" class="ltx_p">The problem with local DP is that the total volume of noise added is much greater than with global DP, which can negatively impact the model’s utility and performance. A future research direction for DP in federated learning is to find a better trade-off between privacy and utility to provide a strong privacy guarantee while maintaining acceptable model performance. Further, DP can only provide privacy at the data level but, in federated learning, communication and aggregation are crucial to updating the global model. Therefore, to guarantee secure communication, security mechanisms need to be incorporated into the framework.</p>
</div>
</section>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Security in Federated learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Security mechanisms normally concern the security of data transmission with cryptographic algorithms and protocols. In federated learning, most of the communications surround model aggregation because all devices must upload their training updates to the aggregator for averaging. To prevent leaks of any individual’s training results, a specific protocol called “secure aggregation” encrypts the client updates at the device level before they are uploaded for aggregation. The protocol guarantees that all updates are aggregated in a secure way and that any other party can only access the cipher-text of a client’s updates – even the server. These protocols involve secret sharing schemes, secure multi-party computation and homomorphic encryption.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Preliminaries of Security Mechanisms</h4>

<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Secret Sharing Schemes</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.5" class="ltx_p">Secret sharing schemes are widely used in many cryptographic protocols. They involve a client with a secret, a set of <math id="S3.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.1.m1.1c">n</annotation></semantics></math> parties, and a collection of subsets of those parties called the access structure <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">beimel2011secret</span></sup></cite>. The secret sharing scheme for the collection distributes shares of the secret to these parties by a dealer according to two requirements: 1) any subset in the collection can reconstruct the secret from its shares of the secret, and any subset not in the collection cannot reveal any partial information about the secret, separately. Secret sharing is motivated by the problem of secure information storage. They have since been developed for numerous other applications in cryptography and distributed computing, such as secure multiparty computation <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ben2019completeness</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">chaum1988multiparty</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">cramer2000general</span></sup></cite> and threshold cryptography <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">desmedt1991shared</span></sup></cite>. Secret sharing schemes are firstly proposed by Blakley <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">blakley1979safeguarding</span></sup></cite> and Shamir <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.5.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shamir1979share</span></sup></cite>. They are a t-out-of-m scheme based on a threshold, where a threshold <math id="S3.SS2.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.2.m2.1c">t</annotation></semantics></math> and the number of the secret shares <math id="S3.SS2.SSS1.Px1.p1.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS1.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.3.m3.1c">m</annotation></semantics></math> that any <math id="S3.SS2.SSS1.Px1.p1.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.4.m4.1c">t</annotation></semantics></math> shares from these <math id="S3.SS2.SSS1.Px1.p1.5.m5.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.5.m5.1a"><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.5.m5.1c">m</annotation></semantics></math> shares can reconstruct the whole secret. Ito et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px1.p1.5.6.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ito1989secret</span></sup></cite> construct a secret sharing schemes for general access structures. However, a major problem with this approach is that the share size required to provide general access is exponential to the number of parties. That said, secret sharing schemes are a good way to protect client updates in federated learning because they can be partitioned into many shares, which helps with the costs and vulnerability associated with communication. Overall, the performance and efficiency of secret sharing schemes depend on a good optimization strategy.</p>
</div>
</section>
<section id="S3.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Secure Multi-party Computation</h5>

<div id="S3.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px2.p1.1" class="ltx_p">Secure multi-party computation is first proposed by Yao <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yao1982protocols</span></sup></cite> in 1982. This technique addresses the problem of having a set of parties calculate an agreed-on function over their private inputs such that all parties can reveal the intended output without obtaining other parties’ inputs <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px2.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hazay2010efficient</span></sup></cite>. The idea is that all parties’ private inputs are protected by an encryption scheme that guarantees the utility of the data for accurately answering a query function. In this sense, multi-party computation is more like a general notion of secure computation comprising a set of techniques as opposed to being a single method. Over the last decade, multi-party computation has seen impressive developments in lower-level primitives, such as oblivious transfer protocols and encryption schemes with homomorphic properties. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px2.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">kairouz2019advances</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhirov2013practical</span></sup></cite>. In federated learning, aggregators average the updates of multiple clients, which contain sensitive information, so multi-party computation schemes are a highly suitable approach to protecting both the clients’ updates and the aggregation process.</p>
</div>
</section>
<section id="S3.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Homomorphic Encryption</h5>

<div id="S3.SS2.SSS1.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px3.p1.1" class="ltx_p">Homomorphic encryption is first suggested by Rivest et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px3.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">rivest1978data</span></sup></cite> in 1978. It is an encryption scheme that allows complex mathematical operations to be performed on cipher-text without changing the nature of the encryption. The two different types of homomorphic encryption are fully homomorphic encryption and partially homomorphic encryption. Fully homomorphic encryption supports both additive and multiplicative operations, while partially homomorphic encryption only supports one or the other <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px3.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">gentry2009fully</span></sup></cite>. Fully homomorphic encryption is strongly recommended in federated learning, even though the cost of computation is much greater because the aggregation process involves both addition and multiplication. Also, because the central server should not be able to decrypt the client updates, a trusted third party must be involved to hold a key <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS1.Px3.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">kairouz2019advances</span></sup></cite>, and the central server must be able to sum the client updates using only cipher-text. Homomorphic encryption exactly meets all these requirements.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Secure Aggregation in Federated Learning</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Secure aggregation is a subclass of multi-party computation algorithms where a group of parties that do not trust each other each hold sensitive information and must collaborate to calculate an aggregated value. The aggregated value should not reveal any party’s information (except what it can learn from its own information). Like homomorphic encryption and secret sharing schemes, each client’s outputs are encrypted before they are shared, which guarantees a secure transit process.</p>
</div>
<section id="S3.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Federated Secure Aggregation Protocol</h5>

<div id="S3.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p1.1" class="ltx_p">In late 2016, Bonawitz et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS2.Px1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bonawitz2016practical</span></sup></cite> propose the first secure aggregation suggested secure aggregation protocol for federated learning to protect the privacy of clients’ model gradients and to guarantee that the server only learns the sum of the clients’ inputs while the users learn nothing. Later, in early 2017, Bonawitz et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS2.Px1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bonawitz2017practical</span></sup></cite> further developed a full version of the protocol for practical applications. A random number masks each client’s raw input to prevent direct disclosure to the central server, and each client generates a private-public key pair for each epoch of the aggregation process. Each client is allowed to combine its private key and every other client’s public key, to generate a private shared key with a hash function. The hash function involves Pseudo Random Generator and Decisional Diffie-Hellman assumption to guarantee each pair of clients’ private shared keys are additive inverse. Because the sum of a pair of private shared keys is zero, all clients’ masks are offset during the aggregation process, and the server can offset the effect of the masks to calculate an accurate aggregation result without needing to know any of the clients’ true inputs.</p>
</div>
<div id="S3.SS2.SSS2.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p2.1" class="ltx_p">The shortcoming of this method is that if any client disconnects after obtaining the mask, but before submitting the masked inputs to the server, the dropped mask cannot be offset in the server’s sum. Consequently, the protocol includes a secret sharing scheme to partition each client’s private key as a secret. The secret requires at least a minimum threshold number of clients to contribute shares to recover the secret. If no clients disconnect before the aggregation process, the scheme is not triggered but, if a client does disconnect, the server sends a request to the other clients to contribute their shares so as to recover the client’s private key. The server then computes and removes the mask using the private key coupled with the public keys from the contributors. However, this solution is not perfect and raises a new problem in that, if a dropped client reconnects and sends its inputs to the server after its private key has been recovered, the server can reveal the true inputs simply by removing the mask. To address this new problem, an additional random number for each client creates a second mask over the first. This second mask is also partitioned as a secret through the secret sharing scheme. For connected clients, the server only needs to recover and remove the sum of all the second masks, while the first mask still protects the inputs without any negative effect on the aggregation process. And, because the inputs of disconnected clients will not contribute to the aggregation, the second mask remains in place to protect the true inputs once the first mask has been recovered and removed. This protocol provides a strong and comprehensive guarantee of security over the aggregation process, but it is not particularly efficient as the key exchanges and secret sharing scheme each add significantly to the communication cost.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">NIKE-based Secure Aggregation Protocol</h5>

<div id="S3.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px2.p1.1" class="ltx_p">To address these two communications burdens, Mandal et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS2.Px2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mandal2018nike</span></sup></cite> propose the non-interactive key establishment protocol (NIKE) and a secure aggregation protocol based on NIKE. NIKE addresses the cost of key sharing. It comprises two non-colluding cryptographic secret service providers who independently calculate pairwise polynomial functions for each client. To generate a shared private key, each client generates a private polynomial function as a private key by multiplying the two polynomial functions. Further, each client has a unique order number assigned by the server, which is public information, and any client is allowed to generate a shared private key by placing the targeted client’s order number into their private polynomial function. Thus, there is no communication cost for generating a shared key, and the protocol guarantees that each pair of client calculations with its own private polynomial function will have the same results.</p>
</div>
<div id="S3.SS2.SSS2.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.Px2.p2.3" class="ltx_p">The NIKE-based secure aggregation protocol reduces the communication costs associated with the secret sharing scheme. The method involves an <math id="S3.SS2.SSS2.Px2.p2.1.m1.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S3.SS2.SSS2.Px2.p2.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS2.Px2.p2.1.m1.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p2.1.m1.1b"><ci id="S3.SS2.SSS2.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px2.p2.1.m1.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p2.1.m1.1c">\ell</annotation></semantics></math>-regular network network where the server randomly divides every <math id="S3.SS2.SSS2.Px2.p2.2.m2.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S3.SS2.SSS2.Px2.p2.2.m2.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.Px2.p2.2.m2.1.1" xref="S3.SS2.SSS2.Px2.p2.2.m2.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p2.2.m2.1b"><ci id="S3.SS2.SSS2.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px2.p2.2.m2.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p2.2.m2.1c">\ell</annotation></semantics></math> clients into neighbor groups. Each client can only calculate private shared keys with their neighbors via the NIKE protocol. These keys are then summed and added as a mask over the true inputs. A 2-out-of-3 secret sharing scheme is applied such that each client’s private shared key combined with the targeted client’s order number is divided into 3 shares. One share is held by the client, and the other two are held by the targeted client and the server, respectively. If a client disconnects, the server only needs to ask its neighbors for the shares to reconstruct and offset the mask. Consequently, the communications costs for reconstructing a disconnected client’s mask is reduced to <math id="S3.SS2.SSS2.Px2.p2.3.m3.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S3.SS2.SSS2.Px2.p2.3.m3.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.Px2.p2.3.m3.1.1" xref="S3.SS2.SSS2.Px2.p2.3.m3.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p2.3.m3.1b"><ci id="S3.SS2.SSS2.Px2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.Px2.p2.3.m3.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p2.3.m3.1c">\ell</annotation></semantics></math> times the client’s private key instead of t times for every shared private key. Again, each client generates a double mask to protect its inputs for the same reasons as outlined above.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">PrivFL</h5>

<div id="S3.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px3.p1.1" class="ltx_p">Mandal and Gong <cite class="ltx_cite ltx_citemacro_cite"><sup id="S3.SS2.SSS2.Px3.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mandal2019privfl</span></sup></cite> further the work of Mandel et al. with a protocol called PrivFL that involves linear and logistic regression models and oblivious prediction for federated learning settings. The two regression protocols make the model more robust to user disconnections. The protocol consists of multiple two-party shared local gradient computation mechanisms followed by a global gradient share-reconstruction protocol. Here, the two parties are the server, which holds the global model and the clients who hold the shares. The server and a single client first jointly run a shared local gradient computation protocol to securely compute two shares of the local gradients. The server then constructs one share of the global gradient with all alive clients via an aggregation protocol and a second share of the global gradient from its own local gradient shares. An additive homomorphic encryption scheme and a secure aggregation protocol with practical crypto-primitives imposed at the beginning of each learning epoch guarantee a safe environment for the training process client-side and the aggregation process server-side.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>

<div id="S3.SS2.SSS2.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px4.p1.1" class="ltx_p">In general, federated learning incorporates DP to protect the training sets and model updates held by the clients, while secure aggregation protocols consisting of secure multi-party computation, secret sharing schemes and homomorphic encryption guarantee the security of the aggregation process. It is important to note, however, that these mechanisms only protect the data; they cannot assess or protect the validity of the training results. In other words, the privacy and security mechanisms currently available for federated learning only protect client updates, not a malicious client’s contribution to the global model. In the next section, we discuss the most common and effective attack models used to infiltrate federated learning systems.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Attacks and Federated Learning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">One of the greatest advantages of federated learning compared to traditional distributed machine learning is the ability to prevent direct leaks of sensitive information by a malicious data server. However, federated learning is still vulnerable to some traditional attack models. On the client-side, adversaries can infer sensitive data in the training set from the training results. Server-side, malicious agents can negatively impact the performance of the global model because, in federated learning, client updates are simply averaged without monitoring the training data or the learning process. As such, an adversarial client that uploads a malicious update to the server for aggregation can have a substantial impact on the global model. In this section, we spell out the various attack methods used to compromise federated learning and the goals and capabilities of the adversaries for each attack.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Adversarial Goals and Capabilities</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As mentioned above, the two broad types of attacks in federated learning are inference at the client level or performance degradation at the global level. Inference attacks seek sensitive information. Performance attacks, called poisoning attacks, have two levels of scope: <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">untargeted attacks</span> and <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">targeted attacks</span>. In an untargeted attack, the aim is to destroy the global model by reducing its accuracy <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS1.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">biggio2012poisoning</span></sup></cite>. Targeted attacks aim to alter the model’s behavior on only one or a few specific tasks while maintaining acceptable performance on all other tasks <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS1.p1.1.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bhagoji2018analyzing</span></sup></cite>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_left">
<span id="S4.T3.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:433.6pt;"><img src="/html/2010.09258/assets/x7.png" id="S4.T3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="156" alt="[Uncaptioned image]">
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Adversary capabilities in various attacks against federated learning</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">TABLE <a href="#S4.T3" title="Table 3 ‣ 4.1 Adversarial Goals and Capabilities ‣ 4 Attacks and Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span> summarizes the adversaries’ capabilities for each of the different types of attacks. The strategy with poisoning attacks is to act as a malicious client and upload invalid updates so as to train the model with a malicious or undesirable dataset. In most cases, poisoning attacks are executed by a solo adversary, although multiple adversaries can easily collude to attack each training epoch. Some adversaries only execute an attack once per training epoch. Further, recall that in federated learning, only a subset of all the participants is randomly chosen for each epoch, so a client may only be chosen once during the entire training process. However, when there are only a limited number of clients participating in the learning task, an adversary may be able to execute repeated attacks across multiple training epochs. The last adversarial capability is model inspections. Some models are white-boxes where the model’s parameters are ‘public’; others are black boxes where they are not. Most attacks in federated learning are white-box attacks because all clients receive the parameters of the global model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Inference Attack</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Preliminary of Membership Inference Attack</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">A membership inference attack is a common tracing attack to determine whether a specific individual is a member of a given dataset or not <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dwork2017exposed</span></sup></cite>. In machine learning, deep learning or federated learning, membership inference attacks aim to determine whether a given data point is present in the training data <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yeom2018privacy</span></sup></cite>. The accuracy of the attack corresponds to the proportion of correct membership predictions made by the adversary, while precision is calculated as the proportion of examples inferred to be members that are indeed members of the target model’s training set. These attacks take advantage of the fact that the behavior of a training model between the training set and the test set may be very different (i.e., the model may be overfit). As such, an adversary can train a machine learning model to recognize the differences in its own behavior versus the target model to determine whether or not an input record is involved in the training process <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS1.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yeom2018privacy</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">rahman2018membership</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">nasr2019comprehensive</span></sup></cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Membership Inference Attacks against Federated Learning</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Even though black-box attacks in federated learning are rare, there have been recent studies into these types of attacks in machine and deep learning scenarios. Here, the attackers can only observe the target model’s outputs as an external spectator <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yeom2018privacy</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">shokri2017membership</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">nasr2019comprehensive</span></sup></cite>. The findings of these studies generally show that the distribution of the training data and the generalizability of the training model are the most significant contributors to privacy leaks. Moreover, over-fit models are more susceptible to membership inference attacks.</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Shadow Models in Black-box Setting</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">Shokri et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shokri2017membership</span></sup></cite> proposed a membership inference attack method based on a shadow training technique. The strategy is to build many shadow models trained in a similar way to the target model with the same training algorithms (e.g. SVM, neural network) and model structure. However, while the shadow training sets have the same format as the target model, they are disjoint. Importantly, the attackers know whether or not a given record is in the training set they give to each of their shadow models. The next step is to train a neural network model using the inputs and the corresponding outputs labeled with "in" or "out" meaning in the training set or out of the training set of the shadow models. Now the attack model can distinguish between the output of the various shadow models based on memberships in the training sets. The accuracy of the attack model rises as the number of shadow models increases.</p>
</div>
<div id="S4.SS2.SSS2.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p2.3" class="ltx_p">A similar idea is proposed by Rahman et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px1.p2.3.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">rahman2018membership</span></sup></cite> to attack differentially private deep learning models <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px1.p2.3.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">abadi2016deep</span></sup></cite>. These researchers use two performance metrics, precision and <math id="S4.SS2.SSS2.Px1.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS2.SSS2.Px1.p2.1.m1.1a"><msub id="S4.SS2.SSS2.Px1.p2.1.m1.1.1" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.2" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.3" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p2.1.m1.1b"><apply id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S4.SS2.SSS2.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.Px1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p2.1.m1.1c">F_{1}</annotation></semantics></math>-score, as assessment metrics in a series of experiments designed to test the vulnerability of differentially private deep learning models. <math id="S4.SS2.SSS2.Px1.p2.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS2.SSS2.Px1.p2.2.m2.1a"><msub id="S4.SS2.SSS2.Px1.p2.2.m2.1.1" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.2" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1.2.cmml">F</mi><mn id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.3" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p2.2.m2.1b"><apply id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S4.SS2.SSS2.Px1.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.Px1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p2.2.m2.1c">F_{1}</annotation></semantics></math>-score is correlated to with precision and recall which is the proportion of the images belonging to the target model’s training dataset that are correctly predicted to be members. The results revealed moderate vulnerability to membership inference attacks but with acceptable utility, and decreasing utility as the strength of the privacy protection grew. In other words, a model’s utility is highly correlated to the DP loss <math id="S4.SS2.SSS2.Px1.p2.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS2.SSS2.Px1.p2.3.m3.1a"><mi id="S4.SS2.SSS2.Px1.p2.3.m3.1.1" xref="S4.SS2.SSS2.Px1.p2.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p2.3.m3.1b"><ci id="S4.SS2.SSS2.Px1.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS2.Px1.p2.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p2.3.m3.1c">\epsilon</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.SSS2.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p3.1" class="ltx_p">Yeom et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px1.p3.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yeom2018privacy</span></sup></cite> further simplify the shadow training method by comparing the classification loss value of the target example with a preset threshold where small loss indicates membership. The idea is that this approach is equivalent to using the shadow models as a linear classifier of loss values. If the model’s confidence in an input prediction is larger than or equal to the preset threshold, it is identified as a member and a non-member otherwise. Their experiments show this strategy to be very effective with an accuracy very close to or better than the classic shadow training method. Song et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px1.p3.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">song2019membership</span></sup></cite> follow this method but use a linear classifier for the threshold to yield a more robust deep learning model. With this approach, membership leaks are directly related to the generalization ability of the training algorithm. The more training data that is leaked, the less robust the model.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deep Neural Networks in White-Box Setting</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.1" class="ltx_p">Nasr et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS2.SSS2.Px2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nasr2019comprehensive</span></sup></cite> recently present a comprehensive framework for analyzing data leaks with deep neural networks by executing membership inference attacks in a white-box setting. All major scenarios and adversarial capabilities in deep learning applications were considered, including model training and fine-tuning, adversaries with prior knowledge, colluding adversaries, and the vulnerabilities of SGD algorithms. A target dataset with one-hot encoding of the true labels is used to infer whether a record was included in the target model’s training set. Attackers are then able to compute the outputs of all the hidden layers, the loss, and the gradients of all layers of the target model for the given dataset. These computation results and true labels can then be used to construct the input features for an attack model consisting of convolutional neural network components and fully connected network components. Nasr and colleagues considered two roles for the attacker: first as a curious server then as a participant in a federated learning setting. A single attack model is used to process all the corresponding inputs over the observed model at once instead of running an individual independent membership inference attack on each participant’s model. Results from their experiments show that the last layer of the network leaks the most membership information. A summary of the different types of membership inference attacks in federated learning follows in <span id="S4.SS2.SSS2.Px2.p1.1.2" class="ltx_text ltx_font_bold">TABLE <a href="#S4.T4" title="Table 4 ‣ Deep Neural Networks in White-Box Setting ‣ 4.2.2 Membership Inference Attacks against Federated Learning ‣ 4.2 Inference Attack ‣ 4 Attacks and Federated Learning ‣ From Distributed Machine Learning To Federated Learning: In The View Of Data Privacy And Security" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_left">
<span id="S4.T4.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:433.6pt;"><img src="/html/2010.09258/assets/x8.png" id="S4.T4.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="94" alt="[Uncaptioned image]">
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Nasr el al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.T4.3.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nasr2019comprehensive</span></sup></cite> Membership inference attacks in federated learning setting</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Poisoning Attack</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Basic Poisoning Attacks</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Poisoning attacks are a kind of causative attack <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">biggio2012poisoning</span></sup></cite>, in which adversaries inject crafted attack points into the training data, such as manipulating a portion of the training data to adversaries’ expected labels. The effect is to change the model’s parameters associated with specific learning tasks during training. Poisoned learning models subsequently misclassify those inputs at the inference stage <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2017targeted</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao2015feature</span></sup></cite>. This attack is based on the premise that an adversary cannot directly access an existing training database, but may contribute new training data <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS1.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">biggio2012poisoning</span></sup></cite>, which provides greater opportunities for the adversary to poison the model.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Convergence prevention</span> and <span id="S4.SS3.SSS1.p2.1.2" class="ltx_text ltx_font_bold">backdoor attacks</span> are two malicious objectives of poisoning attacks <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS1.p2.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">baruch2019little</span></sup></cite>. The goal of a convergence prevention attack is as its name implies – to use malicious workers to ensure the model fails to converge during the training phase. Backdoor attacks are where an adversary manipulates the model during the training process so that the model produces the adversaries’ expected results on an adversarial target. The target can be either a single sample or a class of samples. For instance, an adversary can make a model falsely classify either a specific person as another. The key to a good backdoor attack is to ensure the global model converges and performs well on the test set and that only the accuracy of the targeted tasks suffer.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">There are two categories in data poisoning attacks: <span id="S4.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">clean-label</span> and <span id="S4.SS3.SSS1.p3.1.2" class="ltx_text ltx_font_bold">dirty-label</span>. In clean-label poisoning, adversaries cannot change any labels in the training data, whereas, with dirty-label poisoning, the label of a single targeted record or class can be changed. Adversaries then introduce one or more misclassified data samples into the training process. Dirty-label poisoning with deep learning models normally results in high-confidence misclassifications for the targeted record or class.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p">Poisoning attacks in federated learning are made possible for the following reasons <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS1.p4.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bagdasaryan2018backdoor</span></sup></cite>:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">There are usually plenty of participants in a federated learning system, which makes it easy to hide one or more malicious users in the crowd.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Since all participants locally train their part of the mode, the training process is invisible to the parameter server, and the server cannot verify the authenticity of the updates <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.I1.i2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hayes2018contamination</span></sup></cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Local updates generated by multiple participants might be very different from each other, but the secure aggregation protocol guarantees that local updates cannot be audited by the parameter server, and the encrypted updates are simply averaged.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Poisoning Attacks Against Federated Learning</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Many recent works focus on backdoor attacks against federated learning. For example, Bagdasaryan et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bagdasaryan2018backdoor</span></sup></cite> propose a backdoor attack with a constrain-and-scale technique where the attacker compromises one or more participants and trains a model with poisoned data. This model replace the original model as the result of federated averaging. The constrain-and-scale technique scales the model’s loss with an anomaly detection term, controlled by a hyper-parameter that the importance of evading anomaly detection. The effect is to adjust the trade-off between attack accuracy and the risk of being detected. This type of attack can be executed as either a one-shot attack or a repeated attack. In a one-shot attack setting, the accuracy of the global model on the backdoor task immediately rises to a high level in a single round when the attacker inject the backdoor updates. In a repeated attack, only a very small proportion of participants can make a better performance on the backdoor task in the target model than conventional data poisoning.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.2" class="ltx_p">Bhagoji et al.’s <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p2.2.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bhagoji2018analyzing</span></sup></cite> version of a poisoning attack is based on three assumptions: 1) the malicious adversary is non-colluding; 2) the data are i.i.d, which makes it easy to distinguish malicious and benign updates and harder to achieve a stealth attack; and 3) malicious adversaries have access to partial training data plus auxiliary data drawn from the same distribution as the training and test sets. The strategy is then to execute explicit boosting and alternating minimization processes. Explicit boosting overcomes the effect of scaling at the server in a gradient-based optimizer by scaling the initial updates up to <math id="S4.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mi id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><ci id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">\lambda</annotation></semantics></math> times, where <math id="S4.SS3.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><mi id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><ci id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">\lambda</annotation></semantics></math> is the inverse of step rate in gradient descent. The alternating minimization mechanism boosts the part of the weight update that corresponds to the adversarial objective based on explicit boosting for malicious agents only.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">Fung et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p3.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">fung2018mitigating</span></sup></cite> evaluate the vulnerability of federated learning to Sybil-based poisoning attacks without bounding the expected number of attackers and auxiliary information. The adversary performs poisoning attacks based on either the label-flipping strategy <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p3.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">biggio2012poisoning</span></sup></cite> , in which the labels of honest training examples are flipped to an adversarial target class or the backdoor strategy <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p3.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bagdasaryan2018backdoor</span></sup></cite>Even with only 2 Sybils, the attack is capable of reaching a 96.2% success rate. Fung and colleague’s ultimate finding is that an adversary with enough Sybils could overpower the system regardless of the number of honest clients. Further, existing defence methods could not thwart such an attack in a federated learning setting because those methods rely on observations of the training data, and only the model parameters are observable in federated learning.</p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p4.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2019poisoning</span></sup></cite> propose a poisoning attack using generative adversarial nets (GAN) <cite class="ltx_cite ltx_citemacro_cite"><sup id="S4.SS3.SSS2.p4.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">goodfellow2014generative</span></sup></cite>. Here, the adversary deploys a GAN architecture to reconstruct the private training data of other participants without invading their devices and then uses that data to train the model. First, the attacker pretends to be a benign participant to acquire training results, say under the pretext that those results are needed for a subsequent task. Simultaneously, they train a GAN to mimic prototypical samples of others. The attacker then injects an expected label into the data and generates a poisoned update. This compromises the global model’s performance on the target class but not on any of the other tasks.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Discussion</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.1" class="ltx_p">In general, federated learning is rather vulnerable to poisoning attacks. It is typically easy for a malicious participant to hide in the crowd of clients, and enough malicious participants can overpower the honest clients to compromise the model. Moreover, existing defense methods against such attacks are useless in federated settings because they rely on observing the training data.</p>
</div>
<div id="S4.SS3.SSS2.Px1.p2" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p2.1" class="ltx_p">Thus, there are several open questions to resolve concerning poisoning attacks. First, most existing poisoning attacks depend on repeatedly poisoning epochs rather than finding success with a one-shot approach. However, in real-world federated learning applications, very few clients are randomly selected to participate in the same training task in multiple epochs, which severely limits the chances of a successful attack with only one malicious client. Second, few poisoning attacks consider the problem of unbalanced training data. They assume that each client holds a relatively similar number of training samples, and that each client only trains one class of samples. In turn, it is assumed that both malicious clients and honest clients must contribute equally to the global model. However, it is very likely that clients will hold different numbers of samples and train a variety of classes. A challenge for adversaries is, therefore, to build an attack model capable of making a large impact on the global model with only a very small number of samples despite the large number of samples contributed by honest clients. Techniques involving data enhancement and GANs may overcome this challenge in the future.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications Of Federated Learning</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As smartphones and Internet of Things (IoT) sensors have become ubiquitous, so too is federated learning becoming the go-to solution for scenarios involving large-scale data collection and model training, such as IoT, blockchain and recommendation systems. Compared to traditional machine learning methods, federated learning directly involves client devices in the training process as opposed to relying on a central model. As mentioned several times, offloading operations traditionally performed by the server to the end client devices gives federated learning its two key benefits: a stronger privacy guarantee and reduced communication costs.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Federated learning in IoT Environment and Edge Computing</h3>

<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">IoT Environment and Edge Computing</h5>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">Smart homes, smart cities and smart medical systems <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ren2019federated</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">khan2019federated</span></sup></cite> are increasingly making IoT an important part of our daily life <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">aivodji2019iotfla</span></sup></cite>. The dominant paradigm in IoT is edge computing <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px1.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shi2016edge</span></sup></cite>, where computation devices at the edge of the network process data downstream on behalf of cloud services and upstream on behalf of the IoT service. The result is reduced communication costs, improved data processing efficiency and better decision-making because the edge devices are both data producers and consumers. Federated learning can be thought of as an operating system for edge computing as it provides a learning protocol for coordinating the IoT devices along with all the privacy and security mechanisms and benefits outlined in Section 3 and Section 4 <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px1.p1.1.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2019federated</span></sup></cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Anomaly Detection</h5>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">Anomaly detection plays a significant role in preventing and mitigating the consequences of attacks on mobile edge networks. Many approaches to anomaly detection have been proposed for conventional machine learning and deep learning. However, the success rate of detection relies on the datasets and the sensitive information they contain <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">lim2020federated</span></sup></cite>.Federated learning helps to address this vulnerability by storing the datasets locally. Abebe and Naveen’s <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px2.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">abeshu2018deep</span></sup></cite> anomaly detection method for federated learning in edge networks is based on a detection model that each client helps to train using their local data. The edge nodes upload training updates to the server for aggregation, and the updated model is sent back to the device for the next training epoch. In this way, each node can improve its detection accuracy without sharing any data. A similar idea was proposed by Nguyen et al.<cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px2.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen2019diot</span></sup></cite>, where an IoT security service provider plays the role of a federated learning server, aggregating models trained by IoT gateways as clients. Both frameworks, however, assume that all edge nodes and gateways are honest and positively contribute to the training process, which means malicious participants can do significant damage.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Edge Caching and Computation Offloading</h5>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">Edge caching and computation offloading is another application area of federated learning, which addresses issues associated with limits to the computational power or storage capacity of the edge devices by offloading intensive computation tasks to the cloud server <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">lim2019federated</span></sup></cite>. In these scenarios, federated learning is applied to optimize the caching and offloading among the devices. Wang et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang2019edge</span></sup></cite> design a near-optimal "In-Edge AI" framework using deep reinforcement learning <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mnih2013playing</span></sup></cite> in a federated learning setting. The method optimizes caching and offloading decisions in a mobile edge computing framework <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">beck2014mobile</span></sup></cite> that consists of user equipment covered by base stations. Ren et al.’s <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.5.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ren2019federated</span></sup></cite> method is based on a similar idea of using deep reinforcement learning to optimize offloading decisions for IoT systems consisting of IoT devices and edge nodes. However, the shortcoming of both methods is that the intensity of computations in a deep reinforcement learning model’s training can cause delays in networks with a large number of heterogeneous devices. Yu et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.6.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yu2018federated</span></sup></cite> skirt this issue with a proactive content caching scheme based on federated learning instead of deep reinforcement learning. The model optimizes the caching by making a prediction about the content’s popularity. Further, user privacy is protected by learning the parameters of the stacked auto-encoder locally without revealing the user’s personal information or content request history to the server. Similar systems of federated learning have also been applied to vehicular networks to minimize power consumption while minimizing queuing delays <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.7.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">samarakoon2018federated</span></sup></cite> and for predicting energy demands in electric vehicle networks <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS1.SSS0.Px3.p1.1.8.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">saputra2019energy</span></sup></cite>.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Federated Learning in Blockchain</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Blockchain</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">Blockchain emerged in the last decade as a way to securely transfer Bitcoin <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px1.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nakamoto2019bitcoin</span></sup></cite> without a central regulator. In blockchain, all user accounts and transaction information is saved in a publicly verifiable blockchain <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px1.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zyskind2015decentralizing</span></sup></cite>. Similar to federated learning frameworks, each client is able to access the full blockchain and locally contribute to the global blockchain by adding new blocks in chronological order <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px1.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">swan2015blockchain</span></sup></cite>. A reward is given to a client who successfully contributes blocks to update the chain to encourage more clients to positively participate in the scheme. Client privacy is guaranteed by keeping public keys anonymous to break the flow of information back to the contributor. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px1.p1.1.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nakamoto2019bitcoin</span></sup></cite>. As such, all contributors are anonymous, and the public can only see that someone has added a transaction with an amount not who added the transaction.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">There are several similarities between federated learning frameworks and blockchain. First, the global model (or blockchain) is accessible to every participant, and participants need to download the existing model parameters (or the chain of blocks) before participating in the update process. In federated learning, clients receive the global model from a central server, whereas, with blockchain, miners update the entire chain of blocks from broadcasts by other nodes. Second, all participants fairly contribute to the global model. Federated learning averages the client updates, while each miner in a blockchain has the same opportunity to add a new block to the chain and broadcast the update to the other miners. Third, all data processing with both systems occurs on the client device, not on a central server, and all client contributions are anonymous. In summary, these similarities lead to an appropriate combination of federated learning and blockchain for enhancing privacy and security guarantees in many existing and future applications.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Blockchain Empowered Federated Learning</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">Lu et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px2.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">lu2019blockchain</span></sup></cite> propose a blockchain-empowered secure data sharing architecture for distributed multiple parties in an industrial IoT environment. The system comprises a permissioned blockchain module and a federated learning module. All voluntary parties who agree to share data, then upload retrieval records to the blocks of the permissioned blockchain to check if a set of queries has been proceeded. All results are learned in a federated learning setting that the multi-party data retrieval process identifies the related parties to learn the results of queries and then upload to global model instead transferring raw data directly to the data curator. The data model contains valid information towards the requests and minimized private data of participants.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p2.1" class="ltx_p">Zhao et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS2.SSS0.Px2.p2.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhao2019mobile</span></sup></cite> use blockchain to replace the central aggregator in a reputation-based crowdsourcing system comprising federated learning, mobile edge computing and blockchain. Clients announce their training results to a miner who checks the signature of the uploaded file. Verifiable random functions are then used to determine a subset of miners as leaders by weighting their gained rewards. These miners are preferentially responsible for averaging the client updates and uploading the global model to the blockchain once the validity of the signature has been confirmed. Only the hash of the file location is saved in the blockchain as opposed to the actual data.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Federated Learning in Recommendation System</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Federated learning can act as a form of privacy-preserving machine learning for recommendation systems in classical cases, such as virtual keyboard prediction <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hard2018federated</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2018applied</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">ramaswamy2019federated</span></sup></cite> and preference recommendations <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hegedHus2019decentralized</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">ammad2019federated</span></sup></cite>. Google’s original intention with federated learning was to improve Google services on the Android system, which involve an enormous number of clients and very large-scale data. The data produced and query requests of millions of clients are simply too large to feasibly collect in a central place. As an example, an important application is Google Keyboard (Gboard), which is a virtual keyboard for mobile devices running the Android system <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.3.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hard2018federated</span></sup></cite>. Gboard includes typing features like next word prediction and auto-correction of spelling mistakes. It also offers expression features such as GIFs, stickers and emojis. As both a mobile application and a virtual keyboard, Gboard must guarantee its clients’ privacy because what clients type into their device can be recorded, and what is typed may be sensitive, such as passwords. Federated learning can address this problem by allowing Gboard to train a machine learning model without collecting the clients’ sensitive data <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.4.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2018applied</span></sup></cite>. Long short-term memory <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.5.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hochreiter1997long</span></sup></cite> is used to train a baseline model that selects and displays a query suggestion. A model is then triggered that determines if the suggestion should be shown to the client. Ramaswamy et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p1.1.6.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ramaswamy2019federated</span></sup></cite> consider "Diversification" while focusing on emoji prediction in Gboard. A lack of diversity can lead to situations where only the most frequently-used emoji are predicted regardless of the input. To overcome this issue, Ramaswamy and colleagues scaled the probability of each emoji in keeping with an empirical probability. The proposed method is also applicable to word prediction.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Some applications keep private logs of user activities on the client’s device, such as browser histories and cookies, to help provide recommendations based on user preferences. These logs typically contain a wealth of sensitive information on a user’s interests and habits. Federated learning can protect this information from leaking given the central tenet that no data leaves the device <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p2.1.1.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hegedHus2019decentralized</span></sup></cite>. Ammad-ud-din et al. <cite class="ltx_cite ltx_citemacro_cite"><sup id="S5.SS3.p2.1.2.1" class="ltx_sup"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ammad2019federated</span></sup></cite> propose federated collaborative filtering for recommendation systems based on implicit feedback from clients. A collaborative filter trains a model to learn interactions between a client and a set of items. Then, new items that should be of interest to the client are recommended based on the learned patterns. A federated collaborative filter aggregates the gradients of each client’s filter model into the global model to make recommendations without loss of accuracy.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we surveyed federated learning in the context of data privacy preserving and security. In general, we believe that federated learning is a necessary trend in the advancement of distributed and collaborative machine learning because of its ability to offload computations from the central server. Further, federated learning accommodates the large-scale numbers of participants common to many of today’s online services in a secure way, and the privacy and security this learning framework affords is almost unparalleled. The data used to train the global never leaves the client’s device. Only the training results are uploaded to the central server as a partial model update. The received client updates are then aggregated and averaged; the global model is updated; and the server prepares the next training epoch.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The privacy of the client’s data and the model updates it transmits are protected by global DP and local DP mechanisms applied during the training process, while the global model is protected through a secure aggregation protocol consisting of secure multi-party computation protocols, secret sharing schemes and many other encryption mechanisms. However, each of these protections have specific strengths and weaknesses. For instance, global DP is vulnerable to an adversarial aggregator, whereas local DP can protect the client updates before sending them to the aggregator. However, local DP is sensitive to noise, and too much noise can impact the model’s utility. Secure aggregation protocols impose a numeric “mask” to conceal the true data used to generate the model updates while in transit that is then removed during the aggregation process to maintain accuracy. The downside here is that secure aggregation protocols are computationally expensive, and more work needs to be done to reduce their complexity.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">However, federated learning is still vulnerable to data membership inference attacks and backdoor attacks. Further, adversaries are difficult to detect because federated learning usually involves a large number of participants, and each participant equally contributes to the global model. Therefore, one future direction of research is to investigate how to prevent data leaks due to inference during the training process. Another fruitful direction would be to improve the tolerance of federated learning models to anomalous updates during the aggregation process while still guaranteeing an appropriate level of utility and accuracy. The result would be more robust models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">1</span>
<span class="ltx_bibblock">
Hirt CW, Amsden AA, Cook JL. An arbitrary Lagrangian-Eulerian computing
method for all flow speeds. <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>1974;14(3):227–253.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">2</span>
<span class="ltx_bibblock">
Liska R, Shashkov M, Vachal P, Wendroff B. Optimization-based synchronized
flux-corrected conservative interpolation (remapping) of mass and momentum
for arbitrary Lagrangian-Eulerian methods. <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">J Comput Phys.
</span>2010;229(5):1467–1497.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">3</span>
<span class="ltx_bibblock">
Taylor GI, Green AE. Mechanism of the production of small eddies from large
ones. <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">P Roy Soc Lond A Mat. </span>1937;158(895):499–521.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1098/rspa.1937.0036" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1098/rspa.1937.0036</a>,
<a target="_blank" href="http://rspa.royalsocietypublishing.org/content/158/895/499" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://rspa.royalsocietypublishing.org/content/158/895/499</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">4</span>
<span class="ltx_bibblock">
Knupp PM. Winslow smoothing on two-dimensional unstructured meshes. <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Eng
Comput. </span>1999;15:263–268.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">5</span>
<span class="ltx_bibblock">
Kamm J. <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Evaluation of the Sedov-von Neumann-Taylor blast wave
solution. </span> Technical Report LA-UR-00-6055: Los Alamos National
Laboratory; 2000.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">6</span>
<span class="ltx_bibblock">
Kucharik M, Shashkov M, Wendroff B. An efficient linearity-and-bound-preserving
remapping method. <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>2003;188(2):462–471.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">7</span>
<span class="ltx_bibblock">
Blanchard G, Loubere R. <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">High-Order Conservative Remapping with a
posteriori MOOD stabilization on polygonal meshes. </span>
<a target="_blank" href="https://hal.archives-ouvertes.fr/hal-01207156" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hal.archives-ouvertes.fr/hal-01207156</a>, the HAL Open
Archive, hal-01207156. Accessed January 13, 2016; 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">8</span>
<span class="ltx_bibblock">
Burton DE, Kenamond MA, Morgan NR, Carney TC, Shashkov MJ. An intersection
based ALE scheme (xALE) for cell centered hydrodynamics (CCH). In:
Talk at Multimat 2013, International Conference on Numerical
Methods for Multi-Material Fluid Flows; September 2–6, 2013; San
Francisco.

</span>
<span class="ltx_bibblock">LA-UR-13-26756.2.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">9</span>
<span class="ltx_bibblock">
Berndt M, Breil J, Galera S, Kucharik M, Maire PH, Shashkov M. Two-step hybrid
conservative remapping for multimaterial arbitrary Lagrangian-Eulerian
methods. <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>2011;230(17):6664–6687.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">10</span>
<span class="ltx_bibblock">
Kucharik M, Shashkov M. One-step hybrid remapping algorithm for multi-material
arbitrary Lagrangian-Eulerian methods. <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">J Comput Phys.
</span>2012;231(7):2851–2864.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">11</span>
<span class="ltx_bibblock">
Breil J, Alcin H, Maire PH. A swept intersection-based remapping method for
axisymmetric ReALE computation. <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Int J Numer Meth Fl.
</span>2015;77(11):694–706.

</span>
<span class="ltx_bibblock">Fld.3996.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">12</span>
<span class="ltx_bibblock">
Barth TJ. Numerical methods for gasdynamic systems on unstructured meshes. In:
Kroner D, Rohde C, Ohlberger M, eds. <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">An Introduction to Recent
Developments in Theory and Numerics for Conservation Laws,
Proceedings of the International School on Theory and Numerics for
Conservation Laws</span>, Lecture Notes in Computational Science and
Engineering. Berlin: Springer 1997.

</span>
<span class="ltx_bibblock">ISBN 3-540-65081-4.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">13</span>
<span class="ltx_bibblock">
Lauritzen P, Erath C, Mittal R. On simplifying ‘incremental remap’-based
transport schemes. <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>2011;230(22):7957–7963.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">14</span>
<span class="ltx_bibblock">
Klima M, Kucharik M, Shashkov M. Local error analysis and comparison of the
swept- and intersection-based remapping methods. <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Commun Comput
Phys. </span>2017;21(2):526–558.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">15</span>
<span class="ltx_bibblock">
Dukowicz JK, Baumgardner JR. Incremental remapping as a transport/advection
algorithm. <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>2000;160(1):318–335.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">16</span>
<span class="ltx_bibblock">
Kucharik M, Shashkov M. Flux-based approach for conservative remap of
multi-material quantities in 2D arbitrary Lagrangian-Eulerian
simulations. In: Fořt J, Fürst J, Halama J, Herbin R, Hubert F,
eds. <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Finite Volumes for Complex Applications VI Problems &amp;
Perspectives</span>, Springer Proceedings in Mathematics, vol. 1: Springer
2011 (pp. 623–631).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">17</span>
<span class="ltx_bibblock">
Kucharik M, Shashkov M. Conservative multi-material remap for staggered
multi-material arbitrary Lagrangian-Eulerian methods. <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">J Comput
Phys. </span>2014;258:268–304.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">18</span>
<span class="ltx_bibblock">
Loubere R, Shashkov M. A subcell remapping method on staggered polygonal grids
for arbitrary-Lagrangian-Eulerian methods. <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">J Comput Phys.
</span>2005;209(1):105–138.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">19</span>
<span class="ltx_bibblock">
Caramana EJ, Shashkov MJ. Elimination of artificial grid distortion and
hourglass-type motions by means of Lagrangian subzonal masses and
pressures. <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>1998;142(2):521–561.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">20</span>
<span class="ltx_bibblock">
Hoch P. <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">An arbitrary Lagrangian-Eulerian strategy to solve
compressible fluid flows. </span> Technical Report: CEA; 2009.

</span>
<span class="ltx_bibblock">HAL: hal-00366858.
https://hal.archives-ouvertes.fr/docs/00/36/68/58/PDF/ale2d.pdf. Accessed
January 13, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">21</span>
<span class="ltx_bibblock">
Shashkov M. <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Conservative Finite-Difference Methods on General
Grids</span>.

</span>
<span class="ltx_bibblock">Boca Raton, Florida: CRC Press; 1996.

</span>
<span class="ltx_bibblock">ISBN 0-8493-7375-1.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">22</span>
<span class="ltx_bibblock">
Benson DJ. Computational methods in Lagrangian and Eulerian hydrocodes.
<span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Comput Method Appl M. </span>1992;99(2–3):235–394.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">23</span>
<span class="ltx_bibblock">
Margolin LG, Shashkov M. Second-order sign-preserving conservative
interpolation (remapping) on general grids. <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">J Comput Phys.
</span>2003;184(1):266–298.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">24</span>
<span class="ltx_bibblock">
Kenamond MA, Burton DE. Exact intersection remapping of multi-material
domain-decomposed polygonal meshes. In: Talk at Multimat 2013,
International Conference on Numerical Methods for Multi-Material
Fluid Flows; September 2–6, 2013; San Francisco.

</span>
<span class="ltx_bibblock">LA-UR-13-26794.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">25</span>
<span class="ltx_bibblock">
Dukowicz J. Conservative rezoning (remapping) for general quadrilateral meshes.
<span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">J Comput Phys. </span>1984;54(3):411–424.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">26</span>
<span class="ltx_bibblock">
Margolin LG, Shashkov M. <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Second-order sign-preserving remapping on general
grids. </span> Technical Report LA-UR-02-525: Los Alamos National Laboratory;
2002.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">27</span>
<span class="ltx_bibblock">
Mavriplis DJ. Revisiting the least-squares procedure for gradient
reconstruction on unstructured meshes. In: AIAA 2003-3986. 16th AIAA
Computational Fluid Dynamics Conference; June 23–26, 2003; Orlando,
Florida.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">28</span>
<span class="ltx_bibblock">
Scovazzi G, Love E, Shashkov M. Multi-scale Lagrangian shock hydrodynamics on
Q1/P0 finite elements: Theoretical framework and two-dimensional
computations. <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Comput Method Appl M. </span>2008;197(9–12):1056–1079.

</span>
</li>
</ul>
</section>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2010.09257" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2010.09258" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2010.09258">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2010.09258" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2010.09259" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 04:58:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
