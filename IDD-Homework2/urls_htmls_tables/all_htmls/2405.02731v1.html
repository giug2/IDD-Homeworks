<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles</title>
<!--Generated on Wed May 15 19:25:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Connected and Autonomous Vehicles,  Anomaly Detection,  Intrusion Detection System,  Artificial Intelligence.
" lang="en" name="keywords"/>
<base href="/html/2405.02731v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S1" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S2" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S2.SS1" title="In II Background ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Connected and Autonomous Vehicles</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S2.SS2" title="In II Background ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Attack Surfaces</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S2.SS3" title="In II Background ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Artificial Intelligence</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S2.SS4" title="In II Background ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Anomaly Detection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3.SS1" title="In III Methodology ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Search Terms</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3.SS2" title="In III Methodology ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Inclusion/exclusion Criteria</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3.SS3" title="In III Methodology ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Filtering Stages</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3.SS3.SSS1" title="In III-C Filtering Stages ‣ III Methodology ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Inter-rater Reliability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S3.SS3.SSS2" title="In III-C Filtering Stages ‣ III Methodology ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Data Extraction and Management</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS1" title="In IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Summary of Search Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS2" title="In IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">AI Methods used in Anomaly Detection for CAVs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS2.SSS1" title="In IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>Algorithms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS2.SSS2" title="In IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>Application Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS2.SSS3" title="In IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>3 </span>Safety and Security</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS2.SSS4" title="In IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>4 </span>Open-source</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS3" title="In IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Training of Anomaly Detection Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS3.SSS1" title="In IV-C Training of Anomaly Detection Models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span>Data used in Training Anomaly Detection Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS3.SSS2" title="In IV-C Training of Anomaly Detection Models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span>Generation of Anomalies in the Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS3.SSS3" title="In IV-C Training of Anomaly Detection Models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>3 </span>Characteristics of the Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS3.SSS4" title="In IV-C Training of Anomaly Detection Models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>4 </span>Levels of Autonomy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS4" title="In IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Testing and evaluation of anomaly detection models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS4.SSS1" title="In IV-D Testing and evaluation of anomaly detection models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>1 </span>Testing and Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS4.SSS2" title="In IV-D Testing and evaluation of anomaly detection models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>2 </span>Detection Latency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.SS4.SSS3" title="In IV-D Testing and evaluation of anomaly detection models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>3 </span>Case studies</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS1" title="In V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">AI Methods used in Anomaly Detection for CAVs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS2" title="In V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Training of Anomaly Detection Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS2.SSS1" title="In V-B Training of Anomaly Detection Models ‣ V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>1 </span>Creation, maintenance, and standardisation of benchmarking datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS2.SSS2" title="In V-B Training of Anomaly Detection Models ‣ V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>2 </span>Testing and Evaluation of Anomaly Detection Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS3" title="In V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Limitations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S5.SS4" title="In V Discussion ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Recommendations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S6" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S7" title="In Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Acknowledgement</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\addbibresource</span>
<p class="ltx_p" id="p1.2">references.bib</p>
</div>
<h1 class="ltx_title ltx_title_document">Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">John Roar Ventura Solaas, Nilufer Tuptuk, Enrico Mariconti
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This systematic review focuses on anomaly detection for connected and autonomous vehicles. The initial database search identified 2160 articles, of which 203 were included in this review after rigorous screening and assessment. This study revealed that the most commonly used Artificial Intelligence (AI) algorithms employed in anomaly detection are neural networks like LSTM, CNN, and autoencoders, alongside one-class SVM. Most anomaly-based models were trained using real-world operational vehicle data, although anomalies, such as attacks and faults, were often injected artificially into the datasets. These models were evaluated mostly using five key evaluation metrics: recall, accuracy, precision, F1-score, and false positive rate. The most frequently used selection of evaluation metrics used for anomaly detection models were accuracy, precision, recall, and F1-score.</p>
<p class="ltx_p" id="id2.id2">This systematic review presents several recommendations. First, there is a need to incorporate multiple evaluation metrics to provide a comprehensive assessment of the anomaly detection models. Second, only a small proportion of the studies have made their models open source, indicating a need to share models publicly to facilitate collaboration within the research community, and to validate and compare findings effectively. Third, there is a need for benchmarking datasets with predefined anomalies or cyberattacks to test and improve the effectiveness of the proposed anomaly-based detection models. Furthermore, there is a need for future research to investigate the deployment of anomaly detection to a vehicle to assess its performance on the road. There is a notable lack of research done on intrusion detection systems using different protocols to CAN, such as Ethernet and FlexRay.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Connected and Autonomous Vehicles, Anomaly Detection, Intrusion Detection System, Artificial Intelligence.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The last decade have seen unprecedented growth in the technology of Connected and Autonomous Vehicles (CAVs), which is driven by improvements and innovations in Artificial Intelligence (AI) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">grigorescu_survey_2020</span>]</cite> and enabling technology related to sensors and communication systems. CAVs include a variety of internal and external sensors, including cameras, LiDar (Light Detection and Ranging), radar, GNSS/GPS, and infrared sensors, to help them gather data about their surroundings and make important judgments. These vehicles are expected to be connected to other vehicles through Vehicle-to-Vehicle (V2V) communication and to the infrastructure using the Vehicle-to-Infrastructure (V2I) network. By using in-vehicle communication, the vehicle connects the components of the vehicle and enables the exchange of information between different modules and sensors within the vehicle. Machine Learning (ML) including Deep Learning (DL) techniques are used to process large sets of data and enable CAVs to function safely and independently without the human driver. Examples of such techniques include real-time sensor anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_safety_2020</span>]</cite>, detecting faults in the vehicle parts such as battery <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">li_data-driven_2021</span>]</cite> and detecting driver behaviour anomalies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">f_quader_anomaly_2019</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The different levels of vehicle autonomy have been classified into six levels where level 0 is a normal human driver without any assistance, and level 5 is a self-driving car without human supervision <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">spratlin_jr_jr_autonomous_2021</span>]</cite>. Several companies, such as Waymo (formerly known as the Google Self-Driving Car Project), Cruise, TuSimple, and Aurora, are working actively towards developing level 5 autonomous vehicles. Technological advancements, particularly in deep learning, are enabling the development of fully autonomous vehicles. This is done through advanced data analytics that allows vehicles with the capability to process and understand a large volume of complex data from multiple sources, essential for their operation. Deep learning models can analyse sensor data in real-time, identifying objects, pedestrians, and road features, enhancing the vehicle’s ability to navigate safely. Recent developments in deep learning have demonstrated performance that is superior to that of humans <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">chakraborty_adversarial_2018</span>]</cite>, and it is expected that autonomous vehicles will significantly reduce the risk of road users and other vehicles compared to vehicles operated by human drivers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sparrow_when_2017</span>]</cite>.
Other benefits of autonomous vehicles include reducing isolation for people with disabilities or elderly people; improving access to education, work and leisure; and helping deliver essential goods and groceries <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shapps_connected_2022</span>]</cite>. In April 2023, Wayve <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wayve_asda_2023</span>]</cite> teamed up with the supermarket Asda in the UK, launching a year-long trial delivering groceries to 72,000 households in London using autonomous vehicles. Industry efforts like these show that CAVs are becoming a fast reality and they are expected to grow both in popularity and advance in their technology.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">CAVs have become a promising technology for the future of transportation. However, ensuring their safety and security remains a significant challenge. Anomaly detection, which is the ability to identify abnormal behaviour or events, plays an important role in maintaining the safety and security of CAVs. Anomaly detection can be an effective way to secure autonomous vehicles <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbahadur_survey_2018</span>]</cite>. It could be used to detect faults in the vehicle’s hardware and software, dangerous road anomalies, cyber-physical attacks on the vehicle, or unusual driver behaviour. Furthermore, anomaly detection techniques have already been proposed to address the complex task of ensuring both the security and safety of CAVs. AI has emerged as a promising method for detecting anomalies in CAVs due to its ability to efficiently process vast amounts of data and detect patterns that indicate anomalies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">omar_machine_2013</span>]</cite>. By using AI algorithms, anomaly detection models can learn from historical data on normal vehicle operation to recognise abnormal behaviour.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S1.T1" title="TABLE I ‣ I Introduction ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">I</span></a> presents related surveys and reviews relevant to CAVs. These earlier studies lacked a systematic review and did not cover the following aspects: information on model availability, generation of anomalies, dataset characteristics, and evaluation metrics. Taking into consideration these gaps, a systematic review is carried out to examine the current state of the literature on anomaly detection for CAVs in a systematic and structured way.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Overview of reviews and surveys on anomaly detection for connected and autonomous vehicles</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.3.1.1">
<th class="ltx_td ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S1.T1.3.1.1.1" style="width:122.3pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S1.T1.3.1.1.2"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.3.1.1.2.1.1" style="font-size:80%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbahadur_survey_2018</span><span class="ltx_text" id="S1.T1.3.1.1.2.2.2" style="font-size:80%;">]</span></cite></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S1.T1.3.1.1.3"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.3.1.1.3.1.1" style="font-size:80%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">dixit_anomaly_2022</span><span class="ltx_text" id="S1.T1.3.1.1.3.2.2" style="font-size:80%;">]</span></cite></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S1.T1.3.1.1.4"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.3.1.1.4.1.1" style="font-size:80%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">bogdoll_anomaly_2022</span><span class="ltx_text" id="S1.T1.3.1.1.4.2.2" style="font-size:80%;">]</span></cite></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S1.T1.3.1.1.5"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.3.1.1.5.1.1" style="font-size:80%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">baccari_anomaly_2024</span><span class="ltx_text" id="S1.T1.3.1.1.5.2.2" style="font-size:80%;">]</span></cite></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S1.T1.3.1.1.6"><span class="ltx_text" id="S1.T1.3.1.1.6.1" style="font-size:80%;">This review</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.3.2.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.2.1.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.2.1.1.1">
<span class="ltx_p" id="S1.T1.3.2.1.1.1.1"><span class="ltx_text" id="S1.T1.3.2.1.1.1.1.1" style="font-size:80%;">Type</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.2.1.2"><span class="ltx_text" id="S1.T1.3.2.1.2.1" style="font-size:80%;">Survey</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.2.1.3"><span class="ltx_text" id="S1.T1.3.2.1.3.1" style="font-size:80%;">Survey</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.2.1.4"><span class="ltx_text" id="S1.T1.3.2.1.4.1" style="font-size:80%;">Survey</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.2.1.5"><span class="ltx_text" id="S1.T1.3.2.1.5.1" style="font-size:80%;">Survey</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.2.1.6"><span class="ltx_text" id="S1.T1.3.2.1.6.1" style="font-size:80%;">Systematic review</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.3.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.3.2.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.3.2.1.1">
<span class="ltx_p" id="S1.T1.3.3.2.1.1.1"><span class="ltx_text" id="S1.T1.3.3.2.1.1.1.1" style="font-size:80%;">Years</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.3.2.2"><span class="ltx_text" id="S1.T1.3.3.2.2.1" style="font-size:80%;">Early 2000’s–2018</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.3.2.3"><span class="ltx_text" id="S1.T1.3.3.2.3.1" style="font-size:80%;">2016–2020</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.3.2.4"><span class="ltx_text" id="S1.T1.3.3.2.4.1" style="font-size:80%;">2015–2022</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.3.2.5"><span class="ltx_text" id="S1.T1.3.3.2.5.1" style="font-size:80%;">2019–2023</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.3.2.6"><span class="ltx_text" id="S1.T1.3.3.2.6.1" style="font-size:80%;">2013–2023</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.4.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.4.3.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.4.3.1.1">
<span class="ltx_p" id="S1.T1.3.4.3.1.1.1"><span class="ltx_text" id="S1.T1.3.4.3.1.1.1.1" style="font-size:80%;">Attack surface</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.4.3.2"><span class="ltx_text" id="S1.T1.3.4.3.2.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.4.3.3"><span class="ltx_text" id="S1.T1.3.4.3.3.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.4.3.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.4.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.4.3.6"><span class="ltx_text" id="S1.T1.3.4.3.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.5.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.5.4.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.5.4.1.1">
<span class="ltx_p" id="S1.T1.3.5.4.1.1.1"><span class="ltx_text" id="S1.T1.3.5.4.1.1.1.1" style="font-size:80%;">Data source</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.5.4.2"><span class="ltx_text" id="S1.T1.3.5.4.2.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.5.4.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.5.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.5.4.5"><span class="ltx_text" id="S1.T1.3.5.4.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.5.4.6"><span class="ltx_text" id="S1.T1.3.5.4.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.6.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.6.5.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.6.5.1.1">
<span class="ltx_p" id="S1.T1.3.6.5.1.1.1"><span class="ltx_text" id="S1.T1.3.6.5.1.1.1.1" style="font-size:80%;">Application targeted</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.6.5.2"><span class="ltx_text" id="S1.T1.3.6.5.2.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.6.5.3"><span class="ltx_text" id="S1.T1.3.6.5.3.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.6.5.4"><span class="ltx_text" id="S1.T1.3.6.5.4.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.6.5.5"><span class="ltx_text" id="S1.T1.3.6.5.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.6.5.6"><span class="ltx_text" id="S1.T1.3.6.5.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.7.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.7.6.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.7.6.1.1">
<span class="ltx_p" id="S1.T1.3.7.6.1.1.1"><span class="ltx_text" id="S1.T1.3.7.6.1.1.1.1" style="font-size:80%;">Dataset used</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.7.6.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.7.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.7.6.4"><span class="ltx_text" id="S1.T1.3.7.6.4.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.7.6.5"><span class="ltx_text" id="S1.T1.3.7.6.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.7.6.6"><span class="ltx_text" id="S1.T1.3.7.6.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.8.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.8.7.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.8.7.1.1">
<span class="ltx_p" id="S1.T1.3.8.7.1.1.1"><span class="ltx_text" id="S1.T1.3.8.7.1.1.1.1" style="font-size:80%;">Dataset characteristics</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.8.7.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.8.7.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.8.7.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.8.7.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.8.7.6"><span class="ltx_text" id="S1.T1.3.8.7.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.9.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.9.8.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.9.8.1.1">
<span class="ltx_p" id="S1.T1.3.9.8.1.1.1"><span class="ltx_text" id="S1.T1.3.9.8.1.1.1.1" style="font-size:80%;">Simulation method (data generation)</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.9.8.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.9.8.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.9.8.4"><span class="ltx_text" id="S1.T1.3.9.8.4.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.9.8.5"><span class="ltx_text" id="S1.T1.3.9.8.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.9.8.6"><span class="ltx_text" id="S1.T1.3.9.8.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.10.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.10.9.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.10.9.1.1">
<span class="ltx_p" id="S1.T1.3.10.9.1.1.1"><span class="ltx_text" id="S1.T1.3.10.9.1.1.1.1" style="font-size:80%;">Detection technique</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.10.9.2"><span class="ltx_text" id="S1.T1.3.10.9.2.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.10.9.3"><span class="ltx_text" id="S1.T1.3.10.9.3.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.10.9.4"><span class="ltx_text" id="S1.T1.3.10.9.4.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.10.9.5"><span class="ltx_text" id="S1.T1.3.10.9.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.10.9.6"><span class="ltx_text" id="S1.T1.3.10.9.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.11.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.11.10.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.11.10.1.1">
<span class="ltx_p" id="S1.T1.3.11.10.1.1.1"><span class="ltx_text" id="S1.T1.3.11.10.1.1.1.1" style="font-size:80%;">Security or safety-focused</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.11.10.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.11.10.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.11.10.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.11.10.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.11.10.6"><span class="ltx_text" id="S1.T1.3.11.10.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.12.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.12.11.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.12.11.1.1">
<span class="ltx_p" id="S1.T1.3.12.11.1.1.1"><span class="ltx_text" id="S1.T1.3.12.11.1.1.1.1" style="font-size:80%;">Model availability</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.12.11.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.12.11.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.12.11.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.12.11.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.12.11.6"><span class="ltx_text" id="S1.T1.3.12.11.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.13.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.13.12.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.13.12.1.1">
<span class="ltx_p" id="S1.T1.3.13.12.1.1.1"><span class="ltx_text" id="S1.T1.3.13.12.1.1.1.1" style="font-size:80%;">Anomaly generation</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.13.12.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.13.12.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.13.12.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.13.12.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.13.12.6"><span class="ltx_text" id="S1.T1.3.13.12.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.14.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.14.13.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.14.13.1.1">
<span class="ltx_p" id="S1.T1.3.14.13.1.1.1"><span class="ltx_text" id="S1.T1.3.14.13.1.1.1.1" style="font-size:80%;">Evaluation metrics</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.14.13.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.14.13.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.14.13.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.14.13.5"><span class="ltx_text" id="S1.T1.3.14.13.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.3.14.13.6"><span class="ltx_text" id="S1.T1.3.14.13.6.1" style="font-size:80%;">X</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.15.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S1.T1.3.15.14.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.15.14.1.1">
<span class="ltx_p" id="S1.T1.3.15.14.1.1.1"><span class="ltx_text" id="S1.T1.3.15.14.1.1.1.1" style="font-size:80%;">Anomaly types detected</span></span>
</span>
</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.15.14.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.3.15.14.3"><span class="ltx_text" id="S1.T1.3.15.14.3.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.15.14.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S1.T1.3.15.14.5"></td>
<td class="ltx_td ltx_border_t" id="S1.T1.3.15.14.6"></td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.16.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.3.16.15.1" style="width:122.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.3.16.15.1.1">
<span class="ltx_p" id="S1.T1.3.16.15.1.1.1"><span class="ltx_text" id="S1.T1.3.16.15.1.1.1.1" style="font-size:80%;">Scientific method</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.3.16.15.2"><span class="ltx_text" id="S1.T1.3.16.15.2.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.3.16.15.3"><span class="ltx_text" id="S1.T1.3.16.15.3.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.3.16.15.4"><span class="ltx_text" id="S1.T1.3.16.15.4.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.3.16.15.5"><span class="ltx_text" id="S1.T1.3.16.15.5.1" style="font-size:80%;">X</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.3.16.15.6"><span class="ltx_text" id="S1.T1.3.16.15.6.1" style="font-size:80%;">X</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Conducting a systematic review of anomaly detection for CAVs is important for several reasons. First, the field is rapidly evolving, with advancements and new research being published regularly. A systematic review will ensure that the latest findings are included, thereby providing a comprehensive and contemporary overview of the literature. Moreover, the systematic approach enables us to critically evaluate and synthesise the existing research rigorously to minimise bias <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">delgado-rodriguez_systematic_2018</span>]</cite>. By employing predefined inclusion and exclusion criteria, it is possible to systematically identify and select relevant studies from diverse sources. Lastly, a systematic review allows for the identification of trends, patterns, and gaps in the current literature <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">gotz_supporting_2018</span>]</cite>. By analysing the various methods used for anomaly detection, the training procedures employed, and the evaluation methodologies utilised, it is possible to gain a comprehensive understanding of the strengths and limitations of existing approaches. This knowledge can serve as a foundation for future research directions and inform the development of more transparent and robust anomaly detection techniques.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This systematic review aims to analyse the existing literature on anomaly detection for CAVs focusing on exploring the various methods employed for anomaly detection, the training procedures for detection models, and the evaluation methodologies. This review aims to provide a comprehensive understanding of the current state of anomaly detection for CAVs by answering the following research questions:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">RQ1: What are the different AI methods used to detect anomalies?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">RQ2: How are anomaly detection models for CAVs trained?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">RQ3: How are anomaly detection for CAVs tested and evaluated?</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Connected and Autonomous Vehicles</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">CAVs have emerged as a transformative technology, gradually replacing human drivers to varying extents in the operation of vehicles <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shladover_connected_2018</span>]</cite>. The advent of automated driving systems can be traced back to the early 20th century when initial technological functionalities, such as autonomous speed, brake, lane control, and basic cruise control capabilities, were introduced <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shladover_impacts_2012</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">anderson_autonomous_2014</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">arnaout_exploring_2014</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">pendleton_perception_2017</span>]</cite>. Furthermore, over the past decade, there has been an unprecedented surge in technological advancements, leading to the testing of numerous prototype CAVs on public roads <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">christie_pioneering_2016</span>]</cite>. Consequently, CAVs are widely regarded as the epitome of future automotive engineering <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_real_2020</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">CAVs are different from traditional vehicles in several aspects. CAVs have a higher number of equipped sensors to create a perception of the vehicle’s surroundings. Radar, cameras, and LiDar sensors on the CAV are in charge of perceiving the vehicle’s dynamics (such as location and speed) as well as its immediate environment (such as distances to other vehicles, traffic conditions, and traffic signals) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang_vehicle_2014</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhao_distance_2015</span>]</cite>. This data is processed by the onboard computer, which then issues commands to the Electronic Control Units (ECUs), which in turn drive the relevant actuators to produce the required movement speed and direction. The Controller Area Network (CAN) bus system enables the communication between the in-vehicle network’s actuators, external sensors, ECUs, and the onboard computer (also called the onboard network). CAVs also frequently employ the Global Navigation Satellite System (GNSS) such as Global Positioning System (GPS) to provide precise location data.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">According to the Society of Automotive Engineers (SAE), vehicles are categorised into six levels of autonomy <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sae_international_taxonomy_nodate</span>]</cite>. The six levels, which range from 0 (no autonomous feature) to 5 (completely self-driving vehicle), can be thought of as a progression of self-driving features <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cui_review_2019</span>]</cite>. Level 0 has no automation and completely puts the driver in charge. At Level 1, the vehicle may notify the driver of problems and circumstances using smart sensors. Level 2 automation allows the car to carry out some assistance tasks, but the driver retains control. Nominal autonomy is Level 3, where the majority of safety-critical operations can be carried out by the vehicle under recognised circumstances, but the driver must be prepared to take over. At level 4, also known as high automation, the vehicle is capable of performing all safety-critical driving tasks in constrained spaces without human intervention. Level 5 is the ultimate step of autonomy. At this point, the car is capable of moving under any conditions without a human driver, and the car does not require a steering wheel or a brake pedal.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The new generation of information and communication technologies that connect vehicles to everything is Vehicle-to-Everything (V2X) communication <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_survey_2019</span>]</cite>. V2X communication encapsulates diverse communication modalities, including communication with infrastructure, denominated as Vehicle-to-Infrastructure (V2I); communication with peer vehicles, designated as Vehicle-to-Vehicle (V2V); communication with pedestrians, termed Vehicle-to-Pedestrian (V2P); connections with network systems or cloud-based services, recognised as Vehicle-to-Network (V2N) and Vehicle-to-Cloud (V2C), respectively. Furthermore, internal communication within the vehicular framework is subsumed within the V2X paradigm, encompassing all intra-vehicular components such as sensors, LiDAR systems, cameras, peripheral devices, and the onboard computational unit. Specifically, the rubric of Vehicle-to-Grid (V2G) communication pertains to the communication occurring between Electric Vehicles (EVs) and the electric grid infrastructure, facilitating not only energy consumption for EV charging but also enabling surplus energy discharge into the grid. Also, the domain of Vehicle-to-Device (V2D) communication encompasses the interaction between vehicles and an array of external devices or cloud-hosted services. Essentially, V2X is the communication that occurs external to the vehicle as well as in-vehicle communication. This communication is essential for the proper operation of CAVs, however, reliance on these communication channels also exposes the vehicle to security attacks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Attack Surfaces</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The expanding network communication infrastructure surrounding CAVs increases their vulnerability to security threats, as each connection point represents a potential entry point for attackers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cui_review_2019</span>]</cite>. Potential attackers could exploit vulnerabilities within the V2X network through various connection points, including links within the controller network connecting the CAN bus with ECUs, interconnections among ECUs, connections from ECUs to actuators, and even targeting internal sensors and actuators themselves, highlighting the vulnerabilities in in-vehicle communication. In contemporary vehicles, the proliferation of ECUs, ranging from 70 to 100 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">jadhav_survey_2018</span>]</cite>, in contrast to only two ECUs in the 1980s <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">schmidgall_automotive_2012</span>]</cite>, has significantly escalated the attack surface. Furthermore, CAVs are exposed to heightened risk due to their diverse onboard computer connections, encompassing both wireless interfaces like WiFi for external devices and physical connections like Ethernet and USB, extending to sensors, dashboards, and externally introduced devices. These extended connection points lack robust security measures, rendering CAVs susceptible to various forms of cyberattacks, thereby attracting potential malevolent actors seeking to exploit these vulnerabilities to steal personal data, inflict damage to the property and environment, or cause bodily injury <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">pham_survey_2021</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To address the expanded attack surface in CAVs, several authors have attempted to address security requirements, which are, in essence, an expansion and modification of the Confidentiality, Integrity, and Availability (CIA) triangle. Confidentiality is a principle concerned about the secrecy of information and inaccessibility to unauthorised actors; integrity ensures that data remains trustworthy and accurate; and availability ensures that information is accessible when needed. The CIA triangle is a fundamental framework for designing and evaluating information security measures. The security requirements of CAVs; vehicular ad hoc networks (VANETs), which are wireless networks formed by vehicles and roadside infrastructure for improved road safety and traffic management through V2V and V2I communication; and Intelligent Transportation Systems (ITS), which are communication systems used to enhance the safety, efficiency, and sustainability of transportation networks by improving traffic management, providing real-time information to travellers, and optimising infrastructure utilisation, can be categorised into four subcategories <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">malla_security_2013</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">engoulou_vanet_2014</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">manvi_survey_2017</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">othmane_survey_2015</span>]</cite>:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Authenticity/identification: It is necessary to guarantee the identity of the vehicle driver, the data source, and the vehicle’s position. To stop attacks involving fabricated entities, user authentication is first required. Second, data source authenticity is crucial to determining whether a valid company produced the data. Third, location authenticity is employed to guarantee the accuracy of location information collected through GPS sensors and other vehicles.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Availability: Information sent or shared, services, and functionality must be processed and made readily available in real-time.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Data integrity: Data must be received in the correct form without being tampered with, altered, or deleted inadvertently or maliciously during transmission.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Confidentiality: Exchanged data should not be accessible to harmful or unauthorised users and should only be exposed to authorised and legitimate users.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Artificial Intelligence</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">AI can play an important role in enhancing the protection of CAVs <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">koopman_autonomous_2017</span>]</cite>. It can aid in improving the decision-making process of CAVs, enabling them to make real-time, informed choices based on multiple diverse data inputs and evolving road conditions, ultimately increasing the safety and reliability of autonomous driving systems. With the complexity and variability of real-world driving scenarios, AI, like deep learning algorithms can analyse vast amounts of data collected by sensors, cameras, and other sources in CAVs to identify patterns and detect potential risks or anomalies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">bogdoll_anomaly_2022</span>]</cite>. By using AI techniques, CAVs can learn from historical data and adapt their behaviour to different situations, improving their ability to anticipate and respond to potential hazards. AI algorithms can also help develop robust anomaly detection systems to identify and mitigate malicious attacks or unauthorised access attempts, ensuring the security and integrity of the vehicle’s operation <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">f_van_wyk_real-time_2020</span>]</cite>. Two of the subsections in AI that are used to build anomaly detection models in CAVs are ML and DL.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">ML has become a widely employed method for building models that can learn complex relationships within datasets <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">morales_brief_2022</span>]</cite>. Various approaches can be employed to generate such models, and ML can be broadly classified into three branches: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning is particularly effective when working with labelled data points, allowing for predictive modelling. On the other hand, unsupervised learning techniques are employed to analyse and group datasets without labels, uncovering underlying patterns and structures. Lastly, reinforcement learning focuses on planning and environment control, emphasising the selection of actions that maximise rewards in specific situations. CAVs leverage these algorithms to make predictions and informed decisions regarding driving actions.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">DL models use artificial neural networks with multiple layers, referred to as deep neural networks, to process and interpret complex sensor data. In the context of CAVs, DL is instrumental in several critical aspects. Firstly, it facilitates perception, allowing CAVs to accurately detect and identify objects, pedestrians, road signs, and lane boundaries from data collected by cameras, LiDAR, radar, and other sensors <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">a_ranjbar_safety_2022</span>]</cite>. Secondly, DL enables sophisticated decision-making by incorporating reinforcement learning techniques, which enable CAVs to navigate complex traffic scenarios, make safe lane changes, and respond to dynamic road conditions <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">g_basile_ddpg_2022</span>]</cite>. Additionally, DL is important in mapping and localisation, enabling CAVs to create high-definition maps of their environment and precisely determine their position on the road <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">he_exploring_2020</span>]</cite>. The adaptability and scalability of deep learning models are of paramount importance in the evolution of CAVs, as they can be continuously improved and updated to handle evolving real-world driving scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">Anomaly Detection</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Anomaly detection in the form of monitoring for faults and cyber-physical attacks is important to maintain a high level of security and safety for CAVs. Anomaly detection is not exclusively used to identify cyberattacks—it may also be used to identify components that become defective because of normal usage over time or human error. However, the term intrusion detection system (IDS) is commonly used to refer to anomaly detection used to detect cyber attacks<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zizzo_machine_2021</span>]</cite>. As the attack surface of CAVs expands due to increased interconnectedness with other vehicles and infrastructure, improving their security becomes more necessary than ever to cover the potential points of vulnerabilities. IDSs utilise various techniques, such as signature-based detection, and prediction-based anomaly detection, to identify patterns that indicate potential intrusion attempts or malicious behaviour.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Signature-based IDS is one of the simplest systems for this purpose and is designed to compare the incoming data traffic to a database with known attacks. In this system, an alert will occur when incoming data matches the already stored known attacks. This approach uses the method of blacklisting. Another alternative is a system using a signature-based IDS with a whitelist method. This method only accepts information that corresponds to known benign examples <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">kumar_signature_2012</span>]</cite>. However, the use of signature-based IDS has the drawback of being inflexible—and not capable of detecting unknown attacks. An attacker can bypass the blacklist by making a modest tweak to an attack, but whitelist modes are only useful for smaller systems with specific behaviour requirements.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">A prediction-based anomaly detection system could be an alternative to signature-based IDS. In this approach, signatures will not be created, but a model of data dynamics gathered from a system could be generated. Subsequently, by using statistical techniques, it is possible to find unexpected deviations in the data. More specifically, a future prediction of the system value is computed and compared to the actual observed data. An indicator of whether the system is in an abnormal state is the difference between these two values. This approach, which uses prediction-based methods to monitor features like sensor input and control commands, is used in several papers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">zizzo_machine_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kravchik_detecting_2018</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kravchik_efficient_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">sapkota_falcon_2020</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">To evaluate a machine learning model, several evaluation metrics can be used <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">handelman_peering_2018</span>]</cite>. A confusion matrix is a table that provides a comprehensive view of the performance of a classification model. In the matrix, four classes are highlighted: true positive (TP), false negative (FN), false positive (FP), and true negative (TN). Based on the information provided in the confusion metrics, widely used metrics like accuracy, recall, precision, and F1-score can be computed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This systematic review was conducted using the PRISMA protocol <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">moher_preferred_2015</span>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Search Terms</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In September 2023, an exhaustive literature search was conducted across multiple scholarly databases, including Web of Science, ProQuest, Scopus, IEEE Xplore Digital Library, and ACM Digital Library. The search was restricted to articles published between January 2013 and September 2023 to capture the latest decade of research on the topic. To achieve a balance between sensitivity and specificity in the literature review process, various search terms were piloted and refined. Multiple iterations were carried out, with 100
100 articles for each search iteration. Ultimately, the search term that yielded the highest number of relevant articles was identified using a keyword search with the following keywords: 
<br class="ltx_break"/>
<br class="ltx_break"/>”vehicle*” AND ”anomaly detection”</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Inclusion/exclusion Criteria</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To answer the research questions, and ensure a comprehensive and reliable review, were set to guide the identification and selection of relevant academic literature. The search was restricted to peer-reviewed international conference papers and journal articles. Conversely, articles that were published in magazines or newspapers or those that were behind a paywall that our institution did not have access to were excluded from the review. Additionally, exclusion criteria proposed in Edanz-Learning-Team <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">edanz-learning-team_understanding_2022</span>]</cite> and Meline <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">meline_selecting_2006</span>]</cite> were employed to eliminate any articles that did not meet the criteria for inclusion:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Issues with methodological quality</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Review articles with no original data</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Works which are not relevant to the research question and outcomes</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Sources in languages other than English</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Furthermore, specific criteria have been established to narrow the relevance of the articles. These exclusion criteria are:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Published before 2013</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">The main subject is unmanned aerial vehicles (UAV), military/naval systems, air vehicles, rail vehicles or non-ground vehicles.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">The anomaly detection model is built on supervised models.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Filtering Stages</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Following the initial literature search, a process of removing duplication was conducted using the Zotero software, which identified and eliminated duplicate entries. The remaining articles were then screened using Rayyan, a tool for systematic reviews, to assess conformity with the basic inclusion and exclusion criteria. This tool was also used to remove duplicated articles that were previously not identified using Zotero.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.4.1.1">III-C</span>1 </span>Inter-rater Reliability</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">In the screening stage, identified citations and abstracts were imported to Rayyan, and duplicates were removed. Two researchers have separately read the titles and abstracts of 100 random samples of the identified papers to assess whether they meet the inclusion criteria and to assess inter-rater reliability (IRR) and mitigate coder drift <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">ratajczyk_challenges_2016</span>]</cite>. IRR was assessed using the prevalence- and bias-adjusted kappa (PABAK) statistic, which controls for chance agreement <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">byrt_bias_1993</span>]</cite>. As a result of the screening and the calculation, the PABAK score of 0.89 indicated high inter-rater agreement (see <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cumpston_updated_2019</span>]</cite>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.4.1.1">III-C</span>2 </span>Data Extraction and Management</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">A proforma was created to extract information from each study, ensuring that relevant information was captured <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">cumpston_updated_2019</span>]</cite>. The proforma was piloted on a sample of articles to validate the span of captured data. The proforma captured the following information categorised to each of the research questions:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<ol class="ltx_enumerate" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">What are the different AI methods used to detect anomalies in CAVs?</p>
</div>
<div class="ltx_para" id="S3.I3.i1.p2">
<ul class="ltx_itemize" id="S3.I3.i1.I1">
<li class="ltx_item" id="S3.I3.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.I1.i1.p1">
<p class="ltx_p" id="S3.I3.i1.I1.i1.p1.1">Algorithm used in anomaly detection model</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.I1.i2.p1">
<p class="ltx_p" id="S3.I3.i1.I1.i2.p1.1">Application domain of the model</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.I1.i3.p1">
<p class="ltx_p" id="S3.I3.i1.I1.i3.p1.1">Is the method safety or security focused</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.I1.i4.p1">
<p class="ltx_p" id="S3.I3.i1.I1.i4.p1.1">Open source or not</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">How are anomaly detection for CAVs trained?</p>
<ul class="ltx_itemize" id="S3.I3.i2.I1">
<li class="ltx_item" id="S3.I3.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.I1.i1.p1">
<p class="ltx_p" id="S3.I3.i2.I1.i1.p1.1">Data used in training anomaly detection models</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.I1.i2.p1">
<p class="ltx_p" id="S3.I3.i2.I1.i2.p1.1">Generation of anomalies in the data</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.I1.i3.p1">
<p class="ltx_p" id="S3.I3.i2.I1.i3.p1.1">Size and date of collection</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.I1.i4.p1">
<p class="ltx_p" id="S3.I3.i2.I1.i4.p1.1">Is the data collected for specific levels of autonomy</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1">How can anomaly detection for CAVs be tested and evaluated?</p>
<ul class="ltx_itemize" id="S3.I3.i3.I1">
<li class="ltx_item" id="S3.I3.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.I1.i1.p1">
<p class="ltx_p" id="S3.I3.i3.I1.i1.p1.1">Metrics used to test and evaluate the models</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.I1.i2.p1">
<p class="ltx_p" id="S3.I3.i3.I1.i2.p1.1">Detection latency</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Summary of Search Results</span>
</h3>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="328" id="S4.F1.g1" src="extracted/2405.02731v1/Figures/Systematicreview.drawio.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>PRISMA flow diagram of the identification, screening, and inclusion of studies in the review</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The initial database search yielded 2160 articles (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.F1" title="Figure 1 ‣ IV-A Summary of Search Results ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">1</span></a>). In the first stage, 790 duplicates were identified and removed. In the screening stage, 1370 articles were screened based on titles and articles. Of these, 977 articles were removed. These articles were excluded from the full-text assessment because they focused on unmanned aerial vehicles, naval systems, traffic surveillance, spacecraft, railway systems, launch vehicles, or charging stations. In addition, articles were removed because of no relevance to CAVs. After the screening of titles and abstracts, 393 articles remained for full-text assessment. Of these, 190 articles were excluded due to the same reasons as the previous step, such as lack of access, non-English language, or irrelevance to the study topic. In the end, 203 articles were included in this review.
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The collected data can be found at: https://github.com/JRoarVS/ADSCAVs</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">AI Methods used in Anomaly Detection for CAVs</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.4.1.1">IV-B</span>1 </span>Algorithms</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">This section will explore the first research question: What are the various AI methods used to detect anomalies?</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">The analysis revealed the prevalence of several prominent algorithms across the selected articles. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.F2" title="Figure 2 ‣ IV-B1 Algorithms ‣ IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the 20 most frequently used methods in the reviewed studies. An overview of datasets commonly used in anomaly detection models, including details on the algorithms implemented and the best-performing models for each dataset, can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.T2" title="TABLE II ‣ IV-B4 Open-source ‣ IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">II</span></a>. All datasets listed in this overview are public. Furthermore, this section highlights the top five AI algorithms most frequently employed, excluding traditional statistics-based methods:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Long Short-Term Memory (LSTM): is a type of Recurrent Neural Network (RNN) that is particularly effective in modelling sequential data <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">graves_supervised_2012</span>]</cite>, which was used in 41 papers. LSTM’s distinguishing feature lies in its ability to capture intricate, long-range dependencies and to preserve contextual information, rendering it robust for detecting anomalies within the dynamic, time-series data typically emanating from autonomous vehicles. In practical application, LSTM constructs predictive models that are trained to learn the anticipated data patterns. Outliers from these learned patterns are subsequently identified and flagged as anomalies.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Convolutional Neural Network (CNN): was the second most commonly employed algorithm in the reviewed articles, with a count of 21. CNNs are primarily known for their performance in computer vision tasks, as they effectively extract features from images <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">oshea_introduction_2015</span>]</cite>. In CAVs, CNNs are deployed to analyse visual data, notably images derived from onboard cameras. These networks, underpinned by convolutional layers, are adept at identifying abnormal visual patterns or objects that may signify potential anomalies.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Autoencoder: was the third most prevalent algorithm in the reviewed studies, which was used in 13 articles. Autoencoders, representing unsupervised neural networks, are designed to reconstruct input data by learning a compressed representation of the input data. By training an autoencoder on normal operating conditions, any deviations from the learned representation can be interpreted as anomalies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">g_slavic_kalman_2023</span>]</cite>. The core principle underpinning their operation is the capacity to encode data into a compact latent representation and subsequently decode it back to its original form. Anomalies come to light when the reconstructed data exhibits disparities from the expected input.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Deep Learning: was the fourth most used algorithm. This category includes models rooted in deep learning principles <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lecun_deep_2015</span>]</cite> without specifying a particular algorithm. Deep learning constitutes a subset of machine learning algorithms employing multi-layered neural networks, known as deep neural networks. These networks process data by incorporating data features and utilising multiple layers of processing to represent the data. Deep learning was used in 10 papers.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">One-Class Support Vector Machine: was used in 9 papers. This algorithm is designed to identify a decision boundary that effectively segregates normal data instances from anomalies <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yin_fault_2014</span>]</cite>. One-class SVMs, primarily trained on normal data, possess the ability to establish a hypersphere or hyperplane encapsulating typical data points. Any data instances that deviate beyond this designated hypersphere are categorised as anomalies. These algorithms prove especially efficient when anomalies within the dataset are sparse in proportion.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Algorithms used in anomaly detection for vehicles </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.4.1.1">IV-B</span>2 </span>Application Domain</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In terms of the application domain, the analysis revealed the prevalence of several prominent domains in which anomaly detection techniques were applied. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.F3" title="Figure 3 ‣ IV-B2 Application Domain ‣ IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of the 20 most researched domains covered in the reviewed studies. In this subsection, the focus is on presenting the top five domains that garnered the most attention:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">CAN (Controller Area Network) Bus: was the most frequently studied domain for anomaly detection, with a significant focus observed in 78 articles. The CAN bus network serves as a vital communication backbone within vehicles, enabling ECUs to exchange data necessary for controlling various vehicle systems such as brakes, steering, lighting, and more.
<cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hpl_introduction_2002</span>]</cite>. Anomaly detection in the CAN bus network involves monitoring and analysing the communication traffic, detecting abnormal traffic patterns, and identifying potential security threats or malfunctions.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Vehicle Sensors: was the second most frequently studied field for anomaly detection, with 27 papers. This category encompasses sensor readings from the vehicle’s internal components in the form of time series data. Automotive vehicles contain many different types of sensors installed on a vehicle, such as sensors measuring temperature, Revolutions Per Minute (RPM), speed, acceleration, air quality, and fuel level. Vehicle sensors differs from what is recorded as environment sensors (see Fig. 3), which was the application domain for 5 papers. The category environment sensors encompass models that use data from sensors that relate to the vehicle’s perception of its environmental surroundings.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Image: involves the utilisation of visual data from onboard cameras and sensors to understand the vehicle’s surroundings, contributing to tasks like object detection. Anomaly detection using this type of data allows CAVs to identify irregular or unexpected patterns, objects, or events within the visual systems of the vehicle <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">t_p_kapusi_deep_2022</span>]</cite>. 26 articles focused on this domain.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Internet of Vehicles (IoV): was the fifth most prevalent domain in the reviewed studies, with a count of 11 articles. Anomaly detection in this domain is concerned with identifying abnormal behaviours or events in the interconnected vehicular network to ensure the safety, security, and efficiency of the transportation system <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang_overview_2014</span>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">Lane Detection: encompasses anomaly detection models that focus on detecting anomalous lane driving behaviour through the camera. This category uses the image domain to detect anomalies, but specifically focuses on lane detection. 7 papers focused on anomaly detection for lane abnormalities.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Top 20 applications domains in anomaly detection models for CAVs</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS3.4.1.1">IV-B</span>3 </span>Safety and Security</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">The analysis of the selected articles revealed that out of the total 203 articles, 102 articles specifically emphasised security, 64 articles focused on safety, and 36 articles addressed both safety and security aspects. Data for this section was recorded based on the paper’s primary focus. Safety and security are two critical dimensions that require attention in the context of CAVs. Although there may be some overlap between the two, it is important to distinguish between safety and security concerns in this domain. In an information technology context, safety can be described as the inability of the system to affect its environment and security can be defined as the inability of the environment to affect the system <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">line_safety_2006</span>]</cite>:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">This review treats safety as ensuring the physical well-being of occupants, pedestrians, other road users, and vehicles. Safety measures aim to minimise the risk of accidents, injuries, and fatalities caused by the operation of CAVs. Anomaly detection techniques focused on safety aim to identify deviations, malfunctions, or abnormal behaviours that may compromise the vehicle’s ability to navigate, respond to hazards, or adhere to traffic rules.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Security, on the other hand, focusses on protecting CAVs and their associated infrastructure from malicious attacks such as unauthorised access and data breaches. Anomaly detection techniques focused on security aim to identify abnormal network behaviours, intrusions, cyber threats, and privacy breaches that may compromise the integrity, availability, or confidentiality of the vehicle’s systems or the data it generates. Ensuring security in CAVs involves implementing measures such as authentication, encryption, access control, intrusion detection, and secure communication protocols.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS4.4.1.1">IV-B</span>4 </span>Open-source</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">In the analysis of the reviewed articles, only nine studies made their models publicly available on online accessible platforms, such as on GitHub (see <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">stocco_misbehaviour_2020</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">l_yang_mth-ids_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">d_bogdoll_multimodal_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">a_k_desta_mlids_2020</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">d_stabili_daga_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">agrawal_novelads_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">peralta_outlier_2023</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">di_biase_pixel-wise_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang_bit_2023</span>]</cite>). Most studies collected their data to construct datasets through simulation or real vehicles.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Overview of datasets used in anomaly detection models</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.1.1.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.1.1.1.1">
<span class="ltx_p" id="S4.T2.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.1.1.1.1" style="font-size:70%;">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r" id="S4.T2.3.1.1.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.1.1.2.1">
<span class="ltx_p" id="S4.T2.3.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.2.1.1.1" style="font-size:70%;">Algorithm used</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column" id="S4.T2.3.1.1.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.1.1.3.1">
<span class="ltx_p" id="S4.T2.3.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.3.1.1.1" style="font-size:70%;">Models with highest F1-score</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.2.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.2.1.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.2.1.1.1">
<span class="ltx_p" id="S4.T2.3.2.1.1.1.1"><span class="ltx_text" id="S4.T2.3.2.1.1.1.1.1" style="font-size:70%;">Car-Hacking Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.2.1.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">song_-vehicle_2020</span><span class="ltx_text" id="S4.T2.3.2.1.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.2.1.1.1.1.4" style="font-size:70%;"> — CAN dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.2.1.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.2.1.2.1">
<span class="ltx_enumerate" id="S4.I3">
<span class="ltx_item" id="S4.I3.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I3.i1.p1">
<span class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text" id="S4.I3.i1.p1.1.1" style="font-size:70%;">Long Short-Term Memory </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">p_balaji_canlite_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">agrawal_novelads_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">song_self-supervised_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">balaji_neurocan_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span><span class="ltx_text" id="S4.I3.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I3.i2.p1">
<span class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text" id="S4.I3.i2.p1.1.1" style="font-size:70%;">Genetic Algorithm </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">aksu_mga-ids_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">fenzl_-vehicle_2021</span><span class="ltx_text" id="S4.I3.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I3.i3.p1">
<span class="ltx_p" id="S4.I3.i3.p1.1"><span class="ltx_text" id="S4.I3.i3.p1.1.1" style="font-size:70%;">Spiking Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">y_jaoudi_conversion_2020</span><span class="ltx_text" id="S4.I3.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I3.i4.p1">
<span class="ltx_p" id="S4.I3.i4.p1.1"><span class="ltx_text" id="S4.I3.i4.p1.1.1" style="font-size:70%;">Graph Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao_robust_2023</span><span class="ltx_text" id="S4.I3.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i4.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="S4.I3.i5.p1">
<span class="ltx_p" id="S4.I3.i5.p1.1"><span class="ltx_text" id="S4.I3.i5.p1.1.1" style="font-size:70%;">Statistics-based </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i5.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">khan_intrusion_2023</span><span class="ltx_text" id="S4.I3.i5.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i5.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">6.</span>
<span class="ltx_para" id="S4.I3.i6.p1">
<span class="ltx_p" id="S4.I3.i6.p1.1"><span class="ltx_text" id="S4.I3.i6.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i6.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">hade_detection_2022</span><span class="ltx_text" id="S4.I3.i6.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i6.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i7" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">7.</span>
<span class="ltx_para" id="S4.I3.i7.p1">
<span class="ltx_p" id="S4.I3.i7.p1.1"><span class="ltx_text" id="S4.I3.i7.p1.1.1" style="font-size:70%;">Logarithmic Ratio (Over-sampling strategy) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i7.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">jin_intrusion_2021</span><span class="ltx_text" id="S4.I3.i7.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i7.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i8" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">8.</span>
<span class="ltx_para" id="S4.I3.i8.p1">
<span class="ltx_p" id="S4.I3.i8.p1.1"><span class="ltx_text" id="S4.I3.i8.p1.1.1" style="font-size:70%;">Temporal Convolutional Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i8.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">shi_intrusion_2021</span><span class="ltx_text" id="S4.I3.i8.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i8.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i9" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">9.</span>
<span class="ltx_para" id="S4.I3.i9.p1">
<span class="ltx_p" id="S4.I3.i9.p1.1"><span class="ltx_text" id="S4.I3.i9.p1.1.1" style="font-size:70%;">K-Nearest Neighbor </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i9.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">dangelo_association_2023</span><span class="ltx_text" id="S4.I3.i9.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i9.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I3.i10" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">10.</span>
<span class="ltx_para" id="S4.I3.i10.p1">
<span class="ltx_p" id="S4.I3.i10.p1.1"><span class="ltx_text" id="S4.I3.i10.p1.1.1" style="font-size:70%;">Autoencoder </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I3.i10.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">c_s_wickramasinghe_rx-ads_2023</span><span class="ltx_text" id="S4.I3.i10.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I3.i10.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.2.1.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.2.1.3.1">
<span class="ltx_enumerate" id="S4.I4">
<span class="ltx_item" id="S4.I4.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I4.i1.p1">
<span class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text" id="S4.I4.i1.p1.1.1" style="font-size:70%;">DoS dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I4.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">khan_intrusion_2023</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">dangelo_association_2023</span><span class="ltx_text" id="S4.I4.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I4.i1.p1.1.4" style="font-size:70%;"> achieved 100% precision, accuracy, recall and F1-score.</span></span>
</span></span>
<span class="ltx_item" id="S4.I4.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I4.i2.p1">
<span class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text" id="S4.I4.i2.p1.1.1" style="font-size:70%;">Fuzzy dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I4.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">khan_intrusion_2023</span><span class="ltx_text" id="S4.I4.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I4.i2.p1.1.4" style="font-size:70%;"> achieved 100% precision, accuracy, recall and F1-score.</span></span>
</span></span>
<span class="ltx_item" id="S4.I4.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I4.i3.p1">
<span class="ltx_p" id="S4.I4.i3.p1.1"><span class="ltx_text" id="S4.I4.i3.p1.1.1" style="font-size:70%;">RPM dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I4.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span><span class="ltx_text" id="S4.I4.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I4.i3.p1.1.4" style="font-size:70%;"> achieved 100% precision, accuracy, recall and F1-score.</span></span>
</span></span>
<span class="ltx_item" id="S4.I4.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I4.i4.p1">
<span class="ltx_p" id="S4.I4.i4.p1.1"><span class="ltx_text" id="S4.I4.i4.p1.1.1" style="font-size:70%;">Gear dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I4.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span><span class="ltx_text" id="S4.I4.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I4.i4.p1.1.4" style="font-size:70%;"> achieved 100% precision, accuracy, recall and F1-score.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.3.2.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.2.1.1">
<span class="ltx_p" id="S4.T2.3.3.2.1.1.1"><span class="ltx_text" id="S4.T2.3.3.2.1.1.1.1" style="font-size:70%;">SPMD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.3.2.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">us_department_of_transportation_safety_2022</span><span class="ltx_text" id="S4.T2.3.3.2.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.3.2.1.1.1.4" style="font-size:70%;"> — basic safety messages dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.3.2.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.2.2.1">
<span class="ltx_enumerate" id="S4.I5">
<span class="ltx_item" id="S4.I5.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I5.i1.p1">
<span class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text" id="S4.I5.i1.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">j_watts_dynamic_2022</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">a_r_javed_anomaly_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">f_van_wyk_real-time_2020</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">rajendar_sensor_2022</span><span class="ltx_text" id="S4.I5.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I5.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I5.i2.p1">
<span class="ltx_p" id="S4.I5.i2.p1.1"><span class="ltx_text" id="S4.I5.i2.p1.1.1" style="font-size:70%;">One Class Support Vector Machine </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_real-time_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_anomaly_2023</span><span class="ltx_text" id="S4.I5.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I5.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I5.i3.p1">
<span class="ltx_p" id="S4.I5.i3.p1.1"><span class="ltx_text" id="S4.I5.i3.p1.1.1" style="font-size:70%;">Long Short-Term Memory </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_r_javed_anomaly_2021</span><span class="ltx_text" id="S4.I5.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I5.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I5.i4.p1">
<span class="ltx_p" id="S4.I5.i4.p1.1"><span class="ltx_text" id="S4.I5.i4.p1.1.1" style="font-size:70%;">Estimation-based </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">y_wang_anomaly_2020</span><span class="ltx_text" id="S4.I5.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i4.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I5.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="S4.I5.i5.p1">
<span class="ltx_p" id="S4.I5.i5.p1.1"><span class="ltx_text" id="S4.I5.i5.p1.1.1" style="font-size:70%;">Wavelet Kernel Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i5.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">he_wkn-oc_2023</span><span class="ltx_text" id="S4.I5.i5.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i5.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I5.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">6.</span>
<span class="ltx_para" id="S4.I5.i6.p1">
<span class="ltx_p" id="S4.I5.i6.p1.1"><span class="ltx_text" id="S4.I5.i6.p1.1.1" style="font-size:70%;">Temporal Neural Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I5.i6.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">z_he_vehicle_2023</span><span class="ltx_text" id="S4.I5.i6.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I5.i6.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.3.2.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.3.2.3.1">
<span class="ltx_p" id="S4.T2.3.3.2.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.3.2.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">he_wkn-oc_2023</span><span class="ltx_text" id="S4.T2.3.3.2.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.3.2.3.1.1.3" style="font-size:70%;"> achieved 99.9% accuracy, 99.8% recall, 99.9% precision, and 99.9% F1-score.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.4.3.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.4.3.1.1">
<span class="ltx_p" id="S4.T2.3.4.3.1.1.1"><span class="ltx_text" id="S4.T2.3.4.3.1.1.1.1" style="font-size:70%;">OTIDS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.4.3.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">lee_otids_2017</span><span class="ltx_text" id="S4.T2.3.4.3.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.4.3.1.1.1.4" style="font-size:70%;"> — CAN dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.4.3.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.4.3.2.1">
<span class="ltx_enumerate" id="S4.I6">
<span class="ltx_item" id="S4.I6.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I6.i1.p1">
<span class="ltx_p" id="S4.I6.i1.p1.1"><span class="ltx_text" id="S4.I6.i1.p1.1.1" style="font-size:70%;">Maximum Likelihood Estimator with N-grams </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kalutarage_context-aware_2019</span><span class="ltx_text" id="S4.I6.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I6.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I6.i2.p1">
<span class="ltx_p" id="S4.I6.i2.p1.1"><span class="ltx_text" id="S4.I6.i2.p1.1.1" style="font-size:70%;">One-Class Support Vector Machine </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">o_avatefipour_intelligent_2019</span><span class="ltx_text" id="S4.I6.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I6.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I6.i3.p1">
<span class="ltx_p" id="S4.I6.i3.p1.1"><span class="ltx_text" id="S4.I6.i3.p1.1.1" style="font-size:70%;">Artificial Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_paul_artificial_2021</span><span class="ltx_text" id="S4.I6.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I6.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I6.i4.p1">
<span class="ltx_p" id="S4.I6.i4.p1.1"><span class="ltx_text" id="S4.I6.i4.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">s_-f_lokman_optimised_2018</span><span class="ltx_text" id="S4.I6.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i4.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I6.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="S4.I6.i5.p1">
<span class="ltx_p" id="S4.I6.i5.p1.1"><span class="ltx_text" id="S4.I6.i5.p1.1.1" style="font-size:70%;">Logarithmic Ratio (Over-sampling strategy) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i5.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">jin_intrusion_2021</span><span class="ltx_text" id="S4.I6.i5.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i5.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I6.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">6.</span>
<span class="ltx_para" id="S4.I6.i6.p1">
<span class="ltx_p" id="S4.I6.i6.p1.1"><span class="ltx_text" id="S4.I6.i6.p1.1.1" style="font-size:70%;">Autoencoder </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I6.i6.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">c_s_wickramasinghe_rx-ads_2023</span><span class="ltx_text" id="S4.I6.i6.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I6.i6.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.4.3.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.4.3.3.1">
<span class="ltx_enumerate" id="S4.I7">
<span class="ltx_item" id="S4.I7.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I7.i1.p1">
<span class="ltx_p" id="S4.I7.i1.p1.1"><span class="ltx_text" id="S4.I7.i1.p1.1.1" style="font-size:70%;">DoS dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I7.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_paul_artificial_2021</span><span class="ltx_text" id="S4.I7.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I7.i1.p1.1.4" style="font-size:70%;"> achieved 99.98% accuracy, precision, recall, and F1-score.</span></span>
</span></span>
<span class="ltx_item" id="S4.I7.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I7.i2.p1">
<span class="ltx_p" id="S4.I7.i2.p1.1"><span class="ltx_text" id="S4.I7.i2.p1.1.1" style="font-size:70%;">Fuzzy dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I7.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_paul_artificial_2021</span><span class="ltx_text" id="S4.I7.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I7.i2.p1.1.4" style="font-size:70%;"> achieved 100% accuracy, precision, recall, and F1-score.</span></span>
</span></span>
<span class="ltx_item" id="S4.I7.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I7.i3.p1">
<span class="ltx_p" id="S4.I7.i3.p1.1"><span class="ltx_text" id="S4.I7.i3.p1.1.1" style="font-size:70%;">RPM dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I7.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kalutarage_context-aware_2019</span><span class="ltx_text" id="S4.I7.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I7.i3.p1.1.4" style="font-size:70%;"> achieved 100% accuracy.</span></span>
</span></span>
<span class="ltx_item" id="S4.I7.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I7.i4.p1">
<span class="ltx_p" id="S4.I7.i4.p1.1"><span class="ltx_text" id="S4.I7.i4.p1.1.1" style="font-size:70%;">Gear dataset: </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I7.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kalutarage_context-aware_2019</span><span class="ltx_text" id="S4.I7.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I7.i4.p1.1.4" style="font-size:70%;"> achieved 100% accuracy.</span></span>
</span></span>
</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.5.4.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.5.4.1.1">
<span class="ltx_p" id="S4.T2.3.5.4.1.1.1"><span class="ltx_text" id="S4.T2.3.5.4.1.1.1.1" style="font-size:70%;">SynCAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.5.4.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">hanselmann_canet_2020</span><span class="ltx_text" id="S4.T2.3.5.4.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.5.4.1.1.1.4" style="font-size:70%;"> — CAN dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.5.4.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.5.4.2.1">
<span class="ltx_enumerate" id="S4.I8">
<span class="ltx_item" id="S4.I8.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I8.i1.p1">
<span class="ltx_p" id="S4.I8.i1.p1.1"><span class="ltx_text" id="S4.I8.i1.p1.1.1" style="font-size:70%;">Deep Learning </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I8.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">e_gherbi_dad_2022</span><span class="ltx_text" id="S4.I8.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I8.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I8.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I8.i2.p1">
<span class="ltx_p" id="S4.I8.i2.p1.1"><span class="ltx_text" id="S4.I8.i2.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I8.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">s_v_thiruloga_tenet_2022</span><span class="ltx_text" id="S4.I8.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I8.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I8.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I8.i3.p1">
<span class="ltx_p" id="S4.I8.i3.p1.1"><span class="ltx_text" id="S4.I8.i3.p1.1.1" style="font-size:70%;">One-Class Support Vector Machine </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I8.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kukkala_latte_2021</span><span class="ltx_text" id="S4.I8.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I8.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I8.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="S4.I8.i4.p1">
<span class="ltx_p" id="S4.I8.i4.p1.1"><span class="ltx_text" id="S4.I8.i4.p1.1.1" style="font-size:70%;">Long Short-Term Memory </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I8.i4.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kukkala_latte_2021</span><span class="ltx_text" id="S4.I8.i4.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I8.i4.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I8.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">5.</span>
<span class="ltx_para" id="S4.I8.i5.p1">
<span class="ltx_p" id="S4.I8.i5.p1.1"><span class="ltx_text" id="S4.I8.i5.p1.1.1" style="font-size:70%;">Temporal Convolutional Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I8.i5.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">gherbi_deep_2020</span><span class="ltx_text" id="S4.I8.i5.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I8.i5.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.5.4.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.5.4.3.1">
<span class="ltx_p" id="S4.T2.3.5.4.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.5.4.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">e_gherbi_dad_2022</span><span class="ltx_text" id="S4.T2.3.5.4.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.5.4.3.1.1.3" style="font-size:70%;"> achieved 99.7% F1-score, 99.8% recall, and 99.5% precision.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.6.5.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.6.5.1.1">
<span class="ltx_p" id="S4.T2.3.6.5.1.1.1"><span class="ltx_text" id="S4.T2.3.6.5.1.1.1.1" style="font-size:70%;">Open Sourcing 223 GB of Driving Data by Udacity </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.6.5.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">udacity_inc_open_2016</span><span class="ltx_text" id="S4.T2.3.6.5.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.6.5.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.6.5.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.6.5.2.1">
<span class="ltx_enumerate" id="S4.I9">
<span class="ltx_item" id="S4.I9.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I9.i1.p1">
<span class="ltx_p" id="S4.I9.i1.p1.1"><span class="ltx_text" id="S4.I9.i1.p1.1.1" style="font-size:70%;">Edge Computing-Based </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I9.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">f_guo_detecting_2019</span><span class="ltx_text" id="S4.I9.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I9.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I9.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I9.i2.p1">
<span class="ltx_p" id="S4.I9.i2.p1.1"><span class="ltx_text" id="S4.I9.i2.p1.1.1" style="font-size:70%;">Continuous Wavelet Transform </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I9.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_anomaly_2023</span><span class="ltx_text" id="S4.I9.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I9.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I9.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I9.i3.p1">
<span class="ltx_p" id="S4.I9.i3.p1.1"><span class="ltx_text" id="S4.I9.i3.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I9.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang_anomaly_2023</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">l_wang_multi-sensors_2023</span><span class="ltx_text" id="S4.I9.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I9.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.6.5.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.6.5.3.1">
<span class="ltx_p" id="S4.T2.3.6.5.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.6.5.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">l_wang_multi-sensors_2023</span><span class="ltx_text" id="S4.T2.3.6.5.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.6.5.3.1.1.3" style="font-size:70%;"> achieved 99.7% accuracy, 98,7% recall, 99.43% precision, and 99.06% F1-score.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.7.6.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.7.6.1.1">
<span class="ltx_p" id="S4.T2.3.7.6.1.1.1"><span class="ltx_text" id="S4.T2.3.7.6.1.1.1.1" style="font-size:70%;">KITTI </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.7.6.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">geiger_are_2012</span><span class="ltx_text" id="S4.T2.3.7.6.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.7.6.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.7.6.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.7.6.2.1">
<span class="ltx_enumerate" id="S4.I10">
<span class="ltx_item" id="S4.I10.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I10.i1.p1">
<span class="ltx_p" id="S4.I10.i1.p1.1"><span class="ltx_text" id="S4.I10.i1.p1.1.1" style="font-size:70%;">Generative Adversarial Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I10.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">d_bogdoll_experiments_2022</span><span class="ltx_text" id="S4.I10.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I10.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I10.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I10.i2.p1">
<span class="ltx_p" id="S4.I10.i2.p1.1"><span class="ltx_text" id="S4.I10.i2.p1.1.1" style="font-size:70%;">Regularized Diffusion Process </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I10.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">b_ganguly_unsupervised_2022</span><span class="ltx_text" id="S4.I10.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I10.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I10.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I10.i3.p1">
<span class="ltx_p" id="S4.I10.i3.p1.1"><span class="ltx_text" id="S4.I10.i3.p1.1.1" style="font-size:70%;">Unsupervised Discriminative Feature Learning </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I10.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_ranjbar_scene_2020</span><span class="ltx_text" id="S4.I10.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I10.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.7.6.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.7.6.3.1">
<span class="ltx_p" id="S4.T2.3.7.6.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.7.6.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">b_ganguly_unsupervised_2022</span><span class="ltx_text" id="S4.T2.3.7.6.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.7.6.3.1.1.3" style="font-size:70%;"> did not report F1, but had the highest AUC score. The model achieved 80% AUC, 31% MAE, and 61% OvR.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.8.7.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.8.7.1.1">
<span class="ltx_p" id="S4.T2.3.8.7.1.1.1"><span class="ltx_text" id="S4.T2.3.8.7.1.1.1.1" style="font-size:70%;">UNSW-NB15 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.8.7.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">moustafa_unsw-nb15_2015</span><span class="ltx_text" id="S4.T2.3.8.7.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.8.7.1.1.1.4" style="font-size:70%;"> — Attack dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.8.7.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.8.7.2.1">
<span class="ltx_enumerate" id="S4.I11">
<span class="ltx_item" id="S4.I11.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I11.i1.p1">
<span class="ltx_p" id="S4.I11.i1.p1.1"><span class="ltx_text" id="S4.I11.i1.p1.1.1" style="font-size:70%;">Explainable Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I11.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">aziz_anomaly_2022</span><span class="ltx_text" id="S4.I11.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I11.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I11.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I11.i2.p1">
<span class="ltx_p" id="S4.I11.i2.p1.1"><span class="ltx_text" id="S4.I11.i2.p1.1.1" style="font-size:70%;">Genetic Algorithm </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I11.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">aksu_mga-ids_2022</span><span class="ltx_text" id="S4.I11.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I11.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.8.7.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.8.7.3.1">
<span class="ltx_p" id="S4.T2.3.8.7.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.8.7.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">aziz_anomaly_2022</span><span class="ltx_text" id="S4.T2.3.8.7.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.8.7.3.1.1.3" style="font-size:70%;"> achieves 99.7% accuracy, 99.3% precision, 98.7% recall, and 98.7% F1-score.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.9.8.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.9.8.1.1">
<span class="ltx_p" id="S4.T2.3.9.8.1.1.1"><span class="ltx_text" id="S4.T2.3.9.8.1.1.1.1" style="font-size:70%;">VeReMi </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.9.8.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">kamel_veremi_2020</span><span class="ltx_text" id="S4.T2.3.9.8.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.9.8.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.9.8.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.9.8.2.1">
<span class="ltx_enumerate" id="S4.I12">
<span class="ltx_item" id="S4.I12.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I12.i1.p1">
<span class="ltx_p" id="S4.I12.i1.p1.1"><span class="ltx_text" id="S4.I12.i1.p1.1.1" style="font-size:70%;">Deep Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I12.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">t_alladi_deepadv_2021</span><span class="ltx_text" id="S4.I12.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I12.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I12.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I12.i2.p1">
<span class="ltx_p" id="S4.I12.i2.p1.1"><span class="ltx_text" id="S4.I12.i2.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I12.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">t_alladi_deep_2021</span><span class="ltx_text" id="S4.I12.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I12.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I12.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="S4.I12.i3.p1">
<span class="ltx_p" id="S4.I12.i3.p1.1"><span class="ltx_text" id="S4.I12.i3.p1.1.1" style="font-size:70%;">Long Short-Term Memory </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I12.i3.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">t_alladi_deep_2021</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">x_liu_misbehavior_2022</span><span class="ltx_text" id="S4.I12.i3.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I12.i3.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.9.8.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.9.8.3.1">
<span class="ltx_p" id="S4.T2.3.9.8.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.9.8.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">t_alladi_deepadv_2021</span><span class="ltx_text" id="S4.T2.3.9.8.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.9.8.3.1.1.3" style="font-size:70%;"> achieved 98% accuracy, 95.6% recall, 99.6% precision, and 97.6% F1-score.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.10.9.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.10.9.1.1">
<span class="ltx_p" id="S4.T2.3.10.9.1.1.1"><span class="ltx_text" id="S4.T2.3.10.9.1.1.1.1" style="font-size:70%;">Cityscapes </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.10.9.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">cordts_cityscapes_2015</span><span class="ltx_text" id="S4.T2.3.10.9.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.10.9.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.10.9.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.10.9.2.1">
<span class="ltx_enumerate" id="S4.I13">
<span class="ltx_item" id="S4.I13.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I13.i1.p1">
<span class="ltx_p" id="S4.I13.i1.p1.1"><span class="ltx_text" id="S4.I13.i1.p1.1.1" style="font-size:70%;">Generative Adversarial Networks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I13.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">d_bogdoll_experiments_2022</span><span class="ltx_text" id="S4.I13.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I13.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I13.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I13.i2.p1">
<span class="ltx_p" id="S4.I13.i2.p1.1"><span class="ltx_text" id="S4.I13.i2.p1.1.1" style="font-size:70%;">Regularized Diffusion Process </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I13.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">b_ganguly_unsupervised_2022</span><span class="ltx_text" id="S4.I13.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I13.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.10.9.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.10.9.3.1">
<span class="ltx_p" id="S4.T2.3.10.9.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.10.9.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">b_ganguly_unsupervised_2022</span><span class="ltx_text" id="S4.T2.3.10.9.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.10.9.3.1.1.3" style="font-size:70%;"> achieves 80% AUC, 18% MAE, and 71% overlapping ratio (OvR).</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.11.10.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.11.10.1.1">
<span class="ltx_p" id="S4.T2.3.11.10.1.1.1"><span class="ltx_text" id="S4.T2.3.11.10.1.1.1.1" style="font-size:70%;">BDD100k </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.11.10.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">yu_bdd100k_2020</span><span class="ltx_text" id="S4.T2.3.11.10.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.11.10.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.11.10.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.11.10.2.1">
<span class="ltx_enumerate" id="S4.I14">
<span class="ltx_item" id="S4.I14.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I14.i1.p1">
<span class="ltx_p" id="S4.I14.i1.p1.1"><span class="ltx_text" id="S4.I14.i1.p1.1.1" style="font-size:70%;">Convolutional Neural Network </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I14.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_ranjbar_safety_2022</span><span class="ltx_text" id="S4.I14.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I14.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I14.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I14.i2.p1">
<span class="ltx_p" id="S4.I14.i2.p1.1"><span class="ltx_text" id="S4.I14.i2.p1.1.1" style="font-size:70%;">Unsupervised Discriminative Feature Learning </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I14.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_ranjbar_scene_2020</span><span class="ltx_text" id="S4.I14.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I14.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.11.10.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.11.10.3.1">
<span class="ltx_p" id="S4.T2.3.11.10.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.11.10.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">a_ranjbar_safety_2022</span><span class="ltx_text" id="S4.T2.3.11.10.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.11.10.3.1.1.3" style="font-size:70%;"> did not report F1-score, but achieved 79% AUROC.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.12.11.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.12.11.1.1">
<span class="ltx_p" id="S4.T2.3.12.11.1.1.1"><span class="ltx_text" id="S4.T2.3.12.11.1.1.1.1" style="font-size:70%;">UAH-Driveset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.12.11.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">romera_need_nodate</span><span class="ltx_text" id="S4.T2.3.12.11.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.12.11.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.3.12.11.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.12.11.2.1">
<span class="ltx_enumerate" id="S4.I15">
<span class="ltx_item" id="S4.I15.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I15.i1.p1">
<span class="ltx_p" id="S4.I15.i1.p1.1"><span class="ltx_text" id="S4.I15.i1.p1.1.1" style="font-size:70%;">Kalman Variational Autoencoder </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I15.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">g_slavic_kalman_2023</span><span class="ltx_text" id="S4.I15.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I15.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I15.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I15.i2.p1">
<span class="ltx_p" id="S4.I15.i2.p1.1"><span class="ltx_text" id="S4.I15.i2.p1.1.1" style="font-size:70%;">Generalized Markov Jump Particle Filter </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I15.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">g_slavic_interpretable_2021</span><span class="ltx_text" id="S4.I15.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I15.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.3.12.11.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.12.11.3.1">
<span class="ltx_p" id="S4.T2.3.12.11.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.12.11.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">g_slavic_interpretable_2021</span><span class="ltx_text" id="S4.T2.3.12.11.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.12.11.3.1.1.3" style="font-size:70%;"> did not report F1-score, but achieved 73.3% accuracy.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.13.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.1" style="width:88.2pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.13.12.1.1">
<span class="ltx_p" id="S4.T2.3.13.12.1.1.1"><span class="ltx_text" id="S4.T2.3.13.12.1.1.1.1" style="font-size:70%;">ROAD </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.13.12.1.1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">singh_road_2022</span><span class="ltx_text" id="S4.T2.3.13.12.1.1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.13.12.1.1.1.4" style="font-size:70%;"> — Image dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.3.13.12.2" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.13.12.2.1">
<span class="ltx_enumerate" id="S4.I16">
<span class="ltx_item" id="S4.I16.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="S4.I16.i1.p1">
<span class="ltx_p" id="S4.I16.i1.p1.1"><span class="ltx_text" id="S4.I16.i1.p1.1.1" style="font-size:70%;">Gated Recurrent Unit </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I16.i1.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">s_rajapaksha_keep_2022</span><span class="ltx_text" id="S4.I16.i1.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I16.i1.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
<span class="ltx_item" id="S4.I16.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="S4.I16.i2.p1">
<span class="ltx_p" id="S4.I16.i2.p1.1"><span class="ltx_text" id="S4.I16.i2.p1.1.1" style="font-size:70%;">Logarithmic Ratio (Over-sampling strategy) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.I16.i2.p1.1.2.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">jin_intrusion_2021</span><span class="ltx_text" id="S4.I16.i2.p1.1.3.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.I16.i2.p1.1.4" style="font-size:70%;"></span></span>
</span></span>
</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S4.T2.3.13.12.3" style="width:170.7pt;padding-top:1.75pt;padding-bottom:1.75pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.3.13.12.3.1">
<span class="ltx_p" id="S4.T2.3.13.12.3.1.1"><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.13.12.3.1.1.1.1" style="font-size:70%;">[</span><span class="ltx_ref ltx_missing_citation ltx_ref_self">jin_intrusion_2021</span><span class="ltx_text" id="S4.T2.3.13.12.3.1.1.2.2" style="font-size:70%;">]</span></cite><span class="ltx_text" id="S4.T2.3.13.12.3.1.1.3" style="font-size:70%;"> did not report F1-score, but achieved 99.8 precision, 99.8 recall, 99,8 FMeasure, 99.9 accuracy, and 99.9 AUC.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Training of Anomaly Detection Models</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.4.1.1">IV-C</span>1 </span>Data used in Training Anomaly Detection Models</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">The analysis revealed that out of the analysed articles, 136 studies trained their models using real-world data (that is, data collected from a real vehicle), while 50 studies utilised simulation-based training. Additionally, 15 articles incorporated a combination of both real-world and simulation data. By utilising real-world data, researchers aim to capture the intricacies and complexities of actual driving conditions, including various road surfaces, traffic scenarios, and environmental factors. It is important to note that, the data collected for this section only looks at whether the training data is simulated or collected from a real-world scenario. If the training data is based on real-world data and attacks are later simulated, it is categorised as real-world data.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Simulation environments offer researchers precise control over the parameters, scenarios, and ground truth labels, providing a controlled and repeatable setting for training and evaluation. They allow for the generation of diverse scenarios, including rare or dangerous events that may be difficult to encounter in real-world data <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">birks_emergent_2014</span>]</cite>. Training models on simulated data can facilitate rapid experimentation, scalability, and the exploration of extreme or edge cases that are otherwise hard to obtain in real-world scenarios. In addition to the above approaches, 15 articles adopted a hybrid approach, combining both real-world and simulation data for training their anomaly detection models. The most frequently used methods for generating simulated data were Simulation of Urban MObility (SUMO), a testbed, and OMNET++.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.4.1.1">IV-C</span>2 </span>Generation of Anomalies in the Data</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">The analysis revealed various approaches used by researchers, with different degrees of explanation. It is worth noting that in some cases, there were no explanations provided related to how anomalies were introduced. The results are as follows:</p>
<ul class="ltx_itemize" id="S4.I17">
<li class="ltx_item" id="S4.I17.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i1.p1">
<p class="ltx_p" id="S4.I17.i1.p1.1">Random Injections: In 48 articles, anomalies were generated through random data injections into the dataset. This approach involved modifying existing data within the dataset to represent outliers or injecting outliers randomly. By randomly introducing anomalies, researchers aimed to simulate abnormal scenarios and evaluate the effectiveness of their anomaly detection models in identifying these anomalies.</p>
</div>
</li>
<li class="ltx_item" id="S4.I17.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i2.p1">
<p class="ltx_p" id="S4.I17.i2.p1.1">Attacks Performed While Recording the Log: 41 articles employed attacks performed while recording the CAN log through the Onboard Diagnostic (OBD-II) port. In these cases, real-world attacks were executed in a controlled environment by the research team or in a lab. The attacks were recorded in real-time, capturing the dynamics of the anomalies introduced during the attack.</p>
</div>
</li>
<li class="ltx_item" id="S4.I17.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i3.p1">
<p class="ltx_p" id="S4.I17.i3.p1.1">Simulated Attacks: 28 articles simulated attacks to generate anomalies within the dataset. Simulated attacks provided researchers with precise control over the anomaly characteristics, enabling the evaluation of the detection models’ performance against specific attack types or patterns.</p>
</div>
</li>
<li class="ltx_item" id="S4.I17.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i4.p1">
<p class="ltx_p" id="S4.I17.i4.p1.1">Anomalies Generated: 24 articles utilised algorithms or models to generate anomalies within the dataset. Researchers employed data generation techniques, such as Generative Adversarial Networks (GANs) or other anomaly generation algorithms, to synthesise anomalies that resemble real-world anomalies.</p>
</div>
</li>
<li class="ltx_item" id="S4.I17.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i5.p1">
<p class="ltx_p" id="S4.I17.i5.p1.1">Real-World Anomalies: 11 articles used real-world anomalies—of these, the domain was predominantly road surface detection. These anomalies were derived from actual obstacles encountered in real-world scenarios. Researchers incorporated data captured from real-world road surfaces with irregularities, such as potholes, bumps, cracks, or other physical disturbances.</p>
</div>
</li>
<li class="ltx_item" id="S4.I17.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I17.i6.p1">
<p class="ltx_p" id="S4.I17.i6.p1.1">No Explanation: 50 articles did not provide clear explanations regarding how anomalies were generated within the dataset.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS3.4.1.1">IV-C</span>3 </span>Characteristics of the Datasets</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">The description of the dataset used for training varied throughout the reviewed articles. The different ways to describe the data size include the length of data in time, byte size of data, number of car signals, number of data points, number of data samples, number of messages, number of nodes, and number of packets. Only 47 out of 203 articles contained a description of data size. Furthermore, in terms of the temporal date of collection, only 14 of the articles indicated the time frame of data collection. Moreover, the different datasets used across the publicly available papers can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.T2" title="TABLE II ‣ IV-B4 Open-source ‣ IV-B AI Methods used in Anomaly Detection for CAVs ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS4.4.1.1">IV-C</span>4 </span>Levels of Autonomy</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">None of the papers mentioned whether the data collected or the anomaly detection was developed for a specific level of autonomous vehicles. However, one paper noted that their model was built for highly automated vehicles (HAD) <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">pfeil_why_2022</span>]</cite>. This does not indicate a specific level as defined by the Society of Automotive Engineers <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">sae_international_taxonomy_nodate</span>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Testing and evaluation of anomaly detection models</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS1.4.1.1">IV-D</span>1 </span>Testing and Evaluation Metrics</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">The analysis revealed that the top five evaluation metrics, in terms of frequency, were: recall, accuracy, precision, F1-score, and false positive rate (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.F4" title="Figure 4 ‣ IV-D1 Testing and Evaluation Metrics ‣ IV-D Testing and evaluation of anomaly detection models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">4</span></a>):</p>
<ul class="ltx_itemize" id="S4.I18">
<li class="ltx_item" id="S4.I18.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I18.i1.p1">
<p class="ltx_p" id="S4.I18.i1.p1.1">Accuracy: used 86 times, measures the overall correctness of the anomaly detection model by calculating the ratio of correctly classified instances to the total number of instances.</p>
</div>
</li>
<li class="ltx_item" id="S4.I18.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I18.i2.p1">
<p class="ltx_p" id="S4.I18.i2.p1.1">Precision: used 73 times, measures the proportion of correctly identified anomalies (true positives) out of the total instances identified as anomalies (true positives and false positives).</p>
</div>
</li>
<li class="ltx_item" id="S4.I18.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I18.i3.p1">
<p class="ltx_p" id="S4.I18.i3.p1.1">F1-score: used 62 times, is the harmonic mean of precision and recall. It provides a balance between precision and recall, capturing the trade-off between correctly identifying anomalies and minimising false positives and false negatives.</p>
</div>
</li>
<li class="ltx_item" id="S4.I18.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I18.i4.p1">
<p class="ltx_p" id="S4.I18.i4.p1.1">False Positive Rate: observed 33 times, measures the proportion of normal instances incorrectly labelled as anomalies (false positives) out of the total number of actual normal instances. It focuses on the model’s ability to avoid misclassifying normal instances.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Frequency of metrics used to test and evaluate anomaly detection models</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">The collection of evaluation metrics used collectively in the papers is presented here, as selection metrics to illustrate the commonly chosen evaluation metrics used in the reviewed papers. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.02731v1#S4.F5" title="Figure 5 ‣ IV-D1 Testing and Evaluation Metrics ‣ IV-D Testing and evaluation of anomaly detection models ‣ IV Results ‣ Systematic Review: Anomaly Detection in Connected and Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">5</span></a> provides an overview of the 10 most frequently used evaluation metric combinations in the reviewed studies. Here, the focus is on presenting the top five most commonly used selection of metrics:</p>
<ul class="ltx_itemize" id="S4.I19">
<li class="ltx_item" id="S4.I19.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I19.i1.p1">
<p class="ltx_p" id="S4.I19.i1.p1.1">F1-score, Precision, Recall, and Accuracy: was the most common selection of evaluation metrics, allowed for an evaluation that considers overall correctness, a balance between precision and recall, and a trade-off between correctly identifying anomalies and minimising false positives and false negatives. This combination was used 22 times.</p>
</div>
</li>
<li class="ltx_item" id="S4.I19.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I19.i2.p1">
<p class="ltx_p" id="S4.I19.i2.p1.1">Accuracy: was observed in 21 papers. Researchers relied solely on accuracy to evaluate the overall correctness of the anomaly detection model, without considering additional metrics.</p>
</div>
</li>
<li class="ltx_item" id="S4.I19.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I19.i3.p1">
<p class="ltx_p" id="S4.I19.i3.p1.1">F1-score, Precision, and Recall: emerged as the second most frequent combination, appearing 19 times. These metrics were employed together to evaluate the model’s ability to strike a balance between correctly identifying anomalies and avoiding false positives.</p>
</div>
</li>
<li class="ltx_item" id="S4.I19.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I19.i4.p1">
<p class="ltx_p" id="S4.I19.i4.p1.1">False Positive Rate (FPR) and Recall: was used 10 times. FPR and recall, often employed in receiver operating characteristic (ROC) analysis, provide insights into the model’s ability to avoid misclassifying normal instances (FPR) and correctly detect anomalies (recall).</p>
</div>
</li>
<li class="ltx_item" id="S4.I19.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I19.i5.p1">
<p class="ltx_p" id="S4.I19.i5.p1.1">Accuracy, FPR, and Recall: was used 8 times. Accuracy represents the overall correctness of predictions, capturing the ratio of correctly classified instances to the total number of instances. FPR, on the other hand, focuses on the rate of falsely predicted positive instances out of all negative instances. It helps evaluate the model’s ability to avoid false alarms and misclassifications. Recall, also known as sensitivity, measures the proportion of true positive instances that are correctly identified as positive.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>10 most frequently used combinations of metrics to test and evaluate anomaly detection models</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS2.4.1.1">IV-D</span>2 </span>Detection Latency</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Among the reviewed articles, 18 papers provided data on the detection latency in anomaly detection for CAVs. The detection latency, which represents the time taken to detect anomalies, varied across these studies. The reported detection latency ranged from 0.06 ms to 6,000 ms. The best model in terms of detection latency <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span>]</cite> used long short-term memory on the car hacking dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">song_-vehicle_2020</span>]</cite> and achieved 0.06 ms. Detection latency plays an important role in real-time anomaly response and is an important consideration in ensuring the effectiveness and timeliness of anomaly detection mechanisms for CAVs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS3.4.1.1">IV-D</span>3 </span>Case studies</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">This section presents two case studies showcasing a successful implementation of anomaly detection. The first study uses a CAN dataset and develops a prediction-based IDS. The study described below <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span>]</cite>, presents a novel framework for detecting anomalies and attacks on the CAN bus. The model is trained using the most frequently used dataset (the Car-Hacking Dataset by HCRL) and is one of the highest-scoring models in terms of F1-score. The dataset comes with four attacks: DoS, Fuzzy, RPM spoofing, and gear spoofing. The IDS utilises a prediction-based approach, leveraging the temporal correlation of message contents to detect anomalies and attacks. Two prediction modules are introduced: a Long Short-Term Memory (LSTM) network and a Convolutional LSTM (ConvLSTM) network. An attack is classified based on prediction errors using a Gaussian Naïve Bayes classifier. Evaluation against state-of-the-art one-class classifiers and existing works demonstrate superior accuracy, with 100% F1-score, accuracy, precision, and recall on the RPM and gear spoofing datasets. This study highlights the effectiveness of the proposed IDS framework in enhancing CAV cybersecurity.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS3.p2">
<p class="ltx_p" id="S4.SS4.SSS3.p2.1">The second study uses an image dataset and proposes an innovative intrusion detection system that integrates Space Dimension and Time Dimension Models based on sensor data fusion to detect simultaneous attacks on multiple sensors <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">l_wang_multi-sensors_2023</span>]</cite>. In the Space Dimension Model, correlations among multivariate in-vehicle sensor data are leveraged using an optimised CNN to detect independent and confederate attacks. Vehicle state matrices are constructed to capture the underlying data correlations between sensors, facilitating classification. The Time Dimension Model, on the other hand, utilises the Mahalanobis distance metric to capture abrupt deviations caused by anomalous sensor data over time. The paper utilises the Open Sourcing 223GB of Driving Data by Udacity <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">udacity_inc_open_2016</span>]</cite> image dataset and achieves 99.7% accuracy, 98,7% recall, 99.43% precision, and 99.06% F1-score, which is the highest F1-score for this dataset.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">AI Methods used in Anomaly Detection for CAVs</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In answering the first research question, LSTM, CNN, Autoencoders, other deep learning, and One-Class SVM represent the most commonly employed techniques for detecting anomalies in CAVs, as observed through this systematic review. These five algorithms together are used in 91 of the 203 articles in the review. In the most frequently used CAN dataset, Car-Hacking Dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">song_-vehicle_2020</span>]</cite>, the highest performing algorithm was LSTM which achieved 100% precision, accuracy, recall, and F1-score on the DoS, fuzzy attacks, and Gear spoofing attacks <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">p_mansourian_deep_2023</span>]</cite>. In the most frequently used image dataset, which was from Udacity Inc <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">udacity_inc_open_2016</span>]</cite>, the best-performing model used CNN and achieved 99.7% accuracy, 98,7% recall, 99.43% precision, and 99.06% F1-score <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">l_wang_multi-sensors_2023</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">When it comes to the application domain, the majority of the articles were focused on the CAN bus network, vehicle sensors, the image domain, IoV, and lane detection. The papers that focused on the CAN bus network extracted data from either a simulated environment or through a real vehicle using the OBD-II port. This port is available on most vehicles and enables access to the in-vehicle network traffic. Connected to the in-vehicle network, the vehicle sensors measure the performance of the vehicle’s components, such as sensor data from acceleration, engine RPM, vehicle speed, and GPS. This differs from environment sensors, encompassing sensors that perceive the vehicle’s surroundings. The next most frequently studied area is the image domain. This category predominantly focuses on road anomaly detection, such as potholes or other obstacles that the camera can detect. Furthermore, the next most studied field is IoV, which primarily applies to traffic management, emergency message delivery, traffic, and temperature monitoring <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">s_yaqoob_deep_2023</span>]</cite>. As opposed to the aforementioned categories, IoV entails external communication. Next, the field of lane detection was the fifth most frequently studied domain. In this domain, the authors proposed methods for detecting sudden lane changes. This also used the image domain but is specifically focused on detecting anomalous events such as sudden lane changing that could be dangerous. The findings of this review show that most research is concerned with CAN (78 out of 203 papers).</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The review has highlighted a focus on both security and safety in anomaly detection research for CAVs and shows the recognition of their intertwined importance. While there is a higher number of papers focusing on security, there is a significant high focus on safety too. This is similar to the findings of Rajbahadur et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbahadur_survey_2018</span>]</cite>. By taking into account both security and safety dimensions, researchers can contribute to the development of more resilient, secure, and safe connected and autonomous vehicles.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">The low number of open-source models in the review highlights the need for increased emphasis on open collaboration and transparency within the security research community. As proposed in the recommendation ITU-T X.1382 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">international_telecommunication_union_recommendation_2023</span>]</cite>, it is necessary to start building a platform for sharing data related to information security for CAVs to build a community where academics and companies concerned with connected vehicles can collaborate to defend against cyber threats. UNECE WP.29 mandates consideration for monitoring, detecting, and responding to cyber threats to CAVs for all new vehicle types, which will be enforced July 2024 <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">united_nations_addendum_2021</span>]</cite>. It also includes a mandate for establishing a management system to take accountability for the response and processing of this information. By encouraging researchers to share their models and datasets openly, the security community can benefit from the collective expertise, shared knowledge, reproduce and validate the models, and compare and validate the reliability of the studies, ultimately driving improvements in anomaly detection for CAVs and contributing to safer and more secure autonomous systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Training of Anomaly Detection Models</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In addressing the second research question of how anomaly detection models are trained, the majority of the reviewed papers used real-world data over simulated data. It is worth highlighting that only a few models used real-world attacks or faults during the training process, instead, randomly injecting anomalies into the dataset and performing attacks through the OBD-II port while recording the vehicle’s log emerged as popular methods. This supports the findings of Rajbahadur et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbahadur_survey_2018</span>]</cite>, which found that real-world datasets without simulated attacks are rarely utilised. Incorporating real-world attacks or faults can provide models with exposure to genuine adversarial situations, helping them to better tested against real-world anomalies and threats. None of the reviewed studies included datasets with real-world attacks. As aforementioned, the recommendation <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">international_telecommunication_union_recommendation_2023</span>]</cite> to create a community where data is shared openly with relevant cybersecurity for CAV actors is necessary to share real anomaly data. Currently, the most realistic attack scenario is to attack a vehicle in a secure environment while recording the log.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Furthermore, only one article explicitly mentioned the level of autonomy at which the data was collected or for which anomaly detection was conducted <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">pfeil_why_2022</span>]</cite>. The level of autonomy is a critical factor that influences the complexity of the data and the specific challenges associated with anomaly detection. Understanding the level of autonomy allows for a better interpretation of the results and their relevance to different autonomous driving scenarios. As vehicles transition from conventional to semi-autonomous and fully autonomous modes, the complexity of anomaly detection methodologies may need to undergo a significant evolution. At lower autonomy levels, where human drivers are actively engaged in vehicle operation, anomaly detection may primarily focus on identifying deviations from expected driver behaviour or vehicle performance metrics. Contrarily, as vehicles progress towards higher autonomy levels, where human intervention becomes less frequent or non-existent, anomaly detection must adapt to account for the increased reliance on onboard sensor suites, decision-making algorithms, and communication networks. For instance, integrating Advanced Driver Assistance Systems (ADAS) adds complexity to anomaly detection in CAVs. As CAVs incorporate more sophisticated ADAS functionalities, anomaly detection becomes increasingly challenging, emphasising the importance of robust and adaptable detection systems for ensuring vehicle safety and reliability. Moreover, the dynamic nature of operational contexts across different autonomy levels may require the development of adaptive anomaly detection systems capable of discerning anomalies amidst evolving environmental conditions, traffic scenarios, and system configurations.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS1.4.1.1">V-B</span>1 </span>Creation, maintenance, and standardisation of benchmarking datasets</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">The creation of benchmarking datasets faces several challenges. First, the creators have to decide whether to use real or simulated data. Most models identified in this paper used real-world data: image classification models have trained their models mostly on real images, and time series anomaly detection models have trained on log data from the vehicle’s OBD-II port or recorded sensor data. Next, the authors have to decide on a method for introducing anomalies into the dataset. The scenario that is most similar to a real attack or fault scenario is injecting attacks on the vehicle while it is operating in a controlled environment. A problem identified with some of the datasets is a lack of attack data, which leads to researchers having to generate their anomalies. This restricts the comparability between models using the same dataset. For a dataset to be used for benchmarking, it is most effective to have a dataset with pre-injected or pre-performed attacks. Another limitation of attack data is the lack of variation. The most commonly employed dataset was the car-hacking dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">song_-vehicle_2020</span>]</cite> which includes DoS, fuzzy (randomly injected values), gear spoofing, and RPM spoofing, which is restricted to only four different attacks.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">In terms of maintaining datasets for anomaly detection models, they will have to be updated relating to new attack types. For instance, a more recent dataset <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lampe_can-train-and-test_2024</span>]</cite> addresses this by including more attack types. The authors introduce nine different attack types on the CAN bus: DoS, fuzzing, systematic, gear spoofing, RPM spoofing, speed spoofing, combined spoofing, standstill, and interval. To advance the field of anomaly detection for CAVs, benchmarking datasets will have to add new attack types as they are discovered either in the field of academics or in the industry.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS2.4.1.1">V-B</span>2 </span>Testing and Evaluation of Anomaly Detection Models</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">In addressing the second research question, recall was the most frequently used evaluation metric across all papers. The most frequently used selection of metrics were accuracy, F1-score, precision, and recall. Looking at the most frequently used metric, the use of recall highlights a significance in capturing the ability of a model to identify true positive instances. Given the criticality of detecting anomalies in autonomous vehicles to ensure safe and efficient operation, a high recall value is essential to minimise the chances of false negatives and the potential risks associated with undetected anomalies. Maximising true positives and minimising false negatives should be the highest priority when evaluating anomaly detection models <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">jacob_anomalybench_2020</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">lavin_evaluating_2015</span>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Out of the 203 reviewed articles, only 18 studies provided data on detection latency. This metric measures the time from the anomaly first occurs until it is detected. Detection latency is an important metric in anomaly detection models <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">jacob_anomalybench_2020</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">lavin_evaluating_2015</span>]</cite>. This metric should be included when evaluating anomaly detection models for CAVs since a timely response can potentially avoid a dangerous on-road situation for CAVs. This limited inclusion of detection latency information suggests a gap in reporting and analysing this crucial aspect of anomaly detection.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">Limitations</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">There are two limitations identified in this systematic review that highlight several challenges related to data quality and reporting:</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Prominent limitation pertains to the absence of comprehensive data on detection latency, which directly impacts the promptness of identifying anomalies. The limited availability of such data inhibits a comprehensive analysis of detection latency trends across the reviewed studies, hindering the ability to draw conclusive insights.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">The wide variation in the description of datasets used for training anomaly detection models poses a challenge to standardisation and comparability. The lack of a uniform framework for describing data sets complicates efforts to understand their characteristics and assess their applicability to different scenarios.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">These limitations underscore the need for improved data quality and standardised reporting practices in future studies, ensuring greater transparency, comparability, and depth of analysis in the field of anomaly detection for CAVs. Furthermore, these limitations exacerbate the already existing challenges associated with the lack of baseline evaluations and benchmarking observed in anomaly detection studies, as highlighted by Rajbahadur et al. <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">rajbahadur_survey_2018</span>]</cite> in their study.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.4.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.5.2">Recommendations</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Based on the findings and limitations identified in this systematic review, several recommendations can be proposed to enhance future research in the field of anomaly detection for autonomous vehicles:</p>
<ol class="ltx_enumerate" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">Incorporate multiple evaluation metrics: To provide a comprehensive assessment of anomaly detection models, it is recommended to include multiple evaluation metrics in future research <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lones_how_2021</span>]</cite>. Utilising a diverse set of evaluation metrics allows for a better understanding of the strengths and weaknesses of the models, better trade-off analysis, and improved transparency in reporting the effectiveness of the anomaly detection approaches. There should also be a consensus on what set of evaluation metrics should be used for anomaly detection.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">Open-source anomaly detection models and datasets: Future studies should consider making their models and datasets open-source to foster collaboration, transparency, and reproducibility win the field <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">lones_how_2021</span>]</cite>. This, as aforementioned, will contribute to building a better community for sharing data on information security relating to CAVs as proposed by ITU <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">international_telecommunication_union_recommendation_2023</span>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1">Lack of benchmarking: Many of the datasets used in the papers included in this review do not have predefined anomalies or attacks. Instead, they provide normal traffic data and require researchers to generate their anomalies or attacks for evaluation. This lack of benchmarking makes it difficult to compare the performance of different anomaly detection algorithms consistently. Without predefined attack scenarios, it’s challenging to assess the effectiveness of various detection techniques in a standardised manner. Only 38 of the 203 papers used a dataset with predefined attacks. To combat this problem, developing standardised benchmark datasets that include a variety of predefined attacks and anomalies, will allow for consistent evaluation of detection techniques. While some datasets include simulated attacks (e.g., DoS, fuzzing, spoofing), there is a shortage of comprehensive attack databases specifically tailored to CAVs.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1">Lack of data on the deployment of anomaly detection models: The anomaly detection models reviewed have not been tested in real-world setting, and therefore, their performance remains uncertain. It will be useful for future research to investigate the deployment and maintenance of these models to understand how they will perform in a real setting, vehicles, over time.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.1">Lack of anomaly detection models for Ethernet: As CAVs progress to include more automated functions and ultimately progress to a fully automated vehicle, more components will be connected to the in-vehicle network. This has posed challenges to the traditional CAN, leading Bosch to develop new CAN protocols, CAN FD and CAN XL <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">hartwich_introducing_2020</span>]</cite>, that have increased bandwidth to adapt to this change. Ethernet is used to accommodate the need for bigger bandwidth for technologies, such as LiDAR, radar, and cameras. In this review, only one paper <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">p_meyer_network_2020</span>]</cite> investigates anomaly detection for traffic in the Ethernet. Companies such as Garrett Motion and ETAS have already developed IDS for Ethernet network traffic. This is an area that could benefit from more research to establish effective IDS.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Implementing these recommendations in future research could improve the transparency, reproducibility, and effectiveness of anomaly detection models designed for CAVs. By promoting open collaboration, specifying relevant details, improving data reporting, ensuring uniformity, and utilising a comprehensive set of evaluation metrics, the field can advance more rapidly and foster the development of more reliable, robust, and applicable anomaly detection solutions for CAVs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This systematic literature review examined the landscape of anomaly detection for CAVs, covering a broad spectrum of articles and incorporating a total of 203 research papers in the final review. The review was structured around three principal research inquiries: AI methodologies employed for anomaly detection; the training processes of these models; and the strategies for their testing and evaluation.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The findings indicate that LSTM is the most frequently used AI method in anomaly detection for CAVs, followed by CNN, Autoencoder, other deep learning algorithms, and One-Class Support Vector Machine. The CAV component that has received the most research interest is the CAN bus, with a significant focus on security, although safety also constitutes a substantial portion of the research. Overall, only a small fraction (9 out of 203) of the articles reviewed provided open access to their models.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The review also aimed to understand the training processes of the anomaly detection models proposed for CAVs. The data reveals that real-world data is the preferred choice for training datasets, utilised nearly three times as often as simulated data. Anomalies were introduced into these datasets through various methods, with the most prevalent approach being the random data injection of anomalies into an existing dataset. However, the use of real-world attacks and faults was less common.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">The final research question addressed the evaluation of anomaly detection models. The review identified that accuracy, F1-score, precision, and recall were the most frequently selected set of metrics used to evaluate anomaly detection models. Throughout all papers, recall was the most frequently used metric. Detection latency ranged from 0.06 milliseconds to 6,000 milliseconds but was used as a metric in 18 papers.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">This systematic review provides a comprehensive overview of the current state of anomaly detection for CAVs, highlighting key methodologies, training processes, evaluation strategies, and the current state-of-the-art models for the most frequently used datasets. It emphasises the need for further research to incorporate multiple evaluation metrics; include detection latency as an evaluation metric; open source their models for transparency and reproducibility, and create a community where the vehicle industry and researchers can benefit from the research; and keep benchmarking datasets up to date with known attacks. The recommendation for future research includes assessing the performance of the anomaly detection models when deployed in a vehicle on the road, as well as exploring how anomaly detection can be applied to other communication protocols such as Ethernet traffic.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">While anomaly-based detection is the initial stage of detecting faults and cyber-physical attacks, what follows next, addressing anomalies and response, requires further research and attention, an area that is currently lacking attention.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Acknowledgement</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was supported by the Engineering and Physical Sciences Research Council [EP/S022503/1]. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.</p>
</div>
<div class="ltx_para" id="S7.p2">
<span class="ltx_ERROR undefined" id="S7.p2.1">\AtNextBibliography</span><span class="ltx_ERROR undefined" id="S7.p2.2">\printbibliography</span>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="S7.fig1">
</figure>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">John Roar Ventura Solaas</span> is pursuing a PhD in cybersecurity at the Centre for Doctoral Training (CDT) at University College London. He is funded by the Engineering &amp; Physical Sciences Research Council (EPSRC). His research interest is anomaly detection for connected and autonomous vehicles.</p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="S7.fig2">
</figure>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">Dr Nilufer Tuptuk</span> received her PhD degree in Computer Science from the University College London in 2019. She currently works as an Assistant Professor in the Department of Security and Crime Science at the same institution. Her research interests include cyber-physical systems security, including the Internet of Things, Industrial Control Systems, and Connected and Autonomous Vehicles. Her work involves application and effectiveness of AI-based models in identifying vulnerabilities and detecting anomalous behaviours within these systems.</p>
</div>
<figure class="ltx_figure ltx_align_floatleft" id="S7.fig3">
</figure>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Dr Enrico Mariconti</span> received his PhD degree from University College London in 2019 working on detection and prevention of automated threats such as malware using AI. He currently works as an assistant professor in the Department of Security and Crime Science at UCL and focuses on understanding and countering cyber-physical threats, from IoT devices to social media with a particular focus on hate, harassment, and safety.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 15 19:25:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
