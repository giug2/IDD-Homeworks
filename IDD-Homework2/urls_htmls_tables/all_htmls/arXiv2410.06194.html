<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author</title>
<!--Generated on Tue Oct  8 16:43:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Remote sensing,  Semantic contour extraction,  Vision-language learning
" lang="en" name="keywords"/>
<base href="/html/2410.06194v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S1" title="In Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2" title="In Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.SS1" title="In II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Preliminary: DirectSAM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.SS2" title="In II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Overview of Our Methodology</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.SS3" title="In II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Dataset Construction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.SS4" title="In II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Model Architecture</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3" title="In Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS1" title="In III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS2" title="In III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS3" title="In III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Benchmarking DirectSAM-RS</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS3.SSS0.Px1" title="In III-C Benchmarking DirectSAM-RS ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title">Zero-shot (<span class="ltx_text ltx_font_typewriter">ZS</span>) setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS3.SSS0.Px2" title="In III-C Benchmarking DirectSAM-RS ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title">Fine-tuning (<span class="ltx_text ltx_font_typewriter">FT</span>) setting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS4" title="In III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Ablation of DirectSAM SA-1B Pretraining</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.SS5" title="In III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Importance of Scaling-up Pretraining Data</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S4" title="In Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Works</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images
<span class="ltx_note ltx_role_thanks" id="id12.id1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>*¬†Equal contributions,
üñÇ¬†Corresponding author</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shiyu Miao<sup class="ltx_sup" id="id13.12.id1"><span class="ltx_text ltx_font_italic" id="id13.12.id1.1">‚àó1</span></sup>, Delong Chen<sup class="ltx_sup" id="id14.13.id2"><span class="ltx_text ltx_font_italic" id="id14.13.id2.1">‚àó2</span></sup>, Fan Liu<sup class="ltx_sup" id="id15.14.id3"><span class="ltx_text ltx_font_italic" id="id15.14.id3.1">1</span></sup><sup class="ltx_sup" id="id16.15.id4">üñÇ</sup>, Chuanyi Zhang<sup class="ltx_sup" id="id17.16.id5"><span class="ltx_text ltx_font_italic" id="id17.16.id5.1">1</span></sup>, Yanhui Gu<sup class="ltx_sup" id="id18.17.id6"><span class="ltx_text ltx_font_italic" id="id18.17.id6.1">3</span></sup>, Shengjie Guo<sup class="ltx_sup" id="id19.18.id7"><span class="ltx_text ltx_font_italic" id="id19.18.id7.1">3</span></sup>, Jun Zhou<sup class="ltx_sup" id="id20.19.id8"><span class="ltx_text ltx_font_italic" id="id20.19.id8.1">4</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup" id="id21.20.id1"><span class="ltx_text ltx_font_italic" id="id21.20.id1.1">1</span></sup>College of Computer Science and Software Engineering, Hohai University, Nanjing, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id22.21.id2"><span class="ltx_text ltx_font_italic" id="id22.21.id2.1">2</span></sup>Hong Kong University of Science and Technology, Hong Kong SAR 
<br class="ltx_break"/><sup class="ltx_sup" id="id23.22.id3"><span class="ltx_text ltx_font_italic" id="id23.22.id3.1">3</span></sup>School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id24.23.id4"><span class="ltx_text ltx_font_italic" id="id24.23.id4.1">4</span></sup>School of Information and Communication Technology, Griffith University, Queensland, Australia 
<br class="ltx_break"/>E-mail: 2206010417@hhu.edu.cn, delong.chen@connect.ust.hk, fanliu@hhu.edu.cn, 
<br class="ltx_break"/>20231104@hhu.edu.cn, gu@njnu.edu.cn, gshengjies@gmail.com, jun.zhou@griffith.edu.au

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id25.id1">The Direct Segment Anything Model (DirectSAM) excels in class-agnostic contour extraction. In this paper, we explore its use by applying it to optical remote sensing imagery, where semantic contour extraction‚Äîsuch as identifying buildings, road networks, and coastlines-holds significant practical value. Those applications are currently handled via training specialized small models separately on small datasets in each domain. We introduce a foundation model derived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong segmentation capability acquired from natural images, but also benefits from a large-scale dataset we created for remote sensing semantic contour extraction. This dataset comprises over 34k image-text-contour triplets, making it at least 30 times larger than individual dataset. DirectSAM-RS integrates a prompter module: a text encoder and cross-attention layers attached to the DirectSAM architecture, which allows flexible conditioning on target class labels or referring expressions. We evaluate the DirectSAM-RS in both zero-shot and fine-tuning setting, and demonstrate that it achieves state-of-the-art performance across several downstream benchmarks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Remote sensing, Semantic contour extraction, Vision-language learning

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Direct Segment Anything Model (DirectSAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> is a powerful contour extraction model that enables efficient ‚Äúsegment everything‚Äù, generating comprehensive panoptic subobject-level segmentations. Based on the SA-1B dataset comprising 1 billion masks across 11 million images, DirectSAM‚Äôs large-scale pretraining process consumed 11k NVIDIA A100 GPU hours. This extensive pretraining endowed the model with rich knowledge of visual contours. However, DirectSAM is <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">non-interactive</span> and <span class="ltx_text ltx_font_bold" id="S1.p1.1.2">class-agnostic</span>. These constraints restrict DirectSAM‚Äôs convenient use in applications beyond subobject-level image tokenization. Furthermore, since the model was pretrained primarily on natural images, its generalization to other domains, such as remote sensing imagery, remains limited.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we introduce DirectSAM-RS, a <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">vision-language foundation model</span> for semantic contour extraction in optical remote sensing imagery. As a foundation model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib7" title="">7</a>]</cite>, DirectSAM-RS offers both <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">high flexibility</span> and <span class="ltx_text ltx_font_bold" id="S1.p2.1.3">strong performance</span>, mirroring advantages seen in foundation models from other domains such as CLIP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib8" title="">8</a>]</cite>, RemoteCLIP¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib9" title="">9</a>]</cite>, and Segment Anything Model (SAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This foundation model represents a significant departure from existing models in semantic contour extraction for remote sensing. While this task has long been fundamental in remote sensing, attracting considerable attention, traditional approaches¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib15" title="">15</a>]</cite> have focused on training models separately for different targets. Such a methodology has limited model performance, particularly given the small scale of existing datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib18" title="">18</a>]</cite> (typically fewer than 1k samples).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In contrast, DirectSAM-RS enables knowledge sharing across diverse semantic targets through joint multi-task learning. It stands out as the first vision-language foundation model for contour extraction capable of accepting free-form textual prompts that specify the target, as opposed to existing visual-only models. Moreover, it is the first model able to perform zero-shot semantic contour extraction without using any training samples from downstream datasets.
To create DirectSAM-RS, we introduced novel technical contributions in both data and modeling, briefly described as follows:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">A Large-scale dataset for pretraining</span>. We constructed a semantic contour extraction dataset by repurposing existing semantic segmentation datasets with our proposed Mask2Contour (<span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.2">M2C</span>) transformation. The <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.3">M2C</span> process produces a total of 34k image-text-contour triplets from LoveDA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib19" title="">19</a>]</cite>, iSAID¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib20" title="">20</a>]</cite>, DeepGlobe¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib21" title="">21</a>]</cite>, and RefSegRS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib22" title="">22</a>]</cite> datasets. We name this resulting dataset <span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.4">RemoteContour-34k</span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">A prompter attached to DirectSAM</span>. We develop a prompter architecture, which consists of an encoder that extracts semantic information from the textual prompt, and cross-attention layers inserted into the DirectSAM. These cross-attention layers interleave with the image encoder blocks, facilitating multi-modal information fusion. This simple yet effective prompter design enables flexible conditioning of DirectSAM while preserving its pretrained knowledge.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We validate DirectSAM-RS on three downstream contour extraction datasets: SLSD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib17" title="">17</a>]</cite> for coastline extraction, Beijing Urban Building Extraction (BUBE)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib18" title="">18</a>]</cite>, and LRSNY¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib16" title="">16</a>]</cite> for road extraction. We evaluated both zero-shot and fine-tuning modes. We observed that zero-shot DirectSAM-RS can already outperform some baseline methods, and when we fine-tuned the model, it established new state-of-the-art on all of three benchmarks. Specifically, DirectSAM-RS achieved ODS scores of 0.772, 0.887, and 0.958 on road, building, and coastline extraction benchmarks, respectively. These results represent significant relative performance improvements of 21%, 5%, and 7% over previous best methods. To facilitate future research, the <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.1">RemoteContour-34k</span> dataset and codes will be fully open-sourced and are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/StevenMsy/DirectSAM-RS" title="">https://github.com/StevenMsy/DirectSAM-RS</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Method</span>
</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="S2.F1.1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S2.F1.5.1">Composition of the proposed dataset.</span> We annotated the number of samples and the percentage of each subset and each class. The <span class="ltx_text ltx_font_typewriter" id="S2.F1.6.2">RemoteContour-34k</span> dataset consists both rich semantics (as shown by the word cloud), and diverse visual domains (<span class="ltx_text ltx_font_italic" id="S2.F1.7.3">e.g.,</span> urban, rural).</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="384" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>While being able to successfully identify most key elements, the raw DirectSAM model suffers from <span class="ltx_text" id="S2.F2.3.1" style="color:#FF0000;">missing segmentation</span> of cars and roads, and <span class="ltx_text" id="S2.F2.4.2" style="color:#FF8000;">over-segmentation</span> of part components. Moreover, the model extracts the contours of all semantic targets as it‚Äôs class-agnostic. These issues limit its direct applicability for semantic contour extraction in remote sensing.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Preliminary: DirectSAM</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.3">Let‚Äôs begin by briefly reviewing DirectSAM. It is a SegFormer-based model that takes an RGB image <math alttext="I\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">I</mi><mo id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.3.2" xref="S2.SS1.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S2.SS1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S2.SS1.p1.1.m1.1.1.3.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S2.SS1.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S2.SS1.p1.1.m1.1.1.3.3.4" xref="S2.SS1.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><in id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></in><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">ùêº</ci><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.2">‚Ñù</ci><apply id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3"><times id="S2.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.2">ùêª</ci><ci id="S2.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3.3">ùëä</ci><cn id="S2.SS1.p1.1.m1.1.1.3.3.4.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">I\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_I ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> as input and generates the corresponding contour probability map <math alttext="\hat{Y}\in\mathbb{R}^{H\times W}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mover accent="true" id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">Y</mi><mo id="S2.SS1.p1.2.m2.1.1.2.1" xref="S2.SS1.p1.2.m2.1.1.2.1.cmml">^</mo></mover><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">‚àà</mo><msup id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.3.2" xref="S2.SS1.p1.2.m2.1.1.3.3.2.cmml">H</mi><mo id="S2.SS1.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.2.m2.1.1.3.3.1.cmml">√ó</mo><mi id="S2.SS1.p1.2.m2.1.1.3.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.3.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><ci id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2.1">^</ci><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">ùëå</ci></apply><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">‚Ñù</ci><apply id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3"><times id="S2.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S2.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.2">ùêª</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.3">ùëä</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\hat{Y}\in\mathbb{R}^{H\times W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">over^ start_ARG italic_Y end_ARG ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W end_POSTSUPERSCRIPT</annotation></semantics></math> in <math alttext="(0,1)" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.2"><semantics id="S2.SS1.p1.3.m3.2a"><mrow id="S2.SS1.p1.3.m3.2.3.2" xref="S2.SS1.p1.3.m3.2.3.1.cmml"><mo id="S2.SS1.p1.3.m3.2.3.2.1" stretchy="false" xref="S2.SS1.p1.3.m3.2.3.1.cmml">(</mo><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">0</mn><mo id="S2.SS1.p1.3.m3.2.3.2.2" xref="S2.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.3.m3.2.2" xref="S2.SS1.p1.3.m3.2.2.cmml">1</mn><mo id="S2.SS1.p1.3.m3.2.3.2.3" stretchy="false" xref="S2.SS1.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.2b"><interval closure="open" id="S2.SS1.p1.3.m3.2.3.1.cmml" xref="S2.SS1.p1.3.m3.2.3.2"><cn id="S2.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S2.SS1.p1.3.m3.1.1">0</cn><cn id="S2.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S2.SS1.p1.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.2c">(0,1)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.2d">( 0 , 1 )</annotation></semantics></math>. The model has acquired robust contour extraction capability through extensive pretraining on SA-1B, which is currently the largest image segmentation dataset and was also used to train the powerful SAM models. DirectSAM has demonstrated strong contour extraction capability on natural images, and here we show that it also has potential value for remote sensing. As exemplified in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.F2" title="Figure 2 ‚Ä£ II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">2</span></a>, DirectSAM successfully extracted various visual elements in the given aerial images, such as buildings, tennis courts, and trees. However, several key limitations prohibit its direct application to remote sensing tasks, as discussed below.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Firstly, DirectSAM occasionally fails to detect certain important targets, such as cars and roads noted in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.F2" title="Figure 2 ‚Ä£ II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">2</span></a>. This could be due to the partial label problem of SA-1B and the domain gap between SA-1B and remote sensing images. Second, DirectSAM was trained for subobject-level image segmentation, while semantic contour extraction in remote sensing usually requires the model to operate at the level of object entities. This over-segmentation of fragmented and excessively detailed part components will negatively impact the performance. Finally, DirectSAM is class-agnostic‚Äìit does not distinguish between different semantic categories when extracting contours. This limits its utility for remote sensing applications that often require class-specific structural analysis.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of the proposed Mask2Contour (<span class="ltx_text ltx_font_typewriter" id="S2.F3.2.1">M2C</span>) transformation. It enables us to repurpose the existing semantic segmentation dataset with mask annotations to the semantic contour extraction task.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="283" id="S2.F4.1.g1" src="x4.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S2.F4.3.1">Model architecture of the proposed DirectSAM-RS</span>. We extend the base DirectSAM model (encoder blocks and contour decoder) with a prompter architecture (red). This prompter consists of a text encoder that extracts semantic information from textual prompts, and cross-attention layers that fuse the prompt information into visual feature maps at different stages.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Overview of Our Methodology</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To address the above challenges, we propose DirectSAM-RS, a vision-language foundation model for semantic contour extraction in remote sensing. Our approach is motivated by the intuition that contour extraction capabilities for various targets in remote sensing share fundamental similarities. This insight suggests that a unified multi-tasked model, capable of extracting contours for multiple semantic targets, could outperform specialized models trained on individual tasks. By leveraging this shared knowledge across different tasks, we could create a more robust and generalizable model.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To realize this vision, our methodology incorporates two key elements: a large-scale, semantically diverse dataset spanning multiple contour extraction tasks, and a promptable architecture attached to DirectSAM that dynamically adapts the behavior based on the textual prompt. This combination allows the model to benefit from multi-task learning while maintaining the flexibility to address specific contour extraction tasks as needed. In the following sections, we will delve into the details of our dataset construction and model architecture.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Dataset Construction</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To enhance DirectSAM-RS‚Äôs textual comprehension and generalization capabilities for remote sensing imagery, we introduce the <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.1">RemoteContour-34k</span> dataset, a large-scale, diverse dataset tailored specifically for semantic contour extraction in remote sensing images. As illustrated in Fig.¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.F1" title="Figure 1 ‚Ä£ II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2">RemoteContour-34k</span> encompasses a substantial collection of 34,291 image-text-contour triplets with multiple distinct free-form textual queries. The dataset is curated from a variety of optical remote sensing sources, including semantic segmentation and instance segmentation datasets. This diverse sourcing ensures that <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.3">RemoteContour-34k</span> covers a broad spectrum of visual and textual contexts, providing a robust foundation for training the powerful DirectSAM-RS.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">To maximize the dataset‚Äôs efficacy, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p2.1.1">RemoteContour-34k</span> incorporates samples from several key sources. Primarily, it integrates data from the RefSegRS dataset, complete with corresponding free-form textual queries. To further enhance visual diversity, samples from the LoveDA and iSAID segmentation datasets are included. Recognizing the significance of road-related features in remote sensing applications, we have also incorporated data from the DeepGlobe dataset. This addition addresses the need for a more comprehensive representation of road features, thereby improving the model‚Äôs capacity to detect and interpret road elements in remote sensing imagery.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">The M2C transformation is illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.F3" title="Figure 3 ‚Ä£ II-A Preliminary: DirectSAM ‚Ä£ II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">3</span></a>, where we utilize the border following algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib23" title="">23</a>]</cite> to transform segments to contours.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Model Architecture</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.13">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S2.F4" title="Figure 4 ‚Ä£ II-A Preliminary: DirectSAM ‚Ä£ II Method ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">4</span></a>, DirectSAM-RS adopted and enhanced the transformer-based visual encoder from DirectSAM, leveraging its ability to capture contour-related information from input images effectively. Additionally, we use a promptable text encoder to enable free-form query modeling.
Given an input image <math alttext="I\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><mrow id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">I</mi><mo id="S2.SS4.p1.1.m1.1.1.1" xref="S2.SS4.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml"><mi id="S2.SS4.p1.1.m1.1.1.3.2" xref="S2.SS4.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p1.1.m1.1.1.3.3" xref="S2.SS4.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS4.p1.1.m1.1.1.3.3.2" xref="S2.SS4.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S2.SS4.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S2.SS4.p1.1.m1.1.1.3.3.3" xref="S2.SS4.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S2.SS4.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p1.1.m1.1.1.3.3.1.cmml">√ó</mo><mn id="S2.SS4.p1.1.m1.1.1.3.3.4" xref="S2.SS4.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><in id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1.1"></in><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">ùêº</ci><apply id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.3.2.cmml" xref="S2.SS4.p1.1.m1.1.1.3.2">‚Ñù</ci><apply id="S2.SS4.p1.1.m1.1.1.3.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3"><times id="S2.SS4.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.1"></times><ci id="S2.SS4.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.2">ùêª</ci><ci id="S2.SS4.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.3">ùëä</ci><cn id="S2.SS4.p1.1.m1.1.1.3.3.4.cmml" type="integer" xref="S2.SS4.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">I\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">italic_I ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó 3 end_POSTSUPERSCRIPT</annotation></semantics></math> and query text <math alttext="T" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><mi id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><ci id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">italic_T</annotation></semantics></math>, a hierarchical visual DirectSAM (SegFormer) encoder is utilized to extract multiscale visual feature maps <math alttext="f_{i}\in\mathbb{R}^{C_{i}\times H_{i}\times W_{i}}" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m3.1"><semantics id="S2.SS4.p1.3.m3.1a"><mrow id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml"><msub id="S2.SS4.p1.3.m3.1.1.2" xref="S2.SS4.p1.3.m3.1.1.2.cmml"><mi id="S2.SS4.p1.3.m3.1.1.2.2" xref="S2.SS4.p1.3.m3.1.1.2.2.cmml">f</mi><mi id="S2.SS4.p1.3.m3.1.1.2.3" xref="S2.SS4.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS4.p1.3.m3.1.1.1" xref="S2.SS4.p1.3.m3.1.1.1.cmml">‚àà</mo><msup id="S2.SS4.p1.3.m3.1.1.3" xref="S2.SS4.p1.3.m3.1.1.3.cmml"><mi id="S2.SS4.p1.3.m3.1.1.3.2" xref="S2.SS4.p1.3.m3.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p1.3.m3.1.1.3.3" xref="S2.SS4.p1.3.m3.1.1.3.3.cmml"><msub id="S2.SS4.p1.3.m3.1.1.3.3.2" xref="S2.SS4.p1.3.m3.1.1.3.3.2.cmml"><mi id="S2.SS4.p1.3.m3.1.1.3.3.2.2" xref="S2.SS4.p1.3.m3.1.1.3.3.2.2.cmml">C</mi><mi id="S2.SS4.p1.3.m3.1.1.3.3.2.3" xref="S2.SS4.p1.3.m3.1.1.3.3.2.3.cmml">i</mi></msub><mo id="S2.SS4.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p1.3.m3.1.1.3.3.1.cmml">√ó</mo><msub id="S2.SS4.p1.3.m3.1.1.3.3.3" xref="S2.SS4.p1.3.m3.1.1.3.3.3.cmml"><mi id="S2.SS4.p1.3.m3.1.1.3.3.3.2" xref="S2.SS4.p1.3.m3.1.1.3.3.3.2.cmml">H</mi><mi id="S2.SS4.p1.3.m3.1.1.3.3.3.3" xref="S2.SS4.p1.3.m3.1.1.3.3.3.3.cmml">i</mi></msub><mo id="S2.SS4.p1.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p1.3.m3.1.1.3.3.1.cmml">√ó</mo><msub id="S2.SS4.p1.3.m3.1.1.3.3.4" xref="S2.SS4.p1.3.m3.1.1.3.3.4.cmml"><mi id="S2.SS4.p1.3.m3.1.1.3.3.4.2" xref="S2.SS4.p1.3.m3.1.1.3.3.4.2.cmml">W</mi><mi id="S2.SS4.p1.3.m3.1.1.3.3.4.3" xref="S2.SS4.p1.3.m3.1.1.3.3.4.3.cmml">i</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b"><apply id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1"><in id="S2.SS4.p1.3.m3.1.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1.1"></in><apply id="S2.SS4.p1.3.m3.1.1.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.2.1.cmml" xref="S2.SS4.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.2.2.cmml" xref="S2.SS4.p1.3.m3.1.1.2.2">ùëì</ci><ci id="S2.SS4.p1.3.m3.1.1.2.3.cmml" xref="S2.SS4.p1.3.m3.1.1.2.3">ùëñ</ci></apply><apply id="S2.SS4.p1.3.m3.1.1.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.3.1.cmml" xref="S2.SS4.p1.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.3.2.cmml" xref="S2.SS4.p1.3.m3.1.1.3.2">‚Ñù</ci><apply id="S2.SS4.p1.3.m3.1.1.3.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3"><times id="S2.SS4.p1.3.m3.1.1.3.3.1.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.1"></times><apply id="S2.SS4.p1.3.m3.1.1.3.3.2.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.3.3.2.1.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.3.3.2.2.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.2.2">ùê∂</ci><ci id="S2.SS4.p1.3.m3.1.1.3.3.2.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.2.3">ùëñ</ci></apply><apply id="S2.SS4.p1.3.m3.1.1.3.3.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.3.3.3.1.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.3.3.3.2.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.3.2">ùêª</ci><ci id="S2.SS4.p1.3.m3.1.1.3.3.3.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.3.3">ùëñ</ci></apply><apply id="S2.SS4.p1.3.m3.1.1.3.3.4.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.4"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.1.1.3.3.4.1.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.4">subscript</csymbol><ci id="S2.SS4.p1.3.m3.1.1.3.3.4.2.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.4.2">ùëä</ci><ci id="S2.SS4.p1.3.m3.1.1.3.3.4.3.cmml" xref="S2.SS4.p1.3.m3.1.1.3.3.4.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">f_{i}\in\mathbb{R}^{C_{i}\times H_{i}\times W_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT √ó italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT √ó italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="i\in\{1,2,3,4\}" class="ltx_Math" display="inline" id="S2.SS4.p1.4.m4.4"><semantics id="S2.SS4.p1.4.m4.4a"><mrow id="S2.SS4.p1.4.m4.4.5" xref="S2.SS4.p1.4.m4.4.5.cmml"><mi id="S2.SS4.p1.4.m4.4.5.2" xref="S2.SS4.p1.4.m4.4.5.2.cmml">i</mi><mo id="S2.SS4.p1.4.m4.4.5.1" xref="S2.SS4.p1.4.m4.4.5.1.cmml">‚àà</mo><mrow id="S2.SS4.p1.4.m4.4.5.3.2" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml"><mo id="S2.SS4.p1.4.m4.4.5.3.2.1" stretchy="false" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml">{</mo><mn id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml">1</mn><mo id="S2.SS4.p1.4.m4.4.5.3.2.2" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml">,</mo><mn id="S2.SS4.p1.4.m4.2.2" xref="S2.SS4.p1.4.m4.2.2.cmml">2</mn><mo id="S2.SS4.p1.4.m4.4.5.3.2.3" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml">,</mo><mn id="S2.SS4.p1.4.m4.3.3" xref="S2.SS4.p1.4.m4.3.3.cmml">3</mn><mo id="S2.SS4.p1.4.m4.4.5.3.2.4" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml">,</mo><mn id="S2.SS4.p1.4.m4.4.4" xref="S2.SS4.p1.4.m4.4.4.cmml">4</mn><mo id="S2.SS4.p1.4.m4.4.5.3.2.5" stretchy="false" xref="S2.SS4.p1.4.m4.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.4b"><apply id="S2.SS4.p1.4.m4.4.5.cmml" xref="S2.SS4.p1.4.m4.4.5"><in id="S2.SS4.p1.4.m4.4.5.1.cmml" xref="S2.SS4.p1.4.m4.4.5.1"></in><ci id="S2.SS4.p1.4.m4.4.5.2.cmml" xref="S2.SS4.p1.4.m4.4.5.2">ùëñ</ci><set id="S2.SS4.p1.4.m4.4.5.3.1.cmml" xref="S2.SS4.p1.4.m4.4.5.3.2"><cn id="S2.SS4.p1.4.m4.1.1.cmml" type="integer" xref="S2.SS4.p1.4.m4.1.1">1</cn><cn id="S2.SS4.p1.4.m4.2.2.cmml" type="integer" xref="S2.SS4.p1.4.m4.2.2">2</cn><cn id="S2.SS4.p1.4.m4.3.3.cmml" type="integer" xref="S2.SS4.p1.4.m4.3.3">3</cn><cn id="S2.SS4.p1.4.m4.4.4.cmml" type="integer" xref="S2.SS4.p1.4.m4.4.4">4</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.4c">i\in\{1,2,3,4\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.4.m4.4d">italic_i ‚àà { 1 , 2 , 3 , 4 }</annotation></semantics></math>, and <math alttext="C_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.5.m5.1"><semantics id="S2.SS4.p1.5.m5.1a"><msub id="S2.SS4.p1.5.m5.1.1" xref="S2.SS4.p1.5.m5.1.1.cmml"><mi id="S2.SS4.p1.5.m5.1.1.2" xref="S2.SS4.p1.5.m5.1.1.2.cmml">C</mi><mi id="S2.SS4.p1.5.m5.1.1.3" xref="S2.SS4.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.5.m5.1b"><apply id="S2.SS4.p1.5.m5.1.1.cmml" xref="S2.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.5.m5.1.1.1.cmml" xref="S2.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS4.p1.5.m5.1.1.2.cmml" xref="S2.SS4.p1.5.m5.1.1.2">ùê∂</ci><ci id="S2.SS4.p1.5.m5.1.1.3.cmml" xref="S2.SS4.p1.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.5.m5.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.5.m5.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="H_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.6.m6.1"><semantics id="S2.SS4.p1.6.m6.1a"><msub id="S2.SS4.p1.6.m6.1.1" xref="S2.SS4.p1.6.m6.1.1.cmml"><mi id="S2.SS4.p1.6.m6.1.1.2" xref="S2.SS4.p1.6.m6.1.1.2.cmml">H</mi><mi id="S2.SS4.p1.6.m6.1.1.3" xref="S2.SS4.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.6.m6.1b"><apply id="S2.SS4.p1.6.m6.1.1.cmml" xref="S2.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m6.1.1.1.cmml" xref="S2.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS4.p1.6.m6.1.1.2.cmml" xref="S2.SS4.p1.6.m6.1.1.2">ùêª</ci><ci id="S2.SS4.p1.6.m6.1.1.3.cmml" xref="S2.SS4.p1.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.6.m6.1c">H_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.6.m6.1d">italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="W_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.7.m7.1"><semantics id="S2.SS4.p1.7.m7.1a"><msub id="S2.SS4.p1.7.m7.1.1" xref="S2.SS4.p1.7.m7.1.1.cmml"><mi id="S2.SS4.p1.7.m7.1.1.2" xref="S2.SS4.p1.7.m7.1.1.2.cmml">W</mi><mi id="S2.SS4.p1.7.m7.1.1.3" xref="S2.SS4.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.7.m7.1b"><apply id="S2.SS4.p1.7.m7.1.1.cmml" xref="S2.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.7.m7.1.1.1.cmml" xref="S2.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS4.p1.7.m7.1.1.2.cmml" xref="S2.SS4.p1.7.m7.1.1.2">ùëä</ci><ci id="S2.SS4.p1.7.m7.1.1.3.cmml" xref="S2.SS4.p1.7.m7.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.7.m7.1c">W_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.7.m7.1d">italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represent the number of channels, height, and width of the feature maps from the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS4.p1.8.m8.1"><semantics id="S2.SS4.p1.8.m8.1a"><mi id="S2.SS4.p1.8.m8.1.1" xref="S2.SS4.p1.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.8.m8.1b"><ci id="S2.SS4.p1.8.m8.1.1.cmml" xref="S2.SS4.p1.8.m8.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.8.m8.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.8.m8.1d">italic_i</annotation></semantics></math>-th stage, respectively. Meanwhile, the query <math alttext="T" class="ltx_Math" display="inline" id="S2.SS4.p1.9.m9.1"><semantics id="S2.SS4.p1.9.m9.1a"><mi id="S2.SS4.p1.9.m9.1.1" xref="S2.SS4.p1.9.m9.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.9.m9.1b"><ci id="S2.SS4.p1.9.m9.1.1.cmml" xref="S2.SS4.p1.9.m9.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.9.m9.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.9.m9.1d">italic_T</annotation></semantics></math> is prompted as <span class="ltx_text ltx_font_italic" id="S2.SS4.p1.13.1">‚ÄùEdge of all </span>{<math alttext="T" class="ltx_Math" display="inline" id="S2.SS4.p1.10.m10.1"><semantics id="S2.SS4.p1.10.m10.1a"><mi id="S2.SS4.p1.10.m10.1.1" xref="S2.SS4.p1.10.m10.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.10.m10.1b"><ci id="S2.SS4.p1.10.m10.1.1.cmml" xref="S2.SS4.p1.10.m10.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.10.m10.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.10.m10.1d">italic_T</annotation></semantics></math>}<span class="ltx_text ltx_font_italic" id="S2.SS4.p1.13.2">s ‚Äù</span> and fed into the text encoder to obtain a features sequence <math alttext="E=\{e_{cls},e_{1},e_{2},...,e_{L}\},E\in\mathbb{R}^{(L+1)\times D}" class="ltx_Math" display="inline" id="S2.SS4.p1.11.m11.4"><semantics id="S2.SS4.p1.11.m11.4a"><mrow id="S2.SS4.p1.11.m11.4.4.2" xref="S2.SS4.p1.11.m11.4.4.3.cmml"><mrow id="S2.SS4.p1.11.m11.3.3.1.1" xref="S2.SS4.p1.11.m11.3.3.1.1.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.6" xref="S2.SS4.p1.11.m11.3.3.1.1.6.cmml">E</mi><mo id="S2.SS4.p1.11.m11.3.3.1.1.5" xref="S2.SS4.p1.11.m11.3.3.1.1.5.cmml">=</mo><mrow id="S2.SS4.p1.11.m11.3.3.1.1.4.4" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml"><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.5" stretchy="false" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">{</mo><msub id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.2" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.2.cmml">e</mi><mrow id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.2" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.2.cmml">c</mi><mo id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.3" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.3.cmml">l</mi><mo id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1a" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.4" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.6" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">,</mo><msub id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.2" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.2.cmml">e</mi><mn id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.3" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.7" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">,</mo><msub id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.2" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.2.cmml">e</mi><mn id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.3" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.3.cmml">2</mn></msub><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.8" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">,</mo><mi id="S2.SS4.p1.11.m11.2.2" mathvariant="normal" xref="S2.SS4.p1.11.m11.2.2.cmml">‚Ä¶</mi><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.9" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">,</mo><msub id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.cmml"><mi id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.2" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.2.cmml">e</mi><mi id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.3" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.3.cmml">L</mi></msub><mo id="S2.SS4.p1.11.m11.3.3.1.1.4.4.10" stretchy="false" xref="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml">}</mo></mrow></mrow><mo id="S2.SS4.p1.11.m11.4.4.2.3" xref="S2.SS4.p1.11.m11.4.4.3a.cmml">,</mo><mrow id="S2.SS4.p1.11.m11.4.4.2.2" xref="S2.SS4.p1.11.m11.4.4.2.2.cmml"><mi id="S2.SS4.p1.11.m11.4.4.2.2.2" xref="S2.SS4.p1.11.m11.4.4.2.2.2.cmml">E</mi><mo id="S2.SS4.p1.11.m11.4.4.2.2.1" xref="S2.SS4.p1.11.m11.4.4.2.2.1.cmml">‚àà</mo><msup id="S2.SS4.p1.11.m11.4.4.2.2.3" xref="S2.SS4.p1.11.m11.4.4.2.2.3.cmml"><mi id="S2.SS4.p1.11.m11.4.4.2.2.3.2" xref="S2.SS4.p1.11.m11.4.4.2.2.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p1.11.m11.1.1.1" xref="S2.SS4.p1.11.m11.1.1.1.cmml"><mrow id="S2.SS4.p1.11.m11.1.1.1.1.1" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.cmml"><mo id="S2.SS4.p1.11.m11.1.1.1.1.1.2" stretchy="false" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS4.p1.11.m11.1.1.1.1.1.1" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.cmml"><mi id="S2.SS4.p1.11.m11.1.1.1.1.1.1.2" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.2.cmml">L</mi><mo id="S2.SS4.p1.11.m11.1.1.1.1.1.1.1" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS4.p1.11.m11.1.1.1.1.1.1.3" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS4.p1.11.m11.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.SS4.p1.11.m11.1.1.1.2" rspace="0.222em" xref="S2.SS4.p1.11.m11.1.1.1.2.cmml">√ó</mo><mi id="S2.SS4.p1.11.m11.1.1.1.3" xref="S2.SS4.p1.11.m11.1.1.1.3.cmml">D</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.11.m11.4b"><apply id="S2.SS4.p1.11.m11.4.4.3.cmml" xref="S2.SS4.p1.11.m11.4.4.2"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.4.4.3a.cmml" xref="S2.SS4.p1.11.m11.4.4.2.3">formulae-sequence</csymbol><apply id="S2.SS4.p1.11.m11.3.3.1.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1"><eq id="S2.SS4.p1.11.m11.3.3.1.1.5.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.5"></eq><ci id="S2.SS4.p1.11.m11.3.3.1.1.6.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.6">ùê∏</ci><set id="S2.SS4.p1.11.m11.3.3.1.1.4.5.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4"><apply id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1">subscript</csymbol><ci id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.2">ùëí</ci><apply id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3"><times id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.1"></times><ci id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.2">ùëê</ci><ci id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.3.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.3">ùëô</ci><ci id="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.4.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.1.1.1.3.4">ùë†</ci></apply></apply><apply id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2">subscript</csymbol><ci id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.2">ùëí</ci><cn id="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.3.cmml" type="integer" xref="S2.SS4.p1.11.m11.3.3.1.1.2.2.2.3">1</cn></apply><apply id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3">subscript</csymbol><ci id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.2">ùëí</ci><cn id="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.3.cmml" type="integer" xref="S2.SS4.p1.11.m11.3.3.1.1.3.3.3.3">2</cn></apply><ci id="S2.SS4.p1.11.m11.2.2.cmml" xref="S2.SS4.p1.11.m11.2.2">‚Ä¶</ci><apply id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.1.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4">subscript</csymbol><ci id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.2.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.2">ùëí</ci><ci id="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.3.cmml" xref="S2.SS4.p1.11.m11.3.3.1.1.4.4.4.3">ùêø</ci></apply></set></apply><apply id="S2.SS4.p1.11.m11.4.4.2.2.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2"><in id="S2.SS4.p1.11.m11.4.4.2.2.1.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2.1"></in><ci id="S2.SS4.p1.11.m11.4.4.2.2.2.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2.2">ùê∏</ci><apply id="S2.SS4.p1.11.m11.4.4.2.2.3.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2.3"><csymbol cd="ambiguous" id="S2.SS4.p1.11.m11.4.4.2.2.3.1.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2.3">superscript</csymbol><ci id="S2.SS4.p1.11.m11.4.4.2.2.3.2.cmml" xref="S2.SS4.p1.11.m11.4.4.2.2.3.2">‚Ñù</ci><apply id="S2.SS4.p1.11.m11.1.1.1.cmml" xref="S2.SS4.p1.11.m11.1.1.1"><times id="S2.SS4.p1.11.m11.1.1.1.2.cmml" xref="S2.SS4.p1.11.m11.1.1.1.2"></times><apply id="S2.SS4.p1.11.m11.1.1.1.1.1.1.cmml" xref="S2.SS4.p1.11.m11.1.1.1.1.1"><plus id="S2.SS4.p1.11.m11.1.1.1.1.1.1.1.cmml" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.1"></plus><ci id="S2.SS4.p1.11.m11.1.1.1.1.1.1.2.cmml" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.2">ùêø</ci><cn id="S2.SS4.p1.11.m11.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS4.p1.11.m11.1.1.1.1.1.1.3">1</cn></apply><ci id="S2.SS4.p1.11.m11.1.1.1.3.cmml" xref="S2.SS4.p1.11.m11.1.1.1.3">ùê∑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.11.m11.4c">E=\{e_{cls},e_{1},e_{2},...,e_{L}\},E\in\mathbb{R}^{(L+1)\times D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.11.m11.4d">italic_E = { italic_e start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_e start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT } , italic_E ‚àà blackboard_R start_POSTSUPERSCRIPT ( italic_L + 1 ) √ó italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. Here, <math alttext="L" class="ltx_Math" display="inline" id="S2.SS4.p1.12.m12.1"><semantics id="S2.SS4.p1.12.m12.1a"><mi id="S2.SS4.p1.12.m12.1.1" xref="S2.SS4.p1.12.m12.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.12.m12.1b"><ci id="S2.SS4.p1.12.m12.1.1.cmml" xref="S2.SS4.p1.12.m12.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.12.m12.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.12.m12.1d">italic_L</annotation></semantics></math> denotes the prompt sequence length and <math alttext="D" class="ltx_Math" display="inline" id="S2.SS4.p1.13.m13.1"><semantics id="S2.SS4.p1.13.m13.1a"><mi id="S2.SS4.p1.13.m13.1.1" xref="S2.SS4.p1.13.m13.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.13.m13.1b"><ci id="S2.SS4.p1.13.m13.1.1.cmml" xref="S2.SS4.p1.13.m13.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.13.m13.1c">D</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.13.m13.1d">italic_D</annotation></semantics></math> denotes the embedding dimension.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.5">The extracted visual feature <math alttext="f_{i}" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><msub id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml"><mi id="S2.SS4.p2.1.m1.1.1.2" xref="S2.SS4.p2.1.m1.1.1.2.cmml">f</mi><mi id="S2.SS4.p2.1.m1.1.1.3" xref="S2.SS4.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><apply id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.1.m1.1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p2.1.m1.1.1.2.cmml" xref="S2.SS4.p2.1.m1.1.1.2">ùëì</ci><ci id="S2.SS4.p2.1.m1.1.1.3.cmml" xref="S2.SS4.p2.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">f_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and text embedding <math alttext="T" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1"><semantics id="S2.SS4.p2.2.m2.1a"><mi id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><ci id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">italic_T</annotation></semantics></math> will be fused through a cross-attention module to select the most important semantics from textual information. This process produces a set of fused features <math alttext="\hat{f}_{i}" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.1"><semantics id="S2.SS4.p2.3.m3.1a"><msub id="S2.SS4.p2.3.m3.1.1" xref="S2.SS4.p2.3.m3.1.1.cmml"><mover accent="true" id="S2.SS4.p2.3.m3.1.1.2" xref="S2.SS4.p2.3.m3.1.1.2.cmml"><mi id="S2.SS4.p2.3.m3.1.1.2.2" xref="S2.SS4.p2.3.m3.1.1.2.2.cmml">f</mi><mo id="S2.SS4.p2.3.m3.1.1.2.1" xref="S2.SS4.p2.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S2.SS4.p2.3.m3.1.1.3" xref="S2.SS4.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.1b"><apply id="S2.SS4.p2.3.m3.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.3.m3.1.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1">subscript</csymbol><apply id="S2.SS4.p2.3.m3.1.1.2.cmml" xref="S2.SS4.p2.3.m3.1.1.2"><ci id="S2.SS4.p2.3.m3.1.1.2.1.cmml" xref="S2.SS4.p2.3.m3.1.1.2.1">^</ci><ci id="S2.SS4.p2.3.m3.1.1.2.2.cmml" xref="S2.SS4.p2.3.m3.1.1.2.2">ùëì</ci></apply><ci id="S2.SS4.p2.3.m3.1.1.3.cmml" xref="S2.SS4.p2.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.1c">\hat{f}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.3.m3.1d">over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> that integrate both visual and textual information. Finally, an MLP-based lightweight contour decoder is utilized. This decoder consolidates all hierarchical multimodal feature maps and merges them into a unified feature map <math alttext="F\in\mathbb{R}^{\frac{H}{4}\times\frac{W}{4}\times 4C}" class="ltx_Math" display="inline" id="S2.SS4.p2.4.m4.1"><semantics id="S2.SS4.p2.4.m4.1a"><mrow id="S2.SS4.p2.4.m4.1.1" xref="S2.SS4.p2.4.m4.1.1.cmml"><mi id="S2.SS4.p2.4.m4.1.1.2" xref="S2.SS4.p2.4.m4.1.1.2.cmml">F</mi><mo id="S2.SS4.p2.4.m4.1.1.1" xref="S2.SS4.p2.4.m4.1.1.1.cmml">‚àà</mo><msup id="S2.SS4.p2.4.m4.1.1.3" xref="S2.SS4.p2.4.m4.1.1.3.cmml"><mi id="S2.SS4.p2.4.m4.1.1.3.2" xref="S2.SS4.p2.4.m4.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p2.4.m4.1.1.3.3" xref="S2.SS4.p2.4.m4.1.1.3.3.cmml"><mrow id="S2.SS4.p2.4.m4.1.1.3.3.2" xref="S2.SS4.p2.4.m4.1.1.3.3.2.cmml"><mfrac id="S2.SS4.p2.4.m4.1.1.3.3.2.2" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2.cmml"><mi id="S2.SS4.p2.4.m4.1.1.3.3.2.2.2" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2.2.cmml">H</mi><mn id="S2.SS4.p2.4.m4.1.1.3.3.2.2.3" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2.3.cmml">4</mn></mfrac><mo id="S2.SS4.p2.4.m4.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p2.4.m4.1.1.3.3.2.1.cmml">√ó</mo><mfrac id="S2.SS4.p2.4.m4.1.1.3.3.2.3" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3.cmml"><mi id="S2.SS4.p2.4.m4.1.1.3.3.2.3.2" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3.2.cmml">W</mi><mn id="S2.SS4.p2.4.m4.1.1.3.3.2.3.3" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3.3.cmml">4</mn></mfrac><mo id="S2.SS4.p2.4.m4.1.1.3.3.2.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p2.4.m4.1.1.3.3.2.1.cmml">√ó</mo><mn id="S2.SS4.p2.4.m4.1.1.3.3.2.4" xref="S2.SS4.p2.4.m4.1.1.3.3.2.4.cmml">4</mn></mrow><mo id="S2.SS4.p2.4.m4.1.1.3.3.1" xref="S2.SS4.p2.4.m4.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S2.SS4.p2.4.m4.1.1.3.3.3" xref="S2.SS4.p2.4.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.4.m4.1b"><apply id="S2.SS4.p2.4.m4.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1"><in id="S2.SS4.p2.4.m4.1.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1.1"></in><ci id="S2.SS4.p2.4.m4.1.1.2.cmml" xref="S2.SS4.p2.4.m4.1.1.2">ùêπ</ci><apply id="S2.SS4.p2.4.m4.1.1.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.p2.4.m4.1.1.3.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS4.p2.4.m4.1.1.3.2.cmml" xref="S2.SS4.p2.4.m4.1.1.3.2">‚Ñù</ci><apply id="S2.SS4.p2.4.m4.1.1.3.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3"><times id="S2.SS4.p2.4.m4.1.1.3.3.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.1"></times><apply id="S2.SS4.p2.4.m4.1.1.3.3.2.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2"><times id="S2.SS4.p2.4.m4.1.1.3.3.2.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.1"></times><apply id="S2.SS4.p2.4.m4.1.1.3.3.2.2.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2"><divide id="S2.SS4.p2.4.m4.1.1.3.3.2.2.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2"></divide><ci id="S2.SS4.p2.4.m4.1.1.3.3.2.2.2.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2.2">ùêª</ci><cn id="S2.SS4.p2.4.m4.1.1.3.3.2.2.3.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.3.3.2.2.3">4</cn></apply><apply id="S2.SS4.p2.4.m4.1.1.3.3.2.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3"><divide id="S2.SS4.p2.4.m4.1.1.3.3.2.3.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3"></divide><ci id="S2.SS4.p2.4.m4.1.1.3.3.2.3.2.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3.2">ùëä</ci><cn id="S2.SS4.p2.4.m4.1.1.3.3.2.3.3.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.3.3.2.3.3">4</cn></apply><cn id="S2.SS4.p2.4.m4.1.1.3.3.2.4.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.3.3.2.4">4</cn></apply><ci id="S2.SS4.p2.4.m4.1.1.3.3.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3.3">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.4.m4.1c">F\in\mathbb{R}^{\frac{H}{4}\times\frac{W}{4}\times 4C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.4.m4.1d">italic_F ‚àà blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_H end_ARG start_ARG 4 end_ARG √ó divide start_ARG italic_W end_ARG start_ARG 4 end_ARG √ó 4 italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>. Subsequently, another MLP layer processes this fused feature map to predict the final contour probability map¬†<math alttext="\hat{Y}\in\mathbb{R}^{H\times W}" class="ltx_Math" display="inline" id="S2.SS4.p2.5.m5.1"><semantics id="S2.SS4.p2.5.m5.1a"><mrow id="S2.SS4.p2.5.m5.1.1" xref="S2.SS4.p2.5.m5.1.1.cmml"><mover accent="true" id="S2.SS4.p2.5.m5.1.1.2" xref="S2.SS4.p2.5.m5.1.1.2.cmml"><mi id="S2.SS4.p2.5.m5.1.1.2.2" xref="S2.SS4.p2.5.m5.1.1.2.2.cmml">Y</mi><mo id="S2.SS4.p2.5.m5.1.1.2.1" xref="S2.SS4.p2.5.m5.1.1.2.1.cmml">^</mo></mover><mo id="S2.SS4.p2.5.m5.1.1.1" xref="S2.SS4.p2.5.m5.1.1.1.cmml">‚àà</mo><msup id="S2.SS4.p2.5.m5.1.1.3" xref="S2.SS4.p2.5.m5.1.1.3.cmml"><mi id="S2.SS4.p2.5.m5.1.1.3.2" xref="S2.SS4.p2.5.m5.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p2.5.m5.1.1.3.3" xref="S2.SS4.p2.5.m5.1.1.3.3.cmml"><mi id="S2.SS4.p2.5.m5.1.1.3.3.2" xref="S2.SS4.p2.5.m5.1.1.3.3.2.cmml">H</mi><mo id="S2.SS4.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS4.p2.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S2.SS4.p2.5.m5.1.1.3.3.3" xref="S2.SS4.p2.5.m5.1.1.3.3.3.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.5.m5.1b"><apply id="S2.SS4.p2.5.m5.1.1.cmml" xref="S2.SS4.p2.5.m5.1.1"><in id="S2.SS4.p2.5.m5.1.1.1.cmml" xref="S2.SS4.p2.5.m5.1.1.1"></in><apply id="S2.SS4.p2.5.m5.1.1.2.cmml" xref="S2.SS4.p2.5.m5.1.1.2"><ci id="S2.SS4.p2.5.m5.1.1.2.1.cmml" xref="S2.SS4.p2.5.m5.1.1.2.1">^</ci><ci id="S2.SS4.p2.5.m5.1.1.2.2.cmml" xref="S2.SS4.p2.5.m5.1.1.2.2">ùëå</ci></apply><apply id="S2.SS4.p2.5.m5.1.1.3.cmml" xref="S2.SS4.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.p2.5.m5.1.1.3.1.cmml" xref="S2.SS4.p2.5.m5.1.1.3">superscript</csymbol><ci id="S2.SS4.p2.5.m5.1.1.3.2.cmml" xref="S2.SS4.p2.5.m5.1.1.3.2">‚Ñù</ci><apply id="S2.SS4.p2.5.m5.1.1.3.3.cmml" xref="S2.SS4.p2.5.m5.1.1.3.3"><times id="S2.SS4.p2.5.m5.1.1.3.3.1.cmml" xref="S2.SS4.p2.5.m5.1.1.3.3.1"></times><ci id="S2.SS4.p2.5.m5.1.1.3.3.2.cmml" xref="S2.SS4.p2.5.m5.1.1.3.3.2">ùêª</ci><ci id="S2.SS4.p2.5.m5.1.1.3.3.3.cmml" xref="S2.SS4.p2.5.m5.1.1.3.3.3">ùëä</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.5.m5.1c">\hat{Y}\in\mathbb{R}^{H\times W}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.5.m5.1d">over^ start_ARG italic_Y end_ARG ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In our study, we implemented the DirectSAM-RS using the Huggingface<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib24" title="">24</a>]</cite> framework based on PyTorch<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib25" title="">25</a>]</cite>.
The backbone architecture of our DirectSAM-RS leveraged Segformer-b5, with its hierarchical feature channels set to {64, 128, 320, 512}.
Additionally, the DirectSAM Prompter incorporated cross-attention blocks, with the number of layers set to 2 and the tokens embedding dimension set to 512. DirectSAM-1800px-0424 weight was utilized as our initializing visual encoder parameter. BERT-base-uncased<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib6" title="">6</a>]</cite> pretrained model was utilized as the initial text encoder. During training and inference, all input images were resized to <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">1024</mn><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn id="S3.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.2">1024</cn><cn id="S3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">1024 √ó 1024</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">For optimization during training, we utilized the AdamW<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib26" title="">26</a>]</cite> optimizer, setting <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msub id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">Œ≤</mi><mn id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><eq id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></eq><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2.2">ùõΩ</ci><cn id="S3.SS1.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.2.3">1</cn></apply><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.p2.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_Œ≤ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><msub id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">Œ≤</mi><mn id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><eq id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></eq><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.2">ùõΩ</ci><cn id="S3.SS1.p2.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.2.3">2</cn></apply><cn id="S3.SS1.p2.2.m2.1.1.3.cmml" type="float" xref="S3.SS1.p2.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\beta_{2}=0.95</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_Œ≤ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95</annotation></semantics></math>, with a weight decay of 0.01. The training and validation processes were conducted on a Linux server equipped with 8 NVIDIA RTX 4090 GPUs, with a batch size of 64. During validation, to ensure a fair comparison, all previous methods and ours followed the same training and testing protocol. In the tests, the max-dist index <math alttext="d_{max}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">d</mi><mrow id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">m</mi><mo id="S3.SS1.p2.3.m3.1.1.3.1" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p2.3.m3.1.1.3.1a" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS1.p2.3.m3.1.1.3.4" xref="S3.SS1.p2.3.m3.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ùëë</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><times id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.1"></times><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">ùëö</ci><ci id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3">ùëé</ci><ci id="S3.SS1.p2.3.m3.1.1.3.4.cmml" xref="S3.SS1.p2.3.m3.1.1.3.4">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">d_{max}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math> for contour extraction was set to 0.0075 for all methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Evaluation Metrics</span>
</h3>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance Comparison on
LRSNY, BUBE and SLSD benchmark tasks. </figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:275.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.0pt,77.5pt) scale(0.639929734491397,0.639929734491397) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1">ODS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1">OIS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.5.1">LineIoU@3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.2.2.2">SwinT+OADecoder¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib27" title="">27</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.2.3">.494</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.2.4">.822</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.2.5">.270</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.3.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.3.3.2">SegFormer-b5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib28" title="">28</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.3.3">.635</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.3.4">.884</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.3.5">.391</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.4.4.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.4.4.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.4.4.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.4.4.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.4.4.3.1">\cellcolor</span>[HTML]F3F3F3.202</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.4.4.4.1">\cellcolor</span>[HTML]F3F3F3.634</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.4.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.4.4.5.1">\cellcolor</span>[HTML]F3F3F3.065</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.5.5.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.5.5.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.5.5.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.5.5.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.5.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.5.5.3.1">\cellcolor</span>[HTML]F3F3F3.650</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.5.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.5.5.4.1">\cellcolor</span>[HTML]F3F3F3.937</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.5.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.5.5.5.1">\cellcolor</span>[HTML]F3F3F3.395</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.6.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.6.6.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.6.6.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.6.6.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.6.6.3.1">\cellcolor</span>[HTML]E2ECFD.653</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.6.6.4.1">\cellcolor</span>[HTML]E2ECFD.911</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.6.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.6.6.5.1">\cellcolor</span>[HTML]E2ECFD.337</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.7.7.1"><span class="ltx_text" id="S3.T1.1.1.7.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.7.7.1.1.1">
<span class="ltx_tr" id="S3.T1.1.1.7.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.7.7.1.1.1.1.1">LRSNY¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib16" title="">16</a>]</cite></span></span>
<span class="ltx_tr" id="S3.T1.1.1.7.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.7.7.1.1.1.2.1">(Road)</span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.7.7.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.7.7.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.7.7.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.7.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.7.7.3.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.7.3.2">.772</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.7.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.7.7.4.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.7.4.2">.962</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.7.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.7.7.5.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.7.5.2">.455</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.8.8.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.8.8.2">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span>
BDCN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib29" title="">29</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.8.8.3">.536</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.8.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.8.8.5">.385</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.9">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.9.9.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.9.9.2">SDLED¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib18" title="">18</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.9.3">.842</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.9.4">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.9.5">.615</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.10">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.10.10.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.10.10.2">SegFormer-b5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib28" title="">28</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.10.3">.844</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.10.4">.956</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.10.5">.501</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.11">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.11.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.11.11.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.11.11.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.11.11.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.11.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.11.11.3.1">\cellcolor</span>[HTML]F3F3F3.237</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.11.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.11.11.4.1">\cellcolor</span>[HTML]F3F3F3.555</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.11.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.11.11.5.1">\cellcolor</span>[HTML]F3F3F3.076</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.12">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.12.12.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.12.12.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.12.12.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.12.12.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.12.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.12.12.3.1">\cellcolor</span>[HTML]F3F3F3.864</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.12.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.12.12.4.1">\cellcolor</span>[HTML]F3F3F3.971</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.12.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.12.12.5.1">\cellcolor</span>[HTML]F3F3F3.518</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13.13">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.13.13.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.13.13.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.13.13.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.13.13.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.13.13.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.13.13.3.1">\cellcolor</span>[HTML]E2ECFD.705</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.13.13.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.13.13.4.1">\cellcolor</span>[HTML]E2ECFD.899</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.13.13.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.13.13.5.1">\cellcolor</span>[HTML]E2ECFD.329</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.14.14.1"><span class="ltx_text" id="S3.T1.1.1.14.14.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.14.14.1.1.1">
<span class="ltx_tr" id="S3.T1.1.1.14.14.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.14.14.1.1.1.1.1">BUBE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib18" title="">18</a>]</cite></span></span>
<span class="ltx_tr" id="S3.T1.1.1.14.14.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.14.14.1.1.1.2.1">(Building)</span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.14.14.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.14.14.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.14.14.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.14.14.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.14.14.3.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.14.14.3.2">.887</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.14.14.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.14.14.4.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.14.14.4.2">.997</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.14.14.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.14.14.5.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.14.14.5.2">.565</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.15.15">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.15.15.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.15.15.2">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span>
HED¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib30" title="">30</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.15.15.3">.897</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.15.15.4">.994</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.15.15.5">.768</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.16.16">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.16.16.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.16.16.2">SegFormer-b5¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib28" title="">28</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.16.16.3">.861</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.16.16.4">.964</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.16.16.5">.742</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.17.17">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.17.17.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.17.17.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.17.17.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.17.17.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.17.17.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.17.17.3.1">\cellcolor</span>[HTML]F3F3F3.712</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.17.17.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.17.17.4.1">\cellcolor</span>[HTML]F3F3F3.942</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.17.17.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.17.17.5.1">\cellcolor</span>[HTML]F3F3F3.449</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.18.18">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.18.18.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.18.18.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.18.18.2.1">\cellcolor</span>[HTML]F3F3F3DirectSAM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib1" title="">1</a>]</cite> <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.18.18.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.18.18.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.18.18.3.1">\cellcolor</span>[HTML]F3F3F3.925</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.18.18.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.18.18.4.1">\cellcolor</span>[HTML]F3F3F3.973</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.18.18.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.18.18.5.1">\cellcolor</span>[HTML]F3F3F3.773</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.19.19">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.1.1.19.19.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.1.1.19.19.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.19.19.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.19.19.2.2">ZS</span>
</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.19.19.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.19.19.3.1">\cellcolor</span>[HTML]E2ECFD.639</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.19.19.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.19.19.4.1">\cellcolor</span>[HTML]E2ECFD.803</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.19.19.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.19.19.5.1">\cellcolor</span>[HTML]E2ECFD.504</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.1.20.20.1"><span class="ltx_text" id="S3.T1.1.1.20.20.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.20.20.1.1.1">
<span class="ltx_tr" id="S3.T1.1.1.20.20.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.20.20.1.1.1.1.1">SLSD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib17" title="">17</a>]</cite></span></span>
<span class="ltx_tr" id="S3.T1.1.1.20.20.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.20.20.1.1.1.2.1">(Coastline)</span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.1.20.20.2">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.20.20.2.1">\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.1.20.20.2.2">FT</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.20.20.3">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.20.20.3.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.20.20.3.2">.958</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.20.20.4">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.20.20.4.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.20.20.4.2">1.000</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.20.20.5">
<span class="ltx_ERROR undefined" id="S3.T1.1.1.20.20.5.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.20.20.5.2">.895</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:433.6pt;height:133.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-66.2pt,20.3pt) scale(0.766079257068868,0.766079257068868) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2">
<div class="ltx_block ltx_minipage ltx_align_middle" id="S3.T3.1.1.1.2.1" style="width:281.9pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">TABLE II: </span>Performance comparison on LoveDA validation set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1.1.1.2.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.2.1.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.2.1.1.1.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1.1.1.1.1.1">Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1.1.1.2.1.1">Architecture</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.2.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.2.1.1.1.2.1">
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1.2.1.1.1.1">Pretraining</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1.2.1.2.1.1">Data</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1.3.1">ODS</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.2.1">LSTM-CNN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib31" title="">31</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.2.2">ILSVRC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.2.3">.416</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.3">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.3.1">ConvLSTM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib33" title="">33</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.3.2">Pascal-VOC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.3.3">.459</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.4">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.4.1">LAVT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib35" title="">35</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.4.2">ImageNet22K¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.4.3">.595</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.5">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.5.1">RRSIS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.5.2">ImageNet22K¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.2.1.1.5.3">.627</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.6.1"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.6.2">Cityscapes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib37" title="">37</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.2.1.1.6.3">.663</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.2.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.2.1.1.7.1"><span class="ltx_text" id="S3.T3.1.1.1.2.1.1.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.2.1.1.7.1.1.1">
<span class="ltx_tr" id="S3.T3.1.1.1.2.1.1.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.7.1.1.1.1.1">SegFormer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib28" title="">28</a>]</cite> +</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.2.1.1.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.2.1.1.7.1.1.1.2.1">Prompter</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.2.1.1.7.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.2.1.1.7.2.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.7.2.2">SA-1B¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib10" title="">10</a>]</cite></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.2.1.1.7.3">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.2.1.1.7.3.1">\cellcolor</span>[HTML]E2ECFD<span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.7.3.2">.694</span>
</td>
</tr>
</table>
</div>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1">
<div class="ltx_block ltx_minipage ltx_align_middle" id="S3.T3.1.1.1.1.1" style="width:260.2pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">TABLE III: </span>Ablation study on SA-1B pretrained weights.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1.1.1.1.1.1">
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.2.1">Class</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.3">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.1.3.1">\columncolor</span>[HTML]E2ECFD
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.1.1.1.1.3.2">
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.3.2.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.3.2.1.1.1">w/</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.3.2.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.3.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.3.2.2.1.1">SA-1B</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.1.1.1.1.1.4.1">
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.4.1.1.1.1">w/o</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.4.1.2.1.1">SA-1B</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.1"><math alttext="\Delta(\%)" class="ltx_math_unparsed" display="inline" id="S3.T3.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1b"><mi id="S3.T3.1.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal">Œî</mi><mrow id="S3.T3.1.1.1.1.1.1.1.1.m1.1.2"><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.2.1" stretchy="false">(</mo><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.2.2">%</mo><mo id="S3.T3.1.1.1.1.1.1.1.1.m1.1.2.3" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.1.m1.1c">\Delta(\%)</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.1.1.1.m1.1d">roman_Œî ( % )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.2.1">Road</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.2.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.2.2.1">\columncolor</span>[HTML]E2ECFD.806</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.2.3">.757</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.2.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.2.4.1" style="color:#FE0000;">-6.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.3">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.3.1">
<span class="ltx_rule" style="width:0.0pt;height:7.9pt;background:black;display:inline-block;"></span>
Agriculture</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.3.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.3.2.1">\columncolor</span>[HTML]E2ECFD.668</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.3.3">.625</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.3.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.3.4.1" style="color:#FE0000;">-6.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.4">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.4.1">
<span class="ltx_rule" style="width:0.0pt;height:7.9pt;background:black;display:inline-block;"></span>
Barren</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.4.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.4.2.1">\columncolor</span>[HTML]E2ECFD.666</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.4.3">.606</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.4.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.4.4.1" style="color:#FE0000;">-9.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.5">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.5.1">
<span class="ltx_rule" style="width:0.0pt;height:7.9pt;background:black;display:inline-block;"></span>
Water</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.5.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.5.2.1">\columncolor</span>[HTML]E2ECFD.711</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.5.3">.639</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.5.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.5.4.1" style="color:#FE0000;">-10.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.6">
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.6.1">
<span class="ltx_rule" style="width:0.0pt;height:7.9pt;background:black;display:inline-block;"></span>
Forest</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.6.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.6.2.1">\columncolor</span>[HTML]E2ECFD.756</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.6.3">.673</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.6.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.6.4.1" style="color:#FE0000;">-11.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.1.1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.1.1.1.7.1">
<span class="ltx_rule" style="width:0.0pt;height:7.9pt;background:black;display:inline-block;"></span>
Building</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.1.1.1.7.2">
<span class="ltx_ERROR undefined" id="S3.T3.1.1.1.1.1.1.7.2.1">\columncolor</span>[HTML]E2ECFD.759</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.1.1.1.7.3">.654</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.1.1.1.1.7.4"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.7.4.1" style="color:#FE0000;">-13.8</span></td>
</tr>
</table>
</div>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">Following HED<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#bib.bib30" title="">30</a>]</cite>, we used the following evaluation metrics in the experiment: Optimal Dataset Scale (ODS) and Optimal Image Scale (OIS). Based on the <math alttext="d_{max}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">d</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">m</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">a</mi><mo id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ùëë</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ùëö</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">ùëé</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">d_{max}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math> index and image size <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_S</annotation></semantics></math>, the pixel tolerance <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_T</annotation></semantics></math> is calculated by¬†<math alttext="T=EvenCeil(S\times d_{max})" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">T</mi><mo id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.3" xref="S3.SS2.p1.4.m4.1.1.1.3.cmml">E</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.4" xref="S3.SS2.p1.4.m4.1.1.1.4.cmml">v</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2a" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.5" xref="S3.SS2.p1.4.m4.1.1.1.5.cmml">e</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2b" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.6" xref="S3.SS2.p1.4.m4.1.1.1.6.cmml">n</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2c" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.7" xref="S3.SS2.p1.4.m4.1.1.1.7.cmml">C</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2d" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.8" xref="S3.SS2.p1.4.m4.1.1.1.8.cmml">e</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2e" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.9" xref="S3.SS2.p1.4.m4.1.1.1.9.cmml">i</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2f" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.10" xref="S3.SS2.p1.4.m4.1.1.1.10.cmml">l</mi><mo id="S3.SS2.p1.4.m4.1.1.1.2g" xref="S3.SS2.p1.4.m4.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.SS2.p1.4.m4.1.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.4.m4.1.1.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.2.cmml">S</mi><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.1.cmml">√ó</mo><msub id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.2.cmml">d</mi><mrow id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.2.cmml">m</mi><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.3.cmml">a</mi><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1a" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.4" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.4.cmml">x</mi></mrow></msub></mrow><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><eq id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"></eq><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">ùëá</ci><apply id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.2"></times><ci id="S3.SS2.p1.4.m4.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.3">ùê∏</ci><ci id="S3.SS2.p1.4.m4.1.1.1.4.cmml" xref="S3.SS2.p1.4.m4.1.1.1.4">ùë£</ci><ci id="S3.SS2.p1.4.m4.1.1.1.5.cmml" xref="S3.SS2.p1.4.m4.1.1.1.5">ùëí</ci><ci id="S3.SS2.p1.4.m4.1.1.1.6.cmml" xref="S3.SS2.p1.4.m4.1.1.1.6">ùëõ</ci><ci id="S3.SS2.p1.4.m4.1.1.1.7.cmml" xref="S3.SS2.p1.4.m4.1.1.1.7">ùê∂</ci><ci id="S3.SS2.p1.4.m4.1.1.1.8.cmml" xref="S3.SS2.p1.4.m4.1.1.1.8">ùëí</ci><ci id="S3.SS2.p1.4.m4.1.1.1.9.cmml" xref="S3.SS2.p1.4.m4.1.1.1.9">ùëñ</ci><ci id="S3.SS2.p1.4.m4.1.1.1.10.cmml" xref="S3.SS2.p1.4.m4.1.1.1.10">ùëô</ci><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.1"></times><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.2">ùëÜ</ci><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.2">ùëë</ci><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3"><times id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.1"></times><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.2">ùëö</ci><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.3">ùëé</ci><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.4.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.3.3.4">ùë•</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">T=EvenCeil(S\times d_{max})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_T = italic_E italic_v italic_e italic_n italic_C italic_e italic_i italic_l ( italic_S √ó italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT )</annotation></semantics></math>. EvenCeil rounds the input <math alttext="S\times d_{max}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">S</mi><mo id="S3.SS2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.5.m5.1.1.1.cmml">√ó</mo><msub id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">d</mi><mrow id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.3.2" xref="S3.SS2.p1.5.m5.1.1.3.3.2.cmml">m</mi><mo id="S3.SS2.p1.5.m5.1.1.3.3.1" xref="S3.SS2.p1.5.m5.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.3.cmml">a</mi><mo id="S3.SS2.p1.5.m5.1.1.3.3.1a" xref="S3.SS2.p1.5.m5.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3.4" xref="S3.SS2.p1.5.m5.1.1.3.3.4.cmml">x</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><times id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></times><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">ùëÜ</ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">ùëë</ci><apply id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"><times id="S3.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.1"></times><ci id="S3.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.2">ùëö</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.3">ùëé</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.4.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.4">ùë•</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">S\times d_{max}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_S √ó italic_d start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT</annotation></semantics></math> up to the nearest even integer. The ODS and OIS were calculated under the tolerance <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_T</annotation></semantics></math> setting. Additionally, we employed Line Intersection over Union with a 3-pixel dilated kernel (LineIoU@3), following the evaluation metrics of BUBE.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S3.F5.2.1">Inference examples of both zero-shot and fine-tuned DirectSAM-RS.</span> Zero-shot DirectSAM-RS (left) demonstrates its ability to flexibly adjust the semantic target according to the given prompt, while fine-tuned DirectSAM-RS (right) produces accurate contours for specific classes.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="314" id="S3.F6.g1" src="x6.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S3.F6.4.1">Data scaling experiment</span>. We used different-sized subsets of <span class="ltx_text ltx_font_typewriter" id="S3.F6.5.2">RemoteContour-34k</span> to train the model and observed that model performance increases <span class="ltx_text ltx_font_italic" id="S3.F6.6.3">linearly</span> as the data scales exponentially.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Benchmarking DirectSAM-RS</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We assessed the performance of DirectSAM-RS in both zero-shot and fine-tuning settings. As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.T1" title="TABLE I ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">I</span></a>, DirectSAM-RS achieved state-of-the-art performance across several downstream contour extraction benchmarks.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot (<span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.1.1">ZS</span>) setting</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">We evaluated our DirectSAM-RS model in a zero-shot setting on three downstream contour extraction tasks, achieving strong performance without task-specific fine-tuning, as shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.T1" title="TABLE I ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">I</span></a>. The improvement between DirectSAM-RS¬†<span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.1.1">ZS</span> and DirectSAM¬†<span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px1.p1.1.2">ZS</span> highlighted the significant impact of integrating textual semantics, demonstrating that language information provides generalization potential for DirectSAM-RS.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning (<span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.1.1">FT</span>) setting</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Furthermore, we fine-tuned DirectSAM-RS with each benchmark dataset training split and evaluated it on the validation split. The results, presented in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.T1" title="TABLE I ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">I</span></a>, revealed that DirectSAM-RS significantly surpasses previous road, coastline, and building extraction SOTA methods, achieving notable 21%, 5%, and 7% improvement in ODS metrics respectively. The visualization inference result is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.F5" title="Figure 5 ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">5</span></a>. The powerful performance on different practical landmark extractions showcased that DirectSAM-RS possesses strong application value.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Ablation of DirectSAM SA-1B Pretraining</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.T3" title="TABLE III ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">III</span></a>, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/chendelong/DirectSAM-1800px-0424" title="">https://huggingface.co/chendelong/DirectSAM-1800px-0424</a></span></span></span>) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.T3" title="TABLE III ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">III</span></a> shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Importance of Scaling-up Pretraining Data</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">We conducted pretraining using three different fractions of the data from each category within each sub-dataset: 50%, 30%, and 10%. This approach was used to quantify <span class="ltx_text ltx_font_typewriter" id="S3.SS5.p1.1.1">RemoteContourRS-34k</span>‚Äôs impact on zero-shot contour extraction performance. Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.06194v1#S3.F6" title="Figure 6 ‚Ä£ III-B Evaluation Metrics ‚Ä£ III Experiments ‚Ä£ Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images * Equal contributions, üñÇ Corresponding author"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates that DirectSAM-RS demonstrates improvement when using 100% of the pretraining data. However, there is still potential for further enhancement in zero-shot setting performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusion and Future Works</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We introduce DirectSAM-RS for semantic contour extraction in remote sensing imagery. Its state-of-the-art performance can be attributed to two key factors: a large and diverse dataset with rich semantic information, and an attached prompter that enables the absorption of shared knowledge across different contour extraction tasks. Looking ahead, several promising directions for future work emerge. First, our experiments suggest that model performance has not yet been saturated with increasing data size. Expanding the dataset further could potentially yield additional improvements. Additionally, while we have demonstrated success in building, road, and coastline extraction, there are numerous other remote sensing tasks that could benefit from DirectSAM-RS. Finally, the model‚Äôs strong zero-shot performance suggests potential for few-shot learning in real-world scenarios with limited labeled data.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D.¬†Chen, S.¬†Cahyawijaya, J.¬†Liu, B.¬†Wang, and P.¬†Fung, ‚ÄúSubobject-level image tokenization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2402.14327</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R.¬†Bommasani, D.¬†A. Hudson, E.¬†Adeli, R.¬†Altman, S.¬†Arora, S.¬†von Arx, M.¬†S. Bernstein, J.¬†Bohg, A.¬†Bosselut, E.¬†Brunskill <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et¬†al.</em>, ‚ÄúOn the opportunities and risks of foundation models,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A.¬†Ramesh, P.¬†Dhariwal, A.¬†Nichol, C.¬†Chu, and M.¬†Chen, ‚ÄúHierarchical text-conditional image generation with clip latents.‚Äù

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.¬†Li, D.¬†Li, C.¬†Xiong, and S.¬†Hoi, ‚ÄúBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International conference on machine learning</em>.¬†¬†¬†PMLR, 2022, pp. 12‚Äâ888‚Äì12‚Äâ900.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L.¬†H. Li, P.¬†Zhang, H.¬†Zhang, J.¬†Yang, C.¬†Li, Y.¬†Zhong, L.¬†Wang, L.¬†Yuan, L.¬†Zhang, J.-N. Hwang <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et¬†al.</em>, ‚ÄúGrounded language-image pre-training,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 10‚Äâ965‚Äì10‚Äâ975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.¬†D. M.-W.¬†C. Kenton and L.¬†K. Toutanova, ‚ÄúBert: Pre-training of deep bidirectional transformers for language understanding,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of naacL-HLT</em>, vol.¬†1, 2019, p.¬†2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T.¬†B. Brown, ‚ÄúLanguage models are few-shot learners,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2005.14165</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Radford, J.¬†W. Kim, C.¬†Hallacy, A.¬†Ramesh, G.¬†Goh, S.¬†Agarwal, G.¬†Sastry, A.¬†Askell, P.¬†Mishkin, J.¬†Clark <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et¬†al.</em>, ‚ÄúLearning transferable visual models from natural language supervision,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">International conference on machine learning</em>.¬†¬†¬†PMLR, 2021, pp. 8748‚Äì8763.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F.¬†Liu, D.¬†Chen, Z.¬†Guan, X.¬†Zhou, J.¬†Zhu, Q.¬†Ye, L.¬†Fu, and J.¬†Zhou, ‚ÄúRemoteclip: A vision language foundation model for remote sensing,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.¬†Kirillov, E.¬†Mintun, N.¬†Ravi, H.¬†Mao, C.¬†Rolland, L.¬†Gustafson, T.¬†Xiao, S.¬†Whitehead, A.¬†C. Berg, W.-Y. Lo <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">et¬†al.</em>, ‚ÄúSegment anything,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2">ICCV 2023</em>, 2023, pp. 4015‚Äì4026.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X.¬†Soria, A.¬†Sappa, P.¬†Humanante, and A.¬†Akbarinia, ‚ÄúDense extreme inception network for edge detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Pattern Recognition</em>, vol. 139, p. 109461, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.¬†Liu, M.-M. Cheng, X.¬†Hu, K.¬†Wang, and X.¬†Bai, ‚ÄúRicher convolutional features for edge detection,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 3000‚Äì3009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L.¬†Huan, N.¬†Xue, X.¬†Zheng, W.¬†He, J.¬†Gong, and G.-S. Xia, ‚ÄúUnmixing convolutional features for crisp edge detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol.¬†44, no.¬†10, pp. 6602‚Äì6609, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X.¬†Soria, G.¬†Pomboza-Junez, and A.¬†D. Sappa, ‚ÄúLdc: Lightweight dense cnn for edge detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Access</em>, vol.¬†10, pp. 68‚Äâ281‚Äì68‚Äâ290, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.¬†Khoreva, R.¬†Benenson, M.¬†Omran, M.¬†Hein, and B.¬†Schiele, ‚ÄúWeakly supervised object boundaries,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 183‚Äì192.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z.¬†Chen, C.¬†Wang, J.¬†Li, N.¬†Xie, Y.¬†Han, and J.¬†Du, ‚ÄúReconstruction bias u-net for road extraction from optical remote sensing images,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J.¬†Feng, S.¬†Wang, and Z.¬†Gu, ‚ÄúA novel sea-land segmentation network for enhanced coastline extraction using satellite remote sensing images,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Space Research</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L.¬†Xia, X.¬†Zhang, J.¬†Zhang, H.¬†Yang, and T.¬†Chen, ‚ÄúBuilding extraction from very-high-resolution remote sensing images using semi-supervised semantic edge detection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Remote Sensing</em>, vol.¬†13, no.¬†11, p. 2187, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.¬†Wang, Z.¬†Zheng, X.¬†Lu, and Y.¬†Zhong, ‚ÄúLoveda: A remote sensing land-cover dataset for domain adaptive semantic segmentation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S.¬†Waqas¬†Zamir, A.¬†Arora, A.¬†Gupta, S.¬†Khan, G.¬†Sun, F.¬†Shahbaz¬†Khan, F.¬†Zhu, L.¬†Shao, G.-S. Xia, and X.¬†Bai, ‚Äúisaid: A large-scale dataset for instance segmentation in aerial images,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>, 2019, pp. 28‚Äì37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
I.¬†Demir, K.¬†Koperski, D.¬†Lindenbaum, G.¬†Pang, J.¬†Huang, S.¬†Basu, F.¬†Hughes, D.¬†Tuia, and R.¬†Raskar, ‚ÄúDeepglobe 2018: A challenge to parse the earth through satellite images,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</em>, 2018, pp. 172‚Äì181.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z.¬†Yuan, L.¬†Mou, Y.¬†Hua, and X.¬†X. Zhu, ‚ÄúRrsis: Referring remote sensing image segmentation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.¬†Suzuki <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et¬†al.</em>, ‚ÄúTopological structural analysis of digitized binary images by border following,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">Computer vision, graphics, and image processing</em>, vol.¬†30, no.¬†1, pp. 32‚Äì46, 1985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†M. Jain, ‚ÄúHugging face,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Introduction to transformers for NLP: With the hugging face library and models to solve problems</em>.¬†¬†¬†Springer, 2022, pp. 51‚Äì67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A.¬†Paszke, S.¬†Gross, F.¬†Massa, A.¬†Lerer, J.¬†Bradbury, G.¬†Chanan, T.¬†Killeen, Z.¬†Lin, N.¬†Gimelshein, L.¬†Antiga <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et¬†al.</em>, ‚ÄúPytorch: An imperative style, high-performance deep learning library,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">Advances in neural information processing systems</em>, vol.¬†32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
I.¬†Loshchilov, ‚ÄúDecoupled weight decay regularization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R.¬†Yang, Y.¬†Zhong, Y.¬†Liu, X.¬†Lu, and L.¬†Zhang, ‚ÄúOcclusion-aware road extraction network for high-resolution remote sensing imagery,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E.¬†Xie, W.¬†Wang, Z.¬†Yu, A.¬†Anandkumar, J.¬†M. Alvarez, and P.¬†Luo, ‚ÄúSegformer: Simple and efficient design for semantic segmentation with transformers,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in Neural Information Processing Systems</em>, vol.¬†34, pp. 12‚Äâ077‚Äì12‚Äâ090, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.¬†He, S.¬†Zhang, M.¬†Yang, Y.¬†Shan, and T.¬†Huang, ‚ÄúBi-directional cascade network for perceptual edge detection,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 3828‚Äì3837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S.¬†Xie and Z.¬†Tu, ‚ÄúHolistically-nested edge detection,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ICCV 2015</em>, 2015, pp. 1395‚Äì1403.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R.¬†Hu, M.¬†Rohrbach, and T.¬†Darrell, ‚ÄúSegmentation from natural language expressions,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ECCV 2016</em>, 2016, pp. 108‚Äì124.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
O.¬†Russakovsky, J.¬†Deng, H.¬†Su, J.¬†Krause, S.¬†Satheesh, S.¬†Ma, Z.¬†Huang, A.¬†Karpathy, A.¬†Khosla, M.¬†Bernstein, A.¬†C. Berg, and L.¬†Fei-Fei, ‚ÄúImageNet Large Scale Visual Recognition Challenge,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International Journal of Computer Vision (IJCV)</em>, vol. 115, no.¬†3, pp. 211‚Äì252, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R.¬†Li, K.¬†Li, Y.¬†Kuo, M.¬†Shu, X.¬†Qi, X.¬†Shen, and J.¬†Jia, ‚ÄúReferring image segmentation via recurrent refinement networks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">CVPR 2018</em>, 2018, pp. 5745‚Äì5753.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M.¬†Everingham, L.¬†Van¬†Gool, C.¬†K.¬†I. Williams, J.¬†Winn, and A.¬†Zisserman, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">International Journal of Computer Vision</em>, vol.¬†88, no.¬†2, pp. 303‚Äì338, Jun. 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z.¬†Yang, J.¬†Wang, Y.¬†Tang, K.¬†Chen, H.¬†Zhao, and P.¬†H. Torr, ‚ÄúLavt: Language-aware vision transformer for referring image segmentation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CVPR 2022</em>, 2022, pp. 18‚Äâ155‚Äì18‚Äâ165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J.¬†Deng, W.¬†Dong, R.¬†Socher, L.-J. Li, K.¬†Li, and L.¬†Fei-Fei, ‚ÄúImagenet: A large-scale hierarchical image database,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2009 IEEE conference on computer vision and pattern recognition</em>.¬†¬†¬†Ieee, 2009, pp. 248‚Äì255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M.¬†Cordts, M.¬†Omran, S.¬†Ramos, T.¬†Rehfeld, M.¬†Enzweiler, R.¬†Benenson, U.¬†Franke, S.¬†Roth, and B.¬†Schiele, ‚ÄúThe cityscapes dataset for semantic urban scene understanding,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 16:43:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
