<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.16342] Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning</title><meta property="og:description" content="The continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace h…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.16342">

<!--Generated on Thu Feb 29 14:46:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Novel Consensus,  Blockchain,  Proof-of-Deep-Learning,  FLChain,  Federated Learning,  Deep Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Boyang Li, Bingyu Shen, Qing Lu, Taeho Jung, Yiyu Shi

<span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{bli1,bshen,qlu2,tjung,yshi4}@nd.edu</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering</span>,
<span id="id3.3.id2" class="ltx_text ltx_font_italic">University of Notre Dame</span>, 
<br class="ltx_break">Notre Dame, Indiana, U.S. 
<br class="ltx_break">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">The continuous thriving of the Blockchain society motivates research in novel designs of schemes supporting cryptocurrencies. Previously multiple Proof-of-Deep-Learning(PoDL) consensuses have been proposed to replace hashing with useful work such as deep learning model training tasks. The energy will be more efficiently used while maintaining the ledger. However deep learning models are problem-specific and can be extremely complex. Current PoDL consensuses still require much work to realize in the real world. In this paper, we proposed a novel consensus named Proof-of-Federated-Learning-Subchain(PoFLSC) to fill the gap. We applied a subchain to record the training, challenging, and auditing activities and emphasized the importance of valuable datasets in partner selection. We simulated 20 miners in the subchain to demonstrate the effectiveness of PoFLSC. When we reduce the pool size concerning the reservation priority order, the drop rate difference in the performance in different scenarios further exhibits that the miner with a higher Shapley Value (SV) will gain a better opportunity to be selected when the size of the subchain pool is limited. In the conducted experiments, the PoFLSC consensus supported the subchain manager to be aware of reservation priority and the core partition of contributors to establish and maintain a competitive subchain.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Novel Consensus, Blockchain, Proof-of-Deep-Learning, FLChain, Federated Learning, Deep Learning

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_publicationid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: <span id="id1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">978-8-3503-1019-1/23/$31.00 ©2023 IEEE  </span> <span id="id1.2" class="ltx_text ltx_inline-block" style="width:433.6pt;"> </span></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The popularity of Blockchain in recent years has drawn an unprecedented amount of research attention in this field. Despite the advantages brought by decentralized mechanism, one of the main drawbacks of Blockchain, the tremendous energy cost, is causing increasing concern<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Currently, most blockchains adopted hashing as workload which can only be solved through brute force. While hashing is secure and hard to crack, there’s very limited additional value can be generated during this calculation process.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As a result, Proof-of-Deep-Learning (PoDL)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> was proposed in recent years to address the “wasting energy” issue, which replaced the hash algorithm with deep learning training tasks as the workload. Deep learning algorithms have been widely applied in various research areas such as computer vision and natural language processing. PoDL adopted the Proof-of-Work (PoW)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and focused on designing pipelines and scheduling deep learning tasks to inherit the security properties.
Successive works of PoDL chains such as DLchain<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and DLBC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> further improved security over the original design.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, training a deep learning (DL) model for a specific task is more complex compared with hashing. In computer vision, the state-of-the-art neural network architectures in areas such as object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and image classification<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> varied drastically depending on the details of the problem. Moreover deep learning models are data driven<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The quality of data used to train the model has a direct impact on the model’s performance.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.16342/assets/miner.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The interaction of a miner with others participants</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we introduce the novel consensus Proof-of-Federate-Learning-Subchain (PoFLSC) which is derived from PoDL. This is the first consensus that integrates the importance of the dataset into the task scheduling process among miners. In PoFLSC, miners are encouraged to collect and contribute their private datasets. Both the complexity of the model to train and the value of the dataset will be considered while miners choose mining partners and rank the priority of scheduled tasks. To enhance the security of PoFLSC, we adopted the challenge and witness verification mechanism from Helium<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
In the Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it demonstrates the interaction between a miner and other contributor individuals/groups. Once the miner joins a subchain as a contributor, it maintains a ping-pong network communication to update the most result status with the subchain manager. Once a miner generates a challenge to a subchain, the miner fetches the model from the subchain and returns the performance of the model based on the private dataset of the miner. Amount visible data contributor, they share their metadata with each other to increase their visibility. The tasks of each roll will be introduced in the subsection <a href="#S3.SS2" title="III-B Miner tasks ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To conclude, a novel consensus named PoFLSC is proposed in this work. With the integration of data value and response time, miners will have the incentive to contribute their dataset and therefore made possible more complex deep learning tasks. Energy efficiency is improved compared with PoDL due to the diversity of the dataset introduced by miners. Model performance will also benefit from the increased scale of the dataset.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Related work</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previously, the PoDL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> consensus mainly utilized the computation capability of miners, and the tasks publisher release the training tasks and the training data. In the PoNAS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the tasks publisher provides training data and relatively flexible training tasks. The target task is to search a neural architecture network, thus the actual training tasks can be different from each other.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Federated learning(FL) is proposed as an efficient deep learning method suitable for decentralized data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Using FL, millions of users can train a model together with local data in their devices, and only gradients will be uploaded and aggregated to update the shared model’s weights. Some advantages are essential using FL compared with traditional deep learning training. Firstly, the data remains private during the entire training process. Secondly, the hardware requirement of the user device is minimum and network resources are saved.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Furthermore, the combination of blockchain and FL resolved some of the existing drawbacks in FL such as centralized server, robust network communication, and lack of incentive<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. FL-blockchains<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> removed the central server role and minimized the impact of remote devices failures. More than that, FL-blockchains naturally motivated devices to participate and contribute to the chain.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">In our PoFLSC, because all miners will have a strong incentive to contribute their dataset, the training process will less likely suffer the problem of data scarcity. As a guarantee, it’s important to fairly reward each miner considering the contribution of each dataset. Wang et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed to use Shapley Values (SV) to measure the contributions of participants in FL. Influence estimation for each party in horizontal FL and Shapley estimation for individual feature value are considered in their work. To alleviate the calculation in this work, Ghorbani et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed another Shapley-Value-based evaluation method to quantify the value of each training datum to the trained model’s performance which is named Truncated Monte Carlo Shapley.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Participants</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">1) Miners are the machines that join the decentralized network and contribute the resource for crypt-currency reward.
In PoFLSC, the ability of DL model training, hosting the pool manager, proxy, high-quality data collecting are all crucial to earn rewards.
</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">2) Full nodes will perform three types of checks.
i) All nodes can behave as Type One full nodes and it will record and check all blocks and transactions;
ii) As Type Two full node, each data contributor will generate challenges periodically to test the DL model performance of all its visible subchain;
iii) The Type Three full node will audit the training procedure by repeating it.
</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">3) Task publisher will provide a certain DL model architecture and the sample dataset. In PoFLSC, the public accessible dataset is less scarce and miners will merge the public dataset into the training if the SV of the dataset is higher than the selection threshold. The majority of training data are private datasets from data contributors.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Assumptions</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">This work is based on three assumptions.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">1) The dataset is clean.
In this work, we only consider the quality of datasets. We assume no adversarial attack, poison attack, or miss label issue happens in any dataset. All mentioned attacking cases has been evaluated in related work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. These attack mechanism will reduce the SV of the miner and potentially reduce the performance of the subchain. But the subchain manager will less likely to reserve a miner with low SV dataset. Therefore, the miner with mentioned attacking cases will not be selected.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">2) The size of a dataset from each data contributor is the same and the size of each sample is the same. With this assumption, we can avoid discussing whether a dataset with large in size and medium in quality is more valuable than a dataset with small in size and high in quality.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">3) Each block will finish one task and the tasks between two blocks are independent. Therefore, we will not discuss the case if data contributors wish to hide part of their private dataset in training. Within one block, all the sharing activity will be recorded and data contributors share the high-quality dataset to their core pool partners.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Design</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Overview</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The design of PoFLSC inherits the PoDL consensus and proposes modifications to improve the effectiveness when facing complex scenarios.
PoFLSC allows miners to select/reject partners, challenge/verify the performance and configuration of each other. Specifically, PoFLSC is a free market for miners to select partners by measuring the response time and data value of other miners. This dynamic free market provides incentives for miners to contribute valuable data or a dataset from the minority group.
</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To train a model is a more complex task than hash algorithm workload. It requests high performance of network, computation, storage, data, and model design. PoNAS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> adopted PoDL design and proposed a mining pool solution on top of NAS workload. The pool manager scheduled strong miners to find potential neural network structures and weak miners to fine-tune the given model. The manager will assign a random sub search space within the full space for each strong miner. This training process among different miners is independent, therefore the performance of a single searching task for a miner depends on the neural network architecture, searching space, training data, etc. The performance of partners in the pool cannot hold back the performance of any other member in the pool. When we adopt this mining pool strategy to distribute deep learning framework, an intuitive method is to split the model or training data of deep learning training tasks and assign it as a sub-task to miners. With this strategy, the miners may cumber the overall performance of the pool if the miners are slow in computation power or network speed.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In PoFLSC, response time and value of data are key factors for miners to select partners. The general idea is to encourage miners work with as many partners as possible within one sub-block time of the subchain, then rank the priority of partners according to their data SV. It is allowed that every miner can contribute to many subchain. In general, one main-block finishes one DL training task and one FL global communication round is finished within one sub-block. All miners experience fours phase including the initial phase, core pool establish phase, secondary pool establish phase, and verification phase.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Miner tasks</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">1) Training:
With given neural network architecture and local dataset or partner shared dataset, the training workforce train DL models for certain local epoch and submit the updated gradients to the host or other miners.
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">2) Hosting the pool manager:
Besides strong computation power, the host is the server to manage multiple other miners.
Within the core pool, the host is selected with the conditions:
i) long term reliability exceeds core pool average value;
ii) among all qualified candidacies, the selected host must have the shortest response time. The host also aggregates the gradients of all updated gradients from others member of the core pool and distributes the average global gradients to each member.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">3) Proxy:
The proxy can forward data or requests. With strong proxy involve, the core pool network performance of bandwidth saving and speeds will be improved.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">4) Data Contributor:
Data contributors will collect private datasets for DL training tasks and the owner of the high-quality dataset will be the preferred partner in the PoFLSC.
Because each miner is allowed to contribute in multiple subchains if the miner is able to submit the updated gradients within one sub-block time of the subchain, the high-quality data contributor will earn more opportunity to be the final winner.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Subchain Structure</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In PoFLSC, each block will finish one target task and each block will be divided into multiple sub-blocks. The sub-block time is predefined when releasing the training tasks and it is longer than multiple local epoch time for slow miners. A relatively short sub-block time will limit the number of miners to contribute to the final DL model, while increasing the global communication rounds. With relatively longer sub-block time, it encourages more miners to join the competition and it encourages miners to participate in multiple subchains. Working with multiple subchains in parallel will increase the opportunity to win. Therefore, the miner or a core pool with a valuable dataset or shorter response time will wish to work with multiple subchains and they will increase their winning chance by upgrading to better hardware or collecting valuable data.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The main structure of subchain is similar to PoDL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> which includes a block head and a block body. The block head structure remains the same as in PoDL. The first sub-block head of the current block is the hash of the previous block head. Among all visible subchains, only one winner subchain will write their sub-block information as the block information for the current block. After it runs into the fourth phase, a subchain becomes a candidate once the number of audits and challenges exceed the threshold, respectively. The winner subchain is the one with the best accuracy among all candidates.
All qualified subchains should receive enough audits and challenges as confirmation.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The block body records transaction ledger and activation transaction as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-C Subchain Structure ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
The transaction ledger remains the same as in PoDL. The activation transaction includes information on training, challenges, and audits. All the challenge results will be the model performance. Auditing will further verify the training procedure. In the activation transaction, it records transaction number, activation type, chain ID and model pair, verifier ID and role pair, miner ID and role pair, data ID, and previous dependency transaction number.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.16342/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A sample of activation transaction</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Four phase of intervals</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">There are four phase intervals within one block. Each interval lasts one or more sub-block time.
There is no certain time limit for all subchains to follow. But within one certain subchain, all members follow the same guide. The concept of this phase is for all participants to be aware of the status of the training procedure, thus it will receive resources for the different types of tasks.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In general, once the miner selects partners to establish a core pool or secondary pool. The miners’ pool will adopt federated learning framework to train DL model together. Each node will run the required number of local epoch and submit the gradients to the host manager. The host manager will update the server average gradients and distribute the updated gradients to each following miner. One sub-block time is one global communication round.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">1) The first phase is the initial phase to select partners to form core pools.
Each miner maintains an event queue for an upcoming response time check and the time for the request in the queue is not part of response time. The response time includes the time for training the task and transferring data. Each miner also checks all visible miners for response time and maintains a list of partner candidates. The list adds a new partner if the sum of the response time of all candidates is shorter than one sub-block time or the response time of the new partner is shorter than the longest partner in the list. When the total response time of all partners is longer than one sub-block time, it removes the partner with the longest response time in the list.
Between each pair of partners, they share their partner list starting from the partner with the lowest response time.
Once a partner is a common partner in the pair, it establishes the core pool. All members of the core pool propose one candidate if the response time is shortest among all unconfirmed candidates in their list. Once any proposed candidate is not on the local list, the miner will raise reject and the candidate selection will stop. If the number of all confirmed candidates is bigger than the threshold number, the core pool
will be established, otherwise, the core pool will be demolished. All participants of each core pool will maintain one subchain.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">2) Once the core pool is established, the phase moves on to the second phase. All miners start training tasks and evaluate SV of the partners in each participated subchains. Based on the SV, the miner will rank the priority of these subchains in the local task schedule. The time for the initial phase is much short than one sub-block time and we consider all subchain will keep the same number of sub-block by the end because all subchains start to generate subblocks within the first sub-block and the length of each sub-block time is the same.
Here, each subchain adopts a synchronous federated learning framework in the second phase. The pool manager nodes collect all unconfirmed candidates from each member and repeat the same algorithm in phase one to select partnerships with other pool managers. But all selection behavior happens among the pool manager of each unconfirmed candidate.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">3) Once the partnership among multiple pool managers is confirmed, it establishes the secondary pool, and the phase moves onto the third. Each subchain will split and merge. For each subchain, the number of branches it split equals the number of partnerships. One branch of all subchain within one partnership merge into one subchain. In the third phase, the workforce nodes receive a global average gradients from their manager and continue training on the updated gradients. The manager adopts an asynchronous federated learning framework which allows the manager to boost the response time with faster miners, thus it helps the core pool increase the priority ranking in other schedules. The manager nodes update response time and evaluate SV with other manager nodes in partnership.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">4) A subchain runs into the fourth phase when a secondary pool achieves both requirements: i) received sufficient audits and challenge, ii) qualified performance of workload. This subchain becomes the candidate of the final winner and it receives more challenges and audits from others.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Verification</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Full nodes will perform three types of checks.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">i) All nodes can behave as Type One full nodes and it will record and check all blocks and transactions. In addition, all challenges and verification activation will be recorded as transactions too.
The Type One results will be transaction confirmation and transaction reliability records of a single node.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">ii) As Type Two full node, each data contributor will generate challenges periodically to test the DL model performance of all its visible subchain.
The Type Two full node is limited to only generating one challenge set in one period of time. The challenge set is multiple random subsets of its private dataset. One subchain will receive only one challenge from one Type Two challenge in one period.
The Type Two results will be the DL model performance and pool performance record.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">iii) The Type Three full node will audit the training procedure by repeating it.
Auditing of the training procedure is a heavy workload and it will be finished with multiple nodes in a group.
The results of auditing will be the comprehensive record of a pool and all participants in their performance and reliability.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2307.16342/assets/histogram_G-Shapley.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The histogram of 20 miners with the G-Shapley Value method.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2307.16342/assets/histogram_LOO.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The histogram of 20 miners with the LOO Shapley Value method.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In the experiment, we evaluated the effectiveness of PoFLSC. When the training process is finished in the main chain, all participants agree that the subchain with the best performance model will write the block and initial the next block.
The PoFLSC consensus is proposed to augment the functionality within each block.
In the first phase, it will initiate the core pool based on the response time.
In the secondary phase, the core pool started training and evaluate the SV of each miner. Because the pool need to finish at least one global epoch, the manager will measure the reservation priorities of each miner. When the resource is limit that the pool manager cannot afford handling all miners, the manager will firstly reserve the miner with the highest ranking in reservation priority queue, therefore the pool manager guarantees that the core pool will finish global epoch within one subblock time. It is important that every subchain can meet this commitment, thus the subchain can continue to join the secondary pool and final competition.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We simulated 100 miners to verify the effectiveness.
The training task is MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> digits handwriting recognition and the DL model is of three 2D convolution layers and two fully connected layers.
Each miner randomly select 30 samples from all samples.
The response time between each pair of miners is given and fixed.
The statistics of the response time is simulated to follow the Gaussian distribution.
The demonstration subchain selected top 20 miners as the members of the core pool.
In the experiment, the SV of all miners are calculated in two different methods.
In the Fig. <a href="#S3.F3" title="Figure 3 ‣ III-E Verification ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 ‣ III-E Verification ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, it shows the histogram of the SV of 20 miners. The Fig. <a href="#S3.F3" title="Figure 3 ‣ III-E Verification ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is for G-Shapley Value method and The Fig. <a href="#S3.F4" title="Figure 4 ‣ III-E Verification ‣ III Design ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is for LOO Shapley Value method</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.16342/assets/stat_G-Shapley.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="569" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The statistics results of the G-Shapley Value of the subchain. Each point represents a member.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2307.16342/assets/x2.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The performance of models in two different reservation priority order shows the effectiveness.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2307.16342/assets/stat_LOO.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="571" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The statistics results of the LOO Shapley Value of the subchain. Each point represents a member.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2307.16342/assets/x3.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="416" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The performance of models in two different reservation priority order shows the effectiveness of PoFLSC.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Performance Evaluation</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.F7" title="Figure 7 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, it shows mean and STD. of member where the x coordinate represents the mean of the SV of each member and the y coordinate represents the STD. of the SV of each member.
Here, we compared the performance differences when we reserved members in ascending order and descending order in the Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The solid line represents the descending order which means we will reserve the candidate with the highest SV first when we start adding more member, and it also means we will kick the member with the lowest SV first when we start removing member. The dotted line shows the results of comparative experiment where the manager will reserve the candidate with the lowest SV first and remove the member with the highest SV first.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Due to the limitation of computation resources, we limited the size of the subchain pool to 20 in our simulation.
When we gradually reduce the pool size, we observe the performance drop in all cases. The drop rate of the solid line is more gentle than the dotted line. When the pool size drops to 60%, the performance drops below 20% in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. So the top 40% of members are the key contributors to be reserved to maintain the subchain in this experiment. In practice, the value of this threshold can be various in different tasks and datasets, but it is important for the manager to be aware of reservation priority and the core partition of contributors to maintain a healthy subchain.
In Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the drop rate of the performance means the relevance between the SV and the value of the data for this task.
The drop rate difference between the dotted lines in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S4.F8" title="Figure 8 ‣ IV-A Experimental Setup ‣ IV Experiment ‣ Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that the relevance is higher with G-shapley method than it is with LOO method.
This experiment shows the effectiveness of this novel PoFLSC consensus and it demonstrates the miner with higher SV will achieve a better opportunity to be selected when the size of the subchain pool is limited.
The reservation priority order based on SV is helpful for a subchain manager to select candidates and establish a competitive subchain.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Previously, the PoDL consensus mainly utilized the computation capability of miners for deep learning training. The training tasks and the training data are given by the tasks publisher.
In the PoNAS, the tasks publisher provided training data and relatively flexible training tasks.
The target task is to search a neural architecture network, thus the actual training tasks can be different from each other.
In this work, we considered the complexity of training a deep learning model and proposed the PoFLSC. Here, it emphasizes the importance of training data in DL models. In addition, the design also concerns the response time of miners, verification between miners. The response time can be expended to computation power of miner and specification of network infrastructure, etc. The verification mechanism further strengthens the security of the system.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Because DL models are data-driven, it is necessary to evaluate the value of the dataset in this novel consensus.
To my best knowledge, we are the first to adopt SV into novel consensus and further evaluate the effectiveness of PoFLSC.
The value of datasets in the consensus also improves blockchain security. For example, if there is no mechanism to evaluate the value of each dataset, the honest miners will have less motivation to collect private datasets or generate synthetic datasets, while the attackers have strong motivation to collect or generate high-quality private datasets to improve model performance and win the competition.
Once the consensus requests private datasets and evaluates the value of each private dataset, all miners will have the motivation to collect high-quality data. As a result, more effective and economical methods of data collection are expected to be proposed. For example, existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> have proved to be capable of generating synthetic training data for computer vision tasks utilizing 3D simulation techniques. Besides helping to create incentives for data collection, SV also contributes to detecting low-quality datasets, miss label samples, and adversarial attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Karmakar, R. Demirer, and R. Gupta, “Bitcoin mining activity and volatility
dynamics in the power market,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Economics Letters</em>, vol. 209, p.
110111, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Chenli, B. Li, Y. Shi, and T. Jung, “Energy-recycling blockchain with
proof-of-deep-learning,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Blockchain and Cryptocurrency (ICBC)</em>.   IEEE, 2019, pp. 19–23.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
C. Chenli, B. Li, and T. Jung, “Dlchain: Blockchain with deep learning as
proof-of-useful-work,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">World Congress on Services</em>.   Springer, 2020, pp. 43–60.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Li, C. Chenli, X. Xu, Y. Shi, and T. Jung, “Dlbc: A deep learning-based
consensus in blockchains for deep learning services,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1904.07349</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv</em>,
2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Solomatine, L. M. See, and R. Abrahart, “Data-driven modelling: concepts,
approaches and experiences,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Practical hydroinformatics</em>, pp. 17–30,
2009.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. S. Garewal, “The helium blockchain,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Practical Blockchains and
Cryptocurrencies</em>.   Springer, 2020, pp.
79–112.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. Li, Q. Lu, W. Jiang, T. Jung, and Y. Shi, “A mining pool solution for novel
proof-of-neural-architecture consensus,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International
Conference on Blockchain and Cryptocurrency (ICBC)</em>.   IEEE, 2021, pp. 1–3.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne,
J. Li, D. Niyato, and H. V. Poor, “Federated learning meets blockchain in
edge computing: Opportunities and challenges,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things
Journal</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device federated
learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Letters</em>, vol. 24, no. 6, pp.
1279–1283, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
G. Wang, C. X. Dang, and Z. Zhou, “Measure contribution of participants in
federated learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Big Data
(Big Data)</em>, 2019, pp. 2597–2604.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for machine
learning,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2019, pp. 2242–2251.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist</em>,
vol. 2, 2010.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
B. Shen, B. Li, and W. J. Scheirer, “Automatic virtual 3d city generation for
synthetic data collection,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Winter Conference on
Applications of Computer Vision Workshops (WACVW)</em>.   IEEE, 2021, pp. 161–170.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
N. Bhandari, “Procedural synthetic data for self-driving cars using 3d
graphics,” Ph.D. dissertation, Massachusetts Institute of Technology, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. Wang, Y. Zeng, M. Jin, and R. Jia, “A unified framework for task-driven
data quality management,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.05484</em>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.16341" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.16342" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.16342">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.16342" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.16343" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 14:46:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
