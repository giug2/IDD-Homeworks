<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.08330] Toward Smart Security Enhancement of Federated Learning Networks</title><meta property="og:description" content="As traditional centralized learning networks (CLNs) are facing increasing challenges in terms of privacy preservation, communication overheads, and scalability, federated learning networks (FLNs) have been proposed as …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Toward Smart Security Enhancement of Federated Learning Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Toward Smart Security Enhancement of Federated Learning Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.08330">

<!--Generated on Fri Mar  8 02:55:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning network (FLN),  security,  poisoning attack,  deep reinforcement learning (DRL).
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Toward Smart Security Enhancement of Federated Learning Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junjie Tan, Ying-Chang Liang, Nguyen Cong Luong, and Dusit Niyato
</span><span class="ltx_author_notes">The paper has been accepted by IEEE Network, pending for publication. Junjie Tan and Ying-Chang Liang are with University of Electronic Science and Technology of China; Nguyen Cong Luong is with PHENIKAA University; Dusit Niyato is with Nanyang Technological University. (Corresponding Author: Ying-Chang Liang)</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">As traditional centralized learning networks (CLNs) are facing increasing challenges in terms of privacy preservation, communication overheads, and scalability, federated learning networks (FLNs) have been proposed as a promising alternative paradigm to support the training of machine learning (ML) models.
In contrast to the centralized data storage and processing in CLNs, FLNs exploit a number of edge devices (EDs) to store data and perform training distributively.
In this way, the EDs in FLNs can keep training data locally, which preserves privacy and reduces communication overheads.
However, since the model training within FLNs relies on the contribution of all EDs, the training process can be disrupted if some of the EDs upload incorrect or falsified training results, i.e., poisoning attacks.
In this paper, we review the vulnerabilities of FLNs, and particularly give an overview of poisoning attacks and mainstream countermeasures.
Nevertheless, the existing countermeasures can only provide passive protection and fail to consider the training fees paid for the contributions of the EDs, resulting in a unnecessarily high training cost.
Hence, we present a smart security enhancement framework for FLNs.
In particular, a verify-before-aggregate (VBA) procedure is developed to identify and remove the non-benign training results from the EDs. Afterward, deep reinforcement learning (DRL) is applied to learn the behaving patterns of the EDs and to actively select the EDs that can provide benign training results and charge low training fees.
Simulation results reveal that the proposed framework can protect FLNs effectively and efficiently.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning network (FLN), security, poisoning attack, deep reinforcement learning (DRL).

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Nowadays, tens of billions of connected devices in the world are generating an unprecedentedly huge amount of data. Due to the data-driven nature, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">machine learning</em> (ML) has benefited greatly from the data explosion and becomes a vital enabler in many fields, such as computer vision, autonomous cars, and communications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The core of ML is about training, i.e., to establish and optimize an ML model, e.g., <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">deep neural network</em> (DNN), to seek the relationship contained in the training data, after which the trained ML model can make prediction or decision-making accurately. In traditional ML paradigms, ML models are trained within a <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">centralized learning network</em> (CLN), where a server collects and stores all training data into a centralized dataset. However, the collection of raw data not only scarifies privacy but also places heavy burdens on communication infrastructures. Moreover, the centralized data storage and processing are also hardly scalable to the ever-increasing data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To overcome the challenges, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">federated learning</em> (FL) has been proposed as a favorable alternative of traditional ML paradigms, transforming CLNs into <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">federated learning networks</em> (FLNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In an FLN, multiple <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">edge devices</em> (EDs), e.g., smartphones, train an ML model collaboratively in a distributed manner under the coordination of a server while keeping training data locally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Specifically, the EDs use their own dataset to perform local training parallelly and upload the results, called model updates, to the server, and the server aggregates the received results to update the ML model. After the repeated interactions between the EDs and the server, the ML model can achieve a satisfactory accuracy, indicating the completion of training.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Compared with CLNs, FLNs have many advantages. On the one hand, the EDs do not need to upload raw data, which avoids privacy concerns and reduces communication overheads. On the other hand, FLNs exploit the EDs to store and process data parallelly, which can benefit from the trends toward massive connected devices and the continuously enhanced storage and computation capabilities in each individual device. Nevertheless, as the training process utilizes the model updates from the EDs, FLNs are susceptible to poisoning attacks. In particular, the malicious EDs can falsify and upload poisoned model updates to the server. Besides, attackers can hijack the model updates transmitted over insecure connections. As a result, the server receives and uses poisoned model updates to obtain a tampered ML model.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we first give a brief overview of FLNs and highlight their vulnerabilities to poisoning attacks. After that, we present a summary of the potential poisoning attacks on FLNs and mainstream countermeasures. By analyzing the existing countermeasures, we find that those methods can only provide FLNs with passive protection by means of removing or devaluing part of the model updates received at the server. Consequently, the existing countermeasures have a low utilization of model updates. The problem becomes even more severe in the FLNs with incentive mechanisms, i.e., the EDs charge certain training fees for contributing model updates, incurring an unnecessarily high training cost. Therefore, we propose a smart security enhancement framework to address the issue. In particular, we develop a <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">verify-before-aggregate</em> (VBA) procedure to enable the server to identify and remove poisoned model updates. Then, <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">deep reinforcement learning</em> (DRL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is applied to learn the behaving patterns of the EDs, which are typically determined by attackers and cannot be known to the server, from historical identification results. With the learnt knowledge, DRL allows the server to actively select the EDs that can provide benign model updates at low training fees. Simulation results demonstrate that the proposed framework can offer effective and efficient protection to FLNs.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Fundamentals of Federated Learning Networks</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Components and Functions of FLNs</span>
</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2008.08330/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustrations of (a) system model of a typical CLN, (b) system model of a typical FLN.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Conventionally, ML models are trained centrally, assuming that training data are fully available and stored in a centralized dataset.
Thus, CLNs are designed to support the centralized training. Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A Components and Functions of FLNs ‣ II Fundamentals of Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>a shows a typical CLN, which consists of a server and several EDs. Particularly, the EDs upload their local data to the server to build the centralized training dataset, with which the server trains ML models by using <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">stochastic gradient descent</em> (SGD) algorithms. However, CLNs face critical issues of privacy violation, heavy communication overheads, and inscalability.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In view of the above issues, FL is proposed to enable distributed model training without a centralized training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. As shown in Fig.<a href="#S2.F1" title="Figure 1 ‣ II-A Components and Functions of FLNs ‣ II Fundamentals of Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>b, a typical FLN is also composed of a server and multiple EDs. In FLNs, an ML model is trained by two iterative steps, namely the local model training at the EDs and the global model aggregation at the server. In the first step, the EDs update their local models with the global model downloaded from the server, execute SGD algorithms to train the local models with their own dataset, and upload the increments of the local models, i.e., model updates, to the server. In the second step, the server aggregates the received model updates by adding their average to the previous global model and obtains a new global model. The two steps constitute a training round. After multiple training rounds, the training will be completed once the global model converges. As such, in FLNs, the server and the EDs only exchange the ML model parameters, instead of raw data, which avoids the privacy issues and reduces the communication overheads significantly.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Characteristics and Vulnerabilities of FLNs</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FLNs can be deployed flexibly in various environments, among which the most complicated one is the mobile implementation. For example, FLN has been adopted to orchestrate numerous mobile devices from all around the world to jointly train a language model for Gboard (https://ai.googleblog.com/2017/04/federated-learning-collaborative.html), while these mobile devices are owned by different users and connect to the server using different types of links, e.g., Wi-Fi and LTE. Therefore, the EDs in an FLN can be heterogenous in terms of ownership, computing capabilities, and connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In addition, since FL relies on the joint effort of all EDs to train an ML model, the ML model will be tampered even if only few EDs work abnormally. Thus, FLNs have a broad attack surface. These characteristics, i.e., heterogeneity and broad attack surface, make FLNs vulnerable mainly from two aspects:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Malicious EDs: As smart devices are nowadays getting more sophisticated, flaws are inescapable and make the devices easily compromised by malwares. Meanwhile, a majority of existing FLN designs do not include an authentication mechanism, and thus they cannot prevent attackers from setting up malicious EDs to join FLNs.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Insecure Connections: The EDs in an FLN may connect to the server via various connections. It is difficult to ensure the security of all the connections. For example, wireless connections are threatened by the openness of wireless channels. Although advanced encryption and verification methods can secure the connections, they induce additional overheads and thus are not always preferred, especially for some resource-limited IoT devices. Consequently, there may exist insecure connections, over which the downloaded global model or the uploaded model update can be hijacked and manipulated.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">By exploiting the vulnerabilities of FLNs, attackers can inject poisoned model updates, which will tamper the global model aggregation and decrease the performance, i.e., accuracy, of the ML model. Such attacks are called poisoning attacks. Next, we describe the poisoning attacks on FLNs and existing countermeasures.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Security Issues</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Poisoning Attacks</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Poisoning attacks aim to degrade an accuracy of the ML model by tampering the global model aggregation of FL with poisoned model updates.
According to the sources of poisoned model updates, poisoning attacks can be categorized into data poisoning and model poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Data Poisoning</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Data poisoning is carried out by means of modifying the training data in the compromised EDs. In particular, attackers flip the labels of training data, such that the compromised EDs train the local models using the poisoned data and generate incorrect model updates.
Depending on the attack intention, the labels can be flipped randomly or specifically. On the one hand, unintentional attacks target at decreasing the prediction accuracy on all classes, and thus attackers can flip the labels randomly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. On the other hand, intentional attacks intend to make the ML model achieve a low accuracy on only certain classes, for which reason attackers only flip the labels of the training data in the concerned classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Model Poisoning</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Instead of modifying training data, model poisoning produces poisoned model updates directly according to some pre-defined rules. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> considers that poisoned model updates can be sampled from a Gaussian distribution. Moreover, attackers can manipulate benign model updates into poisoned ones. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, malicious EDs are designed to flip the sign of benign model updates, in order to guide the aggregated global model towards the direction of decreasing accuracy. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> designs poisoned model updates as the negative increment of the global model, leading to the reverse update of the global model. In fact, all the above attacks are unintentional as defined before. Alternatively, intentional model poisoning methods are considered in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, where attackers use a pre-designed compromised model to craft poisoned model updates, aiming to replace the training ML model with the compromised model.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In addition, a scale factor is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> to magnify poisoned model updates, with the objective to further amplify attack effects.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Countermeasures</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the literature, there mainly exist three types of countermeasures to mitigate the poisoning attacks on FLNs, namely the robust aggregation methods, the anomaly detection-based methods, and the hyper methods.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Robust Aggregation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">In FLNs, the server aggregates the received model updates by taking the average, allowing poisoned model updates to bias the global model directly. Hence, it is highly demanded to develop the aggregation methods that are robust to poisoned model updates. COMED, GEOMED, and COTMED are the commonly-used robust aggregation methods, which are proposed to replace the average operation with component-wise median, geometric median, and component-wise trimmed median, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Another method called KRUM is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which updates the global model by only using the most representative model update, i.e., the one with the shortest Euclidean distances from others. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, each model update is preprocessed to be within a bounded norm to prevent the global model from being overwhelmed by only few poisoned model updates. For the same purpose, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposes a <em id="S3.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">robust stochastic aggregation</em> (RSA) method, in which the server binarizes the received model updates before updating the global model.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Anomaly Detection</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Benign and poisoned model updates have different objectives, making them implicitly different in mathematics. The anomaly detection-based methods aim to classify model updates by identifying the differences among them. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> propose to analyze model updates respectively by calculating their cosine similarities and by mapping them into a low-dimensional latent space. Then, the outliers, i.e., poisoned model updates, can be found and removed based on the obtained cosine similarities or the mapped low-dimensional representations.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>Hyper</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">The hyper methods are proposed by combining the above two types of methods to put their merits into full use. In particular, the server needs to first evaluate the received model updates and then aggregate them in a robust way. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, COMED, COTMED, and KRUM are enhanced with a preliminary evaluation procedure, where a model update will be discarded if it achieves an unacceptable loss or accuracy on an auxiliary dataset. Instead of simply discarding model updates, the methods proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> choose to reweight model updates based on the evaluation results. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the server analyzes model updates by using a repeated median estimator, and builds an accumulated confidence record for each ED. According to the confidence records, model updates are reweighted for aggregation. In contrast, the method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is designed to reweight model updates based on their cosine similarities.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Discussions and Open Issues</span>
</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of poisoning attacks and countermeasures.</figcaption><img src="/html/2008.08330/assets/x2.png" id="S3.T1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="285" alt="[Uncaptioned image]">
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We summarize the poisoning attacks and countermeasures in Table <a href="#S3.T1" title="TABLE I ‣ III-C Discussions and Open Issues ‣ III Security Issues ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. From the table, the existing countermeasures are designed to prevent the global model from aggregating poisoned model updates, by way of discarding or devaluing part of the received model updates.
For example, in KRUM, only the most representative model update can be used to update the global model, while the others are all dropped.
In this sense, the existing countermeasures under-utilize model updates.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The under-utilization issue can be negligible if the server can obtain model updates for free. However, in practice, it is not guaranteed that the EDs naturally volunteer to contribute because they may not be interested in the model learnt by the server. Instead, as the owners of many valuable data, the EDs need to consume both computation and communication resources if they are asked to join the FL process. Hence, it is more practical to consider the existence of an incentive mechanism in FLNs, i.e., the EDs charge the server with training fees for contributing model updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To the best of our knowledge, there is a lack of effective security schemes that can deal with poisoning attacks while maintaining low training costs, which motivates us to develop a smart security enhancement framework.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">A Smart Security Enhancement Framework for Federated Learning Networks</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">FLN with Poisoning Attacks</span>
</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2008.08330/assets/x3.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustrations of (a) system model of the considered FLN with <math id="S4.F2.2.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.F2.2.m1.1b"><mi id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><ci id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">M</annotation></semantics></math> EDs, (b) VBA procedure.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this paper, we consider an FLN with a server and <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">M</annotation></semantics></math> EDs, as shown in Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-A FLN with Poisoning Attacks ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a. Particularly, the server pays and orchestrates the EDs to train ML models. Without loss of generality, the ML model is assumed to a DNN. Training the DNN requires multiple training rounds to complete. As an ED needs to invest resources once requested to participate in a training round, the EDs on request will charge the server with training fees before training local DNNs and uploading model updates. The details of each training round are shown in Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-A FLN with Poisoning Attacks ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a. In the FLN, attackers can exploit the malicious EDs and the insecure connections to launch poisoning attacks.
We refer the malicious EDs and the benign EDs with insecure connections to as vulnerable EDs, which can inject poisoned model updates if they are attacked successfully. In contrast, the benign EDs with secure connections are referred to as secure benign EDs, which are free from attacks and always contribute benign model updates.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To handle poisoned model updates, we propose a VBA procedure. As Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-A FLN with Poisoning Attacks ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b shows, the server first obtains temporary DNNs by adding each model update one at a time to the previous global DNN. Then, the temporary DNNs are tested on an auxiliary dataset.
If a temporary DNN can achieve acceptable accuracy, i.e., does not decrease the accuracy of the previous global DNN by a threshold, the corresponding model update is identified to be benign. The threshold exists to deal with the fact that a benign model update may still decrease the accuracy mildly due to natural fluctuations, and it can be designed empirically to be a small value slightly larger than zero.
Finally, the server updates the global DNN with the identified benign model updates.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In principle, the VBA procedure leverages the auxiliary dataset to determine whether a model update is poisoned or not. Thus, a proper auxiliary dataset should be representative of the pattern that the DNN learns, such that the accuracy degradations caused by poisoned model updates can be detected. We notice that the server typically has a set of test data, which are generally provided by the task publisher to evaluate the learnt DNN. Hence, we can use the test dataset as the auxiliary dataset. Note that although the VBA procedure is designed for poisoning attacks, it can be enhanced with extra methods to handle extensive cases. For example, lazy EDs may simply upload historical global model increments to cheat training fees. In this case, the VBA procedure can be additionally enhanced by checking the similarities between the received model updates and the historical global model increments. Since the server knows all the historical global model increments, the lazy EDs can be detected effectively as long as they copy any of them, even at the end of training. In fact, the lazy EDs tend to copy the recent global model increments to avoid destroying system performance, and thus the server only needs to store and use a few global model increments in the similarity check.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">With the VBA procedure, poisoned model updates can be dropped, but the corresponding EDs have already been paid. Thus, it is highly desired to actively select appropriate EDs for each training round, with the objective to obtain the most benign model updates at the least training costs (i.e., the total training fees paid to the EDs). However, the server does not have enough information to select those appropriate EDs at the beginning of a training round. The reasons are two-fold. First, the behavior of vulnerable EDs is determined by attackers. Due to the malicious intention, attackers are impossible to inform the up-coming attacks in advance. Second, a model update cannot be recognized to be benign or not until it has been uploaded, while only the selected EDs can upload model updates in each training round. In other words, the server can only have a partial and historical observation of the EDs. Fortunately, the emergence of DRL makes it possible to make decisions without sufficient information. Next, we develop a smart ED selection strategy based on DRL, which empowers the server to learn the behavior of the EDs and to select EDs properly for each training round.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">A DRL-based ED Selection Strategy</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">DRL is an important ML technique developed for decision-making in a dynamic environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Specifically, the decision maker, called agent, can learn environmental patterns and an optimal decision-making policy without requiring prior knowledge about the environment.
Hence, if we respectively model the server and the EDs as the agent and the environment, DRL can be used to enable the server to learn the behaving patterns of the EDs and to select the proper EDs, despite lacking sufficient prior knowledge.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To apply DRL, the interactions between the server and the EDs should be formulated as a <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">Markov decision process</em> (MDP), which consists of three key elements, namely state, action, and reward.
At the beginning of a training round, a state is obtained by the server as the basis of decision-making, and thus it includes the status of all EDs, i.e., whether their previously uploaded model updates are benign or not.
Then, the server takes an action, i.e., selects a set of EDs.
After collecting all the model updates, the server will receive a reward to indicate how good the taken action is.
To encourage the server to obtain more benign model updates at lower training costs, we design the reward to be the number of received benign model updates minus the training costs, which is also the utility of the training round.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Solving the formulated MDP is to find the optimal ED selection policy that maximizes the long-term cumulative discounted reward.
DRL can solve the MDP effectively, of which the key idea is to establish a <em id="S4.SS2.p3.1.1" class="ltx_emph ltx_font_italic">deep Q-network</em> (DQN) to approximate the Q-values for each state-action pair, i.e., the cumulative discounted rewards achieved by taking each action in each state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The DQN is a DNN containing an input layer, an output layer, and some hidden layers.
Given a specific state as the input, the DQN can predict the Q-value for each action.
To achieve accurate predictions, the DQN needs to be trained by a trial-and-error procedure, in which the agent generates experience by continuously interacting with the environment and feeding the recoded experience into the DQN.
The DQN will eventually converge after analyzing massive historical experience.
With the converged DQN, the server can always select the most appropriate EDs by choosing the action with the largest Q-value, as long as a state is given.
Note that we do not provide all technical details due to space limit. The interested readers can refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> for more details about DRL.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Nevertheless, it is not the end of the story. Recall that only the EDs selected in a training round can upload model updates, while the status of an ED needs to be determined by running the VBA procedure with its model update received at the server. Therefore, the server can only have a partial observation of the state, which transforms the MDP into a <em id="S4.SS2.p4.1.1" class="ltx_emph ltx_font_italic">partially observable Markov decision process</em> (POMDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The information included in a single observation is incomplete, but fortunately we notice that the historical status of the unselected EDs can help to supplement the missing information. In other words, a state can be predicted from the present observation and some historical observations. Following this idea, we first define a pseudo state as a sequence of present and historical observations, and the corresponding taken actions. Next, we design a <em id="S4.SS2.p4.1.2" class="ltx_emph ltx_font_italic">deep recurrent Q-network</em> (DRQN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> by inserting a <em id="S4.SS2.p4.1.3" class="ltx_emph ltx_font_italic">long-short-term-memory</em> (LSTM) layer into the vanilla DQN. LSTM is kind of DNN structure designed for analyzing sequential data, e.g., a period of sound waves. Since the pseudo state is exactly a time sequence of observations and actions, LSTM can be used to analyze the pseudo state and to predict the real state behind it. After the analysis of LSTM, a predicted state can be extracted from the pseudo state and be handled by the DQN.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Numerical Results</span>
</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2008.08330/assets/x4.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="453" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The test accuracy and average utility achieved by the FedAvg and COMED algorithms, and the proposed framework.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2008.08330/assets/x5.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The efficiency of the proposed framework.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">In this part, we conduct simulations to evaluate the proposed smart security enhancement framework in an FLN with ten EDs, including both secure benign EDs and vulnerable EDs. Since vulnerable EDs may upload poisoned model updates, they tend to set a lower price than that of secure benign EDs to attract more requests. In particular, the secure benign EDs and the vulnerable EDs are considered to respectively charge the price of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="float" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.9</annotation></semantics></math> and <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="float" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">0.3</annotation></semantics></math> for each request. If a vulnerable ED is under attack, the labels of training data will be flipped randomly.
Moreover, a scale factor is imposed to magnify the poisoned model updates by <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="integer" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">20</annotation></semantics></math> times. Considering that attackers can launch attacks dynamically, the vulnerable EDs behave differently in each training round, i.e., each vulnerable ED can successfully upload a benign model update according to a hidden pattern.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.11" class="ltx_p">We consider that <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn type="integer" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">20</annotation></semantics></math> learning tasks are published to the FLN, which we simulate by repeatedly training a DNN over an MNIST dataset with <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="60000" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">60000</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="integer" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">60000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">60000</annotation></semantics></math> training samples and <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="10000" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mn id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">10000</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><cn type="integer" id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">10000</annotation></semantics></math> test samples. The training samples are distributed to each ED equally. The DNN to be learnt contains <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn type="integer" id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">2</annotation></semantics></math> <em id="S4.SS3.p2.11.1" class="ltx_emph ltx_font_italic">fully-connected</em> (FC) hidden layers with <math id="S4.SS3.p2.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn type="integer" id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">100</annotation></semantics></math> neurons in each layer. Each learning task consists of <math id="S4.SS3.p2.6.m6.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S4.SS3.p2.6.m6.1a"><mn id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><cn type="integer" id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">1000</annotation></semantics></math> training rounds, and five EDs will be selected in each round to train locally with the minibatch size of <math id="S4.SS3.p2.7.m7.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS3.p2.7.m7.1a"><mn id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><cn type="integer" id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">100</annotation></semantics></math> for <math id="S4.SS3.p2.8.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS3.p2.8.m8.1a"><mn id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><cn type="integer" id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">1</annotation></semantics></math> epoch, using Adam as the SGD optimizer. As for the DRQN, it contains an LSTM hidden layer with <math id="S4.SS3.p2.9.m9.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS3.p2.9.m9.1a"><mn id="S4.SS3.p2.9.m9.1.1" xref="S4.SS3.p2.9.m9.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><cn type="integer" id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">32</annotation></semantics></math> LSTM units and an FC hidden layer with <math id="S4.SS3.p2.10.m10.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS3.p2.10.m10.1a"><mn id="S4.SS3.p2.10.m10.1.1" xref="S4.SS3.p2.10.m10.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m10.1b"><cn type="integer" id="S4.SS3.p2.10.m10.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m10.1c">200</annotation></semantics></math> neurons. Each pseudo state contains three successive observations. The threshold in VBA is set to be <math id="S4.SS3.p2.11.m11.1" class="ltx_Math" alttext="0.005" display="inline"><semantics id="S4.SS3.p2.11.m11.1a"><mn id="S4.SS3.p2.11.m11.1.1" xref="S4.SS3.p2.11.m11.1.1.cmml">0.005</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.11.m11.1b"><cn type="float" id="S4.SS3.p2.11.m11.1.1.cmml" xref="S4.SS3.p2.11.m11.1.1">0.005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.11.m11.1c">0.005</annotation></semantics></math>. Additionally, we take the FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and the COMED algorithms as the benchmark algorithms. For the proposed framework, we consider the full implementation, i.e., “VBA+DRL”, and the VBA-only implementation, where the VBA procedure works with random ED selection.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.4" class="ltx_p">We evaluate and depict in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the performance of the four schemes by increasing the number of the vulnerable EDs from <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn type="integer" id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">3</annotation></semantics></math> to <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn type="integer" id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">9</annotation></semantics></math>. Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>a shows the test accuracy of the DNNs after training, and each data point is the average result of all learning tasks. As seen, the test accuracy of the DNNs trained by the benchmark algorithms is degraded significantly due to attacks. In particular, the FedAvg algorithm is the worst because it does not have any robustness design. For the COMED algorithm, it can train DNNs to achieve an acceptable accuracy of around <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="96\%" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mn id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">96</mn><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">96\%</annotation></semantics></math> at the presence of three vulnerable EDs but fails to prevent severe degradation with more vulnerable EDs.
In contrast, with either implementation, the DNNs trained by the proposed framework can always reach an accuracy of over <math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="97\%" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mrow id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml"><mn id="S4.SS3.p3.4.m4.1.1.2" xref="S4.SS3.p3.4.m4.1.1.2.cmml">97</mn><mo id="S4.SS3.p3.4.m4.1.1.1" xref="S4.SS3.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><apply id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p3.4.m4.1.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.4.m4.1.1.2.cmml" xref="S4.SS3.p3.4.m4.1.1.2">97</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">97\%</annotation></semantics></math>, regardless of the number of the vulnerable EDs in the network. This demonstrates that the VBA procedure can protect the FLN from poisoning attacks effectively. Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>b shows the average utility of each training round. From the figure, the VBA-only implementation and the benchmark algorithms achieve lower utility with more vulnerable EDs. This is because they all select EDs randomly while the growth of vulnerable EDs increases the chance of obtaining poisoned model updates. Meanwhile, the full implementation of our proposed framework can achieve higher utility. The reasons are two-fold. First, more vulnerable EDs bring in more potential benign model updates charging low training fees. Second, enabled by DRL, the server can select EDs smartly after considering both attacks and training costs. Hence, compared with the random ED selection, the DRL-based one can obtain more low-cost and benign model updates.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.8" class="ltx_p">Next, we look at the efficiency of the proposed framework by taking the case with <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mn id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><cn type="integer" id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">9</annotation></semantics></math> vulnerable EDs for example. Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a shows the evolution of the rewards achieved by DRL for <math id="S4.SS3.p4.2.m2.1" class="ltx_Math" alttext="20000" display="inline"><semantics id="S4.SS3.p4.2.m2.1a"><mn id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml">20000</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><cn type="integer" id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1">20000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">20000</annotation></semantics></math> training rounds in the full implementation, which cover all the <math id="S4.SS3.p4.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p4.3.m3.1a"><mn id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><cn type="integer" id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">20</annotation></semantics></math> learning tasks. As seen, DRL takes around <math id="S4.SS3.p4.4.m4.1" class="ltx_Math" alttext="8000" display="inline"><semantics id="S4.SS3.p4.4.m4.1a"><mn id="S4.SS3.p4.4.m4.1.1" xref="S4.SS3.p4.4.m4.1.1.cmml">8000</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.4.m4.1b"><cn type="integer" id="S4.SS3.p4.4.m4.1.1.cmml" xref="S4.SS3.p4.4.m4.1.1">8000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.4.m4.1c">8000</annotation></semantics></math> training rounds (i.e., <math id="S4.SS3.p4.5.m5.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS3.p4.5.m5.1a"><mn id="S4.SS3.p4.5.m5.1.1" xref="S4.SS3.p4.5.m5.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.5.m5.1b"><cn type="integer" id="S4.SS3.p4.5.m5.1.1.cmml" xref="S4.SS3.p4.5.m5.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.5.m5.1c">8</annotation></semantics></math> learning tasks) to converge. Note that although DRL needs some time to converge, it can maintain high performance afterwards. Thus, the convergence time is tolerable from a long-term perspective of the FLN.
We further depict in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b the impacts of DRL on the convergence speed of federated learning. In particular, the red curve shows the evolution of the test accuracy of the DNN trained in the last learning task, where DRL has converged, while the black curve shows that in the first learning task, where DRL just starts to learn and has not converged. For comparison, Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b also shows the evolution of the VBA-only implementation, and each data point is the average result of all learning tasks. As seen, the accuracy increases sharply and can converge at around <math id="S4.SS3.p4.6.m6.1" class="ltx_Math" alttext="600" display="inline"><semantics id="S4.SS3.p4.6.m6.1a"><mn id="S4.SS3.p4.6.m6.1.1" xref="S4.SS3.p4.6.m6.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.6.m6.1b"><cn type="integer" id="S4.SS3.p4.6.m6.1.1.cmml" xref="S4.SS3.p4.6.m6.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.6.m6.1c">600</annotation></semantics></math> training rounds for the last task. As for the first task, the convergence rate is slower and the curve is the same as that of the VBA-only implementation. Hence, as DRL is converging, FL can achieve better convergence performance.
Meanwhile, even before the convergence, the performance of the DRL-based ED selection is not worse than that of the random ED selection.
Finally, in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C Numerical Results ‣ IV A Smart Security Enhancement Framework for Federated Learning Networks ‣ Toward Smart Security Enhancement of Federated Learning Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>c, we compare the four schemes regarding the average time for each training round. Compared with the benchmark algorithms, the proposed framework has higher latency due to the more complicated aggregation procedure. However, the VBA-only and the full implementations can still finish a training round respectively within <math id="S4.SS3.p4.7.m7.1" class="ltx_Math" alttext="279" display="inline"><semantics id="S4.SS3.p4.7.m7.1a"><mn id="S4.SS3.p4.7.m7.1.1" xref="S4.SS3.p4.7.m7.1.1.cmml">279</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.7.m7.1b"><cn type="integer" id="S4.SS3.p4.7.m7.1.1.cmml" xref="S4.SS3.p4.7.m7.1.1">279</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.7.m7.1c">279</annotation></semantics></math>ms and <math id="S4.SS3.p4.8.m8.1" class="ltx_Math" alttext="297" display="inline"><semantics id="S4.SS3.p4.8.m8.1a"><mn id="S4.SS3.p4.8.m8.1.1" xref="S4.SS3.p4.8.m8.1.1.cmml">297</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.8.m8.1b"><cn type="integer" id="S4.SS3.p4.8.m8.1.1.cmml" xref="S4.SS3.p4.8.m8.1.1">297</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.8.m8.1c">297</annotation></semantics></math>ms. In practice, the FLN can choose either implementation depending on the practical latency or computation requirements.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As FLNs are susceptible to poisoning attacks, where attackers attempt to disrupt the training process of FL by injecting poisoned model updates, we have investigated the security issues of FLNs in this paper. In particular, we have first reviewed the vulnerabilities of FLNs, and then provided an overview of poisoning attacks and mainstream countermeasures. However, it has been found that the existing countermeasures fail to consider the costs of requesting EDs to contribute model updates and under-utilize model updates, leading to inefficient training. Therefore, we have proposed a smart security enhancement framework to address this issue. In the proposed framework, we have designed a VBA procedure for identifying and removing poisoned model updates and a DRL-based ED selection strategy for intelligently selecting the EDs that can provide low-cost and benign model updates. Numerical results have demonstrated that the proposed framework can protect FLNs effectively and efficiently.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and D. I.
Kim, “Applications of deep reinforcement learning in communications and
networking: A survey,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surveys Tuts.</em>, vol. 21, no. 4,
pp. 3133–3174, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, vol. 10, no. 2,
pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. AISTATS</em>, vol. 54, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Human-level control through deep reinforcement learning,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">Nature</em>,
vol. 518, no. 7540, pp. 529–533, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Surveys Tuts.</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. Muñoz-González, K. T. Co, and E. C. Lupu, “Byzantine-robust
federated machine learning through adaptive model averaging,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1909.05125</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Xie, S. Koyejo, and I. Gupta, “SLSGD: Secure and efficient distributed
on-device machine learning,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. ECML PKDD</em>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Fu, C. Xie, B. Li, and Q. Chen, “Attack-resistant federated learning with
residual-based reweighting,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.11464</em>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Fung, C. J. Yoon, and I. Beschastnikh, “Mitigating sybils in federated
learning poisoning,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04866</em>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Dong, J. Cheng, M. J. Hossain, and V. C. Leung, “Secure distributed
on-device learning networks with byzantine adversaries,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Netw.</em>,
vol. 33, no. 6, pp. 180–187, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, “RSA: Byzantine-robust
stochastic aggregation methods for distributed learning from heterogeneous
datasets,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. AAAI</em>, vol. 33, 2019, pp. 1544–1551.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Li, Y. Cheng, W. Wang, Y. Liu, and T. Chen, “Learning to detect malicious
clients for robust federated learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2002.00211</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local model poisoning attacks to
byzantine-robust federated learning,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1911.11815</em>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can you really backdoor
federated learning?” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NeurIPS Wkshps.</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Hausknecht and P. Stone, “Deep recurrent Q-learning for partially
observable MDP,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">AAAI Fall Symp. Ser.</em>, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.08329" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.08330" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.08330">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.08330" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.08331" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 02:55:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
