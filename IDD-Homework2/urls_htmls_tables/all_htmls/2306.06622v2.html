<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.06622] Weakly Supervised Visual Question Answer Generation</title><meta property="og:description" content="Growing interest in conversational agents promote two-way human-computer communications involving asking and answering visual questions have become an active area of research in AI. Thus, generation of visual question-…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weakly Supervised Visual Question Answer Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Weakly Supervised Visual Question Answer Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.06622">

<!--Generated on Thu Feb 29 01:12:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Weakly Supervised Visual Question Answer Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Charani Alampalle
<br class="ltx_break">AlphaICs
<br class="ltx_break">Bengaluru, India
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">nagasai.charani@alphaics.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shamanthak Hegde
<br class="ltx_break">KLE Technological University
<br class="ltx_break">Hubballi, India
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">01fe19bcs233@kletech.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Soumya Jahagirdar
<br class="ltx_break">CVIT, IIIT Hyderabad
<br class="ltx_break">Hyderabad, India
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">soumya.jahagirdar@research.iiit.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shankar Gangisetty
<br class="ltx_break">IIIT Hyderabad
<br class="ltx_break">Hyderabad, India
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">shankar.gangisetty@ihub-data.iiit.ac.in</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Growing interest in conversational agents promote two-way human-computer communications involving asking and answering visual questions have become an active area of research in AI. Thus, generation of visual question-answer pair(s) becomes an important and challenging task. To address this issue, we propose a weakly-supervised visual question answer generation method that generates a relevant question-answer pairs for a given input image and associated caption.
Most of the prior works are supervised and depend on the annotated question-answer datasets. In our work, we present a weakly supervised method that synthetically generates question-answer pairs procedurally from visual information and captions. The proposed method initially extracts list of answer words, then does nearest question generation that uses the caption and answer word to generate synthetic question. Next, the relevant question generator converts the nearest question to relevant language question by dependency parsing and in-order tree traversal, finally, fine-tune a ViLBERT model with the question-answer pair(s) generated at end. We perform an exhaustive experimental analysis on VQA dataset and see that our model significantly outperform SOTA methods on BLEU scores. We also show the results wrt baseline models and ablation study.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Conversational agents that can communicate with a human have been an active area of research and becoming popular due to artificial intelligence (AI). In conversational agent communication, visual question answering (VQA) is not only the desired characteristic where the agent answers a natural language question about the image, but also an intelligent agent should have the ability to ask a meaningful and relevant question with respect to its current visual perception. Keeping these points in mind, we can say that the best way to establish communication between humans and machines is by making machines how to ask meaningful and relevant question and at the same time, answer it properly when asked. Recently more focus is given to understanding the scene, asking and answering the questions from images and videos.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.06622/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Our proposed weakly supervised VQA generation method gives importance to visual properties relevant to the given input image in order to generate specific question-answer pairs. Our results are automatically generated compared to VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> i.e., manually generated question-answer pairs.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Works in various domains are mainly focusing on VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and visual question generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Given an image, generating meaningful questions, also known as visual question generation (VQG) is an essential component of a conversational agent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> along with answering the questions. Visual question answer generation (VQAG) is a precursor of visual dialogue systems and might help in building large-scale VQA datasets automatically with less human effort. All the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> on question answer pair generation are dependent largely on datasets which was manual and tedious task to train. With less human effort to build VQA datasets needed for training, we build a VQAG model that can generate meaningful question-answer pairs for a given image and associated captions. Works in the direction of question-answer generation for image, given the caption focuses on generating generic questions or category based answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Unlike these we focus on category based questions (As categories we consider six types of question words like “<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">how many</span>”, “<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">who</span>”, “<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">what</span>”, “<span id="S1.p2.1.4" class="ltx_text ltx_font_italic">which</span>”, “<span id="S1.p2.1.5" class="ltx_text ltx_font_italic">how much</span>”, and “<span id="S1.p2.1.6" class="ltx_text ltx_font_italic">where</span>”) which help the conversational agents to communicate properly in understanding the context. Good question-answer pair is the one that has a tightly focused purpose and must be relevant to the image content. In this work, we fill-up the gap prevailing in the literature by introducing a method to solve the problem of generating question-answer pairs for the given image and associated captions with question being categorised. We refer our work as VQAG.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Without depending on ground-truth QA pair(s) of images, the goal of VQAG is given a natural image and associated captions, list of objects are extracted from the image, generate a natural language question using caption, whose answer is one of the list of objects being identified. As we are not depending on ground-truth QA pair(s) and using captions instead we name our work as Weakly supervised Visual Question Answer Generation. As shown in Fig <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> given an image and associated caption(s), our aim is to generate a relevant language question such that the answer is in the list of the objects identified from the image. Here list of objects is [Fisbee, Person]. The problem is challenging as it requires in-depth semantic interpretation of the caption and from the image the visual content needs to generate meaningful and relevant questions. Question-answer pair generation has been a well-explored area in the language community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, vision and language community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, vision and language works often ignore the important visual information or objects appearing in the image, and only restrict themselves to the overall visual content while generating question and answer pairs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">It should be noted that objects in the image helps not only in asking semantically meaningful and relevant questions connecting visual and textual content, but also helps to avoid generic questions as well as generates detailed question-answer pair(s). Consider for example given an image with two teams of players playing frisbee in ground and enjoying the game shown in Fig <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our proposed method automatically generate questions that are meaningful, relevant, non-generic and detailed, such as “<span id="S1.p4.1.1" class="ltx_text ltx_font_italic">What competitive game a group of guys are playing?</span>” ,“<span id="S1.p4.1.2" class="ltx_text ltx_font_italic">How many teams of people are cluttered together during a frisbee game?</span>”, “<span id="S1.p4.1.3" class="ltx_text ltx_font_italic">What is the game in some sport players are competing and having fun?</span>”. While the questions for the same image in VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has generic questions like “<span id="S1.p4.1.4" class="ltx_text ltx_font_italic">What sport are they playing?</span>”, “<span id="S1.p4.1.5" class="ltx_text ltx_font_italic">Are all the people in the photo wearing shorts?</span>”, “<span id="S1.p4.1.6" class="ltx_text ltx_font_italic">What numbers are the white players?</span>” respectively. We see that our proposed VQAG method generates more meaningful and detailed question-answer pairs unlike VQA. In our proposed approach, we first extract answer from the list of objects identified from the image. We use Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> object detection technique to extract the objects from the image. Obtaining answers from the extracted objects, we then use these answer to generate meaningful question using associated caption (see Section 4). The proposed VQAG method significantly outperforms interms of question-answer generation compare to SOTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We firmly believe that our work will boost ongoing research efforts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> in the broader area of conversational AI and scene-text understanding. Our implementation will be made publicly available on acceptance of the work.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The major contributions of this paper are three folds:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We draw the attention of the Document Analysis and Recognition community to the problem of visual question answer generation by leveraging image caption and semantically bridge the visual content with
the associated caption. We are the first to explore the VQAG problem and is an important step in the development of conversational AI and useful in augmenting the training data of image-based question answering.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose weakly supervised VQA generation method that creates nearest question using caption and visual information, that are then converted to relevant question using in-order traversal by dependency reconstruction method.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Exhaustive experimental analysis are performed on proposed VQAG method. Our model significantly outperforms the existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> interms of qualitative and quantitative results. Extensive ablation study on proposed method is investigated.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2306.06622/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">VQAG. We introduce a method of Visual Question Answer Generation. Given an image and its caption, our goal is to generate a meaningful natural language question whose answer is one of the objects from image.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recently works started studying visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and they are coming up with various ways to solve edge cases in VQA. However, there are only a few works that focus on generating question-answer pairs for given images  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Our goal is to increase the kind of questions that can be generated with correct answers provided they are relevant to the image.
We carry out experiments in generating question-answer pairs in a weakly supervised manner that in turn helps in generating large datasets for solving various VQA tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Question Generation (VQG)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">VQG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is a well-studied problem in literature and is essential for developing conversational agents and visual dialogue systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Further, ability to generate relevant and meaningful question by getting in-depth understanding of the visual content of the image is challenging. Few works in this direction are single question generator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, multiple diverse questions generator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, goal-driven question generator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and learn by context of VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the authors generate question based on category of answer. They proposed an information maximizing visual question generator that maximizes the mutual information between image, answer, and answer category during training that helps to generate questions based on the category of answer being asked without answer being given explicitly. But, some of the questions generated in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> based on category of answer does not seem relevant to few images. For example, consider the image shown in Fig <a href="#S2.F3" title="Figure 3 ‣ 2.2 Visual Question Answer Generation (VQAG) ‣ 2 Related works ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Given an image of two street signs hanging over a pole with names over it as GREENWICH and VESEY respectively at intersection of roads in front of tall buildings. We aim to automatically generate questions that are meaningful, relevant, non-generic and detailed, such as “<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Where VESEY street sign hanging on a pole?</span>” but, if given the category as color wrt same image information maximization VQG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> might have generated question related to colour of the sky, building or clouds which seems very generic though it is category-based generator. In our method, we overcome this issue by first generating answer and depending on the answer, category-based questions are generated.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, authors generate questions based on answer category but the approach taken was different where the mutual information between image, question, and answer category is maximized at latent space. The variational auto-encoder is used to reduce the level of supervision, but still they depend on manually created datasets for ground truth. In our method, we fill this gap with effective usage of visual (image) and textual (caption) meaningful information to generate question-answer pair in a weakly supervised manner.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Question Answer Generation (VQAG)</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Traditionally, VQAG methods focuses on category-based answer type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, authors generate category-wise synthetic question-answer pairs from captions using template-based learning. The categorization is based on answer that not only restricts the number of questions, but also the quality of question degrades. However, in our methods, we categorize based on the type of the question which leads to diversity of question-answer pairs that are being generated related to the input image. We consider six types of question words like “<span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">how many</span>”, “<span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">who</span>”, “<span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">what</span>”, “<span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">which</span>”, “<span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">how much</span>”, and “<span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">where</span>”. In  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, authors generate question-answer pairs from Wikipedia text and cited documents using context, question, and answer triplets. Thus, generating question-answer pairs from images is an ongoing research in AI that helps developing agents to connect easily with the scene and communicate with humans.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2306.06622/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="437" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Example question generated by our model which is meaningful, non generic, detailed and relevant to the image.
</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Given an image and associated caption(s), our aim is to generate a relevant language question such that the answer is in the list of the objects identified from the image. The proposed method should be able to recognize the objects present in the image by properly understanding the visual content, get a clear idea about the caption associated with the image and semantically bridge the textual and visual content inorder to generate meaningful questions. Initially object detection is done using FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> followed by a template based method to generate a question from caption and list of objects. After detecting objects, we extract the answer word from caption which is part of the list of detected objects. We, then proceed with question generation. The question generation is a two step process, (i) nearest question generation and (ii) relevant question generation. The overall architecture of the proposed model is illustrated in Fig <a href="#S3.F4" title="Figure 4 ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.06622/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="390" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Proposed Visual Question Answer Generator (VQAG) architecture. VQAG has four modules, namely, (i) Answer Extraction, (ii) Nearest Question Generator, (iii) Relevant Question Generator, and (iv) Fine-tuning on ViLBERT.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Answer Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">We first extract the list of objects (Os) from the given input image (I) using pre-trained FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with ResNet-101 as backbone. Since we do not want to lose the information from the image that affects the objects being detected, we set the threshold of confidence to a lower value of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="-0.2" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.1.1a" xref="S3.SS1.p1.1.m1.1.1.cmml">−</mo><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><minus id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></minus><cn type="float" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">-0.2</annotation></semantics></math>.
Once we detect objects (<math id="S3.SS1.p1.2.m2.4" class="ltx_Math" alttext="O_{1},O_{2},...,O_{n}" display="inline"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.4.3" xref="S3.SS1.p1.2.m2.4.4.4.cmml"><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">O</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.4" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">O</mi><mn id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.5" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.2.m2.4.4.3.6" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.4.4.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.p1.2.m2.4.4.3.3.2" xref="S3.SS1.p1.2.m2.4.4.3.3.2.cmml">O</mi><mi id="S3.SS1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><list id="S3.SS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4.3"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">𝑂</ci><cn type="integer" id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">𝑂</ci><cn type="integer" id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.p1.2.m2.4.4.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.2">𝑂</ci><ci id="S3.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">O_{1},O_{2},...,O_{n}</annotation></semantics></math>), next extract the related answer word from the captions (<math id="S3.SS1.p1.3.m3.4" class="ltx_Math" alttext="C_{1},C_{2},...,C_{n}" display="inline"><semantics id="S3.SS1.p1.3.m3.4a"><mrow id="S3.SS1.p1.3.m3.4.4.3" xref="S3.SS1.p1.3.m3.4.4.4.cmml"><msub id="S3.SS1.p1.3.m3.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.p1.3.m3.2.2.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.2.cmml">C</mi><mn id="S3.SS1.p1.3.m3.2.2.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.3.m3.4.4.3.4" xref="S3.SS1.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.3.m3.3.3.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS1.p1.3.m3.3.3.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.cmml">C</mi><mn id="S3.SS1.p1.3.m3.3.3.2.2.3" xref="S3.SS1.p1.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.3.m3.4.4.3.5" xref="S3.SS1.p1.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.p1.3.m3.4.4.3.6" xref="S3.SS1.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.3.m3.4.4.3.3" xref="S3.SS1.p1.3.m3.4.4.3.3.cmml"><mi id="S3.SS1.p1.3.m3.4.4.3.3.2" xref="S3.SS1.p1.3.m3.4.4.3.3.2.cmml">C</mi><mi id="S3.SS1.p1.3.m3.4.4.3.3.3" xref="S3.SS1.p1.3.m3.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.4b"><list id="S3.SS1.p1.3.m3.4.4.4.cmml" xref="S3.SS1.p1.3.m3.4.4.3"><apply id="S3.SS1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS1.p1.3.m3.2.2.1.1.3.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2">𝐶</ci><cn type="integer" id="S3.SS1.p1.3.m3.3.3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">…</ci><apply id="S3.SS1.p1.3.m3.4.4.3.3.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.4.4.3.3.1.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.4.4.3.3.2.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.2">𝐶</ci><ci id="S3.SS1.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS1.p1.3.m3.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.4c">C_{1},C_{2},...,C_{n}</annotation></semantics></math>). We then check the words from the list of objects identified in last step from the caption. If the word from the list of objects is not found in caption, we then use noun chunkers or name entity recogniser (NER) from Spacy to extract answer words (<math id="S3.SS1.p1.4.m4.4" class="ltx_Math" alttext="W_{1},W_{2},...,W_{n}" display="inline"><semantics id="S3.SS1.p1.4.m4.4a"><mrow id="S3.SS1.p1.4.m4.4.4.3" xref="S3.SS1.p1.4.m4.4.4.4.cmml"><msub id="S3.SS1.p1.4.m4.2.2.1.1" xref="S3.SS1.p1.4.m4.2.2.1.1.cmml"><mi id="S3.SS1.p1.4.m4.2.2.1.1.2" xref="S3.SS1.p1.4.m4.2.2.1.1.2.cmml">W</mi><mn id="S3.SS1.p1.4.m4.2.2.1.1.3" xref="S3.SS1.p1.4.m4.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.4.m4.4.4.3.4" xref="S3.SS1.p1.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.4.m4.3.3.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.cmml"><mi id="S3.SS1.p1.4.m4.3.3.2.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.2.cmml">W</mi><mn id="S3.SS1.p1.4.m4.3.3.2.2.3" xref="S3.SS1.p1.4.m4.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.4.m4.4.4.3.5" xref="S3.SS1.p1.4.m4.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">…</mi><mo id="S3.SS1.p1.4.m4.4.4.3.6" xref="S3.SS1.p1.4.m4.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.4.m4.4.4.3.3" xref="S3.SS1.p1.4.m4.4.4.3.3.cmml"><mi id="S3.SS1.p1.4.m4.4.4.3.3.2" xref="S3.SS1.p1.4.m4.4.4.3.3.2.cmml">W</mi><mi id="S3.SS1.p1.4.m4.4.4.3.3.3" xref="S3.SS1.p1.4.m4.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.4b"><list id="S3.SS1.p1.4.m4.4.4.4.cmml" xref="S3.SS1.p1.4.m4.4.4.3"><apply id="S3.SS1.p1.4.m4.2.2.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.2">𝑊</ci><cn type="integer" id="S3.SS1.p1.4.m4.2.2.1.1.3.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.4.m4.3.3.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.3.3.2.2.1.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2">𝑊</ci><cn type="integer" id="S3.SS1.p1.4.m4.3.3.2.2.3.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">…</ci><apply id="S3.SS1.p1.4.m4.4.4.3.3.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.4.4.3.3.1.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.4.4.3.3.2.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.2">𝑊</ci><ci id="S3.SS1.p1.4.m4.4.4.3.3.3.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.4c">W_{1},W_{2},...,W_{n}</annotation></semantics></math>) from the captions. After extracting the answer words, we mask the answer word in the caption and call it as masked caption.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<div id="S3.SS1.p2.1" class="ltx_listing ltx_listing">
<div id="algx1.l1" class="ltx_listingline">
<span id="algx1.l1.1" class="ltx_text ltx_font_bold">if</span> <math id="algx1.l1.m1.1" class="ltx_math_unparsed" alttext="O_{i}==&lt;C_{1},...,C_{n}&gt;" display="inline"><semantics id="algx1.l1.m1.1a"><mrow id="algx1.l1.m1.1b"><msub id="algx1.l1.m1.1.1"><mi id="algx1.l1.m1.1.1.2">O</mi><mi id="algx1.l1.m1.1.1.3">i</mi></msub><mo rspace="0em" id="algx1.l1.m1.1.2">=</mo><mo lspace="0em" rspace="0em" id="algx1.l1.m1.1.3">=</mo><mo lspace="0em" id="algx1.l1.m1.1.4">&lt;</mo><msub id="algx1.l1.m1.1.5"><mi id="algx1.l1.m1.1.5.2">C</mi><mn id="algx1.l1.m1.1.5.3">1</mn></msub><mo id="algx1.l1.m1.1.6">,</mo><mi mathvariant="normal" id="algx1.l1.m1.1.7">…</mi><mo id="algx1.l1.m1.1.8">,</mo><msub id="algx1.l1.m1.1.9"><mi id="algx1.l1.m1.1.9.2">C</mi><mi id="algx1.l1.m1.1.9.3">n</mi></msub><mo id="algx1.l1.m1.1.10">&gt;</mo></mrow><annotation encoding="application/x-tex" id="algx1.l1.m1.1c">O_{i}==&lt;C_{1},...,C_{n}&gt;</annotation></semantics></math> <span id="algx1.l1.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="algx1.l2" class="ltx_listingline">     <math id="algx1.l2.m1.1" class="ltx_Math" alttext="ans\leftarrow O_{i}" display="inline"><semantics id="algx1.l2.m1.1a"><mrow id="algx1.l2.m1.1.1" xref="algx1.l2.m1.1.1.cmml"><mrow id="algx1.l2.m1.1.1.2" xref="algx1.l2.m1.1.1.2.cmml"><mi id="algx1.l2.m1.1.1.2.2" xref="algx1.l2.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="algx1.l2.m1.1.1.2.1" xref="algx1.l2.m1.1.1.2.1.cmml">​</mo><mi id="algx1.l2.m1.1.1.2.3" xref="algx1.l2.m1.1.1.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="algx1.l2.m1.1.1.2.1a" xref="algx1.l2.m1.1.1.2.1.cmml">​</mo><mi id="algx1.l2.m1.1.1.2.4" xref="algx1.l2.m1.1.1.2.4.cmml">s</mi></mrow><mo stretchy="false" id="algx1.l2.m1.1.1.1" xref="algx1.l2.m1.1.1.1.cmml">←</mo><msub id="algx1.l2.m1.1.1.3" xref="algx1.l2.m1.1.1.3.cmml"><mi id="algx1.l2.m1.1.1.3.2" xref="algx1.l2.m1.1.1.3.2.cmml">O</mi><mi id="algx1.l2.m1.1.1.3.3" xref="algx1.l2.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algx1.l2.m1.1b"><apply id="algx1.l2.m1.1.1.cmml" xref="algx1.l2.m1.1.1"><ci id="algx1.l2.m1.1.1.1.cmml" xref="algx1.l2.m1.1.1.1">←</ci><apply id="algx1.l2.m1.1.1.2.cmml" xref="algx1.l2.m1.1.1.2"><times id="algx1.l2.m1.1.1.2.1.cmml" xref="algx1.l2.m1.1.1.2.1"></times><ci id="algx1.l2.m1.1.1.2.2.cmml" xref="algx1.l2.m1.1.1.2.2">𝑎</ci><ci id="algx1.l2.m1.1.1.2.3.cmml" xref="algx1.l2.m1.1.1.2.3">𝑛</ci><ci id="algx1.l2.m1.1.1.2.4.cmml" xref="algx1.l2.m1.1.1.2.4">𝑠</ci></apply><apply id="algx1.l2.m1.1.1.3.cmml" xref="algx1.l2.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.3.1.cmml" xref="algx1.l2.m1.1.1.3">subscript</csymbol><ci id="algx1.l2.m1.1.1.3.2.cmml" xref="algx1.l2.m1.1.1.3.2">𝑂</ci><ci id="algx1.l2.m1.1.1.3.3.cmml" xref="algx1.l2.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m1.1c">ans\leftarrow O_{i}</annotation></semantics></math>

</div>
<div id="algx1.l3" class="ltx_listingline">
<span id="algx1.l3.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="algx1.l4" class="ltx_listingline">     <span id="algx1.l4.1" class="ltx_text ltx_font_bold">if</span> <math id="algx1.l4.m1.1" class="ltx_math_unparsed" alttext="O_{i}\neq&lt;C_{1},...,C_{n}&gt;" display="inline"><semantics id="algx1.l4.m1.1a"><mrow id="algx1.l4.m1.1b"><msub id="algx1.l4.m1.1.2"><mi id="algx1.l4.m1.1.2.2">O</mi><mi id="algx1.l4.m1.1.2.3">i</mi></msub><mo rspace="0em" id="algx1.l4.m1.1.3">≠</mo><mo lspace="0em" id="algx1.l4.m1.1.4">&lt;</mo><msub id="algx1.l4.m1.1.5"><mi id="algx1.l4.m1.1.5.2">C</mi><mn id="algx1.l4.m1.1.5.3">1</mn></msub><mo id="algx1.l4.m1.1.6">,</mo><mi mathvariant="normal" id="algx1.l4.m1.1.1">…</mi><mo id="algx1.l4.m1.1.7">,</mo><msub id="algx1.l4.m1.1.8"><mi id="algx1.l4.m1.1.8.2">C</mi><mi id="algx1.l4.m1.1.8.3">n</mi></msub><mo id="algx1.l4.m1.1.9">&gt;</mo></mrow><annotation encoding="application/x-tex" id="algx1.l4.m1.1c">O_{i}\neq&lt;C_{1},...,C_{n}&gt;</annotation></semantics></math> <span id="algx1.l4.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="algx1.l5" class="ltx_listingline">         <math id="algx1.l5.m1.1" class="ltx_Math" alttext="ans\leftarrow W_{i}" display="inline"><semantics id="algx1.l5.m1.1a"><mrow id="algx1.l5.m1.1.1" xref="algx1.l5.m1.1.1.cmml"><mrow id="algx1.l5.m1.1.1.2" xref="algx1.l5.m1.1.1.2.cmml"><mi id="algx1.l5.m1.1.1.2.2" xref="algx1.l5.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="algx1.l5.m1.1.1.2.1" xref="algx1.l5.m1.1.1.2.1.cmml">​</mo><mi id="algx1.l5.m1.1.1.2.3" xref="algx1.l5.m1.1.1.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="algx1.l5.m1.1.1.2.1a" xref="algx1.l5.m1.1.1.2.1.cmml">​</mo><mi id="algx1.l5.m1.1.1.2.4" xref="algx1.l5.m1.1.1.2.4.cmml">s</mi></mrow><mo stretchy="false" id="algx1.l5.m1.1.1.1" xref="algx1.l5.m1.1.1.1.cmml">←</mo><msub id="algx1.l5.m1.1.1.3" xref="algx1.l5.m1.1.1.3.cmml"><mi id="algx1.l5.m1.1.1.3.2" xref="algx1.l5.m1.1.1.3.2.cmml">W</mi><mi id="algx1.l5.m1.1.1.3.3" xref="algx1.l5.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algx1.l5.m1.1b"><apply id="algx1.l5.m1.1.1.cmml" xref="algx1.l5.m1.1.1"><ci id="algx1.l5.m1.1.1.1.cmml" xref="algx1.l5.m1.1.1.1">←</ci><apply id="algx1.l5.m1.1.1.2.cmml" xref="algx1.l5.m1.1.1.2"><times id="algx1.l5.m1.1.1.2.1.cmml" xref="algx1.l5.m1.1.1.2.1"></times><ci id="algx1.l5.m1.1.1.2.2.cmml" xref="algx1.l5.m1.1.1.2.2">𝑎</ci><ci id="algx1.l5.m1.1.1.2.3.cmml" xref="algx1.l5.m1.1.1.2.3">𝑛</ci><ci id="algx1.l5.m1.1.1.2.4.cmml" xref="algx1.l5.m1.1.1.2.4">𝑠</ci></apply><apply id="algx1.l5.m1.1.1.3.cmml" xref="algx1.l5.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l5.m1.1.1.3.1.cmml" xref="algx1.l5.m1.1.1.3">subscript</csymbol><ci id="algx1.l5.m1.1.1.3.2.cmml" xref="algx1.l5.m1.1.1.3.2">𝑊</ci><ci id="algx1.l5.m1.1.1.3.3.cmml" xref="algx1.l5.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l5.m1.1c">ans\leftarrow W_{i}</annotation></semantics></math>

</div>
<div id="algx1.l6" class="ltx_listingline">     <span id="algx1.l6.1" class="ltx_text ltx_font_bold">end</span> <span id="algx1.l6.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="algx1.l7" class="ltx_listingline">
<span id="algx1.l7.1" class="ltx_text ltx_font_bold">end</span> <span id="algx1.l7.2" class="ltx_text ltx_font_bold">if</span>
</div>
</div>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">where <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑊</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">W_{i}</annotation></semantics></math> is answer words from <math id="S3.SS1.p3.2.m2.1" class="ltx_math_unparsed" alttext="&lt;W_{1},W_{2},...,W_{n}&gt;" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1b"><mo id="S3.SS1.p3.2.m2.1.2">&lt;</mo><msub id="S3.SS1.p3.2.m2.1.3"><mi id="S3.SS1.p3.2.m2.1.3.2">W</mi><mn id="S3.SS1.p3.2.m2.1.3.3">1</mn></msub><mo id="S3.SS1.p3.2.m2.1.4">,</mo><msub id="S3.SS1.p3.2.m2.1.5"><mi id="S3.SS1.p3.2.m2.1.5.2">W</mi><mn id="S3.SS1.p3.2.m2.1.5.3">2</mn></msub><mo id="S3.SS1.p3.2.m2.1.6">,</mo><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1">…</mi><mo id="S3.SS1.p3.2.m2.1.7">,</mo><msub id="S3.SS1.p3.2.m2.1.8"><mi id="S3.SS1.p3.2.m2.1.8.2">W</mi><mi id="S3.SS1.p3.2.m2.1.8.3">n</mi></msub><mo id="S3.SS1.p3.2.m2.1.9">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">&lt;W_{1},W_{2},...,W_{n}&gt;</annotation></semantics></math></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Question Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We generate nearest questions from the masked caption. We then introduce a rule-based method to rewrite the nearest questions to meaningful relevant questions, which utilizes the dependency structures. Depending on the answer word, Our model generates six types of questions like “<span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">how many</span>”, “<span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">who</span>”, “<span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">what</span>”, “<span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">which</span>”, “<span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">how much</span>”, and “<span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_italic">where</span>”. The frequency of occurrence wrt each type of question word is shown in Fig <a href="#S3.F5" title="Figure 5 ‣ 3.2 Question Generation ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. As question generation is governed by the caption and the detected objects, the distribution of questions are biased towards the count based category type that is “<span id="S3.SS2.p1.1.7" class="ltx_text ltx_font_italic">how many</span>” and “<span id="S3.SS2.p1.1.8" class="ltx_text ltx_font_italic">how much</span>” in which there is a scope for improvement.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2306.06622/assets/images/graph_image_1.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Distribution of different types of question categories generated by our VQAG model.</span></figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Nearest Question Generation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">This is the first step in generating questions from the masked caption.
Here, we replace the answer words in the statements with a special mask token, depending on its answer category. Using the masked caption and the answer (with a type label ANIMAL in Fig <a href="#S3.F6" title="Figure 6 ‣ 3.3 Fine-tuning on ViLBERT ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> as masked word is animal in this case), we replace the masked word with special category word which gives us with the Nearest question.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Translate Nearest to Relevant Questions</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">To generate the relevant questions from nearest questions, we perform a dependency reconstruction. First, we create the tree from the words taken from nearest question. Then, we move answer-related words in the dependency tree to the front of the question, as answer-related words are crucial in framing the question. We do this procedure of moving answer related words to the front of the question keeping the intuition that relevant questions usually start with question words.
Dependency parsing is applied to the nearest questions, we follow three steps to translate them to relevant questions, (i) keep the right child nodes of the answer-related word and prune its lefts, (ii) for each node in the parsing tree, if the sub tree of its child node contains the answer node, we move the child node to the first child node, (iii) finally, do in-order traversal on the reconstructed tree to obtain the relevant question. Using rule-based method mapping, which replaces each answer category with the most appropriate <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">wh*</span> word. For example, the LOCATION category is mapped to “<span id="S3.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">WHERE</span>” and the COUNT category is replaced by “<span id="S3.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_italic">HOW MANY</span>”.
Fig <a href="#S3.F6" title="Figure 6 ‣ 3.3 Fine-tuning on ViLBERT ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the detailed explanation of Visual QA generation following the above mentioned steps.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fine-tuning on ViLBERT</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Generated questions along with the object words that are identified as most appropriate answers are taken as question-answer pairs with their corresponding images and are fine-tuned on popular and state-of-art work in visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Based on the question-answer pairs generated, we create a new vocabulary and then fine-tune on the new vocabulary being appended to the VQA’s vocabulary. After fine-tuning with our questions and answers, we test it on VQA test dataset and obtain a score of <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="49.2" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">49.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><cn type="float" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">49.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">49.2</annotation></semantics></math>.The reason for less score is, during the testing time, some of the answer words might be missing that are part of new vocabulary which are not found in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> vocabulary as it is up-streaming task.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2306.06622/assets/x5.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="391" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Illustration of the proposed VQAG with an example.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we experimentally validate our proposed method for question-answer pair generation.
The quantitative and qualitative experimental results of the proposed method and the comparative analysis with SOTA is provided.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Quantitative evaluation of our method against other
weakly supervised SOTA model using standard metrics.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<th id="S4.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">BLEU</span></th>
<th id="S4.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">METEOR</span></th>
<th id="S4.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.4.1.1.4.1" class="ltx_text ltx_font_bold">ROUGE-L</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.2.1" class="ltx_tr">
<th id="S4.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">IA2Q <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">30.42</td>
<td id="S4.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">9.42</td>
<td id="S4.T1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T1.4.3.2" class="ltx_tr">
<th id="S4.T1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">V-IA2Q <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T1.4.3.2.2" class="ltx_td ltx_align_center">35.40</td>
<td id="S4.T1.4.3.2.3" class="ltx_td ltx_align_center">13.35</td>
<td id="S4.T1.4.3.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.4.4.3" class="ltx_tr">
<th id="S4.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">IMVQG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<td id="S4.T1.4.4.3.2" class="ltx_td ltx_align_center">31.2</td>
<td id="S4.T1.4.4.3.3" class="ltx_td ltx_align_center">12.11</td>
<td id="S4.T1.4.4.3.4" class="ltx_td ltx_align_center">40.27</td>
</tr>
<tr id="S4.T1.4.5.4" class="ltx_tr">
<th id="S4.T1.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">C3VQG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</th>
<td id="S4.T1.4.5.4.2" class="ltx_td ltx_align_center">41.87</td>
<td id="S4.T1.4.5.4.3" class="ltx_td ltx_align_center">13.60</td>
<td id="S4.T1.4.5.4.4" class="ltx_td ltx_align_center">42.34</td>
</tr>
<tr id="S4.T1.4.6.5" class="ltx_tr">
<th id="S4.T1.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Ours</th>
<td id="S4.T1.4.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.4.6.5.2.1" class="ltx_text ltx_font_bold">47.78</span></td>
<td id="S4.T1.4.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.4.6.5.3.1" class="ltx_text ltx_font_bold">27.61</span></td>
<td id="S4.T1.4.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">18.89</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Comparison of our method with various baselines techniques based on context and filtering block.</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.4.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.4.1.1.2.1" class="ltx_text ltx_font_bold">BLEU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.2.1" class="ltx_tr">
<td id="S4.T2.4.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">w/o list of objects as context, w/o filtering block</td>
<td id="S4.T2.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">45.63</td>
</tr>
<tr id="S4.T2.4.3.2" class="ltx_tr">
<td id="S4.T2.4.3.2.1" class="ltx_td ltx_align_left ltx_border_r">w/ list of objects as context, w/ filtering block</td>
<td id="S4.T2.4.3.2.2" class="ltx_td ltx_align_center">46.29</td>
</tr>
<tr id="S4.T2.4.4.3" class="ltx_tr">
<td id="S4.T2.4.4.3.1" class="ltx_td ltx_align_left ltx_border_r">w/o list of objects as context, w/ filtering block</td>
<td id="S4.T2.4.4.3.2" class="ltx_td ltx_align_center">45.13</td>
</tr>
<tr id="S4.T2.4.5.4" class="ltx_tr">
<td id="S4.T2.4.5.4.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">w/ list of objects as context, w/o filtering block (Ours)</td>
<td id="S4.T2.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.4.5.4.2.1" class="ltx_text ltx_font_bold">47.78</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Since we consider captions along with images to generate question-answer pair, there is no dedicated dataset available for this task containing question-answer pair with captions for the images. We, therefore, make use of the two popular datasets, namely, MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Note that unlike these datasets where originally the task is to generate caption for the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>and answer a question about the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> respectively, our aim is to generate question-answer pairs using captions for the images. In other words, given an image and associated captions (taken from  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>), our proposed method learns to automatically generate question-answer pairs similar to the manually curated VQA dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance metrics </h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We use popular evaluation measures such as bilingual evaluation understudy (BLEU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, recall-oriented understudy for listing evaluation-longest common sub-sequence (ROUGE-L), and metric for evaluation of translation with explicit ordering (METEOR). The BLEU score compares n-grams of the generated question with the n-grams of the reference question and counts the number of matches. The ROUGE-L metric indicates similarity between two sequences based on the length of the longest common sub-sequence even though the sequences are not contiguous. The METEOR is based on the harmonic mean of unigram precision and recall and is considered a better performance measure in the text generation. Higher values of all these performance measures imply better matching of generated questions with the reference questions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We fine-tune our generated QA pair(s) on  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with 4 layer MLP using the Adam optimizer with initial learning rate of 1e-4, batch size of 64. The maximum length of the generated questions is set to 24. We fine-tune the model for 15 epochs. The model is fine-tuned on a single NVIDIA Quadro P5000.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results and discussions </h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Quantitative Analysis</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">We evaluate the performance of our proposed method and compare it against three baseline approaches, ”without list of objects as context - without filtering block, ”without list of objects as context - with filtering block”, ”with list of objects as context - with filtering block”.Explanation for these baselines is provided in section 4.4.2. This comparative result is shown in Table 2 using performance measures discussed in Section 4.2. Note that higher values for all these popularly used performance measure is considered superior. Among the three baseline approaches, ”without list of objects as context - with filtering block” generates comparatively better questions. Our proposed final method significantly outperforms all the baseline models. For example our proposed method improves BLEU-score by 1.14 as compared to the most competitive baseline i.e., ”without list of objects as context - with filtering block”. It should be noted that under these performance measures these gains are considered significant. Further, by design, our proposed method tries to ensure that the answer being object detected in most of the cases and generated question is relevant and meaningful to answer and image. We also demonstrate our results with and without fine-tuning on  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. It is also seen that fine-tuning on ViLBERT plays a major role in achieving better results which in turn prove that our generated QA pair are meaningful. 
<br class="ltx_break"></p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">After fine-tuning on  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we evaluate our generated questions on VQA questions and our scores are compared with  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Our model outperforms all the models as shown in Table 1. The ROUGE-L scores are low compared to the other works as the process of generating questions varies from their approach to our approach. Question generation in their approach follows category based on answers or directly via image features which generates questions similar to that of VQA. Unlike those works we generate questions via captions and detected objects which generates questions way different from VQA.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Experiments</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We experimented with and without using filtering in our proposed architecture (see Fig <a href="#S3.F4" title="Figure 4 ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In case of “with filtering block” we filter the captions mean we consider the captions only if the particular caption has one of the word from list of detected objects for that image. In this, we filter out certain captions and filtered captions are only considered for further question generation.
But this approach removed nearly <math id="S4.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><mn id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><cn type="integer" id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">23</annotation></semantics></math> percent of the captions which in turn lead to lose of good quality questions.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2306.06622/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="565" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Generated question-answer pairs on VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Results showing from our method and ground truth from VQA dataset respectively. [<span id="S4.F7.4.2.1" class="ltx_text ltx_font_bold">Best viewed in color]</span></span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Context as secondary input</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">We used context as secondary input to the nearest question generation.
We experimented using “list of objects and caption as context” and only caption as context. Using of context with “list of objects and caption” helped our model to get better questions which are more relevant to the image as it uses list of objects too along with caption.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Problem with words that are identified as objects</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">In one of the baseline models (w/ list of objects as context, w/ filtering block) we had a problem with the certain object words which are identified by object detector (FasterRCNN) that are not found in captions and hence we may lose few good questions there.
We tackled this problem by appending the word identified to the similar words having the same meaning for the detected objects.
For example, we replaced person by adult, man, woman, boy, girl.
However we removed the filtering block as the scores are low compared to the model without filtering block as shown in Table 2. We removed the filtering block from the methodology diagram Fig <a href="#S3.F4" title="Figure 4 ‣ 3 Proposed approach ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> as it is not considered in the finalized model.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2306.06622/assets/x7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="383" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Failure case examples of visual question generation by our model on images from VQA dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>Qualitative Analysis</h4>

<div id="S4.SS4.SSS5.p1" class="ltx_para">
<p id="S4.SS4.SSS5.p1.1" class="ltx_p">We perform a detailed qualitative analysis of the baselines as well as our proposed method. We first show a comparison of generated QA pair using all the three baselines versus proposed method in Fig <a href="#S4.F9" title="Figure 9 ‣ 4.4.5 Qualitative Analysis ‣ 4.4 Results and discussions ‣ 4 Experiments and Results ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> which is differentiated by color. We observe that the baselines are capable of generating almost linguistically meaningful QA pair with minute chances in sentence formation but not as meaningful as our proposed method. For the given image in Fig <a href="#S4.F9" title="Figure 9 ‣ 4.4.5 Qualitative Analysis ‣ 4.4 Results and discussions ‣ 4 Experiments and Results ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the expected question is “ What has a yellow cartoon dog on it?” which is generated by our proposed method. The baseline approaches have generated questions which are appropriate to their corresponding answers but they are not as meaningful as our proposed method. Though the QA pairs generated by baselines are relevant to the input image, as the complexity of the image and the caption increases, the baseline models fail to generate more appropriate and meaningful questions than the proposed model. The proper utilization of visual and textual information generated better questions in our proposed method.</p>
</div>
<div id="S4.SS4.SSS5.p2" class="ltx_para">
<p id="S4.SS4.SSS5.p2.1" class="ltx_p">The best baseline model “without list of objects as context - with filtering block” generates question “What cartoon dog?” which is more meaningful than rest baseline approaches but it fails to specify the important information in the image as it is not using list of objects as context and not using filtering block that has removed the caption which dont have one of the words from list of objects, in this case the important information can be Frisbee which is missing. The answer here ( “Yellow” ) is generated by noun chunkers and NER toolkit. Whereas the proposed model generates “What has a yellow cartoon dog on it?” which is more relevant to the image and meaningful than baseline approaches.</p>
</div>
<div id="S4.SS4.SSS5.p3" class="ltx_para">
<p id="S4.SS4.SSS5.p3.1" class="ltx_p">Further, more results of our model are shown in Fig <a href="#S4.F7" title="Figure 7 ‣ 4.4.2 Experiments ‣ 4.4 Results and discussions ‣ 4 Experiments and Results ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> questions and answers generated by our model are in blue, and corresponding  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> questions and answers are in green. Here our model successfully generates meaningful and relevant questions to the image.</p>
</div>
<div id="S4.SS4.SSS5.p4" class="ltx_para">
<p id="S4.SS4.SSS5.p4.1" class="ltx_p">The failure of our model pronounced when VQAG misunderstood the scene and output incorrect objects during answer extraction or there is need of generating questions in which objects from image may not act as answer or fails to build the semantic relation between caption and the answer.
Fig <a href="#S4.F9" title="Figure 9 ‣ 4.4.5 Qualitative Analysis ‣ 4.4 Results and discussions ‣ 4 Experiments and Results ‣ Weakly Supervised Visual Question Answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows failure case examples generated by our model. Though the failure case examples does not sense meaningful yet they are relevant to the image.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2306.06622/assets/x8.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Baseline models and our model qualitative comparison on image from VQA dataset.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed a visual question answer generation method in weakly supervised manner for a given image and associated caption. Our proposed method properly utilizes the visual properties and generates the question-answer pairs that are meaningful and relevant to the image. Our method has outperformed on SOTA question generating models with a BLEU score value increased by <span id="S5.p1.1.1" class="ltx_text ltx_font_bold">6%</span>. Ours is the first work towards developing a visual question-answer pair generation model which considers answer as the one of the object from the image, we restrict our scope to generating questions whose answer is the object from image. Our question-answer pair generator can be used in generating large datasets with no human effort and can also be used in task related to meta-learning and self-supervised learning. Future directions include complex, specific and realistic question-answer pair generation that require deeper semantic reasoning using transformers in understanding image and text together.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">VQA: visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433. IEEE Computer Society, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, and Chitta Baral.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Weaqa: Weak supervision via captions for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL/IJCNLP (Findings)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, volume ACL/IJCNLP 2021 of </span><span id="bib.bib2.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Findings of ACL</span><span id="bib.bib2.7.5" class="ltx_text" style="font-size:90%;">, pages 3420–3435. Association for Computational
Linguistics, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Ali Furkan Biten, Rubèn Tito, Andrés Mafla,
Lluís Gómez i Bigorda, Marçal Rusiñol, C. V.
Jawahar, Ernest Valveny, and Dimosthenis Karatzas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Scene text visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 4290–4300. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta,
Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, abs/1504.00325, 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Zhihao Fan, Zhongyu Wei, Piji Li, Yanyan Lan, and Xuanjing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">A question type driven framework to diversify visual question
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 4048–4054. ijcai.org, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas Summers-Stay, Dhruv Batra,
and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vis.</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 127(4):398–414, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Soumya Jahagirdar, Shankar Gangisetty, and Anand Mishra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Look, read and ask: Learning to ask questions by reading text in
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICDAR (1)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, volume 12821 of </span><span id="bib.bib7.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</span><span id="bib.bib7.7.5" class="ltx_text" style="font-size:90%;">, pages 335–349. Springer, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Watching the news: Towards videoqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Winter Conference on Applications of Computer
Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 4430–4439.
IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Mahmoud Khademi and Oliver Schulte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Image caption generation with hierarchical contextual visual spatial
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR Workshops</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1943–1951. Computer Vision
Foundation / IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Alexey K. Kovalev, Makhmud Shaban, Evgeny Osipov, and Aleksandr I. Panov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Vector semiotic model for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Cogn. Syst. Res.</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 71:52–63, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Michael S. Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Information maximizing visual question generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2008–2018. Computer Vision Foundation /
IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and
Ming Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Visual question generation as dual task of visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 6116–6124. Computer Vision Foundation /
IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and
Ming Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Visual question generation as dual task of visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 6116–6124. Computer Vision Foundation /
IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, and Ke Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Harvesting and refining question-answer pairs for unsupervised QA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 6719–6728. Association for Computational
Linguistics, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 13–23, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Ishan Misra, Ross B. Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and
Laurens van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Learning by asking questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 11–20. Computer Vision Foundation / IEEE
Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He,
and Lucy Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Generating natural questions about an image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL (1)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">. The Association for Computer Linguistics, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 311–318. ACL, 2002.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Mengye Ren, Ryan Kiros, and Richard S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Exploring models and data for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 2953–2961, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 39(6):1137–1149,
2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Mourad Sarrouti, Asma Ben Abacha, and Dina Demner-Fushman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Goal-driven visual question generation from radiology images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Inf.</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 12(8):334, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Towards VQA models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 8317–8326. Computer Vision Foundation /
IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Towards VQA models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 8317–8326. Computer Vision Foundation /
IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Shagun Uppal, Anish Madan, Sarthak Bhagat, Yi Yu, and Rajiv Ratn Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">C3VQG: category consistent cyclic visual question generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">MMAsia</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 49:1–49:7. ACM, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Shaohua Wan, Chen Chen, and Alexandros Iosifidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Editorial to special issue on cross-media learning for visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Image Vis. Comput.</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 118:104355, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Xing Xu, Tan Wang, Yang Yang, Alan Hanjalic, and Heng Tao Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Radial graph convolutional network for visual question generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Neural Networks Learn. Syst.</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 32(4):1654–1667,
2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Huayi Zhan, Peixi Xiong, Xin Wang, Xin Wang, and Lan Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Visual question answering by pattern matching and reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neurocomputing</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 467:323–336, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, and Jiawan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Automatic generation of grounded visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 4235–4243. ijcai.org, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.06621" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.06622" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.06622">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.06622" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.06623" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 01:12:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
