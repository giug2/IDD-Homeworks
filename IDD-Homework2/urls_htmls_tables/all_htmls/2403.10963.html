<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!</title>
<!--Generated on Tue Jun 18 02:56:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.10963v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S1" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S2" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S3" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.SS0.SSS0.Px1" title="In 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Datasets and languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.SS0.SSS0.Px2" title="In 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Experimental Settings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.SS0.SSS0.Px1" title="In 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Improvement patterns</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.SS0.SSS0.Px2" title="In 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Controlled test sets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.SS0.SSS0.Px3" title="In 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Usage of the copy mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.SS0.SSS0.Px4" title="In 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.SS0.SSS0.Px5" title="In 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Linguistic complexities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S6" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Challenges for LR NMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S7" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S8" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A1" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Notes on Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Minor Variations</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2.SS0.SSS0.Px1" title="In Appendix B Minor Variations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Pretrained encoder and decoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2.SS0.SSS0.Px2" title="In Appendix B Minor Variations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Single attention head</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2.SS0.SSS0.Px3" title="In Appendix B Minor Variations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Smaller tokenizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2.SS0.SSS0.Px4" title="In Appendix B Minor Variations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title">Identical source and target</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3" title="In Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Visualizations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Niyati Bafna, Philipp Koehn, and David Yarowsky
<br class="ltx_break"/>Johns Hopkins University, Center for Language and Speech Processing 
<br class="ltx_break"/>{nbafna1,phi,yarowsky}@jhu.edu
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural “shortcuts”, such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings (<math alttext="&lt;1" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">&lt;</mo><mn id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><lt id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">absent</csymbol><cn id="id1.1.m1.1.1.3.cmml" type="integer" xref="id1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">&lt;1</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">&lt; 1</annotation></semantics></math> BLEU). However, analysis shows that PGNs do not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour highlights several general challenges for LR NMT, such as modern tokenization strategies, noisy real-world conditions, and linguistic complexities. We call for better scrutiny of linguistically motivated improvements to NMT given the blackbox nature of Transformer models, as well as for a focus on the above problems in the field.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction and Motivation</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">While state-of-the-art (SOTA) Transformer models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib36" title="">2017</a>)</cite> for NMT work well for high-resource language pairs, their performance degrades in low-resource situations <cite class="ltx_cite ltx_citemacro_citep">(Koehn and Knowles, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib20" title="">2017</a>; Sennrich and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib33" title="">2019</a>; Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib18" title="">2020</a>; Haddow et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib13" title="">2022</a>)</cite>; this means that most languages in the world cannot benefit from mainstream advances and models <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib15" title="">2020</a>)</cite>. There is therefore a clear appeal to developing simple architectural mechanisms for these models that are targeted at yielding improvements in data-scarce scenarios, while interfering minimally with mainstream preprocessing, tokenization, and training pipelines.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the context of a low-resource language (LRL), we are often interested in translation to and from a closely related HRL, which possibly has linguistic genealogical, regional, and cultural ties with the LRL,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is the case, for example, for several languages of the Arabic continuum, all closely related to relatively high-resource Modern Standard Arabic, and languages of the Turkic and Indic language continua.</span></span></span> in order to make the abundant content in HRLs available in related LRLs.
We expect that closely related languages share considerable overlap at the subword level from cognates, borrowings and shared vocabulary (see examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S1.F1" title="Figure 1 ‣ 1 Introduction and Motivation ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">1</span></a>). Given the absence of large parallel corpora for our language pair, we aim to leverage this shared knowledge across source and target, intuitively, to provide “easier” routes for our MT model from source to target sentence.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="77" id="S1.F1.g1" src="extracted/5674423/images/hi-bh.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Translation equivalents for Bhojpuri (top) and Hindi (bottom), demonstrating subword overlap.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Pointer Generator Networks (PGNs; <cite class="ltx_cite ltx_citemacro_citet">See et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib31" title="">2017</a>)</cite>) are a mechanism which allow the model, for every output token produced, to either copy some token from the input (“point”) or “generate” a token as per usual from the vocabulary. PGNs have been used for a variety of problems, described in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S2" title="2 Related Work ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a>, often targeted at repeated spans of text in the input and output; however, as we far as we know, this is the first work to study its applicability to LR NMT. In this case, we hypothesise that the pointing mechanism will show advantages for rare shared subwords, for which the best strategy may be to copy them to the output.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We introduce a PGN mechanism into a Transformer-based NMT architecture, and test its performance for 6 language pairs over 4 low-resource training ranges. We work with Hindi-Bhojpuri (hi-bh), Spanish-Catalan (es-ca), and French-Occitan (fr-oc), representing closely-related pairs, Hindi-Marathi (hi-mr), a relatively more distant pair,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Hindi, Bhojpuri, and Marathi belong to the Indic branch of the Indo-European family. Hindi and Bhojpuri further belong to the Shaurasenic sub-branch and are closer lexically and grammatically to each other and other languages on or close to the Hindi Belt such as Punjabi, Rajasthani, Haryanvi, and Maithili, than Hindi is to Marathic languages and dialects; this is supported by lexical and other studies of cross-lingual similarity <cite class="ltx_cite ltx_citemacro_citep">(Sengupta and Saha, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib32" title="">2015</a>; Mundotiya et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib24" title="">2021</a>; Bafna et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib2" title="">2022</a>)</cite>. See Glottolog (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://glottolog.org/resource/languoid/id/cont1248" title="">https://glottolog.org/resource/languoid/id/cont1248</a>) for the phylogenetic tree.</span></span></span> and Spanish-English (es-en) and French-German (fr-de), representing further distant pairs. We expect that <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">PGN</span> will help most for (1) lower-resource scenarios (2) more closely-related language pairs (3) sentence pairs with higher subword overlap. While <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.2">PGN</span> shows improvements in certain settings, our comparative analysis of the benefits of PGNs across the three dimensions above shows clear lack of evidence for the these hypotheses. Further, our visualizations of the PGN mechanism also indicate that observed benefits do not come from intended sources. We discuss various factors that contribute to this failure, highlighting fundamental challenges for LR NMT, such as noisy datasets, mainstream tokenization practices best suited for high-resource scenarios, as well as linguistic and orthographic complexities that may obfuscate underlying source-target similarities.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/niyatibafna/pgns-for-lrmt" title="">https://github.com/niyatibafna/pgns-for-lrmt</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Pointer Networks were introduced to solve problems that involved permuting the input, such as the Traveling Salesman Problem and the complex hull problem <cite class="ltx_cite ltx_citemacro_citep">(Vinyals et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib37" title="">2015</a>)</cite>. Their use in NLP has been largely been for monolingual summarization, where the target may naturally contain identical spans from the source. <cite class="ltx_cite ltx_citemacro_citet">Cheng and Lapata (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib5" title="">2016</a>)</cite> present a complex hierarchical LSTM-based model for summarization, which directly extracts sentences from the text and words from sentences. <cite class="ltx_cite ltx_citemacro_citet">Gulcehre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib12" title="">2016</a>)</cite> use pointer networks in RNN-based sequence-to-sequence models for summarization and machine translation, training their model explicitly to use the pointing mechanism for uncommon words. <cite class="ltx_cite ltx_citemacro_citet">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib11" title="">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">See et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib31" title="">2017</a>)</cite> also incorporate variants of pointer-generator networks into RNN-based sequence-to-sequence learning for summarization.
<cite class="ltx_cite ltx_citemacro_citet">Prabhu and Kann (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib28" title="">2020</a>)</cite> applied PGNs to the task of grapheme-to-phoneme conversion via an explicit source-target mapping. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib38" title="">2021</a>)</cite> proposed a pointer-disambiguator-copier (PDC) system for dictionary-enhanced high-resource NMT, using source word translations as potential candidates for the copying mechanism, with a disambiguator component to select appropriate senses.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Our work is the first to examine the applicability of PGNs as facilitators in the low-resource MT scenario, looking to exploit linguistic relationships between the source and target in the absence of external resources. We work with Transformer-based NMT, and make no changes to standard BPE tokenization schemes or training objectives (unlike <cite class="ltx_cite ltx_citemacro_citet">Gulcehre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib12" title="">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib38" title="">2021</a>)</cite>). This is so that our findings are most relevant in today’s paradigm of generalized strategies for end-to-end multilingual MT; our mechanism can be easily plugged into and trained with any modern (multilingual) MT pipeline.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.3">The <span class="ltx_text ltx_font_smallcaps" id="S3.p1.3.1">PGN</span> model provides two routes to the model for predicting any target token: copying from the source or generating from the vocabulary <cite class="ltx_cite ltx_citemacro_citep">(Prabhu and Kann, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib28" title="">2020</a>)</cite>. Copy and generate distributions at step <math alttext="t" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_t</annotation></semantics></math> are mixed using a learned parameter <math alttext="p_{copy}^{t}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msubsup id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2.2" xref="S3.p1.2.m2.1.1.2.2.cmml">p</mi><mrow id="S3.p1.2.m2.1.1.2.3" xref="S3.p1.2.m2.1.1.2.3.cmml"><mi id="S3.p1.2.m2.1.1.2.3.2" xref="S3.p1.2.m2.1.1.2.3.2.cmml">c</mi><mo id="S3.p1.2.m2.1.1.2.3.1" xref="S3.p1.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.p1.2.m2.1.1.2.3.3" xref="S3.p1.2.m2.1.1.2.3.3.cmml">o</mi><mo id="S3.p1.2.m2.1.1.2.3.1a" xref="S3.p1.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.p1.2.m2.1.1.2.3.4" xref="S3.p1.2.m2.1.1.2.3.4.cmml">p</mi><mo id="S3.p1.2.m2.1.1.2.3.1b" xref="S3.p1.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.p1.2.m2.1.1.2.3.5" xref="S3.p1.2.m2.1.1.2.3.5.cmml">y</mi></mrow><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">superscript</csymbol><apply id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.2.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.2.cmml" xref="S3.p1.2.m2.1.1.2.2">𝑝</ci><apply id="S3.p1.2.m2.1.1.2.3.cmml" xref="S3.p1.2.m2.1.1.2.3"><times id="S3.p1.2.m2.1.1.2.3.1.cmml" xref="S3.p1.2.m2.1.1.2.3.1"></times><ci id="S3.p1.2.m2.1.1.2.3.2.cmml" xref="S3.p1.2.m2.1.1.2.3.2">𝑐</ci><ci id="S3.p1.2.m2.1.1.2.3.3.cmml" xref="S3.p1.2.m2.1.1.2.3.3">𝑜</ci><ci id="S3.p1.2.m2.1.1.2.3.4.cmml" xref="S3.p1.2.m2.1.1.2.3.4">𝑝</ci><ci id="S3.p1.2.m2.1.1.2.3.5.cmml" xref="S3.p1.2.m2.1.1.2.3.5">𝑦</ci></apply></apply><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">p_{copy}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, to obtain the final probability distribution <math alttext="P^{t}" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><msup id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">P</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑃</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">P^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_P start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> for the target token.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A3.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle p_{copy}^{t}=\sigma(\textbf{W}^{T}(\textbf{c}^{t}\oplus\textbf{d%
}^{t}\oplus\textbf{s}^{t})+\textbf{B})" class="ltx_Math" display="inline" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><msubsup id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2.2" xref="S3.Ex1.m1.1.1.3.2.2.cmml">p</mi><mrow id="S3.Ex1.m1.1.1.3.2.3" xref="S3.Ex1.m1.1.1.3.2.3.cmml"><mi id="S3.Ex1.m1.1.1.3.2.3.2" xref="S3.Ex1.m1.1.1.3.2.3.2.cmml">c</mi><mo id="S3.Ex1.m1.1.1.3.2.3.1" xref="S3.Ex1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.3.3" xref="S3.Ex1.m1.1.1.3.2.3.3.cmml">o</mi><mo id="S3.Ex1.m1.1.1.3.2.3.1a" xref="S3.Ex1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.3.4" xref="S3.Ex1.m1.1.1.3.2.3.4.cmml">p</mi><mo id="S3.Ex1.m1.1.1.3.2.3.1b" xref="S3.Ex1.m1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex1.m1.1.1.3.2.3.5" xref="S3.Ex1.m1.1.1.3.2.3.5.cmml">y</mi></mrow><mi id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3.cmml">t</mi></msubsup><mo id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.3.cmml">σ</mi><mo id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml"><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.2a.cmml">W</mtext><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.3.cmml">T</mi></msup><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2a.cmml">c</mtext><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msup><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">⊕</mo><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml">d</mtext><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msup><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1a" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">⊕</mo><msup id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2a.cmml">s</mtext><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.3.cmml">t</mi></msup></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">+</mo><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3a.cmml">B</mtext></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"></eq><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3">superscript</csymbol><apply id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.2.1.cmml" xref="S3.Ex1.m1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.2.cmml" xref="S3.Ex1.m1.1.1.3.2.2">𝑝</ci><apply id="S3.Ex1.m1.1.1.3.2.3.cmml" xref="S3.Ex1.m1.1.1.3.2.3"><times id="S3.Ex1.m1.1.1.3.2.3.1.cmml" xref="S3.Ex1.m1.1.1.3.2.3.1"></times><ci id="S3.Ex1.m1.1.1.3.2.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2.3.2">𝑐</ci><ci id="S3.Ex1.m1.1.1.3.2.3.3.cmml" xref="S3.Ex1.m1.1.1.3.2.3.3">𝑜</ci><ci id="S3.Ex1.m1.1.1.3.2.3.4.cmml" xref="S3.Ex1.m1.1.1.3.2.3.4">𝑝</ci><ci id="S3.Ex1.m1.1.1.3.2.3.5.cmml" xref="S3.Ex1.m1.1.1.3.2.3.5">𝑦</ci></apply></apply><ci id="S3.Ex1.m1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.2"></times><ci id="S3.Ex1.m1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.3">𝜎</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><plus id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2"></plus><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.3.2a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.2">W</mtext></ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.2">c</mtext></ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.2">d</mtext></ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.2">s</mtext></ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.4.3">𝑡</ci></apply></apply></apply><ci id="S3.Ex1.m1.1.1.1.1.1.1.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">B</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle p_{copy}^{t}=\sigma(\textbf{W}^{T}(\textbf{c}^{t}\oplus\textbf{d%
}^{t}\oplus\textbf{s}^{t})+\textbf{B})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_σ ( W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ( c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⊕ d start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⊕ s start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) + B )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\textbf{P}^{t}=p_{copy}^{t}\cdot\textbf{P}_{\textbf{c}}^{t}+(1-p_%
{copy}^{t})\cdot\textbf{P}_{\textbf{g}}^{t}" class="ltx_Math" display="inline" id="S3.Ex2.m1.1"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><msup id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.3.2" xref="S3.Ex2.m1.1.1.3.2a.cmml">P</mtext><mi id="S3.Ex2.m1.1.1.3.3" xref="S3.Ex2.m1.1.1.3.3.cmml">t</mi></msup><mo id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex2.m1.1.1.1" xref="S3.Ex2.m1.1.1.1.cmml"><mrow id="S3.Ex2.m1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.3.cmml"><msubsup id="S3.Ex2.m1.1.1.1.3.2" xref="S3.Ex2.m1.1.1.1.3.2.cmml"><mi id="S3.Ex2.m1.1.1.1.3.2.2.2" xref="S3.Ex2.m1.1.1.1.3.2.2.2.cmml">p</mi><mrow id="S3.Ex2.m1.1.1.1.3.2.2.3" xref="S3.Ex2.m1.1.1.1.3.2.2.3.cmml"><mi id="S3.Ex2.m1.1.1.1.3.2.2.3.2" xref="S3.Ex2.m1.1.1.1.3.2.2.3.2.cmml">c</mi><mo id="S3.Ex2.m1.1.1.1.3.2.2.3.1" xref="S3.Ex2.m1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.3.2.2.3.3" xref="S3.Ex2.m1.1.1.1.3.2.2.3.3.cmml">o</mi><mo id="S3.Ex2.m1.1.1.1.3.2.2.3.1a" xref="S3.Ex2.m1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.3.2.2.3.4" xref="S3.Ex2.m1.1.1.1.3.2.2.3.4.cmml">p</mi><mo id="S3.Ex2.m1.1.1.1.3.2.2.3.1b" xref="S3.Ex2.m1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.3.2.2.3.5" xref="S3.Ex2.m1.1.1.1.3.2.2.3.5.cmml">y</mi></mrow><mi id="S3.Ex2.m1.1.1.1.3.2.3" xref="S3.Ex2.m1.1.1.1.3.2.3.cmml">t</mi></msubsup><mo id="S3.Ex2.m1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.Ex2.m1.1.1.1.3.1.cmml">⋅</mo><msubsup id="S3.Ex2.m1.1.1.1.3.3" xref="S3.Ex2.m1.1.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.3.3.2.2" xref="S3.Ex2.m1.1.1.1.3.3.2.2a.cmml">P</mtext><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.3.3.2.3" xref="S3.Ex2.m1.1.1.1.3.3.2.3a.cmml">c</mtext><mi id="S3.Ex2.m1.1.1.1.3.3.3" xref="S3.Ex2.m1.1.1.1.3.3.3.cmml">t</mi></msubsup></mrow><mo id="S3.Ex2.m1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.Ex2.m1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.Ex2.m1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.Ex2.m1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mrow id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.2.cmml">c</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.3.cmml">o</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1a" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.4" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.4.cmml">p</mi><mo id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1b" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.5" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.5.cmml">y</mi></mrow><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.3.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msubsup></mrow><mo id="S3.Ex2.m1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.Ex2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex2.m1.1.1.1.1.2" rspace="0.222em" xref="S3.Ex2.m1.1.1.1.1.2.cmml">⋅</mo><msubsup id="S3.Ex2.m1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.1.3.2.2" xref="S3.Ex2.m1.1.1.1.1.3.2.2a.cmml">P</mtext><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.1.3.2.3" xref="S3.Ex2.m1.1.1.1.1.3.2.3a.cmml">g</mtext><mi id="S3.Ex2.m1.1.1.1.1.3.3" xref="S3.Ex2.m1.1.1.1.1.3.3.cmml">t</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2"></eq><apply id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.3">superscript</csymbol><ci id="S3.Ex2.m1.1.1.3.2a.cmml" xref="S3.Ex2.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.3.2">P</mtext></ci><ci id="S3.Ex2.m1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.3.3">𝑡</ci></apply><apply id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1"><plus id="S3.Ex2.m1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.2"></plus><apply id="S3.Ex2.m1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.3"><ci id="S3.Ex2.m1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.3.1">⋅</ci><apply id="S3.Ex2.m1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.3.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.2">𝑝</ci><apply id="S3.Ex2.m1.1.1.1.3.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3"><times id="S3.Ex2.m1.1.1.1.3.2.2.3.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3.1"></times><ci id="S3.Ex2.m1.1.1.1.3.2.2.3.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3.2">𝑐</ci><ci id="S3.Ex2.m1.1.1.1.3.2.2.3.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3.3">𝑜</ci><ci id="S3.Ex2.m1.1.1.1.3.2.2.3.4.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3.4">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.3.2.2.3.5.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.3.5">𝑦</ci></apply></apply><ci id="S3.Ex2.m1.1.1.1.3.2.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.3">𝑡</ci></apply><apply id="S3.Ex2.m1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.3.1.cmml" xref="S3.Ex2.m1.1.1.1.3.3">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.3.3.2.cmml" xref="S3.Ex2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.3.3.2.2a.cmml" xref="S3.Ex2.m1.1.1.1.3.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.3.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.3.2.2">P</mtext></ci><ci id="S3.Ex2.m1.1.1.1.3.3.2.3a.cmml" xref="S3.Ex2.m1.1.1.1.3.3.2.3"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.3.3.2.3.cmml" mathsize="70%" xref="S3.Ex2.m1.1.1.1.3.3.2.3">c</mtext></ci></apply><ci id="S3.Ex2.m1.1.1.1.3.3.3.cmml" xref="S3.Ex2.m1.1.1.1.3.3.3">𝑡</ci></apply></apply><apply id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1"><ci id="S3.Ex2.m1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2">⋅</ci><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"><minus id="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.Ex2.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.Ex2.m1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.2">𝑝</ci><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3"><times id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.2">𝑐</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.3">𝑜</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.4">𝑝</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.5.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.2.3.5">𝑦</ci></apply></apply><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S3.Ex2.m1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.3.2.2a.cmml" xref="S3.Ex2.m1.1.1.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.3.2.2">P</mtext></ci><ci id="S3.Ex2.m1.1.1.1.1.3.2.3a.cmml" xref="S3.Ex2.m1.1.1.1.1.3.2.3"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.Ex2.m1.1.1.1.1.3.2.3">g</mtext></ci></apply><ci id="S3.Ex2.m1.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">\displaystyle\textbf{P}^{t}=p_{copy}^{t}\cdot\textbf{P}_{\textbf{c}}^{t}+(1-p_%
{copy}^{t})\cdot\textbf{P}_{\textbf{g}}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.1d">P start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⋅ P start_POSTSUBSCRIPT c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + ( 1 - italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) ⋅ P start_POSTSUBSCRIPT g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.12">Here, <math alttext="\textbf{c}^{t}" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><msup id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2a.cmml">c</mtext><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">superscript</csymbol><ci id="S3.p2.1.m1.1.1.2a.cmml" xref="S3.p2.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">c</mtext></ci><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\textbf{c}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is the context vector, calculated as <math alttext="\textbf{c}^{t}=(\textbf{a}^{t})^{T}\textbf{e}^{t}" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><msup id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.3.2" xref="S3.p2.2.m2.1.1.3.2a.cmml">c</mtext><mi id="S3.p2.2.m2.1.1.3.3" xref="S3.p2.2.m2.1.1.3.3.cmml">t</mi></msup><mo id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">=</mo><mrow id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1.cmml"><msup id="S3.p2.2.m2.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.cmml"><mrow id="S3.p2.2.m2.1.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.1.cmml"><mo id="S3.p2.2.m2.1.1.1.1.1.1.2" stretchy="false" xref="S3.p2.2.m2.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.p2.2.m2.1.1.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.1.1.1.1.1.2" xref="S3.p2.2.m2.1.1.1.1.1.1.1.2a.cmml">a</mtext><mi id="S3.p2.2.m2.1.1.1.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.1.1.1.3.cmml">t</mi></msup><mo id="S3.p2.2.m2.1.1.1.1.1.1.3" stretchy="false" xref="S3.p2.2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.p2.2.m2.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.3.cmml">T</mi></msup><mo id="S3.p2.2.m2.1.1.1.2" xref="S3.p2.2.m2.1.1.1.2.cmml">⁢</mo><msup id="S3.p2.2.m2.1.1.1.3" xref="S3.p2.2.m2.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.1.3.2" xref="S3.p2.2.m2.1.1.1.3.2a.cmml">e</mtext><mi id="S3.p2.2.m2.1.1.1.3.3" xref="S3.p2.2.m2.1.1.1.3.3.cmml">t</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><eq id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2"></eq><apply id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.p2.2.m2.1.1.3.2a.cmml" xref="S3.p2.2.m2.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.3.2">c</mtext></ci><ci id="S3.p2.2.m2.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.3.3">𝑡</ci></apply><apply id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"><times id="S3.p2.2.m2.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.2"></times><apply id="S3.p2.2.m2.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.1">superscript</csymbol><apply id="S3.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.p2.2.m2.1.1.1.1.1.1.1.2a.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.1.2">a</mtext></ci><ci id="S3.p2.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.p2.2.m2.1.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.1.3">𝑇</ci></apply><apply id="S3.p2.2.m2.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.1.3">superscript</csymbol><ci id="S3.p2.2.m2.1.1.1.3.2a.cmml" xref="S3.p2.2.m2.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.2.m2.1.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.1.3.2">e</mtext></ci><ci id="S3.p2.2.m2.1.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.1.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\textbf{c}^{t}=(\textbf{a}^{t})^{T}\textbf{e}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">c start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = ( a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT e start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\textbf{a}^{t}" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><msup id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2a.cmml">a</mtext><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">superscript</csymbol><ci id="S3.p2.3.m3.1.1.2a.cmml" xref="S3.p2.3.m3.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">a</mtext></ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\textbf{a}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> represents cross-attention vector, and <math alttext="\textbf{e}^{t}" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><msup id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2a.cmml">e</mtext><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.p2.4.m4.1.1.2a.cmml" xref="S3.p2.4.m4.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">e</mtext></ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\textbf{e}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">e start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> contains the encoder hidden states. <math alttext="\textbf{d}^{t}" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><msup id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2a.cmml">d</mtext><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">superscript</csymbol><ci id="S3.p2.5.m5.1.1.2a.cmml" xref="S3.p2.5.m5.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">d</mtext></ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\textbf{d}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">d start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\textbf{s}^{t}" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><msup id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2a.cmml">s</mtext><mi id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.p2.6.m6.1.1.2a.cmml" xref="S3.p2.6.m6.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">s</mtext></ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\textbf{s}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">s start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> contain the decoder’s final hidden states and input respectively, <math alttext="\oplus" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><mo id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><csymbol cd="latexml" id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">\oplus</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">⊕</annotation></semantics></math> denotes concatenation, and <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.p2.12.1">W</span> and <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.p2.12.2">B</span> are learned weights and a bias vector respectively.
<math alttext="\textbf{P}_{\textbf{c}}^{t}" class="ltx_Math" display="inline" id="S3.p2.10.m10.1"><semantics id="S3.p2.10.m10.1a"><msubsup id="S3.p2.10.m10.1.1" xref="S3.p2.10.m10.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.10.m10.1.1.2.2" xref="S3.p2.10.m10.1.1.2.2a.cmml">P</mtext><mtext class="ltx_mathvariant_bold" id="S3.p2.10.m10.1.1.2.3" xref="S3.p2.10.m10.1.1.2.3a.cmml">c</mtext><mi id="S3.p2.10.m10.1.1.3" xref="S3.p2.10.m10.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.10.m10.1b"><apply id="S3.p2.10.m10.1.1.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p2.10.m10.1.1.1.cmml" xref="S3.p2.10.m10.1.1">superscript</csymbol><apply id="S3.p2.10.m10.1.1.2.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p2.10.m10.1.1.2.1.cmml" xref="S3.p2.10.m10.1.1">subscript</csymbol><ci id="S3.p2.10.m10.1.1.2.2a.cmml" xref="S3.p2.10.m10.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.10.m10.1.1.2.2.cmml" xref="S3.p2.10.m10.1.1.2.2">P</mtext></ci><ci id="S3.p2.10.m10.1.1.2.3a.cmml" xref="S3.p2.10.m10.1.1.2.3"><mtext class="ltx_mathvariant_bold" id="S3.p2.10.m10.1.1.2.3.cmml" mathsize="70%" xref="S3.p2.10.m10.1.1.2.3">c</mtext></ci></apply><ci id="S3.p2.10.m10.1.1.3.cmml" xref="S3.p2.10.m10.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.10.m10.1c">\textbf{P}_{\textbf{c}}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.10.m10.1d">P start_POSTSUBSCRIPT c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\textbf{P}_{\textbf{g}}^{t}" class="ltx_Math" display="inline" id="S3.p2.11.m11.1"><semantics id="S3.p2.11.m11.1a"><msubsup id="S3.p2.11.m11.1.1" xref="S3.p2.11.m11.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.p2.11.m11.1.1.2.2" xref="S3.p2.11.m11.1.1.2.2a.cmml">P</mtext><mtext class="ltx_mathvariant_bold" id="S3.p2.11.m11.1.1.2.3" xref="S3.p2.11.m11.1.1.2.3a.cmml">g</mtext><mi id="S3.p2.11.m11.1.1.3" xref="S3.p2.11.m11.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.11.m11.1b"><apply id="S3.p2.11.m11.1.1.cmml" xref="S3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p2.11.m11.1.1.1.cmml" xref="S3.p2.11.m11.1.1">superscript</csymbol><apply id="S3.p2.11.m11.1.1.2.cmml" xref="S3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p2.11.m11.1.1.2.1.cmml" xref="S3.p2.11.m11.1.1">subscript</csymbol><ci id="S3.p2.11.m11.1.1.2.2a.cmml" xref="S3.p2.11.m11.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.p2.11.m11.1.1.2.2.cmml" xref="S3.p2.11.m11.1.1.2.2">P</mtext></ci><ci id="S3.p2.11.m11.1.1.2.3a.cmml" xref="S3.p2.11.m11.1.1.2.3"><mtext class="ltx_mathvariant_bold" id="S3.p2.11.m11.1.1.2.3.cmml" mathsize="70%" xref="S3.p2.11.m11.1.1.2.3">g</mtext></ci></apply><ci id="S3.p2.11.m11.1.1.3.cmml" xref="S3.p2.11.m11.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.11.m11.1c">\textbf{P}_{\textbf{g}}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.11.m11.1d">P start_POSTSUBSCRIPT g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> represent the copy and generate distributions (softmaxed logits) respectively at step <math alttext="t" class="ltx_Math" display="inline" id="S3.p2.12.m12.1"><semantics id="S3.p2.12.m12.1a"><mi id="S3.p2.12.m12.1.1" xref="S3.p2.12.m12.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p2.12.m12.1b"><ci id="S3.p2.12.m12.1.1.cmml" xref="S3.p2.12.m12.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.12.m12.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p2.12.m12.1d">italic_t</annotation></semantics></math>. We use cross-attention weights over source tokens for the copy logits, and standard decoder outputs for generate logits.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Datasets and languages</h3>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">We used the WikiMatrix (<span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.1">wm)</span> corpus <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib29" title="">2019</a>)</cite> for <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.2">es-en</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.3">es-ca</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.4">fr-de</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.5">fr-oc</span>. For <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.6">es-ca</span>, we also report results on synthetic Europarl (<span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.7">ep</span>) parallel data <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib19" title="">2005</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The Europarl dataset was automatically translated into Catalan; taken from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Softcatala/Europarl-catalan" title="">https://github.com/Softcatala/Europarl-catalan</a>.</span></span></span> For <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.8">hi-mr</span>, we used the CVIT-PIB corpus <cite class="ltx_cite ltx_citemacro_citep">(Philip et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib27" title="">2021</a>)</cite>, and for the low-resource pair <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.9">hi-bh</span>, we use the NLLB corpus <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib30" title="">2021</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib34" title="">2022</a>; Heffernan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib14" title="">2022</a>)</cite>. See Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.T1" title="Table 1 ‣ Datasets and languages ‣ 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">1</span></a> for dataset heuristics. Note the higher per token overlap as expected for our closely-related group as compared to the others. The <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px1.p1.1.10">hi-bh</span> sentences are extremely short, and share fewer tokens than expected: in this case, this reflects badly parallel data.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A1" title="Appendix A Notes on Datasets ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">A</span></a> for more details on datasets.</span></span></span></p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.2"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.2.1">hi-mr</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.3"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.3.1">hi-bh</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.4"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.4.1">es-en</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.5"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.5.1">es-ca (ep)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.6"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.6.1">es-ca (wm)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.7.1">fr-de</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.8"><span class="ltx_text ltx_font_typewriter" id="S4.T1.1.1.8.1">fr-oc</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1">Avg. common tokens per sent pair</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2">2.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.3">1.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.4">2.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.5">6.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.6">7.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.7">1.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.8">5.06</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.1">Avg. common tokens per target token</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2">0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3">0.16</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.4">0.13</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.5">0.26</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.6">0.29</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.7">0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.8">0.22</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.1">Avg. source sentence length</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2">28.54</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3">6.34</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4">23.43</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.5">26.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.6">25.38</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.7">19.10</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.8">24.09</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.5.1">Avg. target sentence length</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.2">20.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.3">7.98</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.4">20.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.5">26.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.6">24.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.7">16.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.5.8">23.43</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics on commons source-target tokens in our datasets. <span class="ltx_text ltx_font_typewriter" id="S4.T1.4.1">wm</span>: WikiMatrix, <span class="ltx_text ltx_font_typewriter" id="S4.T1.5.2">ep</span>: Europarl. </figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Experimental Settings</h3>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.9">For all language pairs, we performed experiments on dataset subsets of <math alttext="5k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">5</mn><mo id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1"></times><cn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2">5</cn><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">5k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">5 italic_k</annotation></semantics></math>, <math alttext="15k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">15</mn><mo id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1"></times><cn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2">15</cn><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">15k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">15 italic_k</annotation></semantics></math>, <math alttext="30k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">30</mn><mo id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1"><times id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.1"></times><cn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.2">30</cn><ci id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">30k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m3.1d">30 italic_k</annotation></semantics></math>, and <math alttext="60k" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">60</mn><mo id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1"></times><cn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2">60</cn><ci id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">60k</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m4.1d">60 italic_k</annotation></semantics></math> sentences and test sets of <math alttext="5000" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px2.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.5.m5.1b"><cn id="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.5.m5.1c">5000</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.5.m5.1d">5000</annotation></semantics></math> sentences, trained until convergence, with tokenizer size <math alttext="16000" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S4.SS0.SSS0.Px2.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">16000</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.6.m6.1b"><cn id="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1">16000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.6.m6.1c">16000</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.6.m6.1d">16000</annotation></semantics></math>. We computed baseline results (<span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS0.Px2.p1.9.1">NMT</span>) on standard encoder-decoder NMT. All <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS0.Px2.p1.9.2">PGN</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS0.SSS0.Px2.p1.9.3">NMT</span> models use <math alttext="6" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px2.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.7.m7.1b"><cn id="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.7.m7.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.7.m7.1d">6</annotation></semantics></math> encoder and decoder layers, <math alttext="4" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px2.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.8.m8.1b"><cn id="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.8.m8.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.8.m8.1d">4</annotation></semantics></math> attention heads, and a hidden size of <math alttext="512" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.9.m9.1"><semantics id="S4.SS0.SSS0.Px2.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px2.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px2.p1.9.m9.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.9.m9.1b"><cn id="S4.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.9.m9.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.9.m9.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.9.m9.1d">512</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Improvement patterns</h3>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.1">
<tr class="ltx_tr" id="S5.T2.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T2.1.1.2"></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.3"></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.4.1">
<span class="ltx_p" id="S5.T2.1.1.4.1.1">5K</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.5.1">
<span class="ltx_p" id="S5.T2.1.1.5.1.1">15K</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.6.1">
<span class="ltx_p" id="S5.T2.1.1.6.1.1">30K</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.7.1">
<span class="ltx_p" id="S5.T2.1.1.7.1.1">60K</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S5.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.1.1.1">
<span class="ltx_p" id="S5.T2.1.1.1.1.1">Avg. <math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.2.1.1">hi-mr</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.2.1">
<span class="ltx_p" id="S5.T2.1.2.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.2.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.3.1">
<span class="ltx_p" id="S5.T2.1.2.3.1.1">3.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.4.1">
<span class="ltx_p" id="S5.T2.1.2.4.1.1">7.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.5.1">
<span class="ltx_p" id="S5.T2.1.2.5.1.1">11.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.2.6.1">
<span class="ltx_p" id="S5.T2.1.2.6.1.1">16.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.2.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3">
<td class="ltx_td" id="S5.T2.1.3.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.2.1">
<span class="ltx_p" id="S5.T2.1.3.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.3.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.3.1">
<span class="ltx_p" id="S5.T2.1.3.3.1.1">3.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.4.1">
<span class="ltx_p" id="S5.T2.1.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.4.1.1.1">7.9</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.5.1">
<span class="ltx_p" id="S5.T2.1.3.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.5.1.1.1">12.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.6.1">
<span class="ltx_p" id="S5.T2.1.3.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.3.6.1.1.1">18.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.3.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.3.7.1">
<span class="ltx_p" id="S5.T2.1.3.7.1.1">+0.8</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.4.1"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T2.1.4.1.1">hi-bh</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.2.1">
<span class="ltx_p" id="S5.T2.1.4.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.4.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.3.1">
<span class="ltx_p" id="S5.T2.1.4.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.3.1.1.1">5.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.4.1">
<span class="ltx_p" id="S5.T2.1.4.4.1.1">6.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.5.1">
<span class="ltx_p" id="S5.T2.1.4.5.1.1">9.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.4.6.1">
<span class="ltx_p" id="S5.T2.1.4.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.6.1.1.1">15.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.4.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5">
<td class="ltx_td" id="S5.T2.1.5.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.2.1">
<span class="ltx_p" id="S5.T2.1.5.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.5.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.3.1">
<span class="ltx_p" id="S5.T2.1.5.3.1.1">4.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.4.1">
<span class="ltx_p" id="S5.T2.1.5.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.4.1.1.1">8.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.5.1">
<span class="ltx_p" id="S5.T2.1.5.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.5.5.1.1.1">11.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.6.1">
<span class="ltx_p" id="S5.T2.1.5.6.1.1">12.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.5.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.5.7.1">
<span class="ltx_p" id="S5.T2.1.5.7.1.1">-0.2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.6.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.6.1.1">es-en</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.2.1">
<span class="ltx_p" id="S5.T2.1.6.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.6.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.3.1">
<span class="ltx_p" id="S5.T2.1.6.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.3.1.1.1">9.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.4.1">
<span class="ltx_p" id="S5.T2.1.6.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.4.1.1.1">30.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.5.1">
<span class="ltx_p" id="S5.T2.1.6.5.1.1">38.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.6.6.1">
<span class="ltx_p" id="S5.T2.1.6.6.1.1">41.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.6.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7">
<td class="ltx_td" id="S5.T2.1.7.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.2.1">
<span class="ltx_p" id="S5.T2.1.7.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.7.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.3.1">
<span class="ltx_p" id="S5.T2.1.7.3.1.1">9.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.4.1">
<span class="ltx_p" id="S5.T2.1.7.4.1.1">30.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.5.1">
<span class="ltx_p" id="S5.T2.1.7.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.5.1.1.1">38.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.6.1">
<span class="ltx_p" id="S5.T2.1.7.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.7.6.1.1.1">42.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.7.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.7.7.1">
<span class="ltx_p" id="S5.T2.1.7.7.1.1">0.0</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.8.1"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T2.1.8.1.1">es-ca(wm)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.2.1">
<span class="ltx_p" id="S5.T2.1.8.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.8.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.3.1">
<span class="ltx_p" id="S5.T2.1.8.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.8.3.1.1.1">35.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.4.1">
<span class="ltx_p" id="S5.T2.1.8.4.1.1">50.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.5.1">
<span class="ltx_p" id="S5.T2.1.8.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.8.5.1.1.1">54.3</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.8.6.1">
<span class="ltx_p" id="S5.T2.1.8.6.1.1">56.4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.8.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9">
<td class="ltx_td" id="S5.T2.1.9.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.2.1">
<span class="ltx_p" id="S5.T2.1.9.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.9.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.3.1">
<span class="ltx_p" id="S5.T2.1.9.3.1.1">34.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.4.1">
<span class="ltx_p" id="S5.T2.1.9.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.4.1.1.1">51.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.5.1">
<span class="ltx_p" id="S5.T2.1.9.5.1.1">54.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.6.1">
<span class="ltx_p" id="S5.T2.1.9.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.9.6.1.1.1">57.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.9.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.9.7.1">
<span class="ltx_p" id="S5.T2.1.9.7.1.1">0.0</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.10.1"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T2.1.10.1.1">es-ca(ep)</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.2.1">
<span class="ltx_p" id="S5.T2.1.10.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.10.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.3.1">
<span class="ltx_p" id="S5.T2.1.10.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.10.3.1.1.1">62.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.4.1">
<span class="ltx_p" id="S5.T2.1.10.4.1.1">70.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.5.1">
<span class="ltx_p" id="S5.T2.1.10.5.1.1">73.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.10.6.1">
<span class="ltx_p" id="S5.T2.1.10.6.1.1">73.6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.10.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.11">
<td class="ltx_td" id="S5.T2.1.11.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.2.1">
<span class="ltx_p" id="S5.T2.1.11.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.11.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.3.1">
<span class="ltx_p" id="S5.T2.1.11.3.1.1">62.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.4.1">
<span class="ltx_p" id="S5.T2.1.11.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.4.1.1.1">71.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.5.1">
<span class="ltx_p" id="S5.T2.1.11.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.5.1.1.1">74.0</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.6.1">
<span class="ltx_p" id="S5.T2.1.11.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.11.6.1.1.1">74.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.11.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.11.7.1">
<span class="ltx_p" id="S5.T2.1.11.7.1.1">+0.6</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.12.1"><span class="ltx_text ltx_font_typewriter" id="S5.T2.1.12.1.1">fr-de</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.2.1">
<span class="ltx_p" id="S5.T2.1.12.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.12.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.3.1">
<span class="ltx_p" id="S5.T2.1.12.3.1.1">3.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.4.1">
<span class="ltx_p" id="S5.T2.1.12.4.1.1">10.8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.5.1">
<span class="ltx_p" id="S5.T2.1.12.5.1.1">19.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.12.6.1">
<span class="ltx_p" id="S5.T2.1.12.6.1.1">27.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.12.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.13">
<td class="ltx_td" id="S5.T2.1.13.1"></td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.2.1">
<span class="ltx_p" id="S5.T2.1.13.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.13.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.3.1">
<span class="ltx_p" id="S5.T2.1.13.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.3.1.1.1">3.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.4.1">
<span class="ltx_p" id="S5.T2.1.13.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.4.1.1.1">11.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.5.1">
<span class="ltx_p" id="S5.T2.1.13.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.13.5.1.1.1">20.1</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.6.1">
<span class="ltx_p" id="S5.T2.1.13.6.1.1">27.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.1.13.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.13.7.1">
<span class="ltx_p" id="S5.T2.1.13.7.1.1">+0.3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.14.1"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T2.1.14.1.1">fr-oc</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.14.2.1">
<span class="ltx_p" id="S5.T2.1.14.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.14.2.1.1.1">NMT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.14.3.1">
<span class="ltx_p" id="S5.T2.1.14.3.1.1">24.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.14.4.1">
<span class="ltx_p" id="S5.T2.1.14.4.1.1">42.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.14.5.1">
<span class="ltx_p" id="S5.T2.1.14.5.1.1">45.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.14.6.1">
<span class="ltx_p" id="S5.T2.1.14.6.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.14.6.1.1.1">48.7</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.1.14.7"></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.15">
<td class="ltx_td ltx_border_bb" id="S5.T2.1.15.1"></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.2.1">
<span class="ltx_p" id="S5.T2.1.15.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.15.2.1.1.1">PGN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.3.1">
<span class="ltx_p" id="S5.T2.1.15.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.15.3.1.1.1">24.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.4.1">
<span class="ltx_p" id="S5.T2.1.15.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.15.4.1.1.1">43.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.5.1">
<span class="ltx_p" id="S5.T2.1.15.5.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.15.5.1.1.1">46.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.6.1">
<span class="ltx_p" id="S5.T2.1.15.6.1.1">48.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.1.15.7">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.1.15.7.1">
<span class="ltx_p" id="S5.T2.1.15.7.1.1">+0.5</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>spBLEU across dataset sizes (#sents). Closely-related pairs are <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T2.5.1">underlined</span>. <span class="ltx_text ltx_font_typewriter" id="S5.T2.6.2">wm</span>: WikiMatrix, <span class="ltx_text ltx_font_typewriter" id="S5.T2.7.3">ep</span>: Europarl.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.1">
<tr class="ltx_tr" id="S5.T3.1.1">
<td class="ltx_td ltx_border_tt" id="S5.T3.1.1.2"></td>
<td class="ltx_td ltx_border_tt" id="S5.T3.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.4"><span class="ltx_text ltx_font_typewriter" id="S5.T3.1.1.4.1">hi-mr</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.5"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T3.1.1.5.1">hi-bh</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.6"><span class="ltx_text ltx_font_typewriter" id="S5.T3.1.1.6.1">es-en</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.7"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T3.1.1.7.1">es-ca</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.8"><span class="ltx_text ltx_font_typewriter" id="S5.T3.1.1.8.1">fr-de</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.9"><span class="ltx_text ltx_font_typewriter ltx_framed ltx_framed_underline" id="S5.T3.1.1.9.1">fr-oc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.1">
<span class="ltx_text" id="S5.T3.1.1.1.2"></span> <span class="ltx_text" id="S5.T3.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.1.1.1.1.1">
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.1.1.2.1">Avg.</span></span>
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.1.1.1.1.1.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.1.1.m1.1a"><mi id="S5.T3.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S5.T3.1.1.1.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S5.T3.1.1.1.3"></span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.1">L</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.2.2.1">NMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.3">7.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.4">10.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.5.1">26.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.6.1">44.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.7.1">11.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.8.1">35.5</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.2.9"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3">
<td class="ltx_td" id="S5.T3.1.3.1"></td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.3.2.1">PGN</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.3.1">8.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.3.4.1">13.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.5">24.4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.6">44.6</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.7">10.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.8">35.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.9">-0.7</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.1">H</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.4.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.4.2.1">NMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.3">28.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.4.1">22.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.5">69.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.6.1">72.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.7.1">52.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.4.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.4.8.1">65.2</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.4.9"></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5">
<td class="ltx_td ltx_border_bb" id="S5.T3.1.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.1.5.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.5.2.1">PGN</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.3.1">29.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.4">18.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.5.5.1">69.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.6">71.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.7">51.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.8">64.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.5.9">-1.0</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>spBLEU scores on test sets with low (L) and high (H) density of shared source-target subwords.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.3">See results in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.T2" title="Table 2 ‣ Improvement patterns ‣ 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a>.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We report spBLEU since our approach attempts to benefit performance on shared subwords.</span></span></span> Our results are not directly comparable to those in the literature due to differences mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S2" title="2 Related Work ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a> and the size of training bitext (<math alttext="2M" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">2</mn><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><times id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">2</cn><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">2M</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">2 italic_M</annotation></semantics></math> in <cite class="ltx_cite ltx_citemacro_citep">(Gulcehre et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib12" title="">2016</a>)</cite>, <math alttext="~{}1M" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">1</mn><mo id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1"><times id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1"></times><cn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2">1</cn><ci id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">~{}1M</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.2.m2.1d">1 italic_M</annotation></semantics></math> in <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib38" title="">2021</a>)</cite> vs. our maximum resource setting of <math alttext="0.06M" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S5.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">0.06</mn><mo id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1"><times id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.1"></times><cn id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.2">0.06</cn><ci id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.3.m3.1c">0.06M</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.3.m3.1d">0.06 italic_M</annotation></semantics></math>).<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>For a rough idea: <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib38" title="">2021</a>)</cite> report gains in MT of <math alttext="1.5-2.5" class="ltx_Math" display="inline" id="footnote7.m1.1"><semantics id="footnote7.m1.1b"><mrow id="footnote7.m1.1.1" xref="footnote7.m1.1.1.cmml"><mn id="footnote7.m1.1.1.2" xref="footnote7.m1.1.1.2.cmml">1.5</mn><mo id="footnote7.m1.1.1.1" xref="footnote7.m1.1.1.1.cmml">−</mo><mn id="footnote7.m1.1.1.3" xref="footnote7.m1.1.1.3.cmml">2.5</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote7.m1.1c"><apply id="footnote7.m1.1.1.cmml" xref="footnote7.m1.1.1"><minus id="footnote7.m1.1.1.1.cmml" xref="footnote7.m1.1.1.1"></minus><cn id="footnote7.m1.1.1.2.cmml" type="float" xref="footnote7.m1.1.1.2">1.5</cn><cn id="footnote7.m1.1.1.3.cmml" type="float" xref="footnote7.m1.1.1.3">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote7.m1.1d">1.5-2.5</annotation><annotation encoding="application/x-llamapun" id="footnote7.m1.1e">1.5 - 2.5</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter" id="footnote7.1">BLEU</span>.</span></span></span> We see weak improvements in a majority of settings; however, counter to intuition, <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS0.Px1.p1.3.1">PGN</span> does not show a clear advantage for closely-related as compared to more distant pairs, or for lower-resource settings.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Controlled test sets</h3>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">We test the motivating hypothesis that <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS0.Px2.p1.1.1">PGN</span> models is likely to benefit sentence pairs with higher subword overlap. We rank sentence pairs in our test set by percentage of shared subwords in source and target, and construct test subsets with low and high shared-subword density from the top and bottom <math alttext="500" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px2.p1.1.m1.1d">500</annotation></semantics></math> sentences respectively. However, in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.T3" title="Table 3 ‣ Improvement patterns ‣ 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">3</span></a>, we see that in fact that <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS0.Px2.p1.1.2">PGN</span> performs slightly worse than <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS0.Px2.p1.1.3">NMT</span> on both extremes, indicating that observed benefits over the entire test set do not come from subword overlap.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Usage of the copy mechanism</h3>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.3">We record values of <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px3.p1.1.m1.1a"><msub id="S5.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">c</mi><mo id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml">o</mi><mo id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1a" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.4" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1b" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.5" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3"><times id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.2">𝑐</ci><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.1.m1.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> to track the model’s usage of the copy mechanism. While <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px3.p1.2.m2.1a"><msub id="S5.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.2" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.cmml">c</mi><mo id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.3" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.3.cmml">o</mi><mo id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1a" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.4" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.4.cmml">p</mi><mo id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1b" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.5" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3"><times id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.2">𝑐</ci><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.3">𝑜</ci><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.2.m2.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values are relatively high<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Note that it is difficult to comment on absolute values of <math alttext="p_{copy}" class="ltx_Math" display="inline" id="footnote8.m1.1"><semantics id="footnote8.m1.1b"><msub id="footnote8.m1.1.1" xref="footnote8.m1.1.1.cmml"><mi id="footnote8.m1.1.1.2" xref="footnote8.m1.1.1.2.cmml">p</mi><mrow id="footnote8.m1.1.1.3" xref="footnote8.m1.1.1.3.cmml"><mi id="footnote8.m1.1.1.3.2" xref="footnote8.m1.1.1.3.2.cmml">c</mi><mo id="footnote8.m1.1.1.3.1" xref="footnote8.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m1.1.1.3.3" xref="footnote8.m1.1.1.3.3.cmml">o</mi><mo id="footnote8.m1.1.1.3.1b" xref="footnote8.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m1.1.1.3.4" xref="footnote8.m1.1.1.3.4.cmml">p</mi><mo id="footnote8.m1.1.1.3.1c" xref="footnote8.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m1.1.1.3.5" xref="footnote8.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="footnote8.m1.1c"><apply id="footnote8.m1.1.1.cmml" xref="footnote8.m1.1.1"><csymbol cd="ambiguous" id="footnote8.m1.1.1.1.cmml" xref="footnote8.m1.1.1">subscript</csymbol><ci id="footnote8.m1.1.1.2.cmml" xref="footnote8.m1.1.1.2">𝑝</ci><apply id="footnote8.m1.1.1.3.cmml" xref="footnote8.m1.1.1.3"><times id="footnote8.m1.1.1.3.1.cmml" xref="footnote8.m1.1.1.3.1"></times><ci id="footnote8.m1.1.1.3.2.cmml" xref="footnote8.m1.1.1.3.2">𝑐</ci><ci id="footnote8.m1.1.1.3.3.cmml" xref="footnote8.m1.1.1.3.3">𝑜</ci><ci id="footnote8.m1.1.1.3.4.cmml" xref="footnote8.m1.1.1.3.4">𝑝</ci><ci id="footnote8.m1.1.1.3.5.cmml" xref="footnote8.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="footnote8.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math>. The copying distribution is normalized over the sentence length whereas generate distributions are normalized over the vocabulary; even low values of <math alttext="p_{copy}" class="ltx_Math" display="inline" id="footnote8.m2.1"><semantics id="footnote8.m2.1b"><msub id="footnote8.m2.1.1" xref="footnote8.m2.1.1.cmml"><mi id="footnote8.m2.1.1.2" xref="footnote8.m2.1.1.2.cmml">p</mi><mrow id="footnote8.m2.1.1.3" xref="footnote8.m2.1.1.3.cmml"><mi id="footnote8.m2.1.1.3.2" xref="footnote8.m2.1.1.3.2.cmml">c</mi><mo id="footnote8.m2.1.1.3.1" xref="footnote8.m2.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m2.1.1.3.3" xref="footnote8.m2.1.1.3.3.cmml">o</mi><mo id="footnote8.m2.1.1.3.1b" xref="footnote8.m2.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m2.1.1.3.4" xref="footnote8.m2.1.1.3.4.cmml">p</mi><mo id="footnote8.m2.1.1.3.1c" xref="footnote8.m2.1.1.3.1.cmml">⁢</mo><mi id="footnote8.m2.1.1.3.5" xref="footnote8.m2.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="footnote8.m2.1c"><apply id="footnote8.m2.1.1.cmml" xref="footnote8.m2.1.1"><csymbol cd="ambiguous" id="footnote8.m2.1.1.1.cmml" xref="footnote8.m2.1.1">subscript</csymbol><ci id="footnote8.m2.1.1.2.cmml" xref="footnote8.m2.1.1.2">𝑝</ci><apply id="footnote8.m2.1.1.3.cmml" xref="footnote8.m2.1.1.3"><times id="footnote8.m2.1.1.3.1.cmml" xref="footnote8.m2.1.1.3.1"></times><ci id="footnote8.m2.1.1.3.2.cmml" xref="footnote8.m2.1.1.3.2">𝑐</ci><ci id="footnote8.m2.1.1.3.3.cmml" xref="footnote8.m2.1.1.3.3">𝑜</ci><ci id="footnote8.m2.1.1.3.4.cmml" xref="footnote8.m2.1.1.3.4">𝑝</ci><ci id="footnote8.m2.1.1.3.5.cmml" xref="footnote8.m2.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m2.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="footnote8.m2.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> will considerably affect the mixed distribution.</span></span></span> for copied subwords, numerals, and proper nouns, we often see that they they are also high for seemingly random subwords.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3" title="Appendix C Visualizations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">C</span></a> for visualizations of this behaviour. Examples with counter-intuitively high values of <math alttext="p_{copy}" class="ltx_Math" display="inline" id="footnote9.m1.1"><semantics id="footnote9.m1.1b"><msub id="footnote9.m1.1.1" xref="footnote9.m1.1.1.cmml"><mi id="footnote9.m1.1.1.2" xref="footnote9.m1.1.1.2.cmml">p</mi><mrow id="footnote9.m1.1.1.3" xref="footnote9.m1.1.1.3.cmml"><mi id="footnote9.m1.1.1.3.2" xref="footnote9.m1.1.1.3.2.cmml">c</mi><mo id="footnote9.m1.1.1.3.1" xref="footnote9.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote9.m1.1.1.3.3" xref="footnote9.m1.1.1.3.3.cmml">o</mi><mo id="footnote9.m1.1.1.3.1b" xref="footnote9.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote9.m1.1.1.3.4" xref="footnote9.m1.1.1.3.4.cmml">p</mi><mo id="footnote9.m1.1.1.3.1c" xref="footnote9.m1.1.1.3.1.cmml">⁢</mo><mi id="footnote9.m1.1.1.3.5" xref="footnote9.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="footnote9.m1.1c"><apply id="footnote9.m1.1.1.cmml" xref="footnote9.m1.1.1"><csymbol cd="ambiguous" id="footnote9.m1.1.1.1.cmml" xref="footnote9.m1.1.1">subscript</csymbol><ci id="footnote9.m1.1.1.2.cmml" xref="footnote9.m1.1.1.2">𝑝</ci><apply id="footnote9.m1.1.1.3.cmml" xref="footnote9.m1.1.1.3"><times id="footnote9.m1.1.1.3.1.cmml" xref="footnote9.m1.1.1.3.1"></times><ci id="footnote9.m1.1.1.3.2.cmml" xref="footnote9.m1.1.1.3.2">𝑐</ci><ci id="footnote9.m1.1.1.3.3.cmml" xref="footnote9.m1.1.1.3.3">𝑜</ci><ci id="footnote9.m1.1.1.3.4.cmml" xref="footnote9.m1.1.1.3.4">𝑝</ci><ci id="footnote9.m1.1.1.3.5.cmml" xref="footnote9.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="footnote9.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math>: <span class="ltx_text ltx_font_typewriter" id="footnote9.1">quiero-vull (es-ca), behad-atishay (hi-mr)</span>.</span></span></span> We also do not see a relationship between the <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px3.p1.3.m3.1a"><msub id="S5.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">c</mi><mo id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">o</mi><mo id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1a" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.4" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.4.cmml">p</mi><mo id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1b" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.5" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3"><times id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.2">𝑐</ci><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.3">𝑜</ci><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.3.m3.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> value of a target token and the entropy of the cross-attention distribution for that token.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p2.2">A reasonable intuition about <span class="ltx_text ltx_font_smallcaps" id="S5.SS0.SSS0.Px3.p2.2.1">PGN</span> training generalization is that in the absence of any information, the model will default to copying, since this is likely to do better on average than guesses over the entire vocabulary, and that eventually, it will learn to generate language-specific subwords, memorising the relevant strategy for given subwords in encoder states (used to calculate <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p2.1.m1.1"><semantics id="S5.SS0.SSS0.Px3.p2.1.m1.1a"><msub id="S5.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.2" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.2" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.2.cmml">c</mi><mo id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.3" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.cmml">o</mi><mo id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1a" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.4" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.4.cmml">p</mi><mo id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1b" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.5" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p2.1.m1.1b"><apply id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3"><times id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.2">𝑐</ci><ci id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.3">𝑜</ci><ci id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px3.p2.1.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p2.1.m1.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p2.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> as shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S3" title="3 Model ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">3</span></a>). However, our visualizations of cross-attention and <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p2.2.m2.1"><semantics id="S5.SS0.SSS0.Px3.p2.2.m2.1a"><msub id="S5.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.2" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.2" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.2.cmml">c</mi><mo id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.3" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.3.cmml">o</mi><mo id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1a" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.4" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.4.cmml">p</mi><mo id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1b" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.5" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p2.2.m2.1b"><apply id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3"><times id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.2">𝑐</ci><ci id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.3">𝑜</ci><ci id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px3.p2.2.m2.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p2.2.m2.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p2.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> usage throughout training show no evidence of this generalization strategy. It’s possible that since initial cross-attention distributions are noisy, and most subwords are not direct copies, the model is discouraged from copying early on; it’s also possible that the model finds it easier or trivial to encode copied source-target equivalents via the “generate” mechanism and does not need an explicit copier, given that it must additionally learn which subwords should be copied. We discuss potential reasons for this below. In general, it appears that the model uses the copy mechanism to encode a task that is not easily interpretable, possibly resulting in the observed small improvements over some datasets.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Tokenization</h3>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">In theory, the copier would learn best if the tokenizer behaved in a morphologically principled manner.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>e.g. given <span class="ltx_text ltx_font_typewriter" id="footnote10.1">khaya-khalla (ate)</span> in Hindi and Marathi, we ideally want <span class="ltx_text ltx_font_typewriter" id="footnote10.2">kha ##ya</span> and <span class="ltx_text ltx_font_typewriter" id="footnote10.3">kha ##lla</span>. This will allow the common stem <span class="ltx_text ltx_font_typewriter" id="footnote10.4">kha</span> to be copied over, while the language specific inflection subwords can be generated.</span></span></span> However, BPE tokenization generally results in subword splits that may not reflect shared stems in word equivalents <cite class="ltx_cite ltx_citemacro_citep">(Ataman and Federico, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib1" title="">2018</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>e.g. our trained tokenizer contains both <span class="ltx_text ltx_font_typewriter" id="footnote11.1">propuesto</span> (<span class="ltx_text ltx_font_typewriter" id="footnote11.2">es</span>) and <span class="ltx_text ltx_font_typewriter" id="footnote11.3">proposat</span> (<span class="ltx_text ltx_font_typewriter" id="footnote11.4">ca</span>) instead of sharing the subword <span class="ltx_text ltx_font_typewriter" id="footnote11.5">prop</span>.</span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p2.1">A natural idea here may be an investigation of morphologically inspired tokenizers <cite class="ltx_cite ltx_citemacro_citep">(Pan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib26" title="">2020</a>; Ortega et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib25" title="">2020</a>; Chen and Fazio, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib4" title="">2021</a>)</cite>. However, we generally see inconclusive, at best marginal, benefits of such tokenizers over BPE in modern neural MT <cite class="ltx_cite ltx_citemacro_citep">(Macháček et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib22" title="">2018</a>; Domingo et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib9" title="">2019</a>; Mielke et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib23" title="">2021</a>)</cite>, especially those relying on unsupervised morphological segmentation, e.g., with Morfessor <cite class="ltx_cite ltx_citemacro_citep">(Creutz and Lagus, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib7" title="">2007</a>)</cite> in the absence of morphological analysers. These ideas have not been incorporated into mainstream tokenization strategies.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p3">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p3.1">Recent work attempts to solve this general problem by looking at maximisation of shared subwords in multilingual tokenizers <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib6" title="">2020</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib39" title="">2021</a>; Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib21" title="">2023</a>)</cite>; it’s possible that such strategies will dovetail well with PGN mechanisms if widely adopted in the future.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h3 class="ltx_title ltx_title_paragraph">Linguistic complexities</h3>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">While closely-related language show high (subword) vocabulary overlap, word equivalences may be obscured by sound change and orthographic systems; if these changes are word-internal, then even an ideal tokenizer will see different stems/tokens in the source and target.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>e.g. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="footnote12.1">v<span class="ltx_text ltx_font_medium" id="footnote12.1.1">i</span>sh<span class="ltx_text ltx_font_medium" id="footnote12.1.2">was-</span>b<span class="ltx_text ltx_font_medium" id="footnote12.1.3">i</span>s<span class="ltx_text ltx_font_medium" id="footnote12.1.4">was</span></span> (<span class="ltx_text ltx_font_typewriter" id="footnote12.2">hi-bh</span>, sound change), <span class="ltx_text ltx_font_typewriter" id="footnote12.3">website-Webs<span class="ltx_text ltx_font_bold" id="footnote12.3.1">e</span>ite</span> (<span class="ltx_text ltx_font_typewriter" id="footnote12.4">en-de</span>, orthographic system)</span></span></span> Further, we may see that a word that has a cognate in its sister language is translated to a non-cognate, due to semantic drift,
or differences in idiom or usage norms in the two languages, e.g. <span class="ltx_text ltx_font_typewriter" id="S5.SS0.SSS0.Px5.p1.1.1">kitaab-pustak (hi-mr)</span>,
resulting in non-identical subword equivalences. These phenomena are often unpredictable and unsystematic; even if not, they are not trivial to model into tokenization or architectural strategies for MT.

<br class="ltx_break"/>
<br class="ltx_break"/>See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A2" title="Appendix B Minor Variations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">B</span></a> for experiments with minor variants of our approach dealing with pretrained encoder/decoder initialization, tokenizer size, choice of attention head, and identical source-target settings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Challenges for LR NMT</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Incorporating knowledge of linguistic relationships among closely related data-imbalanced language pairs offers a natural strategy for mitigating data scarcity in mainstream NMT between regional languages, and it is crucial to understand the challenges in this realm. We show that while the PGN mechanism offers an intuitive theoretical shortcut for translation between closely related languages, its performance in practice is limited, potentially by the combined effect of noisy real-world datasets containing non-literal translations, the behaviour of standard tokenizers, as well as linguistic complexities beyond simplified ideas of shared vocabulary and cognates. These are inevitable hurdles to any project that attempts to use structural linguistic knowledge to benefit NMT performance.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Further, we show that despite showing benefits in certain settings over the entire test set, the <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.1">PGN</span> mechanism does not perform as expected on target phenomena. The generalization mechanisms of blackbox Transformer-models are not well understood and may not be easily guided by linguistic intuition: we underline the importance of verifying that improvements are coming from the intended places rather than good starts or extra parameters.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Finally, our analysis hints that <span class="ltx_text ltx_font_smallcaps" id="S6.p3.1.1">PGN</span>-like shortcuts may not be worth offering in the first place: “easy” equivalences, a natural target of linguistic interventions, may not be the bottleneck for LR NMT. Instead, it is more likely that the true bottlenecks are handling precisely the above challenges, i.e. non-systematic differences, one-off phenomena, and real-world noise in low-resource conditions.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While we show that our particular flavour of NMT incorporating PGNs does not provide fundamental benefits for low-resource NMT, this is naturally not to say that an improved variant of this idea would not work better. There are several potential ways forward arising from our discussion of the reasons for the failure of our method in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5" title="5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">5</span></a>: for example, using morphological segmentation for tokenization to increase subword overlap, or using priors for <math alttext="p_{copy}" class="ltx_Math" display="inline" id="S7.p1.1.m1.1"><semantics id="S7.p1.1.m1.1a"><msub id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml"><mi id="S7.p1.1.m1.1.1.2" xref="S7.p1.1.m1.1.1.2.cmml">p</mi><mrow id="S7.p1.1.m1.1.1.3" xref="S7.p1.1.m1.1.1.3.cmml"><mi id="S7.p1.1.m1.1.1.3.2" xref="S7.p1.1.m1.1.1.3.2.cmml">c</mi><mo id="S7.p1.1.m1.1.1.3.1" xref="S7.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S7.p1.1.m1.1.1.3.3" xref="S7.p1.1.m1.1.1.3.3.cmml">o</mi><mo id="S7.p1.1.m1.1.1.3.1a" xref="S7.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S7.p1.1.m1.1.1.3.4" xref="S7.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="S7.p1.1.m1.1.1.3.1b" xref="S7.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S7.p1.1.m1.1.1.3.5" xref="S7.p1.1.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><apply id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.p1.1.m1.1.1.1.cmml" xref="S7.p1.1.m1.1.1">subscript</csymbol><ci id="S7.p1.1.m1.1.1.2.cmml" xref="S7.p1.1.m1.1.1.2">𝑝</ci><apply id="S7.p1.1.m1.1.1.3.cmml" xref="S7.p1.1.m1.1.1.3"><times id="S7.p1.1.m1.1.1.3.1.cmml" xref="S7.p1.1.m1.1.1.3.1"></times><ci id="S7.p1.1.m1.1.1.3.2.cmml" xref="S7.p1.1.m1.1.1.3.2">𝑐</ci><ci id="S7.p1.1.m1.1.1.3.3.cmml" xref="S7.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S7.p1.1.m1.1.1.3.4.cmml" xref="S7.p1.1.m1.1.1.3.4">𝑝</ci><ci id="S7.p1.1.m1.1.1.3.5.cmml" xref="S7.p1.1.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="S7.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> so that it is encouraged for shared subwords. Previous work provides different kinds of help to the copier: for example, <cite class="ltx_cite ltx_citemacro_citep">(Gulcehre et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib12" title="">2016</a>)</cite> explicitly train the copier to copy unknown words with a separate training objective. However, as we mention in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S1" title="1 Introduction and Motivation ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">1</span></a>, our motivation lay in designing a simple architectural mechanism that can be easily integrated into mainstream (multilingual) NMT pipelines to make them more capable for low-resource MT, without requiring much additional language-pair specific attention to training paradigms or tools such as morphological analysers and bilingual lexicons, which are in any case of poor quality for low-resource languages. We restrict our negative result to the scope set up by this motivation.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Further, our results are limited to the <math alttext="6" class="ltx_Math" display="inline" id="S7.p2.1.m1.1"><semantics id="S7.p2.1.m1.1a"><mn id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><cn id="S7.p2.1.m1.1.1.cmml" type="integer" xref="S7.p2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S7.p2.1.m1.1d">6</annotation></semantics></math> language pairs that we experimented on. While we simulate identical low-resource conditions for all our language pairs, we clearly see the difference in absolute performances on <span class="ltx_text ltx_font_typewriter" id="S7.p2.1.1">hi-mr</span> or <span class="ltx_text ltx_font_typewriter" id="S7.p2.1.2">hi-bh</span> as compared to the high-resource language pairs: the data for the latter are simply of much better quality. This demonstrates the need to experiment and present further results on non-simulated truly low-resource conditions, such as the hi-bh language pair studied here. Finally, this discussion is only relevant to translation between closely-related languages that share a script (although this is the predominant case), allowing for lexical similarity to be reflected by shared subwords.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this work, we investigate the applicability of Pointer-Generator Networks in NMT, hypothesizing that an explicit copy mechanism will provide benefits for low-resource translation between closely related languages. We show that while we do observe weak improvements, these are not higher for closer-related languages, sentence pairs with higher overlap, or lower resource ranges, contrary to intuition. Our discussion of potential reasons for the failure of this approach highlights several general challenges for low-resource NMT, such as mainstream tokenization strategies, noisy data, and non-systematic linguistic differences.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ataman and Federico (2018)</span>
<span class="ltx_bibblock">
Duygu Ataman and Marcello Federico. 2018.

</span>
<span class="ltx_bibblock">An evaluation of two vocabulary reduction methods for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)</em>, pages 97–110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bafna et al. (2022)</span>
<span class="ltx_bibblock">
Niyati Bafna, Josef van Genabith, Cristina España-Bonet, and Zdeněk Žabokrtský. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.conll-1.9" title="">Combining noisy semantic signals with orthographic cues: Cognate induction for the Indic dialect continuum</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)</em>, pages 110–131, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cañete et al. (2020)</span>
<span class="ltx_bibblock">
José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge Pérez. 2020.

</span>
<span class="ltx_bibblock">Spanish pre-trained bert model and evaluation data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">PML4DC at ICLR 2020</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Fazio (2021)</span>
<span class="ltx_bibblock">
William Chen and Brett Fazio. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.mtsummit-loresmt.3" title="">Morphologically-guided segmentation for translation of agglutinative low-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)</em>, pages 20–31, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng and Lapata (2016)</span>
<span class="ltx_bibblock">
Jianpeng Cheng and Mirella Lapata. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1046" title="">Neural summarization by extracting sentences and words</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 484–494, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2020)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.367" title="">Improving multilingual models with language-clustered vocabularies</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 4536–4546, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creutz and Lagus (2007)</span>
<span class="ltx_bibblock">
Mathias Creutz and Krista Lagus. 2007.

</span>
<span class="ltx_bibblock">Unsupervised models for morpheme segmentation and morphology learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ACM Transactions on Speech and Language Processing (TSLP)</em>, 4(1):1–34.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Domingo et al. (2019)</span>
<span class="ltx_bibblock">
Miguel Domingo, Mercedes Garcıa-Martınez, Alexandre Helle, Francisco Casacuberta, and Manuel Herranz. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1812.08621" title="">How much does tokenization affect neural machine translation?</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Forcada et al. (2011)</span>
<span class="ltx_bibblock">
Mikel L Forcada, Mireia Ginestí-Rosell, Jacob Nordfalk, Jim O’Regan, Sergio Ortiz-Rojas, Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez, Gema Ramírez-Sánchez, and Francis M Tyers. 2011.

</span>
<span class="ltx_bibblock">Apertium: a free/open-source platform for rule-based machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Machine Translation</em>, 25(2):127–144.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2016)</span>
<span class="ltx_bibblock">
Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016.

</span>
<span class="ltx_bibblock">Incorporating copying mechanism in sequence-to-sequence learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1603.06393</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulcehre et al. (2016)</span>
<span class="ltx_bibblock">
Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016.

</span>
<span class="ltx_bibblock">Pointing the unknown words.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:1603.08148</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddow et al. (2022)</span>
<span class="ltx_bibblock">
Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindřich Helcl, and Alexandra Birch. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli_a_00446" title="">Survey of low-resource machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Computational Linguistics</em>, 48(3):673–732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heffernan et al. (2022)</span>
<span class="ltx_bibblock">
Kevin Heffernan, Onur Çelebi, and Holger Schwenk. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.12654" title="">Bitext mining using distilled sentence representations for low-resource languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2020)</span>
<span class="ltx_bibblock">
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.560" title="">The state and fate of linguistic diversity and inclusion in the NLP world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 6282–6293, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi (2022)</span>
<span class="ltx_bibblock">
Raviraj Joshi. 2022.

</span>
<span class="ltx_bibblock">L3cube-hindbert and devbert: Pre-trained bert transformer models for devanagari based hindi and marathi languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2211.11418</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanna et al. (2021)</span>
<span class="ltx_bibblock">
Tanmai Khanna, Jonathan N Washington, Francis M Tyers, Sevilay Bayatlı, Daniel G Swanson, Tommi A Pirinen, Irene Tang, and Hèctor Alòs i Font. 2021.

</span>
<span class="ltx_bibblock">Recent advances in apertium, a free/open-source rule-based machine translation platform for low-resource languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Machine Translation</em>, 35(4):475–502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2020)</span>
<span class="ltx_bibblock">
Yunsu Kim, Miguel Graça, and Hermann Ney. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.5" title="">When and why is unsupervised neural machine translation useless?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 35–44, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2005.mtsummit-papers.11" title="">Europarl: A parallel corpus for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of Machine Translation Summit X: Papers</em>, pages 79–86, Phuket, Thailand.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Knowles (2017)</span>
<span class="ltx_bibblock">
Philipp Koehn and Rebecca Knowles. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-3204" title="">Six challenges for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the First Workshop on Neural Machine Translation</em>, pages 28–39, Vancouver. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2023)</span>
<span class="ltx_bibblock">
Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.813" title="">Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, page 13142–13152, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Macháček et al. (2018)</span>
<span class="ltx_bibblock">
Dominik Macháček, Jonáš Vidra, and Ondřej Bojar. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1806.05482" title="">Morphological and language-agnostic word segmentation for nmt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mielke et al. (2021)</span>
<span class="ltx_bibblock">
Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, and Samson Tan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.10508" title="">Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mundotiya et al. (2021)</span>
<span class="ltx_bibblock">
Rajesh Kumar Mundotiya, Manish Kumar Singh, Rahul Kapur, Swasti Mishra, and Anil Kumar Singh. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3458250" title="">Linguistic resources for bhojpuri, magahi, and maithili: Statistics about them, their similarity estimates, and baselines for three applications</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ACM Trans. Asian Low-Resour. Lang. Inf. Process.</em>, 20(6).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortega et al. (2020)</span>
<span class="ltx_bibblock">
John E Ortega, Richard Castro Mamani, and Kyunghyun Cho. 2020.

</span>
<span class="ltx_bibblock">Neural machine translation with a polysynthetic low resource language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Machine Translation</em>, 34(4):325–346.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2020)</span>
<span class="ltx_bibblock">
Yirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2001.01589" title="">Morphological word segmentation on agglutinative languages for neural machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philip et al. (2021)</span>
<span class="ltx_bibblock">
Jerin Philip, Shashank Siripragada, Vinay P. Namboodiri, and C. V. Jawahar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3430984.3431026" title="">Revisiting low resource status of indian languages in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)</em>, page 178–187, Bangalore India. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prabhu and Kann (2020)</span>
<span class="ltx_bibblock">
Nikhil Prabhu and Katharina Kann. 2020.

</span>
<span class="ltx_bibblock">Making a point: Pointer-generator transformers for disjoint vocabularies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop</em>, pages 85–92.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2019)</span>
<span class="ltx_bibblock">
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. 2019.

</span>
<span class="ltx_bibblock">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:1907.05791</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2021)</span>
<span class="ltx_bibblock">
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.507" title="">CCMatrix: Mining billions of high-quality parallel sentences on the web</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6490–6500, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et al. (2017)</span>
<span class="ltx_bibblock">
Abigail See, Peter J Liu, and Christopher D Manning. 2017.

</span>
<span class="ltx_bibblock">Get to the point: Summarization with pointer-generator networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:1704.04368</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sengupta and Saha (2015)</span>
<span class="ltx_bibblock">
Debapriya Sengupta and Goutam Saha. 2015.

</span>
<span class="ltx_bibblock">Study on similarity among indian languages using language verification framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Artificial Intelligence</em>, 2015:2–2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich and Zhang (2019)</span>
<span class="ltx_bibblock">
Rico Sennrich and Biao Zhang. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1021" title="">Revisiting low-resource neural machine translation: A case study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 211–221, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf" title="">Parallel data, tools and interfaces in OPUS</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12)</em>, pages 2214–2218, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinyals et al. (2015)</span>
<span class="ltx_bibblock">
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015.

</span>
<span class="ltx_bibblock">Pointer networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Tong Zhang, Long Zhang, Wei Ye, Bo Li, Jinan Sun, Xiaoyu Zhu, Wen Zhao, and Shikun Zhang. 2021.

</span>
<span class="ltx_bibblock">Point, disambiguate and copy: Incorporating bilingual dictionaries for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 3970–3979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2021)</span>
<span class="ltx_bibblock">
Bo Zheng, Li Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.257" title="">Allocating large vocabulary capacity for cross-lingual language model pre-training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3203–3215, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Notes on Datasets</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The <span class="ltx_text ltx_font_bold" id="A1.p1.1.1">Hindi-Marathi</span> WikiMatrix dataset <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib30" title="">2021</a>)</cite> has only 11k sentences so we use CVIT-PIB <cite class="ltx_cite ltx_citemacro_citep">(Philip et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib27" title="">2021</a>)</cite> instead. The CVIT-PIB corpus is automatically aligned using an iterative process that depends on neural machine translation into a pivot language and filtering heuristics. Eyeballing the data, we observe a considerable number of non-parallel or even entirely unrelated sentences simply containing some words in common - in general, this corpus is much more likely to contain rough paraphrases as opposed to literal translations.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For <span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Hindi-Bhojpuri</span>, NLLB seems to be the only available parallel text data for now <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib35" title="">2012</a>)</cite>; this corpus has also been automatically crawled <cite class="ltx_cite ltx_citemacro_citep">(Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib34" title="">2022</a>)</cite>. The Hindi-Bhojpuri NLLB dataset contains extremely short sentences as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.T1" title="Table 1 ‣ Datasets and languages ‣ 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">1</span></a>; similarly to above, we observe a high level of noise and non-parallel data. Such datasets naturally do not provide the most favourable training conditions for the <span class="ltx_text ltx_font_smallcaps" id="A1.p2.1.2">PGN</span> models, which rely on literal translations containing shared subwords to teach the copier; however, they are realistic real-word conditions for truly low-resource languages, as we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S6" title="6 Challenges for LR NMT ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">The WikiMatrix dataset <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib29" title="">2019</a>)</cite>, which we use for <span class="ltx_text ltx_font_bold" id="A1.p3.1.1">Spanish-English</span>, <span class="ltx_text ltx_font_bold" id="A1.p3.1.2">Spanish-Catalan</span>, <span class="ltx_text ltx_font_bold" id="A1.p3.1.3">French-German</span>, and <span class="ltx_text ltx_font_bold" id="A1.p3.1.4">French-Occitan</span> is automatically aligned from Wikipedia content in these languages.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">The <span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Spanish-Catalan</span> synthetic Europarl bitext is created by automatically translating the Europarl dataset <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib19" title="">2005</a>)</cite> into Catalan using Apertium <cite class="ltx_cite ltx_citemacro_citep">(Forcada et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib10" title="">2011</a>; Khanna et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib17" title="">2021</a>)</cite>. While this data probably contains some noise due to MT errors and translationese, it’s the most likely of all our datasets to contain literal, linear translations, and we include it as a testbed for this purpose. It is a generally easier dataset - this is clearly visible from spBLEU scores that our models achieve on it in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.T2" title="Table 2 ‣ Improvement patterns ‣ 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Minor Variations</h2>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Pretrained encoder and decoder</h3>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.2">We tried using a pretrained encoder and decoder at initialization of our model and tested this for <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px1.p1.2.1">hi-mr</span> and <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px1.p1.2.2">es-ca</span>. For the former, the encoder and decoder were initialized with Hindi BERT <cite class="ltx_cite ltx_citemacro_citep">(Joshi, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib16" title="">2022</a>)</cite>, and for the latter, we used Spanish BERT <cite class="ltx_cite ltx_citemacro_citep">(Cañete et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib3" title="">2020</a>)</cite>. These pre-trained models are language-specific instances of BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#bib.bib8" title="">2018</a>)</cite>. Note that this means that we also used the pretrained tokenizers of these models, of sizes <math alttext="52000" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="A2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">52000</mn><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.1.m1.1b"><cn id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1">52000</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.1.m1.1c">52000</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.1.m1.1d">52000</annotation></semantics></math> and <math alttext="31002" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="A2.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="A2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">31002</mn><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.2.m2.1b"><cn id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1">31002</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.2.m2.1c">31002</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px1.p1.2.m2.1d">31002</annotation></semantics></math> for Hindi and Spanish respectively, that are only trained on the high-resource source languages; this leads to very poor tokenization in the target language. In general, this set of models take longer to converge due to their size, and show only minor differences in performance. Another related idea is to finetune <span class="ltx_text ltx_font_smallcaps" id="A2.SS0.SSS0.Px1.p1.2.3">NLLB</span> or another multilingual MT model with an incorporated PGN; we did not try this given the lack of encouraging results from these experiments.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Single attention head</h3>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">We also tried using only a single attention head to calculate <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="A2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">p</mi><mrow id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.2" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">c</mi><mo id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.3" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">o</mi><mo id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1a" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.4" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1b" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.5" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.2">𝑝</ci><apply id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3"><times id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.1"></times><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.2">𝑐</ci><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.3">𝑜</ci><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.4.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.4">𝑝</ci><ci id="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.5.cmml" xref="A2.SS0.SSS0.Px2.p1.1.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p1.1.m1.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px2.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> for target tokens, with the motivation that it was maybe better to nudge a single head to encode information about whether target token need to be copied, and leaving other heads to generate, as opposed to asking all heads to do both (which is the case when we average over heads). However, these models give almost identical results as in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5.T2" title="Table 2 ‣ Improvement patterns ‣ 5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Smaller tokenizer</h3>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.4">We hypothesize that using a smaller tokenizer size will force more splits per token, increasing the chance that common stems will be reflected in shared subwords. Accordingly, we tried a tokenizer size of <math alttext="8000" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px3.p1.1.m1.1a"><mn id="A2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">8000</mn><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.1.m1.1b"><cn id="A2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px3.p1.1.m1.1.1">8000</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.1.m1.1c">8000</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.1.m1.1d">8000</annotation></semantics></math> for <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p1.4.1">hi-mr</span> and <span class="ltx_text ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p1.4.2">es-ca</span> for the <math alttext="15k" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="A2.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="A2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">15</mn><mo id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1"><times id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.1"></times><cn id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.2">15</cn><ci id="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.2.m2.1c">15k</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.2.m2.1d">15 italic_k</annotation></semantics></math> and <math alttext="60k" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="A2.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="A2.SS0.SSS0.Px3.p1.3.m3.1.1" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">60</mn><mo id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1"><times id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.1"></times><cn id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.2">60</cn><ci id="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="A2.SS0.SSS0.Px3.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.3.m3.1c">60k</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.3.m3.1d">60 italic_k</annotation></semantics></math> settings; however, performance degrades slightly (about <math alttext="-1" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px3.p1.4.m4.1"><semantics id="A2.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="A2.SS0.SSS0.Px3.p1.4.m4.1.1" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mo id="A2.SS0.SSS0.Px3.p1.4.m4.1.1a" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml">−</mo><mn id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1"><minus id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1"></minus><cn id="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px3.p1.4.m4.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px3.p1.4.m4.1c">-1</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px3.p1.4.m4.1d">- 1</annotation></semantics></math> spBLEU on average) for both <span class="ltx_text ltx_font_smallcaps" id="A2.SS0.SSS0.Px3.p1.4.3">NMT</span> and <span class="ltx_text ltx_font_smallcaps" id="A2.SS0.SSS0.Px3.p1.4.4">PGN</span> approaches without affecting the relative trend.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p2.1">This is not altogether surprising: reducing tokenizer size only increases the degree of splitting in words of a certain (lower) frequency range, rather than affecting the number of splits for all words uniformly. More importantly, while these hyperparameters are important to tune, statistical frequency-based tokenizers behave inherently differently from morphologically-inspired tokenizers, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5" title="5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">5</span></a>, and it is not easy or perhaps possible to achieve a good approximation of the latter by playing with the hyperparameters of the former.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Identical source and target</h3>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.2">Finally, we also trained a Hindi-Hindi model, to remove the effects of noisy translations and non-ideal tokenization of source and target token sequences as discussed in Sections <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.SS0.SSS0.Px1" title="Datasets and languages ‣ 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S5" title="5 Results and Discussion ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">5</span></a>. In this setup, with <math alttext="100\%" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="A2.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="A2.SS0.SSS0.Px4.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">100</mn><mo id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="latexml" id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.1">percent</csymbol><cn id="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px4.p1.1.m1.1c">100\%</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px4.p1.1.m1.1d">100 %</annotation></semantics></math> overlap, the models achieve high test scores (<math alttext="74" class="ltx_Math" display="inline" id="A2.SS0.SSS0.Px4.p1.2.m2.1"><semantics id="A2.SS0.SSS0.Px4.p1.2.m2.1a"><mn id="A2.SS0.SSS0.Px4.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">74</mn><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px4.p1.2.m2.1b"><cn id="A2.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" type="integer" xref="A2.SS0.SSS0.Px4.p1.2.m2.1.1">74</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px4.p1.2.m2.1c">74</annotation><annotation encoding="application/x-llamapun" id="A2.SS0.SSS0.Px4.p1.2.m2.1d">74</annotation></semantics></math> spBLEU) and converge to near-zero usage of the copy mechanism. Clearly, the model still prefers to encode the identity relationship using a generate mechanism.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p2.1">Note that this setup is fundamentally different from our other scenarios - when all tokens are copied, the model no longer needs the distinction between two distinct processes (generating and copying), and therefore does not really need to learn how to make this decision. However, it is still illustrative in demonstrating that model generalization mechanisms, even for highly simplified or trivial tasks, are often not intuitive or human-interpretable.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Visualizations</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.2">See Figures <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3.F2" title="Figure 2 ‣ Appendix C Visualizations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3.F3" title="Figure 3 ‣ Appendix C Visualizations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3.F4" title="Figure 4 ‣ Appendix C Visualizations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A3.F5" title="Figure 5 ‣ Appendix C Visualizations ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">5</span></a> for visualizations of the <span class="ltx_text ltx_font_smallcaps" id="A3.p1.2.1">PGN</span> model’s cross-attention distributions and values of <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><msub id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml"><mi id="A3.p1.1.m1.1.1.2" xref="A3.p1.1.m1.1.1.2.cmml">p</mi><mrow id="A3.p1.1.m1.1.1.3" xref="A3.p1.1.m1.1.1.3.cmml"><mi id="A3.p1.1.m1.1.1.3.2" xref="A3.p1.1.m1.1.1.3.2.cmml">c</mi><mo id="A3.p1.1.m1.1.1.3.1" xref="A3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.1.m1.1.1.3.3" xref="A3.p1.1.m1.1.1.3.3.cmml">o</mi><mo id="A3.p1.1.m1.1.1.3.1a" xref="A3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.1.m1.1.1.3.4" xref="A3.p1.1.m1.1.1.3.4.cmml">p</mi><mo id="A3.p1.1.m1.1.1.3.1b" xref="A3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.1.m1.1.1.3.5" xref="A3.p1.1.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><apply id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p1.1.m1.1.1.1.cmml" xref="A3.p1.1.m1.1.1">subscript</csymbol><ci id="A3.p1.1.m1.1.1.2.cmml" xref="A3.p1.1.m1.1.1.2">𝑝</ci><apply id="A3.p1.1.m1.1.1.3.cmml" xref="A3.p1.1.m1.1.1.3"><times id="A3.p1.1.m1.1.1.3.1.cmml" xref="A3.p1.1.m1.1.1.3.1"></times><ci id="A3.p1.1.m1.1.1.3.2.cmml" xref="A3.p1.1.m1.1.1.3.2">𝑐</ci><ci id="A3.p1.1.m1.1.1.3.3.cmml" xref="A3.p1.1.m1.1.1.3.3">𝑜</ci><ci id="A3.p1.1.m1.1.1.3.4.cmml" xref="A3.p1.1.m1.1.1.3.4">𝑝</ci><ci id="A3.p1.1.m1.1.1.3.5.cmml" xref="A3.p1.1.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> per target token on randomly chosen source-target pairs using an early and late model training checkpoint. We observe that the model does use the copying mechanism as intended in many places, for common subwords (<span class="ltx_text ltx_font_typewriter" id="A3.p1.2.2">udaar</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.3">hi-mr</span>, <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.4">un</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.5">es-ca</span>) as well as named entities (<span class="ltx_text ltx_font_typewriter" id="A3.p1.2.6">Cour</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.7">fr-oc</span>), common borrowings (<span class="ltx_text ltx_font_typewriter" id="A3.p1.2.8">computer</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.9">hi-bh</span>), numbers (<span class="ltx_text ltx_font_typewriter" id="A3.p1.2.10">1970</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.11">fr-oc</span>) and punctuation. However, <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><msub id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml"><mi id="A3.p1.2.m2.1.1.2" xref="A3.p1.2.m2.1.1.2.cmml">p</mi><mrow id="A3.p1.2.m2.1.1.3" xref="A3.p1.2.m2.1.1.3.cmml"><mi id="A3.p1.2.m2.1.1.3.2" xref="A3.p1.2.m2.1.1.3.2.cmml">c</mi><mo id="A3.p1.2.m2.1.1.3.1" xref="A3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.2.m2.1.1.3.3" xref="A3.p1.2.m2.1.1.3.3.cmml">o</mi><mo id="A3.p1.2.m2.1.1.3.1a" xref="A3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.2.m2.1.1.3.4" xref="A3.p1.2.m2.1.1.3.4.cmml">p</mi><mo id="A3.p1.2.m2.1.1.3.1b" xref="A3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.p1.2.m2.1.1.3.5" xref="A3.p1.2.m2.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><apply id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p1.2.m2.1.1.1.cmml" xref="A3.p1.2.m2.1.1">subscript</csymbol><ci id="A3.p1.2.m2.1.1.2.cmml" xref="A3.p1.2.m2.1.1.2">𝑝</ci><apply id="A3.p1.2.m2.1.1.3.cmml" xref="A3.p1.2.m2.1.1.3"><times id="A3.p1.2.m2.1.1.3.1.cmml" xref="A3.p1.2.m2.1.1.3.1"></times><ci id="A3.p1.2.m2.1.1.3.2.cmml" xref="A3.p1.2.m2.1.1.3.2">𝑐</ci><ci id="A3.p1.2.m2.1.1.3.3.cmml" xref="A3.p1.2.m2.1.1.3.3">𝑜</ci><ci id="A3.p1.2.m2.1.1.3.4.cmml" xref="A3.p1.2.m2.1.1.3.4">𝑝</ci><ci id="A3.p1.2.m2.1.1.3.5.cmml" xref="A3.p1.2.m2.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values are also relatively high sometimes for other seemingly random target tokens, e.g. <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.12">canton-costat</span> in <span class="ltx_text ltx_font_typewriter" id="A3.p1.2.13">fr-oc</span>.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">Note that the <span class="ltx_text ltx_font_typewriter" id="A3.p2.1.1">hi-bh</span> source-target sentence pairs are not in fact translations of each other and exemplify the noise we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#S4.SS0.SSS0.Px1" title="Datasets and languages ‣ 4 Experiments ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.10963v3#A1" title="Appendix A Notes on Datasets ‣ Pointer-Generator Networks for Low-Resource Machine Translation: Don’t Copy That!"><span class="ltx_text ltx_ref_tag">A</span></a>. The cross-attention distributions for the <span class="ltx_text ltx_font_typewriter" id="A3.p2.1.2">es-ca</span> and <span class="ltx_text ltx_font_typewriter" id="A3.p2.1.3">fr-oc</span> models are in general much better defined and able to attend to appropriate tokens (in these example visualizations as well as others that we looked at); this is a consequence of the better quality of the data and models in these languages.</p>
</div>
<figure class="ltx_figure" id="A3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="310" id="A3.F2.sf1.g1" src="extracted/5674423/visualizations/es-ca-60k-epoch10.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Epoch 10</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="317" id="A3.F2.sf2.g1" src="extracted/5674423/visualizations/es-ca-60k-epoch30.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Epoch 30</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model’s cross-attention distributions and <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.F2.3.m1.1"><semantics id="A3.F2.3.m1.1b"><msub id="A3.F2.3.m1.1.1" xref="A3.F2.3.m1.1.1.cmml"><mi id="A3.F2.3.m1.1.1.2" xref="A3.F2.3.m1.1.1.2.cmml">p</mi><mrow id="A3.F2.3.m1.1.1.3" xref="A3.F2.3.m1.1.1.3.cmml"><mi id="A3.F2.3.m1.1.1.3.2" xref="A3.F2.3.m1.1.1.3.2.cmml">c</mi><mo id="A3.F2.3.m1.1.1.3.1" xref="A3.F2.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F2.3.m1.1.1.3.3" xref="A3.F2.3.m1.1.1.3.3.cmml">o</mi><mo id="A3.F2.3.m1.1.1.3.1b" xref="A3.F2.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F2.3.m1.1.1.3.4" xref="A3.F2.3.m1.1.1.3.4.cmml">p</mi><mo id="A3.F2.3.m1.1.1.3.1c" xref="A3.F2.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F2.3.m1.1.1.3.5" xref="A3.F2.3.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.F2.3.m1.1c"><apply id="A3.F2.3.m1.1.1.cmml" xref="A3.F2.3.m1.1.1"><csymbol cd="ambiguous" id="A3.F2.3.m1.1.1.1.cmml" xref="A3.F2.3.m1.1.1">subscript</csymbol><ci id="A3.F2.3.m1.1.1.2.cmml" xref="A3.F2.3.m1.1.1.2">𝑝</ci><apply id="A3.F2.3.m1.1.1.3.cmml" xref="A3.F2.3.m1.1.1.3"><times id="A3.F2.3.m1.1.1.3.1.cmml" xref="A3.F2.3.m1.1.1.3.1"></times><ci id="A3.F2.3.m1.1.1.3.2.cmml" xref="A3.F2.3.m1.1.1.3.2">𝑐</ci><ci id="A3.F2.3.m1.1.1.3.3.cmml" xref="A3.F2.3.m1.1.1.3.3">𝑜</ci><ci id="A3.F2.3.m1.1.1.3.4.cmml" xref="A3.F2.3.m1.1.1.3.4">𝑝</ci><ci id="A3.F2.3.m1.1.1.3.5.cmml" xref="A3.F2.3.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F2.3.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.F2.3.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values for two sentence pairs for <span class="ltx_text ltx_font_typewriter" id="A3.F2.6.1">es-ca(ep)</span>, <math alttext="60k" class="ltx_Math" display="inline" id="A3.F2.4.m2.1"><semantics id="A3.F2.4.m2.1b"><mrow id="A3.F2.4.m2.1.1" xref="A3.F2.4.m2.1.1.cmml"><mn id="A3.F2.4.m2.1.1.2" xref="A3.F2.4.m2.1.1.2.cmml">60</mn><mo id="A3.F2.4.m2.1.1.1" xref="A3.F2.4.m2.1.1.1.cmml">⁢</mo><mi id="A3.F2.4.m2.1.1.3" xref="A3.F2.4.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.F2.4.m2.1c"><apply id="A3.F2.4.m2.1.1.cmml" xref="A3.F2.4.m2.1.1"><times id="A3.F2.4.m2.1.1.1.cmml" xref="A3.F2.4.m2.1.1.1"></times><cn id="A3.F2.4.m2.1.1.2.cmml" type="integer" xref="A3.F2.4.m2.1.1.2">60</cn><ci id="A3.F2.4.m2.1.1.3.cmml" xref="A3.F2.4.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F2.4.m2.1d">60k</annotation><annotation encoding="application/x-llamapun" id="A3.F2.4.m2.1e">60 italic_k</annotation></semantics></math> sentences</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="312" id="A3.F3.sf1.g1" src="extracted/5674423/visualizations/fr-oc-15k,epoch10.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Epoch 10</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="314" id="A3.F3.sf2.g1" src="extracted/5674423/visualizations/fr-oc-60k-epoch30.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Epoch 30</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model’s cross-attention distributions and <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.F3.3.m1.1"><semantics id="A3.F3.3.m1.1b"><msub id="A3.F3.3.m1.1.1" xref="A3.F3.3.m1.1.1.cmml"><mi id="A3.F3.3.m1.1.1.2" xref="A3.F3.3.m1.1.1.2.cmml">p</mi><mrow id="A3.F3.3.m1.1.1.3" xref="A3.F3.3.m1.1.1.3.cmml"><mi id="A3.F3.3.m1.1.1.3.2" xref="A3.F3.3.m1.1.1.3.2.cmml">c</mi><mo id="A3.F3.3.m1.1.1.3.1" xref="A3.F3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F3.3.m1.1.1.3.3" xref="A3.F3.3.m1.1.1.3.3.cmml">o</mi><mo id="A3.F3.3.m1.1.1.3.1b" xref="A3.F3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F3.3.m1.1.1.3.4" xref="A3.F3.3.m1.1.1.3.4.cmml">p</mi><mo id="A3.F3.3.m1.1.1.3.1c" xref="A3.F3.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F3.3.m1.1.1.3.5" xref="A3.F3.3.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.F3.3.m1.1c"><apply id="A3.F3.3.m1.1.1.cmml" xref="A3.F3.3.m1.1.1"><csymbol cd="ambiguous" id="A3.F3.3.m1.1.1.1.cmml" xref="A3.F3.3.m1.1.1">subscript</csymbol><ci id="A3.F3.3.m1.1.1.2.cmml" xref="A3.F3.3.m1.1.1.2">𝑝</ci><apply id="A3.F3.3.m1.1.1.3.cmml" xref="A3.F3.3.m1.1.1.3"><times id="A3.F3.3.m1.1.1.3.1.cmml" xref="A3.F3.3.m1.1.1.3.1"></times><ci id="A3.F3.3.m1.1.1.3.2.cmml" xref="A3.F3.3.m1.1.1.3.2">𝑐</ci><ci id="A3.F3.3.m1.1.1.3.3.cmml" xref="A3.F3.3.m1.1.1.3.3">𝑜</ci><ci id="A3.F3.3.m1.1.1.3.4.cmml" xref="A3.F3.3.m1.1.1.3.4">𝑝</ci><ci id="A3.F3.3.m1.1.1.3.5.cmml" xref="A3.F3.3.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F3.3.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.F3.3.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values for two sentence pairs for <span class="ltx_text ltx_font_typewriter" id="A3.F3.6.1">fr-oc</span>, <math alttext="60k" class="ltx_Math" display="inline" id="A3.F3.4.m2.1"><semantics id="A3.F3.4.m2.1b"><mrow id="A3.F3.4.m2.1.1" xref="A3.F3.4.m2.1.1.cmml"><mn id="A3.F3.4.m2.1.1.2" xref="A3.F3.4.m2.1.1.2.cmml">60</mn><mo id="A3.F3.4.m2.1.1.1" xref="A3.F3.4.m2.1.1.1.cmml">⁢</mo><mi id="A3.F3.4.m2.1.1.3" xref="A3.F3.4.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.F3.4.m2.1c"><apply id="A3.F3.4.m2.1.1.cmml" xref="A3.F3.4.m2.1.1"><times id="A3.F3.4.m2.1.1.1.cmml" xref="A3.F3.4.m2.1.1.1"></times><cn id="A3.F3.4.m2.1.1.2.cmml" type="integer" xref="A3.F3.4.m2.1.1.2">60</cn><ci id="A3.F3.4.m2.1.1.3.cmml" xref="A3.F3.4.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F3.4.m2.1d">60k</annotation><annotation encoding="application/x-llamapun" id="A3.F3.4.m2.1e">60 italic_k</annotation></semantics></math> sentences</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="311" id="A3.F4.sf1.g1" src="extracted/5674423/visualizations/hi-mr-60k-epoch10.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Epoch 10</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="309" id="A3.F4.sf2.g1" src="extracted/5674423/visualizations/hi-mr-60k-epoch25.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Epoch 25</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model’s cross-attention distributions and <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.F4.3.m1.1"><semantics id="A3.F4.3.m1.1b"><msub id="A3.F4.3.m1.1.1" xref="A3.F4.3.m1.1.1.cmml"><mi id="A3.F4.3.m1.1.1.2" xref="A3.F4.3.m1.1.1.2.cmml">p</mi><mrow id="A3.F4.3.m1.1.1.3" xref="A3.F4.3.m1.1.1.3.cmml"><mi id="A3.F4.3.m1.1.1.3.2" xref="A3.F4.3.m1.1.1.3.2.cmml">c</mi><mo id="A3.F4.3.m1.1.1.3.1" xref="A3.F4.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F4.3.m1.1.1.3.3" xref="A3.F4.3.m1.1.1.3.3.cmml">o</mi><mo id="A3.F4.3.m1.1.1.3.1b" xref="A3.F4.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F4.3.m1.1.1.3.4" xref="A3.F4.3.m1.1.1.3.4.cmml">p</mi><mo id="A3.F4.3.m1.1.1.3.1c" xref="A3.F4.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F4.3.m1.1.1.3.5" xref="A3.F4.3.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.F4.3.m1.1c"><apply id="A3.F4.3.m1.1.1.cmml" xref="A3.F4.3.m1.1.1"><csymbol cd="ambiguous" id="A3.F4.3.m1.1.1.1.cmml" xref="A3.F4.3.m1.1.1">subscript</csymbol><ci id="A3.F4.3.m1.1.1.2.cmml" xref="A3.F4.3.m1.1.1.2">𝑝</ci><apply id="A3.F4.3.m1.1.1.3.cmml" xref="A3.F4.3.m1.1.1.3"><times id="A3.F4.3.m1.1.1.3.1.cmml" xref="A3.F4.3.m1.1.1.3.1"></times><ci id="A3.F4.3.m1.1.1.3.2.cmml" xref="A3.F4.3.m1.1.1.3.2">𝑐</ci><ci id="A3.F4.3.m1.1.1.3.3.cmml" xref="A3.F4.3.m1.1.1.3.3">𝑜</ci><ci id="A3.F4.3.m1.1.1.3.4.cmml" xref="A3.F4.3.m1.1.1.3.4">𝑝</ci><ci id="A3.F4.3.m1.1.1.3.5.cmml" xref="A3.F4.3.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F4.3.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.F4.3.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values for two sentence pairs for <span class="ltx_text ltx_font_typewriter" id="A3.F4.6.1">hi-mr</span>, <math alttext="60k" class="ltx_Math" display="inline" id="A3.F4.4.m2.1"><semantics id="A3.F4.4.m2.1b"><mrow id="A3.F4.4.m2.1.1" xref="A3.F4.4.m2.1.1.cmml"><mn id="A3.F4.4.m2.1.1.2" xref="A3.F4.4.m2.1.1.2.cmml">60</mn><mo id="A3.F4.4.m2.1.1.1" xref="A3.F4.4.m2.1.1.1.cmml">⁢</mo><mi id="A3.F4.4.m2.1.1.3" xref="A3.F4.4.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.F4.4.m2.1c"><apply id="A3.F4.4.m2.1.1.cmml" xref="A3.F4.4.m2.1.1"><times id="A3.F4.4.m2.1.1.1.cmml" xref="A3.F4.4.m2.1.1.1"></times><cn id="A3.F4.4.m2.1.1.2.cmml" type="integer" xref="A3.F4.4.m2.1.1.2">60</cn><ci id="A3.F4.4.m2.1.1.3.cmml" xref="A3.F4.4.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F4.4.m2.1d">60k</annotation><annotation encoding="application/x-llamapun" id="A3.F4.4.m2.1e">60 italic_k</annotation></semantics></math> sentences</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="309" id="A3.F5.sf1.g1" src="extracted/5674423/visualizations/hi-bh-60k-epoch10.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Epoch 10</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="307" id="A3.F5.sf2.g1" src="extracted/5674423/visualizations/hi-bh-60k-epoch30.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Epoch 30</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Model’s cross-attention distributions and <math alttext="p_{copy}" class="ltx_Math" display="inline" id="A3.F5.3.m1.1"><semantics id="A3.F5.3.m1.1b"><msub id="A3.F5.3.m1.1.1" xref="A3.F5.3.m1.1.1.cmml"><mi id="A3.F5.3.m1.1.1.2" xref="A3.F5.3.m1.1.1.2.cmml">p</mi><mrow id="A3.F5.3.m1.1.1.3" xref="A3.F5.3.m1.1.1.3.cmml"><mi id="A3.F5.3.m1.1.1.3.2" xref="A3.F5.3.m1.1.1.3.2.cmml">c</mi><mo id="A3.F5.3.m1.1.1.3.1" xref="A3.F5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F5.3.m1.1.1.3.3" xref="A3.F5.3.m1.1.1.3.3.cmml">o</mi><mo id="A3.F5.3.m1.1.1.3.1b" xref="A3.F5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F5.3.m1.1.1.3.4" xref="A3.F5.3.m1.1.1.3.4.cmml">p</mi><mo id="A3.F5.3.m1.1.1.3.1c" xref="A3.F5.3.m1.1.1.3.1.cmml">⁢</mo><mi id="A3.F5.3.m1.1.1.3.5" xref="A3.F5.3.m1.1.1.3.5.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.F5.3.m1.1c"><apply id="A3.F5.3.m1.1.1.cmml" xref="A3.F5.3.m1.1.1"><csymbol cd="ambiguous" id="A3.F5.3.m1.1.1.1.cmml" xref="A3.F5.3.m1.1.1">subscript</csymbol><ci id="A3.F5.3.m1.1.1.2.cmml" xref="A3.F5.3.m1.1.1.2">𝑝</ci><apply id="A3.F5.3.m1.1.1.3.cmml" xref="A3.F5.3.m1.1.1.3"><times id="A3.F5.3.m1.1.1.3.1.cmml" xref="A3.F5.3.m1.1.1.3.1"></times><ci id="A3.F5.3.m1.1.1.3.2.cmml" xref="A3.F5.3.m1.1.1.3.2">𝑐</ci><ci id="A3.F5.3.m1.1.1.3.3.cmml" xref="A3.F5.3.m1.1.1.3.3">𝑜</ci><ci id="A3.F5.3.m1.1.1.3.4.cmml" xref="A3.F5.3.m1.1.1.3.4">𝑝</ci><ci id="A3.F5.3.m1.1.1.3.5.cmml" xref="A3.F5.3.m1.1.1.3.5">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F5.3.m1.1d">p_{copy}</annotation><annotation encoding="application/x-llamapun" id="A3.F5.3.m1.1e">italic_p start_POSTSUBSCRIPT italic_c italic_o italic_p italic_y end_POSTSUBSCRIPT</annotation></semantics></math> values for two sentence pairs for <span class="ltx_text ltx_font_typewriter" id="A3.F5.6.1">hi-bh</span>, <math alttext="60k" class="ltx_Math" display="inline" id="A3.F5.4.m2.1"><semantics id="A3.F5.4.m2.1b"><mrow id="A3.F5.4.m2.1.1" xref="A3.F5.4.m2.1.1.cmml"><mn id="A3.F5.4.m2.1.1.2" xref="A3.F5.4.m2.1.1.2.cmml">60</mn><mo id="A3.F5.4.m2.1.1.1" xref="A3.F5.4.m2.1.1.1.cmml">⁢</mo><mi id="A3.F5.4.m2.1.1.3" xref="A3.F5.4.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.F5.4.m2.1c"><apply id="A3.F5.4.m2.1.1.cmml" xref="A3.F5.4.m2.1.1"><times id="A3.F5.4.m2.1.1.1.cmml" xref="A3.F5.4.m2.1.1.1"></times><cn id="A3.F5.4.m2.1.1.2.cmml" type="integer" xref="A3.F5.4.m2.1.1.2">60</cn><ci id="A3.F5.4.m2.1.1.3.cmml" xref="A3.F5.4.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F5.4.m2.1d">60k</annotation><annotation encoding="application/x-llamapun" id="A3.F5.4.m2.1e">60 italic_k</annotation></semantics></math> sentences</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 18 02:56:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
