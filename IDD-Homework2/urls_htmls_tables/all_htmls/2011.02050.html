<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.02050] Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations</title><meta property="og:description" content="Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slot…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.02050">

<!--Generated on Tue Mar 19 04:31:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ke Tran 
<br class="ltx_break">Amazon Translate 
<br class="ltx_break">Berlin, Germany 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">trnke@amazon.com</span> 
<br class="ltx_break">&amp;Ming Tan 
<br class="ltx_break">Amazon Alexa 
<br class="ltx_break">Cambridge, MA, USA 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">mingtan@amazon.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Modern conversational AI systems support natural language understanding for a wide variety of capabilities. While a majority of these tasks can be accomplished using a simple and flat representation of intents and slots, more sophisticated capabilities require complex hierarchical representations supported by semantic parsing. State-of-the-art semantic parsers are trained using supervised learning with data labeled according to a hierarchical schema which might be costly to obtain or not readily available for a new domain. In this work, we explore the possibility of generating synthetic data for neural semantic parsing using a pretrained denoising sequence-to-sequence model (<span id="id3.id1.1" class="ltx_text ltx_font_italic">i</span>.<span id="id3.id1.2" class="ltx_text ltx_font_italic">e</span>., BART). Specifically, we first extract masked <em id="id3.id1.3" class="ltx_emph ltx_font_italic">templates</em> from the existing labeled utterances, and then fine-tune BART to generate synthetic utterances conditioning on the extracted templates. Finally, we use an auxiliary parser (AP) to filter the generated utterances. The AP guarantees the quality of the generated data. We show the potential of our approach when evaluating on the Facebook TOP dataset<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://fb.me/semanticparsingdialog" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://fb.me/semanticparsingdialog</a></span></span></span> for navigation domain.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.7" class="ltx_p">In this work, we investigate semantic parsing with hierarchical representations <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> instead of the traditional logical forms <cite class="ltx_cite ltx_citemacro_citep">(Zettlemoyer and Collins, <a href="#bib.bib12" title="" class="ltx_ref">2005</a>)</cite>. Given an utterance <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S1.p1.1.m1.1a"><mi id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">{\bm{x}}</annotation></semantics></math>, our goal is to produce a tree-structured representation <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S1.p1.2.m2.1a"><mi id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><ci id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">{\bm{y}}</annotation></semantics></math> of the utterance where additional information about intents and slots is introduced at the non-terminal nodes of the tree. We define a <em id="S1.p1.7.1" class="ltx_emph ltx_font_italic">template</em> <math id="S1.p1.3.m3.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S1.p1.3.m3.1a"><mi id="S1.p1.3.m3.1.1" xref="S1.p1.3.m3.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S1.p1.3.m3.1b"><ci id="S1.p1.3.m3.1.1.cmml" xref="S1.p1.3.m3.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.3.m3.1c">{\bm{z}}</annotation></semantics></math> of a given annotation <math id="S1.p1.4.m4.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S1.p1.4.m4.1a"><mi id="S1.p1.4.m4.1.1" xref="S1.p1.4.m4.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S1.p1.4.m4.1b"><ci id="S1.p1.4.m4.1.1.cmml" xref="S1.p1.4.m4.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.4.m4.1c">{\bm{y}}</annotation></semantics></math> as a result of replacing all terminal nodes by a generic <span id="S1.p1.7.2" class="ltx_text ltx_font_typewriter">[mask]</span> node. Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of such an utterance <math id="S1.p1.5.m5.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S1.p1.5.m5.1a"><mi id="S1.p1.5.m5.1.1" xref="S1.p1.5.m5.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S1.p1.5.m5.1b"><ci id="S1.p1.5.m5.1.1.cmml" xref="S1.p1.5.m5.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.5.m5.1c">{\bm{x}}</annotation></semantics></math>, its annotation <math id="S1.p1.6.m6.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S1.p1.6.m6.1a"><mi id="S1.p1.6.m6.1.1" xref="S1.p1.6.m6.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S1.p1.6.m6.1b"><ci id="S1.p1.6.m6.1.1.cmml" xref="S1.p1.6.m6.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.6.m6.1c">{\bm{y}}</annotation></semantics></math> and the corresponding template <math id="S1.p1.7.m7.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S1.p1.7.m7.1a"><mi id="S1.p1.7.m7.1.1" xref="S1.p1.7.m7.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S1.p1.7.m7.1b"><ci id="S1.p1.7.m7.1.1.cmml" xref="S1.p1.7.m7.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.7.m7.1c">{\bm{z}}</annotation></semantics></math>.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.11" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:214.0pt;"><img src="/html/2011.02050/assets/x1.png" id="S1.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="483" height="133" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of an input utterance <math id="S1.F2.7.6.m1.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S1.F2.7.6.m1.1b"><mi id="S1.F2.7.6.m1.1.1" xref="S1.F2.7.6.m1.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S1.F2.7.6.m1.1c"><ci id="S1.F2.7.6.m1.1.1.cmml" xref="S1.F2.7.6.m1.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.7.6.m1.1d">{\bm{x}}</annotation></semantics></math>, its desired output <math id="S1.F2.8.7.m2.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S1.F2.8.7.m2.1b"><mi id="S1.F2.8.7.m2.1.1" xref="S1.F2.8.7.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S1.F2.8.7.m2.1c"><ci id="S1.F2.8.7.m2.1.1.cmml" xref="S1.F2.8.7.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.8.7.m2.1d">{\bm{y}}</annotation></semantics></math>, and the template <math id="S1.F2.9.8.m3.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S1.F2.9.8.m3.1b"><mi id="S1.F2.9.8.m3.1.1" xref="S1.F2.9.8.m3.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S1.F2.9.8.m3.1c"><ci id="S1.F2.9.8.m3.1.1.cmml" xref="S1.F2.9.8.m3.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.9.8.m3.1d">{\bm{z}}</annotation></semantics></math> inferred from <math id="S1.F2.10.9.m4.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S1.F2.10.9.m4.1b"><mi id="S1.F2.10.9.m4.1.1" xref="S1.F2.10.9.m4.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S1.F2.10.9.m4.1c"><ci id="S1.F2.10.9.m4.1.1.cmml" xref="S1.F2.10.9.m4.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.10.9.m4.1d">{\bm{y}}</annotation></semantics></math> . By definition, the template <math id="S1.F2.11.10.m5.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S1.F2.11.10.m5.1b"><mi id="S1.F2.11.10.m5.1.1" xref="S1.F2.11.10.m5.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S1.F2.11.10.m5.1c"><ci id="S1.F2.11.10.m5.1.1.cmml" xref="S1.F2.11.10.m5.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.11.10.m5.1d">{\bm{z}}</annotation></semantics></math> above can be used to generate other utterances such as “<em id="S1.F2.11.13.1" class="ltx_emph ltx_font_italic">how is the 5:00 traffic looking</em>” or “<em id="S1.F2.11.14.2" class="ltx_emph ltx_font_italic">Any construction on my morning route</em>”.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.14" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:214.0pt;"><img src="/html/2011.02050/assets/x2.png" id="S1.F2.12.g1" class="ltx_graphics ltx_img_landscape" width="484" height="322" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Frequency of most 50 common templates in Facebook TOP dataset. The frequency of <math id="S1.F2.14.2.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S1.F2.14.2.m1.1b"><mi id="S1.F2.14.2.m1.1.1" xref="S1.F2.14.2.m1.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S1.F2.14.2.m1.1c"><ci id="S1.F2.14.2.m1.1.1.cmml" xref="S1.F2.14.2.m1.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.14.2.m1.1d">{\bm{z}}</annotation></semantics></math> follows a power-law probability distribution.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The hierarchical representation for task-oriented parsing proposed in <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> aims for ease of annotation and expressiveness. The dataset in their work, Facebook TOP, is the largest publicly available dataset in English for hierarchical semantic parsing. It has more than 44K annotated queries.
We look at the distribution of the templates in Facebook TOP and found that the dataset is highly unbalanced (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The 10 most frequent templates account for 30% of the training data and 14% of the data are singletons, which are utterances with only a single occurrence.
This analysis suggests that it is beneficial to generate more synthetic data for templates with low frequencies. In the field of Natural Language Processing, using synthetic data via back-translation <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> has shown a great success for machine translation <cite class="ltx_cite ltx_citemacro_citep">(Edunov et al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. Unlike machine translation, generating synthetic data for hierarchical semantic parsing is less straightforward. Our work positions itself as one of the first to explore the possibility of generate text from graph (template) for semantic parsing.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a generic framework for augmenting a semantic parser with synthetic data. Our framework consists of two steps.
First, we train a generator, followed by top-<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">p</annotation></semantics></math> sampling to generate diverse synthetic utterances conditioning on the above-mentioned templates. Generated utterances share similar hierarchical structures (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">e</span>., templates) with real training utterances while providing a wide spectrum of lexical variety. Second, we use an auxiliary parser for filtering on the generated candidates. The filtering step guarantees the quality of the synthetic data.
Our generator is a sequence to sequence (seq2seq) model that is pretrained on massive amount of monolingual data with text infilling objective (§<a href="#S2" title="2 Denoising Sequence-to-Sequence as Generator ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). We utilize BART <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, a recently proposed denoising autoencoder, as our generator to avoid training it from scratch. The auxiliary parser can be arbitrary. We experiment with BART-based parser as well as state-of-the-art pointer network parser <cite class="ltx_cite ltx_citemacro_citep">(s2s-pointer; Rongali et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The paper is structured as follows. We introduce our generative model for synthetic data in Section §<a href="#S2" title="2 Denoising Sequence-to-Sequence as Generator ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Experimental results on Facebook TOP dataset and sub-sampled datasets to simulate low-resource scenario are presented in Section §<a href="#S3" title="3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Section §<a href="#S5" title="5 Conclusions ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Denoising Sequence-to-Sequence as Generator</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The generative story for generating synthetic data <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{Y}_{\text{syn}}=\{\widetilde{{\bm{y}}}_{i}\}_{i=1}^{M}" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><msub id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1.3.2" xref="S2.p1.1.m1.1.1.3.2.cmml">𝒴</mi><mtext id="S2.p1.1.m1.1.1.3.3" xref="S2.p1.1.m1.1.1.3.3a.cmml">syn</mtext></msub><mo id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml"><mrow id="S2.p1.1.m1.1.1.1.1.1.1" xref="S2.p1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.p1.1.m1.1.1.1.1.1.1.2" xref="S2.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S2.p1.1.m1.1.1.1.1.1.1.1" xref="S2.p1.1.m1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.p1.1.m1.1.1.1.1.1.1.1.2" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml">𝒚</mi><mo id="S2.p1.1.m1.1.1.1.1.1.1.1.2.1" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S2.p1.1.m1.1.1.1.1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p1.1.m1.1.1.1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.p1.1.m1.1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.1.3.cmml"><mi id="S2.p1.1.m1.1.1.1.1.3.2" xref="S2.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p1.1.m1.1.1.1.1.3.1" xref="S2.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S2.p1.1.m1.1.1.1.1.3.3" xref="S2.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.p1.1.m1.1.1.1.3" xref="S2.p1.1.m1.1.1.1.3.cmml">M</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><eq id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"></eq><apply id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.3.1.cmml" xref="S2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.p1.1.m1.1.1.3.2.cmml" xref="S2.p1.1.m1.1.1.3.2">𝒴</ci><ci id="S2.p1.1.m1.1.1.3.3a.cmml" xref="S2.p1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S2.p1.1.m1.1.1.3.3.cmml" xref="S2.p1.1.m1.1.1.3.3">syn</mtext></ci></apply><apply id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.1">superscript</csymbol><apply id="S2.p1.1.m1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.1">subscript</csymbol><set id="S2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1"><apply id="S2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2"><ci id="S2.p1.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2.1">~</ci><ci id="S2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1.2.2">𝒚</ci></apply><ci id="S2.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S2.p1.1.m1.1.1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.1.1.3"><eq id="S2.p1.1.m1.1.1.1.1.3.1.cmml" xref="S2.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S2.p1.1.m1.1.1.1.1.3.2.cmml" xref="S2.p1.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S2.p1.1.m1.1.1.1.1.3.3.cmml" xref="S2.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S2.p1.1.m1.1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.1.3">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\mathcal{Y}_{\text{syn}}=\{\widetilde{{\bm{y}}}_{i}\}_{i=1}^{M}</annotation></semantics></math>
is given by</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">draw a template <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="{\bm{z}}\sim p_{\phi}({\bm{z}})" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><mrow id="S2.I1.i1.p1.1.m1.1.2" xref="S2.I1.i1.p1.1.m1.1.2.cmml"><mi id="S2.I1.i1.p1.1.m1.1.2.2" xref="S2.I1.i1.p1.1.m1.1.2.2.cmml">𝒛</mi><mo id="S2.I1.i1.p1.1.m1.1.2.1" xref="S2.I1.i1.p1.1.m1.1.2.1.cmml">∼</mo><mrow id="S2.I1.i1.p1.1.m1.1.2.3" xref="S2.I1.i1.p1.1.m1.1.2.3.cmml"><msub id="S2.I1.i1.p1.1.m1.1.2.3.2" xref="S2.I1.i1.p1.1.m1.1.2.3.2.cmml"><mi id="S2.I1.i1.p1.1.m1.1.2.3.2.2" xref="S2.I1.i1.p1.1.m1.1.2.3.2.2.cmml">p</mi><mi id="S2.I1.i1.p1.1.m1.1.2.3.2.3" xref="S2.I1.i1.p1.1.m1.1.2.3.2.3.cmml">ϕ</mi></msub><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.1.2.3.1" xref="S2.I1.i1.p1.1.m1.1.2.3.1.cmml">​</mo><mrow id="S2.I1.i1.p1.1.m1.1.2.3.3.2" xref="S2.I1.i1.p1.1.m1.1.2.3.cmml"><mo stretchy="false" id="S2.I1.i1.p1.1.m1.1.2.3.3.2.1" xref="S2.I1.i1.p1.1.m1.1.2.3.cmml">(</mo><mi id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">𝒛</mi><mo stretchy="false" id="S2.I1.i1.p1.1.m1.1.2.3.3.2.2" xref="S2.I1.i1.p1.1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.2"><csymbol cd="latexml" id="S2.I1.i1.p1.1.m1.1.2.1.cmml" xref="S2.I1.i1.p1.1.m1.1.2.1">similar-to</csymbol><ci id="S2.I1.i1.p1.1.m1.1.2.2.cmml" xref="S2.I1.i1.p1.1.m1.1.2.2">𝒛</ci><apply id="S2.I1.i1.p1.1.m1.1.2.3.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3"><times id="S2.I1.i1.p1.1.m1.1.2.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3.1"></times><apply id="S2.I1.i1.p1.1.m1.1.2.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.2.3.2.1.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3.2">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.2.3.2.2.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3.2.2">𝑝</ci><ci id="S2.I1.i1.p1.1.m1.1.2.3.2.3.cmml" xref="S2.I1.i1.p1.1.m1.1.2.3.2.3">italic-ϕ</ci></apply><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">𝒛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">{\bm{z}}\sim p_{\phi}({\bm{z}})</annotation></semantics></math>;<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>During inference for generating synthetic data, we draw <math id="footnote2.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="footnote2.m1.1b"><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">{\bm{z}}</annotation></semantics></math> uniformly in order to generate more annotations for templates in the long tail.</span></span></span></p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.3" class="ltx_p">draw an annotation <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="{\bm{y}}\sim p_{\theta}({\bm{y}}\,|\,{\bm{z}})" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">𝒚</mi><mo id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">∼</mo><mrow id="S2.I1.i2.p1.1.m1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml"><msub id="S2.I1.i2.p1.1.m1.1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.1.3.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.1.3.2" xref="S2.I1.i2.p1.1.m1.1.1.1.3.2.cmml">p</mi><mi id="S2.I1.i2.p1.1.m1.1.1.1.3.3" xref="S2.I1.i2.p1.1.m1.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.I1.i2.p1.1.m1.1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.I1.i2.p1.1.m1.1.1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.2.cmml">𝒚</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.3.cmml">𝒛</mi></mrow><mo stretchy="false" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">similar-to</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">𝒚</ci><apply id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.2"></times><apply id="S2.I1.i2.p1.1.m1.1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.3.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.3">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.1.3.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.3.2">𝑝</ci><ci id="S2.I1.i2.p1.1.m1.1.1.1.3.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.3.3">𝜃</ci></apply><apply id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.2">𝒚</ci><ci id="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1.1.1.1.3">𝒛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">{\bm{y}}\sim p_{\theta}({\bm{y}}\,|\,{\bm{z}})</annotation></semantics></math> by filling each <span id="S2.I1.i2.p1.3.1" class="ltx_text ltx_font_typewriter">[mask]</span> token in <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mi id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><ci id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">{\bm{z}}</annotation></semantics></math> by a word or sequence of words from vocabulary <math id="S2.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{V}" display="inline"><semantics id="S2.I1.i2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><ci id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">\mathcal{V}</annotation></semantics></math>;</p>
</div>
</li>
</ol>
<p id="S2.p1.6" class="ltx_p">Note that the transformation from annotation <math id="S2.p1.2.m1.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S2.p1.2.m1.1a"><mi id="S2.p1.2.m1.1.1" xref="S2.p1.2.m1.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m1.1b"><ci id="S2.p1.2.m1.1.1.cmml" xref="S2.p1.2.m1.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m1.1c">{\bm{y}}</annotation></semantics></math> to utterance <math id="S2.p1.3.m2.1" class="ltx_Math" alttext="{\bm{x}}" display="inline"><semantics id="S2.p1.3.m2.1a"><mi id="S2.p1.3.m2.1.1" xref="S2.p1.3.m2.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m2.1b"><ci id="S2.p1.3.m2.1.1.cmml" xref="S2.p1.3.m2.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m2.1c">{\bm{x}}</annotation></semantics></math> is deterministic by removing non-terminals from <math id="S2.p1.4.m3.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S2.p1.4.m3.1a"><mi id="S2.p1.4.m3.1.1" xref="S2.p1.4.m3.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m3.1b"><ci id="S2.p1.4.m3.1.1.cmml" xref="S2.p1.4.m3.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m3.1c">{\bm{y}}</annotation></semantics></math>.
While <math id="S2.p1.5.m4.1" class="ltx_Math" alttext="p_{\phi}({\bm{z}})" display="inline"><semantics id="S2.p1.5.m4.1a"><mrow id="S2.p1.5.m4.1.2" xref="S2.p1.5.m4.1.2.cmml"><msub id="S2.p1.5.m4.1.2.2" xref="S2.p1.5.m4.1.2.2.cmml"><mi id="S2.p1.5.m4.1.2.2.2" xref="S2.p1.5.m4.1.2.2.2.cmml">p</mi><mi id="S2.p1.5.m4.1.2.2.3" xref="S2.p1.5.m4.1.2.2.3.cmml">ϕ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p1.5.m4.1.2.1" xref="S2.p1.5.m4.1.2.1.cmml">​</mo><mrow id="S2.p1.5.m4.1.2.3.2" xref="S2.p1.5.m4.1.2.cmml"><mo stretchy="false" id="S2.p1.5.m4.1.2.3.2.1" xref="S2.p1.5.m4.1.2.cmml">(</mo><mi id="S2.p1.5.m4.1.1" xref="S2.p1.5.m4.1.1.cmml">𝒛</mi><mo stretchy="false" id="S2.p1.5.m4.1.2.3.2.2" xref="S2.p1.5.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m4.1b"><apply id="S2.p1.5.m4.1.2.cmml" xref="S2.p1.5.m4.1.2"><times id="S2.p1.5.m4.1.2.1.cmml" xref="S2.p1.5.m4.1.2.1"></times><apply id="S2.p1.5.m4.1.2.2.cmml" xref="S2.p1.5.m4.1.2.2"><csymbol cd="ambiguous" id="S2.p1.5.m4.1.2.2.1.cmml" xref="S2.p1.5.m4.1.2.2">subscript</csymbol><ci id="S2.p1.5.m4.1.2.2.2.cmml" xref="S2.p1.5.m4.1.2.2.2">𝑝</ci><ci id="S2.p1.5.m4.1.2.2.3.cmml" xref="S2.p1.5.m4.1.2.2.3">italic-ϕ</ci></apply><ci id="S2.p1.5.m4.1.1.cmml" xref="S2.p1.5.m4.1.1">𝒛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m4.1c">p_{\phi}({\bm{z}})</annotation></semantics></math> can be modeled by an autoregressive neural language model or a Probabilistic Context Free Grammar <cite class="ltx_cite ltx_citemacro_citep">(Johnson, <a href="#bib.bib5" title="" class="ltx_ref">1998</a>)</cite>, in this work we sample template <math id="S2.p1.6.m5.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S2.p1.6.m5.1a"><mi id="S2.p1.6.m5.1.1" xref="S2.p1.6.m5.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m5.1b"><ci id="S2.p1.6.m5.1.1.cmml" xref="S2.p1.6.m5.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m5.1c">{\bm{z}}</annotation></semantics></math> from seen templates in the data. We leave the possibility of generating new templates to future work.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.2" class="ltx_p">We need a powerful conditional model <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="p_{\theta}({\bm{y}}\,|\,{\bm{z}})" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><msub id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml"><mi id="S2.p2.1.m1.1.1.3.2" xref="S2.p2.1.m1.1.1.3.2.cmml">p</mi><mi id="S2.p2.1.m1.1.1.3.3" xref="S2.p2.1.m1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">​</mo><mrow id="S2.p2.1.m1.1.1.1.1" xref="S2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p2.1.m1.1.1.1.1.2" xref="S2.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p2.1.m1.1.1.1.1.1" xref="S2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.1.1.1.2" xref="S2.p2.1.m1.1.1.1.1.1.2.cmml">𝒚</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S2.p2.1.m1.1.1.1.1.1.1" xref="S2.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.p2.1.m1.1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.1.3.cmml">𝒛</mi></mrow><mo stretchy="false" id="S2.p2.1.m1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2"></times><apply id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.3.1.cmml" xref="S2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S2.p2.1.m1.1.1.3.2.cmml" xref="S2.p2.1.m1.1.1.3.2">𝑝</ci><ci id="S2.p2.1.m1.1.1.3.3.cmml" xref="S2.p2.1.m1.1.1.3.3">𝜃</ci></apply><apply id="S2.p2.1.m1.1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.2">𝒚</ci><ci id="S2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.1.1.1.3">𝒛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">p_{\theta}({\bm{y}}\,|\,{\bm{z}})</annotation></semantics></math> to generate annotation <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">{\bm{y}}</annotation></semantics></math>. Thus, we choose BART, a pretrained denoising autoencoder for sequence-to-sequence, as our model.
Figure <a href="#S2.F3.sf1" title="In Figure 3 ‣ 2 Denoising Sequence-to-Sequence as Generator ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> illustrates the idea behind BART. Given an input sequence (a stream of text), one of five types of noise (Figure <a href="#S2.F3.sf2" title="In Figure 3 ‣ 2 Denoising Sequence-to-Sequence as Generator ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>) is used to corrupt the input sequence. Then BART reconstructs the original sequence by maximizing the likelihood of the original sequence.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2011.02050/assets/x3.png" id="S2.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>BART is trained to reconstruct the corrupted input.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2011.02050/assets/x4.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Five different types of noise introduced in BART.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of BART.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.2" class="ltx_p">Since pretrained BART uses <em id="S2.p3.2.1" class="ltx_emph ltx_font_italic">text infilling</em> as noise to corrupt the input sequence, naturally we can use BART to infill the templates. Text infilling is the task where a number of spans in the original input sequence are replaced by a token <span id="S2.p3.2.2" class="ltx_text ltx_font_typewriter">[mask]</span> and BART is trained to predict the replaced spans in the position of <span id="S2.p3.2.3" class="ltx_text ltx_font_typewriter">[mask]</span> tokens. For our purpose of generating synthetic data, we fine-tune BART on an infilling dataset where the input is a template <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">{\bm{z}}</annotation></semantics></math> with <span id="S2.p3.2.4" class="ltx_text ltx_font_typewriter">[mask]</span> and the output is a linearized tree representation <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="{\bm{y}}" display="inline"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">𝒚</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">𝒚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">{\bm{y}}</annotation></semantics></math> where <span id="S2.p3.2.5" class="ltx_text ltx_font_typewriter">[mask]</span> tokens are replaced by lexical words as shown in Figure <a href="#S2.F4" title="Figure 4 ‣ 2 Denoising Sequence-to-Sequence as Generator ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2011.02050/assets/x5.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="63" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Data for fine-tuning BART.</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">BART source/target construction:</span> We call out a few processing steps to construct this infilling dataset. First, non-terminal words are lowercased. We find this is necessary since the input will be tokenized by BART tokenizer and lowercasing non-terminal words prevents over-segmentation. Second, we make each of the closing brackets “<span id="S2.p4.1.2" class="ltx_text ltx_font_typewriter">]</span>” in the original data explicit (<span id="S2.p4.1.3" class="ltx_text ltx_font_italic">e</span>.<span id="S2.p4.1.4" class="ltx_text ltx_font_italic">g</span>., <span id="S2.p4.1.5" class="ltx_text ltx_font_typewriter">in:get_distance]</span>, <span id="S2.p4.1.6" class="ltx_text ltx_font_typewriter">sl:destination]</span>). This transformation provides the model explicit information of the scope of the intents and slots.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Fine-tuning and generation:</span> We fine-tune BART generator using the (<span id="S2.p5.1.2" class="ltx_text ltx_font_italic">template</span>, <span id="S2.p5.1.3" class="ltx_text ltx_font_italic">annotation</span>) pairs. After fine-tuning, we use the generator to generate full parse trees given templates. To increase the diversity of generated samples, we use top-<math id="S2.p5.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S2.p5.1.m1.1a"><mi id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><ci id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">p</annotation></semantics></math> sampling <cite class="ltx_cite ltx_citemacro_citep">(Holtzman et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> instead of beam search. The generator is trained to generate the tokenized labels together with the words. We remove generated annotations with invalid labels and convert the tokenized labels into the original tags in a post-processing step.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.5" class="ltx_p"><span id="S2.p6.5.1" class="ltx_text ltx_font_bold">Auxiliary parser (AP) for filtering:</span> In our preliminary experiments, we found that the generated samples are noisy. When we train our parser on the concatenation of both real and generated samples, the test accuracy degrades by 1.13% compared with a parser trained purely on real data.
We therefore use an auxiliary parser (AP) to select robust samples. The filtering step is straightforward. First, we train an auxiliary semantic parser <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="f_{\theta}({\bm{x}})" display="inline"><semantics id="S2.p6.1.m1.1a"><mrow id="S2.p6.1.m1.1.2" xref="S2.p6.1.m1.1.2.cmml"><msub id="S2.p6.1.m1.1.2.2" xref="S2.p6.1.m1.1.2.2.cmml"><mi id="S2.p6.1.m1.1.2.2.2" xref="S2.p6.1.m1.1.2.2.2.cmml">f</mi><mi id="S2.p6.1.m1.1.2.2.3" xref="S2.p6.1.m1.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p6.1.m1.1.2.1" xref="S2.p6.1.m1.1.2.1.cmml">​</mo><mrow id="S2.p6.1.m1.1.2.3.2" xref="S2.p6.1.m1.1.2.cmml"><mo stretchy="false" id="S2.p6.1.m1.1.2.3.2.1" xref="S2.p6.1.m1.1.2.cmml">(</mo><mi id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">𝒙</mi><mo stretchy="false" id="S2.p6.1.m1.1.2.3.2.2" xref="S2.p6.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><apply id="S2.p6.1.m1.1.2.cmml" xref="S2.p6.1.m1.1.2"><times id="S2.p6.1.m1.1.2.1.cmml" xref="S2.p6.1.m1.1.2.1"></times><apply id="S2.p6.1.m1.1.2.2.cmml" xref="S2.p6.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.p6.1.m1.1.2.2.1.cmml" xref="S2.p6.1.m1.1.2.2">subscript</csymbol><ci id="S2.p6.1.m1.1.2.2.2.cmml" xref="S2.p6.1.m1.1.2.2.2">𝑓</ci><ci id="S2.p6.1.m1.1.2.2.3.cmml" xref="S2.p6.1.m1.1.2.2.3">𝜃</ci></apply><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">𝒙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">f_{\theta}({\bm{x}})</annotation></semantics></math> on the original Facebook TOP dataset. We then use this trained AP to parse synthetic data <math id="S2.p6.2.m2.2" class="ltx_Math" alttext="(\widetilde{{\bm{x}}}_{i},\widetilde{{\bm{y}}}_{i})" display="inline"><semantics id="S2.p6.2.m2.2a"><mrow id="S2.p6.2.m2.2.2.2" xref="S2.p6.2.m2.2.2.3.cmml"><mo stretchy="false" id="S2.p6.2.m2.2.2.2.3" xref="S2.p6.2.m2.2.2.3.cmml">(</mo><msub id="S2.p6.2.m2.1.1.1.1" xref="S2.p6.2.m2.1.1.1.1.cmml"><mover accent="true" id="S2.p6.2.m2.1.1.1.1.2" xref="S2.p6.2.m2.1.1.1.1.2.cmml"><mi id="S2.p6.2.m2.1.1.1.1.2.2" xref="S2.p6.2.m2.1.1.1.1.2.2.cmml">𝒙</mi><mo id="S2.p6.2.m2.1.1.1.1.2.1" xref="S2.p6.2.m2.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S2.p6.2.m2.1.1.1.1.3" xref="S2.p6.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p6.2.m2.2.2.2.4" xref="S2.p6.2.m2.2.2.3.cmml">,</mo><msub id="S2.p6.2.m2.2.2.2.2" xref="S2.p6.2.m2.2.2.2.2.cmml"><mover accent="true" id="S2.p6.2.m2.2.2.2.2.2" xref="S2.p6.2.m2.2.2.2.2.2.cmml"><mi id="S2.p6.2.m2.2.2.2.2.2.2" xref="S2.p6.2.m2.2.2.2.2.2.2.cmml">𝒚</mi><mo id="S2.p6.2.m2.2.2.2.2.2.1" xref="S2.p6.2.m2.2.2.2.2.2.1.cmml">~</mo></mover><mi id="S2.p6.2.m2.2.2.2.2.3" xref="S2.p6.2.m2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p6.2.m2.2.2.2.5" xref="S2.p6.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.2b"><interval closure="open" id="S2.p6.2.m2.2.2.3.cmml" xref="S2.p6.2.m2.2.2.2"><apply id="S2.p6.2.m2.1.1.1.1.cmml" xref="S2.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.m2.1.1.1.1.1.cmml" xref="S2.p6.2.m2.1.1.1.1">subscript</csymbol><apply id="S2.p6.2.m2.1.1.1.1.2.cmml" xref="S2.p6.2.m2.1.1.1.1.2"><ci id="S2.p6.2.m2.1.1.1.1.2.1.cmml" xref="S2.p6.2.m2.1.1.1.1.2.1">~</ci><ci id="S2.p6.2.m2.1.1.1.1.2.2.cmml" xref="S2.p6.2.m2.1.1.1.1.2.2">𝒙</ci></apply><ci id="S2.p6.2.m2.1.1.1.1.3.cmml" xref="S2.p6.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S2.p6.2.m2.2.2.2.2.cmml" xref="S2.p6.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p6.2.m2.2.2.2.2.1.cmml" xref="S2.p6.2.m2.2.2.2.2">subscript</csymbol><apply id="S2.p6.2.m2.2.2.2.2.2.cmml" xref="S2.p6.2.m2.2.2.2.2.2"><ci id="S2.p6.2.m2.2.2.2.2.2.1.cmml" xref="S2.p6.2.m2.2.2.2.2.2.1">~</ci><ci id="S2.p6.2.m2.2.2.2.2.2.2.cmml" xref="S2.p6.2.m2.2.2.2.2.2.2">𝒚</ci></apply><ci id="S2.p6.2.m2.2.2.2.2.3.cmml" xref="S2.p6.2.m2.2.2.2.2.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.2c">(\widetilde{{\bm{x}}}_{i},\widetilde{{\bm{y}}}_{i})</annotation></semantics></math> and keep those samples where the outputs of the parser <math id="S2.p6.3.m3.1" class="ltx_Math" alttext="f_{\theta}(\widetilde{{\bm{x}}}_{i})" display="inline"><semantics id="S2.p6.3.m3.1a"><mrow id="S2.p6.3.m3.1.1" xref="S2.p6.3.m3.1.1.cmml"><msub id="S2.p6.3.m3.1.1.3" xref="S2.p6.3.m3.1.1.3.cmml"><mi id="S2.p6.3.m3.1.1.3.2" xref="S2.p6.3.m3.1.1.3.2.cmml">f</mi><mi id="S2.p6.3.m3.1.1.3.3" xref="S2.p6.3.m3.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p6.3.m3.1.1.2" xref="S2.p6.3.m3.1.1.2.cmml">​</mo><mrow id="S2.p6.3.m3.1.1.1.1" xref="S2.p6.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p6.3.m3.1.1.1.1.2" xref="S2.p6.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.p6.3.m3.1.1.1.1.1" xref="S2.p6.3.m3.1.1.1.1.1.cmml"><mover accent="true" id="S2.p6.3.m3.1.1.1.1.1.2" xref="S2.p6.3.m3.1.1.1.1.1.2.cmml"><mi id="S2.p6.3.m3.1.1.1.1.1.2.2" xref="S2.p6.3.m3.1.1.1.1.1.2.2.cmml">𝒙</mi><mo id="S2.p6.3.m3.1.1.1.1.1.2.1" xref="S2.p6.3.m3.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S2.p6.3.m3.1.1.1.1.1.3" xref="S2.p6.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p6.3.m3.1.1.1.1.3" xref="S2.p6.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.3.m3.1b"><apply id="S2.p6.3.m3.1.1.cmml" xref="S2.p6.3.m3.1.1"><times id="S2.p6.3.m3.1.1.2.cmml" xref="S2.p6.3.m3.1.1.2"></times><apply id="S2.p6.3.m3.1.1.3.cmml" xref="S2.p6.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.p6.3.m3.1.1.3.1.cmml" xref="S2.p6.3.m3.1.1.3">subscript</csymbol><ci id="S2.p6.3.m3.1.1.3.2.cmml" xref="S2.p6.3.m3.1.1.3.2">𝑓</ci><ci id="S2.p6.3.m3.1.1.3.3.cmml" xref="S2.p6.3.m3.1.1.3.3">𝜃</ci></apply><apply id="S2.p6.3.m3.1.1.1.1.1.cmml" xref="S2.p6.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.3.m3.1.1.1.1.1.1.cmml" xref="S2.p6.3.m3.1.1.1.1">subscript</csymbol><apply id="S2.p6.3.m3.1.1.1.1.1.2.cmml" xref="S2.p6.3.m3.1.1.1.1.1.2"><ci id="S2.p6.3.m3.1.1.1.1.1.2.1.cmml" xref="S2.p6.3.m3.1.1.1.1.1.2.1">~</ci><ci id="S2.p6.3.m3.1.1.1.1.1.2.2.cmml" xref="S2.p6.3.m3.1.1.1.1.1.2.2">𝒙</ci></apply><ci id="S2.p6.3.m3.1.1.1.1.1.3.cmml" xref="S2.p6.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m3.1c">f_{\theta}(\widetilde{{\bm{x}}}_{i})</annotation></semantics></math> match the synthetic labels <math id="S2.p6.4.m4.1" class="ltx_Math" alttext="\widetilde{{\bm{y}}}_{i}" display="inline"><semantics id="S2.p6.4.m4.1a"><msub id="S2.p6.4.m4.1.1" xref="S2.p6.4.m4.1.1.cmml"><mover accent="true" id="S2.p6.4.m4.1.1.2" xref="S2.p6.4.m4.1.1.2.cmml"><mi id="S2.p6.4.m4.1.1.2.2" xref="S2.p6.4.m4.1.1.2.2.cmml">𝒚</mi><mo id="S2.p6.4.m4.1.1.2.1" xref="S2.p6.4.m4.1.1.2.1.cmml">~</mo></mover><mi id="S2.p6.4.m4.1.1.3" xref="S2.p6.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.4.m4.1b"><apply id="S2.p6.4.m4.1.1.cmml" xref="S2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p6.4.m4.1.1.1.cmml" xref="S2.p6.4.m4.1.1">subscript</csymbol><apply id="S2.p6.4.m4.1.1.2.cmml" xref="S2.p6.4.m4.1.1.2"><ci id="S2.p6.4.m4.1.1.2.1.cmml" xref="S2.p6.4.m4.1.1.2.1">~</ci><ci id="S2.p6.4.m4.1.1.2.2.cmml" xref="S2.p6.4.m4.1.1.2.2">𝒚</ci></apply><ci id="S2.p6.4.m4.1.1.3.cmml" xref="S2.p6.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.m4.1c">\widetilde{{\bm{y}}}_{i}</annotation></semantics></math> (i.e., <math id="S2.p6.5.m5.1" class="ltx_Math" alttext="f_{\theta}(\widetilde{{\bm{x}}}_{i})=\widetilde{{\bm{y}}}_{i}" display="inline"><semantics id="S2.p6.5.m5.1a"><mrow id="S2.p6.5.m5.1.1" xref="S2.p6.5.m5.1.1.cmml"><mrow id="S2.p6.5.m5.1.1.1" xref="S2.p6.5.m5.1.1.1.cmml"><msub id="S2.p6.5.m5.1.1.1.3" xref="S2.p6.5.m5.1.1.1.3.cmml"><mi id="S2.p6.5.m5.1.1.1.3.2" xref="S2.p6.5.m5.1.1.1.3.2.cmml">f</mi><mi id="S2.p6.5.m5.1.1.1.3.3" xref="S2.p6.5.m5.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p6.5.m5.1.1.1.2" xref="S2.p6.5.m5.1.1.1.2.cmml">​</mo><mrow id="S2.p6.5.m5.1.1.1.1.1" xref="S2.p6.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p6.5.m5.1.1.1.1.1.2" xref="S2.p6.5.m5.1.1.1.1.1.1.cmml">(</mo><msub id="S2.p6.5.m5.1.1.1.1.1.1" xref="S2.p6.5.m5.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.p6.5.m5.1.1.1.1.1.1.2" xref="S2.p6.5.m5.1.1.1.1.1.1.2.cmml"><mi id="S2.p6.5.m5.1.1.1.1.1.1.2.2" xref="S2.p6.5.m5.1.1.1.1.1.1.2.2.cmml">𝒙</mi><mo id="S2.p6.5.m5.1.1.1.1.1.1.2.1" xref="S2.p6.5.m5.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S2.p6.5.m5.1.1.1.1.1.1.3" xref="S2.p6.5.m5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p6.5.m5.1.1.1.1.1.3" xref="S2.p6.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p6.5.m5.1.1.2" xref="S2.p6.5.m5.1.1.2.cmml">=</mo><msub id="S2.p6.5.m5.1.1.3" xref="S2.p6.5.m5.1.1.3.cmml"><mover accent="true" id="S2.p6.5.m5.1.1.3.2" xref="S2.p6.5.m5.1.1.3.2.cmml"><mi id="S2.p6.5.m5.1.1.3.2.2" xref="S2.p6.5.m5.1.1.3.2.2.cmml">𝒚</mi><mo id="S2.p6.5.m5.1.1.3.2.1" xref="S2.p6.5.m5.1.1.3.2.1.cmml">~</mo></mover><mi id="S2.p6.5.m5.1.1.3.3" xref="S2.p6.5.m5.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.5.m5.1b"><apply id="S2.p6.5.m5.1.1.cmml" xref="S2.p6.5.m5.1.1"><eq id="S2.p6.5.m5.1.1.2.cmml" xref="S2.p6.5.m5.1.1.2"></eq><apply id="S2.p6.5.m5.1.1.1.cmml" xref="S2.p6.5.m5.1.1.1"><times id="S2.p6.5.m5.1.1.1.2.cmml" xref="S2.p6.5.m5.1.1.1.2"></times><apply id="S2.p6.5.m5.1.1.1.3.cmml" xref="S2.p6.5.m5.1.1.1.3"><csymbol cd="ambiguous" id="S2.p6.5.m5.1.1.1.3.1.cmml" xref="S2.p6.5.m5.1.1.1.3">subscript</csymbol><ci id="S2.p6.5.m5.1.1.1.3.2.cmml" xref="S2.p6.5.m5.1.1.1.3.2">𝑓</ci><ci id="S2.p6.5.m5.1.1.1.3.3.cmml" xref="S2.p6.5.m5.1.1.1.3.3">𝜃</ci></apply><apply id="S2.p6.5.m5.1.1.1.1.1.1.cmml" xref="S2.p6.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.5.m5.1.1.1.1.1.1.1.cmml" xref="S2.p6.5.m5.1.1.1.1.1">subscript</csymbol><apply id="S2.p6.5.m5.1.1.1.1.1.1.2.cmml" xref="S2.p6.5.m5.1.1.1.1.1.1.2"><ci id="S2.p6.5.m5.1.1.1.1.1.1.2.1.cmml" xref="S2.p6.5.m5.1.1.1.1.1.1.2.1">~</ci><ci id="S2.p6.5.m5.1.1.1.1.1.1.2.2.cmml" xref="S2.p6.5.m5.1.1.1.1.1.1.2.2">𝒙</ci></apply><ci id="S2.p6.5.m5.1.1.1.1.1.1.3.cmml" xref="S2.p6.5.m5.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.p6.5.m5.1.1.3.cmml" xref="S2.p6.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.p6.5.m5.1.1.3.1.cmml" xref="S2.p6.5.m5.1.1.3">subscript</csymbol><apply id="S2.p6.5.m5.1.1.3.2.cmml" xref="S2.p6.5.m5.1.1.3.2"><ci id="S2.p6.5.m5.1.1.3.2.1.cmml" xref="S2.p6.5.m5.1.1.3.2.1">~</ci><ci id="S2.p6.5.m5.1.1.3.2.2.cmml" xref="S2.p6.5.m5.1.1.3.2.2">𝒚</ci></apply><ci id="S2.p6.5.m5.1.1.3.3.cmml" xref="S2.p6.5.m5.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.5.m5.1c">f_{\theta}(\widetilde{{\bm{x}}}_{i})=\widetilde{{\bm{y}}}_{i}</annotation></semantics></math>).
The AP for filtering can be different from the target parser we train for semantic parsing. Therefore, we propose three settings: (1) BART as AP and a sequence-to-sequence model with pointer networks <cite class="ltx_cite ltx_citemacro_citep">(s2s-pointer; Rongali et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> as the target parser. (2) BART models for both AP and target parser. (3) s2s-pointer models for both AP and target parser. The comparisons and analysis are detailed in Section §<a href="#S3" title="3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use Facebook TOP dataset in our experiments.
Statistics of the dataset are shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. While there are more than 31K annotated utterances in training data, the number of unique templates is about 6K. As we have shown in Section <a href="#S1" title="1 Introduction ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the distribution of the templates is highly unbalanced. In training data, there are 1,511 annotations with the template <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">[IN:UNSUPPORTED_NAVIGATION [mask] ]</span> and 1,046 annotations with template <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">[IN:UNSUPPORTED [mask] ]</span>. In the case of no UNSUPPORTED setting (<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><minus id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">-</annotation></semantics></math> <span id="S3.p1.1.3" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span> in Table <a href="#S3.T2" title="Table 2 ‣ 3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we exclude those annotations with <span id="S3.p1.1.4" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span> templates from train, valid, and test data. This results in 28,414 (template, annotation) pairs for training and 4,032 pairs for validation.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.3.1" class="ltx_tr">
<th id="S3.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Condition</th>
<th id="S3.T1.2.3.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.2.3.1.3.1" class="ltx_text ltx_font_typewriter">train</span></th>
<th id="S3.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.2.3.1.4.1" class="ltx_text ltx_font_typewriter">valid</span></th>
<th id="S3.T1.2.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.2.3.1.5.1" class="ltx_text ltx_font_typewriter">test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><plus id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">+</annotation></semantics></math> <span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span>
</th>
<th id="S3.T1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">31,279</td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">4,462</td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">9,042</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<math id="S3.T1.2.2.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T1.2.2.1.m1.1a"><mo id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><minus id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">-</annotation></semantics></math> <span id="S3.T1.2.2.1.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span>
</th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb">28,414</td>
<td id="S3.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb">4,032</td>
<td id="S3.T1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">8,241</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of samples in Facebook TOP dataset with (<math id="S3.T1.5.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.T1.5.m1.1b"><mo id="S3.T1.5.m1.1.1" xref="S3.T1.5.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.m1.1c"><plus id="S3.T1.5.m1.1.1.cmml" xref="S3.T1.5.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.m1.1d">+</annotation></semantics></math>) and without (<math id="S3.T1.6.m2.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T1.6.m2.1b"><mo id="S3.T1.6.m2.1.1" xref="S3.T1.6.m2.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.m2.1c"><minus id="S3.T1.6.m2.1.1.cmml" xref="S3.T1.6.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.m2.1d">-</annotation></semantics></math>) <span id="S3.T1.8.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span> utterances.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We fine-tune our BART generator using Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> with a linear warmup of 4,000 steps at the peak learning rate of <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="2\mathrm{e}{-5}" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mrow id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml"><mn id="S3.p2.1.m1.1.1.2.2" xref="S3.p2.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.1.2.1" xref="S3.p2.1.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.p2.1.m1.1.1.2.3" xref="S3.p2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">−</mo><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><minus id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></minus><apply id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2"><times id="S3.p2.1.m1.1.1.2.1.cmml" xref="S3.p2.1.m1.1.1.2.1"></times><cn type="integer" id="S3.p2.1.m1.1.1.2.2.cmml" xref="S3.p2.1.m1.1.1.2.2">2</cn><ci id="S3.p2.1.m1.1.1.2.3.cmml" xref="S3.p2.1.m1.1.1.2.3">e</ci></apply><cn type="integer" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">2\mathrm{e}{-5}</annotation></semantics></math>. We pick the best model based on validation perplexity. After fine-tuning, we use the generator to sample 5 full parse trees per template.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2011.02050/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="485" height="74" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sample of five synthetic parse trees generated given a template. Colors indicate the corresponding generated spans per <span id="S3.F5.2.1" class="ltx_text ltx_font_typewriter">[mask]</span> token. The data is reformatted for readability.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.2.2.5" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><minus id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">-</annotation></semantics></math> <span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span>
</td>
<td id="S3.T2.2.2.6" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S3.T2.2.2.2.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.T2.2.2.2.m1.1a"><mo id="S3.T2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><plus id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">+</annotation></semantics></math> <span id="S3.T2.2.2.2.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span>
</td>
</tr>
<tr id="S3.T2.2.3.1" class="ltx_tr">
<td id="S3.T2.2.3.1.1" class="ltx_td ltx_align_left">Data</td>
<td id="S3.T2.2.3.1.2" class="ltx_td ltx_align_left">AP filter</td>
<td id="S3.T2.2.3.1.3" class="ltx_td ltx_align_left">Target parser</td>
<td id="S3.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">#Samples</td>
<td id="S3.T2.2.3.1.5" class="ltx_td ltx_align_left ltx_border_t">Acc (%)</td>
<td id="S3.T2.2.3.1.6" class="ltx_td"></td>
<td id="S3.T2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">#Samples</td>
<td id="S3.T2.2.3.1.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Acc (%)</td>
</tr>
<tr id="S3.T2.2.4.2" class="ltx_tr">
<td id="S3.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.4.2.1.1" class="ltx_text ltx_font_typewriter">Real</span></td>
<td id="S3.T2.2.4.2.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.2.4.2.3" class="ltx_td ltx_align_left ltx_border_t">BART</td>
<td id="S3.T2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">28,414</td>
<td id="S3.T2.2.4.2.5" class="ltx_td ltx_align_left ltx_border_t">83.37</td>
<td id="S3.T2.2.4.2.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t">31,279</td>
<td id="S3.T2.2.4.2.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">81.01</td>
</tr>
<tr id="S3.T2.2.5.3" class="ltx_tr">
<td id="S3.T2.2.5.3.1" class="ltx_td ltx_align_left">+ <span id="S3.T2.2.5.3.1.1" class="ltx_text ltx_font_typewriter">syn</span>
</td>
<td id="S3.T2.2.5.3.2" class="ltx_td ltx_align_left">BART</td>
<td id="S3.T2.2.5.3.3" class="ltx_td ltx_align_left">BART</td>
<td id="S3.T2.2.5.3.4" class="ltx_td ltx_align_center">53,679</td>
<td id="S3.T2.2.5.3.5" class="ltx_td ltx_align_left">84.26 <span id="S3.T2.2.5.3.5.1" class="ltx_text" style="color:#999999;">(+0.89)</span>
</td>
<td id="S3.T2.2.5.3.6" class="ltx_td"></td>
<td id="S3.T2.2.5.3.7" class="ltx_td ltx_align_center">56,547</td>
<td id="S3.T2.2.5.3.8" class="ltx_td ltx_nopad_r ltx_align_left">81.74 <span id="S3.T2.2.5.3.8.1" class="ltx_text" style="color:#999999;">(+0.73)</span>
</td>
</tr>
<tr id="S3.T2.2.6.4" class="ltx_tr">
<td id="S3.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.6.4.1.1" class="ltx_text ltx_font_typewriter">Real</span></td>
<td id="S3.T2.2.6.4.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.2.6.4.3" class="ltx_td ltx_align_left ltx_border_t">s2s-pointer</td>
<td id="S3.T2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">28,414</td>
<td id="S3.T2.2.6.4.5" class="ltx_td ltx_align_left ltx_border_t">84.80</td>
<td id="S3.T2.2.6.4.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_t">31,279</td>
<td id="S3.T2.2.6.4.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">82.10</td>
</tr>
<tr id="S3.T2.2.7.5" class="ltx_tr">
<td id="S3.T2.2.7.5.1" class="ltx_td ltx_align_left"><span id="S3.T2.2.7.5.1.1" class="ltx_text ltx_font_typewriter">+syn</span></td>
<td id="S3.T2.2.7.5.2" class="ltx_td ltx_align_left">BART</td>
<td id="S3.T2.2.7.5.3" class="ltx_td ltx_align_left">s2s-pointer</td>
<td id="S3.T2.2.7.5.4" class="ltx_td ltx_align_center">53,679</td>
<td id="S3.T2.2.7.5.5" class="ltx_td ltx_align_left">85.31 <span id="S3.T2.2.7.5.5.1" class="ltx_text" style="color:#999999;">(+0.51)</span>
</td>
<td id="S3.T2.2.7.5.6" class="ltx_td"></td>
<td id="S3.T2.2.7.5.7" class="ltx_td ltx_align_center">56,355</td>
<td id="S3.T2.2.7.5.8" class="ltx_td ltx_nopad_r ltx_align_left">82.71 <span id="S3.T2.2.7.5.8.1" class="ltx_text" style="color:#999999;">(+0.61)</span>
</td>
</tr>
<tr id="S3.T2.2.8.6" class="ltx_tr">
<td id="S3.T2.2.8.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.2.8.6.1.1" class="ltx_text ltx_font_typewriter">+syn</span></td>
<td id="S3.T2.2.8.6.2" class="ltx_td ltx_align_left ltx_border_bb">s2s-pointer</td>
<td id="S3.T2.2.8.6.3" class="ltx_td ltx_align_left ltx_border_bb">s2s-pointer</td>
<td id="S3.T2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_bb">89,629</td>
<td id="S3.T2.2.8.6.5" class="ltx_td ltx_align_left ltx_border_bb">85.68 <span id="S3.T2.2.8.6.5.1" class="ltx_text" style="color:#999999;">(+0.88)</span>
</td>
<td id="S3.T2.2.8.6.6" class="ltx_td ltx_border_bb"></td>
<td id="S3.T2.2.8.6.7" class="ltx_td ltx_align_center ltx_border_bb">92,264</td>
<td id="S3.T2.2.8.6.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">82.77 <span id="S3.T2.2.8.6.8.1" class="ltx_text" style="color:#999999;">(+0.67)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Exact-match results of our experiments. The AP filter can be a fine-tuned BART for parsing or a s2s-pointer model of <cite class="ltx_cite ltx_citemacro_citet">Rongali et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The exact-match results for the three settings of using BART/s2s-pointer as auxiliary and target parser are given in Table <a href="#S3.T2" title="Table 2 ‣ 3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We first notice that the BART-based parser performs on-par with SOTA model based on pointer network and RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> feature extractor in the work of <cite class="ltx_cite ltx_citemacro_citet">Rongali et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. This suggests that pretraining a general purpose seq2seq model is beneficial for downstream conditional generation task. We also see that using synthetic data brings additional 0.89% for BART-parser and 0.88% for s2s-pointer parser on the exact-match accuracy. The gain of using synthetic data is smaller when <span id="S3.p3.1.1" class="ltx_text ltx_font_typewriter">UNSUPPORTED</span> utterances are present in training and testing data.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.3" class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ 3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the exact match accuracy of BART-based parser on testset with respect to template frequency <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">f</annotation></semantics></math> in training data. We see that synthetic data helps low-frequency templates (<math id="S3.p4.2.m2.1" class="ltx_Math" alttext="f&lt;5" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">f</mi><mo id="S3.p4.2.m2.1.1.1" xref="S3.p4.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><lt id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1"></lt><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">𝑓</ci><cn type="integer" id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">f&lt;5</annotation></semantics></math>) the most (+1.36%). The gain of 0.67% for unseen templates (<math id="S3.p4.3.m3.1" class="ltx_Math" alttext="f=0" display="inline"><semantics id="S3.p4.3.m3.1a"><mrow id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mi id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml">f</mi><mo id="S3.p4.3.m3.1.1.1" xref="S3.p4.3.m3.1.1.1.cmml">=</mo><mn id="S3.p4.3.m3.1.1.3" xref="S3.p4.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><eq id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1.1"></eq><ci id="S3.p4.3.m3.1.1.2.cmml" xref="S3.p4.3.m3.1.1.2">𝑓</ci><cn type="integer" id="S3.p4.3.m3.1.1.3.cmml" xref="S3.p4.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">f=0</annotation></semantics></math>) suggests that there is a room for further improvement by generating new templates.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.3" class="ltx_tr">
<th id="S3.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Training data</th>
<th id="S3.T3.3.3.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S3.T3.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="f\geq 5" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml">f</mi><mo id="S3.T3.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.m1.1.1.1.cmml">≥</mo><mn id="S3.T3.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><geq id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1"></geq><ci id="S3.T3.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2">𝑓</ci><cn type="integer" id="S3.T3.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">f\geq 5</annotation></semantics></math></th>
<th id="S3.T3.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S3.T3.2.2.2.m1.1" class="ltx_Math" alttext="f&lt;5" display="inline"><semantics id="S3.T3.2.2.2.m1.1a"><mrow id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml"><mi id="S3.T3.2.2.2.m1.1.1.2" xref="S3.T3.2.2.2.m1.1.1.2.cmml">f</mi><mo id="S3.T3.2.2.2.m1.1.1.1" xref="S3.T3.2.2.2.m1.1.1.1.cmml">&lt;</mo><mn id="S3.T3.2.2.2.m1.1.1.3" xref="S3.T3.2.2.2.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><apply id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1"><lt id="S3.T3.2.2.2.m1.1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1.1"></lt><ci id="S3.T3.2.2.2.m1.1.1.2.cmml" xref="S3.T3.2.2.2.m1.1.1.2">𝑓</ci><cn type="integer" id="S3.T3.2.2.2.m1.1.1.3.cmml" xref="S3.T3.2.2.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">f&lt;5</annotation></semantics></math></th>
<th id="S3.T3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"><math id="S3.T3.3.3.3.m1.1" class="ltx_Math" alttext="f=0" display="inline"><semantics id="S3.T3.3.3.3.m1.1a"><mrow id="S3.T3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.m1.1.1.cmml"><mi id="S3.T3.3.3.3.m1.1.1.2" xref="S3.T3.3.3.3.m1.1.1.2.cmml">f</mi><mo id="S3.T3.3.3.3.m1.1.1.1" xref="S3.T3.3.3.3.m1.1.1.1.cmml">=</mo><mn id="S3.T3.3.3.3.m1.1.1.3" xref="S3.T3.3.3.3.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.m1.1b"><apply id="S3.T3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1"><eq id="S3.T3.3.3.3.m1.1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1.1"></eq><ci id="S3.T3.3.3.3.m1.1.1.2.cmml" xref="S3.T3.3.3.3.m1.1.1.2">𝑓</ci><cn type="integer" id="S3.T3.3.3.3.m1.1.1.3.cmml" xref="S3.T3.3.3.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.m1.1c">f=0</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.4.5.1" class="ltx_tr">
<td id="S3.T3.4.5.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T3.4.5.1.1.1" class="ltx_text ltx_font_typewriter">Real</span></td>
<td id="S3.T3.4.5.1.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.4.5.1.3" class="ltx_td ltx_align_right ltx_border_t">89.46</td>
<td id="S3.T3.4.5.1.4" class="ltx_td ltx_align_right ltx_border_t">74.70</td>
<td id="S3.T3.4.5.1.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">61.90</td>
</tr>
<tr id="S3.T3.4.6.2" class="ltx_tr">
<td id="S3.T3.4.6.2.1" class="ltx_td ltx_align_left"><span id="S3.T3.4.6.2.1.1" class="ltx_text ltx_font_typewriter">+syn</span></td>
<td id="S3.T3.4.6.2.2" class="ltx_td"></td>
<td id="S3.T3.4.6.2.3" class="ltx_td ltx_align_right">90.30</td>
<td id="S3.T3.4.6.2.4" class="ltx_td ltx_align_right">76.06</td>
<td id="S3.T3.4.6.2.5" class="ltx_td ltx_nopad_r ltx_align_right">62.57</td>
</tr>
<tr id="S3.T3.4.4" class="ltx_tr">
<td id="S3.T3.4.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><math id="S3.T3.4.4.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T3.4.4.1.m1.1a"><mi mathvariant="normal" id="S3.T3.4.4.1.m1.1.1" xref="S3.T3.4.4.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><ci id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T3.4.4.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S3.T3.4.4.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">0.84</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">1.36</td>
<td id="S3.T3.4.4.5" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb ltx_border_t">0.67</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Exact-match accuracy on testset with respect to template frequency <math id="S3.T3.6.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.T3.6.m1.1b"><mi id="S3.T3.6.m1.1.1" xref="S3.T3.6.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.T3.6.m1.1c"><ci id="S3.T3.6.m1.1.1.cmml" xref="S3.T3.6.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.m1.1d">f</annotation></semantics></math> in training data.</figcaption>
</figure>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In order to support new domains (with new intents and slots) for the virtual assistants, we investigate the role of synthetic data when there is a little data available for the new domains.
We simulate this scenario by sub-sampling 6K utterances in the training data as follows: for each template in the training data, we randomly choose one utterance. We use this sub-sampled data for training our parser, generator, and AP. Table <a href="#S3.T4" title="Table 4 ‣ 3 Experiments ‣ Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the mean and variance of the accuracy on five random sub-sampled portions of the train data. We see that in this low resource setting, our approach boosts the accuracy by more than 2% absolute.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.3.1" class="ltx_tr">
<th id="S3.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Training data</th>
<th id="S3.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">#Samples</th>
<th id="S3.T4.2.3.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Acc (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T4.1.1.2.1" class="ltx_text ltx_font_typewriter">Real</span></th>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.1.1.3.1" class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>6,000</td>
<td id="S3.T4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">72.24 <math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.05</td>
</tr>
<tr id="S3.T4.2.2" class="ltx_tr">
<th id="S3.T4.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T4.2.2.2.1" class="ltx_text ltx_font_typewriter">+syn</span></th>
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_center ltx_border_bb"> 30,000</td>
<td id="S3.T4.2.2.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T4.2.2.1.1" class="ltx_text ltx_font_bold">74.31 <math id="S3.T4.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T4.2.2.1.1.m1.1a"><mo id="S3.T4.2.2.1.1.m1.1.1" xref="S3.T4.2.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.2.2.1.1.m1.1.1.cmml" xref="S3.T4.2.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.05</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Average accuracy of five different runs for 6K training examples. The synthetic data is filtered by BART parser, which is trained on 6K samples.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Using pretrained models to generate synthetic data has been studied recently <cite class="ltx_cite ltx_citemacro_citep">(Amin-Nejad et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>; Kumar et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. Their work however focuses on multi-class classification problems. Taking a step further, our work shows a viable path for structured output (<span id="S4.p1.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S4.p1.1.2" class="ltx_text ltx_font_italic">e</span>., parse trees) problems.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have proposed a novel approach for generating synthetic data for hierarchical semantic parsing. Our initial experiments show promising results of this approach and open up possibility for applying it to other problems with highly structured outputs in Natural Language Processing.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We thank reviewers for their constructive comments and suggestions. We also thank Raquel G. Alhama for proofreading this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amin-Nejad et al. (2020)</span>
<span class="ltx_bibblock">
Ali Amin-Nejad, Julia Ive, and Sumithra Velupillai. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/2020.lrec-1.578" title="" class="ltx_ref ltx_href">Exploring
transformer text generation for medical dataset augmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Language Resources and Evaluation
Conference</em>, pages 4699–4708, Marseille, France. European Language Resources
Association.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov et al. (2018)</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1045" title="" class="ltx_ref ltx_href">Understanding
back-translation at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489–500, Brussels, Belgium. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2018)</span>
<span class="ltx_bibblock">
Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and Mike Lewis. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1300" title="" class="ltx_ref ltx_href">Semantic parsing for
task oriented dialog using hierarchical representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2787–2792, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holtzman et al. (2020)</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rygGQyrFvH" title="" class="ltx_ref ltx_href">The curious case
of neural text degeneration</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson (1998)</span>
<span class="ltx_bibblock">
Mark Johnson. 1998.

</span>
<span class="ltx_bibblock">Pcfg models of linguistic tree representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Comput. Linguist.</em>, 24(4):613–632.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al. (2020)</span>
<span class="ltx_bibblock">
Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2003.02245" title="" class="ltx_ref ltx_href">Data augmentation using
pre-trained transformer models</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/2020.acl-main.703" title="" class="ltx_ref ltx_href">BART:
Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7871–7880, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1907.11692.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rongali et al. (2020)</span>
<span class="ltx_bibblock">
Subendhu Rongali, Luca Soldaini, Emilio Monti, and Wael Hamza. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3366423.3380064" title="" class="ltx_ref ltx_href">Don’t parse,
generate! a sequence to sequence architecture for task-oriented semantic
parsing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of The Web Conference 2020</em>, WWW ’20, page
2962–2968, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1009" title="" class="ltx_ref ltx_href">Improving neural
machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zettlemoyer and Collins (2005)</span>
<span class="ltx_bibblock">
Luke S. Zettlemoyer and Michael Collins. 2005.

</span>
<span class="ltx_bibblock">Learning to map sentences to logical form: Structured classification
with probabilistic categorial grammars.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence</em>, UAI’05, page 658–666, Arlington, Virginia, USA.
AUAI Press.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.02049" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.02050" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.02050">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.02050" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.02051" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 04:31:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
