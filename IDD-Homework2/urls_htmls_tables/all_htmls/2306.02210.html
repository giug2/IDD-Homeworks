<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.02210] GPT-FL: Generative Pre-trained Model-Assisted Federated Learning</title><meta property="og:description" content="In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These gene…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GPT-FL: Generative Pre-trained Model-Assisted Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="GPT-FL: Generative Pre-trained Model-Assisted Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.02210">

<!--Generated on Thu Feb 29 02:42:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">GPT-FL: Generative Pre-trained Model-Assisted Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tuo Zhang<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> , Tiantian Feng<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> , Samiul Alam<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">2</span></sup>, 
<br class="ltx_break"><span id="id7.7.4" class="ltx_text ltx_font_bold">Dimitrios Dimitriadis<sup id="id7.7.4.1" class="ltx_sup"><span id="id7.7.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>, Mi Zhang<sup id="id7.7.4.2" class="ltx_sup"><span id="id7.7.4.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup>, Shrikanth S. Narayanan<sup id="id7.7.4.3" class="ltx_sup"><span id="id7.7.4.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>, Salman Avestimehr<sup id="id7.7.4.4" class="ltx_sup"><span id="id7.7.4.4.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>

<br class="ltx_break"><sup id="id14.14.id4" class="ltx_sup">1</sup>University of Southern California, <sup id="id15.15.id5" class="ltx_sup">2</sup>The Ohio State University, <sup id="id16.16.id6" class="ltx_sup">3</sup>Amazon 
<br class="ltx_break">
</span><span class="ltx_author_notes">The first two authors contribute equally</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">In this work, we propose <span id="id17.id1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>, a generative pre-trained model-assisted federated learning (FL) framework. At its core, <span id="id17.id1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework.
We show that <span id="id17.id1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency.
Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with <span id="id17.id1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span>. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, <span id="id17.id1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> consistently achieves significant performance gains, surpassing the results obtained by models trained solely with FL or synthetic data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a privacy-preserving machine learning paradigm that allows a collection of clients to collaboratively train a machine learning model without sharing their private data <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>.
Most existing FL studies such as <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>); Bonawitz et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite> follow the standard FL architecture, where each participating client trains a local model using its own private data and a central server aggregates these locally trained models to update a global model and send it back to the clients for the next round of training.
However, although many efforts have been made <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>); Karimireddy et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>); Reddi et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, the performance of standard FL is still constrained by client drift caused by the heterogeneity in private data distribution across the clients.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To enhance the performance of FL, recent studies propose to incorporate data collected from public spaces such as the internet into the FL process <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Itahara et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>); Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. However, the performance of such public data-based approaches is heavily dependent on the quality of the collected public data. Unfortunately, obtaining the desired public data can be extremely challenging in practice and there is a lack of principled guidance on how to obtain them.
To address the issues of public data-based approaches, FL methods based on synthetic data emerge <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.
In <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, a generative model is trained through knowledge distillation (KD) and the synthetic data are generated from the generative model in an <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">interleaved</span> manner <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">throughout</span> the federated training iterations. Unfortunately, these approaches are confronted with two limitations:
(1) since the training of the generative model and the federated training process interleave, the quality of the synthetic data generated by the generative model before it converges can be extremely unstable. Such low-quality synthetic data would in turn jeopardize the federated training process;
(2) given that KD requires clients to report model weights as teachers to transfer knowledge, they are incompatible with secure aggregation protocols <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>); So et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, which limits their privacy guarantee compared to standard FL.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this work, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>, a generative pre-trained model-assisted FL framework that effectively addresses the issues of existing methods.
The key idea behind <span id="S1.p3.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> is to leverage the knowledge from the generative pre-trained models and to <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">decouple</span> synthetic data generation from the federated training process.
Specifically, <span id="S1.p3.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> prompts the generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server in the centralized manner, which is then fine-tuned with the private client data under the standard FL framework.
By doing this, the proposed <span id="S1.p3.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> is able to combine the advantages of previous methods while addressing their limitations.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of <span id="S1.T1.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> with existing FL methods.</figcaption>
<div id="S1.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:130.5pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-242.6pt,72.7pt) scale(0.471904038303216,0.471904038303216) ;">
<table id="S1.T1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.2.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">External Data</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.3.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Limited to</span></td>
</tr>
<tr id="S1.T1.3.1.1.3.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Smaller</span></td>
</tr>
<tr id="S1.T1.3.1.1.3.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.3.1.3.1.1" class="ltx_text ltx_font_bold">Client Model</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.4.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Generate</span></td>
</tr>
<tr id="S1.T1.3.1.1.4.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Data</span></td>
</tr>
<tr id="S1.T1.3.1.1.4.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.4.1.3.1.1" class="ltx_text ltx_font_bold">during FL</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.5.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Data</span></td>
</tr>
<tr id="S1.T1.3.1.1.5.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">Generator</span></td>
</tr>
<tr id="S1.T1.3.1.1.5.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.5.1.3.1.1" class="ltx_text ltx_font_bold">Location</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.6.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Client Access to</span></td>
</tr>
<tr id="S1.T1.3.1.1.6.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Public/Generated</span></td>
</tr>
<tr id="S1.T1.3.1.1.6.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.6.1.3.1.1" class="ltx_text ltx_font_bold">Data</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.3.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.7.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">Support</span></td>
</tr>
<tr id="S1.T1.3.1.1.7.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">Data</span></td>
</tr>
<tr id="S1.T1.3.1.1.7.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.7.1.3.1.1" class="ltx_text ltx_font_bold">Modality</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.1.1.8" class="ltx_td ltx_align_left ltx_border_tt">
<table id="S1.T1.3.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.8.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.8.1.1.1.1" class="ltx_text ltx_font_bold">Compatibility</span></td>
</tr>
<tr id="S1.T1.3.1.1.8.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.8.1.2.1.1" class="ltx_text ltx_font_bold">with Secure</span></td>
</tr>
<tr id="S1.T1.3.1.1.8.1.3" class="ltx_tr">
<td id="S1.T1.3.1.1.8.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.3.1.1.8.1.3.1.1" class="ltx_text ltx_font_bold">Aggregation</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.3.1.2" class="ltx_tr">
<td id="S1.T1.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t">FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S1.T1.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.2.1" class="ltx_text">No</span></td>
<td id="S1.T1.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.3.1" class="ltx_text">No</span></td>
<td id="S1.T1.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.4.1" class="ltx_text">N/A</span></td>
<td id="S1.T1.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.5.1" class="ltx_text">N/A</span></td>
<td id="S1.T1.3.1.2.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.6.1" class="ltx_text">N/A</span></td>
<td id="S1.T1.3.1.2.7" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.7.1" class="ltx_text">Any</span></td>
<td id="S1.T1.3.1.2.8" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.2.8.1" class="ltx_text">Yes</span></td>
</tr>
<tr id="S1.T1.3.1.3" class="ltx_tr">
<td id="S1.T1.3.1.3.1" class="ltx_td ltx_align_left">FedOpt <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>
</td>
</tr>
<tr id="S1.T1.3.1.4" class="ltx_tr">
<td id="S1.T1.3.1.4.1" class="ltx_td ltx_align_left">FedProx <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>
</td>
</tr>
<tr id="S1.T1.3.1.5" class="ltx_tr">
<td id="S1.T1.3.1.5.1" class="ltx_td ltx_align_left">SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">Karimireddy et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>
</td>
</tr>
<tr id="S1.T1.3.1.6" class="ltx_tr">
<td id="S1.T1.3.1.6.1" class="ltx_td ltx_align_left ltx_border_t">FedDF <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S1.T1.3.1.6.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.6.2.1" class="ltx_text">
<span id="S1.T1.3.1.6.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.3.1.6.2.1.1.1" class="ltx_tr">
<span id="S1.T1.3.1.6.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Public Data</span></span>
</span></span></td>
<td id="S1.T1.3.1.6.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.6.3.1" class="ltx_text">No</span></td>
<td id="S1.T1.3.1.6.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.6.4.1" class="ltx_text">N/A</span></td>
<td id="S1.T1.3.1.6.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.6.5.1" class="ltx_text">N/A</span></td>
<td id="S1.T1.3.1.6.6" class="ltx_td ltx_align_center ltx_border_t">Not Required</td>
<td id="S1.T1.3.1.6.7" class="ltx_td ltx_align_center ltx_border_t">Any</td>
<td id="S1.T1.3.1.6.8" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S1.T1.3.1.6.8.1" class="ltx_text">No</span></td>
</tr>
<tr id="S1.T1.3.1.7" class="ltx_tr">
<td id="S1.T1.3.1.7.1" class="ltx_td ltx_align_left">MOON <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S1.T1.3.1.7.2" class="ltx_td ltx_align_center">Required</td>
<td id="S1.T1.3.1.7.3" class="ltx_td ltx_align_center">Only Image</td>
</tr>
<tr id="S1.T1.3.1.8" class="ltx_tr">
<td id="S1.T1.3.1.8.1" class="ltx_td ltx_align_left">DS-FL <cite class="ltx_cite ltx_citemacro_cite">Itahara et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S1.T1.3.1.8.2" class="ltx_td ltx_align_center">Required</td>
<td id="S1.T1.3.1.8.3" class="ltx_td ltx_align_center">Any</td>
</tr>
<tr id="S1.T1.3.1.9" class="ltx_tr">
<td id="S1.T1.3.1.9.1" class="ltx_td ltx_align_left">Fed-ET <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S1.T1.3.1.9.2" class="ltx_td ltx_align_center">Not Required</td>
<td id="S1.T1.3.1.9.3" class="ltx_td ltx_align_center">Any</td>
</tr>
<tr id="S1.T1.3.1.10" class="ltx_tr">
<td id="S1.T1.3.1.10.1" class="ltx_td ltx_align_left ltx_border_t">FedGen <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S1.T1.3.1.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S1.T1.3.1.10.2.1" class="ltx_text">
<span id="S1.T1.3.1.10.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.3.1.10.2.1.1.1" class="ltx_tr">
<span id="S1.T1.3.1.10.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Generated Data</span></span>
</span></span></td>
<td id="S1.T1.3.1.10.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S1.T1.3.1.10.3.1" class="ltx_text">Yes</span></td>
<td id="S1.T1.3.1.10.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S1.T1.3.1.10.4.1" class="ltx_text">Yes</span></td>
<td id="S1.T1.3.1.10.5" class="ltx_td ltx_align_center ltx_border_t">Client</td>
<td id="S1.T1.3.1.10.6" class="ltx_td ltx_align_center ltx_border_t">Required</td>
<td id="S1.T1.3.1.10.7" class="ltx_td ltx_align_center ltx_border_t">Only Image</td>
<td id="S1.T1.3.1.10.8" class="ltx_td ltx_align_center ltx_border_t">No</td>
</tr>
<tr id="S1.T1.3.1.11" class="ltx_tr">
<td id="S1.T1.3.1.11.1" class="ltx_td ltx_align_left">FedFTG <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S1.T1.3.1.11.2" class="ltx_td ltx_align_center">Server</td>
<td id="S1.T1.3.1.11.3" class="ltx_td ltx_align_center">Not Required</td>
<td id="S1.T1.3.1.11.4" class="ltx_td ltx_align_center">Only Image</td>
<td id="S1.T1.3.1.11.5" class="ltx_td ltx_align_center">No</td>
</tr>
<tr id="S1.T1.3.1.12" class="ltx_tr">
<td id="S1.T1.3.1.12.1" class="ltx_td ltx_align_left">DynaFed <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S1.T1.3.1.12.2" class="ltx_td ltx_align_center">Server</td>
<td id="S1.T1.3.1.12.3" class="ltx_td ltx_align_center">Not Required</td>
<td id="S1.T1.3.1.12.4" class="ltx_td ltx_align_center">Only Image</td>
<td id="S1.T1.3.1.12.5" class="ltx_td ltx_align_center">Yes</td>
</tr>
<tr id="S1.T1.3.1.13" class="ltx_tr">
<td id="S1.T1.3.1.13.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S1.T1.3.1.13.1.1" class="ltx_text ltx_font_bold">GPT-FL (Ours)</span></td>
<td id="S1.T1.3.1.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.2.1" class="ltx_text ltx_font_bold">No</span></td>
<td id="S1.T1.3.1.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.3.1" class="ltx_text ltx_font_bold">No</span></td>
<td id="S1.T1.3.1.13.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.4.1" class="ltx_text ltx_font_bold">Server</span></td>
<td id="S1.T1.3.1.13.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.5.1" class="ltx_text ltx_font_bold">Not Required</span></td>
<td id="S1.T1.3.1.13.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.6.1" class="ltx_text ltx_font_bold">Any</span></td>
<td id="S1.T1.3.1.13.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.13.7.1" class="ltx_text ltx_font_bold">Yes</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The proposed <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> exhibits multifold merits compared to prior arts (Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):
(1) In contrast to public data-based FL methods, <span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> gets rid of the dependency on the availability of the desired public data, offering much more flexibility in its applications.
(2) Compared to other generative data-based approaches, the leverage of generative pre-trained models and the decoupling between synthetic data generation from the federated training process make the generated synthetic data in <span id="S1.p4.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> not impacted by private data distribution on the clients and the structure of the model to be trained.
(3) By leveraging the computational resources on the server, <span id="S1.p4.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> provides a much more efficient way to utilize external data by incorporating them into the pre-training of the downstream model, which significantly reduces the communication and computation costs of FL.
(4) The generation of downstream models using synthetic data takes place on the server. As such, it thereby eliminates the need for clients to bear any additional computational burden.
(5) Lastly, as <span id="S1.p4.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> does not alter the standard FL framework, it is fully compatible with secure aggregation protocols as in standard FL methods. More importantly, <span id="S1.p4.1.6" class="ltx_text ltx_font_typewriter">GPT-FL</span> does not introduce any additional hyper-parameters beyond the standard FL framework. This significantly simplifies the hyper-parameter optimization process, making <span id="S1.p4.1.7" class="ltx_text ltx_font_typewriter">GPT-FL</span> much more practically useful.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We evaluate the performance of <span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> by comparing it against state-of-the-art FL methods under three categories: standard FL methods, public data-based methods, and generated data-based methods on five datasets that cover both image and audio data modalities.
We highlight five of our findings:
(1) <span id="S1.p5.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> consistently outperforms state-of-the-art FL methods under both low and high data heterogeneity scenarios with significant advantages in communication and client sampling efficiency.
(2) Under a zero-shot setting, <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">i.e.</span> no real-world data is available, the downstream model after centralized training with synthetic images as part of <span id="S1.p5.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> achieves higher performance compared to the global model based on standard FL training with private data. On the contrary, the centralized training with synthetic audio performs worse than FL setups due to the impact of data modality and the quality of the generative pre-trained models.
(3) GPT-FL does not fully rely on generated data. Regardless of whether the target data falls within or outside the domain of the pre-trained generative model, <span id="S1.p5.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> can largely improve model performance beyond relying solely on private data in a standard FL framework.
(4) The downstream model generated by synthetic data controls gradient diversity during FL training, improving convergence speed and leading to significant accuracy gains with <span id="S1.p5.1.6" class="ltx_text ltx_font_typewriter">GPT-FL</span>.
(5) <span id="S1.p5.1.7" class="ltx_text ltx_font_typewriter">GPT-FL</span> effectively leverages existing pre-trained downstream models to improve performance in the FL setting, similar to methods under the standard FL framework.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Standard Federated Learning.</span>
In standard federated learning (FL), clients perform local model training on their private data whereas the central server aggregates these locally trained models to update a global model, which is then sent back for the next round of training. To enhance privacy, Secure Aggregation (SA) protocols <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>); So et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> have been proposed to encrypt each model update and reveal only the sum of the updates to the server. However, the performance of FL is jeopardized by client drift which is caused by the heterogeneity of private data distribution. To tackle this issue,
FedProx <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> introduces a proximal term to the local subproblem to constrain the local update closer to the global model; SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">Karimireddy et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> leverages a variance reduction technique to mitigate the effect of drifted local updates; and FedOpt <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> proposes to update the global model by applying a gradient-based server optimizer to the average of the clients’ model updates.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">FL with Public Data.</span>
To further mitigate client drift, recent studies propose to utilize public data (e.g., collected from the internet) in the process of federated training. For example, FedDF <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> leverages public data at the server to aggregate client models through knowledge distillation (KD). DS-FL <cite class="ltx_cite ltx_citemacro_cite">Itahara et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> proposes a similar approach based on semi-supervised FL.
MOON <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> proposes to use contrastive loss to further improve the performance.
Fed-ET <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> introduces a weight consensus distillation scheme using public data to train a large server model with smaller client models.
However, utilizing public data for FL has several limitations: the performance of FL heavily relies on the selected public data. However, it is unclear to which extent should the publish data be related to the training data to guarantee effective knowledge distillation, making it challenging to find appropriate public data for every use case <cite class="ltx_cite ltx_citemacro_cite">Stanton et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>); Alam et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>.
Moreover, the involvement of KD requires clients to send model weights to the server. This requirement makes it incompatible with secure aggregation protocols, making them vulnerable to backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>. Furthermore, some proposed methods <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Lin et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> require clients to process the public data. Such requirement adds an extra computational burden to clients.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">FL with Synthetic Data.</span>
To address the issues of public data-based approaches, FL methods based on synthetic data have been proposed <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.
In particular, FedGen <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> proposes to train a lightweight generator on the server using an ensemble of local models in a data-free manner. The generator is then sent to the clients to regularize local training. FedFTG <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> trains a GAN-based generator where the global model acts as the discriminator. The generated data are then used to fine-tune the global model on the server after model aggregation.
However, training of the generator relies heavily on the global model, which can lead to poor performance under high data heterogeneity.
Additionally, the quality of training the generator is impacted by the structure of the global model <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, making the quality of the synthetic data unstable during training. Furthermore, these approaches are limited to image-related tasks, restricting their applicability to other data modalities. Specifically, both FedGen and FedFTG rely on training MLP-based or GAN-based lightweight generator networks to ensemble user information in a data-free manner, where the lightweight generator may have limitations in generating high-fidelity data. In addition, the MLP-based model is impractical to model temporal structures to signals such as audio and speech. Finally, some approaches <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> could not support secure aggregation protocols due to the KD-based training, which could compromise the privacy of client data.
As an alternative, DynaFed <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> proposes to generate synthetic data via gradient inversion by applying multi-step parameter matching on global model trajectories and using the synthesized data to help aggregate the deflected clients into the global model.
However, using gradient inversion for generating synthetic data could encounter limitations when dealing with high-resolution images <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. In addition, this approach could not be directly used for other data modalities such as audio <cite class="ltx_cite ltx_citemacro_cite">Dang et al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>.
In this work, we propose <span id="S2.p3.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> as a solution to address these limitations.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>GPT-FL: Generative Pre-trained Model-Assisted Federated Learning</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The overall architecture of <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> is illustrated in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Create Prompts based on Label Names ‣ 3 GPT-FL: Generative Pre-trained Model-Assisted Federated Learning ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As shown, <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> consists of four steps.
First, prompts are created based on the label names at the server.
These prompts are then utilized to guide the generative pre-trained models to generate synthetic data. The server uses these generated synthetic data to train a downstream model and distributes the trained model to the clients.
Lastly, the clients use the trained model as the starting point, and finetune the model with their private data under the standard FL framework until it converges.
In the following, we describe the details in each step.
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Create Prompts based on Label Names</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">As the first step of <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>, a prompt that describes the desired content of the data is required to guide the synthetic data generation process. To do so, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> requires the clients to provide the set of label names<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>To protect user data privacy in FL setting, <span id="footnote1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> only requests the set of distinct label names instead of detailed label name distributions, and generates a uniform number of prompts for each label name.</span></span></span> of their private local data to generate prompts.
However, prior research <cite class="ltx_cite ltx_citemacro_cite">Shipard et al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>); He et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> shows that using only label names to generate prompts could restrict the quality and diversity of the generated synthetic data. Moreover, in FL, the server does not have access to detailed descriptions of the private data. To address these issues, <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> incorporates large language models (LLMs) such as GPT-3 to expand each input class’s details and use them as prompts for synthetic data generation.
As an example, for the label name "airplane", <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> uses the following query for the LLM to generate the prompt as follows:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<div id="S3.SS1.p2.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,UTogIiBfIF8gXyBfIGFpcnBsYW5lIF8gXyBfIF8iIFBsZWFzZSBmaWxsIGluIHRoZSBibGFuayBhbmQgbWFrZSBpdCBhcyBhIHByb21wdCB0byBnZW5lcmF0ZSB0aGUgaW1hZ2UKQTogTGFyZ2UgY29tbWVyY2lhbCBhaXJwbGFuZSBpbiB0aGUgYmx1ZSBza3kuCg==" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">Q</span><span id="lstnumberx1.2" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx1.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.4" class="ltx_text ltx_font_typewriter">"</span><span id="lstnumberx1.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.9" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.11" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.12" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.13" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter">airplane</span><span id="lstnumberx1.15" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.16" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.17" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.18" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.19" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.21" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.22" class="ltx_text ltx_lst_identifier ltx_font_typewriter">_</span><span id="lstnumberx1.23" class="ltx_text ltx_font_typewriter">"</span><span id="lstnumberx1.24" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.25" class="ltx_text ltx_lst_identifier ltx_font_typewriter">Please</span><span id="lstnumberx1.26" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.27" class="ltx_text ltx_lst_identifier ltx_font_typewriter">fill</span><span id="lstnumberx1.28" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.29" class="ltx_text ltx_lst_identifier ltx_font_typewriter">in</span><span id="lstnumberx1.30" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.31" class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span id="lstnumberx1.32" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.33" class="ltx_text ltx_lst_identifier ltx_font_typewriter">blank</span><span id="lstnumberx1.34" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.35" class="ltx_text ltx_lst_identifier ltx_font_typewriter">and</span><span id="lstnumberx1.36" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.37" class="ltx_text ltx_lst_identifier ltx_font_typewriter">make</span><span id="lstnumberx1.38" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.39" class="ltx_text ltx_lst_identifier ltx_font_typewriter">it</span><span id="lstnumberx1.40" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.41" class="ltx_text ltx_lst_identifier ltx_font_typewriter">as</span><span id="lstnumberx1.42" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.43" class="ltx_text ltx_lst_identifier ltx_font_typewriter">a</span><span id="lstnumberx1.44" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.45" class="ltx_text ltx_lst_identifier ltx_font_typewriter">prompt</span><span id="lstnumberx1.46" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.47" class="ltx_text ltx_lst_identifier ltx_font_typewriter">to</span><span id="lstnumberx1.48" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.49" class="ltx_text ltx_lst_identifier ltx_font_typewriter">generate</span><span id="lstnumberx1.50" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.51" class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span id="lstnumberx1.52" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx1.53" class="ltx_text ltx_lst_identifier ltx_font_typewriter">image</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">A</span><span id="lstnumberx2.2" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx2.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">Large</span><span id="lstnumberx2.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter">commercial</span><span id="lstnumberx2.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">airplane</span><span id="lstnumberx2.9" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter">in</span><span id="lstnumberx2.11" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.12" class="ltx_text ltx_lst_identifier ltx_font_typewriter">the</span><span id="lstnumberx2.13" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter">blue</span><span id="lstnumberx2.15" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx2.16" class="ltx_text ltx_lst_identifier ltx_font_typewriter">sky</span><span id="lstnumberx2.17" class="ltx_text ltx_font_typewriter">.</span>
</div>
</div>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">Moreover, inspired by <cite class="ltx_cite ltx_citemacro_cite">Shipard et al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>, we randomly set the unconditional guidance scale of the Stable Diffusion model between 1 and 5 to further enrich the data diversity. In addition to the aforementioned techniques, it is worth noting that <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> is flexible and compatible with other prompt engineering techniques that can be used to generate diversified synthetic data.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">It should be noted that <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> can employ Invertible Bloom Lookup Tables (IBLT) to encode label names before sending them to the server so that the label information of each client is not leaked to the server <cite class="ltx_cite ltx_citemacro_cite">Gascón et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. Specifically, each client locally encodes its unique label names into IBLT, a probabilistic data structure that can encode items in an open domain efficiently. The server linearly aggregates these IBLTs via the secure aggregation <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> and decodes the aggregated table for the union of unique label names without revealing individual label information. More details about IBLT in <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> are provided in Appendix <a href="#A1" title="Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> via an illustrative experiment.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2306.02210/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of <span id="S3.F1.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Generate Synthetic Data from Generative Pre-trained Model</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Next, the generated prompts are used as the inputs to the generative pre-trained models to generate synthetic data.
In this work, we utilize the state-of-the-art Latent Diffusion Model <cite class="ltx_cite ltx_citemacro_cite">Rombach et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> loaded with Stable Diffusion V2.1 weights to generate synthetic images for image-based FL applications; and we utilize the state-of-the-art SpeechT5 model <cite class="ltx_cite ltx_citemacro_cite">Ao et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> for text-to-speech and AudioLDM model <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> for text-to-audio to generate synthetic speech and audio data, respectively.
It should be noted that the proposed <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> is a general framework that supports other generative pre-trained models and data modalities beyond images and audio.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Train Downstream Model on Generated Synthetic Data</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">With the generated synthetic data, <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> trains a downstream model on the server in a centralized manner, and distributes the trained model to the clients participated in FL. This trained model acts as the initialized model for the following federated training process.
One note should be emphasized from our empirical experiences is that training with synthetic data is prone to overfitting, as synthetic data tend to be highly patternized compared to real data. To mitigate the effects of overfitting, we adopt relatively large weight decay hyperparameters and small learning rates compared to training with real data. The detailed hyper-parameter selections are listed in Appendix <a href="#A1" title="Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Finetune Trained Downstream Model on Private Client Data with FL</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">Lastly, the clients use the trained model distributed from the server as the starting point, and finetune the model with their private data under the standard FL framework until the finetuning converges.
As such, <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> does not alter the standard FL framework, making it fully compatible with secure aggregation protocols as in standard FL methods.
More importantly,
unlike existing generated data-based approaches <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> does not introduce any additional hyper-parameters beyond the standard FL framework. This significantly simplifies the hyper-parameter optimization process, making <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> much more practically useful.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Datasets, Models, and Tasks.</span> We evaluate the performance of <span id="S4.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> on five datasets from three FL applications: image classification, speech keyword spotting, and environmental sound classification.
For image classification, we conduct experiments on CIFAR-10, CIFAR-100 <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite>, and Oxford 102 Flower <cite class="ltx_cite ltx_citemacro_cite">Nilsback &amp; Zisserman (<a href="#bib.bib24" title="" class="ltx_ref">2008</a>)</cite> using ConvNet <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, ResNet18, ResNet50 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite>, and VGG19 <cite class="ltx_cite ltx_citemacro_cite">Simonyan &amp; Zisserman (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite>. Among them, CIFAR-10 and CIFAR-100 contain images from diverse objects whereas Oxford 102 Flower only contains images of flowers but with higher resolutions for fine-grained classification.
For audio-related tasks, we choose the Google Command speech dataset <cite class="ltx_cite ltx_citemacro_cite">Warden (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite> for keyword spotting and ESC-50 dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">Piczak </a></cite> for environmental sound classification. We followed the previous work <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> to use the same model for these two datasets.
More detailed information about the data-preprocessing method and model setups is described in Appendix.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.4" class="ltx_p"><span id="S4.p2.4.1" class="ltx_text ltx_font_bold">Data Heterogeneity.</span> For CIFAR-10 and CIFAR-100, the training dataset is partitioned heterogeneously amongst 100 clients using the Dirichlet distribution <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="Dir_{K}(\alpha)" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.2" xref="S4.p2.1.m1.1.2.cmml"><mi id="S4.p2.1.m1.1.2.2" xref="S4.p2.1.m1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.2.1" xref="S4.p2.1.m1.1.2.1.cmml">​</mo><mi id="S4.p2.1.m1.1.2.3" xref="S4.p2.1.m1.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.2.1a" xref="S4.p2.1.m1.1.2.1.cmml">​</mo><msub id="S4.p2.1.m1.1.2.4" xref="S4.p2.1.m1.1.2.4.cmml"><mi id="S4.p2.1.m1.1.2.4.2" xref="S4.p2.1.m1.1.2.4.2.cmml">r</mi><mi id="S4.p2.1.m1.1.2.4.3" xref="S4.p2.1.m1.1.2.4.3.cmml">K</mi></msub><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.2.1b" xref="S4.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S4.p2.1.m1.1.2.5.2" xref="S4.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S4.p2.1.m1.1.2.5.2.1" xref="S4.p2.1.m1.1.2.cmml">(</mo><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">α</mi><mo stretchy="false" id="S4.p2.1.m1.1.2.5.2.2" xref="S4.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.2.cmml" xref="S4.p2.1.m1.1.2"><times id="S4.p2.1.m1.1.2.1.cmml" xref="S4.p2.1.m1.1.2.1"></times><ci id="S4.p2.1.m1.1.2.2.cmml" xref="S4.p2.1.m1.1.2.2">𝐷</ci><ci id="S4.p2.1.m1.1.2.3.cmml" xref="S4.p2.1.m1.1.2.3">𝑖</ci><apply id="S4.p2.1.m1.1.2.4.cmml" xref="S4.p2.1.m1.1.2.4"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.2.4.1.cmml" xref="S4.p2.1.m1.1.2.4">subscript</csymbol><ci id="S4.p2.1.m1.1.2.4.2.cmml" xref="S4.p2.1.m1.1.2.4.2">𝑟</ci><ci id="S4.p2.1.m1.1.2.4.3.cmml" xref="S4.p2.1.m1.1.2.4.3">𝐾</ci></apply><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">Dir_{K}(\alpha)</annotation></semantics></math> with <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\alpha</annotation></semantics></math> equal to 0.1 and 0.5 following the previous work <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. With the same method, we partition Flowers102 into 50 subsets due to its relatively small size.
For audio datasets, Google Speech Command is partitioned over speaker IDs, making the dataset naturally non-IID distributed. It contains a total of 105,829 audio recordings collected from 2,618 speakers. The training set includes the recordings from 2,112 speakers and the test set includes the rest. To create non-IID data distributions on ESC-50, we followed the previous work <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> to partition ESC-50 into 100 subsets using <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="Dir_{K}(\alpha)" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.2" xref="S4.p2.3.m3.1.2.cmml"><mi id="S4.p2.3.m3.1.2.2" xref="S4.p2.3.m3.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.2.1" xref="S4.p2.3.m3.1.2.1.cmml">​</mo><mi id="S4.p2.3.m3.1.2.3" xref="S4.p2.3.m3.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.2.1a" xref="S4.p2.3.m3.1.2.1.cmml">​</mo><msub id="S4.p2.3.m3.1.2.4" xref="S4.p2.3.m3.1.2.4.cmml"><mi id="S4.p2.3.m3.1.2.4.2" xref="S4.p2.3.m3.1.2.4.2.cmml">r</mi><mi id="S4.p2.3.m3.1.2.4.3" xref="S4.p2.3.m3.1.2.4.3.cmml">K</mi></msub><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.2.1b" xref="S4.p2.3.m3.1.2.1.cmml">​</mo><mrow id="S4.p2.3.m3.1.2.5.2" xref="S4.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S4.p2.3.m3.1.2.5.2.1" xref="S4.p2.3.m3.1.2.cmml">(</mo><mi id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">α</mi><mo stretchy="false" id="S4.p2.3.m3.1.2.5.2.2" xref="S4.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.2.cmml" xref="S4.p2.3.m3.1.2"><times id="S4.p2.3.m3.1.2.1.cmml" xref="S4.p2.3.m3.1.2.1"></times><ci id="S4.p2.3.m3.1.2.2.cmml" xref="S4.p2.3.m3.1.2.2">𝐷</ci><ci id="S4.p2.3.m3.1.2.3.cmml" xref="S4.p2.3.m3.1.2.3">𝑖</ci><apply id="S4.p2.3.m3.1.2.4.cmml" xref="S4.p2.3.m3.1.2.4"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.2.4.1.cmml" xref="S4.p2.3.m3.1.2.4">subscript</csymbol><ci id="S4.p2.3.m3.1.2.4.2.cmml" xref="S4.p2.3.m3.1.2.4.2">𝑟</ci><ci id="S4.p2.3.m3.1.2.4.3.cmml" xref="S4.p2.3.m3.1.2.4.3">𝐾</ci></apply><ci id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">Dir_{K}(\alpha)</annotation></semantics></math> with <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\alpha</annotation></semantics></math> equal to 0.1.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Baselines and Evaluation Metrics.</span> We compare <span id="S4.p3.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> against three categories of baselines: 1) standard FL methods without the use of public or generated synthetic data – FedAvg, FedProx, and Scaffold; 2) FL methods that involve the use of public data – MOON, FedDF, DS-FL, and Fed-ET; and 3) FL methods that utilize generated synthetic data – FedGen and DynaFed<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We did not compare with FedFTG because its code is not open-source, and we could not reproduce their results following the paper.</span></span></span>.
We use the test accuracy of the trained model as our evaluation metric.
We run experiments with three different random seeds and report the average and standard deviation. The details of the hyper-parameter selection of each dataset and experiment are described in Appendix.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model accuracy comparison between <span id="S4.T2.4.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> and existing FL methods. For public data-based methods MOON, FedDF, DS-FL and Fed-ET, the results on CIFAR-10 and CIFAR-100 are obtained from <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, and the results on Flowers102 are marked as N/A given the practical challenge on finding a set of suitable public data that can boost its performance.</figcaption>
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:156.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.1pt,38.9pt) scale(0.667231145719584,0.667231145719584) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.3.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.2.4.1" class="ltx_text">
<span id="S4.T2.2.2.2.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.2.2.2.4.1.1.1" class="ltx_tr">
<span id="S4.T2.2.2.2.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.2.2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S4.T2.2.2.2.4.1.1.2" class="ltx_tr">
<span id="S4.T2.2.2.2.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.2.2.4.1.1.2.1.1" class="ltx_text ltx_font_bold">Model</span></span></span>
</span></span></td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">
<span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">High Data Heterogeneity</span> (<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{\alpha=0.1}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">α</mi><mo id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">=</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><eq id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></eq><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝛼</ci><cn type="float" id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\mathbf{\alpha=0.1}</annotation></semantics></math>)</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">
<span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_font_bold">Low Data Heterogeneity</span> (<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathbf{\alpha=0.5}" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml">α</mi><mo id="S4.T2.2.2.2.2.m1.1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.1.cmml">=</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><eq id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.1"></eq><ci id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2">𝛼</ci><cn type="float" id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\mathbf{\alpha=0.5}</annotation></semantics></math>)</td>
</tr>
<tr id="S4.T2.2.2.3" class="ltx_tr">
<td id="S4.T2.2.2.3.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.2.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="S4.T2.2.2.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.3.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="S4.T2.2.2.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.4.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
<td id="S4.T2.2.2.3.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.6.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="S4.T2.2.2.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.7.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="S4.T2.2.2.3.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.2.2.3.8.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
</tr>
<tr id="S4.T2.2.2.4" class="ltx_tr">
<td id="S4.T2.2.2.4.1" class="ltx_td ltx_align_center ltx_border_t">FedAvg</td>
<td id="S4.T2.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T2.2.2.4.2.1" class="ltx_text">VGG19</span></td>
<td id="S4.T2.2.2.4.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">71.19 (± 0.27)</td>
<td id="S4.T2.2.2.4.5" class="ltx_td ltx_align_center ltx_border_t">30.21 (± 0.32)</td>
<td id="S4.T2.2.2.4.6" class="ltx_td ltx_align_center ltx_border_t">30.30 (± 0.16)</td>
<td id="S4.T2.2.2.4.7" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.4.8" class="ltx_td ltx_align_center ltx_border_t">74.82 (± 0.23)</td>
<td id="S4.T2.2.2.4.9" class="ltx_td ltx_align_center ltx_border_t">33.12 (± 0.13)</td>
<td id="S4.T2.2.2.4.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">34.75 (± 0.90)</td>
</tr>
<tr id="S4.T2.2.2.5" class="ltx_tr">
<td id="S4.T2.2.2.5.1" class="ltx_td ltx_align_center">FedProx</td>
<td id="S4.T2.2.2.5.2" class="ltx_td"></td>
<td id="S4.T2.2.2.5.3" class="ltx_td ltx_align_center">72.45 (± 0.13)</td>
<td id="S4.T2.2.2.5.4" class="ltx_td ltx_align_center">31.51 (± 0.11)</td>
<td id="S4.T2.2.2.5.5" class="ltx_td ltx_align_center">33.23 (± 0.24)</td>
<td id="S4.T2.2.2.5.6" class="ltx_td"></td>
<td id="S4.T2.2.2.5.7" class="ltx_td ltx_align_center">75.24 (± 0.19)</td>
<td id="S4.T2.2.2.5.8" class="ltx_td ltx_align_center">33.64 (± 0.08)</td>
<td id="S4.T2.2.2.5.9" class="ltx_td ltx_nopad_r ltx_align_center">40.56 (± 0.19)</td>
</tr>
<tr id="S4.T2.2.2.6" class="ltx_tr">
<td id="S4.T2.2.2.6.1" class="ltx_td ltx_align_center">SCAFFOLD</td>
<td id="S4.T2.2.2.6.2" class="ltx_td"></td>
<td id="S4.T2.2.2.6.3" class="ltx_td ltx_align_center">75.12 (± 0.20)</td>
<td id="S4.T2.2.2.6.4" class="ltx_td ltx_align_center">30.61 (± 0.57)</td>
<td id="S4.T2.2.2.6.5" class="ltx_td ltx_align_center">26.75 (± 0.50)</td>
<td id="S4.T2.2.2.6.6" class="ltx_td"></td>
<td id="S4.T2.2.2.6.7" class="ltx_td ltx_align_center">78.69 (± 0.15)</td>
<td id="S4.T2.2.2.6.8" class="ltx_td ltx_align_center">34.91 (± 0.61)</td>
<td id="S4.T2.2.2.6.9" class="ltx_td ltx_nopad_r ltx_align_center">33.21 (± 0.41)</td>
</tr>
<tr id="S4.T2.2.2.7" class="ltx_tr">
<td id="S4.T2.2.2.7.1" class="ltx_td ltx_align_center ltx_border_t">MOON</td>
<td id="S4.T2.2.2.7.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T2.2.2.7.2.1" class="ltx_text">VGG19</span></td>
<td id="S4.T2.2.2.7.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.7.4" class="ltx_td ltx_align_center ltx_border_t">75.68 (± 0.51)</td>
<td id="S4.T2.2.2.7.5" class="ltx_td ltx_align_center ltx_border_t">33.72 (± 0.89)</td>
<td id="S4.T2.2.2.7.6" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.2.2.7.7" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.7.8" class="ltx_td ltx_align_center ltx_border_t">81.17 (± 0.41)</td>
<td id="S4.T2.2.2.7.9" class="ltx_td ltx_align_center ltx_border_t">42.15 (± 0.72)</td>
<td id="S4.T2.2.2.7.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">N/A</td>
</tr>
<tr id="S4.T2.2.2.8" class="ltx_tr">
<td id="S4.T2.2.2.8.1" class="ltx_td ltx_align_center">FedDF</td>
<td id="S4.T2.2.2.8.2" class="ltx_td"></td>
<td id="S4.T2.2.2.8.3" class="ltx_td ltx_align_center">73.81 (± 0.42)</td>
<td id="S4.T2.2.2.8.4" class="ltx_td ltx_align_center">31.87 (± 0.46)</td>
<td id="S4.T2.2.2.8.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.2.2.8.6" class="ltx_td"></td>
<td id="S4.T2.2.2.8.7" class="ltx_td ltx_align_center">76.55 (± 0.32)</td>
<td id="S4.T2.2.2.8.8" class="ltx_td ltx_align_center">37.87 (± 0.31)</td>
<td id="S4.T2.2.2.8.9" class="ltx_td ltx_nopad_r ltx_align_center">N/A</td>
</tr>
<tr id="S4.T2.2.2.9" class="ltx_tr">
<td id="S4.T2.2.2.9.1" class="ltx_td ltx_align_center">DS-FL</td>
<td id="S4.T2.2.2.9.2" class="ltx_td"></td>
<td id="S4.T2.2.2.9.3" class="ltx_td ltx_align_center">65.27 (± 0.53)</td>
<td id="S4.T2.2.2.9.4" class="ltx_td ltx_align_center">29.12 (± 0.51)</td>
<td id="S4.T2.2.2.9.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.2.2.9.6" class="ltx_td"></td>
<td id="S4.T2.2.2.9.7" class="ltx_td ltx_align_center">68.44 (± 0.47)</td>
<td id="S4.T2.2.2.9.8" class="ltx_td ltx_align_center">33.56 (± 0.55)</td>
<td id="S4.T2.2.2.9.9" class="ltx_td ltx_nopad_r ltx_align_center">N/A</td>
</tr>
<tr id="S4.T2.2.2.10" class="ltx_tr">
<td id="S4.T2.2.2.10.1" class="ltx_td ltx_align_center">Fed-ET</td>
<td id="S4.T2.2.2.10.2" class="ltx_td"></td>
<td id="S4.T2.2.2.10.3" class="ltx_td ltx_align_center">78.66 (± 0.31)</td>
<td id="S4.T2.2.2.10.4" class="ltx_td ltx_align_center">35.78 (± 0.45)</td>
<td id="S4.T2.2.2.10.5" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T2.2.2.10.6" class="ltx_td"></td>
<td id="S4.T2.2.2.10.7" class="ltx_td ltx_align_center">81.13 (± 0.28)</td>
<td id="S4.T2.2.2.10.8" class="ltx_td ltx_align_center">41.58 (± 0.36)</td>
<td id="S4.T2.2.2.10.9" class="ltx_td ltx_nopad_r ltx_align_center">N/A</td>
</tr>
<tr id="S4.T2.2.2.11" class="ltx_tr">
<td id="S4.T2.2.2.11.1" class="ltx_td ltx_align_center ltx_border_t">FedGen</td>
<td id="S4.T2.2.2.11.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.2.2.11.2.1" class="ltx_text">ConvNet <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> only reported results on ConvNet. We tested these two methods on VGG19 but they are not converged.</span></span></span></span></td>
<td id="S4.T2.2.2.11.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.11.4" class="ltx_td ltx_align_center ltx_border_t">42.05 (± 0.93)</td>
<td id="S4.T2.2.2.11.5" class="ltx_td ltx_align_center ltx_border_t">26.64 (± 0.66)</td>
<td id="S4.T2.2.2.11.6" class="ltx_td ltx_align_center ltx_border_t">Not Converged</td>
<td id="S4.T2.2.2.11.7" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.11.8" class="ltx_td ltx_align_center ltx_border_t">54.86 (± 0.13)</td>
<td id="S4.T2.2.2.11.9" class="ltx_td ltx_align_center ltx_border_t">34.03 (± 0.42)</td>
<td id="S4.T2.2.2.11.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">Not Converged</td>
</tr>
<tr id="S4.T2.2.2.12" class="ltx_tr">
<td id="S4.T2.2.2.12.1" class="ltx_td ltx_align_center">DynaFed</td>
<td id="S4.T2.2.2.12.2" class="ltx_td"></td>
<td id="S4.T2.2.2.12.3" class="ltx_td ltx_align_center">71.59 (± 0.10)</td>
<td id="S4.T2.2.2.12.4" class="ltx_td ltx_align_center">36.08 (± 0.15)</td>
<td id="S4.T2.2.2.12.5" class="ltx_td ltx_align_center">Not Converged</td>
<td id="S4.T2.2.2.12.6" class="ltx_td"></td>
<td id="S4.T2.2.2.12.7" class="ltx_td ltx_align_center">75.66 (± 0.21)</td>
<td id="S4.T2.2.2.12.8" class="ltx_td ltx_align_center">43.82 (± 0.30)</td>
<td id="S4.T2.2.2.12.9" class="ltx_td ltx_nopad_r ltx_align_center">Not Converged</td>
</tr>
<tr id="S4.T2.2.2.13" class="ltx_tr">
<td id="S4.T2.2.2.13.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T2.2.2.13.1.1" class="ltx_text ltx_font_bold">GPT-FL</span></td>
<td id="S4.T2.2.2.13.2" class="ltx_td ltx_align_center ltx_border_t">VGG19</td>
<td id="S4.T2.2.2.13.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.13.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.4.1" class="ltx_text ltx_font_bold">82.16 (± 0.13)</span></td>
<td id="S4.T2.2.2.13.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.5.1" class="ltx_text ltx_font_bold">47.80 (± 0.32)</span></td>
<td id="S4.T2.2.2.13.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.6.1" class="ltx_text ltx_font_bold">70.56 (± 0.34)</span></td>
<td id="S4.T2.2.2.13.7" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.2.2.13.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.8.1" class="ltx_text ltx_font_bold">82.17 (± 0.20)</span></td>
<td id="S4.T2.2.2.13.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.9.1" class="ltx_text ltx_font_bold">48.39 (± 0.17)</span></td>
<td id="S4.T2.2.2.13.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.2.2.13.10.1" class="ltx_text ltx_font_bold">74.84 (± 0.43)</span></td>
</tr>
<tr id="S4.T2.2.2.14" class="ltx_tr">
<td id="S4.T2.2.2.14.1" class="ltx_td ltx_align_center ltx_border_bb">ConvNet</td>
<td id="S4.T2.2.2.14.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.2.2.14.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.3.1" class="ltx_text ltx_font_bold">72.62 (± 0.24)</span></td>
<td id="S4.T2.2.2.14.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.4.1" class="ltx_text ltx_font_bold">42.66 (± 0.19)</span></td>
<td id="S4.T2.2.2.14.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.5.1" class="ltx_text ltx_font_bold">37.91 (± 0.43)</span></td>
<td id="S4.T2.2.2.14.6" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.2.2.14.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.7.1" class="ltx_text ltx_font_bold">77.18 (± 0.21)</span></td>
<td id="S4.T2.2.2.14.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.8.1" class="ltx_text ltx_font_bold">47.89 (± 0.28)</span></td>
<td id="S4.T2.2.2.14.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.14.9.1" class="ltx_text ltx_font_bold">48.61 (± 0.51)</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Performance Comparison with State-of-the-Art FL Methods</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">First, we compare the performance of <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> with state-of-the-art FL methods.
To enforce fair comparisons, in this experiment, we choose to evaluate on the three image datasets (CIFAR-10, CIFAR-100 and Flowers102) since baseline methods MOON, FedGen and DynaFed only support image data.
Moreover, we used the same models (VGG19 and ConvNet) and experiment settings as previous work <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>); Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.
In each communication round, We randomly sample 10 clients from 100 clients for CIFAR and use all 50 clients for Flowers102. We choose FedAvg as the FL optimizer. All the training starts from random initialization and total number of communication rounds is set to 500.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Overall Performance.</span>
Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes our results.
We make three key observations:
(1) <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> consistently outperforms all the baselines we selected in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> under both low and high data heterogeneity scenarios across all three datasets.
(2) In direct comparison with state-of-the-art generated data-based FL methods, although FedGen and DynaFed perform reasonably well on CIFAR-10 and CIFAR-100, they do not converge on Flowers102 whose images have higher resolutions than CIFAR. Moreover, both FedGen and DynaFed fail to converge when training a larger VGG19 model on Flowers102 and even lower-resolution CIFAR-10/100. In contrast, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> not only converges but also achieves state-of-the-art accuracy on Flowers102. More importantly, <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> is able to support larger model, and its accuracy is significantly higher than the smaller ConvNet.
(3) For Flowers102, as both public data-based and generated data-based FL methods are confronted with challenges, the only viable options are standard FL methods and <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span>. As shown, with the same model, <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">GPT-FL</span> outperforms standard FL methods by a significant margin.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/figures/comm.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Communication costs of standard FL methods, public data-based methods and <span id="S4.F4.1.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> to achieve the target test accuracy.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/figures/comm_syn.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Communication costs of generated data-based methods and <span id="S4.F4.2.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> to achieve the target test accuracy.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/figures/comp_sample.png" id="S4.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Test accuracy of <span id="S4.F4.3.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> for CIFAR-10/100 under different client sampling rates.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Communication Efficiency.</span>
Besides model accuracy, we also compare the communication costs of <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> with existing FL methods on CIFAR-10/100 under high data heterogeneity, where communication cost is measured as the total number of model parameters communicated between the server and clients during federated training until reaching a target model test accuracy.
Specifically, Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the communication cost comparison between standard FL methods, public data-based methods<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We do not compare with FedDF and DS-FL as they do not achieve competitive model accuracy.</span></span></span>, and <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> under VGG19;
and Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the communication cost comparison between generative data-based methods and <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> under ConvNet.
The target test accuracies in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are set to be lower given the low accuracies achieved by FedGen.
As shown, <span id="S4.SS1.p3.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> has the least communication cost among all the methods, achieving up to 94% communication reduction compared to the best-performed public data-based baseline Fed-ET and 98% communication reduction compared to the best-performed generated data-based baseline DynaFed. These results highlight the significant advantage of <span id="S4.SS1.p3.1.6" class="ltx_text ltx_font_typewriter">GPT-FL</span> in communication reduction over state-of-the-art FL methods.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Client Sampling Efficiency.</span>
One critical hyper-parameter of FL is the client sampling rate in each communication round during the federated training process.
In Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we plot the test model accuracies obtained by <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> under low, medium, and high client sampling rates on CIFAR-10/100 with VGG19 under high data heterogeneity.
As shown, even with a single participating client per round, <span id="S4.SS1.p4.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> is able to achieve 80.44% and 43.07% test accuracy on CIFAR-10 and CIFAR-100 respectively. This performance already surpasses all the other FL methods listed in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which employs 9 times more clients for training per round.
These results highlight the significant advantage of <span id="S4.SS1.p4.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> in client sampling efficiency over state-of-the-art FL methods, making <span id="S4.SS1.p4.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> a very attractive solution in challenging scenarios where not many clients can participate at the same time.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy performance of the generated downstream model and standard FL on benchmark datasets. "1x Synthetic" represents the size of synthetic data is one time as the real data.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:82pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-65.4pt,13.4pt) scale(0.752277054084778,0.752277054084778) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">1x Synthetic</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">2x Synthetic</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">3x Synthetic</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">FedOpt</span></td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T3.1.1.2.1.1" class="ltx_text">Image Data</span></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">61.48 (± 0.08)</td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">67.41 (± 0.40)</td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.5.1" class="ltx_text ltx_font_bold">75.65 (± 0.09)</span></td>
<td id="S4.T3.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">64.48 (± 0.13)</td>
<td id="S4.T3.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">72.68 (± 0.22)</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-100</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">24.70 (± 0.00)</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">33.41 (± 0.01)</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.4.1" class="ltx_text ltx_font_bold">41.76 (± 0.03)</span></td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">25.89 (± 0.67)</td>
<td id="S4.T3.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">20.85 (± 0.14)</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_center ltx_border_t">Flowers102</td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">24.94 (± 0.57)</td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">28.26 (± 0.14)</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.4.4.1" class="ltx_text ltx_font_bold">31.29 (± 0.18)</span></td>
<td id="S4.T3.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">30.30 (± 0.16)</td>
<td id="S4.T3.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t">26.43 (± 0.09)</td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T3.1.1.5.1.1" class="ltx_text">Audio Data</span></td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">Google Command</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">24.78 (± 0.04)</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">25.65 (± 0.07)</td>
<td id="S4.T3.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">26.24 (± 0.01)</td>
<td id="S4.T3.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">73.68 (± 0.49)</td>
<td id="S4.T3.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.5.7.1" class="ltx_text ltx_font_bold">83.01 (± 0.23)</span></td>
</tr>
<tr id="S4.T3.1.1.6" class="ltx_tr">
<td id="S4.T3.1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">ESC-50</td>
<td id="S4.T3.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">6.89 (± 0.29)</td>
<td id="S4.T3.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.68 (± 0.35)</td>
<td id="S4.T3.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">12.72 (± 0.31)</td>
<td id="S4.T3.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">22.76 (± 1.01)</td>
<td id="S4.T3.1.1.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.6.6.1" class="ltx_text ltx_font_bold">32.49 (± 0.57)</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Understanding GPT-FL</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">(1) Can we only rely on centralized training with synthetic data to achieve competitive results compared to Federated Learning with private data?</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">To answer this question, we compare the model performance between generated downstream model by centralized training with synthetic data and the global model by standard FL training with private data on both image and audio benchmark datasets. Different from the previous section, we select ResNet18 and ResNet50 models for CIFAR-10 and CIFAR-100 dataset, respectively.
We choose the models proposed in the FedAudio Benchmark <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> for audio tasks.
We report the best F1 score for the audio datasets. The results are summarized in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Impact of Out-of-Domain Data Generation.</span>
We choose the ESC-50 and Google Speech Commands datasets to examine the impact of out-of-domain data generation for the generative pre-trained model. We did not conduct a similar analysis for the image datasets as the LAION-5B <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> open-source dataset for training the Stable Diffusion model we used is a vast collection of publicly available datasets, including nearly all relevant ones for our experiments.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p">Our experiments show that synthetic image outperforms synthetic audio regarding model performance when using centralized training. We observed that centralized training with synthetic images achieves higher accuracy than FL setups for all three image benchmark datasets. In contrast, centralized training with synthetic audio performs worse than FL setups for ESC-50 and Google Speech Command datasets. The finding from the Google Speech Command experiments aligns with the previous study <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite> that utilizes pure synthetic speech data to train the automatic speech recognition system leading to substantial performance degradation. One plausible explanation is related to the relatively small training data sizes (approximately 400M sentences) and constrained domain knowledge (book corpus) compared to training other generative pre-trained models like Stable Diffusion. For example, using human inspection, we discovered that the TTS model fails to synthesize simple spoken words like "house". This deficiency may originate from the lack of short-spoken utterance samples in training data. In addition, the synthesized speech often lacks diversity due to the limited range of speakers represented in the training dataset. On the other hand, there is insufficient knowledge of the audio generation models, making the model performance of using the synthesized audio data as training data remains unknown. However, our manual inspection revealed that the model frequently encounters difficulties in generating audio samples, such as generated audio related to water sounds often sounds like music. This issue could be largely associated with the relatively small data size in pre-training compared to other foundation models.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/figures/flower.png" id="S4.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Impact of synthetic data sample number to the generated downstream model.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/x2.png" id="S4.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="422" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Smoothed Gradient diversity of client updates during training on Google speech commands dataset.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F7.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:127.2pt;"><img src="/html/2306.02210/assets/x3.png" id="S4.F7.3.g1" class="ltx_graphics ltx_img_landscape" width="422" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Learning curve of the global model during training on Google speech commands dataset.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Impact of Numbers of Synthetic Data.</span> With both image and audio data, one commonality is centralized training with synthetic data can benefit from increasing the number of synthetic data. To validate this finding, we test the impact of numbers of synthetic data on the performance of the generated model on the Flowers102 dataset, where we increase the size of the synthetic data up to ten times that of the real data. As shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, our experimental results demonstrate that as we enlarge the amount of synthetic data, the performance of the model improves. One justification for this finding is that enlarging the number of synthetic data enriches the diversity and increases overlap between the synthetic and real data, allowing the model to learn more robust and generalizable features. Even if the data is generated randomly by the label name without any other diversity-enriching guidance from the real data, with more synthetic data, there is an increasing chance that some of these additional synthetic data overlap with the real data, allowing the model to perform better on the real test data.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">(2) What benefits does <span id="S4.SS2.p6.1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> bring?</span></p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p">We explore the benefits that <span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> provides for custom models that are built on top of downstream models generated from synthetic data. Specifically, we want to examine how fine-tuning these downstream models with private data under the FL framework can lead to performance improvements. To demonstrate how <span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> can be integrated with existing FL server optimizers, we evaluate its performance with both FedAvg and FedOpt as the server aggregator. Our experimental results are presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy comparison between generated downstream model, standard FL and <span id="S4.T4.11.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>. Differ from the experiments shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the CIFAR-10 and Flowers102 datasets are trained with ResNet18 model and the CIFAR-100 dataset is trained with ResNet50 model. "<math id="S4.T4.2.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S4.T4.2.m1.1b"><mi mathvariant="normal" id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><ci id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">\Delta</annotation></semantics></math>Metric" represents the accuracy increment by <span id="S4.T4.12.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> on top of the generated downstream model.</figcaption>
<div id="S4.T4.8.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:76pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-86.2pt,16.3pt) scale(0.6975959968639,0.6975959968639) ;">
<table id="S4.T4.8.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.3.1.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T4.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.3.1" class="ltx_text ltx_font_bold">3x Synthetic</span></td>
<td id="S4.T4.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.4.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T4.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.5.1" class="ltx_text ltx_font_bold">FedOpt</span></td>
<td id="S4.T4.3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">GPT-FL<span id="S4.T4.3.1.1.1.6.1.1" class="ltx_text ltx_font_serif"> w/ FedAvg</span></span></td>
<td id="S4.T4.3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.3.1.1.1.7.1" class="ltx_text ltx_font_typewriter ltx_font_bold">GPT-FL<span id="S4.T4.3.1.1.1.7.1.1" class="ltx_text ltx_font_serif"> w/ FedOpt</span></span></td>
<td id="S4.T4.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S4.T4.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S4.T4.3.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T4.3.1.1.1.1.m1.1.1" xref="S4.T4.3.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.1.1.m1.1b"><ci id="S4.T4.3.1.1.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.1.1.m1.1c">\Delta</annotation></semantics></math><span id="S4.T4.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Metric</span>
</td>
</tr>
<tr id="S4.T4.4.2.2.2" class="ltx_tr">
<td id="S4.T4.4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S4.T4.4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">75.65 (± 0.09)</td>
<td id="S4.T4.4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">64.48 (± 0.13)</td>
<td id="S4.T4.4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">72.68 (± 0.22)</td>
<td id="S4.T4.4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.2.2.2.6.1" class="ltx_text ltx_font_bold">81.38 (± 0.05)</span></td>
<td id="S4.T4.4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">79.08 (± 0.17)</td>
<td id="S4.T4.4.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.4.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathbf{\uparrow}" display="inline"><semantics id="S4.T4.4.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T4.4.2.2.2.1.m1.1.1" xref="S4.T4.4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.2.1.m1.1b"><ci id="S4.T4.4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.2.1.m1.1c">\mathbf{\uparrow}</annotation></semantics></math> <span id="S4.T4.4.2.2.2.1.1" class="ltx_text ltx_font_bold">5.73</span>
</td>
</tr>
<tr id="S4.T4.5.3.3.3" class="ltx_tr">
<td id="S4.T4.5.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">CIFAR-100</td>
<td id="S4.T4.5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">41.76 (± 0.03)</td>
<td id="S4.T4.5.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">25.89 (± 0.67)</td>
<td id="S4.T4.5.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">20.85 (± 0.14)</td>
<td id="S4.T4.5.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.3.3.3.6.1" class="ltx_text ltx_font_bold">62.83 (± 0.31)</span></td>
<td id="S4.T4.5.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">48.80 (± 0.12)</td>
<td id="S4.T4.5.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.5.3.3.3.1.m1.1" class="ltx_Math" alttext="\mathbf{\uparrow}" display="inline"><semantics id="S4.T4.5.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T4.5.3.3.3.1.m1.1.1" xref="S4.T4.5.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.3.3.1.m1.1b"><ci id="S4.T4.5.3.3.3.1.m1.1.1.cmml" xref="S4.T4.5.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.3.3.1.m1.1c">\mathbf{\uparrow}</annotation></semantics></math> <span id="S4.T4.5.3.3.3.1.1" class="ltx_text ltx_font_bold">21.07</span>
</td>
</tr>
<tr id="S4.T4.6.4.4.4" class="ltx_tr">
<td id="S4.T4.6.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">Flowers102</td>
<td id="S4.T4.6.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t">31.29 (± 0.18)</td>
<td id="S4.T4.6.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">30.30 (± 0.16)</td>
<td id="S4.T4.6.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">26.43 (± 0.09)</td>
<td id="S4.T4.6.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">70.56 (± 0.34)</td>
<td id="S4.T4.6.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.4.4.4.7.1" class="ltx_text ltx_font_bold">77.57 (± 0.03)</span></td>
<td id="S4.T4.6.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.6.4.4.4.1.m1.1" class="ltx_Math" alttext="\mathbf{\uparrow}" display="inline"><semantics id="S4.T4.6.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T4.6.4.4.4.1.m1.1.1" xref="S4.T4.6.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.4.4.4.1.m1.1b"><ci id="S4.T4.6.4.4.4.1.m1.1.1.cmml" xref="S4.T4.6.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.4.4.4.1.m1.1c">\mathbf{\uparrow}</annotation></semantics></math> <span id="S4.T4.6.4.4.4.1.1" class="ltx_text ltx_font_bold">46.28</span>
</td>
</tr>
<tr id="S4.T4.7.5.5.5" class="ltx_tr">
<td id="S4.T4.7.5.5.5.2" class="ltx_td ltx_align_center ltx_border_t">Google Command</td>
<td id="S4.T4.7.5.5.5.3" class="ltx_td ltx_align_center ltx_border_t">26.24 (± 0.01)</td>
<td id="S4.T4.7.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t">73.68 (± 0.49)</td>
<td id="S4.T4.7.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">83.01 (± 0.23)</td>
<td id="S4.T4.7.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t">81.90 (± 0.20)</td>
<td id="S4.T4.7.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.7.5.5.5.7.1" class="ltx_text ltx_font_bold">83.46 (± 0.11)</span></td>
<td id="S4.T4.7.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S4.T4.7.5.5.5.1.m1.1" class="ltx_Math" alttext="\mathbf{\uparrow}" display="inline"><semantics id="S4.T4.7.5.5.5.1.m1.1a"><mo stretchy="false" id="S4.T4.7.5.5.5.1.m1.1.1" xref="S4.T4.7.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.5.5.5.1.m1.1b"><ci id="S4.T4.7.5.5.5.1.m1.1.1.cmml" xref="S4.T4.7.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.5.5.5.1.m1.1c">\mathbf{\uparrow}</annotation></semantics></math> <span id="S4.T4.7.5.5.5.1.1" class="ltx_text ltx_font_bold">57.22</span>
</td>
</tr>
<tr id="S4.T4.8.6.6.6" class="ltx_tr">
<td id="S4.T4.8.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">ESC-50</td>
<td id="S4.T4.8.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">12.72 (± 0.31)</td>
<td id="S4.T4.8.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">22.76 (± 1.01)</td>
<td id="S4.T4.8.6.6.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">32.49 (± 0.57)</td>
<td id="S4.T4.8.6.6.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">41.80 (± 0.32)</td>
<td id="S4.T4.8.6.6.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.8.6.6.6.7.1" class="ltx_text ltx_font_bold">43.46 (± 0.30)</span></td>
<td id="S4.T4.8.6.6.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S4.T4.8.6.6.6.1.m1.1" class="ltx_Math" alttext="\mathbf{\uparrow}" display="inline"><semantics id="S4.T4.8.6.6.6.1.m1.1a"><mo stretchy="false" id="S4.T4.8.6.6.6.1.m1.1.1" xref="S4.T4.8.6.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.6.6.6.1.m1.1b"><ci id="S4.T4.8.6.6.6.1.m1.1.1.cmml" xref="S4.T4.8.6.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.6.6.6.1.m1.1c">\mathbf{\uparrow}</annotation></semantics></math> <span id="S4.T4.8.6.6.6.1.1" class="ltx_text ltx_font_bold">30.74</span>
</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">Effectiveness of Private Data.</span>
Our experiments demonstrate the effectiveness of incorporating private data with FL into the finetuning process of the downstream model generated from synthetic data. As shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, regardless of the modality and quality of the synthetic data used to generate the downstream model, FL fine-tuning leads to significant performance gains, outperforming the ones trained solely with FL or CL combined with synthetic training by a large margin. Furthermore, we observe that fine-tuning with private data can especially benefit the cases for out-of-domain synthetic data, such as in the audio data. For example, <span id="S4.SS2.p8.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> with FedOpt could achieve 43.46 test accuracy in the ECS-50 dataset, which nearly provides two times increment than standard FL and three times increment than centralized training by synthetic data. These results suggest that leveraging private data with FL in the fine-tuning process can greatly enhance the performance of synthetic data-generated models, making them more suitable for real-world applications.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para ltx_noindent">
<p id="S4.SS2.p9.2" class="ltx_p"><span id="S4.SS2.p9.2.1" class="ltx_text ltx_font_bold">Generated Downstream Model Helps FL Optimization.</span>
To gain a comprehensive understanding of why the custom models built using <span id="S4.SS2.p9.2.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> provide benefits to performance improvements, we decided to compare the gradient diversity between model weights initialized by <span id="S4.SS2.p9.2.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> and random initialization. Specifically, we apply the definition of the gradient diversity introduced from <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite> by adapting the gradients <math id="S4.SS2.p9.1.m1.1" class="ltx_Math" alttext="g_{i}" display="inline"><semantics id="S4.SS2.p9.1.m1.1a"><msub id="S4.SS2.p9.1.m1.1.1" xref="S4.SS2.p9.1.m1.1.1.cmml"><mi id="S4.SS2.p9.1.m1.1.1.2" xref="S4.SS2.p9.1.m1.1.1.2.cmml">g</mi><mi id="S4.SS2.p9.1.m1.1.1.3" xref="S4.SS2.p9.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.1.m1.1b"><apply id="S4.SS2.p9.1.m1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p9.1.m1.1.1.2.cmml" xref="S4.SS2.p9.1.m1.1.1.2">𝑔</ci><ci id="S4.SS2.p9.1.m1.1.1.3.cmml" xref="S4.SS2.p9.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.1.m1.1c">g_{i}</annotation></semantics></math> to client update <math id="S4.SS2.p9.2.m2.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S4.SS2.p9.2.m2.1a"><msub id="S4.SS2.p9.2.m2.1.1" xref="S4.SS2.p9.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p9.2.m2.1.1.2" xref="S4.SS2.p9.2.m2.1.1.2.cmml">Δ</mi><mi id="S4.SS2.p9.2.m2.1.1.3" xref="S4.SS2.p9.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.2.m2.1b"><apply id="S4.SS2.p9.2.m2.1.1.cmml" xref="S4.SS2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.2.m2.1.1.1.cmml" xref="S4.SS2.p9.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p9.2.m2.1.1.2.cmml" xref="S4.SS2.p9.2.m2.1.1.2">Δ</ci><ci id="S4.SS2.p9.2.m2.1.1.3.cmml" xref="S4.SS2.p9.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.2.m2.1c">\Delta_{i}</annotation></semantics></math>:</p>
</div>
<div id="S4.SS2.p10" class="ltx_para ltx_noindent">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\Delta_{\textit{S}}=\frac{\sum_{i\in\textit{S}}||\Delta_{i}||^{2}}{||\sum_{i\in\textit{S}}\Delta_{i}||^{2}}" display="block"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.3" xref="S4.E1.m1.2.3.cmml"><msub id="S4.E1.m1.2.3.2" xref="S4.E1.m1.2.3.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.2.3.2.2" xref="S4.E1.m1.2.3.2.2.cmml">Δ</mi><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.2.3.2.3" xref="S4.E1.m1.2.3.2.3a.cmml">S</mtext></msub><mo id="S4.E1.m1.2.3.1" xref="S4.E1.m1.2.3.1.cmml">=</mo><mfrac id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E1.m1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.2.3.cmml"><mi id="S4.E1.m1.1.1.1.2.3.2" xref="S4.E1.m1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.E1.m1.1.1.1.2.3.1" xref="S4.E1.m1.1.1.1.2.3.1.cmml">∈</mo><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.1.1.1.2.3.3" xref="S4.E1.m1.1.1.1.2.3.3a.cmml">S</mtext></mrow></msub><msup id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.2.cmml"><mo lspace="0em" stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.1.cmml">‖</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml">Δ</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml">2</mn></msup></mrow><msup id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml"><mrow id="S4.E1.m1.2.2.2.1.1" xref="S4.E1.m1.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.2.1.1.2" xref="S4.E1.m1.2.2.2.1.2.1.cmml">‖</mo><mrow id="S4.E1.m1.2.2.2.1.1.1" xref="S4.E1.m1.2.2.2.1.1.1.cmml"><msub id="S4.E1.m1.2.2.2.1.1.1.1" xref="S4.E1.m1.2.2.2.1.1.1.1.cmml"><mo lspace="0em" id="S4.E1.m1.2.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.2.1.1.1.1.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.2.1.1.1.1.3" xref="S4.E1.m1.2.2.2.1.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.2.1.1.1.1.3.2" xref="S4.E1.m1.2.2.2.1.1.1.1.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.2.1.1.1.1.3.1" xref="S4.E1.m1.2.2.2.1.1.1.1.3.1.cmml">∈</mo><mtext class="ltx_mathvariant_italic" id="S4.E1.m1.2.2.2.1.1.1.1.3.3" xref="S4.E1.m1.2.2.2.1.1.1.1.3.3a.cmml">S</mtext></mrow></msub><msub id="S4.E1.m1.2.2.2.1.1.1.2" xref="S4.E1.m1.2.2.2.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.E1.m1.2.2.2.1.1.1.2.2" xref="S4.E1.m1.2.2.2.1.1.1.2.2.cmml">Δ</mi><mi id="S4.E1.m1.2.2.2.1.1.1.2.3" xref="S4.E1.m1.2.2.2.1.1.1.2.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.2.2.2.1.1.3" xref="S4.E1.m1.2.2.2.1.2.1.cmml">‖</mo></mrow><mn id="S4.E1.m1.2.2.2.3" xref="S4.E1.m1.2.2.2.3.cmml">2</mn></msup></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.3.cmml" xref="S4.E1.m1.2.3"><eq id="S4.E1.m1.2.3.1.cmml" xref="S4.E1.m1.2.3.1"></eq><apply id="S4.E1.m1.2.3.2.cmml" xref="S4.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.3.2.1.cmml" xref="S4.E1.m1.2.3.2">subscript</csymbol><ci id="S4.E1.m1.2.3.2.2.cmml" xref="S4.E1.m1.2.3.2.2">Δ</ci><ci id="S4.E1.m1.2.3.2.3a.cmml" xref="S4.E1.m1.2.3.2.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.E1.m1.2.3.2.3.cmml" xref="S4.E1.m1.2.3.2.3">S</mtext></ci></apply><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><divide id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2"></divide><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><apply id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.2.2"></sum><apply id="S4.E1.m1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.2.3"><in id="S4.E1.m1.1.1.1.2.3.1.cmml" xref="S4.E1.m1.1.1.1.2.3.1"></in><ci id="S4.E1.m1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.1.1.1.2.3.2">𝑖</ci><ci id="S4.E1.m1.1.1.1.2.3.3a.cmml" xref="S4.E1.m1.1.1.1.2.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.E1.m1.1.1.1.2.3.3.cmml" xref="S4.E1.m1.1.1.1.2.3.3">S</mtext></ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">Δ</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><cn type="integer" id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3">2</cn></apply></apply><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2">superscript</csymbol><apply id="S4.E1.m1.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.2.1.2.1.cmml" xref="S4.E1.m1.2.2.2.1.1.2">norm</csymbol><apply id="S4.E1.m1.2.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.2.1.1.1"><apply id="S4.E1.m1.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1">subscript</csymbol><sum id="S4.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.2"></sum><apply id="S4.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.3"><in id="S4.E1.m1.2.2.2.1.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.3.1"></in><ci id="S4.E1.m1.2.2.2.1.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.3.2">𝑖</ci><ci id="S4.E1.m1.2.2.2.1.1.1.1.3.3a.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.E1.m1.2.2.2.1.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.2.1.1.1.1.3.3">S</mtext></ci></apply></apply><apply id="S4.E1.m1.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.2.1.1.1.2.2">Δ</ci><ci id="S4.E1.m1.2.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.2.1.1.1.2.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\Delta_{\textit{S}}=\frac{\sum_{i\in\textit{S}}||\Delta_{i}||^{2}}{||\sum_{i\in\textit{S}}\Delta_{i}||^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p11" class="ltx_para ltx_noindent">
<p id="S4.SS2.p11.3" class="ltx_p">where <math id="S4.SS2.p11.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS2.p11.1.m1.1a"><mi id="S4.SS2.p11.1.m1.1.1" xref="S4.SS2.p11.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.1.m1.1b"><ci id="S4.SS2.p11.1.m1.1.1.cmml" xref="S4.SS2.p11.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.1.m1.1c">S</annotation></semantics></math> is the set of sampled clients in each communication round and <math id="S4.SS2.p11.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p11.2.m2.1a"><mi id="S4.SS2.p11.2.m2.1.1" xref="S4.SS2.p11.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.2.m2.1b"><ci id="S4.SS2.p11.2.m2.1.1.cmml" xref="S4.SS2.p11.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.2.m2.1c">i</annotation></semantics></math> represents the client index.
As shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the gradient diversity plot for FedAvg reveals that <span id="S4.SS2.p11.3.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> displays lower initial gradient diversity compared to random initialization. Over training time, both <span id="S4.SS2.p11.3.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> and random initialization converge to similar gradient diversity levels, consistent with the performance curve in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, where a larger <math id="S4.SS2.p11.3.m3.1" class="ltx_Math" alttext="\Delta_{S}" display="inline"><semantics id="S4.SS2.p11.3.m3.1a"><msub id="S4.SS2.p11.3.m3.1.1" xref="S4.SS2.p11.3.m3.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p11.3.m3.1.1.2" xref="S4.SS2.p11.3.m3.1.1.2.cmml">Δ</mi><mi id="S4.SS2.p11.3.m3.1.1.3" xref="S4.SS2.p11.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p11.3.m3.1b"><apply id="S4.SS2.p11.3.m3.1.1.cmml" xref="S4.SS2.p11.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p11.3.m3.1.1.1.cmml" xref="S4.SS2.p11.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p11.3.m3.1.1.2.cmml" xref="S4.SS2.p11.3.m3.1.1.2">Δ</ci><ci id="S4.SS2.p11.3.m3.1.1.3.cmml" xref="S4.SS2.p11.3.m3.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p11.3.m3.1c">\Delta_{S}</annotation></semantics></math> corresponds to slower convergence rate. This aligns with prior findings <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, indicating that starting from a pre-trained model leads to less variation in local client updates, potentially addressing the client drift issue.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy performance comparison between generated downstream model, standard federated learning and <span id="S4.T5.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>. All the training is initialized by ImageNet-based pre-train model.</figcaption>
<div id="S4.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:397.5pt;height:59pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.0pt,6.9pt) scale(0.808579939484954,0.808579939484954) ;">
<table id="S4.T5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<td id="S4.T5.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.2.1" class="ltx_text ltx_font_bold">3x Synthetic</span></td>
<td id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.3.1" class="ltx_text ltx_font_bold">FedAvg</span></td>
<td id="S4.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.4.1" class="ltx_text ltx_font_bold">FedOpt</span></td>
<td id="S4.T5.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">GPT-FL<span id="S4.T5.3.1.1.5.1.1" class="ltx_text ltx_font_serif"> w/ FedAvg</span></span></td>
<td id="S4.T5.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.3.1.1.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">GPT-FL<span id="S4.T5.3.1.1.6.1.1" class="ltx_text ltx_font_serif"> w/ FedOpt</span></span></td>
</tr>
<tr id="S4.T5.3.1.2" class="ltx_tr">
<td id="S4.T5.3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S4.T5.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">72.65 (± 0.05)</td>
<td id="S4.T5.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">66.10 (± 0.03)</td>
<td id="S4.T5.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">79.08 (± 0.39)</td>
<td id="S4.T5.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">75.87 (± 0.73)</td>
<td id="S4.T5.3.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.3.1.2.6.1" class="ltx_text ltx_font_bold">82.20 (± 0.61)</span></td>
</tr>
<tr id="S4.T5.3.1.3" class="ltx_tr">
<td id="S4.T5.3.1.3.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-100</td>
<td id="S4.T5.3.1.3.2" class="ltx_td ltx_align_center ltx_border_t">42.30 (± 0.01)</td>
<td id="S4.T5.3.1.3.3" class="ltx_td ltx_align_center ltx_border_t">62.83 (± 0.03)</td>
<td id="S4.T5.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t">45.27 (± 0.10)</td>
<td id="S4.T5.3.1.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.3.1.3.5.1" class="ltx_text ltx_font_bold">66.84 (± 0.05)</span></td>
<td id="S4.T5.3.1.3.6" class="ltx_td ltx_align_center ltx_border_t">66.03 (± 0.02)</td>
</tr>
<tr id="S4.T5.3.1.4" class="ltx_tr">
<td id="S4.T5.3.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Flowers102</td>
<td id="S4.T5.3.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">41.05 (± 0.26)</td>
<td id="S4.T5.3.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">80.73 (± 0.01)</td>
<td id="S4.T5.3.1.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">87.33 (± 0.29)</td>
<td id="S4.T5.3.1.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">86.18 (± 0.04)</td>
<td id="S4.T5.3.1.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T5.3.1.4.6.1" class="ltx_text ltx_font_bold">88.66 (± 0.40)</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS2.p12" class="ltx_para ltx_noindent">
<p id="S4.SS2.p12.1" class="ltx_p"><span id="S4.SS2.p12.1.1" class="ltx_text ltx_font_bold">Harmonization With Existing Pre-train Model.</span>
As the standard FL framework, <span id="S4.SS2.p12.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> could also benefit from other existing pre-train models. Specifically, besides training from scratch, <span id="S4.SS2.p12.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> could utilize the existing pre-train model to start training the synthetic data to generate downstream model and finetune it again with private data in FL. Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the performance evaluation of <span id="S4.SS2.p12.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> on top of the pre-trained models for image datasets. We follow the approach from prior work <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite> and use the ImageNet pre-trained model available in the PyTorch Torchvision library. Our experiments show that <span id="S4.SS2.p12.1.5" class="ltx_text ltx_font_typewriter">GPT-FL</span> achieves better results compared to training solely with FL or synthetic data, as reported in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Notably, the improvement in performance is consistent across three image benchmark datasets, with a gain ranging from 1% to 11% compared to the results in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. These results demonstrate that <span id="S4.SS2.p12.1.6" class="ltx_text ltx_font_typewriter">GPT-FL</span> can effectively leverage pre-trained models to improve performance in the FL setting.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We present <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>, a generative pre-trained model-assisted federated learning framework. <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> leverages the generative pre-trained model to generate diversified synthetic data for a wide range of data modalities before FL training. This synthetic data is then utilized to construct a downstream model, which undergoes fine-tuning with private data within a standard FL framework. Our experimental results showcase the remarkable performance of <span id="S5.p1.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span> when compared to state-of-the-art FL methods. Moreover, through detailed ablation studies, we demonstrate that <span id="S5.p1.1.4" class="ltx_text ltx_font_typewriter">GPT-FL</span> is a flexible and applicable framework solution to the challenges associated with cross-device FL scenarios.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitations and Future works.</span> Due to the limitations in our computational resources, we cannot further scale up the synthetic data volume in our study, as it may take several weeks for the generation. Besides, we do not investigate the larger model sizes in our current study, which we will pursue it as our future work.
In addition, we want to explore the expansions of the GPT-FL framework. GPT-FL seamlessly integrates with the vanilla FL framework, allowing for harmonization with most of the existing FL methods. We are interested in exploring the combination of the public-data-based FL aggregation scheme and the GPT-FL framework by replacing the public data with synthetic data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al. (2022)</span>
<span class="ltx_bibblock">
Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang.

</span>
<span class="ltx_bibblock">Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2212.01548, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al. (2021)</span>
<span class="ltx_bibblock">
Junyi Ao, Rui Wang, Long Zhou, Shujie Liu, Shuo Ren, Yu Wu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei.

</span>
<span class="ltx_bibblock">Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2016)</span>
<span class="ltx_bibblock">
K. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for federated learning on user-held data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">NIPS Workshop on Private Multi-Party Machine Learning</em>, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1611.04482" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1611.04482</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</em>, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Konecný, Stefano Mazzocchi, H. B. McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1902.01046, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2022)</span>
<span class="ltx_bibblock">
Yae Jee Cho, Andre Manoel, Gauri Joshi, Robert Sim, and Dimitrios Dimitriadis.

</span>
<span class="ltx_bibblock">Heterogeneous ensemble knowledge transfer for training large models in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Artificial Intelligence</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang et al. (2021)</span>
<span class="ltx_bibblock">
Trung Dang, Om Thakkar, Swaroop Indra Ramaswamy, Rajiv Mathews, Peter Chin, and Franccoise Beaufays.

</span>
<span class="ltx_bibblock">A method to reveal speaker identity in distributed asr training, and how to counter it.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  4338–4342, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gascón et al. (2023)</span>
<span class="ltx_bibblock">
Adrià Gascón, Peter Kairouz, Ziteng Sun, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Federated heavy hitter recovery under linear sketching.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2307.13347, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:260154975" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:260154975</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodrich &amp; Mitzenmacher (2011)</span>
<span class="ltx_bibblock">
Michael T. Goodrich and Michael Mitzenmacher.

</span>
<span class="ltx_bibblock">Invertible bloom lookup tables.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, pp.  792–799, 2011.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:11589877" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:11589877</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2015)</span>
<span class="ltx_bibblock">
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  770–778, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2022)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip H. S. Torr, Song Bai, and Xiaojuan Qi.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2210.07574, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2021)</span>
<span class="ltx_bibblock">
Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora.

</span>
<span class="ltx_bibblock">Evaluating gradient inversion attacks and defenses in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itahara et al. (2020)</span>
<span class="ltx_bibblock">
Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and Koji Yamamoto.

</span>
<span class="ltx_bibblock">Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>, 22:191–205, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. (2019)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song.

</span>
<span class="ltx_bibblock">Dataset condensation via efficient synthetic-data parameterization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:18268744" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:18268744</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Jason Li, Ravi Gadde, Boris Ginsburg, and Vitaly Lavrukhin.

</span>
<span class="ltx_bibblock">Training neural speech recognition systems with synthetic speech augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.00707</em>, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Qinbin Li, Bingsheng He, and Dawn Xiaodong Song.

</span>
<span class="ltx_bibblock">Model-contrastive federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  10708–10717, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2006.07242, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haohe Liu, Zehua Chen, Yiitan Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and MarkD . Plumbley.

</span>
<span class="ltx_bibblock">Audioldm: Text-to-audio generation with latent diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2301.12503, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2016)</span>
<span class="ltx_bibblock">
H. B. McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2022)</span>
<span class="ltx_bibblock">
John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, and Michael G. Rabbat.

</span>
<span class="ltx_bibblock">Where to begin? on the impact of pre-training and initialization in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2210.08090, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilsback &amp; Zisserman (2008)</span>
<span class="ltx_bibblock">
Maria-Elena Nilsback and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Automated flower classification over a large number of classes.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</em>, pp.  722–729, 2008.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pi et al. (2022)</span>
<span class="ltx_bibblock">
Renjie Pi, Weizhong Zhang, Yueqi Xie, Jiahui Gao, Xiaoyu Wang, Sunghun Kim, and Qifeng Chen.

</span>
<span class="ltx_bibblock">Dynafed: Tackling client data heterogeneity with global dynamics.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.10878</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
Karol J. Piczak.

</span>
<span class="ltx_bibblock">ESC: Dataset for Environmental Sound Classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Annual ACM Conference on Multimedia</em>, pp.  1015–1018. ACM Press.

</span>
<span class="ltx_bibblock">ISBN 978-1-4503-3459-4.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/2733373.2806390</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://dl.acm.org/citation.cfm?doid=2733373.2806390" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dl.acm.org/citation.cfm?doid=2733373.2806390</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2020)</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary B. Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecný, Sanjiv Kumar, and H. B. McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2003.00295, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2021)</span>
<span class="ltx_bibblock">
Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  10674–10685, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. (2018)</span>
<span class="ltx_bibblock">
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv: Learning</em>, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2210.08402, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shipard et al. (2023)</span>
<span class="ltx_bibblock">
Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, and Clinton Fookes.

</span>
<span class="ltx_bibblock">Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan &amp; Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1409.1556, 2014.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al. (2021)</span>
<span class="ltx_bibblock">
Jinhyun So, Chaoyang He, Chien-Sheng Yang, Songze Li, Qian Yu, Ramy E. Ali, Basak Guler, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Conference on Machine Learning and Systems</em>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanton et al. (2021)</span>
<span class="ltx_bibblock">
Samuel Stanton, Pavel Izmailov, P. Kirichenko, Alexander A. Alemi, and Andrew Gordon Wilson.

</span>
<span class="ltx_bibblock">Does knowledge distillation really work?

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2106.05945, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TensorFlow (2023)</span>
<span class="ltx_bibblock">
TensorFlow.

</span>
<span class="ltx_bibblock">Private heavy hitters, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.tensorflow.org/federated/tutorials/private_heavy_hitters" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/tutorials/private_heavy_hitters</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik K. Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.

</span>
<span class="ltx_bibblock">Attack of the tails: Yes, you really can backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2007.05084, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warden (2018)</span>
<span class="ltx_bibblock">
Pete Warden.

</span>
<span class="ltx_bibblock">Speech commands: A dataset for limited-vocabulary speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1804.03209, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter Bartlett.

</span>
<span class="ltx_bibblock">Gradient diversity: a key ingredient for scalable distributed learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>, pp.  1998–2007. PMLR, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan.

</span>
<span class="ltx_bibblock">Fine-tuning global model via data-free knowledge distillation for non-iid federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  10164–10173, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Tuo Zhang, Lei Gao, Chaoyang He, Mi Zhang, Bhaskar Krishnamachari, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Federated learning for the internet of things: Applications, challenges, and opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em>, 5:24–29, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Tuo Zhang, Tiantian Feng, Samiul Alam, Sunwoo Lee, Mi Zhang, Shrikanth S. Narayanan, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fedaudio: A federated learning benchmark for audio tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP49357.2023.10096500</span>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou.

</span>
<span class="ltx_bibblock">Data-free knowledge distillation for heterogeneous federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of machine learning research</em>, 139:12878–12889, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Integration of IBLT in <span id="A1.SS1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>
</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">Within the <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> framework, the set of distinct label names is sourced from an open domain. The server lacks detailed length information on the set, making it challenging to directly encode the label names properly for secure aggregation. To address this, we propose to locally encode the unique label names into Invertible Bloom Lookup Tables (IBLT) <cite class="ltx_cite ltx_citemacro_cite">Goodrich &amp; Mitzenmacher (<a href="#bib.bib9" title="" class="ltx_ref">2011</a>)</cite> data structure, a randomized data structure efficient in storing key-value pairs within an open domain. IBLT is a bloom filter-type linear data structure that supports the efficient listing of inserted elements and their precise counts, with table size scaling linearly with unique keys. IBLT sketches are amenable to linear summation, thus compatible with secure aggregation protocols.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para ltx_noindent">
<p id="A1.SS1.p2.1" class="ltx_p">In the <span id="A1.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> framework’s IBLT integration, each client locally encodes its distinct label names into IBLT and transmits it to the server. The server performs linear aggregation of these IBLTs through a secure multi-party computation protocol, subsequently decoding the aggregated table to obtain total label name counts without revealing individual label information. By leveraging the collective label name histogram, the server determines the union of distinct label names for data generation, maintaining the privacy of client-specific details. This approach finds validation in prior research <cite class="ltx_cite ltx_citemacro_cite">Gascón et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, where IBLT demonstrated its efficacy in addressing private heavy hitters within federated analytics.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para ltx_noindent">
<p id="A1.SS1.p3.2" class="ltx_p">To better demonstrate the integration of IBLT in <span id="A1.SS1.p3.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>, we provide an illustrated experiment as an example. The experiment is conducted with the TensorFlow Federated IBLT API <cite class="ltx_cite ltx_citemacro_cite">TensorFlow (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>. We partition the CIFAR-10 dataset heterogeneously amongst 100 clients using the Dirichlet distribution <math id="A1.SS1.p3.1.m1.1" class="ltx_Math" alttext="Dir_{K}(\alpha)" display="inline"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.2" xref="A1.SS1.p3.1.m1.1.2.cmml"><mi id="A1.SS1.p3.1.m1.1.2.2" xref="A1.SS1.p3.1.m1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A1.SS1.p3.1.m1.1.2.1" xref="A1.SS1.p3.1.m1.1.2.1.cmml">​</mo><mi id="A1.SS1.p3.1.m1.1.2.3" xref="A1.SS1.p3.1.m1.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A1.SS1.p3.1.m1.1.2.1a" xref="A1.SS1.p3.1.m1.1.2.1.cmml">​</mo><msub id="A1.SS1.p3.1.m1.1.2.4" xref="A1.SS1.p3.1.m1.1.2.4.cmml"><mi id="A1.SS1.p3.1.m1.1.2.4.2" xref="A1.SS1.p3.1.m1.1.2.4.2.cmml">r</mi><mi id="A1.SS1.p3.1.m1.1.2.4.3" xref="A1.SS1.p3.1.m1.1.2.4.3.cmml">K</mi></msub><mo lspace="0em" rspace="0em" id="A1.SS1.p3.1.m1.1.2.1b" xref="A1.SS1.p3.1.m1.1.2.1.cmml">​</mo><mrow id="A1.SS1.p3.1.m1.1.2.5.2" xref="A1.SS1.p3.1.m1.1.2.cmml"><mo stretchy="false" id="A1.SS1.p3.1.m1.1.2.5.2.1" xref="A1.SS1.p3.1.m1.1.2.cmml">(</mo><mi id="A1.SS1.p3.1.m1.1.1" xref="A1.SS1.p3.1.m1.1.1.cmml">α</mi><mo stretchy="false" id="A1.SS1.p3.1.m1.1.2.5.2.2" xref="A1.SS1.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><apply id="A1.SS1.p3.1.m1.1.2.cmml" xref="A1.SS1.p3.1.m1.1.2"><times id="A1.SS1.p3.1.m1.1.2.1.cmml" xref="A1.SS1.p3.1.m1.1.2.1"></times><ci id="A1.SS1.p3.1.m1.1.2.2.cmml" xref="A1.SS1.p3.1.m1.1.2.2">𝐷</ci><ci id="A1.SS1.p3.1.m1.1.2.3.cmml" xref="A1.SS1.p3.1.m1.1.2.3">𝑖</ci><apply id="A1.SS1.p3.1.m1.1.2.4.cmml" xref="A1.SS1.p3.1.m1.1.2.4"><csymbol cd="ambiguous" id="A1.SS1.p3.1.m1.1.2.4.1.cmml" xref="A1.SS1.p3.1.m1.1.2.4">subscript</csymbol><ci id="A1.SS1.p3.1.m1.1.2.4.2.cmml" xref="A1.SS1.p3.1.m1.1.2.4.2">𝑟</ci><ci id="A1.SS1.p3.1.m1.1.2.4.3.cmml" xref="A1.SS1.p3.1.m1.1.2.4.3">𝐾</ci></apply><ci id="A1.SS1.p3.1.m1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">Dir_{K}(\alpha)</annotation></semantics></math> with <math id="A1.SS1.p3.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="A1.SS1.p3.2.m2.1a"><mi id="A1.SS1.p3.2.m2.1.1" xref="A1.SS1.p3.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.2.m2.1b"><ci id="A1.SS1.p3.2.m2.1.1.cmml" xref="A1.SS1.p3.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.2.m2.1c">\alpha</annotation></semantics></math> equal to 0.1. As the server does not know the length of the dataset initially, we set the capacity of the IBLT sketch to 50, which is much larger than the total number of unique labels inside CIFAR-10 (i.e., 10). Each client encodes its unique set of label names into IBLT and sends it to the server. The server would aggregate them via the secure aggregation protocol, which means the server can not access the individual IBLT but only knows the summation of IBLTs. After decoding the aggregated IBLT, the server only gets the following information:</p>
</div>
<div id="A1.SS1.p4" class="ltx_para ltx_noindent">
<div id="A1.SS1.p4.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,TnVtYmVyIG9mIGNsaWVudHMgcGFydGljaXBhdGVkOiAxMDAKRGlzY292ZXJlZCBsYWJlbCBuYW1lcyBhbmQgY291bnRzOgp7J2RvZyc6IDQ5LCAnYXV0b21vYmlsZSc6IDU5LCAnYmlyZCc6IDUwLCAnaG9yc2UnOiAzMiwgJ2NhdCc6IDQ2LCAnZnJvZyc6IDI3LCAnZGVlcic6IDQ0LCAndHJ1Y2snOiAzNywgJ2FpcnBsYW5lJzogNTAsICdzaGlwJzogMzV9" download="">⬇</a></div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">Number</span><span id="lstnumberx3.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">of</span><span id="lstnumberx3.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">clients</span><span id="lstnumberx3.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">participated</span><span id="lstnumberx3.8" class="ltx_text ltx_font_typewriter">:</span><span id="lstnumberx3.9" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx3.10" class="ltx_text ltx_font_typewriter">100</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">Discovered</span><span id="lstnumberx4.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter">label</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">names</span><span id="lstnumberx4.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">and</span><span id="lstnumberx4.8" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter">counts</span><span id="lstnumberx4.10" class="ltx_text ltx_font_typewriter">:</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_font_typewriter">{’</span><span id="lstnumberx5.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">dog</span><span id="lstnumberx5.3" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.5" class="ltx_text ltx_font_typewriter">49,</span><span id="lstnumberx5.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.7" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">automobile</span><span id="lstnumberx5.9" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.10" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.11" class="ltx_text ltx_font_typewriter">59,</span><span id="lstnumberx5.12" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.13" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter">bird</span><span id="lstnumberx5.15" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.16" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.17" class="ltx_text ltx_font_typewriter">50,</span><span id="lstnumberx5.18" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.19" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter">horse</span><span id="lstnumberx5.21" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.22" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.23" class="ltx_text ltx_font_typewriter">32,</span><span id="lstnumberx5.24" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.25" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.26" class="ltx_text ltx_lst_identifier ltx_font_typewriter">cat</span><span id="lstnumberx5.27" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.28" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.29" class="ltx_text ltx_font_typewriter">46,</span><span id="lstnumberx5.30" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.31" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.32" class="ltx_text ltx_lst_identifier ltx_font_typewriter">frog</span><span id="lstnumberx5.33" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.34" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.35" class="ltx_text ltx_font_typewriter">27,</span><span id="lstnumberx5.36" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.37" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.38" class="ltx_text ltx_lst_identifier ltx_font_typewriter">deer</span><span id="lstnumberx5.39" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.40" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.41" class="ltx_text ltx_font_typewriter">44,</span><span id="lstnumberx5.42" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.43" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.44" class="ltx_text ltx_lst_identifier ltx_font_typewriter">truck</span><span id="lstnumberx5.45" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.46" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.47" class="ltx_text ltx_font_typewriter">37,</span><span id="lstnumberx5.48" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.49" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.50" class="ltx_text ltx_lst_identifier ltx_font_typewriter">airplane</span><span id="lstnumberx5.51" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.52" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.53" class="ltx_text ltx_font_typewriter">50,</span><span id="lstnumberx5.54" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.55" class="ltx_text ltx_font_typewriter">’</span><span id="lstnumberx5.56" class="ltx_text ltx_lst_identifier ltx_font_typewriter">ship</span><span id="lstnumberx5.57" class="ltx_text ltx_font_typewriter">’:</span><span id="lstnumberx5.58" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx5.59" class="ltx_text ltx_font_typewriter">35}</span>
</div>
</div>
</div>
<div id="A1.SS1.p5" class="ltx_para ltx_noindent">
<p id="A1.SS1.p5.1" class="ltx_p">The decode information only contains the number of participated clients and the histogram of the label name, which the server could infer the union of distinct label names for data generation. For example, the notation "’dog’:49" denotes there are 49 clients who include the label ’dog’ within their local datasets, but the server lacks knowledge regarding the specific client identities associated with this ’dog’ label in the localized data.
It is crucial to emphasize that the server remains unable to access specific client details, such as the labels held by individual clients. As suggested in the previous work <cite class="ltx_cite ltx_citemacro_cite">TensorFlow (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>); Gascón et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, this algorithm could be further enhanced by adding a differential privacy mechanism. In conclusion, this IBLT-based algorithm will allow parties to jointly compute the union of unique label names without revealing individual label information, addressing concerns about privacy and confidentiality.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Experiment Settings</h3>

<section id="A1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Computing Infrastructure</h4>

<div id="A1.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS1.p1.1" class="ltx_p">All experiments are conducted via CPU/GPU simulation. The simulation experiments are performed on two computing servers with ten GPUs. The server is equipped with AMD EPYC 7502 32-Core Processor and 1024G memory. The GPU is NVIDIA RTX A100.</p>
</div>
</section>
<section id="A1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Datasets and Models</h4>

<div id="A1.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.p1.1" class="ltx_p"><span id="A1.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">CIFAR-10.</span> The CIFAR-10 dataset <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> consists of 60,000 32x32 color images in 10 classes. It has 50,000 training images and 10,000 test images. We normalize the images using the mean and standard deviation of the dataset. For evaluation, we use ConvNet <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, ResNet18 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite>, and VGG19 <cite class="ltx_cite ltx_citemacro_cite">Simonyan &amp; Zisserman (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite> models. Following the previous work <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, the ConvNet has 3 layers with a hidden dimension of 128. The dataset is partitioned using a Dirichlet distribution to emulate a realistic non-iid distribution, following prior work <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="A1.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.p2.1" class="ltx_p"><span id="A1.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">CIFAR-100.</span> The CIFAR-100 dataset <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> is similar to CIFAR-10 but contains 100 classes, with 600 images per class. We apply the same partitioning method as CIFAR-10. For evaluation, we use ConvNet <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, ResNet50 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite>, and VGG19 <cite class="ltx_cite ltx_citemacro_cite">Simonyan &amp; Zisserman (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite> models. The ConvNet architecture is the same as used for CIFAR-10.</p>
</div>
<div id="A1.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.p3.1" class="ltx_p"><span id="A1.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Oxford Flowers 102.</span> The Oxford Flowers 102 <cite class="ltx_cite ltx_citemacro_cite">Nilsback &amp; Zisserman (<a href="#bib.bib24" title="" class="ltx_ref">2008</a>)</cite> (Flowers102) dataset consists of 102 types of flowers, with each type containing between 40 and 258 images. The images exhibit significant variations in scale, angle, and lighting. Some flower categories also have substantial variations within the category and contain several closely related categories. It is divided into training, validation, and test sets. The training and validation sets consist of 10 images per class, totaling 1020 images each. The test set contains the remaining 6149 images, with a minimum of 20 images per class. We resize all images to 224x224 pixels for consistency. For evaluation, we use ConvNet <cite class="ltx_cite ltx_citemacro_cite">Pi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, ResNet18 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib10" title="" class="ltx_ref">2015</a>)</cite>, and VGG19 <cite class="ltx_cite ltx_citemacro_cite">Simonyan &amp; Zisserman (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite> models. We apply the same partitioning method as CIFAR-10. The ConvNet architecture is the same as used for CIFAR-10.</p>
</div>
<div id="A1.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.p4.1" class="ltx_p"><span id="A1.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Google Command.</span> The Google Command dataset <cite class="ltx_cite ltx_citemacro_cite">Warden (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite> comprises 105,829 audio recordings collected from 2,618 speakers. The training set includes recordings from 2,112 speakers, the validation set includes 256 speakers, and the test set includes 250 speakers. It consists of 35 common words from everyday vocabulary, such as "Yes," "No," "Up," and "Down." For evaluation, we use a lightweight model based on related work <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> for a 35-class keyword spotting task, where the model consists of two convolution layers followed by one Gated Recurrent Units (GRU) layer and an average pooling layer is connected to the GRU output, which is then fed through two dense layers to generate the predictions. In this work, to pre-process the raw audio data, a sequence of overlapping Hamming windows is applied to the raw speech signal with a time shift of 10 ms. We calculate the discrete Fourier transform (DFT) with a frame length of 1,024 and compute the Mel-spectrogram with a dimension of 128. The Mel-spectrogram is used for training the keyword spotting model. We follow <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> for this setup.</p>
</div>
<div id="A1.SS2.SSS2.p5" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS2.p5.1" class="ltx_p"><span id="A1.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_bold">ESC-50.</span> The ESC-50 dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">Piczak </a></cite> consists of 2000 environmental audio recordings suitable for environmental sound classification. The dataset contains 5-second-long recordings categorized into 50 semantical classes, with 40 examples per class. These classes are loosely arranged into five major categories: animals, natural soundscapes &amp; water sounds, human &amp; non-speech sounds, interior/domestic sounds, and exterior/urban noises. We employ the same data pre-processing method and model architecture as used in the Google Command dataset.</p>
</div>
</section>
<section id="A1.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span>Hyperparameter Settings</h4>

<div id="A1.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p1.1" class="ltx_p">To determine the optimal hyperparameters, we conducted a search within specified ranges. The client learning rate was searched in the range of 1.00E-09 to 1.00E-00, the server learning rate in the range of 1.00E-09 to 1.00E-00, weight decay in the range of 0.1 to 0.9, input batch size in the range of 8 to 256, and epoch number for centralized training in the range of 100 to 500. The hyperparameter settings for the public data-based methods and standard FL methods in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> followed the settings from the previous work <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. The specific hyperparameter selections for the other experiments are provided below.</p>
</div>
<div id="A1.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p2.1" class="ltx_p"><span id="A1.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Hyperparameter Selection in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</span> The detailed experiment setups for Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are listed in Table <a href="#A1.T6" title="Table 6 ‣ A.2.3 Hyperparameter Settings ‣ A.2 Experiment Settings ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Table <a href="#A1.T7" title="Table 7 ‣ A.2.3 Hyperparameter Settings ‣ A.2 Experiment Settings ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Table <a href="#A1.T8" title="Table 8 ‣ A.2.3 Hyperparameter Settings ‣ A.2 Experiment Settings ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and Table <a href="#A1.T9" title="Table 9 ‣ A.2.3 Hyperparameter Settings ‣ A.2 Experiment Settings ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. For the experiments related to FedGen<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>FedGen: https://github.com/zhuangdizhu/FedGen</span></span></span> and DynaFed<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>DynaFed: https://github.com/pipilurj/DynaFed/tree/main</span></span></span>, we evaluate them with their official implementation code on GitHub.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Experimental setup details of <span id="A1.T6.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> with VGG19 in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></figcaption>
<div id="A1.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:149.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.7pt,15.7pt) scale(0.825344170797152,0.825344170797152) ;">
<table id="A1.T6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.3.1.1" class="ltx_tr">
<td id="A1.T6.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T6.3.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="A1.T6.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.3.1.1.3.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="A1.T6.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.3.1.1.4.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="A1.T6.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.3.1.1.5.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
</tr>
<tr id="A1.T6.3.1.2" class="ltx_tr">
<td id="A1.T6.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Local Epoch</td>
<td id="A1.T6.3.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T6.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T6.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T6.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T6.3.1.3" class="ltx_tr">
<td id="A1.T6.3.1.3.1" class="ltx_td ltx_align_left">Communication Rounds</td>
<td id="A1.T6.3.1.3.2" class="ltx_td"></td>
<td id="A1.T6.3.1.3.3" class="ltx_td ltx_align_center">500</td>
<td id="A1.T6.3.1.3.4" class="ltx_td ltx_align_center">500</td>
<td id="A1.T6.3.1.3.5" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A1.T6.3.1.4" class="ltx_tr">
<td id="A1.T6.3.1.4.1" class="ltx_td ltx_align_left">Cohort Size</td>
<td id="A1.T6.3.1.4.2" class="ltx_td"></td>
<td id="A1.T6.3.1.4.3" class="ltx_td ltx_align_center">10</td>
<td id="A1.T6.3.1.4.4" class="ltx_td ltx_align_center">10</td>
<td id="A1.T6.3.1.4.5" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="A1.T6.3.1.5" class="ltx_tr">
<td id="A1.T6.3.1.5.1" class="ltx_td ltx_align_left">Batch Size</td>
<td id="A1.T6.3.1.5.2" class="ltx_td"></td>
<td id="A1.T6.3.1.5.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T6.3.1.5.4" class="ltx_td ltx_align_center">32</td>
<td id="A1.T6.3.1.5.5" class="ltx_td ltx_align_center">32</td>
</tr>
<tr id="A1.T6.3.1.6" class="ltx_tr">
<td id="A1.T6.3.1.6.1" class="ltx_td ltx_align_left" rowspan="2"><span id="A1.T6.3.1.6.1.1" class="ltx_text">Client Learning Rate</span></td>
<td id="A1.T6.3.1.6.2" class="ltx_td ltx_align_left">High Data Heterogeneity</td>
<td id="A1.T6.3.1.6.3" class="ltx_td ltx_align_center">1.00E-07</td>
<td id="A1.T6.3.1.6.4" class="ltx_td ltx_align_center">1.00E-06</td>
<td id="A1.T6.3.1.6.5" class="ltx_td ltx_align_center">5.00E-03</td>
</tr>
<tr id="A1.T6.3.1.7" class="ltx_tr">
<td id="A1.T6.3.1.7.1" class="ltx_td ltx_align_left">Low Data Heterogeneity</td>
<td id="A1.T6.3.1.7.2" class="ltx_td ltx_align_center">1.00E-07</td>
<td id="A1.T6.3.1.7.3" class="ltx_td ltx_align_center">1.00E-06</td>
<td id="A1.T6.3.1.7.4" class="ltx_td ltx_align_center">5.00E-03</td>
</tr>
<tr id="A1.T6.3.1.8" class="ltx_tr">
<td id="A1.T6.3.1.8.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="A1.T6.3.1.8.2" class="ltx_td"></td>
<td id="A1.T6.3.1.8.3" class="ltx_td ltx_align_center">SGD</td>
<td id="A1.T6.3.1.8.4" class="ltx_td ltx_align_center">SGD</td>
<td id="A1.T6.3.1.8.5" class="ltx_td ltx_align_center">SGD</td>
</tr>
<tr id="A1.T6.3.1.9" class="ltx_tr">
<td id="A1.T6.3.1.9.1" class="ltx_td ltx_align_left">Momentum</td>
<td id="A1.T6.3.1.9.2" class="ltx_td"></td>
<td id="A1.T6.3.1.9.3" class="ltx_td ltx_align_center">0.9</td>
<td id="A1.T6.3.1.9.4" class="ltx_td ltx_align_center">0.9</td>
<td id="A1.T6.3.1.9.5" class="ltx_td ltx_align_center">0.9</td>
</tr>
<tr id="A1.T6.3.1.10" class="ltx_tr">
<td id="A1.T6.3.1.10.1" class="ltx_td ltx_align_left ltx_border_bb">Weight Decay</td>
<td id="A1.T6.3.1.10.2" class="ltx_td ltx_border_bb"></td>
<td id="A1.T6.3.1.10.3" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
<td id="A1.T6.3.1.10.4" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
<td id="A1.T6.3.1.10.5" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
</tr>
</table>
</span></div>
</figure>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Experimental setup details of <span id="A1.T7.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> with ConvNet in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></figcaption>
<div id="A1.T7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:162.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.2pt,18.3pt) scale(0.81564102622058,0.81564102622058) ;">
<table id="A1.T7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.3.1.1" class="ltx_tr">
<td id="A1.T7.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T7.3.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="A1.T7.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T7.3.1.1.3.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="A1.T7.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T7.3.1.1.4.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="A1.T7.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T7.3.1.1.5.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
</tr>
<tr id="A1.T7.3.1.2" class="ltx_tr">
<td id="A1.T7.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Local Epoch</td>
<td id="A1.T7.3.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T7.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T7.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T7.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T7.3.1.3" class="ltx_tr">
<td id="A1.T7.3.1.3.1" class="ltx_td ltx_align_left">Communication Rounds</td>
<td id="A1.T7.3.1.3.2" class="ltx_td"></td>
<td id="A1.T7.3.1.3.3" class="ltx_td ltx_align_center">500</td>
<td id="A1.T7.3.1.3.4" class="ltx_td ltx_align_center">500</td>
<td id="A1.T7.3.1.3.5" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A1.T7.3.1.4" class="ltx_tr">
<td id="A1.T7.3.1.4.1" class="ltx_td ltx_align_left">Cohort Size</td>
<td id="A1.T7.3.1.4.2" class="ltx_td"></td>
<td id="A1.T7.3.1.4.3" class="ltx_td ltx_align_center">10</td>
<td id="A1.T7.3.1.4.4" class="ltx_td ltx_align_center">10</td>
<td id="A1.T7.3.1.4.5" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="A1.T7.3.1.5" class="ltx_tr">
<td id="A1.T7.3.1.5.1" class="ltx_td ltx_align_left">Batch Size</td>
<td id="A1.T7.3.1.5.2" class="ltx_td"></td>
<td id="A1.T7.3.1.5.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T7.3.1.5.4" class="ltx_td ltx_align_center">32</td>
<td id="A1.T7.3.1.5.5" class="ltx_td ltx_align_center">32</td>
</tr>
<tr id="A1.T7.3.1.6" class="ltx_tr">
<td id="A1.T7.3.1.6.1" class="ltx_td ltx_align_left" rowspan="2"><span id="A1.T7.3.1.6.1.1" class="ltx_text">Client Learning Rate</span></td>
<td id="A1.T7.3.1.6.2" class="ltx_td ltx_align_left">High Data Heterogeneity</td>
<td id="A1.T7.3.1.6.3" class="ltx_td ltx_align_center">2.00E-07</td>
<td id="A1.T7.3.1.6.4" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T7.3.1.6.5" class="ltx_td ltx_align_center">1.00E-04</td>
</tr>
<tr id="A1.T7.3.1.7" class="ltx_tr">
<td id="A1.T7.3.1.7.1" class="ltx_td ltx_align_left">Low Data Heterogeneity</td>
<td id="A1.T7.3.1.7.2" class="ltx_td ltx_align_center">5.00E-06</td>
<td id="A1.T7.3.1.7.3" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T7.3.1.7.4" class="ltx_td ltx_align_center">5.00E-03</td>
</tr>
<tr id="A1.T7.3.1.8" class="ltx_tr">
<td id="A1.T7.3.1.8.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="A1.T7.3.1.8.2" class="ltx_td"></td>
<td id="A1.T7.3.1.8.3" class="ltx_td ltx_align_center">AdamW</td>
<td id="A1.T7.3.1.8.4" class="ltx_td ltx_align_center">AdamW</td>
<td id="A1.T7.3.1.8.5" class="ltx_td ltx_align_center">SGD</td>
</tr>
<tr id="A1.T7.3.1.9" class="ltx_tr">
<td id="A1.T7.3.1.9.1" class="ltx_td ltx_align_left">Betas</td>
<td id="A1.T7.3.1.9.2" class="ltx_td"></td>
<td id="A1.T7.3.1.9.3" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T7.3.1.9.4" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T7.3.1.9.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="A1.T7.3.1.10" class="ltx_tr">
<td id="A1.T7.3.1.10.1" class="ltx_td ltx_align_left">Eps</td>
<td id="A1.T7.3.1.10.2" class="ltx_td"></td>
<td id="A1.T7.3.1.10.3" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T7.3.1.10.4" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T7.3.1.10.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="A1.T7.3.1.11" class="ltx_tr">
<td id="A1.T7.3.1.11.1" class="ltx_td ltx_align_left ltx_border_bb">Weight Decay</td>
<td id="A1.T7.3.1.11.2" class="ltx_td ltx_border_bb"></td>
<td id="A1.T7.3.1.11.3" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
<td id="A1.T7.3.1.11.4" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
<td id="A1.T7.3.1.11.5" class="ltx_td ltx_align_center ltx_border_bb">5.00E-04</td>
</tr>
</table>
</span></div>
</figure>
<figure id="A1.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Experimental setup details of FedGen with ConvNet in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></figcaption>
<div id="A1.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:204.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.8pt,24.0pt) scale(0.809665457880769,0.809665457880769) ;">
<table id="A1.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T8.1.1.1" class="ltx_tr">
<td id="A1.T8.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T8.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="A1.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="A1.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="A1.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
</tr>
<tr id="A1.T8.1.1.2" class="ltx_tr">
<td id="A1.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Local Epoch</td>
<td id="A1.T8.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="A1.T8.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">5</td>
</tr>
<tr id="A1.T8.1.1.3" class="ltx_tr">
<td id="A1.T8.1.1.3.1" class="ltx_td ltx_align_left">Communication Rounds</td>
<td id="A1.T8.1.1.3.2" class="ltx_td"></td>
<td id="A1.T8.1.1.3.3" class="ltx_td ltx_align_center">500</td>
<td id="A1.T8.1.1.3.4" class="ltx_td ltx_align_center">500</td>
<td id="A1.T8.1.1.3.5" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A1.T8.1.1.4" class="ltx_tr">
<td id="A1.T8.1.1.4.1" class="ltx_td ltx_align_left">Cohort Size</td>
<td id="A1.T8.1.1.4.2" class="ltx_td"></td>
<td id="A1.T8.1.1.4.3" class="ltx_td ltx_align_center">10</td>
<td id="A1.T8.1.1.4.4" class="ltx_td ltx_align_center">10</td>
<td id="A1.T8.1.1.4.5" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="A1.T8.1.1.5" class="ltx_tr">
<td id="A1.T8.1.1.5.1" class="ltx_td ltx_align_left">Batch Size</td>
<td id="A1.T8.1.1.5.2" class="ltx_td"></td>
<td id="A1.T8.1.1.5.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T8.1.1.5.4" class="ltx_td ltx_align_center">32</td>
<td id="A1.T8.1.1.5.5" class="ltx_td ltx_align_center">32</td>
</tr>
<tr id="A1.T8.1.1.6" class="ltx_tr">
<td id="A1.T8.1.1.6.1" class="ltx_td ltx_align_left">Generator Batch Size</td>
<td id="A1.T8.1.1.6.2" class="ltx_td"></td>
<td id="A1.T8.1.1.6.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T8.1.1.6.4" class="ltx_td ltx_align_center">32</td>
<td id="A1.T8.1.1.6.5" class="ltx_td ltx_align_center">32</td>
</tr>
<tr id="A1.T8.1.1.7" class="ltx_tr">
<td id="A1.T8.1.1.7.1" class="ltx_td ltx_align_left" rowspan="2"><span id="A1.T8.1.1.7.1.1" class="ltx_text">Client Learning Rate</span></td>
<td id="A1.T8.1.1.7.2" class="ltx_td ltx_align_left">High Data Heterogeneity</td>
<td id="A1.T8.1.1.7.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.7.4" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.7.5" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T8.1.1.8" class="ltx_tr">
<td id="A1.T8.1.1.8.1" class="ltx_td ltx_align_left">Low Data Heterogeneity</td>
<td id="A1.T8.1.1.8.2" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.8.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.8.4" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T8.1.1.9" class="ltx_tr">
<td id="A1.T8.1.1.9.1" class="ltx_td ltx_align_left">Ensemble Learning Rate</td>
<td id="A1.T8.1.1.9.2" class="ltx_td"></td>
<td id="A1.T8.1.1.9.3" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T8.1.1.9.4" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T8.1.1.9.5" class="ltx_td ltx_align_center">1.00E-04</td>
</tr>
<tr id="A1.T8.1.1.10" class="ltx_tr">
<td id="A1.T8.1.1.10.1" class="ltx_td ltx_align_left">Personal Learning Rate</td>
<td id="A1.T8.1.1.10.2" class="ltx_td"></td>
<td id="A1.T8.1.1.10.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.10.4" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T8.1.1.10.5" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T8.1.1.11" class="ltx_tr">
<td id="A1.T8.1.1.11.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="A1.T8.1.1.11.2" class="ltx_td"></td>
<td id="A1.T8.1.1.11.3" class="ltx_td ltx_align_center">Adam</td>
<td id="A1.T8.1.1.11.4" class="ltx_td ltx_align_center">Adam</td>
<td id="A1.T8.1.1.11.5" class="ltx_td ltx_align_center">Adam</td>
</tr>
<tr id="A1.T8.1.1.12" class="ltx_tr">
<td id="A1.T8.1.1.12.1" class="ltx_td ltx_align_left">Betas</td>
<td id="A1.T8.1.1.12.2" class="ltx_td"></td>
<td id="A1.T8.1.1.12.3" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T8.1.1.12.4" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T8.1.1.12.5" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
</tr>
<tr id="A1.T8.1.1.13" class="ltx_tr">
<td id="A1.T8.1.1.13.1" class="ltx_td ltx_align_left">Eps</td>
<td id="A1.T8.1.1.13.2" class="ltx_td"></td>
<td id="A1.T8.1.1.13.3" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T8.1.1.13.4" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T8.1.1.13.5" class="ltx_td ltx_align_center">1.00E-08</td>
</tr>
<tr id="A1.T8.1.1.14" class="ltx_tr">
<td id="A1.T8.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb">Weight Decay</td>
<td id="A1.T8.1.1.14.2" class="ltx_td ltx_border_bb"></td>
<td id="A1.T8.1.1.14.3" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
<td id="A1.T8.1.1.14.4" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
<td id="A1.T8.1.1.14.5" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
</tr>
</table>
</span></div>
</figure>
<figure id="A1.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Experimental setup details of DynaFed with ConvNet in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></figcaption>
<div id="A1.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:244.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.4pt,40.3pt) scale(0.751445915143252,0.751445915143252) ;">
<table id="A1.T9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T9.1.1.1" class="ltx_tr">
<td id="A1.T9.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A1.T9.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="A1.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T9.1.1.1.3.1" class="ltx_text ltx_font_bold">CIFAR-10</span></td>
<td id="A1.T9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T9.1.1.1.4.1" class="ltx_text ltx_font_bold">CIFAR-100</span></td>
<td id="A1.T9.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T9.1.1.1.5.1" class="ltx_text ltx_font_bold">Flowers102</span></td>
</tr>
<tr id="A1.T9.1.1.2" class="ltx_tr">
<td id="A1.T9.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Local Epoch</td>
<td id="A1.T9.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A1.T9.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T9.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T9.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">1</td>
</tr>
<tr id="A1.T9.1.1.3" class="ltx_tr">
<td id="A1.T9.1.1.3.1" class="ltx_td ltx_align_left">Communication Rounds</td>
<td id="A1.T9.1.1.3.2" class="ltx_td"></td>
<td id="A1.T9.1.1.3.3" class="ltx_td ltx_align_center">500</td>
<td id="A1.T9.1.1.3.4" class="ltx_td ltx_align_center">500</td>
<td id="A1.T9.1.1.3.5" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A1.T9.1.1.4" class="ltx_tr">
<td id="A1.T9.1.1.4.1" class="ltx_td ltx_align_left">Cohort Size</td>
<td id="A1.T9.1.1.4.2" class="ltx_td"></td>
<td id="A1.T9.1.1.4.3" class="ltx_td ltx_align_center">10</td>
<td id="A1.T9.1.1.4.4" class="ltx_td ltx_align_center">10</td>
<td id="A1.T9.1.1.4.5" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="A1.T9.1.1.5" class="ltx_tr">
<td id="A1.T9.1.1.5.1" class="ltx_td ltx_align_left">Batch Size</td>
<td id="A1.T9.1.1.5.2" class="ltx_td"></td>
<td id="A1.T9.1.1.5.3" class="ltx_td ltx_align_center">32</td>
<td id="A1.T9.1.1.5.4" class="ltx_td ltx_align_center">32</td>
<td id="A1.T9.1.1.5.5" class="ltx_td ltx_align_center">32</td>
</tr>
<tr id="A1.T9.1.1.6" class="ltx_tr">
<td id="A1.T9.1.1.6.1" class="ltx_td ltx_align_left">Synthetic Images Learning Rate</td>
<td id="A1.T9.1.1.6.2" class="ltx_td"></td>
<td id="A1.T9.1.1.6.3" class="ltx_td ltx_align_center">5.00E-02</td>
<td id="A1.T9.1.1.6.4" class="ltx_td ltx_align_center">5.00E-02</td>
<td id="A1.T9.1.1.6.5" class="ltx_td ltx_align_center">5.00E-02</td>
</tr>
<tr id="A1.T9.1.1.7" class="ltx_tr">
<td id="A1.T9.1.1.7.1" class="ltx_td ltx_align_left">Distill Interval</td>
<td id="A1.T9.1.1.7.2" class="ltx_td"></td>
<td id="A1.T9.1.1.7.3" class="ltx_td ltx_align_center">1</td>
<td id="A1.T9.1.1.7.4" class="ltx_td ltx_align_center">1</td>
<td id="A1.T9.1.1.7.5" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="A1.T9.1.1.8" class="ltx_tr">
<td id="A1.T9.1.1.8.1" class="ltx_td ltx_align_left">Distill Iteration</td>
<td id="A1.T9.1.1.8.2" class="ltx_td"></td>
<td id="A1.T9.1.1.8.3" class="ltx_td ltx_align_center">20</td>
<td id="A1.T9.1.1.8.4" class="ltx_td ltx_align_center">8</td>
<td id="A1.T9.1.1.8.5" class="ltx_td ltx_align_center">20</td>
</tr>
<tr id="A1.T9.1.1.9" class="ltx_tr">
<td id="A1.T9.1.1.9.1" class="ltx_td ltx_align_left">Distill Step</td>
<td id="A1.T9.1.1.9.2" class="ltx_td"></td>
<td id="A1.T9.1.1.9.3" class="ltx_td ltx_align_center">3000</td>
<td id="A1.T9.1.1.9.4" class="ltx_td ltx_align_center">200</td>
<td id="A1.T9.1.1.9.5" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A1.T9.1.1.10" class="ltx_tr">
<td id="A1.T9.1.1.10.1" class="ltx_td ltx_align_left">Distill Learning Rate</td>
<td id="A1.T9.1.1.10.2" class="ltx_td"></td>
<td id="A1.T9.1.1.10.3" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T9.1.1.10.4" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T9.1.1.10.5" class="ltx_td ltx_align_center">1.00E-04</td>
</tr>
<tr id="A1.T9.1.1.11" class="ltx_tr">
<td id="A1.T9.1.1.11.1" class="ltx_td ltx_align_left" rowspan="2"><span id="A1.T9.1.1.11.1.1" class="ltx_text">Client Learning Rate</span></td>
<td id="A1.T9.1.1.11.2" class="ltx_td ltx_align_left">High Data Heterogeneity</td>
<td id="A1.T9.1.1.11.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.11.4" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.11.5" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T9.1.1.12" class="ltx_tr">
<td id="A1.T9.1.1.12.1" class="ltx_td ltx_align_left">Low Data Heterogeneity</td>
<td id="A1.T9.1.1.12.2" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.12.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.12.4" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T9.1.1.13" class="ltx_tr">
<td id="A1.T9.1.1.13.1" class="ltx_td ltx_align_left">Ensemble Learning Rate</td>
<td id="A1.T9.1.1.13.2" class="ltx_td"></td>
<td id="A1.T9.1.1.13.3" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T9.1.1.13.4" class="ltx_td ltx_align_center">1.00E-04</td>
<td id="A1.T9.1.1.13.5" class="ltx_td ltx_align_center">1.00E-04</td>
</tr>
<tr id="A1.T9.1.1.14" class="ltx_tr">
<td id="A1.T9.1.1.14.1" class="ltx_td ltx_align_left">Personal Learning Rate</td>
<td id="A1.T9.1.1.14.2" class="ltx_td"></td>
<td id="A1.T9.1.1.14.3" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.14.4" class="ltx_td ltx_align_center">1.00E-02</td>
<td id="A1.T9.1.1.14.5" class="ltx_td ltx_align_center">1.00E-02</td>
</tr>
<tr id="A1.T9.1.1.15" class="ltx_tr">
<td id="A1.T9.1.1.15.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="A1.T9.1.1.15.2" class="ltx_td"></td>
<td id="A1.T9.1.1.15.3" class="ltx_td ltx_align_center">Adam</td>
<td id="A1.T9.1.1.15.4" class="ltx_td ltx_align_center">Adam</td>
<td id="A1.T9.1.1.15.5" class="ltx_td ltx_align_center">Adam</td>
</tr>
<tr id="A1.T9.1.1.16" class="ltx_tr">
<td id="A1.T9.1.1.16.1" class="ltx_td ltx_align_left">Betas</td>
<td id="A1.T9.1.1.16.2" class="ltx_td"></td>
<td id="A1.T9.1.1.16.3" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T9.1.1.16.4" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
<td id="A1.T9.1.1.16.5" class="ltx_td ltx_align_center">(0.9, 0.999)</td>
</tr>
<tr id="A1.T9.1.1.17" class="ltx_tr">
<td id="A1.T9.1.1.17.1" class="ltx_td ltx_align_left">Eps</td>
<td id="A1.T9.1.1.17.2" class="ltx_td"></td>
<td id="A1.T9.1.1.17.3" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T9.1.1.17.4" class="ltx_td ltx_align_center">1.00E-08</td>
<td id="A1.T9.1.1.17.5" class="ltx_td ltx_align_center">1.00E-08</td>
</tr>
<tr id="A1.T9.1.1.18" class="ltx_tr">
<td id="A1.T9.1.1.18.1" class="ltx_td ltx_align_left ltx_border_bb">Weight Decay</td>
<td id="A1.T9.1.1.18.2" class="ltx_td ltx_border_bb"></td>
<td id="A1.T9.1.1.18.3" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
<td id="A1.T9.1.1.18.4" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
<td id="A1.T9.1.1.18.5" class="ltx_td ltx_align_center ltx_border_bb">1.00E-02</td>
</tr>
</table>
</span></div>
</figure>
<div id="A1.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p3.1" class="ltx_p"><span id="A1.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Hyperparameter Selection in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</span>
For the centralized training in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we used the following hyperparameter settings. For image data, the batch size was set to 32, and the optimizer was AdamW with weight decay set to 0.9 and cosine annealing learning rate decay. The initial learning rate was 1.00E-04 for CIFAR-10/CIFAR-100 and 3.00E-04 for Flowers102. For audio data, the batch size was set to 64, and the optimizer was Adam with weight decay set to 1.00E-04. The initial learning rate was 5.00E-05 for both datasets.</p>
</div>
<div id="A1.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p4.1" class="ltx_p">For the standard FL training in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-03. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, both the local and server learning rates are set to 1.00E-01. For the Flowers102 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-02. For all audio data, the experimental settings strictly follow the FedAudio benchmark <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="A1.SS2.SSS3.p5" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p5.1" class="ltx_p">For the <span id="A1.SS2.SSS3.p5.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> training in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Performance Comparison with State-of-the-Art FL Methods ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 5.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 3.00E-04 and the server learning rate is set to 7.00E-04. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 5.00E-04 and the server learning rate is set to 1.00E-03. For the Flowers102 dataset, the local learning rate is set to 5.00E-03 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-04 and the server learning rate is set to 1.00E-04. For audio data, the batch size is set to 16, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. We set the local learning rate to 5.00E-02 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-03 and the server learning rate is set to 5.00E-04 for both two datasets.</p>
</div>
<div id="A1.SS2.SSS3.p6" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p6.1" class="ltx_p"><span id="A1.SS2.SSS3.p6.1.1" class="ltx_text ltx_font_bold">Hyperparameter Selection in Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</span> For the centralized training in Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the hyperparameter selection is follows. For all image data, we set the batch size to 32, and choose AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov &amp; Hutter (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> as the optimizer with weight decay equal to 0.9 and cosine annealing learning rate decay. For the CIFAR-10 dataset, we used an initial learning rate of 8.00E-06; for the CIFAR-100 dataset, we used an initial learning rate of 5.00E-06; for the Flowers102 dataset, we used an initial learning rate of 2.00E-05.</p>
</div>
<div id="A1.SS2.SSS3.p7" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p7.1" class="ltx_p">For the standard FL training in Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we use the hyperparameter setting as follows. For all image data, we set the batch size to 32, and choose SGD as the local optimizer with weight decay equal to 5.00E-04. With FedOpt as the server aggregator, we choose Adam as the server optimizer. For the CIFAR-10 dataset, we choose the local learning rate as 1.00E-01 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-03 and the server learning rate as 1.00E-03 with FedOpt as the server aggregator. For the CIFAR-100 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 5.00E-03 and the server learning rate as 7.00E-03 with FedOpt as the server aggregator. For the Flowers102 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 5.00E-04 with FedOpt as the server aggregator.</p>
</div>
<div id="A1.SS2.SSS3.p8" class="ltx_para ltx_noindent">
<p id="A1.SS2.SSS3.p8.1" class="ltx_p">For <span id="A1.SS2.SSS3.p8.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> training in Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Understanding GPT-FL ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we use the hyperparameter setting as follows. For all image data, we set the batch size to 32, and choose SGD as the local optimizer with weight decay equal to 5.00E-04. With FedOpt as the server aggregator, we choose Adam as the server optimizer. For CIFAR-10 dataset, we choose the local learning rate as 1.00E-07 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-07 and the server learning rate as 1.00E-05 with FedOpt as the server aggregator. For CIFAR-100 dataset, we choose the local learning rate as 1.00E-04 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 1.00E-05 with FedOpt as server aggregator. For Flowers102 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 1.00E-04 with FedOpt as the server aggregator.</p>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Quality of the Generated Synthetic Data</h3>

<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.p1.1" class="ltx_p">As shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="A1.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">GPT-FL</span> outperforms both generated data-based approaches FedGen and DynaFed significantly across all experimental conditions. One plausible reason for this could be associated with the quality of the generated synthetic data. Specifically, both FedGen and DynaFed rely on training MLP-based generator networks to ensemble user information in a data-free manner, where the lightweight generator may have limitations in generating high-fidelity data. The results of Flowers102 provide empirical evidence that such a lightweight generator has constrained capabilities in synthesizing image output on input images with larger sizes, making it challenging for the global model to converge. To illustrate this, Figure <a href="#A1.F9" title="Figure 9 ‣ A.3 Quality of the Generated Synthetic Data ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and Figure <a href="#A1.F9" title="Figure 9 ‣ A.3 Quality of the Generated Synthetic Data ‣ Appendix A Appendix ‣ GPT-FL: Generative Pre-trained Model-Assisted Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> illustrate the synthetic images generated by <span id="A1.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-FL</span> and DynaFed, respectively. As shown, the learned generator of DynaFed fails to generate high-fidelity data as in <span id="A1.SS3.p1.1.3" class="ltx_text ltx_font_typewriter">GPT-FL</span>.</p>
</div>
<figure id="A1.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F9.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:194.8pt;"><img src="/html/2306.02210/assets/figures/gen_pic_gpt.png" id="A1.F9.1.g1" class="ltx_graphics ltx_img_portrait" width="548" height="819" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Synthetic CIFAR-10 data by <span id="A1.F9.1.2.1" class="ltx_text ltx_font_typewriter">GPT-FL</span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F9.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:194.8pt;"><img src="/html/2306.02210/assets/figures/dynafed.png" id="A1.F9.2.g1" class="ltx_graphics ltx_img_portrait" width="548" height="821" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Synthetic CIFAR-10 data by DynaFed.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.02209" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.02210" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.02210">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.02210" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.02211" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 02:42:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
