<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1903.08621] Column2Vec: Structural Understanding via Distributed Representations of Database Schemas</title><meta property="og:description" content="We present Column2Vec, a distributed representation of database columns based on column metadata.
Our distributed representation has several applications.
Using known names for groups of columns (i.e., a table name), w…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Column2Vec: Structural Understanding via Distributed Representations of Database Schemas">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Column2Vec: Structural Understanding via Distributed Representations of Database Schemas">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1903.08621">

<!--Generated on Sat Mar 16 17:44:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on 2019.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Column2Vec: Structural Understanding via Distributed Representations of Database Schemas</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael J. Mior, Alexander G. Ororbia
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mmior@cs.rit.edu,%20ago@cs.rit.edu">mmior@cs.rit.edu, ago@cs.rit.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Rochester Institute of Technology</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">102 Lomb Memorial Drive</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Rochester</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_state">New York</span><span id="id5.5.id5" class="ltx_text ltx_affiliation_postcode">14623-5608</span>
</span></span></span>
</div>
<div class="ltx_dates">(2019; Date: 18 March 2019)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id6.id1" class="ltx_p">We present Column2Vec, a distributed representation of database columns based on column metadata.
Our distributed representation has several applications.
Using known names for groups of columns (i.e., a table name), we train a model to generate an appropriate name for columns in an unnamed table.
We demonstrate the viability of our approach using schema information collected from open source applications on GitHub.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2019</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>aiDM ’19: Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management; July 5, 2019; Amsterdam, NL</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>aiDM ’19: Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management, July 5, 2019, Amsterdam, NL</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">It has become increasingly common for enterprise data management platforms to soak up as much data as possible from a variety of sources.
These “data lakes” are often lacking in metadata which makes the structure of stored data challenging to understand.
This limits the usability of this additional data since significant time must be invested by data scientists when adding useful metadata before the data can be analyzed or integrated with other sources.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One common operation is database normalization. When performing normalization, it is common to decompose a single database table into multiple tables based on dependencies which are inferred from the data.
The final schema typically more closely represents logical entities in the underlying data.
Consider the single table below:</p>
</div>
<div id="S1.p3" class="ltx_para">
<pre id="S1.p3.1" class="ltx_verbatim ltx_font_typewriter">
  authorID, firstName, lastName, ISBN, title
</pre>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This table contains information on both authors and books.
A standard normalization algorithm to convert this table into Boyce-Codd Normal Form (BCNF) <cite class="ltx_cite ltx_citemacro_citep">(Codd, <a href="#bib.bib6" title="" class="ltx_ref">1974</a>)</cite> would produce three tables:</p>
</div>
<div id="S1.p5" class="ltx_para">
<pre id="S1.p5.1" class="ltx_verbatim ltx_font_typewriter">
  authorID, firstName, lastName
  ISBN, title
  authorID, ISBN
</pre>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">These tables represent information on authors, books, and the relationship between them.
This normal form is useful for data integration tasks but there exists no obvious approach for producing meaningful names for each of these tables.
Currently these names are manually assigned by the database designer.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our work makes the following contributions:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A semantic embedding of database column names</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A method for using these embeddings to assign meaningful names to tables containing a given set of column names</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A metric for evaluating generated table names which shows the usefulness of our prediction.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">One of the central motivations behind this work comes from recent progress that has been made in the area of natural language processing (NLP) due to the use of distributed representations.
Distributed representations, or <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">embeddings</em>, refer to the mapping of input examples to vectors of values, of possibly lower dimensionality than the input itself.
These embeddings are usually produced through the use of an artificial neural network (ANN).
Each element in one such vector is not necessarily associated with one particular concept, feature, or object, but rather works in tandem with the other elements in the same vector to represent a set of features or concepts that describe the input itself.
The Word2vec family of models, i.e., skip-gram and continuous bag of words, have yielded some of the most widely-used embeddings in NLP, where a simple feed-forward ANN language model is trained on a large collection of documents.
The internal weight vectors, which each map to a particular token, are then used in some subsequent predictive language task, e.g., text classification or chunking.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Other variants have been proposed since Word2vec’s initial public release, such as GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2014</a>)</cite>, ELMo <cite class="ltx_cite ltx_citemacro_citep">(Peters et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>, and BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>.
Interestingly enough, these representations can be composed into representations of phrases and sentences, by averaging, summing, or concatenating the embeddings for each of the constituent words, yielding a possible distributed representation of the phrase/sentence itself.
Embeddings have found use in domains even outside of NLP, such as in graph/network representation <cite class="ltx_cite ltx_citemacro_citep">(Grover and
Leskovec, <a href="#bib.bib9" title="" class="ltx_ref">2016</a>; Narayanan et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>, including entity resolution <cite class="ltx_cite ltx_citemacro_citep">(Ebraheem et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>, concept modeling <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>; Moody, <a href="#bib.bib14" title="" class="ltx_ref">2016</a>; Sherkat and
Milios, <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> and data curation <cite class="ltx_cite ltx_citemacro_citep">(Thirumuruganathan et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>.
In short, distributed representations have facilitated the construction of a useful, alternative means of comparing, aggregating, and manipulating the fundamental elements of a data type.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1903.08621/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="295" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Recurrent generative model of table titles.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this work, for tables themselves, we hypothesize that the same potential for discovering aggregated embeddings exists as well – for each column name, we could find its particular distributed representation, and, after examining the entire set of column names, we could compose a plausible distributed representation of the entire table itself by applying some aggregation operation to the constituent embeddings.
Furthermore, these aggregate table embeddings could be used in the generation of plausible names for the tables themselves.
In this work, we will propose two possible ways in which this might be implemented and empirically explore the effects of one of these.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Models for Table Name Generation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will describe two architectures, one simple and one complex, for generating table names.
We will then provide some experimental results for the first model in this study.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Model # 1</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">There are two main steps in this approach to generating database table names.
First, we produce a word embedding for each table and column name as discussed in Section <a href="#S3.SS1.SSS1" title="3.1.1. Embeddings for Database Columns ‣ 3.1. Model # 1 ‣ 3. Models for Table Name Generation ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>.
Then, as discussed in Section <a href="#S3.SS1.SSS2" title="3.1.2. Table Name Generation ‣ 3.1. Model # 1 ‣ 3. Models for Table Name Generation ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a> we use the vectors generated for each column name in a table to predict a meaningful name for the table.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Embeddings for Database Columns</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">To generate word embeddings for Column2Vec, we make use of fastText <cite class="ltx_cite ltx_citemacro_citep">(Bojanowski et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>.
This is primarily because fastText allows embeddings to be generated for terms which are outside of the original vocabulary the model was trained on.
This is important in our setting since table and column names contain significant variation and may not appear as an exact match in any existing data.
For example, approximately half of the table names we see in the data set used for our preliminary results in Section <a href="#S4" title="4. Preliminary results ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> appear only once.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">To train the fastText model, we first construct documents based on the names of tables and columns of known database schemas.
For example, one document might consist of the string <span id="S3.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_typewriter">"authors authorID firstName lastName"</span>.
Training on a collection of such documents allows us to generate embeddings, or <em id="S3.SS1.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">word vectors</em> for each of the table and column names in our training set.
In addition, fastText also enables us to generate word vectors for terms outside of this vocabulary by looking at subword information.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Table Name Generation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">For our generation task, we aim to take as input a list of column names and assign the most meaningful table name from tables observed in our training set.
Our prediction is based on comparing the similarity of the word vectors for each column vectors.
Since we want to incorporate semantic context from all column names, we need to combine the individual word vectors generated for each column.
Mikolov et al. <cite class="ltx_cite ltx_citemacro_citep">(Mikolov et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2013</a>)</cite> showed that the summation of word vectors is most likely to produce a vector for a term which is semantically similar to the composition of each term.
In our case, we expect the sum of the word vectors associated with a set of columns will produce a vector which is close (in vector space) to a word vector representation of a viable table name.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">To produce a table name that might be suitable for the given set of columns, we use the nearest neighbor based on cosine similarity.
That is, after summing the individual column vectors, we select the table name which corresponds to the word vector most similar to this summed vector.
Currently our model considers/examines only the single closest vector.
However, in future work, we intend to investigate the case where <math id="S3.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mi id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><ci id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">k</annotation></semantics></math> nearest closest neighbors are considered for further evaluation.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Model # 2</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Another approach we propose is based on a recurrent neural architecture that learns a generative model of table names, conditioned on a distributed representation of table metadata.
A graphical depiction of a basic version of this model can be found in Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The generative model can be decomposed into three general modules: 1) a column name encoder/processor, 2) a table representation aggregation function, and 3) a conditional name generator.
All three functions could be as simple as feed-forward network models or as complex and powerful as a gated recurrent neural network (RNN), such as Long Short Term Memory <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and
Schmidhuber, <a href="#bib.bib10" title="" class="ltx_ref">1997</a>)</cite> or <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\Delta</annotation></semantics></math>-RNN <cite class="ltx_cite ltx_citemacro_citep">(Ororbia II
et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>.
We envision the column name encoder and table name generator to operate at the character symbolic level (or the subword level) to circumvent the problem of an incredibly high-dimensional input/output space that word-level models have to face.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The column name encoder can be a recurrent network that builds a stateful representation of a single column by iteratively processing the underlying characters that compose the word or phrase used to label the column name itself.
This column name encoder would be shared across column names to drastically reduce the number of parameters required in our model and allow it to easily handle a variable number of columns.
A more difficult, but perhaps quite fruitful, extension of this encoder would be to have it process the data values associated with a particular column name as well.
This encoder would be applied to each column name and output a set of <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">K</annotation></semantics></math> column name representations.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">K</annotation></semantics></math> embeddings produced by the column name encoder would then be run through an aggregation function, which could itself be a nonlinear multilayer perceptron (MLP) or a simple function such as averaging or (weighted) summation.
This function’s primarily role is to create a single, fixed-length representation of a set of column names, or rather, the table of interest itself.
This table representation would then be used to guide a generative model of text, which could be a simple RNN as depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
By focusing on a character or subword-level RNN generative model, we can naturally handle names of variable length and even potentially generate non-standard symbols (such as alphanumeric strings).
The types of names that would be generated by this model would largely depend on the table dataset used to construct the overall model, however, if large enough, the model might be able to produce some interesting, creative table names or even a set of candidates that the human user would finally select from and/or error-correct (providing additional feedback to the model).</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Parameters of the entire end-to-end model could be learned using reverse-mode differentiation to optimize an objective such as the negative log likelihood of the name text in the training set.
Since the entire system is soft, or rather, makes use of differentiable nonlinear transformations, calculating gradients would not be difficult, though the path of credit assignment could potentially be long depending on how long some table names and column names are (since in RNN structures, the model parameters are shared over time-steps, so in the case of characters, we apply the RNN parameters once per character).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Preliminary results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To examine the effectiveness of our first model, we used a set of table schemas (column and table names) collected from a crawl of open source repositories on GitHub <cite class="ltx_cite ltx_citemacro_citep">(Hoffa, <a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>.
Files in all repositories were checked for the syntactically valid <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">CREATE TABLE</span> statements and the table and column names were extracted from these statements.
This resulted in a total of <math id="S4.p1.1.m1.2" class="ltx_Math" alttext="436,545" display="inline"><semantics id="S4.p1.1.m1.2a"><mrow id="S4.p1.1.m1.2.3.2" xref="S4.p1.1.m1.2.3.1.cmml"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">436</mn><mo id="S4.p1.1.m1.2.3.2.1" xref="S4.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.p1.1.m1.2.2" xref="S4.p1.1.m1.2.2.cmml">545</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.2b"><list id="S4.p1.1.m1.2.3.1.cmml" xref="S4.p1.1.m1.2.3.2"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">436</cn><cn type="integer" id="S4.p1.1.m1.2.2.cmml" xref="S4.p1.1.m1.2.2">545</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.2c">436,545</annotation></semantics></math> tables combined with the names of each column in the table.
We leave implementation and evaluation of our second model as future work.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Data cleaning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The dataset pulled from GitHub contains a significant amount of test and dummy data which is not useful for our problem.
For example, a table with the name <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">bb</span> and columns <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">col1</span> and <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">col2</span> would be discarded.
We implemented a simple set of rules to filter the extracted information:
(1) trigrams appearing in table or column names appearing only once, eliminating random entries,
(2) names with special characters,
(3) names with a large number of digits, and
(4) names which consist of only two repeated characters (e.g., <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">bb</span>).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Model training</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The data was split into 90% training data with 10% reserved for testing.
Word vectors were trained using the fastText skip-gram model with each document consisting of the table name and associated column names.
We performed hyper-parameter optimization using the tree-structured Parzen estimator approach described by Bergstra et al. <cite class="ltx_cite ltx_citemacro_citep">(Bergstra et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2011</a>)</cite> on a subset of the data.
A k-nearest neighbors model was then trained using the table name word vectors (in the training set) to generate table names as described in Section <a href="#S3" title="3. Models for Table Name Generation ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Table name quality</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table names may consist of multiple words concatenated together.
A table storing blog posts may be called <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">blogposts</span>, but we might consider a suggestion of the name <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">posts</span> to be a reasonable substitute.
We want to be able to identify how well the components of our predicted names match the original.
To identify the components of a name, we first attempt to split names into words via a dynamic programming approach that aims to infer the position of spaces by attempting to maximize the probability of individual word frequencies <cite class="ltx_cite ltx_citemacro_citep">(Anderson, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.
This is based off of a sample of words from the English Wikipedia corpus.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">To evaluate the quality of the generated table names, we define a metric based on the <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">F_{1}</annotation></semantics></math> score <cite class="ltx_cite ltx_citemacro_citep">(Chinchor, <a href="#bib.bib5" title="" class="ltx_ref">1992</a>)</cite> which combines precision and recall.
We use the words split from the table names as mentioned above when calculating the <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">F</mi><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝐹</ci><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">F_{1}</annotation></semantics></math> score.
However, we also want words which are semantically similar to rank high.
For example, the name <span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_typewriter">books</span> may a suitable alternative to the name <span id="S4.SS3.p2.2.2" class="ltx_text ltx_font_typewriter">library</span>.
To capture these relationships, we use the path similarity from WordNet::Similarity <cite class="ltx_cite ltx_citemacro_citep">(Pedersen
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2004</a>)</cite>.
When computing precision and recall, instead of using intersection between the predicted and original names, we calculate <em id="S4.SS3.p2.2.3" class="ltx_emph ltx_font_italic">fuzzy</em> precision and recall using this WordNet similarity metric.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1903.08621/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="177" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Cumulative distribution of evaluation metric</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ 4.3. Table name quality ‣ 4. Preliminary results ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the cumulative distribution of the evaluation metric.
Approximately one third of the predicted table names evaluate to zero.
Of these, roughly 60% have the same two predicted tables, <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">vtp</span> and <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">defaultrequiredlengthncharcolumns</span>.
Neither of these are semantically meaningful.
Further preprocessing to remove names with limited semantic value will likely improve this result.
We also see that approximately 1% of the values have the highest possible score of 1.0.
In this case, names differed just in pluralization and punctuation, such as <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">recipe_ingredients</span> and <span id="S4.SS3.p3.1.4" class="ltx_text ltx_font_typewriter">recipeingredient</span>.
We provide examples of results which fall between these two extremes in the following section.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Table Name Samples</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.4. Table Name Samples ‣ 4. Preliminary results ‣ Column2Vec: Structural Understanding via Distributed Representations of Database Schemas" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives examples of the original names assigned to tables, i.e., the ground truth, as well as the predicted name and the value for our evaluation metric.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Metric value</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Original name</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Predicted name</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">tcardfieldoption</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">ttextboxsettings</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center">0.13</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">subscription</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">oc_product_description</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center">0.20</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center">portfolio</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">projects</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center">0.25</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center">campaignchain_group</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center">userseries</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center">0.33</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">comments</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center">content</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center">0.39</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center">mg_tag_properties</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center">forum_articles_tags</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.8.7.1" class="ltx_td ltx_align_center">0.43</td>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center">users_clients</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center">osiam_client</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.9.8.1" class="ltx_td ltx_align_center">0.50</td>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_center">item</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_center">symbol</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.10.9.1" class="ltx_td ltx_align_center">0.56</td>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_align_center">useridentity</td>
<td id="S4.T1.1.10.9.3" class="ltx_td ltx_align_center">usercredentials</td>
</tr>
<tr id="S4.T1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.11.10.1" class="ltx_td ltx_align_center">0.67</td>
<td id="S4.T1.1.11.10.2" class="ltx_td ltx_align_center">initializedmodules</td>
<td id="S4.T1.1.11.10.3" class="ltx_td ltx_align_center">module</td>
</tr>
<tr id="S4.T1.1.12.11" class="ltx_tr">
<td id="S4.T1.1.12.11.1" class="ltx_td ltx_align_center">0.71</td>
<td id="S4.T1.1.12.11.2" class="ltx_td ltx_align_center">artist_tag_raw</td>
<td id="S4.T1.1.12.11.3" class="ltx_td ltx_align_center">release_tag_raw</td>
</tr>
<tr id="S4.T1.1.13.12" class="ltx_tr">
<td id="S4.T1.1.13.12.1" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T1.1.13.12.2" class="ltx_td ltx_align_center">sales_order_item</td>
<td id="S4.T1.1.13.12.3" class="ltx_td ltx_align_center">sales_order_line</td>
</tr>
<tr id="S4.T1.1.14.13" class="ltx_tr">
<td id="S4.T1.1.14.13.1" class="ltx_td ltx_align_center">0.80</td>
<td id="S4.T1.1.14.13.2" class="ltx_td ltx_align_center">arena_team</td>
<td id="S4.T1.1.14.13.3" class="ltx_td ltx_align_center">arena_team_member</td>
</tr>
<tr id="S4.T1.1.15.14" class="ltx_tr">
<td id="S4.T1.1.15.14.1" class="ltx_td ltx_align_center">0.86</td>
<td id="S4.T1.1.15.14.2" class="ltx_td ltx_align_center">inserttesttablefilter</td>
<td id="S4.T1.1.15.14.3" class="ltx_td ltx_align_center">inserttesttable</td>
</tr>
<tr id="S4.T1.1.16.15" class="ltx_tr">
<td id="S4.T1.1.16.15.1" class="ltx_td ltx_align_center">1.00</td>
<td id="S4.T1.1.16.15.2" class="ltx_td ltx_align_center">position</td>
<td id="S4.T1.1.16.15.3" class="ltx_td ltx_align_center">positions</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Example table name predictions</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Although the examples above suggest that our metric is useful, in the future we intend to perform a more thorough evaluation to determine whether this metric correlates with human judgment.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">To better demonstrate our generation/evaluation process, consider predicting a name for a table with columns <span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_typewriter">id</span>, <span id="S4.SS4.p3.1.2" class="ltx_text ltx_font_typewriter">calendarid</span>, <span id="S4.SS4.p3.1.3" class="ltx_text ltx_font_typewriter">name</span>, <span id="S4.SS4.p3.1.4" class="ltx_text ltx_font_typewriter">eventdate</span>, and <span id="S4.SS4.p3.1.5" class="ltx_text ltx_font_typewriter">locutionid</span>.
We generate word vectors for each of these columns and sum them.
When searching for the table name with the closest word vector, we find <span id="S4.SS4.p3.1.6" class="ltx_text ltx_font_typewriter">eventdates</span>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.2" class="ltx_p">The original name given to this table was <span id="S4.SS4.p4.2.1" class="ltx_text ltx_font_typewriter">holidaydates</span>.
To evaluate this according to our metric we first split each name into words giving [<span id="S4.SS4.p4.2.2" class="ltx_text ltx_font_typewriter">holiday</span>, <span id="S4.SS4.p4.2.3" class="ltx_text ltx_font_typewriter">dates</span>] and <span id="S4.SS4.p4.2.4" class="ltx_text ltx_font_typewriter">event</span>, <span id="S4.SS4.p4.2.5" class="ltx_text ltx_font_typewriter">dates</span>].
Based on the WordNet path similarity, <span id="S4.SS4.p4.2.6" class="ltx_text ltx_font_typewriter">holiday</span> and <span id="S4.SS4.p4.2.7" class="ltx_text ltx_font_typewriter">event</span> have a similarity of 0.14.
Since <span id="S4.SS4.p4.2.8" class="ltx_text ltx_font_typewriter">dates</span> matches exactly, we end up with precision and recall values of <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="P=R=(1.14/2)=0.57" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mrow id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml"><mi id="S4.SS4.p4.1.m1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.3.cmml">P</mi><mo id="S4.SS4.p4.1.m1.1.1.4" xref="S4.SS4.p4.1.m1.1.1.4.cmml">=</mo><mi id="S4.SS4.p4.1.m1.1.1.5" xref="S4.SS4.p4.1.m1.1.1.5.cmml">R</mi><mo id="S4.SS4.p4.1.m1.1.1.6" xref="S4.SS4.p4.1.m1.1.1.6.cmml">=</mo><mrow id="S4.SS4.p4.1.m1.1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p4.1.m1.1.1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p4.1.m1.1.1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.1.cmml"><mn id="S4.SS4.p4.1.m1.1.1.1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.1.1.1.2.cmml">1.14</mn><mo id="S4.SS4.p4.1.m1.1.1.1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.1.1.cmml">/</mo><mn id="S4.SS4.p4.1.m1.1.1.1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S4.SS4.p4.1.m1.1.1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.SS4.p4.1.m1.1.1.7" xref="S4.SS4.p4.1.m1.1.1.7.cmml">=</mo><mn id="S4.SS4.p4.1.m1.1.1.8" xref="S4.SS4.p4.1.m1.1.1.8.cmml">0.57</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><apply id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1"><and id="S4.SS4.p4.1.m1.1.1a.cmml" xref="S4.SS4.p4.1.m1.1.1"></and><apply id="S4.SS4.p4.1.m1.1.1b.cmml" xref="S4.SS4.p4.1.m1.1.1"><eq id="S4.SS4.p4.1.m1.1.1.4.cmml" xref="S4.SS4.p4.1.m1.1.1.4"></eq><ci id="S4.SS4.p4.1.m1.1.1.3.cmml" xref="S4.SS4.p4.1.m1.1.1.3">𝑃</ci><ci id="S4.SS4.p4.1.m1.1.1.5.cmml" xref="S4.SS4.p4.1.m1.1.1.5">𝑅</ci></apply><apply id="S4.SS4.p4.1.m1.1.1c.cmml" xref="S4.SS4.p4.1.m1.1.1"><eq id="S4.SS4.p4.1.m1.1.1.6.cmml" xref="S4.SS4.p4.1.m1.1.1.6"></eq><share href="#S4.SS4.p4.1.m1.1.1.5.cmml" id="S4.SS4.p4.1.m1.1.1d.cmml" xref="S4.SS4.p4.1.m1.1.1"></share><apply id="S4.SS4.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1"><divide id="S4.SS4.p4.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.1.1"></divide><cn type="float" id="S4.SS4.p4.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.1.2">1.14</cn><cn type="integer" id="S4.SS4.p4.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.1.3">2</cn></apply></apply><apply id="S4.SS4.p4.1.m1.1.1e.cmml" xref="S4.SS4.p4.1.m1.1.1"><eq id="S4.SS4.p4.1.m1.1.1.7.cmml" xref="S4.SS4.p4.1.m1.1.1.7"></eq><share href="#S4.SS4.p4.1.m1.1.1.1.cmml" id="S4.SS4.p4.1.m1.1.1f.cmml" xref="S4.SS4.p4.1.m1.1.1"></share><cn type="float" id="S4.SS4.p4.1.m1.1.1.8.cmml" xref="S4.SS4.p4.1.m1.1.1.8">0.57</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">P=R=(1.14/2)=0.57</annotation></semantics></math>.
Our final metric value is then <math id="S4.SS4.p4.2.m2.1" class="ltx_Math" alttext="2PR/(P+R)=0.57" display="inline"><semantics id="S4.SS4.p4.2.m2.1a"><mrow id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml"><mrow id="S4.SS4.p4.2.m2.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.cmml"><mrow id="S4.SS4.p4.2.m2.1.1.1.3" xref="S4.SS4.p4.2.m2.1.1.1.3.cmml"><mn id="S4.SS4.p4.2.m2.1.1.1.3.2" xref="S4.SS4.p4.2.m2.1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS4.p4.2.m2.1.1.1.3.1" xref="S4.SS4.p4.2.m2.1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.2.m2.1.1.1.3.3" xref="S4.SS4.p4.2.m2.1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p4.2.m2.1.1.1.3.1a" xref="S4.SS4.p4.2.m2.1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p4.2.m2.1.1.1.3.4" xref="S4.SS4.p4.2.m2.1.1.1.3.4.cmml">R</mi></mrow><mo id="S4.SS4.p4.2.m2.1.1.1.2" xref="S4.SS4.p4.2.m2.1.1.1.2.cmml">/</mo><mrow id="S4.SS4.p4.2.m2.1.1.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p4.2.m2.1.1.1.1.1.2" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p4.2.m2.1.1.1.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.cmml"><mi id="S4.SS4.p4.2.m2.1.1.1.1.1.1.2" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.2.cmml">P</mi><mo id="S4.SS4.p4.2.m2.1.1.1.1.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mi id="S4.SS4.p4.2.m2.1.1.1.1.1.1.3" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.3.cmml">R</mi></mrow><mo stretchy="false" id="S4.SS4.p4.2.m2.1.1.1.1.1.3" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS4.p4.2.m2.1.1.2" xref="S4.SS4.p4.2.m2.1.1.2.cmml">=</mo><mn id="S4.SS4.p4.2.m2.1.1.3" xref="S4.SS4.p4.2.m2.1.1.3.cmml">0.57</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><apply id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1"><eq id="S4.SS4.p4.2.m2.1.1.2.cmml" xref="S4.SS4.p4.2.m2.1.1.2"></eq><apply id="S4.SS4.p4.2.m2.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1"><divide id="S4.SS4.p4.2.m2.1.1.1.2.cmml" xref="S4.SS4.p4.2.m2.1.1.1.2"></divide><apply id="S4.SS4.p4.2.m2.1.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.1.3"><times id="S4.SS4.p4.2.m2.1.1.1.3.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1.3.1"></times><cn type="integer" id="S4.SS4.p4.2.m2.1.1.1.3.2.cmml" xref="S4.SS4.p4.2.m2.1.1.1.3.2">2</cn><ci id="S4.SS4.p4.2.m2.1.1.1.3.3.cmml" xref="S4.SS4.p4.2.m2.1.1.1.3.3">𝑃</ci><ci id="S4.SS4.p4.2.m2.1.1.1.3.4.cmml" xref="S4.SS4.p4.2.m2.1.1.1.3.4">𝑅</ci></apply><apply id="S4.SS4.p4.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1.1.1"><plus id="S4.SS4.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.1"></plus><ci id="S4.SS4.p4.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.2">𝑃</ci><ci id="S4.SS4.p4.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.1.1.1.1.3">𝑅</ci></apply></apply><cn type="float" id="S4.SS4.p4.2.m2.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.3">0.57</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">2PR/(P+R)=0.57</annotation></semantics></math>.
Note that in this case, we have no indication based on column names that the table specifically refers to holidays.
In the future, we will explore other sources of contextual information which may help produce more accurate results.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Generating meaningful names for tables given only constituent column names is a challenging problem.
Our approach, based on distributed representations, is able to generate meaningful table names given the names of the columns contained in the table.
While our metric does prove useful, additional work is needed to compare it properly against human judgment.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We believe that incorporating information on the data stored in these columns (e.g. data type and value distribution) will make this representation even more useful. In addition to generating tables names, such a representation would likely benefit other data integration tasks, e.g., deciding which tables in a large set may be meaningfully joined together.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson (2018)</span>
<span class="ltx_bibblock">
Derek Anderson.
2018.

</span>
<span class="ltx_bibblock">Word Ninja.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://pypi.org/project/wordninja/0.1.5/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/wordninja/0.1.5/</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergstra et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
James Bergstra, Rémi
Bardenet, Yoshua Bengio, and Balázs
Kégl. 2011.

</span>
<span class="ltx_bibblock">Algorithms for Hyper-parameter Optimization. In
<em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th International Conference on
Neural Information Processing Systems</em> <em id="bib.bib3.4.2" class="ltx_emph ltx_font_italic">(NIPS’11)</em>.
Curran Associates Inc., USA,
2546–2554.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard
Grave, Armand Joulin, and Tomas
Mikolov. 2017.

</span>
<span class="ltx_bibblock">Enriching Word Vectors with Subword Information.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Transactions of the Association for
Computational Linguistics</em> 5 (2017),
135–146.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chinchor (1992)</span>
<span class="ltx_bibblock">
Nancy Chinchor.
1992.

</span>
<span class="ltx_bibblock">MUC-4 evaluation metrics. In
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">MUC 1992</em>. Association for
Computational Linguistics, McLean, Virginia,
22–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Codd (1974)</span>
<span class="ltx_bibblock">
Edgar F. Codd.
1974.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Recent Investigations into Relational Data
Base Systems</em>.

</span>
<span class="ltx_bibblock">Technical Report RJ1385.
IBM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin
et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2018.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language, Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1810.04805
(2018), 14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebraheem et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Muhammad Ebraheem,
Saravanan Thirumuruganathan, Shafiq R.
Joty, Mourad Ouzzani, and Nan Tang.
2017.

</span>
<span class="ltx_bibblock">DeepER - Deep Entity Resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1710.00597
(2017), 14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grover and
Leskovec (2016)</span>
<span class="ltx_bibblock">
Aditya Grover and Jure
Leskovec. 2016.

</span>
<span class="ltx_bibblock">node2vec: Scalable feature learning for networks.
In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGKDD international
conference on Knowledge discovery and data mining</em>.
ACM, San Francisco, CA,
855–864.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and
Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and
Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Neural computation</em> 9,
8 (1997), 1735–1780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffa (2016)</span>
<span class="ltx_bibblock">
Felipe Hoffa.
2016.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Retrieved Feb. 15 2019 from
https://medium.com/google-cloud/github-on-bigquery-analyze-all-the-code-b3576fd2b150.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Shaohua Li, Tat-Seng
Chua, Jun Zhu, and Chunyan Miao.
2016.

</span>
<span class="ltx_bibblock">Generative Topic Embedding: a Continuous
Representation of Documents (Extended Version with Proofs).

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1606.02979
(2016), 13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya
Sutskever, Kai Chen, Greg Corrado, and
Jeffrey Dean. 2013.

</span>
<span class="ltx_bibblock">Distributed Representations of Words and Phrases
and Their Compositionality. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
26th International Conference on Neural Information Processing Systems -
Volume 2</em> <em id="bib.bib13.4.2" class="ltx_emph ltx_font_italic">(NIPS’13)</em>. Curran
Associates Inc., USA, 3111–3119.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moody (2016)</span>
<span class="ltx_bibblock">
Christopher E. Moody.
2016.

</span>
<span class="ltx_bibblock">Mixing Dirichlet Topic Models and Word Embeddings
to Make lda2vec.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1605.02019
(2016), 8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Annamalai Narayanan,
Mahinthan Chandramohan, Rajasekar
Venkatesan, Lihui Chen, Yang Liu, and
Shantanu Jaiswal. 2017.

</span>
<span class="ltx_bibblock">graph2vec: Learning distributed representations of
graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1707.05005
(2017), 8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ororbia II
et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Alexander G. Ororbia II,
Tomas Mikolov, and David Reitter.
2017.

</span>
<span class="ltx_bibblock">Learning simpler language models with the
differential state framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Neural computation</em> 29,
12 (2017), 3327–3352.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedersen
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Ted Pedersen, Siddharth
Patwardhan, and Jason Michelizzi.
2004.

</span>
<span class="ltx_bibblock">WordNet::Similarity: Measuring the Relatedness of
Concepts. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Demonstration Papers at HLT-NAACL
2004</em>. Association for Computational Linguistics,
Stroudsburg, PA, USA, 38–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington,
Richard Socher, and Christopher D.
Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation. In
<em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">EMNLP</em>. Association for
Computational Linguistics, Doha, Qatar,
1532–1543.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Matthew E. Peters, Mark
Neumann, Mohit Iyyer, Matt Gardner,
Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock">Deep Contextualized Word Representations. In
<em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the NAACL:
Human Language Technologies</em>. NAACL,
New Orleans, Louisiana, 2227–2237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sherkat and
Milios (2017)</span>
<span class="ltx_bibblock">
Ehsan Sherkat and
Evangelos E. Milios. 2017.

</span>
<span class="ltx_bibblock">Vector Embedding of Wikipedia Concepts and
Entities.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1702.03470
(2017), 11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirumuruganathan et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Saravanan Thirumuruganathan,
Nan Tang, and Mourad Ouzzani.
2018.

</span>
<span class="ltx_bibblock">Data Curation with Deep Learning [Vision]: Towards
Self Driving Data Curation.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1803.01384
(2018), 14.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1903.08620" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1903.08621" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1903.08621">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1903.08621" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1903.08622" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 17:44:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
