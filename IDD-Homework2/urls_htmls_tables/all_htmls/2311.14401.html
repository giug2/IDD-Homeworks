<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.14401] Prototype of deployment of Federated Learning with IoT devices</title><meta property="og:description" content="In the age of technology, data is an increasingly important resource. This importance
is growing in the field of Artificial Intelligence (AI), where sub fields such as Machine Learning (ML) need more and more data to a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Prototype of deployment of Federated Learning with IoT devices">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Prototype of deployment of Federated Learning with IoT devices">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.14401">

<!--Generated on Tue Feb 27 17:20:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="federated learning,  machine learning,  Internet of Things,  raspberry pi,  privacy,  mqtt,  distributed learning,  deep learning">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Prototype of deployment of Federated Learning with IoT devices</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo García Santaclara
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">I&amp;C Lab. atlanTTic research centre. University of Vigo</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Vigo</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Spain</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_postcode">36310</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ana Fernández Vilas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">I&amp;C Lab. atlanTTic research centre. University of Vigo</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Vigo</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_country">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:avilas@det.uvigo.es">avilas@det.uvigo.es</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/1234-5678-9012" title="ORCID identifier" class="ltx_ref">1234-5678-9012</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rebeca P. Díaz Redondo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_affiliation_institution">I&amp;C Lab. atlanTTic research centre. University of Vigo</span><span id="id9.2.id2" class="ltx_text ltx_affiliation_city">Vigo</span><span id="id10.3.id3" class="ltx_text ltx_affiliation_country">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rebeca@det.uvigo.es">rebeca@det.uvigo.es</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/1234-5678-9012" title="ORCID identifier" class="ltx_ref">1234-5678-9012</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id11.id1" class="ltx_p">In the age of technology, data is an increasingly important resource. This importance
is growing in the field of Artificial Intelligence (AI), where sub fields such as Machine Learning (ML) need more and more data to achieve better results. Internet of Things (IoT) is the connection of sensors and smart objects to collect and exchange data, in addition to achieving many other tasks. A huge amount of the resource desired, data, is stored in mobile devices, sensors and other Internet of Things (IoT) devices, but remains there due to data protection restrictions. At the same time these devices do not have enough data or computational capacity to train good models. Moreover, transmitting, storing and processing all this data on a centralised server is problematic. Federated Learning (FL) provides an innovative solution that allows devices to learn in a collaborative way. More importantly, it accomplishes this without violating data protection laws. FL is currently growing, and there are several solutions that implement it. This article presents a prototype of a FL solution where the IoT devices used were raspberry pi boards. The results compare the performance of a solution of this type with those obtained in traditional approaches. In addition, the FL solution performance was tested in a hostile environment. A convolutional neural network (CNN) and a image data set were used. The results show the feasibility and usability of these techniques, although in many cases they do not reach the performance of traditional approaches.</p>
</div>
<div class="ltx_keywords">federated learning, machine learning, Internet of Things, raspberry pi, privacy, mqtt, distributed learning, deep learning
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The <span id="S1.p1.1.1" class="ltx_text ltx_font_bold">Internet of Things (IoT)</span> paradigm is composed of sensors and smart objects interconnected to collect and exchange data, take actions, automate different tasks, over the Internet and other communication protocols. The data generated by all these devices can come from many parts of the world and must be put to good use. IoT devices have endless uses and applications, of which a few of the most worth mentioning include: (1) Industry; where collected data provide insight into productivity and efficiency so that different aspects of the production chain can be improved., e.g. machine utilization, speeding up improvement, etc. (2) Smart cities, where IoT play a central role in areas such as parking management, healthcare monitoring, waste management, etc <cite class="ltx_cite ltx_citemacro_citep">(Saleem et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>; (3) Smart home, where the integration of different devices, appliances and sensors within a house increases comfort and efficiency in energy consumption; (4) Smart grid, where sensors can prevent failure points, extend component life and optimise costs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">According to <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">I</span>oT Analytics <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>iot-analytics.com</span></span></span> the number of connected IoT devices in Spring 2022 exceed 12 billion worldwide, growing by 9% this year, despite all the supply problems due to the pandemic and other issues. This number is set to grow dramatically by 2025, surpassing 25 billion connected IoT devices by then. It is difficult to estimate the amount of data generated by all these IoT devices but it is quite clear that the numbers are massive. All this generated data is very valuable, but with the increasing number of IoT devices, it does not scale well to analyse all of them with centralised solutions. At these sizes, storage capacity, processing and even transmission become challenges. Achieving the processing capacity for gigantic amounts of data on a centralised server is very costly. In addition, a centralised model compromises data security and privacy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There is a growing common view that the transition from centralized ML to distributed ML at the network edge is necessary, but largely complex, for a number of reasons <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>. Most prominent among these is the disconnection between current principles of network practice (coding, link communications, random access, protocol assumptions) and the way ML algorithms are designed and analyzed, with ideal trustful agents <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. While it is generally agreed that the intelligence of ML should be moved closer to the devices (data producers located at the network edge) and benefit from plentiful computing nodes, the emerging design of efficient distributed ML algorithms has to deal explicitly with the heterogeneity of the computing and communication equipment (e.g, from IoT sensors to cloud servers; from wireless channels and strong interference to local data; from privacy concerns to public data) <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A first breakthrough for employing multiple nodes for training and guaranteeing privacy is federated learning, which enables model synthesis out from a large corpus of decentralized data <cite class="ltx_cite ltx_citemacro_citep">(Aledhari et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>. The Federated Learning approach is based on training on devices with their own data, these devices share their models, which are aggregated on a central coordinating server. This way, the devices do not have to transmit their data to a server at any time. This technique also makes it possible to share the computational tasks among many devices. Since having to train a model on a server with extremely large amounts of data is becoming an increasingly frequent problem for big companies. This technology must be flexible in its use, as most possible usage scenarios contemplate problems in communications or in the availability of clients. A case where this is easy to visualise is an implementation with smartphones, where it is very likely that some of them will lack internet connection or even battery power. Therefore, the performance of Federated Learning for cases where communication is unstable should be tested.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contribution of this work is introducing an implementation architecture for FL in IoT scenarios. This implementation combines the Amazon Cloud with and edge layer consisting on restricted Raspberry devices. For this implementation technologies, we report our results under ideal conditions where the devices in the edge are always working and the communication is considered reliable and, in the otter hand, some hostile configurations. The results allows to extract conclusions about the performance of FL on IoT scenarios. More sophisticated FL approaches such as collaborative Federated Learning are being researched in the literature to achieve a more realistic approach to the privacy, security communication and reliability conditions of machine learning for IoT. The paper is organised as follows. Section <a href="#S2" title="2. Background ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> establishes a context for Federated Learning and Section <a href="#S3" title="3. Scenario ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is a brief overview of the technological basis of out IoT and it outlines the different parts of the architecture for the experiments. Then, Section <a href="#S4" title="4. Experiment ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the conditions under which these experiments were performed. In Section <a href="#S5" title="5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the results obtained after conducting the experiments are discussed. Finally, the paper is ended with a conclusion in Section <a href="#S6" title="6. Conclusions and future work ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The concept of <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning</span> was first proposed by Google in 2016 <cite class="ltx_cite ltx_citemacro_citep">(Konečný et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>. Originally, their intention was to alleviate the problem of having too much data for a single node, where storing the whole dataset on a single node became unfeasible. His proposal was to build the ML models based on datasets that were distributed across multiple devices. This at the same time satisfied the concern that large enterprises have in recent years to improve data security and user privacy, as decentralising data fulfils the function of making data leakage more difficult <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. Conventionally, if a set of data owners set out to train an ML model, they would gather all the data together to then train the model. In a system with Federated Learning, the owners collaboratively train a joint model, each training the model with their own data and then exchanging the results with the other owners, improving together a global model. This aims to achieve a similar accuracy to the one originally described without exposing each owner’s data to the others.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In 2017 McMahan et al. <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> presented two main concepts <span id="S2.p2.1.1" class="ltx_text ltx_font_bold">FederatedSGD(FedSGD)</span> and <span id="S2.p2.1.2" class="ltx_text ltx_font_bold">FederatedAveraging(FedAVG)</span>. In FedSGD, a fraction C of the customers in each round is chosen, the initial model is communicated to these and the model is trained by each client, then the average model is calculated. A typical FedSGD implementation is one with C=1 and a fixed learning coefficient. In which each client trains with its own data the current global model, communicates the model obtained to the server and the server is in charge of aggregating them and updating the global model. A solution, as described above, is known as Federated Averaging. The amount of computation done in each round is controlled by 3 parameters; C, the number of clients in each round. E, the number of training iterations each client does in each round. B, the size of the local minibatch used in client updates.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2311.14401/assets/Definitions/FL1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Federated Learning steps</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Therefore, generally, each iteration of a Federated Learning algorithm consists of the following steps. An illustration describing these steps can be found in the <a href="#S2.F1" title="Figure 1 ‣ 2. Background ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>: (1)selected sample of customers downloads the current global model; (2) Each of these selected clients computes an updated model based on the data they possess; (3) The models updated by the clients are sent to the server; (4) At the server, all these updated models are aggregated to improve the global model.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Due to its characteristics, its use is advantageous in environments where connectivity is not guaranteed, especially if the number of clients is significant. Since, even if a client is disconnected at a given point in time, due to the functioning of Federated Learning, when it becomes available again, it will pick up the global model, drawing on the data of all those involved in the communication. This means that the sudden addition of new clients will not lead to a deterioration of the global model. As the whole process could also be anonymous, new clients could join in to further improve the model, without this being a problem.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The use of systems with Federated Learning still has some challenges to overcome. The main one is heterogeneity in systems and data. Clients may have very different computational, storage or communication capabilities, and devices may experience power shortages or connectivity problems during an iteration. Moreover, it cannot be assumed that the data on each client is IID (independent and identically distributed) <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>, which complicates model training tasks. Communication can also become a problem, being a potential bottleneck. To mitigate the possibility of it becoming a drawback, communication efficiency can be improved, mainly by reducing the size of the messages transmitted in each round.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Scenario</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Federated Learning uses the Edge Computing paradigm for its operation. Edge Computing is closely related to IoT technologies. It is based on bringing the processing and data storage closer to where the data is being generated, the clients in this case. Therefore, when intelligence is figured out in a Edge computing environment, Federated Learning turns into the natural solution. We propose and scenario where federated learning and edge computing technologies are integrated into the architecture in <a href="#S3.F2" title="Figure 2 ‣ 3. Scenario ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. The Edge is materialised as a set of edge devices, Raspberry in our implementation; this edge layer in integrated into a public cloud, Amazon cloud. At the cloud level, an EC2 <cite class="ltx_cite ltx_citemacro_citep">(Amazon, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> instance, a virtual server in Amazon’s Elastic Compute Cloud, plays the role of FL aggregator or central server. It will be responsible for creating the initial model and distributing it to the edge devices. It will then wait to receive the trained models from the clients, create the global model and redistribute it, so that everyone improves jointly.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">From an architectonic point of view, the integration of the edge layer into the cloud relies on the usage of AWS IoT Core, and AWS feature that enables IoT devices to connect to the AWS Cloud <cite class="ltx_cite ltx_citemacro_citep">(Amazon IoT Core, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>. It is responsible for registering devices, acting as the device registrar. It also acts as the gateway to the Cloud, in addition to authorisations and being the messaging broker. Every raspberry used was registered as a ”thing” in AWS IoT, in order to be able to communicate with the Core, which manages communications. AWS IoT Core plays the role of message broker between the edge layer and the FL aggregator by using MQTT as communication protocol, and would allow the models to be transmitted back and forth.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">From a ML point of view, both edge devices and the aggregator will execute ML processes in order to create, train and evaluate the models. TensorFlow <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>, an open source library mainly used for Machine Learning, was the library used to perform all of the aforementioned operations. On the server, it was used to create the initial model and to evaluate the performance of the global model. On the clients, it was required to train the model.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.14401/assets/Definitions/Architecture.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>System architecture.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Any Federated Learning solution requires clients with their own data to train their models, and a central server that coordinates communications and which is responsible for weighting the models of the clients and distributing the global model. For our experiment, it is necessary to set up some edge devices to act as clients. Providing them with data so that they can train their respective models. Consequently, a dataset will also have to be obtained in order to distribute its data among all the clients.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The choice of the dataset and model to be used had to take into account the processing and dynamic memory capacity of the raspberrys. The model could not be too complex so that the raspberrys would not be able to train it. Besides, the dataset would have to be extensive so that it could be divided among several clients. That choice was to take the MNIST dataset <cite class="ltx_cite ltx_citemacro_citep">(LeCun and Cortes, <a href="#bib.bib10" title="" class="ltx_ref">2010</a>)</cite>. This is a dataset made up of handwritten numbers by different people. The training set consists of 60,000 samples while the test set has 10,000 samples. The images are made up of 28x28 pixels, they look as shown in <a href="#S4.F3" title="Figure 3 ‣ 4. Experiment ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. This dataset is well known and is common for people who are learning Machine Learning techniques.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2311.14401/assets/Definitions/mnist.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>MNIST dataset samples.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Raspberry boards were chosen to simulate the devices that would act as clients. Five boards were used for the experiment, three of them Raspberry Pi 3 Model B whereas the other two were Raspberry Pi 2 Model B. All of them have 1 Gb of RAM, this plus CPU capacity supposed a limiting factor. Their description is as follows:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Raspberry Pi 3 Model B <cite class="ltx_cite ltx_citemacro_citep">(Rapsberry Pi 3, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>: CPU (Broadcom BCM2387 64bit ARMv7 Quad Core 1.2GHz); RAM (1GB LPDDR2; Wifi (Yes)</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Raspberry Pi 2 Model B <cite class="ltx_cite ltx_citemacro_citep">(Rapsberry Pi 2, <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>: CPU (Broadcom BCM2836 900MHz quad-core ARM Cortex-A7); RAM (1GB LPDDR2); Wifi (No)</p>
</div>
</li>
</ul>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">An AWS instance, as mentioned above, simulates the central server, with the following technical characteristics: Model t2micro; vCPU 1; Mem 8 GiB.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The model used to be trained with this dataset is a simple convolutional neural network, which are known for performing well in image classification tasks. Keras <cite class="ltx_cite ltx_citemacro_citep">(Chollet, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>, a deep learning library, running on top of TensorFlow. is used to create the model. The model looks as shown in <a href="#S4.T1" title="Table 1 ‣ 4. Experiment ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, it consists of an input layer, two hidden layers and an output layer. The input layer flattens the input data, then a regular dense layer that uses <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">ReLU</span> as activation function and a dropout layer which helps prevent overfitting, finally the output layer is another regular dense layer which uses <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">Softmax</span> as activation function. <span id="S4.p5.1.3" class="ltx_text ltx_font_italic">ReLU</span> activation function is a linear function that will output the input directly if it is positive, or zero otherwise. <span id="S4.p5.1.4" class="ltx_text ltx_font_italic">Softmax</span> activation function converts a vector of numbers into a vector of probabilities.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Layer (type)</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Output Shape</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Flatten</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">784</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dense</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">128</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dropout</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">128</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Dense</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">10</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>CNN model.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The list of weights is passed as a bytearray in the message payload via MQTT communication. In the receiver, the model is reconstructed with the keras <cite class="ltx_cite ltx_citemacro_citep">(Chollet, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> method <span id="S4.p6.1.1" class="ltx_text ltx_font_italic">loadmodel()</span>. On each raspberry pi board, 4 clients were simulated using multiprocessing, where each process is a client. Making a total of 20 clients participating in Federated Learning. This was the maximum number of participants in the different tests.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have deployed different experiments where the focus was put on the distinctive characteristics of IoT environments in terms of reliability of tiny devices and communication issues. So, we consider a friendly environment <a href="#S5.SS1" title="5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> where IoT devices do not fail and communications from the edge to the central aggregator and back is always possible. After that, different hostile scenarios are considered in section <a href="#S5.SS2" title="5.2. Hostile environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>,</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Friendly environment</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As mentioned in the previous section, we use the MNIST one dataset. It follows a quite uniform distribution of samples, which can be seen in the <a href="#S5.T2" title="Table 2 ‣ 5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. In its totality it has 60,000 samples for training and 10,000 for testing. The choice was that each client would have 300 training samples, as a lower number would not give as well-representative results, and a larger number would start to give problems to the Raspberry devices. On the other hand, the 10,000 test samples are always used, which is necessary so that the results of the different tests can be compared.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Dataset</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 0</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 1</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 2</th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 3</th>
<th id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Train</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,923</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6,742</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,958</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6,131</td>
<td id="S5.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,842</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Test</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">980</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,135</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,032</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,010</td>
<td id="S5.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">982</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Dataset</th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 5</th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 6</th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 7</th>
<th id="S5.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 8</th>
<th id="S5.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">label 9</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<th id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Train</th>
<td id="S5.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,421</td>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,918</td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6,265</td>
<td id="S5.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,851</td>
<td id="S5.T2.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5,949</td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<th id="S5.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Test</th>
<td id="S5.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">892</td>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">958</td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,028</td>
<td id="S5.T2.2.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">974</td>
<td id="S5.T2.2.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,009</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Distribution of MNIST samples</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The objective of the model trained is to predict which number is the one in the image that is introduced. There are different metrics to measure performance. In order to asses the performance, we use the regular measures: (1) <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">Accuracy</span>, which is the number of correct predictions divided by the number of total predictions, this metric works best when the number of samples of each label is the same. MNIST is close to having this equality; (2)<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">Confusion matrix</span>, a matrix showing the number of False positives, False negatives, True positives and True negatives; (3) <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">F1 score</span>, which is the harmonic mean between precision and recall, and seeks a compromise between this two; (4) <span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">MAE</span> and <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">MSE</span>, which aim to give an average of the distance between predicted and actual values, this is not useful for this classification problem, since similar numbers are for example 1 and 7, but their distance would not express anything; and (5) <span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">Loss</span>, which is not a metric, but is used by the neural network when training, being the distance between real and predicted values. Being what the neural network seeks to minimise during training.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The decision made was to use accuracy and loss. Accuracy is one of the most universal metrics and allows to easily know the performance. Loss, on the other hand, relates well to precision, as combined they allow us to know what is happening. For example, if both increase, it could be due to overfitting, i.e. it will adjust to learning the particular cases we teach it and will be unable to recognise new data. However, if the accuracy increases while the loss decreases, it is assumable that the neural network is learning correctly.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">The second important point to consider is how to evaluate the performance of Federated Learning. The results were compared with those obtained in a traditional architecture, where data is centralised. Therefore, the performance of Federated Learning would be compared with the results that a client would obtain with only its own data, in case data restrictions prevented them from being shared, and on the other hand, with the results that a centralised server would obtain with the data of all the clients.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2311.14401/assets/Definitions/CentralizedDataTraining.png" id="S5.F4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="202" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Centralised training with data from all the clients.</figcaption>
</figure>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Plotly <cite class="ltx_cite ltx_citemacro_citep">(Plotly, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, a tool for data analysis and visualisation, was used for the creation of all the graphs. The results obtained for the tests with centralised data can be seen in <a href="#S5.F4" title="Figure 4 ‣ 5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>. The maximum efficiency achieved by a single client is <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">78.23%</span>, which is not a high number, but it is reasonable since it does not have enough samples to achieve a higher effectiveness. Furthermore, from epoch 15 on-wards the loss starts to increase, which may be due to overfitting as the model has few samples.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2311.14401/assets/Definitions/full_compar1.png" id="S5.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="135" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2311.14401/assets/Definitions/full_compar2.png" id="S5.F5.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="133" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2311.14401/assets/Definitions/full_compar3.png" id="S5.F5.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="139" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Federated Learning results.</figcaption>
</figure>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">In contrast, the results with data from 20, 10 and 15 clients are much better. These have respectively 3000, 4500 and 6000 samples for training, and achieve efficiencies of <span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">92.87%</span>, <span id="S5.SS1.p6.1.2" class="ltx_text ltx_font_bold">94.26%</span> and <span id="S5.SS1.p6.1.3" class="ltx_text ltx_font_bold">95.23%</span>. In these cases, the loss starts to increase around epoch 20-25, training from then on does not improve the model.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">In the case of the Federated Learning tests, 10, 15 and 20 clients were also used. In turn, for each of these cases, 1,3,5 epochs were used, i.e. the number of times the model will be trained on the entire dataset. This is to test how it would influence the fact that more training would be done on all models in each round of communication. The results can be found in <a href="#S5.F5" title="Figure 5 ‣ 5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">It can be noticed how effectiveness improves as the number of clients and the amount of training in each round of communication increases. Since effectiveness improves with the number of epochs, a compromise should be found with the effort that each client makes to train his model. A full breakdown of the results can be found in <a href="#S5.T3" title="Table 3 ‣ 5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a></p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">10 Clients</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">15 Clients</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">20 Clients</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1 Epoch F.L.</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84.63%</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.03%</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.42%</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">3 Epoch F.L.</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.03%</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.68%</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.13%</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">5 Epoch F.L.</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.73%</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">89.2%</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.55%</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Centralized data</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.87%</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">94.26%</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">95.23%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Accuracy Comparation</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2311.14401/assets/Definitions/ComparisonHostile.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Performance in a hostile environment.</figcaption>
</figure>
<div id="S5.SS1.p9" class="ltx_para">
<p id="S5.SS1.p9.1" class="ltx_p">The highest number achieved with Federated Learning is <span id="S5.SS1.p9.1.1" class="ltx_text ltx_font_bold">90.55%</span>, which is a respectable result, although it is a little far from the <span id="S5.SS1.p9.1.2" class="ltx_text ltx_font_bold">95.23%</span> that would be obtained with centralised data. However, although it sometimes happens, the objective of Federated Learning is not to improve the results of traditional centralised models, but to serve in cases where it is not possible to use them. Actually, even the worst result of the scenarios tested with Federated Learning, with 10 clients and 1 epoch, and <span id="S5.SS1.p9.1.3" class="ltx_text ltx_font_bold">84.55%</span> accuracy, is quite better than the <span id="S5.SS1.p9.1.4" class="ltx_text ltx_font_bold">78.23%</span> achieved by an single client.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Hostile environment</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Usual Federated Learning scenarios are unfriendly. As their use with mobile devices is common, many problems can arise, such as being disconnected due to lack of battery, or not having an internet connection.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The performance of this prototype was put to the test in this type of environment. The results were tested with the most effective set of the ones above, 20 clients and 5 epochs. A comparison was made if 4, 8 and 12 clients were disconnected in each round of communication. The results can be found in <a href="#S5.F6" title="Figure 6 ‣ 5.1. Friendly environment ‣ 5. Results ‣ Prototype of deployment of Federated Learning with IoT devices" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a></p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">When a client joins the communication, it waits until it receives the model from the server. This way, the clients pick up the most updated model when it joins. In addition, it gets the current round of communication. The fact that communication takes place with no need for the server to know which clients are interacting at any given moment makes it work well in this situation.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In this case, the results are very influenced by which clients are disconnected in each round of communication. For example, it can be seen how for the intermediate scenario the loss fluctuates considerably.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">In terms of accuracy, values of <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">86.37%</span>, <span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_bold">87.95%</span> and <span id="S5.SS2.p5.1.3" class="ltx_text ltx_font_bold">89.67%</span> were reached respectively. A comparison can be drawn between the case with hostile environment and 4 clients disconnecting in each round, and the friendly environment case where there were always 15 clients. The first case achieves a better result because in addition to 1 more client, after finishing the ten rounds of communication it has managed to take into account the data of all 20 clients, while the second case is limited to the data of the same 15 clients.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions and future work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This article explores the feasibility and effectiveness of using Federated Learning, concluding that the use of these techniques is highly beneficial. A prototype was implemented using an Amazon Web Services EC2 instance as the coordinator server and raspberry’s boards as edge devices. In our experiments, Federated Learning has been proven to achieve better results than would be achieved by an individual client (edge device). However, it does not necessarily improve the performance of traditional techniques where all data is centralised.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Moreover, its application is also useful in situations where the environment is not comfortable. One of its main advantages is that its implementation is still beneficial when communication is unstable or slow. These are the regular scenarios in the realisation of the IoT, especially where FL will be commonly deployed. Last but not least, a edge-based FL approach is also beneficial in terms of privacy and security as the data remains at the edge and only model are exposure to inferential and poisoning model attacks among other. In future lines of work, interesting paths remain to be explored. Some of the possibilities would be to test how performance would be improved by using more clients and larger datasets. This would allow to know how important it is to reach a certain number of customers. Another possibility is to use alternatives to FederatedAVG, such as CO-OP <cite class="ltx_cite ltx_citemacro_citep">(Wang, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, which allows communication to be asynchronous. On the other hand, FL works only when data are local and the edge devices collaborate perfectly, thus being unsuitable to distributed training and inference. FL generally involves the exchange of large volumes of data, so a naive deployment over wireless networks is exceedingly costly, slow and vulnerable to outer attacks or to hidden collusion among the computing nodes. To overcome those fundamental bottlenecks, another line of future work focuses on distributed ML as the appropriate approach for addressing the performance issues and the privacy requirements that edge-intelligent services demand.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the European Regional Development Fund (ERDF) and the Galician Regional Government, under the agreement for funding the Atlantic Research Center for Information and Communication Technologies (AtlanTTic). This work was also supported by the Spanish Government under research project “Enhancing Communication Protocols with Machine Learning while Protecting Sensitive Data (COMPROMISE) (PID2020-113795RB-C33/AEI/10.13039/501100011033).

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Martín Abadi, Paul
Barham, Jianmin Chen, Zhifeng Chen,
Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat,
Geoffrey Irving, Michael Isard,
et al<span id="bib.bib2.3.1" class="ltx_text">.</span> 2016.

</span>
<span class="ltx_bibblock">Tensorflow: A system for large-scale machine
learning. In <em id="bib.bib2.4.1" class="ltx_emph ltx_font_italic">12th USENIX symposium on operating
systems design and implementation</em>. 265–283.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aledhari et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mohammed Aledhari, Rehma
Razzak, Reza M. Parizi, and Fahad
Saeed. 2020.

</span>
<span class="ltx_bibblock">Federated Learning: A Survey on Enabling
Technologies, Protocols, and Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 8
(2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2020.3013541" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2020.3013541</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon (2022)</span>
<span class="ltx_bibblock">
Amazon 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Amazon EC2</em>.

</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://aws.amazon.com/ec2/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/ec2/</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon IoT Core (2022)</span>
<span class="ltx_bibblock">
Amazon IoT Core 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IoT Core</em>.

</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://aws.amazon.com/es/iot-core" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/es/iot-core</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AWS Free Tier (2022)</span>
<span class="ltx_bibblock">
AWS Free Tier 2022.

</span>
<span class="ltx_bibblock">AWS Free Tier.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://aws.amazon.com/es/free" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/es/free</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chollet (2015)</span>
<span class="ltx_bibblock">
François Chollet.
2015.

</span>
<span class="ltx_bibblock">keras.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/fchollet/keras" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fchollet/keras</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed:2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz et al.
2021.

</span>
<span class="ltx_bibblock">Advances and Open Problems in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine
Learning</em> 14, 1–2
(2021), 1–210.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1561/2200000083" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1561/2200000083</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečný,
H. Brendan McMahan, Daniel Ramage, and
Peter Richtárik. 2016.

</span>
<span class="ltx_bibblock">Federated Optimization: Distributed Machine
Learning for On-Device Intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>
(2016).

</span>
<span class="ltx_bibblock">arXiv:1610.02527 [cs.LG]

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun and Cortes (2010)</span>
<span class="ltx_bibblock">
Yann LeCun and Corinna
Cortes. 2010.

</span>
<span class="ltx_bibblock">MNIST handwritten digit database.

</span>
<span class="ltx_bibblock">http://yann.lecun.com/exdb/mnist/.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://yann.lecun.com/exdb/mnist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist/</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jeffrey Li, Mikhail
Khodak, Sebastian Caldas, and Ameet
Talwalkar. 2020.

</span>
<span class="ltx_bibblock">Differentially Private Meta-Learning. In
<em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hung T. Nguyen, Vikash
Sehwag, Seyyedali Hosseinalipour,
Christopher G. Brinton, Mung Chiang,
and H. Vincent Poor. 2021.

</span>
<span class="ltx_bibblock">Fast-Convergent Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em> 39, 1
(2021), 201–218.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/JSAC.2020.3036952" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/JSAC.2020.3036952</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jihong Park, Sumudu
Samarakoon, Mehdi Bennis, and Mérouane
Debbah. 2019.

</span>
<span class="ltx_bibblock">Wireless Network Intelligence at the Edge.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em> 107,
11 (2019), 2204–2239.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://dblp.uni-trier.de/db/journals/pieee/pieee107.html#ParkSBD19" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dblp.uni-trier.de/db/journals/pieee/pieee107.html#ParkSBD19</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plotly (2022)</span>
<span class="ltx_bibblock">
Plotly 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Collaborative data science</em>.

</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://plot.ly" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://plot.ly</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rapsberry Pi 2 (2022)</span>
<span class="ltx_bibblock">
Rapsberry Pi 2 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">RPI 2 Model B</em>.

</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://www.raspberrypi.org/products/raspberry-pi-2-model-b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.raspberrypi.org/products/raspberry-pi-2-model-b</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rapsberry Pi 3 (2022)</span>
<span class="ltx_bibblock">
Rapsberry Pi 3 2022.

</span>
<span class="ltx_bibblock">RPI 3 Model B.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
Retrieved May 27, 2022 from <a target="_blank" href="https://www.raspberrypi.org/products/raspberry-pi-3-model-b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.raspberrypi.org/products/raspberry-pi-3-model-b</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saleem et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Saleem Ibraheem Saleem, S
Zeebaree, Diyar Qader Zeebaree, and
Adnan Mohsin Abdulazeez. 2020.

</span>
<span class="ltx_bibblock">Building smart cities applications based on iot
technologies: A review.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Technology Reports of Kansai University</em>
62, 3 (2020),
1083–1092.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2017)</span>
<span class="ltx_bibblock">
Yushi Wang.
2017.

</span>
<span class="ltx_bibblock">Co-op: Cooperative machine learning from mobile
devices.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Yong Cheng, Yan Kang,
Tianjian Chen, and Han Yu.
2019.

</span>
<span class="ltx_bibblock">Federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Synthesis Lectures on Artificial Intelligence
and Machine Learning</em> 13, 3
(2019), 1–207.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li,
Liangzhen Lai, Naveen Suda,
Damon Civin, and Vikas Chandra.
2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Guangxu Zhu, Dongzhu Liu,
Yuqing Du, Changsheng You,
Jun Zhang, and Kaibin Huang.
2020.

</span>
<span class="ltx_bibblock">Toward an Intelligent Edge: Wireless Communication
Meets Machine Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>
58, 1 (2020),
19–25.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/MCOM.001.1900103" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MCOM.001.1900103</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.14400" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.14401" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.14401">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.14401" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.14402" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 17:20:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
