<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.14441] Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification</title><meta property="og:description" content="Exploiting both audio and visual modalities for video classification is a challenging task, as the existing methods require large model architectures, leading to high computational complexity and resource requirements.…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.14441">

<!--Generated on Thu Sep  5 14:44:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Audio-visual fusion Video classification Model efficiency">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">(eccv)                Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is *not* recommended for camera-ready version</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{mahrukh.awan,asmar.nadeem,m.awan,armin.mustafa,sameed.husain}@surrey.ac.uk</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahrukh Awan 
</span><span class="ltx_author_notes">These authors contributed equally to this work.11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Asmar Nadeem
</span><span class="ltx_author_notes">**11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Muhammad Junaid Awan
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Armin Mustafa
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Syed Sameed Husain
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Exploiting both audio and visual modalities for video classification is a challenging task, as the existing methods require large model architectures, leading to high computational complexity and resource requirements. Smaller architectures, on the other hand, struggle to achieve optimal performance. In this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that introduces a compact model architecture specifically designed to capture intricate audio-visual relationships in video data. Through extensive experiments on the challenging YouTube-8M dataset, we demonstrate that Attend-Fusion achieves an F1 score of 75.64% with only 72M parameters, which is comparable to the performance of larger baseline models such as Fully-Connected Late Fusion (75.96% F1 score, 341M parameters). Attend-Fusion achieves similar performance to the larger baseline model while reducing the model size by nearly 80%, highlighting its efficiency in terms of model complexity. Our work demonstrates that the Attend-Fusion model effectively combines audio and visual information for video classification, achieving competitive performance with significantly reduced model size. This approach opens new possibilities for deploying high-performance video understanding systems in resource-constrained environments across various applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Audio-visual fusion Video classification Model efficiency
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Audio-Visual video understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> represents the frontier of video classification, building upon the foundations laid by static image recognition and extending into the complex realm of temporal and multimodal data processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. While datasets like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> revolutionized image classification, the emergence of large-scale video datasets such as YouTube-8M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> has shifted the focus to video analysis, enabling the evaluation of models’ capabilities in multi-label video classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and their ability to interpret temporal and multimodal information across visual and audio modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.14441/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;"> Overview of our proposed audio-visual video classification framework on YouTube-8M dataset, illustrating different fusion mechanisms of audio and visual modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The YouTube-8M dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a large-scale dataset that comprises millions of YouTube videos, each annotated with labels from a diverse vocabulary of 4,716 visual entities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The task involves predicting these multiple labels for each video, making it a multi-label classification problem. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the pipeline of the fusion of visual and audio features for video classification. YouTube-8M is a multimodal dataset featuring visual and audio features, enabling the development of models that integrate both modalities for comprehensive video content analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Historically, audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and visual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> data have been studied independently, each with its own set of applications, capturing only partial information about the subject of interest, limiting their performance and making them susceptible to noise within that single modality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Recent methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> have begun to explore the integration of multiple modalities in audio-visual tasks. Our approach fuses audio and visual modalities (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) for comprehensive video content understanding. Audio-visual video classification<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> requires effective multimodal fusion, overcoming single-modality limitations. Despite progress, developing efficient fusion methods while maintaining high accuracy remains a challenge. Our research proposes an efficient audio-visual fusion approach for the YouTube-8M dataset, addressing two main questions: (1) How to leverage complementary multimodal information for improved video classification effectively? (2) Can a compact architecture achieve comparable performance to larger models while being more efficient?</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an overview of our proposed audio-visual video classification framework. The input video is first separated into its audio and visual components, which are then processed by their respective encoders to extract meaningful features. These features are fused using different strategies, including baseline methods and our proposed approach. The baseline methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> consist of fully connected networks, fully connected residual networks, and fully connected gated residual networks, which are commonly used in multimodal learning. On the other hand, our proposed approach introduces three novel architectures: fully connected attention, audio-visual attention, and Attend-Fusion.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Among our proposed methods, Attend-Fusion stands out as the most advanced and effective architecture. It leverages late fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to integrate information from audio and visual modalities effectively. Attend-Fusion enables the model to learn both modality-specific and cross-modal representations. It incorporates attention mechanisms to capture complex temporal and modal relationships in the video data.
To demonstrate the efficacy of Attend-Fusion, we conduct extensive experiments and compare its performance against the baseline methods. In addition to the commonly used Global Average Precision (GAP), we focus on the F1 score, which provides a more comprehensive evaluation of classification performance, especially in multi-label scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Attend-Fusion employs approximately 72 million parameters, which is significantly smaller compared to leading state-of-the-art models that typically use around 341 million parameters. This represents a reduction of over 80% in model size, highlighting the efficiency of our approach. This efficiency not only demonstrates the robustness of our methodology but also addresses critical concerns in the field regarding model complexity and computational resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. By achieving high performance with more compact models, our work contributes to the ongoing efforts to develop more sustainable and deployable AI solutions for video understanding tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. This has significant implications for real-world applications, as our approach can enable the deployment of accurate video classification systems on resource-constrained devices and facilitate more efficient processing of large-scale video datasets.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our approach focuses on the collaborative fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> of audio and visual learning components, employing diverse strategies tailored for each modality. As highlighted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we incorporate fully connected networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and various attention mechanisms<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> to enhance the learning process from both audio and visual data. We explore a range of fusion methods, from early to late fusion, leveraging the capabilities of these sophisticated techniques. We intend to make videos more comprehensible by strategically exploiting the fusion between audio and visual cues.
Our main contributions in this work are as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel, efficient approach for fusing audio and visual information that achieves high classification accuracy on the YouTube-8M dataset while significantly reducing model size compared to state-of-the-art methods.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce and evaluate advanced attention mechanisms tailored for capturing complex temporal and modal relationships in video data, demonstrating their effectiveness in audio-visual learning.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide a comprehensive analysis of the trade-offs between model size, computational efficiency, and classification accuracy, contributing insights for sustainable AI development in video understanding.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Through this research, we aim to bridge the gap between academic advancements and industry applications in the rapidly evolving field of audio-visual machine learning.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Proliferation of multimedia content, advancements in deep learning and high demands for models to understand and interpret AV(audio-visual) data in various tasks have led to recent development in the Deep AV learning domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Leveraging the complementary nature of audio and visual modalities and learning their shared representation is fundamental to improving the accuracy of semantic understanding in AV learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> provided an overview of key architectures, methodologies and systematic comparison for deep AV correlation learning approaches to leverage the complementary information given the complex and dynamic nature of both modalities. Attention Mechanism has proven to be able to curtail noise in audio and visual modalities that complicates the video understanding task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> by selectively focusing on the most relevant parts and effective integration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> improving the interpretability of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Deep AV Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Deep learning techniques have been extensively employed in audio-visual learning tasks. Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> have been widely used for learning spatial features from visual data, while Recurrent Neural Networks (RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Long Short-Term Memory (LSTM) networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have been utilized to capture temporal dependencies in sequential data. Recently, transformer networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> have gained popularity in audio-visual learning due to their ability to capture long-range dependencies and model multi-modal interactions effectively. Various transformer-based architectures, such as the Audio-Visual Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, Cross-Modal Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, and Multi-modal Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, have been proposed for tasks such as audio-visual speech recognition, audio-visual event localization, and multi-modal sentiment analysis. These techniques have been applied to various benchmarked datasets, such as UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and YouTube-8M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, to address the unique challenges posed by video data, including high dimensionality and temporal dependencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Video Classification on YouTube-8M Dataset</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The YouTube-8M dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> has become a popular benchmark for video classification tasks, particularly in the context of audio-visual learning. This large-scale dataset consists of millions of YouTube videos annotated with a diverse set of labels, making it a challenging testbed for multi-label video classification. Various approaches have been proposed to tackle the unique challenges posed by this dataset, such as its scale, diversity, and the presence of noisy labels.
Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> introduced a collaborative experts model that utilizes a mixture of experts and a classifier to handle the multi-label classification problem on the YouTube-8M dataset. Gkalelis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed a subclass deep neural network approach that learns a set of subclasses for each label, capturing the complex relationships between video content and labels. Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> presented a multi-modal fusion framework that leverages both audio and visual features to improve video classification performance on the YouTube-8M dataset. Their approach incorporates a cross-modal attention mechanism to selectively focus on relevant audio and visual cues.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In addition to these methods, several studies have explored the use of temporal models, such as LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and attention-based mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, to capture the temporal dependencies and multi-modal interactions in videos from the YouTube-8M dataset. These approaches have demonstrated the importance of effectively leveraging both audio and visual modalities for accurate video classification on this challenging benchmark.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion Techniques in AV Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Integrating and synchronizing audio-visual modalities is a fundamental challenge in audio-visual learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Traditional fusion approaches, such as early and late fusion, are still widely used due to their simplicity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. However, state-of-the-art frameworks often incorporate these approaches with advanced techniques. The Slow Fusion Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> introduces a Multi-modal Transfer Module (MMTM) for feature modality fusion, while the Attention-based Multi-modal Fusion Module (AMFM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> incorporates attention mechanisms to selectively fuse audio and visual features.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Attention Mechanisms in Audio-Visual Learning</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Attention mechanisms have proven effective in addressing the challenges of audio-visual learning by focusing on the most relevant parts of the input and enabling effective cross-modal integration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Several state-of-the-art frameworks have been proposed that incorporate attention mechanisms to capture complex temporal and modal relationships in video data, such as Hierarchical Audiovisual Synchronization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Generalized Zero-shot Learning with Cross-modal Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and Multi-level Attention Fusion Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>.
Attention mechanisms have also been widely employed in various audio-visual tasks, including event recognition using multi-level attention fusion networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, sound localization using instance attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and dual attention matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, and regression-based tasks using recursive joint attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Audio-Visual Representation Learning</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Learning effective representations from audio-visual data is crucial for various downstream tasks. Self-supervised learning approaches have been proposed to leverage the inherent synchronization and correspondence between audio and visual modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. These methods aim to learn rich audio-visual representations without the need for explicit human annotations. Contrastive learning techniques have also been employed to learn discriminative audio-visual embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">Our proposed approach builds upon the existing methods discussed in this section. We leverage the power of attention mechanisms to effectively integrate audio and visual features while maintaining a compact model architecture. Our approach differs from previous works by introducing a novel combination of early and late fusion strategies, along with attention mechanisms specifically designed for capturing temporal and modal relationships in video data. By incorporating these advanced techniques, we aim to push the boundaries of audio-visual learning and contribute to the state-of-the-art in this rapidly evolving field.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baseline Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To establish a robust baseline for our study on the YouTube-8M dataset, we implement a series of foundational models derived from prior research in large-scale video classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. These models incorporate both unimodal and multimodal approaches with various fusion techniques, as illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our comparative analysis encompasses a range of architectures, including:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Fully Connected (FC) Audio and FC Visual networks for unimodal analysis - Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.I1.i1.p1.1.1" class="ltx_text" style="color:#FF0000;">(a,b)</span>  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Standard FC with early and late fusion strategies for multimodal analysis - Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.I1.i2.p1.1.1" class="ltx_text" style="color:#FF0000;">(c,d)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">FC Residual Networks (FCRN) with early and late fusion - Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.I1.i3.p1.1.1" class="ltx_text" style="color:#FF0000;">(e,f)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">FC Residual Gated Networks (FCRGN) with early and late fusion - Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.I1.i4.p1.1.1" class="ltx_text" style="color:#FF0000;">(g,h)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></p>
</div>
</li>
</ul>
</div>
<figure id="S3.F2" class="ltx_figure ltx_align_center"><img src="/html/2408.14441/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Illustration of diverse baseline network architectures for audio-visual (AV) fusion: (a,b) Fully Connected (FC) Audio-only and Video-only networks to assess the impact of individual modalities; (c,d) Fully Connected Neural Networks (FCNNs) with early and late fusion strategies; (e,f) Fully Connected Residual Networks (FCRNs) with early and late fusion; and (g,h) Fully Connected Residual Gated Networks (FCRGNs) with early and late fusion, incorporating gating mechanisms for selective feature attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. </span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.5" class="ltx_p"><span id="S3.SS1.p2.5.1" class="ltx_text ltx_font_bold">Residual Block</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>: The residual block used in the baseline models (FCRN and FCRGN) is defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathbf{y}=\mathcal{F}(\mathbf{x},{W_{i}})+\mathbf{x}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">𝐲</mi><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐱</mi><mo id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">W</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">+</mo><mi id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml">𝐱</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><ci id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">𝐲</ci><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><plus id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></plus><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><times id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">ℱ</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐱</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">𝑊</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">𝑖</ci></apply></interval></apply><ci id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3">𝐱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathbf{y}=\mathcal{F}(\mathbf{x},{W_{i}})+\mathbf{x}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.4" class="ltx_p">where <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathbf{x}</annotation></semantics></math> and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{y}</annotation></semantics></math> are the input and output of the block, respectively, <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{F}</annotation></semantics></math> represents the residual mapping, and <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="{W_{i}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">W</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑊</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">{W_{i}}</annotation></semantics></math> are the learnable weights.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.6" class="ltx_p"><span id="S3.SS1.p3.6.1" class="ltx_text ltx_font_bold">Gating Mechanism</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>: The FCRGN models incorporate a gating mechanism to control the flow of information. The gating operation is defined as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathbf{g}=\sigma(\mathbf{W}_{g}\mathbf{x}+\mathbf{b}_{g})" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">𝐠</mi><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml">𝐖</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">𝐱</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">𝐛</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">g</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝐠</ci><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝜎</ci><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1"></times><apply id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2">𝐖</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3">𝑔</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">𝐱</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">𝐛</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝑔</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathbf{g}=\sigma(\mathbf{W}_{g}\mathbf{x}+\mathbf{b}_{g})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="\mathbf{y}=\mathbf{g}\odot\mathcal{F}(\mathbf{x},{W_{i}})+(1-\mathbf{g})\odot\mathbf{x}" display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><mi id="S3.E3.m1.3.3.4" xref="S3.E3.m1.3.3.4.cmml">𝐲</mi><mo id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml">=</mo><mrow id="S3.E3.m1.3.3.2" xref="S3.E3.m1.3.3.2.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml">𝐠</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">⊙</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.1.1.3.3" xref="S3.E3.m1.2.2.1.1.3.3.cmml">ℱ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">𝐱</mi><mo id="S3.E3.m1.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.2.cmml">,</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.2.cmml">W</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.4" xref="S3.E3.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.2.3" xref="S3.E3.m1.3.3.2.3.cmml">+</mo><mrow id="S3.E3.m1.3.3.2.2" xref="S3.E3.m1.3.3.2.2.cmml"><mrow id="S3.E3.m1.3.3.2.2.1.1" xref="S3.E3.m1.3.3.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.2.2.1.1.2" xref="S3.E3.m1.3.3.2.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.2.2.1.1.1" xref="S3.E3.m1.3.3.2.2.1.1.1.cmml"><mn id="S3.E3.m1.3.3.2.2.1.1.1.2" xref="S3.E3.m1.3.3.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.3.3.2.2.1.1.1.1" xref="S3.E3.m1.3.3.2.2.1.1.1.1.cmml">−</mo><mi id="S3.E3.m1.3.3.2.2.1.1.1.3" xref="S3.E3.m1.3.3.2.2.1.1.1.3.cmml">𝐠</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.3.3.2.2.1.1.3" xref="S3.E3.m1.3.3.2.2.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E3.m1.3.3.2.2.2" xref="S3.E3.m1.3.3.2.2.2.cmml">⊙</mo><mi id="S3.E3.m1.3.3.2.2.3" xref="S3.E3.m1.3.3.2.2.3.cmml">𝐱</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><eq id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3"></eq><ci id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3.4">𝐲</ci><apply id="S3.E3.m1.3.3.2.cmml" xref="S3.E3.m1.3.3.2"><plus id="S3.E3.m1.3.3.2.3.cmml" xref="S3.E3.m1.3.3.2.3"></plus><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1.1"><times id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></times><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><csymbol cd="latexml" id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1">direct-product</csymbol><ci id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2">𝐠</ci><ci id="S3.E3.m1.2.2.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.3.3">ℱ</ci></apply><interval closure="open" id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝐱</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.2">𝑊</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.3">𝑖</ci></apply></interval></apply><apply id="S3.E3.m1.3.3.2.2.cmml" xref="S3.E3.m1.3.3.2.2"><csymbol cd="latexml" id="S3.E3.m1.3.3.2.2.2.cmml" xref="S3.E3.m1.3.3.2.2.2">direct-product</csymbol><apply id="S3.E3.m1.3.3.2.2.1.1.1.cmml" xref="S3.E3.m1.3.3.2.2.1.1"><minus id="S3.E3.m1.3.3.2.2.1.1.1.1.cmml" xref="S3.E3.m1.3.3.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E3.m1.3.3.2.2.1.1.1.2.cmml" xref="S3.E3.m1.3.3.2.2.1.1.1.2">1</cn><ci id="S3.E3.m1.3.3.2.2.1.1.1.3.cmml" xref="S3.E3.m1.3.3.2.2.1.1.1.3">𝐠</ci></apply><ci id="S3.E3.m1.3.3.2.2.3.cmml" xref="S3.E3.m1.3.3.2.2.3">𝐱</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\mathbf{y}=\mathbf{g}\odot\mathcal{F}(\mathbf{x},{W_{i}})+(1-\mathbf{g})\odot\mathbf{x}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.5" class="ltx_p">where <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{g}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">𝐠</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝐠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathbf{g}</annotation></semantics></math> is the gating vector, <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\sigma</annotation></semantics></math> is the sigmoid activation function, <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{W}_{g}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">𝐖</mi><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝐖</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathbf{W}_{g}</annotation></semantics></math> and <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{b}_{g}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">𝐛</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝐛</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\mathbf{b}_{g}</annotation></semantics></math> are the learnable weights and biases of the gating layer, and <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mo id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><csymbol cd="latexml" id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\odot</annotation></semantics></math> denotes element-wise multiplication.
<br class="ltx_break"><span id="S3.SS1.p3.5.1" class="ltx_text ltx_font_bold">Input Features</span>: We employ the same input features as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which are video-level mean and standard deviation features (MF+STD). The input features are extracted from pre-trained Inception networks. Specifically, the frame-level features are obtained from two separate Inception networks, one for video (1024-dimensional) and another for audio (128-dimensional).
<br class="ltx_break"><span id="S3.SS1.p3.5.2" class="ltx_text ltx_font_bold">Unimodal Networks</span>: The FC Audio and FC Visual networks consist of three fully connected layers each, assessing the isolated impact of audio and visual modalities, respectively.
<br class="ltx_break"><span id="S3.SS1.p3.5.3" class="ltx_text ltx_font_bold">Multimodal Networks</span>: The standard FC network comprises three fully connected layers and is evaluated with both early and late fusion strategies. The FCRN architecture incorporates skip connections to facilitate complex feature learning and implicit regularization. The FCRGN enhances the FCRN by introducing context gating mechanisms for selective feature attention.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Proposed Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">While the baseline models employ fully connected layers for processing audio-visual features, our proposed method leverages attention mechanism to dynamically focus on the most relevant parts of the input and capture long-range dependencies. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of our attention-based network architectures.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>3.2.1 FC Attention Network</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.4" class="ltx_p">The FC Attention Network (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS1.p1.4.1" class="ltx_text" style="color:#FF0000;">(a)</span>) integrates self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> mechanisms to prioritize salient features within each modality. The attended audio and visual features are fused through concatenation, followed by fully connected layers with ReLU activation and dropout regularization. The final classification layer employs a sigmoid activation function for multi-label classification.
<br class="ltx_break"><span id="S3.SS2.SSS1.p1.4.2" class="ltx_text ltx_font_bold">Self-Attention Mechanism:</span> Let <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{N\times d}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">𝐗</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><in id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></in><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">𝐗</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2">𝑁</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">\mathbf{X}\in\mathbb{R}^{N\times d}</annotation></semantics></math> be the input feature matrix, where <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">N</annotation></semantics></math> is the number of features and <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mi id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">d</annotation></semantics></math> is the feature dimension. The self-attention mechanism computes the attended features <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{X}_{att}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><msub id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml">𝐗</mi><mrow id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1a" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3.4" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">𝐗</ci><apply id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3"><times id="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2">𝑎</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3">𝑡</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.4.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">\mathbf{X}_{att}</annotation></semantics></math> as follows:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\mathbf{Q}=\mathbf{X}\mathbf{W}_{Q},\quad\mathbf{K}=\mathbf{X}\mathbf{W}_{K},\quad\mathbf{V}=\mathbf{X}\mathbf{W}_{V}" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.3.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">𝐐</mi><mo id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><msub id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">𝐗𝐖</mi><mi id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml">Q</mi></msub></mrow><mo rspace="1.167em" id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.3.cmml"><mrow id="S3.E4.m1.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.2.cmml">𝐊</mi><mo id="S3.E4.m1.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.cmml">=</mo><msub id="S3.E4.m1.2.2.2.2.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.3.2" xref="S3.E4.m1.2.2.2.2.1.1.3.2.cmml">𝐗𝐖</mi><mi id="S3.E4.m1.2.2.2.2.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.3.3.cmml">K</mi></msub></mrow><mo rspace="1.167em" id="S3.E4.m1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.2.cmml">𝐕</mi><mo id="S3.E4.m1.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.2.2.1.cmml">=</mo><msub id="S3.E4.m1.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.cmml">𝐗𝐖</mi><mi id="S3.E4.m1.2.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.cmml">V</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><ci id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2">𝐐</ci><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">𝐗𝐖</ci><ci id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3">𝑄</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.3a.cmml" xref="S3.E4.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1"><eq id="S3.E4.m1.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1"></eq><ci id="S3.E4.m1.2.2.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2">𝐊</ci><apply id="S3.E4.m1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.2">𝐗𝐖</ci><ci id="S3.E4.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.3">𝐾</ci></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2"><eq id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.1"></eq><ci id="S3.E4.m1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.2">𝐕</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2">𝐗𝐖</ci><ci id="S3.E4.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3">𝑉</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathbf{Q}=\mathbf{X}\mathbf{W}_{Q},\quad\mathbf{K}=\mathbf{X}\mathbf{W}_{K},\quad\mathbf{V}=\mathbf{X}\mathbf{W}_{V}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\mathbf{X}_{att}=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d}}\right)\mathbf{V}" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.2" xref="S3.E5.m1.1.2.cmml"><msub id="S3.E5.m1.1.2.2" xref="S3.E5.m1.1.2.2.cmml"><mi id="S3.E5.m1.1.2.2.2" xref="S3.E5.m1.1.2.2.2.cmml">𝐗</mi><mrow id="S3.E5.m1.1.2.2.3" xref="S3.E5.m1.1.2.2.3.cmml"><mi id="S3.E5.m1.1.2.2.3.2" xref="S3.E5.m1.1.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.2.3.1" xref="S3.E5.m1.1.2.2.3.1.cmml">​</mo><mi id="S3.E5.m1.1.2.2.3.3" xref="S3.E5.m1.1.2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.2.3.1a" xref="S3.E5.m1.1.2.2.3.1.cmml">​</mo><mi id="S3.E5.m1.1.2.2.3.4" xref="S3.E5.m1.1.2.2.3.4.cmml">t</mi></mrow></msub><mo id="S3.E5.m1.1.2.1" xref="S3.E5.m1.1.2.1.cmml">=</mo><mrow id="S3.E5.m1.1.2.3" xref="S3.E5.m1.1.2.3.cmml"><mtext id="S3.E5.m1.1.2.3.2" xref="S3.E5.m1.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.3.1" xref="S3.E5.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E5.m1.1.2.3.3.2" xref="S3.E5.m1.1.1.cmml"><mo id="S3.E5.m1.1.2.3.3.2.1" xref="S3.E5.m1.1.1.cmml">(</mo><mfrac id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msup id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.2.2" xref="S3.E5.m1.1.1.2.2.cmml">𝐐𝐊</mi><mi id="S3.E5.m1.1.1.2.3" xref="S3.E5.m1.1.1.2.3.cmml">T</mi></msup><msqrt id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.E5.m1.1.2.3.3.2.2" xref="S3.E5.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.3.1a" xref="S3.E5.m1.1.2.3.1.cmml">​</mo><mi id="S3.E5.m1.1.2.3.4" xref="S3.E5.m1.1.2.3.4.cmml">𝐕</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.2.cmml" xref="S3.E5.m1.1.2"><eq id="S3.E5.m1.1.2.1.cmml" xref="S3.E5.m1.1.2.1"></eq><apply id="S3.E5.m1.1.2.2.cmml" xref="S3.E5.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.2.2.1.cmml" xref="S3.E5.m1.1.2.2">subscript</csymbol><ci id="S3.E5.m1.1.2.2.2.cmml" xref="S3.E5.m1.1.2.2.2">𝐗</ci><apply id="S3.E5.m1.1.2.2.3.cmml" xref="S3.E5.m1.1.2.2.3"><times id="S3.E5.m1.1.2.2.3.1.cmml" xref="S3.E5.m1.1.2.2.3.1"></times><ci id="S3.E5.m1.1.2.2.3.2.cmml" xref="S3.E5.m1.1.2.2.3.2">𝑎</ci><ci id="S3.E5.m1.1.2.2.3.3.cmml" xref="S3.E5.m1.1.2.2.3.3">𝑡</ci><ci id="S3.E5.m1.1.2.2.3.4.cmml" xref="S3.E5.m1.1.2.2.3.4">𝑡</ci></apply></apply><apply id="S3.E5.m1.1.2.3.cmml" xref="S3.E5.m1.1.2.3"><times id="S3.E5.m1.1.2.3.1.cmml" xref="S3.E5.m1.1.2.3.1"></times><ci id="S3.E5.m1.1.2.3.2a.cmml" xref="S3.E5.m1.1.2.3.2"><mtext id="S3.E5.m1.1.2.3.2.cmml" xref="S3.E5.m1.1.2.3.2">softmax</mtext></ci><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.2.3.3.2"><divide id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.2.3.3.2"></divide><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.2">superscript</csymbol><ci id="S3.E5.m1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.2.2">𝐐𝐊</ci><ci id="S3.E5.m1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.2.3">𝑇</ci></apply><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><root id="S3.E5.m1.1.1.3a.cmml" xref="S3.E5.m1.1.1.3"></root><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">𝑑</ci></apply></apply><ci id="S3.E5.m1.1.2.3.4.cmml" xref="S3.E5.m1.1.2.3.4">𝐕</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\mathbf{X}_{att}=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d}}\right)\mathbf{V}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p1.7" class="ltx_p">where <math id="S3.SS2.SSS1.p1.5.m1.1" class="ltx_Math" alttext="\mathbf{W}_{Q}" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m1.1a"><msub id="S3.SS2.SSS1.p1.5.m1.1.1" xref="S3.SS2.SSS1.p1.5.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.5.m1.1.1.2" xref="S3.SS2.SSS1.p1.5.m1.1.1.2.cmml">𝐖</mi><mi id="S3.SS2.SSS1.p1.5.m1.1.1.3" xref="S3.SS2.SSS1.p1.5.m1.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m1.1b"><apply id="S3.SS2.SSS1.p1.5.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.5.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.5.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m1.1.1.2">𝐖</ci><ci id="S3.SS2.SSS1.p1.5.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m1.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m1.1c">\mathbf{W}_{Q}</annotation></semantics></math>, <math id="S3.SS2.SSS1.p1.6.m2.1" class="ltx_Math" alttext="\mathbf{W}_{K}" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m2.1a"><msub id="S3.SS2.SSS1.p1.6.m2.1.1" xref="S3.SS2.SSS1.p1.6.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p1.6.m2.1.1.2" xref="S3.SS2.SSS1.p1.6.m2.1.1.2.cmml">𝐖</mi><mi id="S3.SS2.SSS1.p1.6.m2.1.1.3" xref="S3.SS2.SSS1.p1.6.m2.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m2.1b"><apply id="S3.SS2.SSS1.p1.6.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m2.1.1.2">𝐖</ci><ci id="S3.SS2.SSS1.p1.6.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m2.1c">\mathbf{W}_{K}</annotation></semantics></math>, and <math id="S3.SS2.SSS1.p1.7.m3.1" class="ltx_Math" alttext="\mathbf{W}_{V}" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m3.1a"><msub id="S3.SS2.SSS1.p1.7.m3.1.1" xref="S3.SS2.SSS1.p1.7.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.7.m3.1.1.2" xref="S3.SS2.SSS1.p1.7.m3.1.1.2.cmml">𝐖</mi><mi id="S3.SS2.SSS1.p1.7.m3.1.1.3" xref="S3.SS2.SSS1.p1.7.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m3.1b"><apply id="S3.SS2.SSS1.p1.7.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.7.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.7.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.7.m3.1.1.2">𝐖</ci><ci id="S3.SS2.SSS1.p1.7.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.7.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m3.1c">\mathbf{W}_{V}</annotation></semantics></math> are learnable weight matrices.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.14441/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_square" width="461" height="389" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">Attention-based network architectures: (a) FC Attention Network; (b,c) FC Residual Attention Networks with early and late fusion; (d,e) AV Attention Fusion Networks; (f) Self and Cross Modal Attention Network; (g) Network with Self-Attended Features for Cross Modal Attention.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>3.2.2 Residual Attention Networks</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The Fully Connected Residual Attention Networks (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS2.p1.1.1" class="ltx_text" style="color:#FF0000;">(b,c)</span>) combine attention mechanisms with residual learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We experiment with early and late fusion variants to understand the impact of multimodal integration timing on the network’s performance. The residual block is similar to what we use in Section <a href="#S3.SS1" title="3.1 Baseline Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>3.2.3 Attend-Fusion</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Attend-Fusion (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS3.p1.1.1" class="ltx_text" style="color:#FF0000;">(d)</span>) is a novel architecture that effectively combines attention mechanisms and multi-stage fusion for audio-visual video classification. It processes audio and visual features separately through attention networks, which consist of fully connected layers and self-attention mechanisms, similar to in Section <a href="#S3.SS2.SSS1" title="3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a><span id="S3.SS2.SSS3.p1.1.2" class="ltx_text" style="color:#FF0000;">.1</span>. The self-attention allows the model to focus on the most relevant features within each modality.
The attended audio and visual features are then fused using a late fusion strategy, where they are concatenated along the feature dimension. The fused features undergo further processing through fully connected layers, which learn to capture complex interactions between the modalities.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Attend-Fusion’s key advantages include its ability to learn modality-specific and cross-modal representations at different stages, and its compact and efficient design. By leveraging attention mechanisms and late fusion, Attend-Fusion achieves comparative performance while maintaining a smaller model size compared to baseline approaches.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>3.2.4 Audio-Visual Attention Network</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.4" class="ltx_p">The Audio-Visual Attention Network (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS4.p1.4.1" class="ltx_text" style="color:#FF0000;">(e)</span>) introduces cross-modal attention to capture nuanced interactions between audio and visual streams. The architecture employs a hierarchical attention approach, with self-attention mechanisms followed by cross-modal attention, to learn refined multimodal representations.
<br class="ltx_break"><span id="S3.SS2.SSS4.p1.4.2" class="ltx_text ltx_font_bold">Cross-Modal Attention:</span> Let <math id="S3.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{X}a" display="inline"><semantics id="S3.SS2.SSS4.p1.1.m1.1a"><mrow id="S3.SS2.SSS4.p1.1.m1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS4.p1.1.m1.1.1.2" xref="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.1.m1.1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.1.m1.1.1.3" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.1.m1.1b"><apply id="S3.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1"><times id="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.1"></times><ci id="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.1.m1.1c">\mathbf{X}a</annotation></semantics></math> and <math id="S3.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{X}v" display="inline"><semantics id="S3.SS2.SSS4.p1.2.m2.1a"><mrow id="S3.SS2.SSS4.p1.2.m2.1.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS4.p1.2.m2.1.1.2" xref="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.2.m2.1.1.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.2.m2.1.1.3" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.2.m2.1b"><apply id="S3.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1"><times id="S3.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.2.m2.1c">\mathbf{X}v</annotation></semantics></math> be the attended audio and visual features, respectively. The cross-modal attention mechanism computes the audio-guided visual features <math id="S3.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{X}{v|a}" display="inline"><semantics id="S3.SS2.SSS4.p1.3.m3.1a"><mrow id="S3.SS2.SSS4.p1.3.m3.1.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS4.p1.3.m3.1.1.2" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS4.p1.3.m3.1.1.2.2" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.3.m3.1.1.2.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.3.m3.1.1.2.3" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.3.cmml">v</mi></mrow><mo fence="false" id="S3.SS2.SSS4.p1.3.m3.1.1.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS4.p1.3.m3.1.1.3" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.3.m3.1b"><apply id="S3.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS4.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.1">conditional</csymbol><apply id="S3.SS2.SSS4.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.2"><times id="S3.SS2.SSS4.p1.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.1"></times><ci id="S3.SS2.SSS4.p1.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.3">𝑣</ci></apply><ci id="S3.SS2.SSS4.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.3.m3.1c">\mathbf{X}{v|a}</annotation></semantics></math> and the visual-guided audio features <math id="S3.SS2.SSS4.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{X}{a|v}" display="inline"><semantics id="S3.SS2.SSS4.p1.4.m4.1a"><mrow id="S3.SS2.SSS4.p1.4.m4.1.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.cmml"><mrow id="S3.SS2.SSS4.p1.4.m4.1.1.2" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS4.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.4.m4.1.1.2.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.4.m4.1.1.2.3" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.3.cmml">a</mi></mrow><mo fence="false" id="S3.SS2.SSS4.p1.4.m4.1.1.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS4.p1.4.m4.1.1.3" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.4.m4.1b"><apply id="S3.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS4.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.1">conditional</csymbol><apply id="S3.SS2.SSS4.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.2"><times id="S3.SS2.SSS4.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.1"></times><ci id="S3.SS2.SSS4.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.3">𝑎</ci></apply><ci id="S3.SS2.SSS4.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.4.m4.1c">\mathbf{X}{a|v}</annotation></semantics></math> as follows:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\mathbf{Q}_{a}=\mathbf{X}a\mathbf{W}{Q_{a}},\quad\mathbf{K}_{v}=\mathbf{X}v\mathbf{W}{K_{v}},\quad\mathbf{V}v=\mathbf{X}v\mathbf{W}{V_{v}}" display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2.2" xref="S3.E6.m1.2.2.3.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.2.2.cmml">𝐐</mi><mi id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3.cmml">a</mi></msub><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.1a" xref="S3.E6.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.3.4" xref="S3.E6.m1.1.1.1.1.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.1b" xref="S3.E6.m1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.E6.m1.1.1.1.1.3.5" xref="S3.E6.m1.1.1.1.1.3.5.cmml"><mi id="S3.E6.m1.1.1.1.1.3.5.2" xref="S3.E6.m1.1.1.1.1.3.5.2.cmml">Q</mi><mi id="S3.E6.m1.1.1.1.1.3.5.3" xref="S3.E6.m1.1.1.1.1.3.5.3.cmml">a</mi></msub></mrow></mrow><mo rspace="1.167em" id="S3.E6.m1.2.2.2.3" xref="S3.E6.m1.2.2.3a.cmml">,</mo><mrow id="S3.E6.m1.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.3.cmml"><mrow id="S3.E6.m1.2.2.2.2.1.1" xref="S3.E6.m1.2.2.2.2.1.1.cmml"><msub id="S3.E6.m1.2.2.2.2.1.1.2" xref="S3.E6.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E6.m1.2.2.2.2.1.1.2.2" xref="S3.E6.m1.2.2.2.2.1.1.2.2.cmml">𝐊</mi><mi id="S3.E6.m1.2.2.2.2.1.1.2.3" xref="S3.E6.m1.2.2.2.2.1.1.2.3.cmml">v</mi></msub><mo id="S3.E6.m1.2.2.2.2.1.1.1" xref="S3.E6.m1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.2.2.2.2.1.1.3" xref="S3.E6.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E6.m1.2.2.2.2.1.1.3.2" xref="S3.E6.m1.2.2.2.2.1.1.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.1.1.3.1" xref="S3.E6.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.1.1.3.3" xref="S3.E6.m1.2.2.2.2.1.1.3.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.1.1.3.1a" xref="S3.E6.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.1.1.3.4" xref="S3.E6.m1.2.2.2.2.1.1.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.1.1.3.1b" xref="S3.E6.m1.2.2.2.2.1.1.3.1.cmml">​</mo><msub id="S3.E6.m1.2.2.2.2.1.1.3.5" xref="S3.E6.m1.2.2.2.2.1.1.3.5.cmml"><mi id="S3.E6.m1.2.2.2.2.1.1.3.5.2" xref="S3.E6.m1.2.2.2.2.1.1.3.5.2.cmml">K</mi><mi id="S3.E6.m1.2.2.2.2.1.1.3.5.3" xref="S3.E6.m1.2.2.2.2.1.1.3.5.3.cmml">v</mi></msub></mrow></mrow><mo rspace="1.167em" id="S3.E6.m1.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E6.m1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.cmml"><mrow id="S3.E6.m1.2.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.2.2" xref="S3.E6.m1.2.2.2.2.2.2.2.2.cmml">𝐕</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.2.2.2.1" xref="S3.E6.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.2.2.2.3.cmml">v</mi></mrow><mo id="S3.E6.m1.2.2.2.2.2.2.1" xref="S3.E6.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.E6.m1.2.2.2.2.2.2.3" xref="S3.E6.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.3.2" xref="S3.E6.m1.2.2.2.2.2.2.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.2.2.3.1" xref="S3.E6.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.2.2.3.3" xref="S3.E6.m1.2.2.2.2.2.2.3.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.2.2.3.1a" xref="S3.E6.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E6.m1.2.2.2.2.2.2.3.4" xref="S3.E6.m1.2.2.2.2.2.2.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.2.2.2.2.3.1b" xref="S3.E6.m1.2.2.2.2.2.2.3.1.cmml">​</mo><msub id="S3.E6.m1.2.2.2.2.2.2.3.5" xref="S3.E6.m1.2.2.2.2.2.2.3.5.cmml"><mi id="S3.E6.m1.2.2.2.2.2.2.3.5.2" xref="S3.E6.m1.2.2.2.2.2.2.3.5.2.cmml">V</mi><mi id="S3.E6.m1.2.2.2.2.2.2.3.5.3" xref="S3.E6.m1.2.2.2.2.2.2.3.5.3.cmml">v</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.3.cmml" xref="S3.E6.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.3a.cmml" xref="S3.E6.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2">𝐐</ci><ci id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3">𝑎</ci></apply><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><times id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1"></times><ci id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2">𝐗</ci><ci id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3">𝑎</ci><ci id="S3.E6.m1.1.1.1.1.3.4.cmml" xref="S3.E6.m1.1.1.1.1.3.4">𝐖</ci><apply id="S3.E6.m1.1.1.1.1.3.5.cmml" xref="S3.E6.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.5.1.cmml" xref="S3.E6.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.5.2.cmml" xref="S3.E6.m1.1.1.1.1.3.5.2">𝑄</ci><ci id="S3.E6.m1.1.1.1.1.3.5.3.cmml" xref="S3.E6.m1.1.1.1.1.3.5.3">𝑎</ci></apply></apply></apply><apply id="S3.E6.m1.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.3a.cmml" xref="S3.E6.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.2.2.2.2.1.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1"><eq id="S3.E6.m1.2.2.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.1"></eq><apply id="S3.E6.m1.2.2.2.2.1.1.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E6.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2.2">𝐊</ci><ci id="S3.E6.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.2.3">𝑣</ci></apply><apply id="S3.E6.m1.2.2.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3"><times id="S3.E6.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.1"></times><ci id="S3.E6.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.2">𝐗</ci><ci id="S3.E6.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.3">𝑣</ci><ci id="S3.E6.m1.2.2.2.2.1.1.3.4.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.4">𝐖</ci><apply id="S3.E6.m1.2.2.2.2.1.1.3.5.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.1.1.3.5.1.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.5">subscript</csymbol><ci id="S3.E6.m1.2.2.2.2.1.1.3.5.2.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.5.2">𝐾</ci><ci id="S3.E6.m1.2.2.2.2.1.1.3.5.3.cmml" xref="S3.E6.m1.2.2.2.2.1.1.3.5.3">𝑣</ci></apply></apply></apply><apply id="S3.E6.m1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2"><eq id="S3.E6.m1.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.1"></eq><apply id="S3.E6.m1.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2"><times id="S3.E6.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2.1"></times><ci id="S3.E6.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2.2">𝐕</ci><ci id="S3.E6.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.2.3">𝑣</ci></apply><apply id="S3.E6.m1.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3"><times id="S3.E6.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.1"></times><ci id="S3.E6.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.2">𝐗</ci><ci id="S3.E6.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.3">𝑣</ci><ci id="S3.E6.m1.2.2.2.2.2.2.3.4.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.4">𝐖</ci><apply id="S3.E6.m1.2.2.2.2.2.2.3.5.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.5"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.2.2.2.2.3.5.1.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.5">subscript</csymbol><ci id="S3.E6.m1.2.2.2.2.2.2.3.5.2.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.5.2">𝑉</ci><ci id="S3.E6.m1.2.2.2.2.2.2.3.5.3.cmml" xref="S3.E6.m1.2.2.2.2.2.2.3.5.3">𝑣</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\mathbf{Q}_{a}=\mathbf{X}a\mathbf{W}{Q_{a}},\quad\mathbf{K}_{v}=\mathbf{X}v\mathbf{W}{K_{v}},\quad\mathbf{V}v=\mathbf{X}v\mathbf{W}{V_{v}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\mathbf{X}{v|a}=\text{softmax}\left(\frac{\mathbf{Q}_{a}\mathbf{K}_{v}^{T}}{\sqrt{d}}\right)\mathbf{V}_{v}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.2" xref="S3.E7.m1.1.2.cmml"><mrow id="S3.E7.m1.1.2.2" xref="S3.E7.m1.1.2.2.cmml"><mrow id="S3.E7.m1.1.2.2.2" xref="S3.E7.m1.1.2.2.2.cmml"><mi id="S3.E7.m1.1.2.2.2.2" xref="S3.E7.m1.1.2.2.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.2.2.2.1" xref="S3.E7.m1.1.2.2.2.1.cmml">​</mo><mi id="S3.E7.m1.1.2.2.2.3" xref="S3.E7.m1.1.2.2.2.3.cmml">v</mi></mrow><mo fence="false" id="S3.E7.m1.1.2.2.1" xref="S3.E7.m1.1.2.2.1.cmml">|</mo><mi id="S3.E7.m1.1.2.2.3" xref="S3.E7.m1.1.2.2.3.cmml">a</mi></mrow><mo id="S3.E7.m1.1.2.1" xref="S3.E7.m1.1.2.1.cmml">=</mo><mrow id="S3.E7.m1.1.2.3" xref="S3.E7.m1.1.2.3.cmml"><mtext id="S3.E7.m1.1.2.3.2" xref="S3.E7.m1.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.2.3.1" xref="S3.E7.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E7.m1.1.2.3.3.2" xref="S3.E7.m1.1.1.cmml"><mo id="S3.E7.m1.1.2.3.3.2.1" xref="S3.E7.m1.1.1.cmml">(</mo><mfrac id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mrow id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml"><msub id="S3.E7.m1.1.1.2.2" xref="S3.E7.m1.1.1.2.2.cmml"><mi id="S3.E7.m1.1.1.2.2.2" xref="S3.E7.m1.1.1.2.2.2.cmml">𝐐</mi><mi id="S3.E7.m1.1.1.2.2.3" xref="S3.E7.m1.1.1.2.2.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.2.1" xref="S3.E7.m1.1.1.2.1.cmml">​</mo><msubsup id="S3.E7.m1.1.1.2.3" xref="S3.E7.m1.1.1.2.3.cmml"><mi id="S3.E7.m1.1.1.2.3.2.2" xref="S3.E7.m1.1.1.2.3.2.2.cmml">𝐊</mi><mi id="S3.E7.m1.1.1.2.3.2.3" xref="S3.E7.m1.1.1.2.3.2.3.cmml">v</mi><mi id="S3.E7.m1.1.1.2.3.3" xref="S3.E7.m1.1.1.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.E7.m1.1.2.3.3.2.2" xref="S3.E7.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.2.3.1a" xref="S3.E7.m1.1.2.3.1.cmml">​</mo><msub id="S3.E7.m1.1.2.3.4" xref="S3.E7.m1.1.2.3.4.cmml"><mi id="S3.E7.m1.1.2.3.4.2" xref="S3.E7.m1.1.2.3.4.2.cmml">𝐕</mi><mi id="S3.E7.m1.1.2.3.4.3" xref="S3.E7.m1.1.2.3.4.3.cmml">v</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.2.cmml" xref="S3.E7.m1.1.2"><eq id="S3.E7.m1.1.2.1.cmml" xref="S3.E7.m1.1.2.1"></eq><apply id="S3.E7.m1.1.2.2.cmml" xref="S3.E7.m1.1.2.2"><csymbol cd="latexml" id="S3.E7.m1.1.2.2.1.cmml" xref="S3.E7.m1.1.2.2.1">conditional</csymbol><apply id="S3.E7.m1.1.2.2.2.cmml" xref="S3.E7.m1.1.2.2.2"><times id="S3.E7.m1.1.2.2.2.1.cmml" xref="S3.E7.m1.1.2.2.2.1"></times><ci id="S3.E7.m1.1.2.2.2.2.cmml" xref="S3.E7.m1.1.2.2.2.2">𝐗</ci><ci id="S3.E7.m1.1.2.2.2.3.cmml" xref="S3.E7.m1.1.2.2.2.3">𝑣</ci></apply><ci id="S3.E7.m1.1.2.2.3.cmml" xref="S3.E7.m1.1.2.2.3">𝑎</ci></apply><apply id="S3.E7.m1.1.2.3.cmml" xref="S3.E7.m1.1.2.3"><times id="S3.E7.m1.1.2.3.1.cmml" xref="S3.E7.m1.1.2.3.1"></times><ci id="S3.E7.m1.1.2.3.2a.cmml" xref="S3.E7.m1.1.2.3.2"><mtext id="S3.E7.m1.1.2.3.2.cmml" xref="S3.E7.m1.1.2.3.2">softmax</mtext></ci><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.2.3.3.2"><divide id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.2.3.3.2"></divide><apply id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"><times id="S3.E7.m1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.2.1"></times><apply id="S3.E7.m1.1.1.2.2.cmml" xref="S3.E7.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.2.1.cmml" xref="S3.E7.m1.1.1.2.2">subscript</csymbol><ci id="S3.E7.m1.1.1.2.2.2.cmml" xref="S3.E7.m1.1.1.2.2.2">𝐐</ci><ci id="S3.E7.m1.1.1.2.2.3.cmml" xref="S3.E7.m1.1.1.2.2.3">𝑎</ci></apply><apply id="S3.E7.m1.1.1.2.3.cmml" xref="S3.E7.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.3.1.cmml" xref="S3.E7.m1.1.1.2.3">superscript</csymbol><apply id="S3.E7.m1.1.1.2.3.2.cmml" xref="S3.E7.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.2.3.2.1.cmml" xref="S3.E7.m1.1.1.2.3">subscript</csymbol><ci id="S3.E7.m1.1.1.2.3.2.2.cmml" xref="S3.E7.m1.1.1.2.3.2.2">𝐊</ci><ci id="S3.E7.m1.1.1.2.3.2.3.cmml" xref="S3.E7.m1.1.1.2.3.2.3">𝑣</ci></apply><ci id="S3.E7.m1.1.1.2.3.3.cmml" xref="S3.E7.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><root id="S3.E7.m1.1.1.3a.cmml" xref="S3.E7.m1.1.1.3"></root><ci id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2">𝑑</ci></apply></apply><apply id="S3.E7.m1.1.2.3.4.cmml" xref="S3.E7.m1.1.2.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.1.2.3.4.1.cmml" xref="S3.E7.m1.1.2.3.4">subscript</csymbol><ci id="S3.E7.m1.1.2.3.4.2.cmml" xref="S3.E7.m1.1.2.3.4.2">𝐕</ci><ci id="S3.E7.m1.1.2.3.4.3.cmml" xref="S3.E7.m1.1.2.3.4.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\mathbf{X}{v|a}=\text{softmax}\left(\frac{\mathbf{Q}_{a}\mathbf{K}_{v}^{T}}{\sqrt{d}}\right)\mathbf{V}_{v}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.2" class="ltx_Math" alttext="\mathbf{Q}_{v}=\mathbf{X}v\mathbf{W}{Q_{v}},\quad\mathbf{K}_{a}=\mathbf{X}a\mathbf{W}{K_{a}},\quad\mathbf{V}a=\mathbf{X}a\mathbf{W}{V_{a}}" display="block"><semantics id="S3.E8.m1.2a"><mrow id="S3.E8.m1.2.2.2" xref="S3.E8.m1.2.2.3.cmml"><mrow id="S3.E8.m1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.cmml"><msub id="S3.E8.m1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.2.cmml"><mi id="S3.E8.m1.1.1.1.1.2.2" xref="S3.E8.m1.1.1.1.1.2.2.cmml">𝐐</mi><mi id="S3.E8.m1.1.1.1.1.2.3" xref="S3.E8.m1.1.1.1.1.2.3.cmml">v</mi></msub><mo id="S3.E8.m1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E8.m1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.3.1" xref="S3.E8.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.3.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.3.1a" xref="S3.E8.m1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E8.m1.1.1.1.1.3.4" xref="S3.E8.m1.1.1.1.1.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.3.1b" xref="S3.E8.m1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.E8.m1.1.1.1.1.3.5" xref="S3.E8.m1.1.1.1.1.3.5.cmml"><mi id="S3.E8.m1.1.1.1.1.3.5.2" xref="S3.E8.m1.1.1.1.1.3.5.2.cmml">Q</mi><mi id="S3.E8.m1.1.1.1.1.3.5.3" xref="S3.E8.m1.1.1.1.1.3.5.3.cmml">v</mi></msub></mrow></mrow><mo rspace="1.167em" id="S3.E8.m1.2.2.2.3" xref="S3.E8.m1.2.2.3a.cmml">,</mo><mrow id="S3.E8.m1.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.3.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1" xref="S3.E8.m1.2.2.2.2.1.1.cmml"><msub id="S3.E8.m1.2.2.2.2.1.1.2" xref="S3.E8.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E8.m1.2.2.2.2.1.1.2.2" xref="S3.E8.m1.2.2.2.2.1.1.2.2.cmml">𝐊</mi><mi id="S3.E8.m1.2.2.2.2.1.1.2.3" xref="S3.E8.m1.2.2.2.2.1.1.2.3.cmml">a</mi></msub><mo id="S3.E8.m1.2.2.2.2.1.1.1" xref="S3.E8.m1.2.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E8.m1.2.2.2.2.1.1.3" xref="S3.E8.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E8.m1.2.2.2.2.1.1.3.2" xref="S3.E8.m1.2.2.2.2.1.1.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E8.m1.2.2.2.2.1.1.3.3" xref="S3.E8.m1.2.2.2.2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1a" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E8.m1.2.2.2.2.1.1.3.4" xref="S3.E8.m1.2.2.2.2.1.1.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1b" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml">​</mo><msub id="S3.E8.m1.2.2.2.2.1.1.3.5" xref="S3.E8.m1.2.2.2.2.1.1.3.5.cmml"><mi id="S3.E8.m1.2.2.2.2.1.1.3.5.2" xref="S3.E8.m1.2.2.2.2.1.1.3.5.2.cmml">K</mi><mi id="S3.E8.m1.2.2.2.2.1.1.3.5.3" xref="S3.E8.m1.2.2.2.2.1.1.3.5.3.cmml">a</mi></msub></mrow></mrow><mo rspace="1.167em" id="S3.E8.m1.2.2.2.2.2.3" xref="S3.E8.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E8.m1.2.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.2.2.cmml"><mrow id="S3.E8.m1.2.2.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E8.m1.2.2.2.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.2.2.2.2.cmml">𝐕</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.2.2.2.1" xref="S3.E8.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.E8.m1.2.2.2.2.2.2.2.3" xref="S3.E8.m1.2.2.2.2.2.2.2.3.cmml">a</mi></mrow><mo id="S3.E8.m1.2.2.2.2.2.2.1" xref="S3.E8.m1.2.2.2.2.2.2.1.cmml">=</mo><mrow id="S3.E8.m1.2.2.2.2.2.2.3" xref="S3.E8.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E8.m1.2.2.2.2.2.2.3.2" xref="S3.E8.m1.2.2.2.2.2.2.3.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.2.2.3.1" xref="S3.E8.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E8.m1.2.2.2.2.2.2.3.3" xref="S3.E8.m1.2.2.2.2.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.2.2.3.1a" xref="S3.E8.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E8.m1.2.2.2.2.2.2.3.4" xref="S3.E8.m1.2.2.2.2.2.2.3.4.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.2.2.3.1b" xref="S3.E8.m1.2.2.2.2.2.2.3.1.cmml">​</mo><msub id="S3.E8.m1.2.2.2.2.2.2.3.5" xref="S3.E8.m1.2.2.2.2.2.2.3.5.cmml"><mi id="S3.E8.m1.2.2.2.2.2.2.3.5.2" xref="S3.E8.m1.2.2.2.2.2.2.3.5.2.cmml">V</mi><mi id="S3.E8.m1.2.2.2.2.2.2.3.5.3" xref="S3.E8.m1.2.2.2.2.2.2.3.5.3.cmml">a</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.2b"><apply id="S3.E8.m1.2.2.3.cmml" xref="S3.E8.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.3a.cmml" xref="S3.E8.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E8.m1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1"><eq id="S3.E8.m1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1"></eq><apply id="S3.E8.m1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.2.2.cmml" xref="S3.E8.m1.1.1.1.1.2.2">𝐐</ci><ci id="S3.E8.m1.1.1.1.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.2.3">𝑣</ci></apply><apply id="S3.E8.m1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.3"><times id="S3.E8.m1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.3.1"></times><ci id="S3.E8.m1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.3.2">𝐗</ci><ci id="S3.E8.m1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.3.3">𝑣</ci><ci id="S3.E8.m1.1.1.1.1.3.4.cmml" xref="S3.E8.m1.1.1.1.1.3.4">𝐖</ci><apply id="S3.E8.m1.1.1.1.1.3.5.cmml" xref="S3.E8.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.3.5.1.cmml" xref="S3.E8.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.3.5.2.cmml" xref="S3.E8.m1.1.1.1.1.3.5.2">𝑄</ci><ci id="S3.E8.m1.1.1.1.1.3.5.3.cmml" xref="S3.E8.m1.1.1.1.1.3.5.3">𝑣</ci></apply></apply></apply><apply id="S3.E8.m1.2.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.2.2.3a.cmml" xref="S3.E8.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E8.m1.2.2.2.2.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1"><eq id="S3.E8.m1.2.2.2.2.1.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.1"></eq><apply id="S3.E8.m1.2.2.2.2.1.1.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E8.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2.2">𝐊</ci><ci id="S3.E8.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2.3">𝑎</ci></apply><apply id="S3.E8.m1.2.2.2.2.1.1.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3"><times id="S3.E8.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1"></times><ci id="S3.E8.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.2">𝐗</ci><ci id="S3.E8.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.3">𝑎</ci><ci id="S3.E8.m1.2.2.2.2.1.1.3.4.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.4">𝐖</ci><apply id="S3.E8.m1.2.2.2.2.1.1.3.5.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.2.2.1.1.3.5.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.5">subscript</csymbol><ci id="S3.E8.m1.2.2.2.2.1.1.3.5.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.5.2">𝐾</ci><ci id="S3.E8.m1.2.2.2.2.1.1.3.5.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.5.3">𝑎</ci></apply></apply></apply><apply id="S3.E8.m1.2.2.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2"><eq id="S3.E8.m1.2.2.2.2.2.2.1.cmml" xref="S3.E8.m1.2.2.2.2.2.2.1"></eq><apply id="S3.E8.m1.2.2.2.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2.2"><times id="S3.E8.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E8.m1.2.2.2.2.2.2.2.1"></times><ci id="S3.E8.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2.2.2">𝐕</ci><ci id="S3.E8.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.2.2.2.2.3">𝑎</ci></apply><apply id="S3.E8.m1.2.2.2.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3"><times id="S3.E8.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.1"></times><ci id="S3.E8.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.2">𝐗</ci><ci id="S3.E8.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.E8.m1.2.2.2.2.2.2.3.4.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.4">𝐖</ci><apply id="S3.E8.m1.2.2.2.2.2.2.3.5.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.5"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.2.2.2.2.3.5.1.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.5">subscript</csymbol><ci id="S3.E8.m1.2.2.2.2.2.2.3.5.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.5.2">𝑉</ci><ci id="S3.E8.m1.2.2.2.2.2.2.3.5.3.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3.5.3">𝑎</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.2c">\mathbf{Q}_{v}=\mathbf{X}v\mathbf{W}{Q_{v}},\quad\mathbf{K}_{a}=\mathbf{X}a\mathbf{W}{K_{a}},\quad\mathbf{V}a=\mathbf{X}a\mathbf{W}{V_{a}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.1" class="ltx_Math" alttext="\mathbf{X}{a|v}=\text{softmax}\left(\frac{\mathbf{Q}_{v}\mathbf{K}_{a}^{T}}{\sqrt{d}}\right)\mathbf{V}_{a}" display="block"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.2" xref="S3.E9.m1.1.2.cmml"><mrow id="S3.E9.m1.1.2.2" xref="S3.E9.m1.1.2.2.cmml"><mrow id="S3.E9.m1.1.2.2.2" xref="S3.E9.m1.1.2.2.2.cmml"><mi id="S3.E9.m1.1.2.2.2.2" xref="S3.E9.m1.1.2.2.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.2.2.2.1" xref="S3.E9.m1.1.2.2.2.1.cmml">​</mo><mi id="S3.E9.m1.1.2.2.2.3" xref="S3.E9.m1.1.2.2.2.3.cmml">a</mi></mrow><mo fence="false" id="S3.E9.m1.1.2.2.1" xref="S3.E9.m1.1.2.2.1.cmml">|</mo><mi id="S3.E9.m1.1.2.2.3" xref="S3.E9.m1.1.2.2.3.cmml">v</mi></mrow><mo id="S3.E9.m1.1.2.1" xref="S3.E9.m1.1.2.1.cmml">=</mo><mrow id="S3.E9.m1.1.2.3" xref="S3.E9.m1.1.2.3.cmml"><mtext id="S3.E9.m1.1.2.3.2" xref="S3.E9.m1.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.2.3.1" xref="S3.E9.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E9.m1.1.2.3.3.2" xref="S3.E9.m1.1.1.cmml"><mo id="S3.E9.m1.1.2.3.3.2.1" xref="S3.E9.m1.1.1.cmml">(</mo><mfrac id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><mrow id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml"><msub id="S3.E9.m1.1.1.2.2" xref="S3.E9.m1.1.1.2.2.cmml"><mi id="S3.E9.m1.1.1.2.2.2" xref="S3.E9.m1.1.1.2.2.2.cmml">𝐐</mi><mi id="S3.E9.m1.1.1.2.2.3" xref="S3.E9.m1.1.1.2.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.2.1" xref="S3.E9.m1.1.1.2.1.cmml">​</mo><msubsup id="S3.E9.m1.1.1.2.3" xref="S3.E9.m1.1.1.2.3.cmml"><mi id="S3.E9.m1.1.1.2.3.2.2" xref="S3.E9.m1.1.1.2.3.2.2.cmml">𝐊</mi><mi id="S3.E9.m1.1.1.2.3.2.3" xref="S3.E9.m1.1.1.2.3.2.3.cmml">a</mi><mi id="S3.E9.m1.1.1.2.3.3" xref="S3.E9.m1.1.1.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.3.2" xref="S3.E9.m1.1.1.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.E9.m1.1.2.3.3.2.2" xref="S3.E9.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.2.3.1a" xref="S3.E9.m1.1.2.3.1.cmml">​</mo><msub id="S3.E9.m1.1.2.3.4" xref="S3.E9.m1.1.2.3.4.cmml"><mi id="S3.E9.m1.1.2.3.4.2" xref="S3.E9.m1.1.2.3.4.2.cmml">𝐕</mi><mi id="S3.E9.m1.1.2.3.4.3" xref="S3.E9.m1.1.2.3.4.3.cmml">a</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.2.cmml" xref="S3.E9.m1.1.2"><eq id="S3.E9.m1.1.2.1.cmml" xref="S3.E9.m1.1.2.1"></eq><apply id="S3.E9.m1.1.2.2.cmml" xref="S3.E9.m1.1.2.2"><csymbol cd="latexml" id="S3.E9.m1.1.2.2.1.cmml" xref="S3.E9.m1.1.2.2.1">conditional</csymbol><apply id="S3.E9.m1.1.2.2.2.cmml" xref="S3.E9.m1.1.2.2.2"><times id="S3.E9.m1.1.2.2.2.1.cmml" xref="S3.E9.m1.1.2.2.2.1"></times><ci id="S3.E9.m1.1.2.2.2.2.cmml" xref="S3.E9.m1.1.2.2.2.2">𝐗</ci><ci id="S3.E9.m1.1.2.2.2.3.cmml" xref="S3.E9.m1.1.2.2.2.3">𝑎</ci></apply><ci id="S3.E9.m1.1.2.2.3.cmml" xref="S3.E9.m1.1.2.2.3">𝑣</ci></apply><apply id="S3.E9.m1.1.2.3.cmml" xref="S3.E9.m1.1.2.3"><times id="S3.E9.m1.1.2.3.1.cmml" xref="S3.E9.m1.1.2.3.1"></times><ci id="S3.E9.m1.1.2.3.2a.cmml" xref="S3.E9.m1.1.2.3.2"><mtext id="S3.E9.m1.1.2.3.2.cmml" xref="S3.E9.m1.1.2.3.2">softmax</mtext></ci><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.2.3.3.2"><divide id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.2.3.3.2"></divide><apply id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2"><times id="S3.E9.m1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.2.1"></times><apply id="S3.E9.m1.1.1.2.2.cmml" xref="S3.E9.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.2.2.1.cmml" xref="S3.E9.m1.1.1.2.2">subscript</csymbol><ci id="S3.E9.m1.1.1.2.2.2.cmml" xref="S3.E9.m1.1.1.2.2.2">𝐐</ci><ci id="S3.E9.m1.1.1.2.2.3.cmml" xref="S3.E9.m1.1.1.2.2.3">𝑣</ci></apply><apply id="S3.E9.m1.1.1.2.3.cmml" xref="S3.E9.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.2.3.1.cmml" xref="S3.E9.m1.1.1.2.3">superscript</csymbol><apply id="S3.E9.m1.1.1.2.3.2.cmml" xref="S3.E9.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.2.3.2.1.cmml" xref="S3.E9.m1.1.1.2.3">subscript</csymbol><ci id="S3.E9.m1.1.1.2.3.2.2.cmml" xref="S3.E9.m1.1.1.2.3.2.2">𝐊</ci><ci id="S3.E9.m1.1.1.2.3.2.3.cmml" xref="S3.E9.m1.1.1.2.3.2.3">𝑎</ci></apply><ci id="S3.E9.m1.1.1.2.3.3.cmml" xref="S3.E9.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3"><root id="S3.E9.m1.1.1.3a.cmml" xref="S3.E9.m1.1.1.3"></root><ci id="S3.E9.m1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.3.2">𝑑</ci></apply></apply><apply id="S3.E9.m1.1.2.3.4.cmml" xref="S3.E9.m1.1.2.3.4"><csymbol cd="ambiguous" id="S3.E9.m1.1.2.3.4.1.cmml" xref="S3.E9.m1.1.2.3.4">subscript</csymbol><ci id="S3.E9.m1.1.2.3.4.2.cmml" xref="S3.E9.m1.1.2.3.4.2">𝐕</ci><ci id="S3.E9.m1.1.2.3.4.3.cmml" xref="S3.E9.m1.1.2.3.4.3">𝑎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\mathbf{X}{a|v}=\text{softmax}\left(\frac{\mathbf{Q}_{v}\mathbf{K}_{a}^{T}}{\sqrt{d}}\right)\mathbf{V}_{a}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS4.p1.8" class="ltx_p">The attended features <math id="S3.SS2.SSS4.p1.5.m1.1" class="ltx_Math" alttext="\mathbf{X}{v|a}" display="inline"><semantics id="S3.SS2.SSS4.p1.5.m1.1a"><mrow id="S3.SS2.SSS4.p1.5.m1.1.1" xref="S3.SS2.SSS4.p1.5.m1.1.1.cmml"><mrow id="S3.SS2.SSS4.p1.5.m1.1.1.2" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.cmml"><mi id="S3.SS2.SSS4.p1.5.m1.1.1.2.2" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.5.m1.1.1.2.1" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.5.m1.1.1.2.3" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.3.cmml">v</mi></mrow><mo fence="false" id="S3.SS2.SSS4.p1.5.m1.1.1.1" xref="S3.SS2.SSS4.p1.5.m1.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS4.p1.5.m1.1.1.3" xref="S3.SS2.SSS4.p1.5.m1.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.5.m1.1b"><apply id="S3.SS2.SSS4.p1.5.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS4.p1.5.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.1">conditional</csymbol><apply id="S3.SS2.SSS4.p1.5.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.2"><times id="S3.SS2.SSS4.p1.5.m1.1.1.2.1.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.1"></times><ci id="S3.SS2.SSS4.p1.5.m1.1.1.2.2.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.5.m1.1.1.2.3.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.2.3">𝑣</ci></apply><ci id="S3.SS2.SSS4.p1.5.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p1.5.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.5.m1.1c">\mathbf{X}{v|a}</annotation></semantics></math> and <math id="S3.SS2.SSS4.p1.6.m2.1" class="ltx_Math" alttext="\mathbf{X}{a|v}" display="inline"><semantics id="S3.SS2.SSS4.p1.6.m2.1a"><mrow id="S3.SS2.SSS4.p1.6.m2.1.1" xref="S3.SS2.SSS4.p1.6.m2.1.1.cmml"><mrow id="S3.SS2.SSS4.p1.6.m2.1.1.2" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.cmml"><mi id="S3.SS2.SSS4.p1.6.m2.1.1.2.2" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.2.cmml">𝐗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.6.m2.1.1.2.1" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.6.m2.1.1.2.3" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.3.cmml">a</mi></mrow><mo fence="false" id="S3.SS2.SSS4.p1.6.m2.1.1.1" xref="S3.SS2.SSS4.p1.6.m2.1.1.1.cmml">|</mo><mi id="S3.SS2.SSS4.p1.6.m2.1.1.3" xref="S3.SS2.SSS4.p1.6.m2.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.6.m2.1b"><apply id="S3.SS2.SSS4.p1.6.m2.1.1.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS4.p1.6.m2.1.1.1.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.1">conditional</csymbol><apply id="S3.SS2.SSS4.p1.6.m2.1.1.2.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.2"><times id="S3.SS2.SSS4.p1.6.m2.1.1.2.1.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.1"></times><ci id="S3.SS2.SSS4.p1.6.m2.1.1.2.2.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.6.m2.1.1.2.3.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.2.3">𝑎</ci></apply><ci id="S3.SS2.SSS4.p1.6.m2.1.1.3.cmml" xref="S3.SS2.SSS4.p1.6.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.6.m2.1c">\mathbf{X}{a|v}</annotation></semantics></math> are then concatenated with the self-attended features <math id="S3.SS2.SSS4.p1.7.m3.1" class="ltx_Math" alttext="\mathbf{X}_{a}" display="inline"><semantics id="S3.SS2.SSS4.p1.7.m3.1a"><msub id="S3.SS2.SSS4.p1.7.m3.1.1" xref="S3.SS2.SSS4.p1.7.m3.1.1.cmml"><mi id="S3.SS2.SSS4.p1.7.m3.1.1.2" xref="S3.SS2.SSS4.p1.7.m3.1.1.2.cmml">𝐗</mi><mi id="S3.SS2.SSS4.p1.7.m3.1.1.3" xref="S3.SS2.SSS4.p1.7.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.7.m3.1b"><apply id="S3.SS2.SSS4.p1.7.m3.1.1.cmml" xref="S3.SS2.SSS4.p1.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.7.m3.1.1.1.cmml" xref="S3.SS2.SSS4.p1.7.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.7.m3.1.1.2.cmml" xref="S3.SS2.SSS4.p1.7.m3.1.1.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.7.m3.1.1.3.cmml" xref="S3.SS2.SSS4.p1.7.m3.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.7.m3.1c">\mathbf{X}_{a}</annotation></semantics></math> and <math id="S3.SS2.SSS4.p1.8.m4.1" class="ltx_Math" alttext="\mathbf{X}_{v}" display="inline"><semantics id="S3.SS2.SSS4.p1.8.m4.1a"><msub id="S3.SS2.SSS4.p1.8.m4.1.1" xref="S3.SS2.SSS4.p1.8.m4.1.1.cmml"><mi id="S3.SS2.SSS4.p1.8.m4.1.1.2" xref="S3.SS2.SSS4.p1.8.m4.1.1.2.cmml">𝐗</mi><mi id="S3.SS2.SSS4.p1.8.m4.1.1.3" xref="S3.SS2.SSS4.p1.8.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.8.m4.1b"><apply id="S3.SS2.SSS4.p1.8.m4.1.1.cmml" xref="S3.SS2.SSS4.p1.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.8.m4.1.1.1.cmml" xref="S3.SS2.SSS4.p1.8.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.8.m4.1.1.2.cmml" xref="S3.SS2.SSS4.p1.8.m4.1.1.2">𝐗</ci><ci id="S3.SS2.SSS4.p1.8.m4.1.1.3.cmml" xref="S3.SS2.SSS4.p1.8.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.8.m4.1c">\mathbf{X}_{v}</annotation></semantics></math> for further processing.</p>
</div>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>3.2.5 Self and Cross Modal Attention Network</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">The Self and Cross Modal Attention Network (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS5.p1.1.1" class="ltx_text" style="color:#FF0000;">(f)</span>) extends the Audio-Visual Attention Network by incorporating additional self-attention layers after the cross-modal attention. This allows the network to refine the learned representations further by capturing intra-modal dependencies.</p>
</div>
</section>
<section id="S3.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>3.2.6 Self-Attended Cross-Modal FCRN Network</h4>

<div id="S3.SS2.SSS6.p1" class="ltx_para">
<p id="S3.SS2.SSS6.p1.1" class="ltx_p">The Self-Attended Cross-Modal FCRN Network (Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 3.2.1 FC Attention Network ‣ 3.2 Proposed Models ‣ 3 Methodology ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.SSS6.p1.1.1" class="ltx_text" style="color:#FF0000;">(g)</span>) combines the self-attention and cross-modal attention mechanisms with the residual learning framework. The network employs residual blocks to facilitate the learning of complex feature interactions while leveraging the attention mechanisms to focus on relevant information.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Loss Function</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.7" class="ltx_p">We employ the cross-entropy loss function for multi-label classification:</p>
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E10.m1.13" class="ltx_Math" alttext="\mathcal{L}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})+(1-{y}_{i,c})\log(1-\hat{y}_{i,c})" display="block"><semantics id="S3.E10.m1.13a"><mrow id="S3.E10.m1.13.13" xref="S3.E10.m1.13.13.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E10.m1.13.13.5" xref="S3.E10.m1.13.13.5.cmml">ℒ</mi><mo id="S3.E10.m1.13.13.4" xref="S3.E10.m1.13.13.4.cmml">=</mo><mrow id="S3.E10.m1.13.13.3" xref="S3.E10.m1.13.13.3.cmml"><mrow id="S3.E10.m1.11.11.1.1" xref="S3.E10.m1.11.11.1.1.cmml"><mo id="S3.E10.m1.11.11.1.1a" xref="S3.E10.m1.11.11.1.1.cmml">−</mo><mrow id="S3.E10.m1.11.11.1.1.1" xref="S3.E10.m1.11.11.1.1.1.cmml"><mfrac id="S3.E10.m1.11.11.1.1.1.3" xref="S3.E10.m1.11.11.1.1.1.3.cmml"><mn id="S3.E10.m1.11.11.1.1.1.3.2" xref="S3.E10.m1.11.11.1.1.1.3.2.cmml">1</mn><mi id="S3.E10.m1.11.11.1.1.1.3.3" xref="S3.E10.m1.11.11.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E10.m1.11.11.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.11.11.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.cmml"><munderover id="S3.E10.m1.11.11.1.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E10.m1.11.11.1.1.1.1.2.2.2" xref="S3.E10.m1.11.11.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E10.m1.11.11.1.1.1.1.2.2.3" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.cmml"><mi id="S3.E10.m1.11.11.1.1.1.1.2.2.3.2" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E10.m1.11.11.1.1.1.1.2.2.3.1" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E10.m1.11.11.1.1.1.1.2.2.3.3" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E10.m1.11.11.1.1.1.1.2.3" xref="S3.E10.m1.11.11.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E10.m1.11.11.1.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.1.cmml"><munderover id="S3.E10.m1.11.11.1.1.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E10.m1.11.11.1.1.1.1.1.2.2.2" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.2" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.2.cmml">c</mi><mo id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.1" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.3" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E10.m1.11.11.1.1.1.1.1.2.3" xref="S3.E10.m1.11.11.1.1.1.1.1.2.3.cmml">C</mi></munderover><mrow id="S3.E10.m1.11.11.1.1.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.1.1.cmml"><msub id="S3.E10.m1.11.11.1.1.1.1.1.1.3" xref="S3.E10.m1.11.11.1.1.1.1.1.1.3.cmml"><mi id="S3.E10.m1.11.11.1.1.1.1.1.1.3.2" xref="S3.E10.m1.11.11.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.E10.m1.2.2.2.4" xref="S3.E10.m1.2.2.2.3.cmml"><mi id="S3.E10.m1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.cmml">i</mi><mo id="S3.E10.m1.2.2.2.4.1" xref="S3.E10.m1.2.2.2.3.cmml">,</mo><mi id="S3.E10.m1.2.2.2.2" xref="S3.E10.m1.2.2.2.2.cmml">c</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.E10.m1.11.11.1.1.1.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E10.m1.9.9" xref="S3.E10.m1.9.9.cmml">log</mi><mo id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1a" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E10.m1.4.4.2.4" xref="S3.E10.m1.4.4.2.3.cmml"><mi id="S3.E10.m1.3.3.1.1" xref="S3.E10.m1.3.3.1.1.cmml">i</mi><mo id="S3.E10.m1.4.4.2.4.1" xref="S3.E10.m1.4.4.2.3.cmml">,</mo><mi id="S3.E10.m1.4.4.2.2" xref="S3.E10.m1.4.4.2.2.cmml">c</mi></mrow></msub><mo stretchy="false" id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.3" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E10.m1.13.13.3.4" xref="S3.E10.m1.13.13.3.4.cmml">+</mo><mrow id="S3.E10.m1.13.13.3.3" xref="S3.E10.m1.13.13.3.3.cmml"><mrow id="S3.E10.m1.12.12.2.2.1.1" xref="S3.E10.m1.12.12.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E10.m1.12.12.2.2.1.1.2" xref="S3.E10.m1.12.12.2.2.1.1.1.cmml">(</mo><mrow id="S3.E10.m1.12.12.2.2.1.1.1" xref="S3.E10.m1.12.12.2.2.1.1.1.cmml"><mn id="S3.E10.m1.12.12.2.2.1.1.1.2" xref="S3.E10.m1.12.12.2.2.1.1.1.2.cmml">1</mn><mo id="S3.E10.m1.12.12.2.2.1.1.1.1" xref="S3.E10.m1.12.12.2.2.1.1.1.1.cmml">−</mo><msub id="S3.E10.m1.12.12.2.2.1.1.1.3" xref="S3.E10.m1.12.12.2.2.1.1.1.3.cmml"><mi id="S3.E10.m1.12.12.2.2.1.1.1.3.2" xref="S3.E10.m1.12.12.2.2.1.1.1.3.2.cmml">y</mi><mrow id="S3.E10.m1.6.6.2.4" xref="S3.E10.m1.6.6.2.3.cmml"><mi id="S3.E10.m1.5.5.1.1" xref="S3.E10.m1.5.5.1.1.cmml">i</mi><mo id="S3.E10.m1.6.6.2.4.1" xref="S3.E10.m1.6.6.2.3.cmml">,</mo><mi id="S3.E10.m1.6.6.2.2" xref="S3.E10.m1.6.6.2.2.cmml">c</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E10.m1.12.12.2.2.1.1.3" xref="S3.E10.m1.12.12.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E10.m1.13.13.3.3.3" xref="S3.E10.m1.13.13.3.3.3.cmml">​</mo><mrow id="S3.E10.m1.13.13.3.3.2.1" xref="S3.E10.m1.13.13.3.3.2.2.cmml"><mi id="S3.E10.m1.10.10" xref="S3.E10.m1.10.10.cmml">log</mi><mo id="S3.E10.m1.13.13.3.3.2.1a" xref="S3.E10.m1.13.13.3.3.2.2.cmml">⁡</mo><mrow id="S3.E10.m1.13.13.3.3.2.1.1" xref="S3.E10.m1.13.13.3.3.2.2.cmml"><mo stretchy="false" id="S3.E10.m1.13.13.3.3.2.1.1.2" xref="S3.E10.m1.13.13.3.3.2.2.cmml">(</mo><mrow id="S3.E10.m1.13.13.3.3.2.1.1.1" xref="S3.E10.m1.13.13.3.3.2.1.1.1.cmml"><mn id="S3.E10.m1.13.13.3.3.2.1.1.1.2" xref="S3.E10.m1.13.13.3.3.2.1.1.1.2.cmml">1</mn><mo id="S3.E10.m1.13.13.3.3.2.1.1.1.1" xref="S3.E10.m1.13.13.3.3.2.1.1.1.1.cmml">−</mo><msub id="S3.E10.m1.13.13.3.3.2.1.1.1.3" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.cmml"><mover accent="true" id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.cmml"><mi id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.2" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.2.cmml">y</mi><mo id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.1" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E10.m1.8.8.2.4" xref="S3.E10.m1.8.8.2.3.cmml"><mi id="S3.E10.m1.7.7.1.1" xref="S3.E10.m1.7.7.1.1.cmml">i</mi><mo id="S3.E10.m1.8.8.2.4.1" xref="S3.E10.m1.8.8.2.3.cmml">,</mo><mi id="S3.E10.m1.8.8.2.2" xref="S3.E10.m1.8.8.2.2.cmml">c</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E10.m1.13.13.3.3.2.1.1.3" xref="S3.E10.m1.13.13.3.3.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.13b"><apply id="S3.E10.m1.13.13.cmml" xref="S3.E10.m1.13.13"><eq id="S3.E10.m1.13.13.4.cmml" xref="S3.E10.m1.13.13.4"></eq><ci id="S3.E10.m1.13.13.5.cmml" xref="S3.E10.m1.13.13.5">ℒ</ci><apply id="S3.E10.m1.13.13.3.cmml" xref="S3.E10.m1.13.13.3"><plus id="S3.E10.m1.13.13.3.4.cmml" xref="S3.E10.m1.13.13.3.4"></plus><apply id="S3.E10.m1.11.11.1.1.cmml" xref="S3.E10.m1.11.11.1.1"><minus id="S3.E10.m1.11.11.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1"></minus><apply id="S3.E10.m1.11.11.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1"><times id="S3.E10.m1.11.11.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.2"></times><apply id="S3.E10.m1.11.11.1.1.1.3.cmml" xref="S3.E10.m1.11.11.1.1.1.3"><divide id="S3.E10.m1.11.11.1.1.1.3.1.cmml" xref="S3.E10.m1.11.11.1.1.1.3"></divide><cn type="integer" id="S3.E10.m1.11.11.1.1.1.3.2.cmml" xref="S3.E10.m1.11.11.1.1.1.3.2">1</cn><ci id="S3.E10.m1.11.11.1.1.1.3.3.cmml" xref="S3.E10.m1.11.11.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E10.m1.11.11.1.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1"><apply id="S3.E10.m1.11.11.1.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.2.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2">superscript</csymbol><apply id="S3.E10.m1.11.11.1.1.1.1.2.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.2.2.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2">subscript</csymbol><sum id="S3.E10.m1.11.11.1.1.1.1.2.2.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.2.2"></sum><apply id="S3.E10.m1.11.11.1.1.1.1.2.2.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3"><eq id="S3.E10.m1.11.11.1.1.1.1.2.2.3.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.1"></eq><ci id="S3.E10.m1.11.11.1.1.1.1.2.2.3.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E10.m1.11.11.1.1.1.1.2.2.3.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E10.m1.11.11.1.1.1.1.2.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E10.m1.11.11.1.1.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1"><apply id="S3.E10.m1.11.11.1.1.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.1.2.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E10.m1.11.11.1.1.1.1.1.2.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.1.2.2.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E10.m1.11.11.1.1.1.1.1.2.2.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.2"></sum><apply id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3"><eq id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.2">𝑐</ci><cn type="integer" id="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E10.m1.11.11.1.1.1.1.1.2.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.2.3">𝐶</ci></apply><apply id="S3.E10.m1.11.11.1.1.1.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1"><times id="S3.E10.m1.11.11.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.2"></times><apply id="S3.E10.m1.11.11.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.1.1.3.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.11.11.1.1.1.1.1.1.3.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.3.2">𝑦</ci><list id="S3.E10.m1.2.2.2.3.cmml" xref="S3.E10.m1.2.2.2.4"><ci id="S3.E10.m1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1">𝑖</ci><ci id="S3.E10.m1.2.2.2.2.cmml" xref="S3.E10.m1.2.2.2.2">𝑐</ci></list></apply><apply id="S3.E10.m1.11.11.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1"><log id="S3.E10.m1.9.9.cmml" xref="S3.E10.m1.9.9"></log><apply id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E10.m1.11.11.1.1.1.1.1.1.1.1.1.1.2.2">𝑦</ci></apply><list id="S3.E10.m1.4.4.2.3.cmml" xref="S3.E10.m1.4.4.2.4"><ci id="S3.E10.m1.3.3.1.1.cmml" xref="S3.E10.m1.3.3.1.1">𝑖</ci><ci id="S3.E10.m1.4.4.2.2.cmml" xref="S3.E10.m1.4.4.2.2">𝑐</ci></list></apply></apply></apply></apply></apply></apply></apply><apply id="S3.E10.m1.13.13.3.3.cmml" xref="S3.E10.m1.13.13.3.3"><times id="S3.E10.m1.13.13.3.3.3.cmml" xref="S3.E10.m1.13.13.3.3.3"></times><apply id="S3.E10.m1.12.12.2.2.1.1.1.cmml" xref="S3.E10.m1.12.12.2.2.1.1"><minus id="S3.E10.m1.12.12.2.2.1.1.1.1.cmml" xref="S3.E10.m1.12.12.2.2.1.1.1.1"></minus><cn type="integer" id="S3.E10.m1.12.12.2.2.1.1.1.2.cmml" xref="S3.E10.m1.12.12.2.2.1.1.1.2">1</cn><apply id="S3.E10.m1.12.12.2.2.1.1.1.3.cmml" xref="S3.E10.m1.12.12.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.12.12.2.2.1.1.1.3.1.cmml" xref="S3.E10.m1.12.12.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E10.m1.12.12.2.2.1.1.1.3.2.cmml" xref="S3.E10.m1.12.12.2.2.1.1.1.3.2">𝑦</ci><list id="S3.E10.m1.6.6.2.3.cmml" xref="S3.E10.m1.6.6.2.4"><ci id="S3.E10.m1.5.5.1.1.cmml" xref="S3.E10.m1.5.5.1.1">𝑖</ci><ci id="S3.E10.m1.6.6.2.2.cmml" xref="S3.E10.m1.6.6.2.2">𝑐</ci></list></apply></apply><apply id="S3.E10.m1.13.13.3.3.2.2.cmml" xref="S3.E10.m1.13.13.3.3.2.1"><log id="S3.E10.m1.10.10.cmml" xref="S3.E10.m1.10.10"></log><apply id="S3.E10.m1.13.13.3.3.2.1.1.1.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1"><minus id="S3.E10.m1.13.13.3.3.2.1.1.1.1.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.1"></minus><cn type="integer" id="S3.E10.m1.13.13.3.3.2.1.1.1.2.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.2">1</cn><apply id="S3.E10.m1.13.13.3.3.2.1.1.1.3.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m1.13.13.3.3.2.1.1.1.3.1.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3">subscript</csymbol><apply id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2"><ci id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.1.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.1">^</ci><ci id="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.2.cmml" xref="S3.E10.m1.13.13.3.3.2.1.1.1.3.2.2">𝑦</ci></apply><list id="S3.E10.m1.8.8.2.3.cmml" xref="S3.E10.m1.8.8.2.4"><ci id="S3.E10.m1.7.7.1.1.cmml" xref="S3.E10.m1.7.7.1.1">𝑖</ci><ci id="S3.E10.m1.8.8.2.2.cmml" xref="S3.E10.m1.8.8.2.2">𝑐</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.13c">\mathcal{L}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})+(1-{y}_{i,c})\log(1-\hat{y}_{i,c})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.6" class="ltx_p">where <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation></semantics></math> is the number of samples, <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">C</annotation></semantics></math> is the number of classes, <math id="S3.SS3.p1.3.m3.2" class="ltx_Math" alttext="y_{i,c}" display="inline"><semantics id="S3.SS3.p1.3.m3.2a"><msub id="S3.SS3.p1.3.m3.2.3" xref="S3.SS3.p1.3.m3.2.3.cmml"><mi id="S3.SS3.p1.3.m3.2.3.2" xref="S3.SS3.p1.3.m3.2.3.2.cmml">y</mi><mrow id="S3.SS3.p1.3.m3.2.2.2.4" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p1.3.m3.2.2.2.4.1" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p1.3.m3.2.2.2.2" xref="S3.SS3.p1.3.m3.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.2b"><apply id="S3.SS3.p1.3.m3.2.3.cmml" xref="S3.SS3.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.3.1.cmml" xref="S3.SS3.p1.3.m3.2.3">subscript</csymbol><ci id="S3.SS3.p1.3.m3.2.3.2.cmml" xref="S3.SS3.p1.3.m3.2.3.2">𝑦</ci><list id="S3.SS3.p1.3.m3.2.2.2.3.cmml" xref="S3.SS3.p1.3.m3.2.2.2.4"><ci id="S3.SS3.p1.3.m3.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1">𝑖</ci><ci id="S3.SS3.p1.3.m3.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2">𝑐</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.2c">y_{i,c}</annotation></semantics></math> is the ground truth label for sample <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">i</annotation></semantics></math> and class <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">c</annotation></semantics></math>, and <math id="S3.SS3.p1.6.m6.2" class="ltx_Math" alttext="\hat{y}_{i,c}" display="inline"><semantics id="S3.SS3.p1.6.m6.2a"><msub id="S3.SS3.p1.6.m6.2.3" xref="S3.SS3.p1.6.m6.2.3.cmml"><mover accent="true" id="S3.SS3.p1.6.m6.2.3.2" xref="S3.SS3.p1.6.m6.2.3.2.cmml"><mi id="S3.SS3.p1.6.m6.2.3.2.2" xref="S3.SS3.p1.6.m6.2.3.2.2.cmml">y</mi><mo id="S3.SS3.p1.6.m6.2.3.2.1" xref="S3.SS3.p1.6.m6.2.3.2.1.cmml">^</mo></mover><mrow id="S3.SS3.p1.6.m6.2.2.2.4" xref="S3.SS3.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS3.p1.6.m6.1.1.1.1" xref="S3.SS3.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p1.6.m6.2.2.2.4.1" xref="S3.SS3.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p1.6.m6.2.2.2.2" xref="S3.SS3.p1.6.m6.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.2b"><apply id="S3.SS3.p1.6.m6.2.3.cmml" xref="S3.SS3.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.2.3.1.cmml" xref="S3.SS3.p1.6.m6.2.3">subscript</csymbol><apply id="S3.SS3.p1.6.m6.2.3.2.cmml" xref="S3.SS3.p1.6.m6.2.3.2"><ci id="S3.SS3.p1.6.m6.2.3.2.1.cmml" xref="S3.SS3.p1.6.m6.2.3.2.1">^</ci><ci id="S3.SS3.p1.6.m6.2.3.2.2.cmml" xref="S3.SS3.p1.6.m6.2.3.2.2">𝑦</ci></apply><list id="S3.SS3.p1.6.m6.2.2.2.3.cmml" xref="S3.SS3.p1.6.m6.2.2.2.4"><ci id="S3.SS3.p1.6.m6.1.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1.1">𝑖</ci><ci id="S3.SS3.p1.6.m6.2.2.2.2.cmml" xref="S3.SS3.p1.6.m6.2.2.2.2">𝑐</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.2c">\hat{y}_{i,c}</annotation></semantics></math> is the predicted probability.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimentation and Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">All models are implemented using PyTorch and trained on NVIDIA RTX3090 GPUs. We apply dropout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> with a rate of 0.4 to the fully connected layers to prevent overfitting. The dimensions of fully connected layers are set to 8K for the baseline models and 2K for the proposed models, and the dimensions of the attention layers are set to 1024. All models are trained using the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> with a learning rate of 0.0001. The models are trained for 20 epochs on the YouTube-8M dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">We evaluate the performance of our models using the Global Average Precision (GAP) metric and F1 score. The GAP metric is the mean Average Precision (AP) across all classes. The AP for a single class is defined as:</p>
<table id="S4.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E11.m1.2" class="ltx_Math" alttext="AP=\sum_{k=1}^{n}P(k)\Delta r(k)" display="block"><semantics id="S4.E11.m1.2a"><mrow id="S4.E11.m1.2.3" xref="S4.E11.m1.2.3.cmml"><mrow id="S4.E11.m1.2.3.2" xref="S4.E11.m1.2.3.2.cmml"><mi id="S4.E11.m1.2.3.2.2" xref="S4.E11.m1.2.3.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E11.m1.2.3.2.1" xref="S4.E11.m1.2.3.2.1.cmml">​</mo><mi id="S4.E11.m1.2.3.2.3" xref="S4.E11.m1.2.3.2.3.cmml">P</mi></mrow><mo rspace="0.111em" id="S4.E11.m1.2.3.1" xref="S4.E11.m1.2.3.1.cmml">=</mo><mrow id="S4.E11.m1.2.3.3" xref="S4.E11.m1.2.3.3.cmml"><munderover id="S4.E11.m1.2.3.3.1" xref="S4.E11.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S4.E11.m1.2.3.3.1.2.2" xref="S4.E11.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E11.m1.2.3.3.1.2.3" xref="S4.E11.m1.2.3.3.1.2.3.cmml"><mi id="S4.E11.m1.2.3.3.1.2.3.2" xref="S4.E11.m1.2.3.3.1.2.3.2.cmml">k</mi><mo id="S4.E11.m1.2.3.3.1.2.3.1" xref="S4.E11.m1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E11.m1.2.3.3.1.2.3.3" xref="S4.E11.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E11.m1.2.3.3.1.3" xref="S4.E11.m1.2.3.3.1.3.cmml">n</mi></munderover><mrow id="S4.E11.m1.2.3.3.2" xref="S4.E11.m1.2.3.3.2.cmml"><mi id="S4.E11.m1.2.3.3.2.2" xref="S4.E11.m1.2.3.3.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E11.m1.2.3.3.2.1" xref="S4.E11.m1.2.3.3.2.1.cmml">​</mo><mrow id="S4.E11.m1.2.3.3.2.3.2" xref="S4.E11.m1.2.3.3.2.cmml"><mo stretchy="false" id="S4.E11.m1.2.3.3.2.3.2.1" xref="S4.E11.m1.2.3.3.2.cmml">(</mo><mi id="S4.E11.m1.1.1" xref="S4.E11.m1.1.1.cmml">k</mi><mo stretchy="false" id="S4.E11.m1.2.3.3.2.3.2.2" xref="S4.E11.m1.2.3.3.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E11.m1.2.3.3.2.1a" xref="S4.E11.m1.2.3.3.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.E11.m1.2.3.3.2.4" xref="S4.E11.m1.2.3.3.2.4.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.E11.m1.2.3.3.2.1b" xref="S4.E11.m1.2.3.3.2.1.cmml">​</mo><mi id="S4.E11.m1.2.3.3.2.5" xref="S4.E11.m1.2.3.3.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E11.m1.2.3.3.2.1c" xref="S4.E11.m1.2.3.3.2.1.cmml">​</mo><mrow id="S4.E11.m1.2.3.3.2.6.2" xref="S4.E11.m1.2.3.3.2.cmml"><mo stretchy="false" id="S4.E11.m1.2.3.3.2.6.2.1" xref="S4.E11.m1.2.3.3.2.cmml">(</mo><mi id="S4.E11.m1.2.2" xref="S4.E11.m1.2.2.cmml">k</mi><mo stretchy="false" id="S4.E11.m1.2.3.3.2.6.2.2" xref="S4.E11.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E11.m1.2b"><apply id="S4.E11.m1.2.3.cmml" xref="S4.E11.m1.2.3"><eq id="S4.E11.m1.2.3.1.cmml" xref="S4.E11.m1.2.3.1"></eq><apply id="S4.E11.m1.2.3.2.cmml" xref="S4.E11.m1.2.3.2"><times id="S4.E11.m1.2.3.2.1.cmml" xref="S4.E11.m1.2.3.2.1"></times><ci id="S4.E11.m1.2.3.2.2.cmml" xref="S4.E11.m1.2.3.2.2">𝐴</ci><ci id="S4.E11.m1.2.3.2.3.cmml" xref="S4.E11.m1.2.3.2.3">𝑃</ci></apply><apply id="S4.E11.m1.2.3.3.cmml" xref="S4.E11.m1.2.3.3"><apply id="S4.E11.m1.2.3.3.1.cmml" xref="S4.E11.m1.2.3.3.1"><csymbol cd="ambiguous" id="S4.E11.m1.2.3.3.1.1.cmml" xref="S4.E11.m1.2.3.3.1">superscript</csymbol><apply id="S4.E11.m1.2.3.3.1.2.cmml" xref="S4.E11.m1.2.3.3.1"><csymbol cd="ambiguous" id="S4.E11.m1.2.3.3.1.2.1.cmml" xref="S4.E11.m1.2.3.3.1">subscript</csymbol><sum id="S4.E11.m1.2.3.3.1.2.2.cmml" xref="S4.E11.m1.2.3.3.1.2.2"></sum><apply id="S4.E11.m1.2.3.3.1.2.3.cmml" xref="S4.E11.m1.2.3.3.1.2.3"><eq id="S4.E11.m1.2.3.3.1.2.3.1.cmml" xref="S4.E11.m1.2.3.3.1.2.3.1"></eq><ci id="S4.E11.m1.2.3.3.1.2.3.2.cmml" xref="S4.E11.m1.2.3.3.1.2.3.2">𝑘</ci><cn type="integer" id="S4.E11.m1.2.3.3.1.2.3.3.cmml" xref="S4.E11.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E11.m1.2.3.3.1.3.cmml" xref="S4.E11.m1.2.3.3.1.3">𝑛</ci></apply><apply id="S4.E11.m1.2.3.3.2.cmml" xref="S4.E11.m1.2.3.3.2"><times id="S4.E11.m1.2.3.3.2.1.cmml" xref="S4.E11.m1.2.3.3.2.1"></times><ci id="S4.E11.m1.2.3.3.2.2.cmml" xref="S4.E11.m1.2.3.3.2.2">𝑃</ci><ci id="S4.E11.m1.1.1.cmml" xref="S4.E11.m1.1.1">𝑘</ci><ci id="S4.E11.m1.2.3.3.2.4.cmml" xref="S4.E11.m1.2.3.3.2.4">Δ</ci><ci id="S4.E11.m1.2.3.3.2.5.cmml" xref="S4.E11.m1.2.3.3.2.5">𝑟</ci><ci id="S4.E11.m1.2.2.cmml" xref="S4.E11.m1.2.2">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E11.m1.2c">AP=\sum_{k=1}^{n}P(k)\Delta r(k)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p1.6" class="ltx_p">where <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">n</annotation></semantics></math> is the number of test samples, <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="P(k)" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.2" xref="S4.SS2.p1.2.m2.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.2.2" xref="S4.SS2.p1.2.m2.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.2.1" xref="S4.SS2.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S4.SS2.p1.2.m2.1.2.3.2" xref="S4.SS2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.2.m2.1.2.3.2.1" xref="S4.SS2.p1.2.m2.1.2.cmml">(</mo><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">k</mi><mo stretchy="false" id="S4.SS2.p1.2.m2.1.2.3.2.2" xref="S4.SS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.2.cmml" xref="S4.SS2.p1.2.m2.1.2"><times id="S4.SS2.p1.2.m2.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.2.1"></times><ci id="S4.SS2.p1.2.m2.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.2.2">𝑃</ci><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">P(k)</annotation></semantics></math> is the precision at cut-off <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">k</annotation></semantics></math> in the list, and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\Delta r(k)" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.2" xref="S4.SS2.p1.4.m4.1.2.cmml"><mi mathvariant="normal" id="S4.SS2.p1.4.m4.1.2.2" xref="S4.SS2.p1.4.m4.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.2.1" xref="S4.SS2.p1.4.m4.1.2.1.cmml">​</mo><mi id="S4.SS2.p1.4.m4.1.2.3" xref="S4.SS2.p1.4.m4.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.2.1a" xref="S4.SS2.p1.4.m4.1.2.1.cmml">​</mo><mrow id="S4.SS2.p1.4.m4.1.2.4.2" xref="S4.SS2.p1.4.m4.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.4.m4.1.2.4.2.1" xref="S4.SS2.p1.4.m4.1.2.cmml">(</mo><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">k</mi><mo stretchy="false" id="S4.SS2.p1.4.m4.1.2.4.2.2" xref="S4.SS2.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.2.cmml" xref="S4.SS2.p1.4.m4.1.2"><times id="S4.SS2.p1.4.m4.1.2.1.cmml" xref="S4.SS2.p1.4.m4.1.2.1"></times><ci id="S4.SS2.p1.4.m4.1.2.2.cmml" xref="S4.SS2.p1.4.m4.1.2.2">Δ</ci><ci id="S4.SS2.p1.4.m4.1.2.3.cmml" xref="S4.SS2.p1.4.m4.1.2.3">𝑟</ci><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\Delta r(k)</annotation></semantics></math> is the change in recall from items <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="k-1" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">k</mi><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">−</mo><mn id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><minus id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></minus><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝑘</ci><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">k-1</annotation></semantics></math> to <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">k</annotation></semantics></math>.
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The F1 score is the harmonic mean of precision and recall, providing a balanced measure of the model’s performance. It is calculated as follows:</p>
<table id="S4.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E12.m1.1" class="ltx_Math" alttext="F1=2\cdot\frac{precision\cdot recall}{precision+recall}" display="block"><semantics id="S4.E12.m1.1a"><mrow id="S4.E12.m1.1.1" xref="S4.E12.m1.1.1.cmml"><mrow id="S4.E12.m1.1.1.2" xref="S4.E12.m1.1.1.2.cmml"><mi id="S4.E12.m1.1.1.2.2" xref="S4.E12.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.2.1" xref="S4.E12.m1.1.1.2.1.cmml">​</mo><mn id="S4.E12.m1.1.1.2.3" xref="S4.E12.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.E12.m1.1.1.1" xref="S4.E12.m1.1.1.1.cmml">=</mo><mrow id="S4.E12.m1.1.1.3" xref="S4.E12.m1.1.1.3.cmml"><mn id="S4.E12.m1.1.1.3.2" xref="S4.E12.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E12.m1.1.1.3.1" xref="S4.E12.m1.1.1.3.1.cmml">⋅</mo><mfrac id="S4.E12.m1.1.1.3.3" xref="S4.E12.m1.1.1.3.3.cmml"><mrow id="S4.E12.m1.1.1.3.3.2" xref="S4.E12.m1.1.1.3.3.2.cmml"><mrow id="S4.E12.m1.1.1.3.3.2.2" xref="S4.E12.m1.1.1.3.3.2.2.cmml"><mrow id="S4.E12.m1.1.1.3.3.2.2.2" xref="S4.E12.m1.1.1.3.3.2.2.2.cmml"><mi id="S4.E12.m1.1.1.3.3.2.2.2.2" xref="S4.E12.m1.1.1.3.3.2.2.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.3" xref="S4.E12.m1.1.1.3.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1a" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.4" xref="S4.E12.m1.1.1.3.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1b" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.5" xref="S4.E12.m1.1.1.3.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1c" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.6" xref="S4.E12.m1.1.1.3.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1d" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.7" xref="S4.E12.m1.1.1.3.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1e" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.8" xref="S4.E12.m1.1.1.3.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1f" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.9" xref="S4.E12.m1.1.1.3.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.2.2.1g" xref="S4.E12.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.2.2.10" xref="S4.E12.m1.1.1.3.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.E12.m1.1.1.3.3.2.2.1" xref="S4.E12.m1.1.1.3.3.2.2.1.cmml">⋅</mo><mi id="S4.E12.m1.1.1.3.3.2.2.3" xref="S4.E12.m1.1.1.3.3.2.2.3.cmml">r</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.1" xref="S4.E12.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.3" xref="S4.E12.m1.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.1a" xref="S4.E12.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.4" xref="S4.E12.m1.1.1.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.1b" xref="S4.E12.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.5" xref="S4.E12.m1.1.1.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.1c" xref="S4.E12.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.6" xref="S4.E12.m1.1.1.3.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.2.1d" xref="S4.E12.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.2.7" xref="S4.E12.m1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S4.E12.m1.1.1.3.3.3" xref="S4.E12.m1.1.1.3.3.3.cmml"><mrow id="S4.E12.m1.1.1.3.3.3.2" xref="S4.E12.m1.1.1.3.3.3.2.cmml"><mi id="S4.E12.m1.1.1.3.3.3.2.2" xref="S4.E12.m1.1.1.3.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.3" xref="S4.E12.m1.1.1.3.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1a" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.4" xref="S4.E12.m1.1.1.3.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1b" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.5" xref="S4.E12.m1.1.1.3.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1c" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.6" xref="S4.E12.m1.1.1.3.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1d" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.7" xref="S4.E12.m1.1.1.3.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1e" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.8" xref="S4.E12.m1.1.1.3.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1f" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.9" xref="S4.E12.m1.1.1.3.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.2.1g" xref="S4.E12.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.2.10" xref="S4.E12.m1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S4.E12.m1.1.1.3.3.3.1" xref="S4.E12.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S4.E12.m1.1.1.3.3.3.3" xref="S4.E12.m1.1.1.3.3.3.3.cmml"><mi id="S4.E12.m1.1.1.3.3.3.3.2" xref="S4.E12.m1.1.1.3.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.3.1" xref="S4.E12.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.3.3" xref="S4.E12.m1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.3.1a" xref="S4.E12.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.3.4" xref="S4.E12.m1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.3.1b" xref="S4.E12.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.3.5" xref="S4.E12.m1.1.1.3.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.3.1c" xref="S4.E12.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.3.6" xref="S4.E12.m1.1.1.3.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.1.1.3.3.3.3.1d" xref="S4.E12.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E12.m1.1.1.3.3.3.3.7" xref="S4.E12.m1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E12.m1.1b"><apply id="S4.E12.m1.1.1.cmml" xref="S4.E12.m1.1.1"><eq id="S4.E12.m1.1.1.1.cmml" xref="S4.E12.m1.1.1.1"></eq><apply id="S4.E12.m1.1.1.2.cmml" xref="S4.E12.m1.1.1.2"><times id="S4.E12.m1.1.1.2.1.cmml" xref="S4.E12.m1.1.1.2.1"></times><ci id="S4.E12.m1.1.1.2.2.cmml" xref="S4.E12.m1.1.1.2.2">𝐹</ci><cn type="integer" id="S4.E12.m1.1.1.2.3.cmml" xref="S4.E12.m1.1.1.2.3">1</cn></apply><apply id="S4.E12.m1.1.1.3.cmml" xref="S4.E12.m1.1.1.3"><ci id="S4.E12.m1.1.1.3.1.cmml" xref="S4.E12.m1.1.1.3.1">⋅</ci><cn type="integer" id="S4.E12.m1.1.1.3.2.cmml" xref="S4.E12.m1.1.1.3.2">2</cn><apply id="S4.E12.m1.1.1.3.3.cmml" xref="S4.E12.m1.1.1.3.3"><divide id="S4.E12.m1.1.1.3.3.1.cmml" xref="S4.E12.m1.1.1.3.3"></divide><apply id="S4.E12.m1.1.1.3.3.2.cmml" xref="S4.E12.m1.1.1.3.3.2"><times id="S4.E12.m1.1.1.3.3.2.1.cmml" xref="S4.E12.m1.1.1.3.3.2.1"></times><apply id="S4.E12.m1.1.1.3.3.2.2.cmml" xref="S4.E12.m1.1.1.3.3.2.2"><ci id="S4.E12.m1.1.1.3.3.2.2.1.cmml" xref="S4.E12.m1.1.1.3.3.2.2.1">⋅</ci><apply id="S4.E12.m1.1.1.3.3.2.2.2.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2"><times id="S4.E12.m1.1.1.3.3.2.2.2.1.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.1"></times><ci id="S4.E12.m1.1.1.3.3.2.2.2.2.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.2">𝑝</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.3.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.3">𝑟</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.4.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.4">𝑒</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.5.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.5">𝑐</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.6.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.6">𝑖</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.7.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.7">𝑠</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.8.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.8">𝑖</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.9.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.9">𝑜</ci><ci id="S4.E12.m1.1.1.3.3.2.2.2.10.cmml" xref="S4.E12.m1.1.1.3.3.2.2.2.10">𝑛</ci></apply><ci id="S4.E12.m1.1.1.3.3.2.2.3.cmml" xref="S4.E12.m1.1.1.3.3.2.2.3">𝑟</ci></apply><ci id="S4.E12.m1.1.1.3.3.2.3.cmml" xref="S4.E12.m1.1.1.3.3.2.3">𝑒</ci><ci id="S4.E12.m1.1.1.3.3.2.4.cmml" xref="S4.E12.m1.1.1.3.3.2.4">𝑐</ci><ci id="S4.E12.m1.1.1.3.3.2.5.cmml" xref="S4.E12.m1.1.1.3.3.2.5">𝑎</ci><ci id="S4.E12.m1.1.1.3.3.2.6.cmml" xref="S4.E12.m1.1.1.3.3.2.6">𝑙</ci><ci id="S4.E12.m1.1.1.3.3.2.7.cmml" xref="S4.E12.m1.1.1.3.3.2.7">𝑙</ci></apply><apply id="S4.E12.m1.1.1.3.3.3.cmml" xref="S4.E12.m1.1.1.3.3.3"><plus id="S4.E12.m1.1.1.3.3.3.1.cmml" xref="S4.E12.m1.1.1.3.3.3.1"></plus><apply id="S4.E12.m1.1.1.3.3.3.2.cmml" xref="S4.E12.m1.1.1.3.3.3.2"><times id="S4.E12.m1.1.1.3.3.3.2.1.cmml" xref="S4.E12.m1.1.1.3.3.3.2.1"></times><ci id="S4.E12.m1.1.1.3.3.3.2.2.cmml" xref="S4.E12.m1.1.1.3.3.3.2.2">𝑝</ci><ci id="S4.E12.m1.1.1.3.3.3.2.3.cmml" xref="S4.E12.m1.1.1.3.3.3.2.3">𝑟</ci><ci id="S4.E12.m1.1.1.3.3.3.2.4.cmml" xref="S4.E12.m1.1.1.3.3.3.2.4">𝑒</ci><ci id="S4.E12.m1.1.1.3.3.3.2.5.cmml" xref="S4.E12.m1.1.1.3.3.3.2.5">𝑐</ci><ci id="S4.E12.m1.1.1.3.3.3.2.6.cmml" xref="S4.E12.m1.1.1.3.3.3.2.6">𝑖</ci><ci id="S4.E12.m1.1.1.3.3.3.2.7.cmml" xref="S4.E12.m1.1.1.3.3.3.2.7">𝑠</ci><ci id="S4.E12.m1.1.1.3.3.3.2.8.cmml" xref="S4.E12.m1.1.1.3.3.3.2.8">𝑖</ci><ci id="S4.E12.m1.1.1.3.3.3.2.9.cmml" xref="S4.E12.m1.1.1.3.3.3.2.9">𝑜</ci><ci id="S4.E12.m1.1.1.3.3.3.2.10.cmml" xref="S4.E12.m1.1.1.3.3.3.2.10">𝑛</ci></apply><apply id="S4.E12.m1.1.1.3.3.3.3.cmml" xref="S4.E12.m1.1.1.3.3.3.3"><times id="S4.E12.m1.1.1.3.3.3.3.1.cmml" xref="S4.E12.m1.1.1.3.3.3.3.1"></times><ci id="S4.E12.m1.1.1.3.3.3.3.2.cmml" xref="S4.E12.m1.1.1.3.3.3.3.2">𝑟</ci><ci id="S4.E12.m1.1.1.3.3.3.3.3.cmml" xref="S4.E12.m1.1.1.3.3.3.3.3">𝑒</ci><ci id="S4.E12.m1.1.1.3.3.3.3.4.cmml" xref="S4.E12.m1.1.1.3.3.3.3.4">𝑐</ci><ci id="S4.E12.m1.1.1.3.3.3.3.5.cmml" xref="S4.E12.m1.1.1.3.3.3.3.5">𝑎</ci><ci id="S4.E12.m1.1.1.3.3.3.3.6.cmml" xref="S4.E12.m1.1.1.3.3.3.3.6">𝑙</ci><ci id="S4.E12.m1.1.1.3.3.3.3.7.cmml" xref="S4.E12.m1.1.1.3.3.3.3.7">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E12.m1.1c">F1=2\cdot\frac{precision\cdot recall}{precision+recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.2" class="ltx_p">where precision is the fraction of true positive predictions among all positive predictions, and recall is the fraction of true positive predictions among all actual positive instances.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Discussion</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Overview</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In this section, we present a comprehensive evaluation of our proposed audio-visual video classification models, comparing their performance with baseline approaches on the YouTube-8M dataset. We report both the Global Average Precision (GAP) and F1 scores for each model, providing a holistic view of their classification accuracy. Additionally, we conduct ablation studies to investigate the impact of various components and design choices on the performance of our models.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Quantitative Results</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the performance comparison of our proposed models with the baseline models on the YouTube-8M dataset, reporting both GAP and F1 scores. The results demonstrate that the attention-based models consistently outperform the baseline models, with the Self-Attended Cross-Modal FCRN Network achieving the highest GAP of 80.68%. However, the Attend-Fusion model achieves the F1 score of 75.64% with significantly fewer parameters (72M) compared to the best-performing baseline model, FC Late Fusion (341M parameters), which achieves an F1 score of 75.96%.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Among the baseline models, the FC Late Fusion achieves the best performance with a GAP of 80.87% and an F1 score of 75.96%. The FC Audio and FC Visual models, which rely on a single modality, perform significantly worse than the multimodal approaches, highlighting the importance of leveraging both audio and visual information for accurate video classification.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of Baseline Results, Introduced Models, and Ablation Results</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.4.1.1" class="ltx_tr">
<td id="S4.T1.4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S4.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T1.4.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S4.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">GAP%</span></td>
<td id="S4.T1.4.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S4.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">F1 Score%</span></td>
<td id="S4.T1.4.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S4.T1.4.1.1.4.1" class="ltx_text ltx_font_bold">Params(M)</span></td>
</tr>
<tr id="S4.T1.4.2.2" class="ltx_tr">
<td id="S4.T1.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.4.2.2.1.1" class="ltx_text ltx_font_bold">Baseline Models</span></td>
<td id="S4.T1.4.2.2.2" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T1.4.2.2.3" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T1.4.2.2.4" class="ltx_td ltx_nopad_l ltx_border_t"></td>
</tr>
<tr id="S4.T1.4.3.3" class="ltx_tr">
<td id="S4.T1.4.3.3.1" class="ltx_td ltx_align_left ltx_border_t">FC Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></td>
<td id="S4.T1.4.3.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">50.32</td>
<td id="S4.T1.4.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">49.09</td>
<td id="S4.T1.4.3.3.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">103</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<td id="S4.T1.4.4.4.1" class="ltx_td ltx_align_left">FC Visual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></td>
<td id="S4.T1.4.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center">76.25</td>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_nopad_l ltx_align_center">72.69</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center">110</td>
</tr>
<tr id="S4.T1.4.5.5" class="ltx_tr">
<td id="S4.T1.4.5.5.1" class="ltx_td ltx_align_left">FC Early Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></td>
<td id="S4.T1.4.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center">80.16</td>
<td id="S4.T1.4.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center">75.25</td>
<td id="S4.T1.4.5.5.4" class="ltx_td ltx_nopad_l ltx_align_center">111</td>
</tr>
<tr id="S4.T1.4.6.6" class="ltx_tr">
<td id="S4.T1.4.6.6.1" class="ltx_td ltx_align_left">
<span id="S4.T1.4.6.6.1.1" class="ltx_text ltx_font_bold">FC Late Fusion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></td>
<td id="S4.T1.4.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.4.6.6.2.1" class="ltx_text ltx_font_bold">80.87</span></td>
<td id="S4.T1.4.6.6.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.4.6.6.3.1" class="ltx_text ltx_font_bold">75.96</span></td>
<td id="S4.T1.4.6.6.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.4.6.6.4.1" class="ltx_text ltx_font_bold">341</span></td>
</tr>
<tr id="S4.T1.4.7.7" class="ltx_tr">
<td id="S4.T1.4.7.7.1" class="ltx_td ltx_align_left">FC Residual Early Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></td>
<td id="S4.T1.4.7.7.2" class="ltx_td ltx_nopad_l ltx_align_center">80.73</td>
<td id="S4.T1.4.7.7.3" class="ltx_td ltx_nopad_l ltx_align_center">75.79</td>
<td id="S4.T1.4.7.7.4" class="ltx_td ltx_nopad_l ltx_align_center">175</td>
</tr>
<tr id="S4.T1.4.8.8" class="ltx_tr">
<td id="S4.T1.4.8.8.1" class="ltx_td ltx_align_left">FC Residual Late Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></td>
<td id="S4.T1.4.8.8.2" class="ltx_td ltx_nopad_l ltx_align_center">79.10</td>
<td id="S4.T1.4.8.8.3" class="ltx_td ltx_nopad_l ltx_align_center">74.37</td>
<td id="S4.T1.4.8.8.4" class="ltx_td ltx_nopad_l ltx_align_center">341</td>
</tr>
<tr id="S4.T1.4.9.9" class="ltx_tr">
<td id="S4.T1.4.9.9.1" class="ltx_td ltx_align_left">FC Residual Gated Early Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></td>
<td id="S4.T1.4.9.9.2" class="ltx_td ltx_nopad_l ltx_align_center">79.75</td>
<td id="S4.T1.4.9.9.3" class="ltx_td ltx_nopad_l ltx_align_center">74.20</td>
<td id="S4.T1.4.9.9.4" class="ltx_td ltx_nopad_l ltx_align_center">175</td>
</tr>
<tr id="S4.T1.4.10.10" class="ltx_tr">
<td id="S4.T1.4.10.10.1" class="ltx_td ltx_align_left">FC Residual Gated Late Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></td>
<td id="S4.T1.4.10.10.2" class="ltx_td ltx_nopad_l ltx_align_center">79.20</td>
<td id="S4.T1.4.10.10.3" class="ltx_td ltx_nopad_l ltx_align_center">74.24</td>
<td id="S4.T1.4.10.10.4" class="ltx_td ltx_nopad_l ltx_align_center">416</td>
</tr>
<tr id="S4.T1.4.11.11" class="ltx_tr">
<td id="S4.T1.4.11.11.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.4.11.11.1.1" class="ltx_text ltx_font_bold">Our Models</span></td>
<td id="S4.T1.4.11.11.2" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T1.4.11.11.3" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T1.4.11.11.4" class="ltx_td ltx_nopad_l ltx_border_t"></td>
</tr>
<tr id="S4.T1.4.12.12" class="ltx_tr">
<td id="S4.T1.4.12.12.1" class="ltx_td ltx_align_left ltx_border_t">FC Attention</td>
<td id="S4.T1.4.12.12.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">79.76</td>
<td id="S4.T1.4.12.12.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">75.09</td>
<td id="S4.T1.4.12.12.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">83</td>
</tr>
<tr id="S4.T1.4.13.13" class="ltx_tr">
<td id="S4.T1.4.13.13.1" class="ltx_td ltx_align_left">Residual Attention Early Fusion</td>
<td id="S4.T1.4.13.13.2" class="ltx_td ltx_nopad_l ltx_align_center">80.53</td>
<td id="S4.T1.4.13.13.3" class="ltx_td ltx_nopad_l ltx_align_center">75.34</td>
<td id="S4.T1.4.13.13.4" class="ltx_td ltx_nopad_l ltx_align_center">79</td>
</tr>
<tr id="S4.T1.4.14.14" class="ltx_tr">
<td id="S4.T1.4.14.14.1" class="ltx_td ltx_align_left">Residual Attention Late Fusion</td>
<td id="S4.T1.4.14.14.2" class="ltx_td ltx_nopad_l ltx_align_center">80.59</td>
<td id="S4.T1.4.14.14.3" class="ltx_td ltx_nopad_l ltx_align_center">75.56</td>
<td id="S4.T1.4.14.14.4" class="ltx_td ltx_nopad_l ltx_align_center">83</td>
</tr>
<tr id="S4.T1.4.15.15" class="ltx_tr">
<td id="S4.T1.4.15.15.1" class="ltx_td ltx_align_left">
<span id="S4.T1.4.15.15.1.1" class="ltx_text ltx_font_bold">Attend-Fusion</span></td>
<td id="S4.T1.4.15.15.2" class="ltx_td ltx_nopad_l ltx_align_center">80.55</td>
<td id="S4.T1.4.15.15.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.4.15.15.3.1" class="ltx_text ltx_font_bold">75.64</span></td>
<td id="S4.T1.4.15.15.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T1.4.15.15.4.1" class="ltx_text ltx_font_bold">72</span></td>
</tr>
<tr id="S4.T1.4.16.16" class="ltx_tr">
<td id="S4.T1.4.16.16.1" class="ltx_td ltx_align_left">Audio-Visual Attention Network</td>
<td id="S4.T1.4.16.16.2" class="ltx_td ltx_nopad_l ltx_align_center">80.44</td>
<td id="S4.T1.4.16.16.3" class="ltx_td ltx_nopad_l ltx_align_center">75.56</td>
<td id="S4.T1.4.16.16.4" class="ltx_td ltx_nopad_l ltx_align_center">113</td>
</tr>
<tr id="S4.T1.4.17.17" class="ltx_tr">
<td id="S4.T1.4.17.17.1" class="ltx_td ltx_align_left">Self and Cross Modal Attention Network</td>
<td id="S4.T1.4.17.17.2" class="ltx_td ltx_nopad_l ltx_align_center">80.61</td>
<td id="S4.T1.4.17.17.3" class="ltx_td ltx_nopad_l ltx_align_center">75.52</td>
<td id="S4.T1.4.17.17.4" class="ltx_td ltx_nopad_l ltx_align_center">172</td>
</tr>
<tr id="S4.T1.4.18.18" class="ltx_tr">
<td id="S4.T1.4.18.18.1" class="ltx_td ltx_align_left ltx_border_bb">Self-Attended Cross-Modal FCRN Network</td>
<td id="S4.T1.4.18.18.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T1.4.18.18.2.1" class="ltx_text ltx_font_bold">80.68</span></td>
<td id="S4.T1.4.18.18.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">75.45</td>
<td id="S4.T1.4.18.18.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">172</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">Our proposed attention-based models demonstrate superior performance compared to baselines. The Self-Attended Cross-Modal FCRN Network achieves the highest GAP of 80.68%, while the Attend-Fusion model achieves the best F1 score of 75.64% with a much smaller model size (72M parameters). These results show the effectiveness of attention mechanisms in capturing relevant features and dependencies within and between modalities.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2408.14441/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Qualitative results comparing the top-3 predictions of our proposed Attend-Fusion model, the state-of-the-art (SOTA) baseline, and the ground truth labels on representative examples from the YouTube-8M dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Qualitative Analysis</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">To gain further insights into the performance of our proposed Attend-Fusion model, we conduct a qualitative analysis by examining its predictions on a set of representative examples from the YouTube-8M dataset. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents a comparison of the top-3 predictions made by our Attend-Fusion model, the state-of-the-art (SOTA) baseline, and the ground-truth labels for six different videos.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">In Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a), both Attend-Fusion and the SOTA baseline correctly predict all three labels: "Fifa Street", "Games", and "Football". This shows their ability to accurately classify sports-related video content.
The Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(b) shows a video related to a water well and pump. Attend-Fusion correctly predicts all three labels, including "Concrete", "Water well", and "Pump". In contrast, the SOTA baseline misses the "Pump" label and predicts "Water" instead, highlighting the superior performance of our model in capturing fine-grained details.</p>
</div>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">In Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(c), a video is featuring a Segway PT vehicle and cycling. Both Attend-Fusion and the SOTA baseline accurately predict all three labels, showcasing their effectiveness in classifying transportation-related content. In Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(d), both models correctly predict the labels "Daenerys Targaryen", "Gabrielle" and "Music Video", demonstrating their ability to recognize characters and identify the video genre.
Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(e) presents a video from the game "Gears of War". While both models correctly predict the game title and the "Games" label, Attend-Fusion accurately classifies it as a "Video Game", whereas the SOTA baseline predicts "Gears of War" as the third label, which is redundant.</p>
</div>
<div id="S4.SS3.SSS3.p4" class="ltx_para">
<p id="S4.SS3.SSS3.p4.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Quantitative Results ‣ 4.3 Results and Discussion ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(f) shows a video related to a harmonica, nature, and an album. Attend-Fusion correctly predicts all three labels, showcasing its ability to identify musical instruments and themes. The SOTA baseline, however, misclassifies the "Album" label as "Guitar", indicating its limitations in distinguishing between different musical elements.
These qualitative examples demonstrate the superior performance of our Attend-Fusion model in accurately classifying videos across various domains, including sports, transportation, gaming, music, and more. The model’s ability to capture fine-grained details and maintain coherence in its predictions highlights the effectiveness of the attention mechanism in integrating audio and visual information for enhanced video classification.</p>
</div>
<div id="S4.SS3.SSS3.p5" class="ltx_para">
<p id="S4.SS3.SSS3.p5.1" class="ltx_p">The Attend-Fusion model strikes a balance between performance and efficiency, making it a promising choice for real-world applications. This efficiency is particularly valuable in scenarios where real-time video classification is needed or when deploying models on resource-constrained devices.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To further investigate the contributions of different components in our proposed models, we conduct ablation studies as shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the ablation results for the best-performing baseline model, FC Late Fusion. We evaluate the impact of using only the audio or visual modality. The results show that using only the audio modality (Audio Only) leads to a significant drop in performance, with a GAP of 50.32% and an F1 score of 49.09%. Similarly, using only the visual modality (Visual Only) also results in a performance decrease, with a GAP of 76.25% and an F1 score of 72.69%. These findings highlight the importance of multimodal fusion for achieving high classification accuracy.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Ablation Results: Best Baseline.</figcaption>
<table id="S4.T3.fig1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.fig1.3.1.1" class="ltx_tr">
<th id="S4.T3.fig1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S4.T3.fig1.3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">GAP</span></th>
<th id="S4.T3.fig1.3.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">F1</span></th>
<th id="S4.T3.fig1.3.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig1.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Params</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.fig1.3.2.1" class="ltx_tr">
<td id="S4.T3.fig1.3.2.1.1" class="ltx_td"></td>
<th id="S4.T3.fig1.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig1.3.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(%)</span></th>
<th id="S4.T3.fig1.3.2.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig1.3.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(%)</span></th>
<th id="S4.T3.fig1.3.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig1.3.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(M)</span></th>
</tr>
<tr id="S4.T3.fig1.3.3.2" class="ltx_tr">
<th id="S4.T3.fig1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig1.3.3.2.1.1" class="ltx_text" style="font-size:90%;">FC Late Fusion</span></th>
<th id="S4.T3.fig1.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig1.3.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">80.87</span></th>
<th id="S4.T3.fig1.3.3.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig1.3.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.96</span></th>
<th id="S4.T3.fig1.3.3.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig1.3.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">341</span></th>
</tr>
<tr id="S4.T3.fig1.3.4.3" class="ltx_tr">
<td id="S4.T3.fig1.3.4.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.fig1.3.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation</span></td>
<td id="S4.T3.fig1.3.4.3.2" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T3.fig1.3.4.3.3" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T3.fig1.3.4.3.4" class="ltx_td ltx_nopad_l ltx_border_t"></td>
</tr>
<tr id="S4.T3.fig1.3.5.4" class="ltx_tr">
<td id="S4.T3.fig1.3.5.4.1" class="ltx_td ltx_align_left"><span id="S4.T3.fig1.3.5.4.1.1" class="ltx_text" style="font-size:90%;">Audio Only</span></td>
<td id="S4.T3.fig1.3.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig1.3.5.4.2.1" class="ltx_text" style="font-size:90%;">50.32</span></td>
<td id="S4.T3.fig1.3.5.4.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig1.3.5.4.3.1" class="ltx_text" style="font-size:90%;">49.09</span></td>
<td id="S4.T3.fig1.3.5.4.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig1.3.5.4.4.1" class="ltx_text" style="font-size:90%;">103</span></td>
</tr>
<tr id="S4.T3.fig1.3.6.5" class="ltx_tr">
<td id="S4.T3.fig1.3.6.5.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.fig1.3.6.5.1.1" class="ltx_text" style="font-size:90%;">Visual Only</span></td>
<td id="S4.T3.fig1.3.6.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig1.3.6.5.2.1" class="ltx_text" style="font-size:90%;">76.25</span></td>
<td id="S4.T3.fig1.3.6.5.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig1.3.6.5.3.1" class="ltx_text" style="font-size:90%;">72.69</span></td>
<td id="S4.T3.fig1.3.6.5.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig1.3.6.5.4.1" class="ltx_text" style="font-size:90%;">110</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Ablation Results: Best Proposed Model.</figcaption>
<table id="S4.T3.fig2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.fig2.3.1.1" class="ltx_tr">
<th id="S4.T3.fig2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig2.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S4.T3.fig2.3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig2.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">GAP</span></th>
<th id="S4.T3.fig2.3.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig2.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">F1</span></th>
<th id="S4.T3.fig2.3.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.fig2.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Params</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.fig2.3.2.1" class="ltx_tr">
<td id="S4.T3.fig2.3.2.1.1" class="ltx_td"></td>
<th id="S4.T3.fig2.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig2.3.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(%)</span></th>
<th id="S4.T3.fig2.3.2.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig2.3.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(%)</span></th>
<th id="S4.T3.fig2.3.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.fig2.3.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">(M)</span></th>
</tr>
<tr id="S4.T3.fig2.3.3.2" class="ltx_tr">
<th id="S4.T3.fig2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig2.3.3.2.1.1" class="ltx_text" style="font-size:90%;">Attend-Fusion</span></th>
<th id="S4.T3.fig2.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig2.3.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">80.55</span></th>
<th id="S4.T3.fig2.3.3.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig2.3.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.64</span></th>
<th id="S4.T3.fig2.3.3.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.fig2.3.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">72</span></th>
</tr>
<tr id="S4.T3.fig2.3.4.3" class="ltx_tr">
<td id="S4.T3.fig2.3.4.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.fig2.3.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation</span></td>
<td id="S4.T3.fig2.3.4.3.2" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T3.fig2.3.4.3.3" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S4.T3.fig2.3.4.3.4" class="ltx_td ltx_nopad_l ltx_border_t"></td>
</tr>
<tr id="S4.T3.fig2.3.5.4" class="ltx_tr">
<td id="S4.T3.fig2.3.5.4.1" class="ltx_td ltx_align_left"><span id="S4.T3.fig2.3.5.4.1.1" class="ltx_text" style="font-size:90%;">Audio Only</span></td>
<td id="S4.T3.fig2.3.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.5.4.2.1" class="ltx_text" style="font-size:90%;">49.59</span></td>
<td id="S4.T3.fig2.3.5.4.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.5.4.3.1" class="ltx_text" style="font-size:90%;">49.15</span></td>
<td id="S4.T3.fig2.3.5.4.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.5.4.4.1" class="ltx_text" style="font-size:90%;">35</span></td>
</tr>
<tr id="S4.T3.fig2.3.6.5" class="ltx_tr">
<td id="S4.T3.fig2.3.6.5.1" class="ltx_td ltx_align_left"><span id="S4.T3.fig2.3.6.5.1.1" class="ltx_text" style="font-size:90%;">Visual Only</span></td>
<td id="S4.T3.fig2.3.6.5.2" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.6.5.2.1" class="ltx_text" style="font-size:90%;">76.96</span></td>
<td id="S4.T3.fig2.3.6.5.3" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.6.5.3.1" class="ltx_text" style="font-size:90%;">71.87</span></td>
<td id="S4.T3.fig2.3.6.5.4" class="ltx_td ltx_nopad_l ltx_align_center"><span id="S4.T3.fig2.3.6.5.4.1" class="ltx_text" style="font-size:90%;">37</span></td>
</tr>
<tr id="S4.T3.fig2.3.7.6" class="ltx_tr">
<td id="S4.T3.fig2.3.7.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.fig2.3.7.6.1.1" class="ltx_text" style="font-size:90%;">No Attention</span></td>
<td id="S4.T3.fig2.3.7.6.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig2.3.7.6.2.1" class="ltx_text" style="font-size:90%;">79.06</span></td>
<td id="S4.T3.fig2.3.7.6.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig2.3.7.6.3.1" class="ltx_text" style="font-size:90%;">73.31</span></td>
<td id="S4.T3.fig2.3.7.6.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb"><span id="S4.T3.fig2.3.7.6.4.1" class="ltx_text" style="font-size:90%;">22</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experimentation and Results ‣ Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the ablation results for our most efficient model, Attend-Fusion. We investigate the impact of removing the attention mechanism (No Attention) and using only the audio or visual modality (Audio Only and Visual Only). Removing the attention mechanism leads to a drop in performance, with a GAP of 79.06% and an F1 score of 73.31%. This demonstrates the effectiveness of attention in capturing relevant features and improving classification accuracy. Using only the audio or visual modality results in a significant performance decrease, confirming the importance of multimodal fusion in the Attend-Fusion architecture.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The experimental results demonstrate the superiority of our proposed attention-based models for audio-visual video classification. By effectively leveraging the complementary information from both modalities and selectively attending to relevant features, our models achieve state-of-the-art performance on the challenging YouTube-8M dataset.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The Attend-Fusion model stands out as a particularly promising approach, achieving competitive performance with a significantly smaller model size compared to the baselines. This efficiency makes it well-suited for real-world applications where computational resources are limited, such as mobile devices or edge computing scenarios. These findings can guide future research in designing more efficient and accurate audio-visual video classification models.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Overall, our work demonstrates the potential of attention-based models for tackling the challenging task of audio-visual video classification. By effectively leveraging the rich information present in both modalities and selectively attending to relevant features, our models push the boundaries of video understanding and pave the way for more advanced and efficient approaches in this field.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research was partly supported by the British Broadcasting Corporation Research and Development (BBC R&amp;D), Engineering and Physical Sciences Research Council (EPSRC) Grant EP/V038087/1 “BBC Prosperity Partnership: Future Personalised Object-Based Media Experiences Delivered at Scale Anywhere”.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abdullah, M., Ahmad, M., Han, D.: Facial expression recognition in videos: An
cnn-lstm based model for video classification. In: 2020 International
Conference on Electronics, Information, and Communication (ICEIC). pp. 1–3.
IEEE (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan,
B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification
benchmark. arXiv preprint arXiv:1609.08675 (2016)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan,
B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification
benchmark (2016)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Arandjelovic, R., Zisserman, A.: Look, listen and learn. In: Proceedings of the
IEEE international conference on computer vision. pp. 609–617 (2017)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Berghi, D., Cieciura, C., Einabadi, F., Glancy, M., Camilleri, O.C., Foster,
P., Nadeem, A., Sardari, F., Zhao, J., Volino, M., et al.: Forecasterflexobm:
A multi-view audio-visual dataset for flexible object-based media production

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bhaskar, S., Thasleema, T.: Lstm model for visual speech recognition through
facial expressions. Multimedia Tools and Applications <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">82</span>(4),
5455–5472 (2023)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Bober-Irizar, M., Husain, S., Ong, E.J., Bober, M.: Cultivating dnn diversity
for large scale video labelling. arXiv preprint arXiv:1707.04272 (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Boulahia, S.Y., Amamra, A., Madi, M.R., Daikh, S.: Early, intermediate and late
fusion strategies for robust deep learning-based multimodal action
recognition. Machine Vision and Applications <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">32</span>(6),  121 (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Brousmiche, M., Rouat, J., Dupont, S.: Multi-level attention fusion network for
audio-visual event recognition. arXiv preprint arXiv:2106.06736 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chen, L., Srivastava, S., Duan, Z., Xu, C.: Deep cross-modal audio-visual
generation. In: Proceedings of the on Thematic Workshops of ACM Multimedia
2017. pp. 349–357 (2017)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen, P., Liu, S., Zhao, H., Jia, J.: Distilling knowledge via knowledge
review. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 5008–5017 (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Cheng, Y., Wang, R., Pan, Z., Feng, R., Zhang, Y.: Look, listen, and attend:
Co-attention network for self-supervised audio-visual representation
learning. In: Proceedings of the 28th ACM International Conference on
Multimedia. pp. 3884–3892 (2020)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database pp. 248–255 (2009)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ding, Y., Xu, Y., Zhang, S.X., Cong, Y., Wang, L.: Self-supervised learning for
audio-visual speaker diarization. In: ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.
4367–4371. IEEE (2020)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Pinz, A., Wildes, R.P.: Spatiotemporal multiplier networks
for video action recognition. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4768–4777 (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video
retrieval. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part IV 16. pp. 214–229. Springer
(2020)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gao, J., Li, P., Chen, Z., Zhang, J.: A survey on deep learning for multimodal
data fusion. Neural Computation <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">32</span>(5), 829–864 (2020)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Gkalelis, N., Mezaris, V.: Subclass deep neural networks: re-enabling neglected
classes in deep network training for multimedia classification. In:
International Conference on Multimedia Modeling. pp. 227–238. Springer
(2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Gu, Y., Yang, K., Fu, S., Chen, S., Li, X., Marsic, I.: Hybrid attention based
multimodal network for spoken language classification. In: Proceedings of the
Conference. association for Computational Linguistics. meeting. vol. 2018,
p. 2379. NIH Public Access (2018)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Hao, Y., Wang, S., Cao, P., Gao, X., Xu, T., Wu, J., He, X.: Attention in
attention: Modeling context correlation for efficient video classification.
IEEE Transactions on Circuits and Systems for Video Technology
<span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">32</span>(10), 7120–7132 (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770–778 (2016)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore,
R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn
architectures for large-scale audio classification. In: 2017 ieee
international conference on acoustics, speech and signal processing (icassp).
pp. 131–135. IEEE (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hori, C., Hori, T., Lee, T.Y., Zhang, Z., Harsham, B., Hershey, J.R., Marks,
T.K., Sumi, K.: Attention-based multimodal fusion for video description. In:
Proceedings of the IEEE international conference on computer vision. pp.
4193–4202 (2017)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Iashin, V., Rahtu, E.: Multi-modal dense video captioning. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.
pp. 958–959 (2020)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Joze, H.R.V., Shaban, A., Iuzzolino, M.L., Koishida, K.: Mmtm: Multimodal
transfer module for cnn fusion. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.:
Large-scale video classification with convolutional neural networks. In:
Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition. pp. 1725–1732 (2014)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Khosravan, N., Ardeshir, S., Puri, R.: On attention modules for audio-visual
synchronization. In: CVPR Workshops. pp. 25–28 (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Korbar, B., Tran, D., Torresani, L.: Cooperative learning of audio and video
models from self-supervised synchronization. Advances in Neural Information
Processing Systems <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">31</span> (2018)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Lahat, D., Adali, T., Jutten, C.: Multimodal data fusion: an overview of
methods, challenges, and prospects. Proceedings of the IEEE <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">103</span>(9),
1449–1477 (2015)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Lee, J., Reade, W., Sukthankar, R., Toderici, G., et al.: The 2nd youtube-8m
large-scale video understanding challenge. In: Proceedings of the European
Conference on Computer Vision (ECCV) Workshops. pp. 0–0 (2018)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Li, X., Wu, H., Li, M., Liu, H.: Multi-label video classification via coupling
attentional multiple instance learning with label relation graph. Pattern
Recognition Letters <span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">156</span>, 53–59 (2022)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lin, Y.B., Wang, Y.C.F.: Audiovisual transformer with instance attention for
audio-visual event localization. In: Proceedings of the Asian Conference on
Computer Vision (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv
preprint arXiv:1711.05101 (2017)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lv, F., Chen, X., Huang, Y., Duan, L., Lin, G.: Progressive modality
reinforcement for human multimodal emotion recognition from unaligned
multimodal sequences. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 2554–2562 (2021)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mercea, O.B., Hummel, T., Koepke, A.S., Akata, Z.: Temporal and cross-modal
attention for audio-visual zero-shot learning. In: European Conference on
Computer Vision. pp. 488–505. Springer (2022)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Mercea, O.B., Riesch, L., Koepke, A., Akata, Z.: Audio-visual generalised
zero-shot learning with cross-modal attention and language. In: Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition. pp.
10553–10563 (2022)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Miech, A., Laptev, I., Sivic, J.: Learnable pooling with context gating for
video classification. arXiv preprint arXiv:1706.06905 (2017)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Morgado, P., Li, Y., Nvasconcelos, N.: Learning representations from
audio-visual spatial alignment. Advances in Neural Information Processing
Systems <span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">33</span>, 4733–4744 (2020)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Morgado, P., Vasconcelos, N., Misra, I.: Audio-visual instance discrimination
with cross-modal agreement. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 12475–12486 (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nadeem, A., Hilton, A., Dawes, R., Thomas, G., Mustafa, A.: Sem-pos:
Grammatically and semantically correct video captioning. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2605–2615 (2023)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Nadeem, A., Hilton, A., Dawes, R., Thomas, G., Mustafa, A.: Cad-contextual
multi-modal alignment for dynamic avqa. In: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision. pp. 7251–7263 (2024)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Nadeem, A., Sardari, F., Dawes, R., Husain, S.S., Hilton, A., Mustafa, A.:
Narrativebridge: Enhancing video captioning with causal-temporal narrative.
arXiv preprint arXiv:2406.06499 (2024)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., Sun, C.: Attention
bottlenecks for multimodal fusion. Advances in neural information processing
systems <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">34</span>, 14200–14213 (2021)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Okazaki, S., Kong, Q., Yoshinaga, T.: A multi-modal fusion approach for
audio-visual scene classification enhanced by clip variants. In: DCASE. pp.
95–99 (2021)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ong, E.J., Husain, S.S., Bober-Irizar, M., Bober, M.: Deep architectures and
ensembles for semantic video classification. IEEE Transactions on Circuits
and Systems for Video Technology <span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">29</span>(12), 3568–3582 (2018)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Planamente, M., Plizzari, C., Alberti, E., Caputo, B.: Domain generalization
through audio-visual relative norm alignment in first person action
recognition. In: Proceedings of the IEEE/CVF winter conference on
applications of computer vision. pp. 1807–1818 (2022)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Poppe, R.: A survey on vision-based human action recognition. Image and vision
computing <span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">28</span>(6), 976–990 (2010)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Powers, D.M.: Evaluation: from precision, recall and f-measure to roc,
informedness, markedness and correlation. arXiv preprint arXiv:2010.16061
(2020)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Praveen, R.G., Granger, E., Cardinal, P.: Recursive joint attention for
audio-visual fusion in regression based emotion recognition. In: ICASSP
2023-2023 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). pp. 1–5. IEEE (2023)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Praveen, R.G., de Melo, W.C., Ullah, N., Aslam, H., Zeeshan, O., Denorme, T.,
Pedersoli, M., Koerich, A.L., Bacon, S., Cardinal, P., et al.: A joint
cross-attention model for audio-visual fusion in dimensional emotion
recognition. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 2486–2495 (2022)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Reddy, D.R.: Speech recognition by machine: A review. Proceedings of the IEEE
<span id="bib.bib51.1.1" class="ltx_text ltx_font_bold">64</span>(4), 501–531 (1976)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Rehman, A., Belhaouari, S.B.: Deep learning for video classification: A review
(2021)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Shah, A., Geng, S., Gao, P., Cherian, A., Hori, T., Marks, T.K., Le Roux, J.,
Hori, C.: Audio-visual scene-aware dialog and reasoning using audio-visual
transformers with joint student-teacher learning. In: ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP).
pp. 7732–7736. IEEE (2022)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Sharafi, M., Yazdchi, M., Rasti, J.: Audio-visual emotion recognition using
k-means clustering and spatio-temporal cnn. In: 2023 6th International
Conference on Pattern Recognition and Image Analysis (IPRIA). pp. 1–6. IEEE
(2023)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action
recognition in videos. Advances in neural information processing systems
<span id="bib.bib55.1.1" class="ltx_text ltx_font_bold">27</span> (2014)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Song, Q., Sun, B., Li, S.: Multimodal sparse transformer network for
audio-visual speech recognition. IEEE Transactions on Neural Networks and
Learning Systems (2022)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions
classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overfitting. The
journal of machine learning research <span id="bib.bib58.1.1" class="ltx_text ltx_font_bold">15</span>(1), 1929–1958 (2014)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu, J.: Human action
recognition from various data modalities: A review. IEEE transactions on
pattern analysis and machine intelligence (2022)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Sundar, A., Heck, L.: Multimodal conversational ai: A survey of datasets and
approaches. arXiv preprint arXiv:2205.06907 (2022)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov,
R.: Multimodal transformer for unaligned multimodal language sequences. In:
Proceedings of the conference. Association for computational linguistics.
Meeting. vol. 2019, p. 6558. NIH Public Access (2019)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Tschannen, M., Djolonga, J., Ritter, M., Mahendran, A., Houlsby, N., Gelly, S.,
Lucic, M.: Self-supervised learning of video-induced visual invariances. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 13806–13815 (2020)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural
information processing systems <span id="bib.bib63.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Vilaça, L., Yu, Y., Viana, P.: Recent advances and challenges in deep
audio-visual correlation learning (2022)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Wang, G., Chen, C., Fan, D.P., Hao, A., Qin, H.: From semantic categories to
fixations: A novel weakly-supervised visual-auditory saliency detection
approach. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 15119–15128 (2021)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Wang, H.D., Zhang, T., Wu, J.: The monkeytyping solution to the youtube-8m
video understanding challenge. arXiv preprint arXiv:1706.05150 (2017)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Wei, Y., Hu, D., Tian, Y., Li, X.: Learning in audio-visual context: A review,
analysis, and new perspective (2022)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Wu, Y., Zhu, L., Yan, Y., Yang, Y.: Dual attention matching for audio-visual
event localization. In: Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) (October 2019)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Yu, J., Cheng, Y., Zhao, R.W., Feng, R., Zhang, Y.: Mm-pyramid: Multimodal
pyramid attentional network for audio-visual event localization and video
parsing. In: Proceedings of the 30th ACM International Conference on
Multimedia. pp. 6241–6249 (2022)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zhang, J., Yu, Y., Tang, S., Wu, J., Li, W.: Variational autoencoder with cca
for audio–visual cross-modal retrieval. ACM Transactions on Multimedia
Computing, Communications and Applications <span id="bib.bib70.1.1" class="ltx_text ltx_font_bold">19</span>(3s), 1–21 (2023)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhang, S., Zhang, S., Huang, T., Gao, W., Tian, Q.: Learning affective features
with a hybrid deep model for audio–visual emotion recognition. IEEE
Transactions on Circuits and Systems for Video Technology <span id="bib.bib71.1.1" class="ltx_text ltx_font_bold">28</span>(10),
3030–3043 (2017)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Zhang, Z., An, L., Cui, Z., Dong, T., et al.: Facial affect recognition based
on transformer encoder and audiovisual fusion for the abaw5 challenge. arXiv
preprint arXiv:2303.09158 (2023)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Zhu, H., Luo, M., Wang, R., Zheng, A., He, R.: Deep audio-visual learning: A
survey (2020)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.14440" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.14441" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.14441">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.14441" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.14442" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 14:44:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
