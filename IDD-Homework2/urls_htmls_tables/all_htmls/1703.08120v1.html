<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1703.08120] Recurrent and Contextual Models for Visual Question Answering</title><meta property="og:description" content="We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we exp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Recurrent and Contextual Models for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Recurrent and Contextual Models for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1703.08120">

<!--Generated on Mon Feb 26 19:25:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Recurrent and Contextual Models for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Abhijit Sharang
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter">abhisg@stanford.edu</span>
<span id="id5.2.id2" class="ltx_ERROR undefined">\And</span>Eric Lau
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id6.3.id3" class="ltx_text ltx_font_typewriter">eclau@stanford.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.3" class="ltx_p">We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to <math id="id1.1.m1.1" class="ltx_Math" alttext="64.6\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">64.6</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="float" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">64.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">64.6\%</annotation></semantics></math>, while the ensemble of all models achieves the best accuracy of <math id="id2.2.m2.1" class="ltx_Math" alttext="66.67\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">66.67</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn type="float" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">66.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">66.67\%</annotation></semantics></math>, within <math id="id3.3.m3.1" class="ltx_Math" alttext="0.5\%" display="inline"><semantics id="id3.3.m3.1a"><mrow id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mn id="id3.3.m3.1.1.2" xref="id3.3.m3.1.1.2.cmml">0.5</mn><mo id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><csymbol cd="latexml" id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">percent</csymbol><cn type="float" id="id3.3.m3.1.1.2.cmml" xref="id3.3.m3.1.1.2">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">0.5\%</annotation></semantics></math> of the current state-of-the-art for Visual7W.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Question answering is regarded as a complex task in natural language processing and artificial intelligence in general. A high-performance QA system should demonstrate a wide range of capabilities, such as semantic reasoning, sentiment analysis, and contextual inference of language. Visual question-answering (vQA) is an extension of text-based QA which requires understanding of both images and questions about them. Common methods involve using convolutional and recurrent neural networks to map image/text pairs to some vector space which is representational of the interaction of the image with the text. Several compositional models for combining these multimodal representations have been explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In the literature, there exists two divergent trends in state-of-the-art vQA systems. One is toward increasingly complex recurrent models, often with complex attention mechanisms over the images and text. In these models, the textual content often governs which areas of the image are more important for reasoning about the correct answer.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The other trend is towards much more basic models involving simple features like bag-of-words and models such as the multilayer perceptron (MLP) which achieve performance on current vQA datasets comparable to, and often in excess of, their more complex counterparts. These studies suggest bias in current visual question answering datasets which allows models to “guess” the answer without having to develop a deeper understanding of the question or image context.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In this work, we develop and evaluate recurrent models to systematically combine these image and text features together in incrementally more expressive ways while maintaining model simplicity for better generalization, on the Visual7W dataset. Specifically, we investigate novel multiple-choice answering variants of a basic existing recurrent visual-QA model involving simple concatenation of LSTM-encoded visual and text features that is fed to a MLP; separate LSTM encodings of question and image features into a context vector to augment the LSTM-encoding of the related answer choice; attention over the question and image; and the overall ensemble of all our models. We evaluate these against a simple baseline in which bag-of-words features for the question and answers are concatenated with the image representation and fed to a MLP, which is inspired by the current state-of-the-art in the literature. In the course of our study, we hope to better characterize the potential dataset biases mentioned previously for Visual7W.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Standard and Visual Question Answering</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Though textual question answering is a well-established standard task in natural language processing, relatively recent improvements in recurrent neural network (RNN) models, as well as convolutional neural network (CNN) models for image recognition, have been effectively applied in combination to vQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Several vQA datasets have been released, such as VQA 1.0, Visual Genome, and Visual7W. However, recent studies have shown that some datasets, such as VQA 1.0, exhibit strong language bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and surprisingly few questions require non-trivial reasoning and abstraction to arrive at the answer. Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> consists of 327,939 QA pairs and claims to have greater question-answer complexity and diversity to more rigorously evaluate vQA models. VQA 2.0 is under development and purports to have improvements in the same vein.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Trend 1: Toward Complex Neural Architectures</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">A predictable trend of recent vQA systems in the literature has been towards more complex neural architectures often involving recurrent models and attention mechanisms. For example, Shih et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> attempted to map both the question and answer text and images into a latent space where inner products of textual features yielded attention regions for the associated image. In addition, Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> attempt to create a co-attention model that performs joint reasoning over the question and image.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Trend 2: Toward Simple Baselines</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Recent work on very simple baselines has resulted in basic systems with performance comparable to or exceeding that of more complex recurrent systems on VQA 1.0 and Visual7W. For instance, Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> demonstrated a simple non-recurrent model where the input question is transformed into a word feature using naive bag-of-words, simply concatenated with deep image features obtained using GoogLeNet, and fed to a softmax layer to predict the answer class.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.2" class="ltx_p">Similarly, Jabri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> created a system that instead took the full question-image-answer triplet <math id="S2.SS3.p2.1.m1.3" class="ltx_Math" alttext="(q,a,I)" display="inline"><semantics id="S2.SS3.p2.1.m1.3a"><mrow id="S2.SS3.p2.1.m1.3.4.2" xref="S2.SS3.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S2.SS3.p2.1.m1.3.4.2.1" xref="S2.SS3.p2.1.m1.3.4.1.cmml">(</mo><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">q</mi><mo id="S2.SS3.p2.1.m1.3.4.2.2" xref="S2.SS3.p2.1.m1.3.4.1.cmml">,</mo><mi id="S2.SS3.p2.1.m1.2.2" xref="S2.SS3.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS3.p2.1.m1.3.4.2.3" xref="S2.SS3.p2.1.m1.3.4.1.cmml">,</mo><mi id="S2.SS3.p2.1.m1.3.3" xref="S2.SS3.p2.1.m1.3.3.cmml">I</mi><mo stretchy="false" id="S2.SS3.p2.1.m1.3.4.2.4" xref="S2.SS3.p2.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.3b"><vector id="S2.SS3.p2.1.m1.3.4.1.cmml" xref="S2.SS3.p2.1.m1.3.4.2"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">𝑞</ci><ci id="S2.SS3.p2.1.m1.2.2.cmml" xref="S2.SS3.p2.1.m1.2.2">𝑎</ci><ci id="S2.SS3.p2.1.m1.3.3.cmml" xref="S2.SS3.p2.1.m1.3.3">𝐼</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.3c">(q,a,I)</annotation></semantics></math> as the sample input. In their system, 300-dimensional bag-of-words word2vec features for the question and multiple choice answer are concatenated with 2048-dimensional deep image features from Resnet-101 and are fed to a MLP with 8192 hidden units. Their model achieves an accuracy of <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="67.1\%" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><mrow id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml"><mn id="S2.SS3.p2.2.m2.1.1.2" xref="S2.SS3.p2.2.m2.1.1.2.cmml">67.1</mn><mo id="S2.SS3.p2.2.m2.1.1.1" xref="S2.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><apply id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S2.SS3.p2.2.m2.1.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.SS3.p2.2.m2.1.1.2.cmml" xref="S2.SS3.p2.2.m2.1.1.2">67.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">67.1\%</annotation></semantics></math>, the current state-of-the-art on Visual7W.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.5" class="ltx_p">We explore model architectures of incrementally increasing complexity, starting with simple non-recurrent baselines and then incrementally more complex recurrent, contextual, and attentional models which combine some or all of the <math id="S3.p1.1.m1.3" class="ltx_Math" alttext="(q,a,I)" display="inline"><semantics id="S3.p1.1.m1.3a"><mrow id="S3.p1.1.m1.3.4.2" xref="S3.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.p1.1.m1.3.4.2.1" xref="S3.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">q</mi><mo id="S3.p1.1.m1.3.4.2.2" xref="S3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">a</mi><mo id="S3.p1.1.m1.3.4.2.3" xref="S3.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.p1.1.m1.3.3" xref="S3.p1.1.m1.3.3.cmml">I</mi><mo stretchy="false" id="S3.p1.1.m1.3.4.2.4" xref="S3.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.3b"><vector id="S3.p1.1.m1.3.4.1.cmml" xref="S3.p1.1.m1.3.4.2"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑞</ci><ci id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">𝑎</ci><ci id="S3.p1.1.m1.3.3.cmml" xref="S3.p1.1.m1.3.3">𝐼</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.3c">(q,a,I)</annotation></semantics></math> triplet for a given question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">q</annotation></semantics></math>, answer choice <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">a</annotation></semantics></math> (of which there are four per <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">q</annotation></semantics></math>), and CNN image features <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">I</annotation></semantics></math>, into new intermediate neural feature representations. A diagram of our models is shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Approach ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For our individual experimental models, we describe relevant modifications leading to the generation of the feature vector only; the rest of the model can be assumed to be the same, minus minor hyperparameter adjustments elsewhere.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1703.08120/assets/model_diagram.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="588" height="388" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Experimental models. In [2] and [3], we replace the dotted portion of [1] with minor adjustments elsewhere. For each LSTM, we use the last hidden state as input to the next layer.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baseline: Simple Non-Recurrent Bag-Of-Words</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.4" class="ltx_p">The simplest model for the task is an adoption of the bag-of-words (BOW) model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. QA text is mapped to word embeddings and images are mapped to representations obtained from a pre-trained CNN after removing the last dense layer. We use <span id="S3.SS1.p1.4.1" class="ltx_text ltx_font_italic">Glove-300</span> for word embeddings and <span id="S3.SS1.p1.4.2" class="ltx_text ltx_font_italic">ResNet-50</span> for image embeddings. Each word representation is 300-dimensional, and each image representation is <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="7\times 7\times 2048" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1a" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">7</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">7</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">7\times 7\times 2048</annotation></semantics></math>-dimensional. Subsequently, we obtain the BOW representations of the text sequences by averaging the word embeddings over all words in the sequence and the bag-of-images representation by averaging over each <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">7</cn><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">7\times 7</annotation></semantics></math> stack across <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">2048</annotation></semantics></math> stacks. These are concatenated together and used as input to a MLP consisting of a fully connected hidden layer and output layer which produces a single score for each <math id="S3.SS1.p1.4.m4.3" class="ltx_Math" alttext="(q,a,I)" display="inline"><semantics id="S3.SS1.p1.4.m4.3a"><mrow id="S3.SS1.p1.4.m4.3.4.2" xref="S3.SS1.p1.4.m4.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.3.4.2.1" xref="S3.SS1.p1.4.m4.3.4.1.cmml">(</mo><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">q</mi><mo id="S3.SS1.p1.4.m4.3.4.2.2" xref="S3.SS1.p1.4.m4.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">a</mi><mo id="S3.SS1.p1.4.m4.3.4.2.3" xref="S3.SS1.p1.4.m4.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.3.3" xref="S3.SS1.p1.4.m4.3.3.cmml">I</mi><mo stretchy="false" id="S3.SS1.p1.4.m4.3.4.2.4" xref="S3.SS1.p1.4.m4.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.3b"><vector id="S3.SS1.p1.4.m4.3.4.1.cmml" xref="S3.SS1.p1.4.m4.3.4.2"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑞</ci><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">𝑎</ci><ci id="S3.SS1.p1.4.m4.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3">𝐼</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.3c">(q,a,I)</annotation></semantics></math> triplet. These scores are then concatenated as a vector of size 4 and normalized using softmax. Categorical cross-entropy is used as the loss function for training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Version [1]: LSTM-Encoding of Question and Image Text</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Version [1] creates recurrent representations of the question and answer sequences. The question and answer word embedding sequences are each passed through their own LSTMs and the last hidden state of each is the vector representation of each sequence. However, we also intend to remain as agnostic to the order of words as possible. For instance “a cute dog” and “a dog which is cute” should have close mappings in the transformed space. Thus, we use bidirectional LSTMs where the sequences are read forward and backward and the concatenation serves as the representation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Version [2]: Augmenting With Context From Question and Image</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">The baseline and V[1] have minimal interaction between the question image and text and hence may not capture deeper semantic relationships required for proper reasoning. We want a representation where the image embedding affects how the model represents the text and vice-versa. We define the notion of <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">context</em>, which is captured by mapping the image and the text to a joint space.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.7" class="ltx_p">To obtain the context vector <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">C</annotation></semantics></math>, we transform the question sequence to a sequence {<math id="S3.SS3.p2.2.m2.2" class="ltx_Math" alttext="(q_{w},I)" display="inline"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.2.1" xref="S3.SS3.p2.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.1.2" xref="S3.SS3.p2.2.m2.2.2.2.cmml">(</mo><msub id="S3.SS3.p2.2.m2.2.2.1.1" xref="S3.SS3.p2.2.m2.2.2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.2.2.1.1.2" xref="S3.SS3.p2.2.m2.2.2.1.1.2.cmml">q</mi><mi id="S3.SS3.p2.2.m2.2.2.1.1.3" xref="S3.SS3.p2.2.m2.2.2.1.1.3.cmml">w</mi></msub><mo id="S3.SS3.p2.2.m2.2.2.1.3" xref="S3.SS3.p2.2.m2.2.2.2.cmml">,</mo><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">I</mi><mo stretchy="false" id="S3.SS3.p2.2.m2.2.2.1.4" xref="S3.SS3.p2.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><interval closure="open" id="S3.SS3.p2.2.m2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.1"><apply id="S3.SS3.p2.2.m2.2.2.1.1.cmml" xref="S3.SS3.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.2.2.1.1.2">𝑞</ci><ci id="S3.SS3.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.2.2.1.1.3">𝑤</ci></apply><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐼</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">(q_{w},I)</annotation></semantics></math>}, where <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="q_{w}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">q</mi><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝑞</ci><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">q_{w}</annotation></semantics></math> is the word embedding representation of a question word <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">w</annotation></semantics></math> and <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">I</annotation></semantics></math> is the bag-of-images representation as obtained above. The transformed sequence is passed through a bidirectional LSTM as before, and the final hidden state is used as <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mi id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><ci id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">C</annotation></semantics></math>. We experiment with three architectures which use <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mi id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">C</annotation></semantics></math> to obtain deeper interaction between the text and the image. These models are as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The most basic architecture obtains the representation from the bidirectional LSTM on the answer sequence and concatenates it with <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">C</annotation></semantics></math>. This vector is then used as input to the MLP.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.3" class="ltx_p">In the second variant, the input to the MLP consists of the concatenation of the bidirectional LSTM representation of the answer sequence, <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">C</annotation></semantics></math>, and a transformation of <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">I</annotation></semantics></math>. To obtain the image feature transformation, <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">I</annotation></semantics></math> is fed through a fully-connected dense layer with softmax activation, which reduces its dimension.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i3.p1.1" class="ltx_p">In the third variant, we augment the each answer word with <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">C</annotation></semantics></math> and pass it through its bidirectional LSTM. The augmented answer representation is concatenated with the LSTM-encoding of the question only, as in V[1], and is passed as the feature vector to the MLP.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Version [3]: Augmenting With Attention Over Images</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">While the context model captures some interaction between the image and the text, it exerts a uniform effect on our models. In reality, we might want the image representation to be influenced by the word representation at each timestep in the sequence. Hence, instead of simply averaging over the <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">7</cn><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">7\times 7</annotation></semantics></math> values in the 2048-dimensional stack for each word, we want the 2048-dimensional image representation to be dependent on the word with which it is interacting.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.10" class="ltx_p">To achieve this, we introduce an attention module. This module transforms the <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="7\times 7\times 2048" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mn id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.1.m1.1.1.1a" xref="S3.SS4.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p2.1.m1.1.1.4" xref="S3.SS4.p2.1.m1.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><times id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">7</cn><cn type="integer" id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">7</cn><cn type="integer" id="S3.SS4.p2.1.m1.1.1.4.cmml" xref="S3.SS4.p2.1.m1.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">7\times 7\times 2048</annotation></semantics></math>-dimensional image embedding to <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="1\times 2048" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mn id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><times id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">1</cn><cn type="integer" id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">1\times 2048</annotation></semantics></math>-dimensional embedding in the following manner. Suppose <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">w</annotation></semantics></math> is the embedding of the word under interaction and <math id="S3.SS4.p2.4.m4.1" class="ltx_math_unparsed" alttext="\{I_{1},..,I_{2048}\}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mrow id="S3.SS4.p2.4.m4.1b"><mo stretchy="false" id="S3.SS4.p2.4.m4.1.1">{</mo><msub id="S3.SS4.p2.4.m4.1.2"><mi id="S3.SS4.p2.4.m4.1.2.2">I</mi><mn id="S3.SS4.p2.4.m4.1.2.3">1</mn></msub><mo id="S3.SS4.p2.4.m4.1.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS4.p2.4.m4.1.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS4.p2.4.m4.1.5">.</mo><mo id="S3.SS4.p2.4.m4.1.6">,</mo><msub id="S3.SS4.p2.4.m4.1.7"><mi id="S3.SS4.p2.4.m4.1.7.2">I</mi><mn id="S3.SS4.p2.4.m4.1.7.3">2048</mn></msub><mo stretchy="false" id="S3.SS4.p2.4.m4.1.8">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\{I_{1},..,I_{2048}\}</annotation></semantics></math> is the raw image embedding. Then, <math id="S3.SS4.p2.5.m5.2" class="ltx_Math" alttext="A_{j}=MLP([I_{j},w])" display="inline"><semantics id="S3.SS4.p2.5.m5.2a"><mrow id="S3.SS4.p2.5.m5.2.2" xref="S3.SS4.p2.5.m5.2.2.cmml"><msub id="S3.SS4.p2.5.m5.2.2.3" xref="S3.SS4.p2.5.m5.2.2.3.cmml"><mi id="S3.SS4.p2.5.m5.2.2.3.2" xref="S3.SS4.p2.5.m5.2.2.3.2.cmml">A</mi><mi id="S3.SS4.p2.5.m5.2.2.3.3" xref="S3.SS4.p2.5.m5.2.2.3.3.cmml">j</mi></msub><mo id="S3.SS4.p2.5.m5.2.2.2" xref="S3.SS4.p2.5.m5.2.2.2.cmml">=</mo><mrow id="S3.SS4.p2.5.m5.2.2.1" xref="S3.SS4.p2.5.m5.2.2.1.cmml"><mi id="S3.SS4.p2.5.m5.2.2.1.3" xref="S3.SS4.p2.5.m5.2.2.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.5.m5.2.2.1.2" xref="S3.SS4.p2.5.m5.2.2.1.2.cmml">​</mo><mi id="S3.SS4.p2.5.m5.2.2.1.4" xref="S3.SS4.p2.5.m5.2.2.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.5.m5.2.2.1.2a" xref="S3.SS4.p2.5.m5.2.2.1.2.cmml">​</mo><mi id="S3.SS4.p2.5.m5.2.2.1.5" xref="S3.SS4.p2.5.m5.2.2.1.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.5.m5.2.2.1.2b" xref="S3.SS4.p2.5.m5.2.2.1.2.cmml">​</mo><mrow id="S3.SS4.p2.5.m5.2.2.1.1.1" xref="S3.SS4.p2.5.m5.2.2.1.cmml"><mo stretchy="false" id="S3.SS4.p2.5.m5.2.2.1.1.1.2" xref="S3.SS4.p2.5.m5.2.2.1.cmml">(</mo><mrow id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.2" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.2.cmml">[</mo><msub id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.2" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.3" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.3" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.2.cmml">,</mo><mi id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml">w</mi><mo stretchy="false" id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.4" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.2.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS4.p2.5.m5.2.2.1.1.1.3" xref="S3.SS4.p2.5.m5.2.2.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.2b"><apply id="S3.SS4.p2.5.m5.2.2.cmml" xref="S3.SS4.p2.5.m5.2.2"><eq id="S3.SS4.p2.5.m5.2.2.2.cmml" xref="S3.SS4.p2.5.m5.2.2.2"></eq><apply id="S3.SS4.p2.5.m5.2.2.3.cmml" xref="S3.SS4.p2.5.m5.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.2.2.3.1.cmml" xref="S3.SS4.p2.5.m5.2.2.3">subscript</csymbol><ci id="S3.SS4.p2.5.m5.2.2.3.2.cmml" xref="S3.SS4.p2.5.m5.2.2.3.2">𝐴</ci><ci id="S3.SS4.p2.5.m5.2.2.3.3.cmml" xref="S3.SS4.p2.5.m5.2.2.3.3">𝑗</ci></apply><apply id="S3.SS4.p2.5.m5.2.2.1.cmml" xref="S3.SS4.p2.5.m5.2.2.1"><times id="S3.SS4.p2.5.m5.2.2.1.2.cmml" xref="S3.SS4.p2.5.m5.2.2.1.2"></times><ci id="S3.SS4.p2.5.m5.2.2.1.3.cmml" xref="S3.SS4.p2.5.m5.2.2.1.3">𝑀</ci><ci id="S3.SS4.p2.5.m5.2.2.1.4.cmml" xref="S3.SS4.p2.5.m5.2.2.1.4">𝐿</ci><ci id="S3.SS4.p2.5.m5.2.2.1.5.cmml" xref="S3.SS4.p2.5.m5.2.2.1.5">𝑃</ci><interval closure="closed" id="S3.SS4.p2.5.m5.2.2.1.1.1.1.2.cmml" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1"><apply id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.5.m5.2.2.1.1.1.1.1.1.3">𝑗</ci></apply><ci id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">𝑤</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.2c">A_{j}=MLP([I_{j},w])</annotation></semantics></math>, where <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="MLP" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><mrow id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi id="S3.SS4.p2.6.m6.1.1.2" xref="S3.SS4.p2.6.m6.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.6.m6.1.1.1" xref="S3.SS4.p2.6.m6.1.1.1.cmml">​</mo><mi id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.6.m6.1.1.1a" xref="S3.SS4.p2.6.m6.1.1.1.cmml">​</mo><mi id="S3.SS4.p2.6.m6.1.1.4" xref="S3.SS4.p2.6.m6.1.1.4.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><times id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1.1"></times><ci id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2">𝑀</ci><ci id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">𝐿</ci><ci id="S3.SS4.p2.6.m6.1.1.4.cmml" xref="S3.SS4.p2.6.m6.1.1.4">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">MLP</annotation></semantics></math> consists of two fully connected dense layers with <span id="S3.SS4.p2.10.1" class="ltx_text ltx_font_italic">ReLu</span> and <span id="S3.SS4.p2.10.2" class="ltx_text ltx_font_italic">tanh</span> activations with the second layer outputting a scalar. Hence, for the text sequence <math id="S3.SS4.p2.7.m7.1" class="ltx_math_unparsed" alttext="[t_{1},..,t_{n}]" display="inline"><semantics id="S3.SS4.p2.7.m7.1a"><mrow id="S3.SS4.p2.7.m7.1b"><mo stretchy="false" id="S3.SS4.p2.7.m7.1.1">[</mo><msub id="S3.SS4.p2.7.m7.1.2"><mi id="S3.SS4.p2.7.m7.1.2.2">t</mi><mn id="S3.SS4.p2.7.m7.1.2.3">1</mn></msub><mo id="S3.SS4.p2.7.m7.1.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS4.p2.7.m7.1.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS4.p2.7.m7.1.5">.</mo><mo id="S3.SS4.p2.7.m7.1.6">,</mo><msub id="S3.SS4.p2.7.m7.1.7"><mi id="S3.SS4.p2.7.m7.1.7.2">t</mi><mi id="S3.SS4.p2.7.m7.1.7.3">n</mi></msub><mo stretchy="false" id="S3.SS4.p2.7.m7.1.8">]</mo></mrow><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m7.1c">[t_{1},..,t_{n}]</annotation></semantics></math>, we obtain a corresponding image attention sequence <math id="S3.SS4.p2.8.m8.1" class="ltx_math_unparsed" alttext="[I^{\prime}_{1},..,I^{\prime}_{n}]" display="inline"><semantics id="S3.SS4.p2.8.m8.1a"><mrow id="S3.SS4.p2.8.m8.1b"><mo stretchy="false" id="S3.SS4.p2.8.m8.1.1">[</mo><msubsup id="S3.SS4.p2.8.m8.1.2"><mi id="S3.SS4.p2.8.m8.1.2.2.2">I</mi><mn id="S3.SS4.p2.8.m8.1.2.3">1</mn><mo id="S3.SS4.p2.8.m8.1.2.2.3">′</mo></msubsup><mo id="S3.SS4.p2.8.m8.1.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS4.p2.8.m8.1.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS4.p2.8.m8.1.5">.</mo><mo id="S3.SS4.p2.8.m8.1.6">,</mo><msubsup id="S3.SS4.p2.8.m8.1.7"><mi id="S3.SS4.p2.8.m8.1.7.2.2">I</mi><mi id="S3.SS4.p2.8.m8.1.7.3">n</mi><mo id="S3.SS4.p2.8.m8.1.7.2.3">′</mo></msubsup><mo stretchy="false" id="S3.SS4.p2.8.m8.1.8">]</mo></mrow><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m8.1c">[I^{\prime}_{1},..,I^{\prime}_{n}]</annotation></semantics></math>, where each <math id="S3.SS4.p2.9.m9.1" class="ltx_Math" alttext="I^{\prime}" display="inline"><semantics id="S3.SS4.p2.9.m9.1a"><msup id="S3.SS4.p2.9.m9.1.1" xref="S3.SS4.p2.9.m9.1.1.cmml"><mi id="S3.SS4.p2.9.m9.1.1.2" xref="S3.SS4.p2.9.m9.1.1.2.cmml">I</mi><mo id="S3.SS4.p2.9.m9.1.1.3" xref="S3.SS4.p2.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m9.1b"><apply id="S3.SS4.p2.9.m9.1.1.cmml" xref="S3.SS4.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m9.1.1.1.cmml" xref="S3.SS4.p2.9.m9.1.1">superscript</csymbol><ci id="S3.SS4.p2.9.m9.1.1.2.cmml" xref="S3.SS4.p2.9.m9.1.1.2">𝐼</ci><ci id="S3.SS4.p2.9.m9.1.1.3.cmml" xref="S3.SS4.p2.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m9.1c">I^{\prime}</annotation></semantics></math> is a <math id="S3.SS4.p2.10.m10.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S3.SS4.p2.10.m10.1a"><mn id="S3.SS4.p2.10.m10.1.1" xref="S3.SS4.p2.10.m10.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.10.m10.1b"><cn type="integer" id="S3.SS4.p2.10.m10.1.1.cmml" xref="S3.SS4.p2.10.m10.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.10.m10.1c">2048</annotation></semantics></math>-dimensional attention representation as calculated above.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.2" class="ltx_p">We incorporate the attended images by using the sequence of {<math id="S3.SS4.p3.1.m1.2" class="ltx_Math" alttext="(q_{w},I^{\prime}_{w})" display="inline"><semantics id="S3.SS4.p3.1.m1.2a"><mrow id="S3.SS4.p3.1.m1.2.2.2" xref="S3.SS4.p3.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS4.p3.1.m1.2.2.2.3" xref="S3.SS4.p3.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS4.p3.1.m1.1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.1.1.2.cmml">q</mi><mi id="S3.SS4.p3.1.m1.1.1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.1.1.3.cmml">w</mi></msub><mo id="S3.SS4.p3.1.m1.2.2.2.4" xref="S3.SS4.p3.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS4.p3.1.m1.2.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS4.p3.1.m1.2.2.2.2.2.2" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2.cmml">I</mi><mi id="S3.SS4.p3.1.m1.2.2.2.2.3" xref="S3.SS4.p3.1.m1.2.2.2.2.3.cmml">w</mi><mo id="S3.SS4.p3.1.m1.2.2.2.2.2.3" xref="S3.SS4.p3.1.m1.2.2.2.2.2.3.cmml">′</mo></msubsup><mo stretchy="false" id="S3.SS4.p3.1.m1.2.2.2.5" xref="S3.SS4.p3.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.2b"><interval closure="open" id="S3.SS4.p3.1.m1.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2"><apply id="S3.SS4.p3.1.m1.1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.2">𝑞</ci><ci id="S3.SS4.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.1.1.3">𝑤</ci></apply><apply id="S3.SS4.p3.1.m1.2.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2">subscript</csymbol><apply id="S3.SS4.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2">superscript</csymbol><ci id="S3.SS4.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.2">𝐼</ci><ci id="S3.SS4.p3.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.2.3">′</ci></apply><ci id="S3.SS4.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS4.p3.1.m1.2.2.2.2.3">𝑤</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.2c">(q_{w},I^{\prime}_{w})</annotation></semantics></math>} instead of {<math id="S3.SS4.p3.2.m2.2" class="ltx_Math" alttext="(q_{w},I)" display="inline"><semantics id="S3.SS4.p3.2.m2.2a"><mrow id="S3.SS4.p3.2.m2.2.2.1" xref="S3.SS4.p3.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS4.p3.2.m2.2.2.1.2" xref="S3.SS4.p3.2.m2.2.2.2.cmml">(</mo><msub id="S3.SS4.p3.2.m2.2.2.1.1" xref="S3.SS4.p3.2.m2.2.2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.2.2.1.1.2" xref="S3.SS4.p3.2.m2.2.2.1.1.2.cmml">q</mi><mi id="S3.SS4.p3.2.m2.2.2.1.1.3" xref="S3.SS4.p3.2.m2.2.2.1.1.3.cmml">w</mi></msub><mo id="S3.SS4.p3.2.m2.2.2.1.3" xref="S3.SS4.p3.2.m2.2.2.2.cmml">,</mo><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">I</mi><mo stretchy="false" id="S3.SS4.p3.2.m2.2.2.1.4" xref="S3.SS4.p3.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.2b"><interval closure="open" id="S3.SS4.p3.2.m2.2.2.2.cmml" xref="S3.SS4.p3.2.m2.2.2.1"><apply id="S3.SS4.p3.2.m2.2.2.1.1.cmml" xref="S3.SS4.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS4.p3.2.m2.2.2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.2.2.1.1.2">𝑞</ci><ci id="S3.SS4.p3.2.m2.2.2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.2.2.1.1.3">𝑤</ci></apply><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐼</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.2c">(q_{w},I)</annotation></semantics></math>} as input to one LSTM. We concatenate its last hidden layer with the answer LSTM representation and the transformed image representation as in V[2] as the feature vector to the aforementioned MLP for score generation.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We use the <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">Telling</em> portion of the Visual7W dataset consisting of 69,817 training samples, 28,020 validation samples, and 42,031 test samples. Each sample consists of an image, a question string and four option strings out of which only one choice is correct. To prevent data contamination, each image occurs in exactly one partition of the dataset. The question types are broken down into <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">who, what, when, where, why</span>, and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">how</span>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.3" class="ltx_p">In each of the models described above, the architecture becomes the same after the generation of the feature vector. The final two layers consist of a fully connected hidden layer with a large number of hidden units and <span id="S4.p2.3.1" class="ltx_text ltx_font_italic">ReLU</span> activations and a fully connected output layer with a single unit with sigmoid activations. This copy of the network is run for <math id="S4.p2.1.m1.3" class="ltx_Math" alttext="(q,a,I)" display="inline"><semantics id="S4.p2.1.m1.3a"><mrow id="S4.p2.1.m1.3.4.2" xref="S4.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S4.p2.1.m1.3.4.2.1" xref="S4.p2.1.m1.3.4.1.cmml">(</mo><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">q</mi><mo id="S4.p2.1.m1.3.4.2.2" xref="S4.p2.1.m1.3.4.1.cmml">,</mo><mi id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">a</mi><mo id="S4.p2.1.m1.3.4.2.3" xref="S4.p2.1.m1.3.4.1.cmml">,</mo><mi id="S4.p2.1.m1.3.3" xref="S4.p2.1.m1.3.3.cmml">I</mi><mo stretchy="false" id="S4.p2.1.m1.3.4.2.4" xref="S4.p2.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.3b"><vector id="S4.p2.1.m1.3.4.1.cmml" xref="S4.p2.1.m1.3.4.2"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑞</ci><ci id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2">𝑎</ci><ci id="S4.p2.1.m1.3.3.cmml" xref="S4.p2.1.m1.3.3">𝐼</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.3c">(q,a,I)</annotation></semantics></math> triplets over all four answer choices for a given question <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">q</annotation></semantics></math>, and the scores are then normalized by a softmax layer. We experiment with different numbers of units in the hidden layer for each model. To prevent overfitting, we also introduce a dropout layer right after the feature vector and tune the dropout rate; and an <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">L</mi><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝐿</ci><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">L_{2}</annotation></semantics></math> regularizer in the output dense layer. Each layer leading to the feature vector in all the models, except the simple BOW model, also has mechanisms for preventing overfitting through dropout and regularization.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">We use categorical cross-entropy to measure the difference between the actual score for a <math id="S4.p3.1.m1.3" class="ltx_Math" alttext="(q,a,I)" display="inline"><semantics id="S4.p3.1.m1.3a"><mrow id="S4.p3.1.m1.3.4.2" xref="S4.p3.1.m1.3.4.1.cmml"><mo stretchy="false" id="S4.p3.1.m1.3.4.2.1" xref="S4.p3.1.m1.3.4.1.cmml">(</mo><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">q</mi><mo id="S4.p3.1.m1.3.4.2.2" xref="S4.p3.1.m1.3.4.1.cmml">,</mo><mi id="S4.p3.1.m1.2.2" xref="S4.p3.1.m1.2.2.cmml">a</mi><mo id="S4.p3.1.m1.3.4.2.3" xref="S4.p3.1.m1.3.4.1.cmml">,</mo><mi id="S4.p3.1.m1.3.3" xref="S4.p3.1.m1.3.3.cmml">I</mi><mo stretchy="false" id="S4.p3.1.m1.3.4.2.4" xref="S4.p3.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.3b"><vector id="S4.p3.1.m1.3.4.1.cmml" xref="S4.p3.1.m1.3.4.2"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝑞</ci><ci id="S4.p3.1.m1.2.2.cmml" xref="S4.p3.1.m1.2.2">𝑎</ci><ci id="S4.p3.1.m1.3.3.cmml" xref="S4.p3.1.m1.3.3">𝐼</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.3c">(q,a,I)</annotation></semantics></math> triplet and the normalized score obtained from the network. Finally, the network is tuned using Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for a certain number of iterations with the learning rate, minibatch size, and number of iterations tuned with other hyperparameters. At every step of optimization, the validation accuracy is compared against the best validation accuracy obtained so far, and the snapshot of the model with best accuracy is saved. This model is then used for testing. The entire experimental framework is run with a Tensorflow back-end <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> with front-end code written in Keras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The baseline and all experimental models are our own implementations.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">To handle variable length sequences, we use the <span id="S4.p4.1.1" class="ltx_text ltx_font_typewriter">pad_sequences</span> function available in Keras, which prepends the sequences with zeros to make every sequence of equal length. To ensure these zeros do not affect the loss and the gradient, we set <span id="S4.p4.1.2" class="ltx_text ltx_font_typewriter">mask_zero</span> flag to True, which ignores any zero in the input during the loss calculation and gradient update.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We explore three variants of the BOW baseline. The first version uses the image, the question and the answers. The second version does not use the image, and the third version is only trained on the answers, both of which are useful for studying dataset bias. In all three cases, convergence is reached in 50-60 iterations of training with the default Adam optimizer.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Version [1]: LSTM QA-Encoding</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.2" class="ltx_p">The three variants as described above are used here as well. We tune the hyperparameters for regularization and dropout applied to the matrices <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">U</annotation></semantics></math> and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">W</annotation></semantics></math>. Convergence is reached in 90-100 iterations with the default Adam optimizer.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Versions [2] and [3]: Context and Attention Augmentations</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">Since the more complicated context and attention models have higher susceptibility of overfitting to the training set, the dropout and regularization parameters used in these models is more severe. Also, these models have a more unpredictable optimization landscape and thus the Adam optimizer used for these models has a learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msup id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mo id="S4.SS3.p1.1.m1.1.1.3a" xref="S4.SS3.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">10</cn><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><minus id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">10^{-4}</annotation></semantics></math>, which is 0.1 times the learning rate of the default optimizer.
We generally observe these models take a much longer to converge and have lower validation accuracy at convergence than the simple models.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ensemble Model</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">Ensembling is an effective technique to obtain a stronger model from a series of weaker models. It works best when models exhibit diversity, that is, different models focus on different aspects of the data and together produce an ensemble model which can produce robust results on each aspect. We hypothesize that our selection of models with increasing complexity is diverse enough to produce a strong ensemble model. For this purpose, we pass each test example through each of the ten models and choose the option chosen by the majority of models as the answer to the question, while breaking ties arbitrarily. We do not perform any training on the ensemble and do not weigh the models. Hence, each model’s vote is counted equally.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We exploit our step-by-step progression in additional model complexity to explore their diversity in terms of general and specific performance enhancements and limitations. Results for the array of models is shown in Table <a href="#S5.T1" title="Table 1 ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. A comparison of the ensemble of our models with other models in the literature for the Visual7W dataset is shown in Table <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Model</th>
<td id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">What</span></td>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Who</span></td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">When</span></td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">How</span></td>
<td id="S5.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Where</span></td>
<td id="S5.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Why</span></td>
<td id="S5.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">Overall</span></td>
</tr>
<tr id="S5.T1.1.2.2" class="ltx_tr">
<th id="S5.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Baseline models</span></th>
<td id="S5.T1.1.2.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.2.2.8" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.1.3.3" class="ltx_tr">
<th id="S5.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BOW(A)</th>
<td id="S5.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">0.493</td>
<td id="S5.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">0.64</td>
<td id="S5.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">0.763</td>
<td id="S5.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">0.52</td>
<td id="S5.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r">0.59</td>
<td id="S5.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r">0.583</td>
<td id="S5.T1.1.3.3.8" class="ltx_td ltx_align_center">0.546</td>
</tr>
<tr id="S5.T1.1.4.4" class="ltx_tr">
<th id="S5.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BOW(Q,A)</th>
<td id="S5.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">0.565</td>
<td id="S5.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">0.668</td>
<td id="S5.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">0.78</td>
<td id="S5.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">0.533</td>
<td id="S5.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">0.621</td>
<td id="S5.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r">0.621</td>
<td id="S5.T1.1.4.4.8" class="ltx_td ltx_align_center">0.593</td>
</tr>
<tr id="S5.T1.1.5.5" class="ltx_tr">
<th id="S5.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BOW(Q,A,I)</th>
<td id="S5.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">0.607</td>
<td id="S5.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">0.704</td>
<td id="S5.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.1.5.5.4.1" class="ltx_text ltx_font_bold">0.817</span></td>
<td id="S5.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">0.528</td>
<td id="S5.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">0.717</td>
<td id="S5.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">0.63</td>
<td id="S5.T1.1.5.5.8" class="ltx_td ltx_align_center">0.634</td>
</tr>
<tr id="S5.T1.1.6.6" class="ltx_tr">
<th id="S5.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T1.1.6.6.1.1" class="ltx_text ltx_font_bold">Simple models</span></th>
<td id="S5.T1.1.6.6.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.6.6.8" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.1.7.7" class="ltx_tr">
<th id="S5.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BiLSTM(A)</th>
<td id="S5.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">0.494</td>
<td id="S5.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">0.647</td>
<td id="S5.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">0.76</td>
<td id="S5.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">0.514</td>
<td id="S5.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">0.59</td>
<td id="S5.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r">0.566</td>
<td id="S5.T1.1.7.7.8" class="ltx_td ltx_align_center">0.545</td>
</tr>
<tr id="S5.T1.1.8.8" class="ltx_tr">
<th id="S5.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BiLSTMs(Q,A)</th>
<td id="S5.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">0.566</td>
<td id="S5.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">0.674</td>
<td id="S5.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">0.77</td>
<td id="S5.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r">0.553</td>
<td id="S5.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">0.621</td>
<td id="S5.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r">0.614</td>
<td id="S5.T1.1.8.8.8" class="ltx_td ltx_align_center">0.596</td>
</tr>
<tr id="S5.T1.1.9.9" class="ltx_tr">
<th id="S5.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BiLSTMs(Q,A) + I</th>
<td id="S5.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">0.623</td>
<td id="S5.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">0.701</td>
<td id="S5.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">0.815</td>
<td id="S5.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r">0.562</td>
<td id="S5.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">0.721</td>
<td id="S5.T1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r">0.621</td>
<td id="S5.T1.1.9.9.8" class="ltx_td ltx_align_center">0.646</td>
</tr>
<tr id="S5.T1.1.10.10" class="ltx_tr">
<th id="S5.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T1.1.10.10.1.1" class="ltx_text ltx_font_bold">Contextual Models</span></th>
<td id="S5.T1.1.10.10.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.7" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.10.10.8" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T1.1.11.11" class="ltx_tr">
<th id="S5.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LSTMs(A)-Context</th>
<td id="S5.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r">0.588</td>
<td id="S5.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">0.685</td>
<td id="S5.T1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">0.788</td>
<td id="S5.T1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r">0.557</td>
<td id="S5.T1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r">0.677</td>
<td id="S5.T1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r">0.625</td>
<td id="S5.T1.1.11.11.8" class="ltx_td ltx_align_center">0.619</td>
</tr>
<tr id="S5.T1.1.12.12" class="ltx_tr">
<th id="S5.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LSTMs(A)-Context + I</th>
<td id="S5.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r">0.596</td>
<td id="S5.T1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r">0.701</td>
<td id="S5.T1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r">0.812</td>
<td id="S5.T1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r">0.532</td>
<td id="S5.T1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r">0.72</td>
<td id="S5.T1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r">0.616</td>
<td id="S5.T1.1.12.12.8" class="ltx_td ltx_align_center">0.628</td>
</tr>
<tr id="S5.T1.1.13.13" class="ltx_tr">
<th id="S5.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LSTMs(Q,A,I)-Context</th>
<td id="S5.T1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r">0.61</td>
<td id="S5.T1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">0.695</td>
<td id="S5.T1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">0.815</td>
<td id="S5.T1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r">0.562</td>
<td id="S5.T1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r">0.714</td>
<td id="S5.T1.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r">0.627</td>
<td id="S5.T1.1.13.13.8" class="ltx_td ltx_align_center">0.639</td>
</tr>
<tr id="S5.T1.1.14.14" class="ltx_tr">
<th id="S5.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LSTMs(Q,A,I’)-Attention</th>
<td id="S5.T1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r">0.51</td>
<td id="S5.T1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r">0.597</td>
<td id="S5.T1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r">0.742</td>
<td id="S5.T1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r">0.522</td>
<td id="S5.T1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r">0.61</td>
<td id="S5.T1.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r">0.528</td>
<td id="S5.T1.1.14.14.8" class="ltx_td ltx_align_center">0.548</td>
</tr>
<tr id="S5.T1.1.15.15" class="ltx_tr">
<th id="S5.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Ensemble-All</th>
<td id="S5.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.15.15.2.1" class="ltx_text ltx_font_bold">0.641</span></td>
<td id="S5.T1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.15.15.3.1" class="ltx_text ltx_font_bold">0.721</span></td>
<td id="S5.T1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.814</td>
<td id="S5.T1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.15.15.5.1" class="ltx_text ltx_font_bold">0.589</span></td>
<td id="S5.T1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.15.15.6.1" class="ltx_text ltx_font_bold">0.734</span></td>
<td id="S5.T1.1.15.15.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.15.15.7.1" class="ltx_text ltx_font_bold">0.67</span></td>
<td id="S5.T1.1.15.15.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T1.1.15.15.8.1" class="ltx_text ltx_font_bold">0.667</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Test accuracies across all models, broken down by question type.</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">What</span></th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Who</span></th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">When</span></th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">How</span></th>
<th id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Where</span></th>
<th id="S5.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Why</span></th>
<th id="S5.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T2.1.1.1.8.1" class="ltx_text ltx_font_bold">Overall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Fritz et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.489</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.581</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.713</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.503</td>
<td id="S5.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.544</td>
<td id="S5.T2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.513</td>
<td id="S5.T2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">0.521</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.515</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.595</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.75</td>
<td id="S5.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.498</td>
<td id="S5.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">0.57</td>
<td id="S5.T2.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">0.555</td>
<td id="S5.T2.1.3.2.8" class="ltx_td ltx_align_center">0.556</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Jabri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.4.3.2.1" class="ltx_text ltx_font_bold">0.645</span></td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.4.3.3.1" class="ltx_text ltx_font_bold">0.729</span></td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.4.3.4.1" class="ltx_text ltx_font_bold">0.821</span></td>
<td id="S5.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">0.564</td>
<td id="S5.T2.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.4.3.6.1" class="ltx_text ltx_font_bold">0.759</span></td>
<td id="S5.T2.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.1.4.3.7.1" class="ltx_text ltx_font_bold">0.68</span></td>
<td id="S5.T2.1.4.3.8" class="ltx_td ltx_align_center"><span id="S5.T2.1.4.3.8.1" class="ltx_text ltx_font_bold">0.671</span></td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Ensemble model</th>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.641</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.721</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.814</td>
<td id="S5.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T2.1.5.4.5.1" class="ltx_text ltx_font_bold">0.589</span></td>
<td id="S5.T2.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.734</td>
<td id="S5.T2.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.67</td>
<td id="S5.T2.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b">0.667</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the ensemble model with other models in the literature on Visual7W. The other results are from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Effect of Additional Complexity: Context and Attention</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">We observe evidence of greater image reasoning for our models augmented with context and attention. To obtain further insight, we explore questions that only these models classify accurately. From the questions for which these models are the “experts”, we note that most seem to focus on aspects of the image relating to color and spatial positioning. Figures <a href="#S5.F2" title="Figure 2 ‣ 5.1 Effect of Additional Complexity: Context and Attention ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.F3" title="Figure 3 ‣ 5.1 Effect of Additional Complexity: Context and Attention ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrate some questions which only the model with attention and the models with context answer correctly, respectively. Answering these questions requires focusing on specific sections of the image and understanding the underlying semantics, which is the goal of these enhanced models. For example, the question ”How many white blocks are shown on the plate in between sinks?” requires reasoning about the position of the sink, recognizing the blocks, and subsequently filtering out only the white blocks. The attention model is able to capture this relationship. Similarly, the question ”When was picture taken?” requires context generation by associating fading light with evening, which the contextual models are able to do.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">Interestingly, we find that simply augmenting the answer and question LSTM-encoded feature inputs to the MLP for the contextual and attentional models, respectively, and keeping the default image embeddings from V[1] is less effective than removing the image embedding and using the augmentations alone with the original LSTM-encodings of the question and answer, respectively. This points to useful subtle interactions between each pair of inputs that can be easily “masked” by the concatenation of a high-dimensional image feature representation.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">While we observe increased image reasoning for V[3], it performs worse overall across all question types. However, the overall accuracy is commensurate with that reported by Zhu et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in their more complicated implementation of attention. Generally, however, the best individual model was V[1], which is evidence of trend (2) seen in the literature towards simpler architectures with high performance, alluding to strong dataset bias as we explore subsequently.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/1703.08120/assets/attention.jpg" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of questions which only the attention model answers correctly.</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/1703.08120/assets/context.jpg" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="388" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of questions which only the context models answer correctly.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Effect of Ensembling</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Our qualitative and quantitative observations about the diversity of our models are supported by the improved performance of our ensemble, which successfully exploits the diversity across its constituent classifiers. In each question category, except the “when” category, there is a marked improvement in the accuracy of around 2-3% in the ensemble when compared with the accuracy of the best singleton model, indicating that different models specialize at different aspects of the data and their combination has strength across all aspects. The “when” category does not show a marked improvement because of higher homogeneity of questions in the category, which is also demonstrated by our subsequent dataset bias analysis. Examples of questions answered correctly by the ensemble are shown in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Effect of Ensembling ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/1703.08120/assets/ensemble__1_.jpg" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="353" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of questions that the ensemble answers correctly. The parenthesized number for each option indicates the number of constituent models voting for it. </figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evidence of Dataset Bias</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">We see that in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.3 Evidence of Dataset Bias ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, there are substantial percentages of questions that all or none of the classifiers answer correctly. Predictably, we see that these “easy” questions can be answered without knowledge of the image at all, while “hard” questions that all classifiers fail to answer correctly require more in-depth reasoning about the image. Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Evidence of Dataset Bias ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a breakdown of hard questions (answered correctly by less than 3 models), easy questions (answered correctly by more than 7 models) and fair questions (answered correctly by 3-7 models). The “who”, “when”, “where” and “why” categories have substantial number of easy questions, with the “when” category having a percentage as high as 71.3%. This points at homogeneity in the questions and bias in the text which can picked up by models which do not include images. On the other hand, 33.5% and 25.1% of questions in the “how” and “what” categories, respectively, are answered correctly by very few models.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‣ 5.3 Evidence of Dataset Bias ‣ 5 Results ‣ Recurrent and Contextual Models for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of questions that were answered correctly by only one model. The attention and simple baseline have the highest fractions of such questions (16.5% and 18.5%) respectively, which demonstrates that the very complex and very simple models perform well on certain tasks.
Hence, while sophisticated image-reasoning models are bound to perform better on “hard” questions if trained properly, the effect of bias is strong enough for simpler models to dominate the overall performance by maximizing correct answers on “easy” questions. Future versions of this dataset could factor these bias observations into consideration.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1703.08120/assets/bias_all.png" id="S5.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="161" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visual7W dataset biases across all question types. Around 28% of questions are answered correctly by all models while around 13% of questions are not correctly answered by any classifier. </figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/1703.08120/assets/single_question.png" id="S5.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="158" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Fraction of “hard” questions that were answered correctly only by one model, across all models. The attention and simple baseline answer 16.5% and 18% of such questions, respectively.</figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Category</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Hard questions</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Fair questions</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Easy questions</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">What</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.251</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.359</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.389</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Who</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.198</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.241</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.563</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">When</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.132</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.157</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.713</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">How</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.335</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.248</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.417</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<th id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Where</th>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">0.18</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.31</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.51</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<th id="S5.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Why</th>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.238</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.305</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.458</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Fraction of easy, fair and hard questions for each question type.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In summary, we have proposed a series of recurrent, contextual, and attentional models for visual question answering that explore the effect of the balance between model expressiveness and simplicity on performance. While we observe that our simplest experimental model involving simple LSTM-encodings of the QA text achieves the best individual model performance, confirming a trend towards simpler architectures found in the literature, we also see that our more complex contextual and attentional models demonstrate noticeable improvements in image reasoning. This is exploited in the improved performance of our ensemble model, which achieves a test accuracy within <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="0.5\%" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">0.5</mn><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">0.5\%</annotation></semantics></math> of state-of-the-art on Visual7W. In evaluating the performance of our series of models, we find evidence of dataset bias in Visual7W, in the form of significant percentages of both universally easy and hard questions that all or none of our models correctly answer, respectively. Future studies would involve evaluating our trained models on other current visual question answering datasets such as VQA 1.0 and 2.0; and training on VQA and evaluating on V7W to explore inter-dataset biases and whether our models generalize; and also investigating more complex models of image attention.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">The authors would like to thank Danqi Chen for her advice and help for this project; and Dr. Manning and Dr. Socher for their instruction.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den
Hengel.

</span>
<span class="ltx_bibblock">Visual question answering: A survey of methods and datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.05910</span>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7W: Grounded Question Answering in Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition</span>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Kevin J Shih, Saurabh Singh, and Derek Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4613–4621, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>, pages
289–297, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens van der Maaten.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 727–739.
Springer, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Diederik Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Martín Abadi et al.

</span>
<span class="ltx_bibblock">TensorFlow: Large-scale machine learning on heterogeneous systems,
2015.

</span>
<span class="ltx_bibblock">Software available from tensorflow.org.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
François Chollet.

</span>
<span class="ltx_bibblock">Keras.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/fchollet/keras" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fchollet/keras</a>, 2015.

</span>
</li>
</ul>
</section>
<section id="S6.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Appendix</h4>

<figure id="S6.F7" class="ltx_figure"><img src="/html/1703.08120/assets/biases_all_types.png" id="S6.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="502" height="267" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visual7W dataset biases broken down by question type.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1703.08119" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1703.08120" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1703.08120">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1703.08120" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1703.08121" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 19:25:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
