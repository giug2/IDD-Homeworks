<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract</title>
<!--Generated on Fri Sep 20 09:01:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2409.12813v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S1" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S2" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Equipment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S2.SS1" title="In II Equipment ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">ASV Korkyra</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S2.SS2" title="In II Equipment ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">ROV and Acoustic Localization System</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.SS1" title="In III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Dataset Collection and Labeling</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.SS2" title="In III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Biofouling Estimation Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.SS2.SSS1" title="In III-B Biofouling Estimation Framework ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Biofouling Buildup Quantification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.SS2.SSS2" title="In III-B Biofouling Estimation Framework ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Image Segmentation Node</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.SS3" title="In III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Closed-Loop ROV Control System</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S4" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS1" title="In V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Autonomous ROV Control Loop Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS2" title="In V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Image Labeling Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS3" title="In V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">UNet Architecture Segmentation Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS4" title="In V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Biofouling Estimation Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS4.SSS1" title="In V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>1 </span>Framework Filters Results Using UNet for Semantic Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS4.SSS2" title="In V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>2 </span>Effect of Filming Conditions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS4.SSS3" title="In V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span>3 </span>Choosing the UNet Architecture Instead of Logistical Regression</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S6" title="In Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Matej Fabijanić 1,
Nadir Kapetanović 1,
Nikola Mišković 1




</span><span class="ltx_author_notes">
Research work presented in this article has been supported by
the European Union under Grant Agreement Number 101060395 - MONUSEN project (MONtenegrin center for Underwater SEnsor Networks. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Executive Agency (REA). Neither the European Union nor the granting authority can be held responsible for them;
the “Razvoj autonomnog besposadnog višenamjenskog broda” project (KK.01.2.1.02.0342) co-financed by the European Union from the European Regional Development Fund within the Operational Program “Competitiveness and Cohesion 2014-2020”;
the Croatian National Recovery and Resilience funded project Smart Blue Tourism, G.A. No. NPOO.C1.6.R1-I2.01-V3.0007;
the content of the publication is the sole responsibility of the project partner UNIZG-FER; and by ONR Robot Aided Diver Navigation in Mapped Environments - ROADMAP project under Grant Agreement No. N000142112274
<span class="ltx_contact ltx_role_affiliation">1 University of Zagreb Faculty of Electrical Engineering and Computing, Zagreb, Croatia
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1">The process of fish cage inspections, which is a necessary maintenance task at any fish farm, be it small-scale or industrial, is a task that has the potential to be fully automated. Replacing trained divers who perform regular inspections with autonomous marine vehicles would lower the costs of manpower and remove the risks associated with humans performing underwater inspections. Achieving such a level of autonomy implies developing an image processing algorithm that is capable of estimating the state of biofouling buildup. The aim of this work is to propose a complete solution for automating the said inspection process; from developing an autonomous control algorithm for an ROV, to automatically segmenting images of fish cages, and accurately estimating the state of biofouling. The first part is achieved by modifying a commercially available ROV with an acoustic SBL positioning system and developing a closed-loop control system. The second part is realized by implementing a proposed biofouling estimation framework, which relies on AI to perform image segmentation, and by processing images using established computer vision methods to obtain a rough estimate of the distance of the ROV from the fish cage. This also involved developing a labeling tool in order to create a dataset of images for the neural network performing the semantic segmentation to be trained on. The experimental results show the viability of using an ROV fitted with an acoustic transponder for autonomous missions, and demonstrate the biofouling estimation framework’s ability to provide accurate assessments, alongside satisfactory distance estimation capabilities. In conclusion, the achieved biofouling estimation accuracy showcases clear potential for use in the aquaculture industry.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span class="ltx_text" id="id2.id1">
<span class="ltx_text ltx_font_italic" id="id2.id1.1">fish cage inspection; aquaculture biofouling estimation; underwater image segmentation;
autonomous ROV control loop; image annotation tool</span>
</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">There has been a growing acknowledgment of the crucial role played by small-scale fisheries and industrial aquaculture in ensuring global food security and nutrition in the 21st century.
Global aquaculture production has shown a rising trend over the last 30 years, with around 80 million tonnes of seafood produced in 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib1" title="">1</a>]</cite>.
Aquaculture is known to be highly labor-intensive, requiring significant human involvement in various tasks such as feeding, cleaning and processing as opposed to the envisioned streamlined machine-supported industrial agriculture.
The emergence of autonomous robots presents an opportunity to supplement and enhance these labor-intensive operations.
By integrating autonomous robots, the aquaculture sector can achieve higher efficiency, reduce operational costs, and ensure sustainable fishing practices for the future.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Previous research in this field often deals with only one specific aspect of aquaculture activities, like net damage detection in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib3" title="">3</a>]</cite>.
Other work such as that by Duda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib4" title="">4</a>]</cite> uses computer vision techniques to achieve ROV pose estimation and briefly touches on biological buildup on the net, but only mentions it as a potential problem for pose estimation.
More work by Livanos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib5" title="">5</a>]</cite> discusses enhancing ROV autonomy level through intelligent navigation, but does not showcase a use for any specific fishery maintenance task.
Work by <span class="ltx_text" id="S1.p2.1.1">Qiu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib6" title="">6</a>]</cite></span> examines the estimation of built-up biofouling using image processing, but only briefly mentions using an ROV to capture footage needed for research.
To the best of the authors’ knowledge, there is no literature on the development of a complete autonomous fish cage inspection system that includes not only visual biofouling estimation, but also control and localization of an underwater vehicle.
A research project named HEKTOR (Heterogeneous autonomous robotic system in viticulture and mariculture) aimed to fill this knowledge gap and offer a solution that enables efficient coordination among heterogeneous autonomous robots, as can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">1</span></a>.
More information about the project can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="460" id="S1.F1.g1" src="extracted/5867909/figures/Hektor_Schema.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">A schematic of the proposed HEKTOR underwater inspection solution.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One task that can, or better said should be automated, is the inspection of fish cage nets in aquaculture.
The net gradually accumulates biofouling which has many negative consequences.
The main problem is the reduction in available area for clean water to flow through which causes the water inside the pen to become less oxygenated and more fouled, ultimately resulting in increased fish sickness and death rate.
Accompanying problems include the addition of extra mass to the pen structure, thus placing stress on the mooring ropes, damaging the net and causing a need for reknitting it.
Periodic visual inspections by divers are currently necessary to assess the condition of the pens and determine the appropriate timing for cleaning.
Using an ROV or an AUV to perform said visual inspections would reduce the need for divers, helping to create a more streamlined, autonomous, and risk-averse inspection process.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The proposed automated underwater fish cage inspection process includes an autonomous surface vehicle (ASV) working in cooperation with an ROV as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">1</span></a>.
The camera feed is streamed from the ROV, while biofouling estimation, control, and underwater algorithms are run on the ASV’s onboard computer.
The inspection process is envisioned to be split into two tasks.
The first task is processing the footage obtained from a vehicle filming underwater to estimate the amount of biofouling present on the net.
The second task is developing an autonomous control algorithm to maneuver an ROV around the pens.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main research goal was to develop an image processing algorithm that could be combined with the control algorithm in order to accurately assess the amount of biofouling accumulated on the nets.
This research contributes in several key areas.
Firstly, a neural network is successfully utilized for accurately segmenting underwater images of fish pens, enabling precise identification of pen structures.
The architecture used for AI segmentation is the popularly employed UNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib9" title="">9</a>]</cite>, explained in more detail later.
Secondly, the research explores and tests the feasibility of retrofitting an ROV with an underwater transponder to achieve precise localization.
This modification improves upon its manual operation while also enabling autonomous missions, enhancing both its operational ease and versatility.
Thirdly, the paper proposes a technique for estimating the extent of biofouling buildup on a given pen.
By combining image segmentation with localization, the technique provides a valuable means of quantifying the level of biofouling on fish pens.
Lastly, the proposed biofouling estimation technique is successfully tested in controlled and repeatable experimental scenarios.
Collectively, these contributions improve fish pen biofouling analysis, and highlight potential advancements in the field of underwater robotics and mariculture practices.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Equipment</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">ASV Korkyra</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A custom-made aluminum catamaran named Korkyra was developed to function as a versatile remote-controlled or autonomous surface vehicle as a part of HEKTOR, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S2.F2" title="Figure 2 ‣ II-A ASV Korkyra ‣ II Equipment ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">2</span></a>.
This specially designed catamaran boasts several key features, one of which is a landing platform dedicated to accommodating a lightweight drone for aerial operations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib10" title="">10</a>]</cite>.
Additionally, it incorporates a docking and tether management system intended for seamless integration with an ROV, enabling underwater mission capabilities, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib11" title="">11</a>]</cite>.
It also features a robust metal frame that enables the mounting of diverse tools and sensors such as cameras, sonar, lidar, etc.
A powerful onboard computer is used to support real-time control algorithms, data processing, and other computational requirements.
With its purpose-built design and advanced functionalities, Korkyra serves as a valuable asset for remote-controlled or autonomous operations.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="211" id="S2.F2.g1" src="extracted/5867909/figures/korkyra.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.5.2" style="font-size:90%;">Autonomous surface vehicle Korkyra: <span class="ltx_text ltx_font_bold" id="S2.F2.5.2.1">left</span>—TMS mounted, <span class="ltx_text ltx_font_bold" id="S2.F2.5.2.2">right</span>—LP mounted onto the ASV.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">ROV and Acoustic Localization System</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Autonomous systems must be able to precisely estimate their own position relative to obstacles, structures, and other objects in order to navigate and operate effectively in cluttered environments such as fisheries.
An underwater acoustic positioning system is essential for autonomous underwater missions due to the absence of GPS and standard RF-based positioning systems in underwater environments.
This technology plays a vital role in enabling autonomous underwater missions by providing reliable positioning information where traditional positioning systems cannot operate effectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib12" title="">12</a>]</cite>.
By using acoustic signals, the positioning system enables accurate navigation, mapping, and control of underwater vehicles in real-time.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">A commercially available ROV was acquired from Blueye, a Norwegian company specializing in underwater technology. The ROV was mounted with a transponder belonging to an SBL acoustic underwater positioning system, as can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S2.F3" title="Figure 3 ‣ II-B ROV and Acoustic Localization System ‣ II Equipment ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">3</span></a>.
The SBL acoustic localization system is the Underwater GPS G2 acquired from WaterLinked, also a Norwegian company specializing in acoustic subsea communication and positioning systems.
An L-shaped fixed position configuration with 4 transceivers placed at various depths, and 1 transponder mounted on the ROV was precise enough for use in a controlled test inspection scenario.
A secondary, but perhaps more realistic rectangular configuration with the 4 transceivers mounted on an ASV was also tested and provided good results in rough weather conditions.
The depth reading was taken directly from the ROV because the sensor readings were far more precise than those received from the SBL transponder.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S2.F3.g1" src="extracted/5867909/figures/combinedActionShotAndCloseup.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">The used Blueye Pro ROV and a closeup of the retrofitted WaterLinked Underwater GPS G2 transponder.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Dataset Collection and Labeling</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The research involved collecting underwater video footage of fish cages in the Adriatic Sea during the summers of 2020 and 2021 to study biofouling buildup.
This footage, showcasing cages in various states of fouling, was used to create a dataset of nearly 4,000 images.
Additional footage was collected in a controlled seawater pool in 2022, adding around 1,000 more images.
To analyze the buildup, the research focused on image segmentation, requiring labeled images of the cages.
A labeling tool was developed to streamline this process by using the K-Means clustering algorithm to group pixels of similar colors.
This approach effectively handled large image volumes, simplifying images down to key colors without losing structural detail.
The tool allowed operators to label images efficiently by selecting clusters and assigning them to categories such as “sea”, “cage”, “fish”, and “blurry” as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F4" title="Figure 4 ‣ III-A Dataset Collection and Labeling ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="315" id="S3.F4.g1" src="extracted/5867909/figures/whole_screen.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Screenshot of the labeling tool developed for easier dataset creating. The image in the top left is the original image with clusters of pixels turned on or off. The image in the top right replaces pixels from the original image with their respective centroids resulting from K-Means clustering. It is possible to choose the color space of the image, change the K hyperparameter, and turn the pixels on and off using the toolbar above the images. The legend pop-up window in the bottom left is used for turning the grouped pixels “on or off” and assigning labels. The color squares represent the resulting centroids of K-Means clustering.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Biofouling Estimation Framework</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The modular biofouling estimation framework proposed herein is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F5" title="Figure 5 ‣ III-B Biofouling Estimation Framework ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">5</span></a>.
The framework comprises several interconnected nodes with each node serving a specific purpose.
The framework and the nodes came about as a product of developing image processing algorithms and the ROV control loop in the Robotic Operating System (ROS).
Node (1), shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F5" title="Figure 5 ‣ III-B Biofouling Estimation Framework ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">5</span></a>, is responsible for executing the image segmentation process, separating the fish pen structure and its net from the background.
Node (2) combines the segmented data with the distance information to reconstruct how a clean net would appear at that specific filming distance.
Node (3) compares the two binary images, one of the ideal net state and one of the current state, and the result quantifies the extent of biofouling coverage on the net’s surface.
Node (4) implements pose (distance) estimation from a single camera if the distance from the filmed net is not known from a 3D map of the environment or some other source.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">A particular implementation of the framework developed during research calculates an estimated distance from the net by determining the approximate distance between each center of the small squares in the net, shown in more detail in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F6" title="Figure 6 ‣ III-B Biofouling Estimation Framework ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">6</span></a>.
The selection of centers as good features in the biofouling estimation process was based on their inherent stability.
The squares gradually reduce in size as biofouling builds up on the net, however the position of the center remains constant which makes the centers robust features for detection.
By having knowledge of the distance in pixels for specific features on an object in an image, as well as the corresponding real-life distances, along with information about the camera sensor used for capturing the picture such as the physical size of the sensor and its focal length, it is possible to estimate the distance from the observed object to the camera.
This allows for a reasonable estimation of the distance from the camera to the observed fish pen.
This method of distance estimation can be classified as a geometric approach to the problem, as opposed to using a more time-consuming and computationally heavier deep-learning approach.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="186" id="S3.F5.g1" src="extracted/5867909/figures/estimationFramework.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Schematic of the used biofouling estimation framework.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="191" id="S3.F6.g1" src="extracted/5867909/figures/idealVizdetails.drawio.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Ideal net visualizer part of the framework shown in more detail as it is implemented.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>Biofouling Buildup Quantification</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The net is filmed with as uniform a movement as possible at a fixed distance from the net.
The ROVs movement speed was such that filming at 1Hz was frequent enough to capture the entire net area.
Uniform movement of the ROV ensures that each segment of the net is filmed for roughly the same amount of time.
The entire footage can be fed into the framework, and the amount of biofouling for the filmed area would be the average of all of the estimations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.4.1.1">III-B</span>2 </span>Image Segmentation Node</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">The labeled data from the image labeling process was initially used to train a logistic regression model, which aimed to classify pixels as belonging to the fish cage structure or the background (sea and fish) based on color. This model, trained on an 80-20% train-test split, showed limitations in handling variations like lighting conditions and image blurriness, leading to its abandonment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib13" title="">13</a>]</cite>.
A more robust approach using neural networks, specifically the UNet architecture, was then adopted for image segmentation.
UNet, known for its U-shaped design, effectively captures both high-level context and fine details, producing accurate pixel-wise segmentation masks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib9" title="">9</a>]</cite>.
The model was trained on randomly selected images from the dataset, using the same 80-20% split, with only basic sharpening applied to images before segmentation.
The goal was to accurately distinguish between the fish pen net and the surrounding background.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Closed-Loop ROV Control System</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">A lawnmower pattern trajectory controller was designed to serve as a proof-of-concept to test the possibility of complete autonomous control for the modified ROV.
This test aimed to assess functionalities such as precise position estimation, efficient trajectory planning, and to validate the responsiveness of the ROV’s control loop.
Each state in the control algorithm represents a specific action for the ROV, such as moving to a starting position, swaying, descending, or resurfacing to a predetermined end point.
A general schematic for the control system can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F7" title="Figure 7 ‣ III-C Closed-Loop ROV Control System ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">7</span></a>, and the different states of the controller can be seen in a UML class diagram of the controller implementation in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S3.F8" title="Figure 8 ‣ III-C Closed-Loop ROV Control System ‣ III Methodology ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">8</span></a>.
The control system implemented for the ROV managed surge and heave motions.
Additionally, the ROV features built-in automatic heading maintenance, that is, it points to a constant direction during operation.
The position error is used as an input into a classic PID controller that generates thruster commands, so that the control system can be used for any 2-DOF ROV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="202" id="S3.F7.g1" src="extracted/5867909/figures/controlLoopENG.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S3.F7.3.2" style="font-size:90%;">Schematic depicting the ROV control loop.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="249" id="S3.F8.g1" src="extracted/5867909/figures/Controller_Class_Diagram.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S3.F8.3.2" style="font-size:90%;">Schematic showing the implemented control loop class diagram. The “Controller” class generates waypoints by taking into account the leftmost and rightmost possible values for the position, depth, and how many discrete points to generate along the horizontal and vertical axes. The outputs are values for thruster speed along two controlled degrees of freedom (surge and sway).</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental Setup</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.4">Validation experiments for the biofouling estimation framework were conducted in a controlled seawater pool in Biograd, Croatia in late September and early October of 2022.
For the purpose of conducting these trials, a pen net was acquired from an industrial fish farm and deployed within the controlled environment of an Olympic-sized pool.
The dimension of the net is <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">∼</annotation></semantics></math>14 m wide and <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">∼</annotation></semantics></math>3 m high, so <math alttext="\sim" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mo id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><csymbol cd="latexml" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">∼</annotation></semantics></math>42 m<sup class="ltx_sup" id="S4.p1.4.1">2</sup> of area in total.
To simulate biofouling buildup in this scenario, camouflage-pattern colored square patches were strategically hand-placed onto the fish cage net as can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S4.F9" title="Figure 9 ‣ IV Experimental Setup ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">9</span></a>.
The patches were 25cm x 25cm squares.
They were designed to mimic the visual appearance of underwater biological fouling using a brown–yellow color scheme.
An increasing number of patches were added to the net in each iteration of the experiment.
The net was filmed once with no patches, once with patches covering 22%, 33%, and 44%, and three times with patches covering 66% of the net with greater distance to the net each time.
The ROV was autonomously controlled during the underwater missions.
The implementation of autonomous control in the controlled ocean-floor pool environment not only facilitated more consistent data collection but also served to test the developed control algorithm.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F9.g1" src="extracted/5867909/figures/Pool_and_patch.jpeg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.4.2.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.2.1" style="font-size:90%;">The Olympic sea water pool used for testing, with the net dragged out in order to place patches for biofouling simulation. The affixed patches on the net can be seen close<math alttext="-" class="ltx_Math" display="inline" id="S4.F9.2.1.m1.1"><semantics id="S4.F9.2.1.m1.1b"><mo id="S4.F9.2.1.m1.1.1" xref="S4.F9.2.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.F9.2.1.m1.1c"><minus id="S4.F9.2.1.m1.1.1.cmml" xref="S4.F9.2.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.2.1.m1.1d">-</annotation><annotation encoding="application/x-llamapun" id="S4.F9.2.1.m1.1e">-</annotation></semantics></math>up filmed during a mission in the red circle.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">A screenshot of the UWGPS system GUI can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S4.F10" title="Figure 10 ‣ IV Experimental Setup ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">10</span></a>, taken during one testing of the ASV+ROV combination in open sea.
The green line represents the ASV trajectory which should always be available as it has the UWGPS box mounted, while the blue line represents the trajectory of the free-moving transponder that is attached to the ROV in this case, and can travel out of the search range.
Due to limitations of working in a pool, a fixed baseline of transponders was positioned around the pool’s edge.
Although this approach differed from the anticipated ASV + ROV combination, it provided a needed practical solution for achieving reliable positioning data during the experimental setup in the pool environment.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="244" id="S4.F10.g1" src="extracted/5867909/figures/screenshot_uwgps.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">Screenshot of the UWGPS GUI while the ROV is tethered to the ASV which also acts as the carrier for the short baseline transponder setup. Since the trajectory of the ROV is visualized, this software was used to roughly estimate the precision of the SBL setup.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Autonomous ROV Control Loop Results</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Each ROV mission took around 15 min to cover the aforementioned <math alttext="42\,\text{m}^{2}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">42</mn><mo id="S5.SS1.p1.1.m1.1.1.1" lspace="0.170em" xref="S5.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><msup id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"><mtext id="S5.SS1.p1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.3.2a.cmml">m</mtext><mn id="S5.SS1.p1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><cn id="S5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2">42</cn><apply id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.3.2a.cmml" xref="S5.SS1.p1.1.m1.1.1.3.2"><mtext id="S5.SS1.p1.1.m1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.3.2">m</mtext></ci><cn id="S5.SS1.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">42\,\text{m}^{2}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">42 m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>.
The 3D plot in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F11" title="Figure 11 ‣ V-A Autonomous ROV Control Loop Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">11</span></a> visualizes the ROV’s movement during a mission, and its ideal trajectory generated by the control algorithm.
The Y-axis in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F11" title="Figure 11 ‣ V-A Autonomous ROV Control Loop Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">11</span></a> correlates to the distance from the ROV to the net since the net was strung out straight.
The controller was implemented with this assumption, so it tried to keep a constant Y-coordinate throughout the mission.
The position is plotted every second.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="679" id="S5.F11.g1" src="extracted/5867909/figures/biof33_nice.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S5.F11.3.2" style="font-size:90%;">3D plot of the ROV position. The blue line represents a perfect trajectory, the colored dots represent actual positions in time during one mission.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2">Having the ROV’s surge speed be between <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">∼</annotation></semantics></math>0.1 m/s and <math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mo id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">∼</annotation></semantics></math>0.2 m/s has shown to be a good compromise between the quality of footage regarding image sharpness, and the duration of a mission.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Image Labeling Results</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The images picked for the dataset were represented in the LAB color space.
The LAB color space separates the luminance (L) channel from the color information.
During the image labeling process, only the central part of the full HD image was considered due to concerns related to camera distortion and overall poor image quality near the edges.
Images exhibiting labeling errors, low image quality, or inconsistencies were identified and removed from the training dataset.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.T1" title="Table I ‣ V-B Image Labeling Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">I</span></a> shows the distribution of images in the dataset.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.2.1.1" style="font-size:90%;">Table I</span>: </span><span class="ltx_text" id="S5.T1.3.2" style="font-size:90%;">Table showing how many images per filming year were labeled in total, and how many of the labeled images ended up in the training/validation dataset for the neural network.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4" style="width:433.6pt;height:105.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(68.9pt,-16.8pt) scale(1.46579880838843,1.46579880838843) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.4.1.1.1.1" style="padding-left:6.7pt;padding-right:6.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.1.1">Footage year</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.4.1.1.1.2" style="padding-left:6.7pt;padding-right:6.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.2.1">Location</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.1.3" style="padding-left:6.7pt;padding-right:6.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.3.1">Labeled imagess</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.1.1.1.4" style="padding-left:6.7pt;padding-right:6.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.1.1.1.4.1">Images in dataset</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.1.2.1.1" style="padding-left:6.7pt;padding-right:6.7pt;">2020</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.1.2.1.2" style="padding-left:6.7pt;padding-right:6.7pt;">Ugljan</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.2.1.3" style="padding-left:6.7pt;padding-right:6.7pt;">938</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.2.1.4" style="padding-left:6.7pt;padding-right:6.7pt;">261</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.1.3.2.1" style="padding-left:6.7pt;padding-right:6.7pt;">2021</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.1.3.2.2" style="padding-left:6.7pt;padding-right:6.7pt;">Ugljan</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.2.3" style="padding-left:6.7pt;padding-right:6.7pt;">405</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.4.1.3.2.4" style="padding-left:6.7pt;padding-right:6.7pt;">197</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.4.1.4.3.1" style="padding-left:6.7pt;padding-right:6.7pt;">2022</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T1.4.1.4.3.2" style="padding-left:6.7pt;padding-right:6.7pt;">Biograd</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.1.4.3.3" style="padding-left:6.7pt;padding-right:6.7pt;">919</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.4.1.4.3.4" style="padding-left:6.7pt;padding-right:6.7pt;">694</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">UNet Architecture Segmentation Results</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">As mentioned earlier, in order to train the UNet model the standard 80–20%
train–test split was used, meaning that the model was trained on 80% of available annotated images in the dataset, and the results of the training steps were validated on a randomly selected 20% of images in the dataset that are unseen during training.
The training process was stopped once no more discernible improvement was shown from one iteration to the next.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">Dice score is a common metric used for scoring the performance of image segmentation models, ranging from <math alttext="0.0" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn id="S5.SS3.p2.1.m1.1.1.cmml" type="float" xref="S5.SS3.p2.1.m1.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">0.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">0.0</annotation></semantics></math> to <math alttext="1.0" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><mn id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><cn id="S5.SS3.p2.2.m2.1.1.cmml" type="float" xref="S5.SS3.p2.2.m2.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">1.0</annotation></semantics></math> (a higher value is better).
It measures the similarity or overlap between the predicted segmentation mask and the ground truth segmentation mask <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib16" title="">16</a>]</cite>.
The highest Dice score achieved on the validation set of images was 0.8434 during the 9th training epoch, and the weights calculated to achieve this coefficient were saved to be used later on.
The Dice score can be seen changing during the training process in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F12" title="Figure 12 ‣ V-C UNet Architecture Segmentation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">12</span></a>.
A visualization of how the trained UNet model successfully segments the images can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F13" title="Figure 13 ‣ V-C UNet Architecture Segmentation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="310" id="S5.F12.g1" src="extracted/5867909/figures/validation.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S5.F12.3.2" style="font-size:90%;">Plot showing the popular Dice score used in image segmentation analysis and its change during the training.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F13.g1" src="extracted/5867909/figures/segmented_joined.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F13.4.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S5.F13.5.2" style="font-size:90%;">
Result of predictions made by a trained UNet neural network segmentation model for: <span class="ltx_text ltx_font_bold" id="S5.F13.5.2.1">left</span>—an image taken in the controlled conditions in Biograd, <span class="ltx_text ltx_font_bold" id="S5.F13.5.2.2">right</span>—an image taken at a real fishery near Ugljan.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.4.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.5.2">Biofouling Estimation Results</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">As previously mentioned, an industrial fishery provided a fish pen net for the experimental setup.
Square patches were affixed onto the net in a series of iterations to simulate biofouling.
The patches were incrementally added in four stages, progressively covering larger areas of the net.
All of missions had the ROV at around 1 m away from the net being filmed, except a repeated mission at 66% where the distance was purposefully increased to around 1.5 m.
To establish a benchmark result, the biofouling estimation algorithm was initially applied without the implementation of any filtering methods.
All of the recorded footage in a filming session was only cropped around the center of the frame and fed into the estimation algorithm.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.T2" title="Table II ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">II</span></a> shows the benchmark estimated percentages.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">Table II</span>: </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">Table showing the actual simulated biofouling percentage, and the estimated biofouling percentage generated by the estimation framework.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T2.4" style="width:433.6pt;height:101.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(24.2pt,-5.6pt) scale(1.12537617191868,1.12537617191868) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.1.1.1.1" style="padding-left:53.9pt;padding-right:53.9pt;">                 <span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1.1">Actual Biofouling</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.1.1.1.2" style="padding-left:53.9pt;padding-right:53.9pt;">                 <span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.2.1">Estimated Biofouling</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.2.1.1" style="padding-left:53.9pt;padding-right:53.9pt;">                 22.00%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.2.1.2" style="padding-left:53.9pt;padding-right:53.9pt;">                 16.00%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.3.2.1" style="padding-left:53.9pt;padding-right:53.9pt;">                 33.00%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.3.2.2" style="padding-left:53.9pt;padding-right:53.9pt;">                 32.19%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.4.3.1" style="padding-left:53.9pt;padding-right:53.9pt;">                 44.00%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.1.4.3.2" style="padding-left:53.9pt;padding-right:53.9pt;">                 41.02%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.4.1.5.4.1" style="padding-left:53.9pt;padding-right:53.9pt;">                 66.00%</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T2.4.1.5.4.2" style="padding-left:53.9pt;padding-right:53.9pt;">                 65.48%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS1.4.1.1">V-D</span>1 </span>Framework Filters Results Using UNet for Semantic Segmentation</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">The two filter methods implemented were the exclusion of footage during non-uniform movement, and the contour filtering based on size and shape of detected contours.
The difference to the estimated percentage that the filters made can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F14" title="Figure 14 ‣ V-D1 Framework Filters Results Using UNet for Semantic Segmentation ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">14</span></a>.
The apparently small change is due to the fact that the autonomous filming worked well in a sense that the ROV’s speed was consistent throughout the mission and the angle of the filming was good.
Each area of the net is filmed for around the same amount of time, and the neural network semantic segmentation model performed well, so the benchmark result without filtering was close to correct from the start.
When the filming conditions are not perfect, such as in the repeated 66% coverage scenario tested in Biograd, then the filters help out.
The filming conditions were purposefully worsened by having a greater distance from the ROV to the net during filming.
The combination of both filters reduced the estimation error by 1.75% in total, as can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.T3" title="Table III ‣ V-D1 Framework Filters Results Using UNet for Semantic Segmentation ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F14"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="273" id="S5.F14.g1" src="extracted/5867909/figures/pogreska_filteri.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F14.2.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S5.F14.3.2" style="font-size:90%;">Bar plot showing the error in the estimated percentage of biofouling.</span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">Table III</span>: </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Table showing the actual simulated biofouling percentage, and the estimated biofouling percentage generated by the estimation framework with the combinations of the footage and computer vision filters turned on.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4" style="width:433.6pt;height:119.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.4pt,-5.6pt) scale(1.10370977607614,1.10370977607614) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.4.1.1.1.1" style="padding-left:25.5pt;padding-right:25.5pt;"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.2" style="padding-left:25.5pt;padding-right:25.5pt;">        <span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.2.1">Value</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.3" style="padding-left:25.5pt;padding-right:25.5pt;">        <span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.3.1">Biofouling Estim. Error</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.1.2.1.1" style="padding-left:25.5pt;padding-right:25.5pt;">        Actual biofouling</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.2.1.2" style="padding-left:25.5pt;padding-right:25.5pt;">        66.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.2.1.3" style="padding-left:25.5pt;padding-right:25.5pt;">        /</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.1.3.2.1" style="padding-left:25.5pt;padding-right:25.5pt;">        No filter estim.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.2.2" style="padding-left:25.5pt;padding-right:25.5pt;">        48.04%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.3.2.3" style="padding-left:25.5pt;padding-right:25.5pt;">        17.96%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.1.4.3.1" style="padding-left:25.5pt;padding-right:25.5pt;">        Contour filter estim.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.4.3.2" style="padding-left:25.5pt;padding-right:25.5pt;">        49.44%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.4.3.3" style="padding-left:25.5pt;padding-right:25.5pt;">        16.56%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.4.1.5.4.1" style="padding-left:25.5pt;padding-right:25.5pt;">        Movement filter estim.</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.5.4.2" style="padding-left:25.5pt;padding-right:25.5pt;">        48.98%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.1.5.4.3" style="padding-left:25.5pt;padding-right:25.5pt;">        17.02%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T3.4.1.6.5.1" style="padding-left:25.5pt;padding-right:25.5pt;">        Combined filter estim.</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.1.6.5.2" style="padding-left:25.5pt;padding-right:25.5pt;">        49.79%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.4.1.6.5.3" style="padding-left:25.5pt;padding-right:25.5pt;">        16.21%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS2.4.1.1">V-D</span>2 </span>Effect of Filming Conditions</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">An issue arose during a control test where the ROV’s heading angle was fixed, and a clean net with no biofouling patches was filmed.
Despite the net being clean, the estimation incorrectly reported 32.37% biofouling due to several factors: the net’s slight convexity and waviness (as it was not tightly strung), the ROV’s heading potentially drifting due to an imperfectly calibrated compass, and filming from a tilted angle.
Furthermore, one of the factors during filming is the time of day and the position of the Sun.
The used Blueye Pro ROV does not have an HDR camera, meaning that an overexposure of one part of the camera sensor to light ruins the rest of the image.
This effect poses a problem when filming near the surface.
Filming missions should be planned accordingly, holding them early in the morning or late afternoon, or filming the cages with the Sun behind the camera.
In addition, it goes without saying that the filming distance greatly impacts image quality and the estimation process as a whole.
Filming should be done at a distance where the the net can be clearly separated from the background, i.e., the edges should be sharp and easily discernible.
As mentioned earlier in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.SS2" title="V-B Image Labeling Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>, the sea floor could have a big impact on the computer vision component of the framework.
Having the sea floor visible increases the already difficult challenge of accurate semantic image segmentation.
Luckily, the scenario is unlikely because fish farms should be situated 3km away from shore and have 50 meters of depth available to limit the environmental impact, as mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib18" title="">18</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS4.SSS3.4.1.1">V-D</span>3 </span>Choosing the UNet Architecture Instead of Logistical Regression</h4>
<div class="ltx_para" id="S5.SS4.SSS3.p1">
<p class="ltx_p" id="S5.SS4.SSS3.p1.1">It is important to note that the results seen so far were achieved using the trained UNet architecture model for semantic image segmentation.
While logistical regression initially seemed promising for semantic segmentation (previous research done in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib13" title="">13</a>]</cite>), its limitations became evident during testing in Biograd.
It quickly became apparent that the method is not robust enough.
Small changes in lighting conditions completely threw off the segmentation which then produced unusable results.
Adding the footage captured in Biograd into the training dataset does improve estimation results, but at the same time it degrades the quality of segmentation from footage captured in previous years.
Still, the biofouling estimation algorithm was run with both the old and new (added footage from Biograd) logistical regression models, and the results can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.T4" title="Table IV ‣ V-D3 Choosing the UNet Architecture Instead of Logistical Regression ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">IV</span></a>.
All of the estimated percentages in the table are generated by the framework without using any of the filters mentioned before.
It is apparent that using logistical regression not trained on new footage produces much more inaccurate results than the other two models in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F15" title="Figure 15 ‣ V-D3 Choosing the UNet Architecture Instead of Logistical Regression ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">15</span></a>.
Training a logistical regression model with new footage does improve performance, but at the cost of overtraining which can be seen by poor performance for the highest biofouling scenario.
The results for that particular test are worse because the filming conditions are different in a sense that the previous three filming missions were carriedo ut on Wednesday afternoon, and the 66% biofouling coverage filming was done the next day on Thursday morning.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.2.1.1" style="font-size:90%;">Table IV</span>: </span><span class="ltx_text" id="S5.T4.3.2" style="font-size:90%;">Table showing the average estimate of biofouling percentage when using the footage filters, for each semantic segmentation model.</span></figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T4.4" style="width:433.6pt;height:113.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.9pt,-0.2pt) scale(1.00429290145792,1.00429290145792) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.4.1.1.1.1" style="padding-left:16.9pt;padding-right:16.9pt;">     <span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.1.1">Actual Biofouling</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.4.1.1.1.2" style="padding-left:16.9pt;padding-right:16.9pt;">     
<table class="ltx_tabular ltx_align_middle" id="S5.T4.4.1.1.1.2.1">
<tr class="ltx_tr" id="S5.T4.4.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.1.1.1.2.1.1.1" style="padding-left:16.9pt;padding-right:16.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.2.1.1.1.1">     Log. Reg. Estim.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.1.1.1.2.1.2.1" style="padding-left:16.9pt;padding-right:16.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.2.1.2.1.1">     (Old Footage)</span></td>
</tr>
</table></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.4.1.1.1.3" style="padding-left:16.9pt;padding-right:16.9pt;">     
<table class="ltx_tabular ltx_align_middle" id="S5.T4.4.1.1.1.3.1">
<tr class="ltx_tr" id="S5.T4.4.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.1.1.1.3.1.1.1" style="padding-left:16.9pt;padding-right:16.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.3.1.1.1.1">     Log. Reg. Estim.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.4.1.1.1.3.1.2.1" style="padding-left:16.9pt;padding-right:16.9pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.3.1.2.1.1">     (New Footage)</span></td>
</tr>
</table></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.4.1.1.1.4" style="padding-left:16.9pt;padding-right:16.9pt;">     <span class="ltx_text ltx_font_bold" id="S5.T4.4.1.1.1.4.1">UNet</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.2.1.1" style="padding-left:16.9pt;padding-right:16.9pt;">     22%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.2.1.2" style="padding-left:16.9pt;padding-right:16.9pt;">     29.96%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.2.1.3" style="padding-left:16.9pt;padding-right:16.9pt;">     20.99%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.2.1.4" style="padding-left:16.9pt;padding-right:16.9pt;">     16.00%</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.3.2.1" style="padding-left:16.9pt;padding-right:16.9pt;">     33%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.3.2.2" style="padding-left:16.9pt;padding-right:16.9pt;">     50.12%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.3.2.3" style="padding-left:16.9pt;padding-right:16.9pt;">     34.63%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.3.2.4" style="padding-left:16.9pt;padding-right:16.9pt;">     32.19%</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.4.3.1" style="padding-left:16.9pt;padding-right:16.9pt;">     44%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.4.3.2" style="padding-left:16.9pt;padding-right:16.9pt;">     53.49%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.4.3.3" style="padding-left:16.9pt;padding-right:16.9pt;">     41.73%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.1.4.3.4" style="padding-left:16.9pt;padding-right:16.9pt;">     41.02%</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.1.5.4.1" style="padding-left:16.9pt;padding-right:16.9pt;">     66%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.1.5.4.2" style="padding-left:16.9pt;padding-right:16.9pt;">     52.45%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.1.5.4.3" style="padding-left:16.9pt;padding-right:16.9pt;">     50.05%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.4.1.5.4.4" style="padding-left:16.9pt;padding-right:16.9pt;">     65.48%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F15"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="327" id="S5.F15.g1" src="extracted/5867909/figures/logreg_errors.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F15.2.1.1" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text" id="S5.F15.3.2" style="font-size:90%;">Bar plot showing the average absolute error in the estimated percentage of biofouling for logistical regression models trained on just old (blue bar) and combined (orange bar) data, and UNet.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.SSS3.p2">
<p class="ltx_p" id="S5.SS4.SSS3.p2.1">To sum up, using a robust UNet model for the image segmentation node in the biofouling framework produced an average absolute biofouling estimation error of just 2.54%, as can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#S5.F15" title="Figure 15 ‣ V-D3 Choosing the UNet Architecture Instead of Logistical Regression ‣ V-D Biofouling Estimation Results ‣ V Results ‣ Autonomous Visual Fish Pen Inspections for Estimating the State of Biofouling Buildup Using ROV - Extended Abstract"><span class="ltx_text ltx_ref_tag">15</span></a>.
The image segmentation is the most computationally heavy task of the framework, and the time taken to segment an image using a trained UNet model depends on the power of the onboard computer in the ASV if the estimation is to be done in real-time, or the workstation if the estimation is to be carried out after filming is complete.
Using a dedicated GPU to run the UNet model is heavily recommended because it reduces the processing time of a 960 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS4.SSS3.p2.1.m1.1"><semantics id="S5.SS4.SSS3.p2.1.m1.1a"><mo id="S5.SS4.SSS3.p2.1.m1.1.1" xref="S5.SS4.SSS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS3.p2.1.m1.1b"><times id="S5.SS4.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS3.p2.1.m1.1d">×</annotation></semantics></math> 540 resolution image from a few seconds using a CPU, to the millisecond order of magnitude using a GPU. Furthermore, using UNet for segmentation means that potentially obtaining a diverse range of footage from various fisheries filmed under different conditions could only enhance the model’s robustness and performance, whereas opting for simpler models like logistical regression would lead to overtraining when faced with varying scenarios.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In conclusion, the main contributions of this research were: (1) development of a labeling tool in order to create a curated dataset of labeled underwater HD images of fish pens, (2) development and implementation of a framework successfully used for estimating the amount of biofouling on the nets of fish pens, that incorporates a trained AI neural network model used for the task of semantic segmentation of underwater images of fish pens, and (3) development of an autonomous closed-loop control system using the available SDK for the ROV and the available localization data supplied by the retrofitted underwater positioning system.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The control loop algorithm successfully controlled the ROV in a pool setting using point-to-point navigation.
Although the scenario in which the experiment was conducted is not an exact replica of the actual conditions in a fish farm, it still demonstrates the possibility of using the ROV as an autonomous vehicle to perform inspection missions.
To achieve localization in a complex environment the control algorithm could be modified to include a three-dimensional map of the farm and also fuse SONAR measurements if such a sensor would be mounted onto the ASV, together with UWGPS and live camera footage.
The labeling tool made it possible to accurately segment and semantically label the images of fish pens at around 30 s or less per image, thus allowing us to create a dataset of more than a thousand images within a satisfactory time frame.
Perhaps most important, the implementation of the proposed framework which was tested in a controlled environment proved to be a success with the absolute value of the estimation error roughly being 2.5%.
It is also worth noting that carrying out the inspection mission in one take while keeping the velocity of the ROV almost constant throughout, without much backtracking or spending too much time filming one area in relation to the rest of the net, produces an accurate estimation.
Precisely determining filming positions to keep the overlap of footage fed to the estimation framework at a minimum might not be cost-effective to develop.
Due to the nature of the current inspection process which involves divers estimating the state of the net, a “good enough” estimation is satisfactory.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">For future endeavors, the developed framework should be tested at an industrial fishery using the original idea of pairing the ASV Korkyra with an ROV.
This field testing would aim to validate the framework’s effectiveness in a challenging environment like an industrial fish farming operation.
Furthermore, the previously developed proprietary tether management system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib11" title="">11</a>]</cite> that also relies on localization of the ROV should be integrated physically onto the ASV.
Lastly, the project also envisions an air surveillance aspect using a light autonomous drone that could take off and land from the ASV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.12813v2#bib.bib10" title="">10</a>]</cite>.
The combined heterogeneous system of robots should be tested in a real-world scenario in the future.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. Subasinghe, D. Soto, and J. Jia, “Global aquaculture and its role in sustainable development,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Reviews in aquaculture</em>, vol. 1, no. 1, pp. 2–9, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Paspalakis, K. Moirogiorgou, N. Papandroulakis, G. Giakos, and M. Zervakis, “Automated fish cage net inspection using image processing techniques,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IET Image Processing</em>, vol. 14, no. 10, pp. 2028–2034, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Betancourt, W. Coral, and J. Colorado, “An integrated rov solution for underwater net-cage inspection in fish farms using computer vision,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">SN Applied Sciences</em>, vol. 2, pp. 1–15, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Duda, J. Schwendner, A. Stahl, and P. Rundtop, “Visual pose estimation for autonomous inspection of fish pens,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">OCEANS 2015-Genova</em>.   IEEE, 2015, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Livanos, M. Zervakis, V. Chalkiadakis, K. Moirogiorgou, G. Giakos, and N. Papandroulakis, “Intelligent navigation and control of a prototype autonomous underwater vehicle for automated inspection of aquaculture net pen cages,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2018 IEEE International Conference on Imaging Systems and Techniques (IST)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W. Qiu, V. Pakrashi, and B. Ghosh, “Fishing net health state estimation using underwater imaging,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Journal of Marine Science and Engineering</em>, vol. 8, no. 9, p. 707, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Goričanec, N. Kapetanović, I. Vatavuk, I. Hrabar, G. Vasiljević, G. Gledec, D. Stuhne, S. Bogdan, M. Orsag, T. Petrović <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Heterogeneous autonomous robotic system in viticulture and mariculture-project overview,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">2021 16th international conference on telecommunications (ConTEL)</em>.   IEEE, 2021, pp. 181–188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N. Kapetanović, Đ. Nađ, and N. Mišković, “Towards a heterogeneous robotic system for autonomous inspection in mariculture,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">OCEANS 2021: San Diego–Porto</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W. Chen, and J. Wu, “Unet 3+: A full-scale connected unet for medical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2020, pp. 1055–1059.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Kapetanović, M. Oreč, N. Mišković, and F. Ferreira, “Landing platform for autonomous inspection missions in mariculture using an asv and a uav,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IFAC-PapersOnLine</em>, vol. 55, no. 31, pp. 130–135, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
N. Kapetanović, K. Krčmar, N. Mišković, and Ð. Nađ, “Tether management system for autonomous inspection missions in mariculture using an asv and an rov,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IFAC-PapersOnLine</em>, vol. 55, no. 31, pp. 327–332, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Vickery, “Acoustic positioning systems. a practical overview of current systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No. 98CH36290)</em>.   IEEE, 1998, pp. 5–17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Fabijanić, N. Kapetanović, and N. Mišković, “Biofouling estimation in mariculture,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">OCEANS 2022, Hampton Roads</em>.   IEEE, 2022, pp. 1–7.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. A. Johnson and M. H. Moradi, <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">PID control</em>.   Springer, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Bertels, T. Eelbode, M. Berman, D. Vandermeulen, F. Maes, R. Bisschops, and M. B. Blaschko, “Optimizing the dice score and jaccard index for medical image segmentation: Theory and practice,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part II 22</em>.   Springer, 2019, pp. 92–100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Holmer, “Environmental issues of fish farming in offshore waters: perspectives, concerns and research needs,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Aquaculture Environment Interactions</em>, vol. 1, no. 1, pp. 57–70, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Sanz-Lazaro, N. Casado-Coy, E. M. Calderero, and U. A. Villamar, “The environmental effect on the seabed of an offshore marine fish farm in the tropical pacific,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Journal of Environmental Management</em>, vol. 300, p. 113712, 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0301479721017746</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 09:01:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
