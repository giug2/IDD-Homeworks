<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.13473] Towards Interpretable Federated Learning</title><meta property="og:description" content="Federated learning (FL) enables multiple data owners to build machine learning models collaboratively without exposing their private local data.
In order for FL to achieve widespread adoption, it is important to balanc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Interpretable Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Interpretable Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.13473">

<!--Generated on Thu Feb 29 22:58:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Interpretable Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Anran Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rui Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ming Hu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luu Anh Tuan&amp;Han Yu<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Corresponding author</span></span></span>
School of Computer Science and Engineering, Nanyang Technological University, Singapore
<br class="ltx_break">{anran.li, rui.liu, ming.hu, anhtuan.luu, han.yu}@ntu.edu.sg
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning (FL) enables multiple data owners to build machine learning models collaboratively without exposing their private local data.
In order for FL to achieve widespread adoption, it is important to balance the need for performance, privacy-preservation and interpretability, especially in mission critical applications such as finance and healthcare.
Thus, interpretable federated learning (IFL) has become an emerging topic of research attracting significant interest from the academia and the industry alike.
Its interdisciplinary nature can be challenging for new researchers to pick up. In this paper, we bridge this gap by providing (to the best of our knowledge) the first survey on IFL. We propose a unique IFL taxonomy which covers relevant works enabling FL models to explain the prediction results, support model debugging, and provide insights into the contributions made by individual data owners or data samples, which in turn, is crucial for allocating rewards fairly to motivate active and reliable participation in FL.
We conduct comprehensive analysis of the representative IFL approaches, the commonly adopted performance evaluation metrics, and promising directions towards building versatile IFL techniques.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) has been proposed to enable multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models while preserving local data privacy <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2017</a>); Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>); Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>. Based on the distribution of local data, there are two main categories of FL scenarios: 1) horizontal federated learning (HFL), and 2) vertical federated learning (VFL). Under HFL <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>, data owners’ local datasets have little overlap in the sample space, but large overlaps in the feature space. Under VFL <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>, data owners’ local datasets have large overlaps in the sample space, but little overlap in the feature space. FL has been adopted by a wide range of applications, including next-word predictors on users’ smartphones <cite class="ltx_cite ltx_citemacro_cite">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>, smart healthcare <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2022b</a>)</cite> and Industry 4.0 <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Today’s FL models are often built on highly complex non-linear base models (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, deep neural networks (DNNs)).
This makes it difficult for FL stakeholders to understand the internal working mechanisms of the models and decision making processes of the FL training frameworks. This lack of interpretability may diminish trust for this emerging technology, hindering wider adoption.
To overcome these issues, interpretable FL (IFL) models capable of explaining the rationale behind model behaviors or provide insight into the working mechanisms of the models are in high demand, especially in mission critical applications such as finance and healthcare.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As a promising technology to enhance system safety and build trust among FL stakeholders, IFL has attracted significant research interest from the academic and the industry in recent years.
Compared to the current interpretable artificial intelligence (AI) methods designed for centralized machine learning <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, IFL is more challenging due to the invisibility of local data to outsiders and the resource constraints in terms of local computation and communication power.
It is an interdisciplinary field as it requires expertise from machine learning, optimization, cryptography and human factors in order to build viable solutions. This makes it challenging for researchers new to the field to grasp the latest development. Currently, there is no survey paper on this important and rapidly developing topic.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To bridge this gap, we provide (to the best of our knowledge) the first survey of the IFL literature in this paper. We propose a unique IFL taxonomy which covers highly relevant works enabling FL models to explain the prediction results, support model debugging, and provide insights into the contributions made by individual data owners or data samples, which in turn, is crucial for allocating rewards fairly to motivate active and reliable participation in FL.
We conduct comprehensive analysis of the representative IFL approaches, the commonly adopted performance evaluation metrics, and promising directions towards building versatile interpretable federated learning technologies.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The Proposed IFL Taxonomy</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we provide an overview of the proposed IFL taxonomy, discussing its structure from the perspectives of the stakeholders and the need for privacy protection.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2302.13473/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed taxonomy for the IFL literature.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Stakeholders Analysis</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In a typical FL system, there are two types of direct stakeholders: 1) the FL server, and 2) FL clients. They are directly involved in the FL training processes. In general, under the coordination of the FL server, clients collaboratively train an FL model by sharing their local models (in a variety of forms) trained on their local datasets. Apart from them, there could be indirect stakeholders involved in IFL who may be interested in obtaining explanations regarding the FL model or the FL training process. These may include researchers and developers, regulatory agencies, policymakers and civil societies, <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>.
Stakeholders may require different functionalities with regards to IFL.
For instance, the FL server may need to know why a client has selected the specific data samples or features for a given FL task, and the rationale behind a specific prediction by the FL model.
The FL clients may need to know the basis on which they are being selected or excluded by the server for a given FL task, as well as the rationale behind allocating them a certain reward for their efforts.
Researchers and developers may be interested to know under what conditions an FL model might fail so as to help with debugging. Regulatory agencies, policymakers and civil societies may be interested to know how well a given FL training framework complies with the current regulations. The diverse needs of the stakeholders require different IFL techniques.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Privacy Protection Analysis</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy Protection Targets:</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">In FL, the invisibility of local data is key to protecting privacy. On the other hand, it makes achieving interpretability a challenge.
The diverse stakeholder needs also require different data privacy protection targets.
For explaining to the FL server, the interpretable models should preserve each participant’s data privacy. That is: 1) the local training data and their distributions should not be exposed to any party other than their original owners; and 2) the local data cannot be obtained or inferred by any party other than their original owners.
For explaining to the FL clients, in addition to the above two privacy protection requirements, IFL should also protect a client’s labels from being exposed to or inferred by any party other than the FL server under VFL.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Threat Models:</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">Existing IFL works are often based on the following threat models. 1) <em id="S2.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Semi-honest FL participants</em>: they follow the FL training protocol (<em id="S2.SS2.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, truthfully upload their local model parameters, do not collude with one another), but try to infer other clients’ private information.
2) <em id="S2.SS2.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">Malicious FL participants</em>: the adversary can compromise FL clients and manipulate the local models during the learning process in order to compromise the global FL model to fulfil their ulterior goals.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy Protection Techniques:</h5>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">Following the above threat models, existing IFL works generally adopt the following privacy protection techniques: differential privacy (DP), homomorphic encryption (HE), and secure multiparty computation (MPC).
For instance, FLDebugger <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2021b</a>)</cite> designs two DP-based influential sample identification methods to determine the impacts of individual training samples while preserving the privacy of clients’ training data. These methods leverage the clip-based approach to bound the added noise and achieve identification performance comparable to the noise-free version.
We summarize the privacy protection targets, threat models and privacy protection techniques adopted by IFL in the proposed taxonomy.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>IFL Taxonomy Structure</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Based on the stakeholder and privacy protection analysis while considering the FL training process, we propose a taxonomy for the IFL literature as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 The Proposed IFL Taxonomy ‣ Towards Interpretable Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The taxonomy first identifies IFL approaches adopted by the client selection stage, sample selection stage, feature selection stage, model optimization stage and contribution evaluation stage.
Then, it further differentiates various techniques for achieving IFL in the above stages, and highlights the stakeholders, threat models, privacy protection targets and techniques, as well as the evaluation metrics adopted by each of them to provide a concise overview of current IFL research.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>IFL Approaches</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Interpretable Client Selection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The performance of the global FL model largely depends on the quality of the local data.
IFL client selection helps the FL server understand FL model behaviours by tracing back to the distributed training datasets to identify and select clients holding high-quality data and are important to model aggregation.
It can be achieved through: 1) importance-based techniques, and 2) influence-based techniques.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Importance-based Techniques</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">These methods attempt to provide insights into the global FL model by selecting important or representative clients in each training round. There are four main approaches for calculating client importance.</p>
</div>
<section id="S3.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Model Deviation-based Methods:</h5>

<div id="S3.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px1.p1.1" class="ltx_p">Based on the observation that local model updates from clients with noisy samples are significantly larger than normal, model deviation-based methods have been proposed to provide interpretations on the quality of local datasets.
For FL with semi-honest participants, the deviations of the local updates from the global updates are leveraged to identify negatively influential clients with noisy samples <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2021c</a>)</cite>.
For FL with malicious participants,
<cite class="ltx_cite ltx_citemacro_cite">Blanchard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite> proposed the Krum aggregation method which selects local models similar to other local models (<em id="S3.SS1.SSS1.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, with the smallest sum of Euclidean distance) as the global model. However, Euclidean distance between two local models can be significantly influenced by a single model parameter, which can mislead Krum. To address this issue, <cite class="ltx_cite ltx_citemacro_cite">Guerraoui <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite> proposed Bulyan which combines Krum and a variant of the trimmed mean <cite class="ltx_cite ltx_citemacro_cite">Yin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib50" title="" class="ltx_ref">2018</a>)</cite> to identify and aggregate high quality FL model updates received from the clients.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Loss-based Methods:</h5>

<div id="S3.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px2.p1.1" class="ltx_p">Although model deviation-based methods provide decent explanations on the quality of local datasets, they cannot reflect the dynamic changes in client importance during the process of FL model training.
Loss-based methods employ local losses <cite class="ltx_cite ltx_citemacro_cite">Cho <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> or loss-based utilities <cite class="ltx_cite ltx_citemacro_cite">Lai <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>); Goetz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite> to quantify dynamically changing client importance. The loss of a client is calculated by summing up the losses of the data samples belonging to this client. If it is larger than a threshold, the client is considered to be important and selected to participate in FL training. This approach is efficient for a client with limited resources to calculate. However, it still does not provide an accurate measure of client importance.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Gradient Norm-based Methods:</h5>

<div id="S3.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px3.p1.1" class="ltx_p">Gradient norm-based methods provide more accurate client importance measurements compared to loss-based methods. In <cite class="ltx_cite ltx_citemacro_cite">Katharopoulos and
Fleuret (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>, the gradient norm of each sample of a client is computed and added up to obtain the gradient norm of the client. However, it is prohibitively expensive for a client to compute. Thus, <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2021a</a>)</cite> leverages the gradient upper bound norm to trade off between approximation accuracy and efficiency.</p>
</div>
</section>
<section id="S3.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attention-based Methods:</h5>

<div id="S3.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS1.Px4.p1.1" class="ltx_p">This approach aims to dynamically learn the client importance during FL model training without prior design of importance metrics. In <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2021b</a>)</cite>, a data-driven approach was proposed by introducing an attention mechanism to measure client importance. The probability distribution for client selection is adjusted based on real-time local training performance. However, it incurs high computation and communication overhead for model training due to the additional parameters to be trained.</p>
</div>
</section>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Influence-based Techniques</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Influence-based methods aim to identify the impact of clients’ datasets on FL model predictions. They can be divided in to two categories. The first category <cite class="ltx_cite ltx_citemacro_cite">Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>); Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> perturbs or removes FL clients or their training samples to retrain the FL model. Then, the difference in performance between the new model and the original one is used to measure client influence. These methods are useful when the local datasets are similar in size and uniformly distributed. However, they become unstable in more complex FL scenarios with datasets of varied sizes and uneven distributions. Besides, since they require retraining on all clients’ datasets, the evaluation process can be very expensive.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.7" class="ltx_p">To avoid the expensive retraining,
influence function methods have been proposed <cite class="ltx_cite ltx_citemacro_cite">Koh and Liang (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>. They use the second-order optimization technique, and generally remain accurate even as the underlying assumptions of differentiability and convexity are not holding. A straightforward method to employ
influence functions in IFL is Fed-influence <cite class="ltx_cite ltx_citemacro_cite">Xue <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>. It measures the influence of a client by summing up the influence values of all its samples since the influence function has an additive property when measuring changes in test predictions <cite class="ltx_cite ltx_citemacro_cite">Koh <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. However, this method requires participants to directly calculate and transmit the Hessian matrix, which incurs large computation overhead (<em id="S3.SS1.SSS2.p2.7.1" class="ltx_emph ltx_font_italic">e.g.</em>, <math id="S3.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="O(p^{3}n)" display="inline"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mrow id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.1.m1.1.1.3" xref="S3.SS1.SSS2.p2.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.1.m1.1.1.2" xref="S3.SS1.SSS2.p2.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p2.1.m1.1.1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.2" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.cmml"><msup id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.2" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml">p</mi><mn id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.3" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml">3</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.3" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><apply id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1"><times id="S3.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.2"></times><ci id="S3.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.3">𝑂</ci><apply id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1"><times id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.1"></times><apply id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.2">𝑝</ci><cn type="integer" id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.2.3">3</cn></apply><ci id="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">O(p^{3}n)</annotation></semantics></math> computing operations, where <math id="S3.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS1.SSS2.p2.2.m2.1a"><mi id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.1b"><ci id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.1c">p</annotation></semantics></math> is the size of model parameters and <math id="S3.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.SSS2.p2.3.m3.1a"><mi id="S3.SS1.SSS2.p2.3.m3.1.1" xref="S3.SS1.SSS2.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.3.m3.1b"><ci id="S3.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.3.m3.1c">n</annotation></semantics></math> is the number of total training samples) and communication overhead (<em id="S3.SS1.SSS2.p2.7.2" class="ltx_emph ltx_font_italic">e.g.</em>, <math id="S3.SS1.SSS2.p2.4.m4.1" class="ltx_Math" alttext="O(p^{2}k)" display="inline"><semantics id="S3.SS1.SSS2.p2.4.m4.1a"><mrow id="S3.SS1.SSS2.p2.4.m4.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS1.SSS2.p2.4.m4.1.1.3" xref="S3.SS1.SSS2.p2.4.m4.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.4.m4.1.1.2" xref="S3.SS1.SSS2.p2.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p2.4.m4.1.1.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.2" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.cmml"><msup id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.2" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.2.cmml">p</mi><mn id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.3" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.3.cmml">k</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.3" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.4.m4.1b"><apply id="S3.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1"><times id="S3.SS1.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.2"></times><ci id="S3.SS1.SSS2.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.3">𝑂</ci><apply id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1"><times id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.1"></times><apply id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.2">𝑝</ci><cn type="integer" id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.2.3">2</cn></apply><ci id="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.4.m4.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.4.m4.1c">O(p^{2}k)</annotation></semantics></math> computing operations, where <math id="S3.SS1.SSS2.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.SSS2.p2.5.m5.1a"><mi id="S3.SS1.SSS2.p2.5.m5.1.1" xref="S3.SS1.SSS2.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.5.m5.1b"><ci id="S3.SS1.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.5.m5.1c">k</annotation></semantics></math> is the number of clients).
To reduce the cost of influence calculation, <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2021c</a>, <a href="#bib.bib24" title="" class="ltx_ref">b</a>)</cite> leverage the Hessian vector product (HVP) to approximate the influence values, reducing the computation and communication costs to <math id="S3.SS1.SSS2.p2.6.m6.1" class="ltx_Math" alttext="O(np)" display="inline"><semantics id="S3.SS1.SSS2.p2.6.m6.1a"><mrow id="S3.SS1.SSS2.p2.6.m6.1.1" xref="S3.SS1.SSS2.p2.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p2.6.m6.1.1.3" xref="S3.SS1.SSS2.p2.6.m6.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.6.m6.1.1.2" xref="S3.SS1.SSS2.p2.6.m6.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p2.6.m6.1.1.1.1" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.2" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.2" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.1" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.3" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml">p</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.3" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.6.m6.1b"><apply id="S3.SS1.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1"><times id="S3.SS1.SSS2.p2.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.2"></times><ci id="S3.SS1.SSS2.p2.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.3">𝑂</ci><apply id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1"><times id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.6.m6.1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.6.m6.1c">O(np)</annotation></semantics></math> and <math id="S3.SS1.SSS2.p2.7.m7.1" class="ltx_Math" alttext="O(pk)" display="inline"><semantics id="S3.SS1.SSS2.p2.7.m7.1a"><mrow id="S3.SS1.SSS2.p2.7.m7.1.1" xref="S3.SS1.SSS2.p2.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p2.7.m7.1.1.3" xref="S3.SS1.SSS2.p2.7.m7.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.7.m7.1.1.2" xref="S3.SS1.SSS2.p2.7.m7.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS2.p2.7.m7.1.1.1.1" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.2" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.2" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.1" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.3" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.3.cmml">k</mi></mrow><mo stretchy="false" id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.3" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.7.m7.1b"><apply id="S3.SS1.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1"><times id="S3.SS1.SSS2.p2.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.2"></times><ci id="S3.SS1.SSS2.p2.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.3">𝑂</ci><apply id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1"><times id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p2.7.m7.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.7.m7.1c">O(pk)</annotation></semantics></math>, respectively.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Interpretable Sample Selection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In FL systems with large variety of data owned by the clients, training data may not be equally important for a given FL task <cite class="ltx_cite ltx_citemacro_cite">Katharopoulos and
Fleuret (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>. On one hand, it is likely that only a subset of local data from a client are relevant for the learning task, while the rest might negatively impact model training. On the other hand, among the relevant data from a client, knowledge embedded within some samples might have been extracted after some training rounds. Thus, they can be ignored afterwards without affecting final model performance.
IFL client selection models treat all training samples of each client equally, which leads to potential waste of local computation and communication resources, and slows down model convergence.
Therefore, IFL sample selection methods have been proposed for the server and clients to interpret the usefulness of local data in order to improve training efficiency and model performance.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Relevance-based Techniques</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Techniques have been proposed to select the subset of data samples relevant to the given FL task before starting the learning process. They usually leverage a benchmark model trained on a small benchmark dataset that is task-specific to evaluate the relevance of individual samples, and select those with high relevance.
Then, clients only use the selected subset of data samples in the FL model training process <cite class="ltx_cite ltx_citemacro_cite">Tuor <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>. In practice, the benchmark models are often hard to find, which limits the applicability of such techniques.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Importance-based Techniques</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Existing importance-based IFL sample selection methods can be divided into two categories. The first evaluates sample importance based on losses. FedBalancer <cite class="ltx_cite ltx_citemacro_cite">Shin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> regards samples with losses exceeding a threshold as more important samples when training the current FL model, and prioritizes them in sample selection. However, this method cannot handle erroneous samples which can also have significantly larger losses than the correct samples.
The second leverages the gradient norm upper bound to quantify sample importance.
For a given FL task under a budget, the server iteratively selects a subset of the most important clients, which in turn, select important local samples to build their training batches <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2021a</a>)</cite>. To mitigate the impact of erroneous samples, a threshold (<em id="S3.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, the median gradient norm of samples) is often adopted to filter outliers.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Influence-based Techniques</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.6" class="ltx_p">Influence-based IFL sample selection methods have been proposed for clients to determine the impacts of individual local training samples on model predictions. Influence function methods are commonly used in determining how the model parameters change when a training point is perturbed <cite class="ltx_cite ltx_citemacro_cite">Koh and Liang (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>. However, in large-scale FL systems, considering the large <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">n</annotation></semantics></math> and <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">p</annotation></semantics></math> in deep neural models, directly calculating influence values for all training samples will incur prohibitively high computation overhead (<em id="S3.SS2.SSS3.p1.6.1" class="ltx_emph ltx_font_italic">e.g.</em>, <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="O(np^{2}+p^{3})" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mrow id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.3.m3.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS2.SSS3.p1.3.m3.1.1.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.cmml"><mrow id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.2.cmml">p</mi><mn id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.1.cmml">+</mo><msup id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.2.cmml">p</mi><mn id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.3.cmml">3</mn></msup></mrow><mo stretchy="false" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><apply id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1"><times id="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.2"></times><ci id="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1"><plus id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.1"></plus><apply id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2"><times id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.1"></times><ci id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.2">𝑛</ci><apply id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.2">𝑝</ci><cn type="integer" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.2.3.3">2</cn></apply></apply><apply id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.2">𝑝</ci><cn type="integer" id="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.1.1.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">O(np^{2}+p^{3})</annotation></semantics></math> operations) and communication overhead (<em id="S3.SS2.SSS3.p1.6.2" class="ltx_emph ltx_font_italic">e.g.</em>, <math id="S3.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="O(kp^{2}+np)" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><mrow id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.4.m4.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS2.SSS3.p1.4.m4.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.cmml"><mrow id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.2.cmml">p</mi><mn id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.3.cmml">p</mi></mrow></mrow><mo stretchy="false" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><apply id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1"><times id="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2"></times><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1"><plus id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.1"></plus><apply id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2"><times id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.1"></times><ci id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.2">𝑘</ci><apply id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.2">𝑝</ci><cn type="integer" id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.2.3.3">2</cn></apply></apply><apply id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3"><times id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.2">𝑛</ci><ci id="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.1.1.3.3">𝑝</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">O(kp^{2}+np)</annotation></semantics></math> cost).
Thus, existing works adopt efficient influence approximation methods. One category <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2021c</a>, <a href="#bib.bib24" title="" class="ltx_ref">b</a>)</cite> leverages HVP approximation methods, which reduce computation and communication overhead to <math id="S3.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="O(np)" display="inline"><semantics id="S3.SS2.SSS3.p1.5.m5.1a"><mrow id="S3.SS2.SSS3.p1.5.m5.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS3.p1.5.m5.1.1.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.5.m5.1.1.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml">​</mo><mrow id="S3.SS2.SSS3.p1.5.m5.1.1.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.3.cmml">p</mi></mrow><mo stretchy="false" id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m5.1b"><apply id="S3.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1"><times id="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.2"></times><ci id="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1"><times id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m5.1c">O(np)</annotation></semantics></math> and <math id="S3.SS2.SSS3.p1.6.m6.1" class="ltx_Math" alttext="O(pk)" display="inline"><semantics id="S3.SS2.SSS3.p1.6.m6.1a"><mrow id="S3.SS2.SSS3.p1.6.m6.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS3.p1.6.m6.1.1.3" xref="S3.SS2.SSS3.p1.6.m6.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.6.m6.1.1.2" xref="S3.SS2.SSS3.p1.6.m6.1.1.2.cmml">​</mo><mrow id="S3.SS2.SSS3.p1.6.m6.1.1.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.2" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.3.cmml">k</mi></mrow><mo stretchy="false" id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.3" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m6.1b"><apply id="S3.SS2.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1"><times id="S3.SS2.SSS3.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.2"></times><ci id="S3.SS2.SSS3.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1"><times id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m6.1c">O(pk)</annotation></semantics></math>, respectively. Another category <cite class="ltx_cite ltx_citemacro_cite">Rokvic <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite> utilizes the sign of the influence value rather than the exact influence value to measure the influence of the training samples (based on the observation that a positive influence value indicates that a data sample has a positive impact on the prediction; and vice-versa). However, this method suffers from large approximation errors when the percentage of noisy data increases.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Interpretable Feature Selection</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The quality of clients’ local features determines the effectiveness of their local models, which in turn, affect the performance of the global FL model.
In practice, clients can possess noisy features that are irrelevant to the learning task, or a large number of redundant features which might result in model performance degradation and excessive parameter transmission. Thus, the interpretation of features (<em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, identifying noisy features and important features) is vital for FL. In addition, it can also provide insight into the internal working of FL models. We divide IFL feature selection approaches into two categories: 1) model-agnostic techniques, and 2) model-specific techniques.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Model-Agnostic Techniques</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Model-agnostic IFL feature selection approaches treat an FL model as a black-box and do not inspect the model parameters. It aims to measure the relevance of each feature to the learning task and discard the irrelevant ones.
It can be achieved through both supervised and unsupervised interpretation methods.</p>
</div>
<section id="S3.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Supervised Interpretation:</h5>

<div id="S3.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.Px1.p1.1" class="ltx_p">Supervised interpretation methods calculate per-feature relevance scores based on statistical measures (<em id="S3.SS3.SSS1.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, mutual information, Gini-impurity, F-statistics).
<cite class="ltx_cite ltx_citemacro_cite">Cassará <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> proposed to iteratively identify redundant or irrelevant features in a distributed manner without exchanging any raw data. It builds on two components, a mutual information-based feature selection algorithm executed by the clients, and an aggregation function based on the Bayes theorem executed by the server.
In <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2021d</a>)</cite>, an MPC-based protocol was proposed for private feature scoring through Gini impurity, which can improve prediction accuracy while reducing model complexity.
Similarly, <cite class="ltx_cite ltx_citemacro_cite">Pan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite> proposed an MPC-based protocol for private feature relevance estimation through F-statistics to perform feature selection for VFL.
They both follow the malicious threat model in which there is an adversary that corrupts no more than half of the participants.</p>
</div>
</section>
<section id="S3.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Unsupervised Interpretation:</h5>

<div id="S3.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS1.Px2.p1.1" class="ltx_p">Since clients’ data are not always labeled, unsupervised interpretation methods for IFL feature selection have been proposed. In <cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, a feature average relevance one-class support vector machine, Far-ocsvm, was proposed to detect outlier features. This is followed by a feature relevance hierarchical clustering step to gather representative features. By using a variable threshold algorithm, Far-ocsvm can handle the non-IID problem.</p>
</div>
</section>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Model-Specific Techniques</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.4" class="ltx_p">Model-specific techniques treat the FL models as white-boxes, and explicitly utilize the structure and intermediate parameters of the FL model to generates explanations.
The least absolute shrinkage and selection operator (LASSO) is a well-known embedded feature selection method, with the goal of minimizing the loss while enforcing an <math id="S3.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="l_{1}" display="inline"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml">l</mi><mn id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">l_{1}</annotation></semantics></math> constraint on the weights of the features. However, LASSO is restricted to the domain of linear functions and suffers from shrinkage of model parameters. In <cite class="ltx_cite ltx_citemacro_cite">Feng (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, the <math id="S3.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS3.SSS2.p1.2.m2.1a"><msub id="S3.SS3.SSS2.p1.2.m2.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.1.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.1.2.cmml">l</mi><mn id="S3.SS3.SSS2.p1.2.m2.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.1b"><apply id="S3.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.1c">l_{2}</annotation></semantics></math> constraints are leveraged on feature weights in combination with an auto-encoder to select important features for deep VFL models. However, the <math id="S3.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS3.SSS2.p1.3.m3.1a"><msub id="S3.SS3.SSS2.p1.3.m3.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.1.1.2" xref="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml">l</mi><mn id="S3.SS3.SSS2.p1.3.m3.1.1.3" xref="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.1b"><apply id="S3.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.1c">l_{2}</annotation></semantics></math> constraint still suffers from shrinkage of model parameters, and requires post-training thresholds for useful features to be selected. To solve these issues, <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">li2023fedsdg</span></cite> proposed
a federated feature selection approach, FedSDG-FS, which consists of a Gaussian stochastic dual-gate based on the <math id="S3.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="l_{0}" display="inline"><semantics id="S3.SS3.SSS2.p1.4.m4.1a"><msub id="S3.SS3.SSS2.p1.4.m4.1.1" xref="S3.SS3.SSS2.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS2.p1.4.m4.1.1.2" xref="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml">l</mi><mn id="S3.SS3.SSS2.p1.4.m4.1.1.3" xref="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m4.1b"><apply id="S3.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m4.1c">l_{0}</annotation></semantics></math> constraints to efficiently approximate the probability of a feature being selected.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Interpretable Model Optimization</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the context of IFL, interpretability in model optimization can be achieved by designing inherently interpretable models or robust aggregation methods. The interpretable models directly incorporate interpretability into the model structures (either globally interpretable or providing interpretable individual predictions). Interpretable robust aggregation enables the FL server to understand the quality of clients’ updates to perform quality-aware model aggregation.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Constructing Inherently Interpretable Models</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Interpretable models can be constructed in two main ways: 1) constructing self-explanatory models (<em id="S3.SS4.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, tree-based models and linear models, instead of complex and opaque models); and 2) training models using data with interpretability constraints. In general, the more interpretable a model is the less accurate it tends to become.
Thus, trade-offs between predictive accuracy and interpretability must be made when constructing self-explanatory models or incorporating interpretability constraints into the models.</p>
</div>
<section id="S3.SS4.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Constructing Self-Explanatory Models:</h5>

<div id="S3.SS4.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px1.p1.1" class="ltx_p">Self-explanatory models, such as decision trees or random forests, can help enhance the efficiency and scalability of IFL. Under the HFL scenario, FedForest <cite class="ltx_cite ltx_citemacro_cite">Dong <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> leverages the Gradient Boosting Decision Tree (GBDT) model as the core classification algorithm, which is interpretable and efficient compared with neural networks (NNs).
In <cite class="ltx_cite ltx_citemacro_cite">Imakura <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>, an interpretable non-model sharing collaborative data analysis framework was built based on intermediate representations generated from individual local data samples. However, it assumes the availability of a shared public anchor dataset, which might not be always possible to find in practice.</p>
</div>
<div id="S3.SS4.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.Px1.p2.1" class="ltx_p">Under the VFL scenario, state-of-the-art frameworks employ anonymous features to avoid possible data breaches. However, this negatively impacts model interpretability <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>.
To address this issue in the inference process, <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2021a</a>)</cite> first observed that it is possible to express the prediction results of a tree as the intersection of results of sub-models of the tree held by all FL participants. Based on this observation, they proposed a method to protect data privacy while allowing the disclosure of the meaning of the features by concealing the decision paths.</p>
</div>
</section>
<section id="S3.SS4.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Adding Interpretability Constraints:</h5>

<div id="S3.SS4.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px2.p1.1" class="ltx_p">Alternatively, the interpretability of an FL model can be enhanced by incorporating interpretability constraints. In <cite class="ltx_cite ltx_citemacro_cite">Tong <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>, a method was proposed to enforce sparsity terms in order to reduce the number of features used for prediction, thereby improving interpretability and reducing computation cost for resource constrained clients. Other works impose semantic constraints on FL models to further improve interpretability.
For instance, the interpretable federated NNs <cite class="ltx_cite ltx_citemacro_cite">Roschewitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> add the element-wise learned normalization layer to extract feature-wise interoperability information to enhance the detection and understanding of inter-client compatibility issues.</p>
</div>
</section>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Interpretable Robust Aggregation Techniques</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Robust aggregation-based IFL can be divided into two categories based on the threat models they are designed to handle. For semi-honest FL participants, existing works usually adopt incentive-based methods.
In <cite class="ltx_cite ltx_citemacro_cite">Kang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, the authors proposed an incentive mechanism that combines client reputation with contract theory to assign higher weights to high-quality updates, while motivating high-reputation clients to participate in FL.
However, it makes assumptions that the server has knowledge about clients’ data quality and computation resources to enable the server to design high-paying contracts only for high-quality clients. Such information is difficult to reliably obtain in the context of FL.
To address this limitation, <cite class="ltx_cite ltx_citemacro_cite">Pandey <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>); Zhan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite> proposed Stackelberg game-based incentive mechanisms, in which the server allocates rewards to the clients with the goal of achieving optimal local accuracy; while each client individually maximizes its own rewards subject to cost constraints. Nevertheless, these methods can only work when the local data samples are IID.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">For malicious FL participants, the general approach attempts to assign lower aggregating weights to outlier clients. In <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite>, an aggregation method that mitigates the influence of Byzantine clients was proposed to assign lower weights to such clients. The robustness of the Byzantine client estimator in this method was also analyzed with influence values based on the observation that lower influence values correspond to stronger resistance against outliers.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Interpretable Contribution Evaluation</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">IFL contribution evaluation can be achieved by assessing the clients’ contributions to the performance of the final FL model, and assessing the contributions of the features towards a specific model prediction.
The former is related to IFL client selection research, as the results of IFL client contribution evaluation is often used as a basis for incentivizing clients to participate in FL training and updating their reputations in preparation for future rounds of client selection.</p>
</div>
<section id="S3.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Client Contribution Evaluation Techniques</h4>

<section id="S3.SS5.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Utility Game-based Interpretation:</h5>

<div id="S3.SS5.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS5.SSS1.Px1.p1.1" class="ltx_p">Utility game-based FL client contribution evaluation measures the change in coalition utility when clients join. In utility games <cite class="ltx_cite ltx_citemacro_cite">Gollapudi <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, a group of players join a team to generate social utility, and receive their payments in accordance with their contributions.
The most common profit allocation schemes include fair value games, labor union games, and Shapley value (SV)-based games. Fair value games utilize the marginal loss of overall utility when the player leaves the union to measure a player’s utility, while labor union games measure the player’s utility using the marginal gain to the overall utility when the player joins the coalition. In <cite class="ltx_cite ltx_citemacro_cite">Nishio <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, client contribution is evaluated through the gradient-based fair value scheme. However, for this type of methods, participants’ contributions are influenced by the order in which they join the federation. Therefore, SV-based approaches have been more widely adopted in IFL to perform contribution evaluation that is free from the influence of the order of joining FL.</p>
</div>
</section>
<section id="S3.SS5.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SV-based Interpretation:</h5>

<div id="S3.SS5.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS5.SSS1.Px2.p1.1" class="ltx_p">SV-based methods provide insights into clients’ datasets through evaluating their contributions to the performance of the final FL model. SV is a classic approach for quantifying individual contributions within a group under the Cooperative Game Theory.
It assigns each participant a unique value using its contribution to the utility of all possible subsets to which it belongs. Existing SV-based methods for IFL can be divided into two categories based on the threat models.</p>
</div>
<div id="S3.SS5.SSS1.Px2.p2" class="ltx_para">
<p id="S3.SS5.SSS1.Px2.p2.1" class="ltx_p">For the semi-honest FL participants, existing works focus on improving computational efficiency, while maintaining SV estimation accuracy. This is because calculating the canonical SVs is prohibitively costly as the number of utility function evaluations required grows exponentially with respect to the number of FL participants.
There are two main approaches for achieving efficient SV calculation: 1) accelerating within-round evaluations, and 2) reducing the number of rounds of sub-model evaluations required.
For the first approach, gradient-based Shapley <cite class="ltx_cite ltx_citemacro_cite">Nagalapatti and
Narayanam (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and local embedding-based Shapley <cite class="ltx_cite ltx_citemacro_cite">Fan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> have been proposed to reconstruct sub-models for different client permutations instead of re-training them from scratch.
For the second approach, there are three main methods. The first method evaluates every possible FL sub-model as in the original SV setting. In <cite class="ltx_cite ltx_citemacro_cite">Song <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, the authors proposed two gradient-based SV estimation approaches: one-round (OR) evaluation, which computes SVs only once after all training rounds are completed; and multi-round (MR) evaluation, which computes SVs during each round of FL model training. In a follow up work, <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2020</a>)</cite> proposed the Truncated MR (TMR) method based on OR and MR to eliminate unnecessary sub-model reconstructions for more efficient evaluations.
The second method focuses on randomly sampling permutation evaluations <cite class="ltx_cite ltx_citemacro_cite">Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>, <a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite>.
They take the estimated SV of an FL participant as its expected contribution to the group before sampling it in a random permutation. The sample average is then used to approximate the expectation.</p>
</div>
<div id="S3.SS5.SSS1.Px2.p3" class="ltx_para">
<p id="S3.SS5.SSS1.Px2.p3.3" class="ltx_p">The above methods significantly reduce computation costs, from the original <math id="S3.SS5.SSS1.Px2.p3.1.m1.1" class="ltx_Math" alttext="O(2^{K})" display="inline"><semantics id="S3.SS5.SSS1.Px2.p3.1.m1.1a"><mrow id="S3.SS5.SSS1.Px2.p3.1.m1.1.1" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.cmml"><mi id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.3" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.2" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.2" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml"><mn id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.2" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.2.cmml">2</mn><mi id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.3" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.3.cmml">K</mi></msup><mo stretchy="false" id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.3" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.Px2.p3.1.m1.1b"><apply id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1"><times id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.2.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.2"></times><ci id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.3.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.3">𝑂</ci><apply id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1">superscript</csymbol><cn type="integer" id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.2">2</cn><ci id="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS1.Px2.p3.1.m1.1.1.1.1.1.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.Px2.p3.1.m1.1c">O(2^{K})</annotation></semantics></math> to <math id="S3.SS5.SSS1.Px2.p3.2.m2.1" class="ltx_Math" alttext="O(m\log m)" display="inline"><semantics id="S3.SS5.SSS1.Px2.p3.2.m2.1a"><mrow id="S3.SS5.SSS1.Px2.p3.2.m2.1.1" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.cmml"><mi id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.3" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.2" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.2" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.2" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.2.cmml">m</mi><mo lspace="0.167em" rspace="0em" id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.1" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.1" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3a" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.2" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.2.cmml">m</mi></mrow></mrow><mo stretchy="false" id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.3" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.Px2.p3.2.m2.1b"><apply id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1"><times id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.2.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.2"></times><ci id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.3.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.3">𝑂</ci><apply id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1"><times id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.1"></times><ci id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.2">𝑚</ci><apply id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3"><log id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.1"></log><ci id="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS5.SSS1.Px2.p3.2.m2.1.1.1.1.1.3.2">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.Px2.p3.2.m2.1c">O(m\log m)</annotation></semantics></math>, where <math id="S3.SS5.SSS1.Px2.p3.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS5.SSS1.Px2.p3.3.m3.1a"><mi id="S3.SS5.SSS1.Px2.p3.3.m3.1.1" xref="S3.SS5.SSS1.Px2.p3.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS1.Px2.p3.3.m3.1b"><ci id="S3.SS5.SSS1.Px2.p3.3.m3.1.1.cmml" xref="S3.SS5.SSS1.Px2.p3.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS1.Px2.p3.3.m3.1c">m</annotation></semantics></math> is the number of FL participants selected per training round. However, since the number of sampled permutations is fixed, potentially important FL participant permutations might be overlooked, resulting in inaccurate SV estimation. To address this shortcoming, the third approach leveraging the guided Monte Carlo sampling technique has emerged. They combine this sampling technique with the within-round and between-round truncation techniques to prioritize FL participant permutations based on their importance, and reduce unnecessary evaluations <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib29" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib30" title="" class="ltx_ref">b</a>)</cite>.</p>
</div>
<div id="S3.SS5.SSS1.Px2.p4" class="ltx_para">
<p id="S3.SS5.SSS1.Px2.p4.1" class="ltx_p">This category of works assume that the FL server has direct access to the original FL model and a public test dataset. This might not always be valid in practice as the test data might be regarded by the FL clients as their own private assets.
Hence, for the malicious case, the general approach studies the problem of secure SV calculation. In <cite class="ltx_cite ltx_citemacro_cite">Zheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>, an efficient two-server secure SV calculation protocol was proposed which utilizes a hybrid privacy protection scheme to avoid ciphertext-ciphertext multiplications between the test data and the models. An approximation method was also proposed to accelerate secure SV calcuation by identifying and skipping some test samples without significantly affecting the evaluation accuracy. Another work <cite class="ltx_cite ltx_citemacro_cite">Ma <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> proposed a group-based SV computation protocol and a blockchain-based secure FL framework that adopts secure aggregation to protect clients’ privacy during the training.</p>
</div>
</section>
</section>
<section id="S3.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Feature Contribution Evaluation Techniques</h4>

<section id="S3.SS5.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SVs-based Interpretation:</h5>

<div id="S3.SS5.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS5.SSS2.Px1.p1.1" class="ltx_p">Existing SV-based IFL feature contribution evaluations methods <cite class="ltx_cite ltx_citemacro_cite">Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib43" title="" class="ltx_ref">2019</a>); Wang (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite> mainly leverage gradient-based SV estimation approaches to achieve high efficiency. However, they make strong assumptions that the FL server needs to know all specific IDs of the clients local features, and return the prediction part with all its features turned off. These assumptions violate the privacy of client data, and make them unsuitable for practical VFL applications.</p>
</div>
</section>
<section id="S3.SS5.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attention-based Interpretation:</h5>

<div id="S3.SS5.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS5.SSS2.Px2.p1.1" class="ltx_p">A common approach is to employ attention mechanisms that enable the server to interpret which part of inputs are utilized by the global FL model. In <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, a hierarchical attention mechanism was proposed in which task-specific attentions are developed to evaluate personal feature correlations at the client level. A temporal attention layer is also created to evaluate cross-client temporal correlations at the FL server level. The final visualization of the attention weights can inform the clients and the server what features the global model is focused on when making individual predictions.</p>
</div>
</section>
<section id="S3.SS5.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Activation-based Interpretation:</h5>

<div id="S3.SS5.SSS2.Px3.p1" class="ltx_para">
<p id="S3.SS5.SSS2.Px3.p1.1" class="ltx_p">Activation-based methods focus on extracting input features that highly activated neurons of a trained FL model.
Flames2Graph <cite class="ltx_cite ltx_citemacro_cite">Younis <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite> offers a personalized IFL solution for the multivariate time series classification problem. It extracts and visualizes the essential subsequences that highly activate network neurons in each client, and builds a temporal evolution graph that captures the temporal dependencies among these sequences.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>IFL Performance Evaluation Metrics</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To evaluate the performance of a given IFL approach, it is important to understand how useful the interpretations are and how expensive the interpretations are generated. Thus, at present, research in this field generally adopts two main categories of evaluation metrics on the effectiveness and efficiency of IFL approaches.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Effectiveness Metrics</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Post-Interpretation Performance:</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">One useful function of IFL is to further improve the performance of black-box models by adjusting the explanation models. Thus, the effectiveness of the explanations can be evaluated through changes in model performance (<em id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, accuracy, errors in classification tasks) before and after the adjustments. In <cite class="ltx_cite ltx_citemacro_cite">Cho <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2021a</a>)</cite>, the benefits of IFL client selection is reflected through lower error rates compared to the original FL models.
Reasonable interpretations can assist researchers with model diagnostics, but whether greater performance improvement alone can be used to infer better interpretations is still in doubt <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Faithfulness:</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">A significant question in IFL is whether the important clients, samples and features identified are truly the relevant ones. To gauge the faithfulness of these explanations, a common method is leave-some-out retraining. It removes the identified important clients, samples or features according to their importance values from the explanations, retrain the FL models, and measure the changes in performance.
If the identified clients, samples or features are truly important, a significant degradation in performance is expected.
In <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref">2021c</a>)</cite>, the influence scores for both noisy samples and positively important samples are calculated. The influence scores have been found to be significantly and positively correlated with the actual retraining changes in losses. In this way, the faithfulness of influence functions can be demonstrated.
Reasonable faithfulness metrics should give high scores to IFL approaches emphasizing the relevant FL entities, or clearly reflecting the working mechanisms of the FL models.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Efficiency Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">A defining characteristic of FL is that clients typically have limited computation and communication resources (<em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, sensors, mobile devices, edge devices), which makes the efficiency of IFL important.
According to clients’ different resource consumption preferences, there are two efficiency metrics, computation cost and communication cost.
The computation cost of an IFL technique can be evaluated by the amount of resources it requires. It has been assessed using computation time (usually measured through the number of elementary operations required) and memory storage requirements.
The communication cost of an IFL technique can be measured by the amount of transmission it requires. This is often assessed by the number of bytes of the model parameters transmitted when training an IFL model. Nevertheless, existing research has not directly evaluated the quality of the interpretations generated by IFL approaches, as well as how they might impact the need for privacy preservation.
</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Promising Future Research Directions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Through this survey, we found that IFL research works today mostly focus on generating interpretation during individual FL training stages or for individual entities involved in different types of FL.
To make interpretability an integral part of future FL systems and support the emergence of sustainable FL ecosystems based on effective incentivization, we envision the following promising future research directions.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Interpretable Model Approximation:</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">Existing IFL approaches leveraging self-explanatory models or adding interpretability constraints often lead to reductions in prediction accuracy.
Interpretable model extraction, also referred to as mimic learning, is a promising approach for enhancing interpretability while maintaining a high level of predictive performance. It can be used to approximate a complex FL model with an easy-to-understand model (<em id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, decision trees, rule-based models, linear models). As long as the approximation is sufficiently close, the statistical properties of the complex model can be mimicked by the interpretable model. In this way, it could lead to IFL models with prediction performance comparable to non-interpretable FL models with much improved interpretability.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hard Sample-Aware Noise-Robust IFL:</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Existing IFL client and sample selection methods either ignore the existence of label noise, or simply use hand-crafted loss or gradient thresholds to filter out noisy clients/samples. However, in practice, prior knowledge of the thresholds for distinguishing noisy samples from clean samples is often not available. Besides, these methods cannot be used to distinguish positively influential clients/samples from noisy ones. This is because to mitigate noisy labels, samples with small training losses are preferred as they are more likely to be clean data. When attempting to identify positively influential samples, those with large training losses are preferred as they induce large changes in model parameters. Thus, designing IFL approaches that are hard sample-aware and noise-robust while preserving privacy is desired.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Interpretability under Complex Threat Models:</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">Most of the current IFL approaches are built on the simple threat model of semi-honest participants. This makes them vulnerable to situations in which the server or clients are malicious or colluding. This simplifying assumption needs to be relaxed to enable future IFL approaches to handle more realistic threats in practical applications. In addition, understanding how the adversaries may leverage the interpretations generated by IFL approaches to compromise the system is also important for IFL to be adopted by mission critical applications.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy and Efficiency Trade-off:</h5>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">The privacy-preserving techniques employed by existing IFL approaches incur high computation and communication costs. This make IFL unsuitable for FL systems consisting of resource-constrained devices (<em id="S5.SS0.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, AIoT systems). Thus, research on trade-offs between privacy and efficiency is important for IFL to be adopted by such systems.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Interpretability Evaluation:</h5>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.1" class="ltx_p">IFL models are designed according to distinct principles and are implemented in various forms. This makes general interpretability evaluation challenging. Existing evaluation metrics have the following limitations. Firstly, for post-interpretation performance metrics, it remains doubtful that greater performance improvement alone directly indicate good interpretability. Secondly, the traditional leave-some-out retraining metrics are computationally expensive. Last but not least, none of the existing IFL evaluation metrics measures how much privacy might be exposed for a given level of interpretability achieved, which is crucial in the context of federated learning. Therefore, designing more appropriate and efficient interpretability evaluation metrics deserves further investigation. Such an undertaking will likely need interdisciplinary effort spanning AI and social sciences, and require standardization for adoption by the industry.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Peva Blanchard, Rachid El Mhamdi, and Julien Stainer.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassará <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Pietro Cassará, Alberto Gotta, and Lorenzo Valerio.

</span>
<span class="ltx_bibblock">Federated feature selection for cyber-physical systems of systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Vehicular Technology</span>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yujing Chen, Yue Ning, Zheng Chai, and Huzefa Rangwala.

</span>
<span class="ltx_bibblock">Federated multi-task learning with hierarchical attention for sensor
data analytics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">2020 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8. IEEE, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021a]</span>
<span class="ltx_bibblock">
Xiaolin Chen, Shuai Zhou, Kai Yang, Hao Fan, Zejin Feng, Zhong Chen, Hu Wang,
and Yongji Wang.

</span>
<span class="ltx_bibblock">Fed-eini: An efficient and interpretable inference framework for
decision tree ensembles in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.09540</span>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021b]</span>
<span class="ltx_bibblock">
Zihan Chen, Kai Fong Ernest Chong, and Tony QS Quek.

</span>
<span class="ltx_bibblock">Dynamic attention-based communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2108.05765</span>, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Yuanyuan Chen, Zichen Chen, Sheng Guo, Yansong Zhao, Zelei Liu, Pengcheng Wu,
Chengyi Yang, Zengxiang Li, and Han Yu.

</span>
<span class="ltx_bibblock">Efficient training of large-scale industrial fault diagnostic models
through federated opportunistic block dropout,.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">IAAI</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span id="bib.bib7.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, Dimitrios
Papadopoulos, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secureboost: A lossless federated learning framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, 36(6):87–98, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yae Jee Cho, Jianyu Wang, and Gauri Joshi.

</span>
<span class="ltx_bibblock">Client selection in federated learning: Convergence analysis and
power-of-choice selection strategies.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.01243</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Tian Dong, Song Li, Han Qiu, and Jialiang Lu.

</span>
<span class="ltx_bibblock">An interpretable federated learning-based network intrusion detection
framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.03134</span>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Zhenan Fan, Huang Fang, Zirui Zhou, Jian Pei, Michael P Friedlander, and Yong
Zhang.

</span>
<span class="ltx_bibblock">Fair and efficient contribution valuation for vertical federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.02658</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng [2022]</span>
<span class="ltx_bibblock">
Siwei Feng.

</span>
<span class="ltx_bibblock">Vertical federated learning-based feature selection with
non-overlapping sample utilization.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, 208:118097, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goetz <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jack Goetz, Kshitiz Malik, Seungwhan Bui, Honglei Liu, and Anuj Kumar.

</span>
<span class="ltx_bibblock">Active federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.12641</span>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gollapudi <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Sreenivas Gollapudi, Debmalya Kollias, and Venetia Pliatsika.

</span>
<span class="ltx_bibblock">Profit sharing and efficiency in utility games.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">ESA</span>, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guerraoui <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Rachid Guerraoui, Sébastien Rouault, et al.

</span>
<span class="ltx_bibblock">The hidden vulnerability of distributed learning in byzantium.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">ICML</span>, pages 3521–3530, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Andrew Hard, Swaroop Rao, Françoise Beaufays, Hubert Augenstein,
Chloé Kiddon, and Daniel Ramage.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.03604</span>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imakura <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Akira Imakura, Yukihiko Inaba, and Tetsuya Sakurai.

</span>
<span class="ltx_bibblock">Interpretable collaborative data analysis on distributed data.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, 177:114891, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang <span id="bib.bib17.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Shengli Niyato, and Junshan Zhang.

</span>
<span class="ltx_bibblock">Incentive mechanism for reliable federated learning: A joint
optimization approach to combining reputation and contract theory.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 6(6):10700–10714, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katharopoulos and
Fleuret [2018]</span>
<span class="ltx_bibblock">
Angelos Katharopoulos and François Fleuret.

</span>
<span class="ltx_bibblock">Not all samples are created equal: Deep learning with importance
sampling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ICML</span>, pages 2525–2534, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh and Liang [2017]</span>
<span class="ltx_bibblock">
Pang Wei Koh and Percy Liang.

</span>
<span class="ltx_bibblock">Understanding black-box predictions via influence functions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">ICML</span>, pages 1885–1894, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh <span id="bib.bib20.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Pang Wei W Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang.

</span>
<span class="ltx_bibblock">On the accuracy of influence functions for measuring group effects.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Oort: Efficient federated learning via guided participant selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">OSDI</span>, pages 19–35, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Xiao-Hui Li, Caleb Chen Cao, Shenjia Shi, Xun Xue, et al.

</span>
<span class="ltx_bibblock">A survey of data-driven and knowledge-aware explainable AI.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">IEEE TKDE</span>, 34(1):29–49, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021a]</span>
<span class="ltx_bibblock">
Anran Li, Lan Zhang, Juntao Tan, and Xiang-Yang Li.

</span>
<span class="ltx_bibblock">Sample-level data selection for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">IEEE INFOCOM</span>, pages 1–10. IEEE, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib24.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021b]</span>
<span class="ltx_bibblock">
Anran Li, Lan Zhang, Junhao Wang, Feng Han, and Xiang-Yang Li.

</span>
<span class="ltx_bibblock">Privacy-preserving efficient federated-learning model debugging.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic">IEEE TPDS</span>, 33(10):2291–2303, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021c]</span>
<span class="ltx_bibblock">
Anran Li, Lan Zhang, Junhao Wang, Juntao Tan, Feng Han, Yaxuan Qin, Nikolaos M
Freris, and Xiang-Yang Li.

</span>
<span class="ltx_bibblock">Efficient federated-learning model debugging.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">ICDE</span>, pages 372–383, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib26.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021d]</span>
<span class="ltx_bibblock">
Xiling Li, Rafael Dowsley, and Martine De Cock.

</span>
<span class="ltx_bibblock">Privacy-preserving feature selection with secure multiparty
computation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.3.1" class="ltx_text ltx_font_italic">ICML</span>, pages 6326–6336, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022a]</span>
<span class="ltx_bibblock">
Anran Li, Hongyi Peng, Lan Zhang, Jiahui Huang, Qing Guo, Han Yu, and Yang Liu.

</span>
<span class="ltx_bibblock">FedSDG-FS: Efficient and secure feature selection for vertical
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.01801</span>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib28.3.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022b]</span>
<span class="ltx_bibblock">
Cen-Jhih Li, Pin-Han Huang, Yi-Ting Ma, Hung Hung, and Su-Yun Huang.

</span>
<span class="ltx_bibblock">Robust aggregation for federated learning by minimum
<math id="bib.bib28.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="bib.bib28.1.m1.1a"><mi id="bib.bib28.1.m1.1.1" xref="bib.bib28.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="bib.bib28.1.m1.1b"><ci id="bib.bib28.1.m1.1.1.cmml" xref="bib.bib28.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib28.1.m1.1c">\gamma</annotation></semantics></math>-divergence estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text ltx_font_italic">Entropy</span>, 24(5):686, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span id="bib.bib29.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022a]</span>
<span class="ltx_bibblock">
Zelei Liu, Yuanyuan Chen, Han Yu, Yang Liu, and Lizhen Cui.

</span>
<span class="ltx_bibblock">Gtg-shapley: Efficient and accurate participant contribution
evaluation in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic">ACM TIST</span>, 13(4):1–21, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span id="bib.bib30.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022b]</span>
<span class="ltx_bibblock">
Zelei Liu, Yuanyuan Chen, Yansong Zhao, Han Yu, Zaiqing Nie, Qian Xu, and Qiang
Yang.

</span>
<span class="ltx_bibblock">Contribution-aware federated learning for smart healthcare.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">IAAI</span>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma <span id="bib.bib31.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Shuaicheng Ma, Yang Cao, and Li Xiong.

</span>
<span class="ltx_bibblock">Transparent contribution evaluation for secure federated learning on
blockchain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">IEEE 37th ICDEW Workshops</span>, pages 88–91. IEEE, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan <span id="bib.bib32.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.3.1" class="ltx_text ltx_font_italic">AISTATS</span>, pages 1273–1282, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagalapatti and
Narayanam [2021]</span>
<span class="ltx_bibblock">
Lokesh Nagalapatti and Ramasuri Narayanam.

</span>
<span class="ltx_bibblock">Game of gradients: Mitigating irrelevant clients in federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">AAAI</span>, volume 35, pages 9046–9054, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio <span id="bib.bib34.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Takayuki Nishio, Ryoichi Shinkuma, and Narayan B Mandayam.

</span>
<span class="ltx_bibblock">Estimation of individual device contributions for incentivizing
federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.3.1" class="ltx_text ltx_font_italic">2020 IEEE Globecom Workshops</span>, pages 1–6. IEEE, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan <span id="bib.bib35.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Fucheng Pan, Dan Meng, Yu Zhang, and Xiaolin Li.

</span>
<span class="ltx_bibblock">Secure federated feature selection for cross-feature federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic">arXiv preprint</span>, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandey <span id="bib.bib36.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Shashi Raj Pandey, Nguyen H Tran, and Choong Seon Hong.

</span>
<span class="ltx_bibblock">A crowdsourcing framework for on-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>, 19(5):3241–3256,
2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rokvic <span id="bib.bib37.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Ljubomir Rokvic, Panayiotis Danassis, and Boi Faltings.

</span>
<span class="ltx_bibblock">Privacy-preserving data filtering in federated learning using
influence approximation.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.11518</span>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roschewitz <span id="bib.bib38.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
David Roschewitz, Mary-Anne Hartley, Luca Corinzia, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Ifedavg: Interpretable data-interoperability for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.06580</span>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin <span id="bib.bib39.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Jaemin Shin, Yuanchun Li, Yunxin Liu, and Sung-Ju Lee.

</span>
<span class="ltx_bibblock">Sample selection with deadline control for efficient federated
learning on heterogeneous clients.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.01601</span>, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song <span id="bib.bib40.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Tianshu Song, Yongxin Tong, and Shuyue Wei.

</span>
<span class="ltx_bibblock">Profit allocation for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.3.1" class="ltx_text ltx_font_italic">IEEE BigData</span>, pages 2577–2586, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong <span id="bib.bib41.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Qianqian Tong, Guannan Liang, Jiahao Ding, Tan Zhu, Miao Pan, and Jinbo Bi.

</span>
<span class="ltx_bibblock">Federated optimization of l0-norm regularized sparse learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic">Algorithms</span>, 15(9):319, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuor <span id="bib.bib42.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Tiffany Tuor, Shiqiang Wang, Bong Jun Ko, Changchang Liu, and Kin K Leung.

</span>
<span class="ltx_bibblock">Overcoming noisy and irrelevant data in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.3.1" class="ltx_text ltx_font_italic">ICPR</span>, pages 5020–5027, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib43.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Guan Wang, Charlie Xiaoqian Dang, and Ziye Zhou.

</span>
<span class="ltx_bibblock">Measure contribution of participants in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.3.1" class="ltx_text ltx_font_italic">IEEE Big Data</span>, pages 2597–2604, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib44.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song.

</span>
<span class="ltx_bibblock">A principled approach to data valuation for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.3.1" class="ltx_text ltx_font_italic">Federated Learning</span>, pages 153–167. Springer, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib45.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Junhao Wang, Lan Zhang, Anran Li, Xuanke You, and Haoran Cheng.

</span>
<span class="ltx_bibblock">Efficient participant contribution evaluation for horizontal and
vertical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.3.1" class="ltx_text ltx_font_italic">ICDE</span>, pages 911–923, 2022.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang [2019]</span>
<span class="ltx_bibblock">
Guan Wang.

</span>
<span class="ltx_bibblock">Interpret federated learning with shapley values.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.04519</span>, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span id="bib.bib47.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Shuyue Wei, Yongxin Tong, Zimu Zhou, and Tianshu Song.

</span>
<span class="ltx_bibblock">Efficient and fair data valuation for horizontal federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.3.1" class="ltx_text ltx_font_italic">Federated Learning</span>, pages 139–152. Springer, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue <span id="bib.bib48.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Yihao Xue, Chaoyue Niu, Shaojie Tang, Fan Wu, and Guihai Chen.

</span>
<span class="ltx_bibblock">Toward understanding the influence of individual clients in federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.3.1" class="ltx_text ltx_font_italic">AAAI</span>, volume 35, pages 10560–10567, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib49.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic">Federated Learning</span>, volume Synthesis Lectures on Artificial
Intelligence and Machine Learning.

</span>
<span class="ltx_bibblock">Springer, Cham, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin <span id="bib.bib50.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards optimal statistical
rates.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.3.1" class="ltx_text ltx_font_italic">ICML</span>, pages 5650–5659, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Younis <span id="bib.bib51.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Raneen Younis, Zahra Ahmadi, Abdul Hakmeh, and Marco Fisichella.

</span>
<span class="ltx_bibblock">Flames2graph: An interpretable federated multivariate time series
classification framework.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhan <span id="bib.bib52.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Yufeng Zhan, Peng Li, Zhihao Qu, Deze Zeng, and Song Guo.

</span>
<span class="ltx_bibblock">A learning-based incentive mechanism for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 7(7):6360–6368, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span id="bib.bib53.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Lin Zhang, Lixin Fan, Yong Luo, and Ling-Yu Duan.

</span>
<span class="ltx_bibblock">Intrinsic performance influence based participant contribution
estimation for horizontal federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic">ACM TIST</span>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span id="bib.bib54.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Xunzheng Zhang, Alex Mavromatics, Reza Vafeas, and Dimitra Simeonidou.

</span>
<span class="ltx_bibblock">Federated feature selection for horizontal federated learning in iot
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, pages 1–1, 2023.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng <span id="bib.bib55.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Shuyuan Zheng, Yang Cao, and Masatoshi Yoshikawa.

</span>
<span class="ltx_bibblock">Secure shapley value for cross-silo federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.04856</span>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.13472" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.13473" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.13473">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.13473" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.13474" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 22:58:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
