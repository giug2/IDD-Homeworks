<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>(Unfair) Norms in Fairness Research: A Meta-Analysis</title>
<!--Generated on Mon Jun 17 17:12:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.16895v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S1" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S2" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S2.SS1" title="In 2 Related Work ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data and Geographic Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S2.SS2" title="In 2 Related Work ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Western-Centric Values and Perspectives</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3.SS1" title="In 3 Methods ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Research Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3.SS2" title="In 3 Methods ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data and Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS1" title="In 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Quantitative Findings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS1.SSS1" title="In 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Data and Authorship Provenance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS1.SSS2" title="In 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Sensitive Attributes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS2" title="In 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Sociotechnical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS2.SSS1" title="In 4.2 Sociotechnical Analysis ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>US-Centrism and the Limits of Global Fungibility</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.SS2.SSS2" title="In 4.2 Sociotechnical Analysis ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Binary Formulations and the Erasure of Intersectional Realities</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S5" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S6" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S7" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Positionality Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S8" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Expanded Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.SS1" title="In Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Data and Authorship Provenance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.SS2" title="In Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Study Design</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A2" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Fairness Definitions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A3" title="In (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Sampled Papers</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\correspondingauthor</span>
<p class="ltx_p" id="p1.2">jjchien@ucsd.edu
 















<span class="ltx_text" id="p1.2.1" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">(Unfair) Norms in Fairness Research:
<br class="ltx_break"/>A Meta-Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer Chien <span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0009-0009-8768-1761
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of California San Diego
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A. Stevie Bergman <span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-4331-1357
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kevin R. McKee <span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0002-4412-1686
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nenad Tomasev <span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0000-0003-1624-0220
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vinodkumar Prabhakaran <span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0003-3329-2305
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rida Qadri <span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0001-5690-0997
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nahema Marchal <span class="ltx_ERROR undefined" id="id7.1.id1">\orcidlink</span>0000-0002-8518-3840
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Isaac <span class="ltx_ERROR undefined" id="id8.1.id1">\orcidlink</span>0000-0002-1297-5409
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1"><span class="ltx_text" id="id9.id1.1" lang="en">Algorithmic fairness has emerged as a critical concern in artificial intelligence (AI) research. However, the development of fair AI systems is not an objective process. Fairness is an inherently subjective concept, shaped by the values, experiences, and identities of those involved in research and development. To better understand the norms and values embedded in current fairness research, we conduct a meta-analysis of algorithmic fairness papers from two leading conferences on AI fairness and ethics, AIES and FAccT, covering a final sample of 139 papers over the period from 2018 to 2022. Our investigation reveals two concerning trends: first, a US-centric perspective dominates throughout fairness research; and second, fairness studies exhibit a widespread reliance on binary codifications of human identity (e.g., “Black/White”, “male/female”). These findings highlight how current research often overlooks the complexities of identity and lived experiences, ultimately failing to represent diverse global contexts when defining algorithmic bias and fairness. We discuss the limitations of these research design choices and offer recommendations for fostering more inclusive and representative approaches to fairness in AI systems, urging a paradigm shift that embraces nuanced, global understandings of human identity and values.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>algorithmic fairness, meta-analysis, reflexivity, geographic bias
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The widespread adoption of artificial intelligence (AI) brings with it the potential for substantial harm. AI systems frequently encode and amplify historical biases <cite class="ltx_cite ltx_citemacro_citep">(Lum and Isaac, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib63" title="">2016</a>; Buolamwini and Gebru, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib15" title="">2018</a>; Eubanks, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib32" title="">2018</a>; D’ignazio and Klein, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib27" title="">2020</a>; Zou and Khern-am nuai, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib91" title="">2023</a>)</cite>, thus exacerbating and perpetuating discrimination against marginalized communities <cite class="ltx_cite ltx_citemacro_citep">(Eubanks, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib32" title="">2018</a>; Benjamin, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib7" title="">2020</a>; Irani et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib50" title="">2010</a>)</cite>. As a consequence, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">algorithmic fairness</span> has emerged as a growing priority for AI developers, ethicists, policymakers, and regulators <cite class="ltx_cite ltx_citemacro_citep">(Pfeiffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib72" title="">2023</a>; Kleanthous et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib55" title="">2022</a>; Lepri et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib60" title="">2018</a>; Zarsky, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib90" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There is no single definition of fairness <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib68" title="">2021</a>; Barocas et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib6" title="">2023</a>)</cite>. Accordingly, approaches to algorithmic fairness vary widely, encompassing individual and group perspectives, parity and equity considerations, and beyond <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib30" title="">2012</a>; Binns, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib9" title="">2020</a>; Chien and Danks, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib20" title="">2023</a>)</cite>. While this pluralism has allowed researchers to generate a diverse toolkit of metrics, techniques, and frameworks for mitigating bias, it also introduces a crucial challenge: the inherent subjectivity of fairness itself.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Fairness is not a universal, abstract concept. It is deeply intertwined with the values, experiences, and identities of those involved in the research process <cite class="ltx_cite ltx_citemacro_citep">(Guillemin and Gillam, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib40" title="">2004</a>; Fook, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib34" title="">1999</a>; McCabe and Holmes, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib67" title="">2009</a>; Bunge, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib14" title="">2018</a>; Iliadis and Russo, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib49" title="">2016</a>)</cite>.
Thus, fairness research is sensitive to contextual questions of who is doing the work, what they are studying, and how they are studying it. A comprehensive understanding of fairness necessitates a conscious effort to acknowledge and understand the values embedded in the research process itself.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we undertake a reflexive meta-analysis of the research literature on algorithmic fairness, exploring the values and perspectives embedded within the research process itself. We focus our meta-analysis on the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) and the ACM Conference of Fairness, Accountability, and Transparency (FAccT), two prominent conferences on algorithmic fairness. Our investigation reveals two concerning trends: a US-centric bias in authorship and definitions of sensitive attributes, and the predominance of binary formulations for sensitive attributes (e.g., “Black/White” or “male/female”). These findings indicate that predominant approaches to fairness radically oversimplify the complexities of identity and fail to reflect the diversity of lived experiences with AI across the globe. Overall, the results from our meta-analysis lead us to join the growing chorus of calls for a paradigm shift in fairness research. Moving forward, the research community must embrace a more nuanced and contextual understanding of human values, supporting the development of genuinely representative, inclusive, and fair AI systems.</p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our meta-analysis builds upon a growing body of research surveying and critiquing the fields of AI ethics and algorithmic fairness.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data and Geographic Bias</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In recent years, critical researchers have increasingly examined AI ethics and fairness research for geographic bias. Close scrutiny reveals that the datasets used within these fields often fail to accurately represent the global population. <cite class="ltx_cite ltx_citemacro_citet">Septiandri et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib79" title="">2023</a>)</cite> survey FAccT proceedings and finds a bias towards Western, educated, industrialized, rich, and democratic perspectives participants and data. <cite class="ltx_cite ltx_citemacro_citet">Abdu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib1" title="">2023</a>)</cite> examine racial categories in FAccT proceedings through the lens of institutional influences and values, finding that projects adopt racial categories inconsistently, often following country-specific legal frameworks, while rarely explicitly describing or justifying their choices. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Koch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib56" title="">2021</a>)</cite> touch on the consequences of geographic bias in their exploration of the increasing, over-concentrated usage of a limited group of datasets in machine learning research.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Western-Centric Values and Perspectives</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">AI ethics research often exhibits a bias toward countries in the Global North, presenting their cultural perspectives and values as universal and globally fungible. <cite class="ltx_cite ltx_citemacro_citet">Hagerty and Rubinov (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib42" title="">2019</a>)</cite> study this phenomenon in global social science scholarship and argue that mitigating the worldwide harms caused by AI deployments in the Global North will require deep understandings of a broad set of geographical, cultural, and social contexts. In an examination of the proceedings at FAccT and the ACM CHI Conference on Human Factors in Computing Systems, <cite class="ltx_cite ltx_citemacro_citet">van Berkel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib88" title="">2023</a>)</cite> document a bias toward the US across study design, participant recruitment, and country affiliation of paper authors. <cite class="ltx_cite ltx_citemacro_citet">Birhane et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib11" title="">2022b</a>)</cite> critique ethics papers at AIES and FAccT as relying on speculative, theoretical foundations from Western philosophy, resulting in a dearth of concrete use cases and a lack of acknowledgement for afflicted communities. <cite class="ltx_cite ltx_citemacro_citet">Sambasivan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib76" title="">2021</a>)</cite> similarly examine the philosophical roots, legal frameworks, and axes of discrimination within AI ethics to refocus on what values and norms translate and fail to translate to applications in India. Motivated by the non-portability of algorithmic fairness to India, <cite class="ltx_cite ltx_citemacro_citet">Sambasivan (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib75" title="">2021</a>)</cite> reviews data practices in the machine learning pipeline and discusses the need for expanded representation, calling for the creation and maintenance of work focused on the Global South.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This study aims to examine the practices of problem selection and formulation within algorithmic fairness research. What values do researchers explicitly and implicitly elevate? How does each research project approach its object(s) of study? How does each project interpret and present its results?</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In answering these questions, we aim to reflexively surface and record common practices within fairness research—and subsequently to inform future best practice. We employed a meta-analysis methodology for our study, given its ability to synthesize and critically analyze existing research <cite class="ltx_cite ltx_citemacro_citep">(Borenstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib13" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.2">
<tr class="ltx_tr" id="S3.T1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.1.1.1">
<span class="ltx_p" id="S3.T1.2.1.1.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1.1">Collected Label</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.1.2.1">
<span class="ltx_p" id="S3.T1.2.1.2.1.1" style="width:281.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.2.1.1.1">Definition</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.2.1.1">
<span class="ltx_p" id="S3.T1.2.2.1.1.1" style="width:130.1pt;">Paper Title</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.2.2.1">
<span class="ltx_p" id="S3.T1.2.2.2.1.1" style="width:281.9pt;">As provided</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.3.1.1">
<span class="ltx_p" id="S3.T1.2.3.1.1.1" style="width:130.1pt;">Author Country Affiliation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.3.2.1">
<span class="ltx_p" id="S3.T1.2.3.2.1.1" style="width:281.9pt;">IBAN two-letter country codes for each author, comma separated</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.4.1.1">
<span class="ltx_p" id="S3.T1.2.4.1.1.1" style="width:130.1pt;">Year Published</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.4.2.1">
<span class="ltx_p" id="S3.T1.2.4.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.4.2.1.1.1">2018</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.4.2.1.1.2">2019</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.4.2.1.1.3">2020</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.4.2.1.1.4">2021</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.4.2.1.1.5">2022</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.5.1.1">
<span class="ltx_p" id="S3.T1.2.5.1.1.1" style="width:130.1pt;">Venue</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.5.2.1">
<span class="ltx_p" id="S3.T1.2.5.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.5.2.1.1.1">AIES</span> or <span class="ltx_text ltx_font_italic" id="S3.T1.2.5.2.1.1.2">FAccT</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.6.1.1">
<span class="ltx_p" id="S3.T1.2.6.1.1.1" style="width:130.1pt;">Study Type</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.6.2.1">
<span class="ltx_p" id="S3.T1.2.6.2.1.1" style="width:281.9pt;">All that apply from: [<span class="ltx_text ltx_font_italic" id="S3.T1.2.6.2.1.1.1">Retrospective</span> (uses existing datasets); <span class="ltx_text ltx_font_italic" id="S3.T1.2.6.2.1.1.2">Theory</span> (develops novel definitions, proofs, or guarantees); <span class="ltx_text ltx_font_italic" id="S3.T1.2.6.2.1.1.3">Prospective</span> (collects new datasets)]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.7.1.1">
<span class="ltx_p" id="S3.T1.2.7.1.1.1" style="width:130.1pt;">Dataset Name</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.7.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.7.2.1">
<span class="ltx_p" id="S3.T1.2.7.2.1.1" style="width:281.9pt;">As provided</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.8.1.1">
<span class="ltx_p" id="S3.T1.2.8.1.1.1" style="width:130.1pt;">Dataset Origin Country</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.8.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.8.2.1">
<span class="ltx_p" id="S3.T1.2.8.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.8.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.8.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.8.2.1.1.3">Unspecified</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.9.1.1">
<span class="ltx_p" id="S3.T1.2.9.1.1.1" style="width:130.1pt;">Data Type</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.9.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.9.2.1">
<span class="ltx_p" id="S3.T1.2.9.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.1">Tabular</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.2">Text</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.3">Time Series</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.4">Still Images</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.5">Video</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.9.2.1.1.6">Audio</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.10.1.1">
<span class="ltx_p" id="S3.T1.2.10.1.1.1" style="width:130.1pt;">Topic Domain</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.10.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.10.2.1">
<span class="ltx_p" id="S3.T1.2.10.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.1">Criminal Justice</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.2">Education</span>, <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.3">Media</span> (e.g., entertainment, news); <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.4">Finance</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.5">Health/Medicine</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.6">Hiring</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.7">Social Media</span> (e.g., networks); <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.8">Informational</span> (e.g., Wikipedia); <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.9">Social Welfare</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.10.2.1.1.10">Obscured</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.11.1.1">
<span class="ltx_p" id="S3.T1.2.11.1.1.1" style="width:130.1pt;">Sensitive Attribute Category(s) Studied</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.11.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.11.2.1">
<span class="ltx_p" id="S3.T1.2.11.2.1.1" style="width:281.9pt;">As specified (e.g. “Race”, “Geolocation”)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.12.1.1">
<span class="ltx_p" id="S3.T1.2.12.1.1.1" style="width:130.1pt;">Attribute Specific Labels</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.12.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.12.2.1">
<span class="ltx_p" id="S3.T1.2.12.2.1.1" style="width:281.9pt;">If a sensitive attribute was specified, the labels used (e.g. “Asian”, “Female”, “Age (1-18)”)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.13.1.1">
<span class="ltx_p" id="S3.T1.2.13.1.1.1" style="width:130.1pt;">Degree of Sensitive Attribute Measurement</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.13.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.13.2.1">
<span class="ltx_p" id="S3.T1.2.13.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.13.2.1.1.1">No Acknowledgement</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.13.2.1.1.2">Acknowledgement</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.13.2.1.1.3">Acknowledgement and Mitigation</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.14.1.1">
<span class="ltx_p" id="S3.T1.2.14.1.1.1" style="width:130.1pt;">Degree of Sensitive Attribute Limitation Addressed</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.14.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.14.2.1">
<span class="ltx_p" id="S3.T1.2.14.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.14.2.1.1.1">No Acknowledgement</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.14.2.1.1.2">Acknowledgement</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.14.2.1.1.3">Acknowledgement and Mitigation</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.15.1.1">
<span class="ltx_p" id="S3.T1.2.15.1.1.1" style="width:130.1pt;">Proxy used for Sensitive Attribute</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.15.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.15.2.1">
<span class="ltx_p" id="S3.T1.2.15.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.15.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.15.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.15.2.1.1.3">Not Applicable (NA)</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.16.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.16.1.1">
<span class="ltx_p" id="S3.T1.2.16.1.1.1" style="width:130.1pt;">Proxy Category</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.16.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.16.2.1">
<span class="ltx_p" id="S3.T1.2.16.2.1.1" style="width:281.9pt;">If a proxy is used, the category the proxy is used for (e.g. “Race”)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.17.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.17.1.1">
<span class="ltx_p" id="S3.T1.2.17.1.1.1" style="width:130.1pt;">Proxy Label</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.17.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.17.2.1">
<span class="ltx_p" id="S3.T1.2.17.2.1.1" style="width:281.9pt;">If a proxy is used, the labels used (e.g. “Skin Tone”)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.18.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.18.1.1">
<span class="ltx_p" id="S3.T1.2.18.1.1.1" style="width:130.1pt;">Does the paper address intersectionality?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.18.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.18.2.1">
<span class="ltx_p" id="S3.T1.2.18.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.18.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.18.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.18.2.1.1.3">NA</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.19.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.19.1.1">
<span class="ltx_p" id="S3.T1.2.19.1.1.1" style="width:130.1pt;">Intersectional Category(s)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.19.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.19.2.1">
<span class="ltx_p" id="S3.T1.2.19.2.1.1" style="width:281.9pt;">If intersectional, the sensitive attribute category(s) studied (e.g. “Race”)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.20.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.20.1.1">
<span class="ltx_p" id="S3.T1.2.20.1.1.1" style="width:130.1pt;">Degree of Dataset Limitation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.20.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.20.2.1">
<span class="ltx_p" id="S3.T1.2.20.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.20.2.1.1.1">No Acknowledgement</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.20.2.1.1.2">Acknowledgement</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.20.2.1.1.3">Acknowledgement and Mitigation</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.21">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.21.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.21.1.1">
<span class="ltx_p" id="S3.T1.2.21.1.1.1" style="width:130.1pt;">Fairness Metric Name</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.21.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.21.2.1">
<span class="ltx_p" id="S3.T1.2.21.2.1.1" style="width:281.9pt;">As provided</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.22.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.22.1.1">
<span class="ltx_p" id="S3.T1.2.22.1.1.1" style="width:130.1pt;">Metric Type</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.22.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.22.2.1">
<span class="ltx_p" id="S3.T1.2.22.2.1.1" style="width:281.9pt;">All that apply from: [<span class="ltx_text ltx_font_italic" id="S3.T1.2.22.2.1.1.1">Group</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.22.2.1.1.2">Individual</span>, <span class="ltx_text ltx_font_italic" id="S3.T1.2.22.2.1.1.3">Counterfactual</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.22.2.1.1.4">Contrastive</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.23">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.23.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.23.1.1">
<span class="ltx_p" id="S3.T1.2.23.1.1.1" style="width:130.1pt;">Explicit Inputs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.23.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.23.2.1">
<span class="ltx_p" id="S3.T1.2.23.2.1.1" style="width:281.9pt;">Whether the sensitive attributes are explicit inputs to the model [<span class="ltx_text ltx_font_italic" id="S3.T1.2.23.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.23.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.23.2.1.1.3">NA</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.24">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.24.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.24.1.1">
<span class="ltx_p" id="S3.T1.2.24.1.1.1" style="width:130.1pt;">Performance Trade-off</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.24.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.24.2.1">
<span class="ltx_p" id="S3.T1.2.24.2.1.1" style="width:281.9pt;">[<span class="ltx_text ltx_font_italic" id="S3.T1.2.24.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.24.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.24.2.1.1.3">Unclear</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.25">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.25.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.25.1.1">
<span class="ltx_p" id="S3.T1.2.25.1.1.1" style="width:130.1pt;">Deep Learning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.25.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.25.2.1">
<span class="ltx_p" id="S3.T1.2.25.2.1.1" style="width:281.9pt;">Whether the model uses deep learning [<span class="ltx_text ltx_font_italic" id="S3.T1.2.25.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.25.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.25.2.1.1.3">NA</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.26">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.26.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.26.1.1">
<span class="ltx_p" id="S3.T1.2.26.1.1.1" style="width:130.1pt;">Participatory</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.26.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.26.2.1">
<span class="ltx_p" id="S3.T1.2.26.2.1.1" style="width:281.9pt;">Whether the paper takes a participatory approach [<span class="ltx_text ltx_font_italic" id="S3.T1.2.26.2.1.1.1">Yes</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.26.2.1.1.2">No</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.26.2.1.1.3">NA</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.27">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.27.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.27.1.1">
<span class="ltx_p" id="S3.T1.2.27.1.1.1" style="width:130.1pt;">Human Factors</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.2.27.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.27.2.1">
<span class="ltx_p" id="S3.T1.2.27.2.1.1" style="width:281.9pt;">Degree to which the paper addresses how humans factor into the decision-making process [<span class="ltx_text ltx_font_italic" id="S3.T1.2.27.2.1.1.1">No Acknowledgement</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.27.2.1.1.2">Acknowledgement</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.27.2.1.1.3">Acknowledgement and Mitigation</span>]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.28">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.2.28.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.28.1.1">
<span class="ltx_p" id="S3.T1.2.28.1.1.1" style="width:130.1pt;">Pipeline Intervention</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.2.28.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.28.2.1">
<span class="ltx_p" id="S3.T1.2.28.2.1.1" style="width:281.9pt;">The stage of the pipeline where the intervention is proposed [<span class="ltx_text ltx_font_italic" id="S3.T1.2.28.2.1.1.1">Pre-processing</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.28.2.1.1.2">In-processing</span>; <span class="ltx_text ltx_font_italic" id="S3.T1.2.28.2.1.1.3">Post-processing</span>; or <span class="ltx_text ltx_font_italic" id="S3.T1.2.28.2.1.1.4">Multiple Points</span>]</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Table of Collected Labels &amp; Definitions. We pre-generated initial levels for variables such as “Data Type” and expanded levels for instances that did not fit the existing scheme.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Research Questions</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To contribute to ongoing efforts to identify the normative choices and biases within fairness research, we investigate the following research questions:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.1.1.1">RQ1:</span></span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">What geographic and cultural biases are present in algorithmic fairness research?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.1.1.1">RQ2:</span></span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">What are the implicit norms embedded in the research process for algorithmic fairness, especially those affecting the diversity and representativeness of the research?</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data and Analysis</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our review investigated papers from two flagship conferences on algorithmic fairness and AI ethics: the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) and the ACM Conference of Fairness, Accountability, and Transparency (FAccT). Both have grown substantially since their establishment in 2018, with interdisciplinary proceedings (including computer science, law and policy, social sciences, ethics and philosophy) and attendees (including researchers, policymakers, and practitioners). Together, the conferences represent principal sites of the research discourse on algorithmic fairness <cite class="ltx_cite ltx_citemacro_citep">(Acuna and Liang, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib2" title="">2021</a>; Birhane et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib11" title="">2022b</a>)</cite>. We collected all papers included in the conferences’ proceedings from 2018 (their inception) through 2022 (the most recent year available during our data collection period), accessing the papers through the conference websites and the ACM Digital Library. This initial sample comprised 265 and 416 papers from AIES and FAccT, respectively. We subsequently removed non-archival and abstract-only papers, and then filtered our sample to papers that specify at least one formal fairness definition and one model-based decision-making process. This final sample included 139 papers (52 from AIES and 87 from FAccT, respectively).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We focus our annotation and analysis on several crucial aspects of the sampled research, including dataset characteristics, author information, and modeling choices. To ensure consistency in our data collection, we collaboratively developed and defined our coding scheme. During this process, we independently coded two papers and compared results to clarify any points of confusion or disagreement.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We begin by examining the datasets employed in each paper, recording the dataset name, type (e.g., tabular, text), and topic domain. We further document the sensitive attributes studied, noting the specific categorization scheme applied to attribute labels, label definitions, and collection methods. We pay particular attention to whether papers employed proxies to study sensitive attributes, whether the papers discussed intersectionality, and the extent to which papers acknowledged and mitigated limitations related to these design choices. For simplicity, we categorize the degree of these practices on a three-point scale: (<span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">no acknowledgement</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">acknowledgement</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.3">acknowledgement and mitigation</span>). Additionally, we note any available information on dataset provenance, allowing for identification of potential biases towards specific regions or populations and for comparisons with authors’ country affiliations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">We next catalogue author information. We document the countries noted in author affiliations and record whether each paper incorporated participatory feedback throughout the development process. We also assess the extent to which authors acknowledged any interaction effects of human-in-the-loop processes in the decision-making process, employing the same three-point scale as for acknowledging dataset limitations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">We subsequently examine model design decisions. We note whether each model explicitly received sensitive attributes as inputs, record the name assigned to the model’s fairness metric, and classify the metric within <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1">contrastive</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.2">counterfactual</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.3">group</span>, or <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.4">individual fairness</span>. We also record any mitigation methods used, categorizing each as a <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.5">constraint</span> or <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.6">objective</span>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">After coding the full sample, we quantify temporal and aggregate trends in these metrics. Guided by our research questions, we pay particular attention to author country affiliation, dataset country affiliation, sensitive attribute labels (number and instantiation), and label definition.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">Finally, we enrich our empirical findings through a <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.1.1">sociotechnical</span> analysis. Sociotechnical approaches emphasize the inherently intertwined nature of social and technical systems, allowing them to explore the ways in which social dynamics shape technological development <cite class="ltx_cite ltx_citemacro_citep">(Cherns, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib19" title="">1976</a>; Dolata et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib29" title="">2022</a>)</cite>. By applying this framework, we gain a more holistic understanding of the complex interplay between technological systems, human actors, and their broader societal context within algorithmic fairness research. We also adopt a reflexive stance, acknowledging that our own positions as researchers inevitably shape our perspective and analysis <cite class="ltx_cite ltx_citemacro_citep">(Collins, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib22" title="">1992</a>; Harding, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib45" title="">1991</a>; Haraway, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib44" title="">1988</a>)</cite>. This self-awareness helps us to identify implicit norms and assumptions embedded within the field, leading to a more nuanced understanding of the practice of algorithmic fairness research.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="452" id="S3.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Conference, Study Type, and Dataset Topic Domains Over Time. (a) The number of papers published by each conference trends upward over time. (b) Over all years, a majority of papers are retrospective (conducting empirical analyses on pre-existing datasets). (c) Papers examine an increasing variety of topic domains over time. Overall, finance generally prevails as the most popular dataset domain application, followed by criminal justice.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Quantitative Findings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Overall, we examine a total of 139 papers, with 52 and 87 from AIES and FAccT, respectively (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3.F1" title="Figure 1 ‣ 3.2 Data and Analysis ‣ 3 Methods ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></span></a>a). These papers map to a total of 124 unique datasets. Retrospective studies (research empirically examining fairness within pre-existing datasets) represented the majority of papers, followed by theory (research contributing theoretical guarantees of fairness definitions). The prevalence of prospective studies (research collecting and empirically examining novel data) remained relatively low across most years, increasing the most in 2022 (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3.F1" title="Figure 1 ‣ 3.2 Data and Analysis ‣ 3 Methods ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></span></a>b). Dataset domains spanned a wide range of topics, including finance, criminal justice, health, and politics. Finance emerged as the majority domain within most years (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S3.F1" title="Figure 1 ‣ 3.2 Data and Analysis ‣ 3 Methods ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></span></a>c).
<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.SS2" title="A.2 Study Design ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Appendix: Study Design</a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A2" title="Appendix B Fairness Definitions ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Appendix: Fairness Definitions</a> provide additional results concerning fairness formulations.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Data and Authorship Provenance</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F2" title="Figure 2 ‣ 4.1.1 Data and Authorship Provenance ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></span></a> visualizes geographic patterns in authorship provenance and dataset provenance through cartograms. These depict country size proportional to their representation.
In terms of author affiliation, the US held the greatest count and proportion of affiliations (412, 78.0%), followed by Germany (19, 3.5%), Canada (14, 2.7%), the United Kingdom (13, 2.5%), Australia (12, 2.3%), Italy (11, 2.0%), Switzerland (10, 1.9%), and India (10, 1.9%).
When broken down by year, we find that 80.6% of the papers published each year feature at least one author with ties to the United States (see e.g. <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.SS1" title="A.1 Data and Authorship Provenance ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Appendix: Data and Authorship Provenance</a>). For dataset provenance, the US again emerged as the most common country of origin (77, 72.6%), followed by Germany (9, 8.5%) and then Colombia, Rwanda, Australia, Burundi, Costa Rica, Finland, Hungary, Iceland, India, Kenya, Malawi, Mexico, Mozambique, New Zealand, Portugal, Senegal, South Africa, Sweden, Switzerland, Tanzania, Thailand, Uganda, the United Kingdom, Zambia, and Zimbabwe, all with fewer than three datasets (<math alttext="\leq" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mo id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><leq id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">≤</annotation></semantics></math>2.8% each, 18.9% collectively). In our sample, 15.9% of papers acknowledge at least one dataset limitation. In addition, 1.4% employ some form of mitigation, such as introducing additional data <cite class="ltx_cite ltx_citemacro_citep">(e.g., Dixon et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib28" title="">2018</a>)</cite> or re-annotating the data with an expert <cite class="ltx_cite ltx_citemacro_citep">(e.g., Buolamwini and Gebru, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib15" title="">2018</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="268" id="S4.F2.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Distribution of Author Country-Affiliation and Dataset Country-Affiliation.
These cartograms represent country sizes proportional to the count of (a) author affiliations and (b) datasets attributed to each country. The US emerges as the most highly represented country for both authorship origin and data provenance. Graphics created with <cite class="ltx_cite ltx_citemacro_citet">Gastner et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib36" title="">2018</a>)</cite>.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="426" id="S4.F3.g1" src="x3.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Sensitive Attributes Studied Over Time. Count reflects the number of times papers in our sample analyzed each sensitive attribute category in a given year.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Sensitive Attributes</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">To account for the wide range of variables studied by the algorithmic fairness community, we define “sensitive attributes” as the features across which a study aims to measure or ensure fairness. This broad definition includes features protected by law (e.g., “protected attributes” such as race or gender) as well as those with potential for social or economic discrimination (e.g., income). Papers in our sample examined a total of 49 sensitive attributes, in which gender, race, and age emerged as the top three categories of sensitive attributes in our sample (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F3" title="Figure 3 ‣ 4.1.1 Data and Authorship Provenance ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></span></a>).<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Some papers used multiple categories interchangeably (e.g., “sex/gender”, “race/ethnicity”), without providing definitions for each constituent category.
While we believe that many of these categories represent distinct social constructs (e.g., <cite class="ltx_cite ltx_citemacro_citep">Unger, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib86" title="">1979</a>; Lips, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib61" title="">2020</a></cite>), unpacking these groupings retrospectively proved impractical given the available information.
Thus, for the purposes of this work, we combine categories such as sex and gender.
However, we urge future work to carefully consider and define the specific social constructs under investigation.</span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">Our analysis revealed substantial variation in the labels assigned to each sensitive attribute (that is, variation in the category options for sensitive attributes, such as “female” or “Black”), even when papers relied on the same dataset (see Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F4" title="Figure 4 ‣ 4.1.2 Sensitive Attributes ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></span></a> &amp; <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F5" title="Figure 5 ‣ 4.1.2 Sensitive Attributes ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></span></a>). Race labels included “Black/African American”, “White/Caucasian”, “Asian/South Asian/East Asian/Pacific Islander”, “Hispanic/Latino/Mexican-American”, “Native American/American Indian/Eskimo”, and “unknown/other”. Age labels exhibited similar inconsistencies, employing various cutoff points such as 0-17/18+, 0-24/25+, 0-64/65+, as well as broader categorizations like “young/old” and systems using intervals of 5, 10, or 15 years. Gender displayed the least variation, with most papers employing “female/male”. Although some datasets included third categories such as “NA/other/not sure” <cite class="ltx_cite ltx_citemacro_citep">(Ekstrand et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib31" title="">2018</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib89" title="">2022</a>; Usunier et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib87" title="">2022</a>)</cite>, “non-binary or choose not to disclose ” <cite class="ltx_cite ltx_citemacro_citep">(Schoeffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib77" title="">2022</a>)</cite>, or “two spirit” <cite class="ltx_cite ltx_citemacro_citep">(Suresh et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib82" title="">2022</a>)</cite>, papers generally excluded these categories from analyses and empirical demonstrations.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Most papers (89.7%) failed to provide any definition for sensitive attribute labels. This issue was particularly pronounced for age, with 20.6% of studies investigating age neglecting to define the age ranges that they used.
Beyond simply defining the labels, most papers also omitted information regarding the source of their labels; researchers rarely clarified whether the labels originated from third-party observation or self-disclosure. Similarly, papers often did not provide any additional data or criteria that might have been used to determine the labels.
Papers in our sample, for instance, frequently aggregated multiple categories into broader labels—such as “young” and “old” or “African-American” and “non-African American”—without providing any explanation.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="953" id="S4.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Distribution of Label Formulations for Sensitive Attributes Across Fairness Studies. This alluvial diagram illustrates the range of formulations that fairness studies use to label sensitive attributes. Each colored band represents, on the left, one of the three most frequently studied sensitive attributes in our sample, and on the right, the number of studies utilizing a particular formulation. The grey bars in the middle indicate the number of unique categories employed within each formulation. Studies predominantly formulate gender as a “female/male” binary. Across race, the top two formulations are “Black/White” and “African-American/non-African-American”. Finally, age shows some increased diversity in the number of categories considered, though papers still exhibit a strong tendency towards binary formulations (e.g., 0-24 and 25+ or 0-64 and 65+).
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="486" id="S4.F5.g1" src="x5.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Binary Formulations for Sensitive Attributes. This alluvial diagram highlights the binary label formulations used for the three most studied sensitive attributes (gender, race, and age) within fairness studies. Each colored band represents, on the left, one of the three most frequently studied sensitive attributes in our sample, and on the right, the number of studies utilizing a particular formulation. The grey bars in the middle indicate the number of unique categories employed within each formulation.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Sociotechnical Analysis</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To further contextualize our quantitative findings, we conduct a sociotechnical analysis to explore and make explicit the unstated influences shaping algorithmic fairness research. This analysis focuses on identifying potential geographical, cultural, and implicit norms within the fairness research community and exploring how these biases might affect the diversity and representativeness of conducted studies. Given the interconnected nature of social and technical systems <cite class="ltx_cite ltx_citemacro_citep">(Cherns, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib19" title="">1976</a>; Dolata et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib29" title="">2022</a>)</cite>, these social norms and biases will ultimately influence the technological outcomes of algorithmic fairness research. Consequently, we consider whether these norms and biases affected the scope of research, the disclosure of limitations, and assumptions about global fungibility. For this analysis, we drew on our own experiences as scientists studying algorithmic fairness and as members of marginalized communities affected by deployed systems <cite class="ltx_cite ltx_citemacro_citep">(Collins, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib22" title="">1992</a>; Haraway, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib44" title="">1988</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Despite the existing discourse on bias and limitations in fairness research (see <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S2" title="2 Related Work ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Related Work</a>), we observed two striking patterns across the empirical proceedings at AIES and FAccT: a disproportionate bias toward the US in both author affiliations and dataset origin, and a pervasive tendency towards binary formulations of sensitive attributes, particularly for gender, race, and age.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>US-Centrism and the Limits of Global Fungibility</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Our meta-analysis reveals a clear and substantial bias toward the US in algorithmic fairness research. We find that 80.6% of papers published each year feature at least one US-based author, and that the US also dominates dataset provenance (72.6%; see <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F2" title="Figure 2 ‣ 4.1.1 Data and Authorship Provenance ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></span></a> &amp; <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.SS1" title="A.1 Data and Authorship Provenance ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Appendix: Data and Authorship Provenance</a>). In our sample, papers rarely acknowledged or addressed potential biases arising from the predominance of the US in author affiliations and dataset origins.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We observe similar patterns among other research hubs in the Global North. For instance, Germany (representing 3.5% of author affiliations and 8.5% of dataset origins), the UK (2.5% and 2.8%), and Australia (2.3% and 2.8%)—though less overall prominent than the US—each fail to acknowledge or mitigate their potential geographic biases.</span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">The very labels that researchers use to define sensitive attributes reveal a substantial bias toward American norms and values. For instance, papers in our sample predominantly frame race as a Black/White binary (e.g., “White/Non-White”, “Black/Non-Black”). This simplification not only obscures nuances within Black identity (e.g., distinguishing between African and African-American experiences; <cite class="ltx_cite ltx_citemacro_citep">Agyemang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib4" title="">2005</a></cite>), but also homogenizes a diverse array of backgrounds under the label “White” (e.g., Mexican, Caribbean, Puerto Rican, Middle Eastern, and North African identities; <cite class="ltx_cite ltx_citemacro_citep">Overmyer-Velázquez, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib71" title="">2013</a>; Denton and Massey, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib26" title="">1989</a>; Landale and Oropesa, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib58" title="">2002</a>; Awad et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib5" title="">2021</a></cite>). This binary framing reflects a cultural phenomenon specific to the US: in particular, the historical and ongoing salience of the Black/White racial dichotomy, within American culture <cite class="ltx_cite ltx_citemacro_citep">(Goldstein, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib38" title="">2006</a>; Jones, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib51" title="">2015</a>)</cite>. Similarly, a number of papers and datasets exploring fairness with respect to age use 25 and 65 years as key demarcation points (Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F4" title="Figure 4 ‣ 4.1.2 Sensitive Attributes ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">4</span></a> &amp; <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F5" title="Figure 5 ‣ 4.1.2 Sensitive Attributes ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">5</span></a>). This practice echoes US legal frameworks regarding adulthood, dependence status, and retirement <cite class="ltx_cite ltx_citemacro_citep">(Hamilton, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib43" title="">2016</a>; Agich, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib3" title="">2003</a>; Dailey and Rosenbury, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib24" title="">2018</a>; Boni-Saenz, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib12" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">These specific demarcation points cohere with US societal structures, but do not universally translate to other countries with different legal and social norms surrounding adulthood, retirement, and other life stages <cite class="ltx_cite ltx_citemacro_citep">(Malone and Rudner, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib64" title="">2011</a>; Bhabha, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib8" title="">2008</a>; OECD, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib70" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib69" title="">2009</a>)</cite>. These choices coalesce into a clear pattern of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.1">US-centrism</span>—the tendency to view the world through the political, economic, and social lens of American society <cite class="ltx_cite ltx_citemacro_citep">(Shabbar and Roberts, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib81" title="">2017</a>; Chaturvedi and Painter, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib18" title="">2007</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">The US-centrism pervading algorithmic fairness research inevitably shapes the field’s outputs. Research does not exist in a vacuum—it is profoundly influenced by the socio-cultural and legal environments in which it originates <cite class="ltx_cite ltx_citemacro_citep">(Hagerty and Rubinov, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib42" title="">2019</a>; Cole and Bertenthal, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib21" title="">2017</a>; Turner and Wiber, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib85" title="">2023</a>)</cite>. The definition of demographic categories demands particular attention here. Though often framed as objective and neutral, demographic labels carry a long history of weaponization, particularly against marginalized groups. Notions of lineage, morality, aesthetics, sexuality, and gender have long been used to define and differentiate social groups within the US, often to the detriment of those deemed “different” (see e.g. <cite class="ltx_cite ltx_citemacro_citep">Hollinger, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib48" title="">2005</a>; Kitano et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib54" title="">1984</a>; DeCuir-Gunby, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib25" title="">2014</a>; Harris, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib46" title="">1993</a>; Gilman and Peters, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib37" title="">1999</a>; Said, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib74" title="">1993</a>; Tasca et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib83" title="">2012</a>; Kamin and Omari, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib52" title="">1998</a>; Gooren and Gijs, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib39" title="">2015</a></cite>). By neglecting to explicitly define sensitive attributes and acknowledge this history of US-centric categorization, algorithmic fairness research obscures the power dynamics inherent in its label choices. The lack of precise definitions effectively shields these conceptual frameworks from the critical scrutiny needed to deconstruct and remake systems of power. As a result, algorithmic fairness research risks encoding US-centric inequalities and social hierarchies into the very systems intended to promote fairness.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">The uncritical adoption of US-centric norms extends beyond label choices to additional aspects of research design. For instance, researchers in our sample rarely justified their choice of fairness metrics, evaluation datasets, or even problem framing. These design choices, often shaped by the US socio-political context, are presented as default or universally applicable without acknowledging their inherent limitations. This assumption of global fungibility obscures the need for localized benchmarks and diverse perspectives (cf. <cite class="ltx_cite ltx_citemacro_citep">Castro Torres and Alburez-Gutierrez, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib17" title="">2022</a></cite>). The subsequent lack of effort to contextualize research further entrenches the predominance of US-specific norms and design choices. As a result, the assumed generalizability of these research findings to other contexts goes largely unquestioned, and the perceived preeminence of US-centric values in the field remains unchallenged <cite class="ltx_cite ltx_citemacro_citep">(Birhane et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib11" title="">2022b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Binary Formulations and the Erasure of Intersectional Realities</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Our meta-analysis reveals a second concerning trend in algorithmic fairness research: the pervasive use of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">binary formulations</span> for sensitive attributes (see <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#S4.F4" title="Figure 4 ‣ 4.1.2 Sensitive Attributes ‣ 4.1 Quantitative Findings ‣ 4 Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></span></a>). This binary framing imposes an overly simplistic structure that obscures the complexities of lived experiences and positions groups as isolated, opposing poles on a single axis of power and privilege. For instance, fairness studies often categorize age as “young” versus “old”, despite the enormous range of communities that exist within these two age groups. This dualistic logic presumes homogeneity within each class, ignoring the diverse experiences and identities within those broad categories.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">These assumptions carry profound implications for algorithmic fairness.
Binary formulations minimize the unique challenges faced by members of understudied groups. For instance, the “male/female” binary erases individuals who identify outside this dichotomy. Similarly, studies that focus on “Black/White” comparisons in the US obscure the experiences of Hispanic, Asian, and Indigenous communities. Yet research efforts that adopt a binary approach often incorrectly assume that their findings concerning harms and mitigations will translate seamlessly across groups.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Ironically, by treating each of its categories in isolation, this framework equates distinct experiences of discrimination, assuming that—for instance—bias toward racial and gender minorities pose interchangeable challenges.
In essence, the binary approach to fairness encourages a view of discrimination as a one-size-fits-all problem, neglecting the nuanced ways in which different forms of oppression intersect and interact. By overlooking the interconnected nature of social categories, binary formulations fail to address the compounding effects of intersectionality in the matrix of domination: axes of discrimination are neither separable nor additive <cite class="ltx_cite ltx_citemacro_citep">(Crenshaw, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib23" title="">1989</a>; Collins, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib22" title="">1992</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">This flawed logic can burden minority groups with testing the performance of putatively fair—but actually misaligned—AI systems. Unfortunately, the emphasis on binary comparisons in fairness research creates further issues by implicitly positioning groups against each other, rather than collaborators against a system of oppression (cf. the “racial wedge” in political discourse; <cite class="ltx_cite ltx_citemacro_citep">Puri, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib73" title="">2016</a></cite>; and the trans-exclusionary movement in modern feminism; <cite class="ltx_cite ltx_citemacro_citep">Caslin, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib16" title="">2024</a>; Fahs, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib33" title="">2024</a>; Serano, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib80" title="">2016</a></cite>). This combination of inequitable burdens and manufactured divisions ultimately undermines the very goals of algorithmic fairness, hindering the development of truly equitable and inclusive systems.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1">The binary approach to fairness not only oversimplifies complex social identities, but also tends to treat those identities as fixed and unchanging. Papers in our sample rarely discussed how or disclosed whether identity labels could change over time. By framing sensitive attributes as static and binary categories, algorithmic fairness research overlooks the dynamic and fluid nature of identity <cite class="ltx_cite ltx_citemacro_citep">(Keyes, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib53" title="">2019</a>; Tomasev et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib84" title="">2021</a>; Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib62" title="">2022</a>)</cite>. The fluidity and complexity of identity demands that algorithmic fairness research move beyond static and binary solutions, embracing approaches that can adapt to the intersectional and evolving nature of social inequalities.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our meta-analysis surfaces two prominent trends in contemporary algorithmic fairness research: a disproportionate US-centric bias in author backgrounds, data origins, and design choices; and a widespread policy of casting sensitive attributes into binaries.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Despite popular depictions of research as an objective pursuit of truth, we recognize research as <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">social praxis</span>—a process inherently shaped by contemporary norms and historical context <cite class="ltx_cite ltx_citemacro_citep">(Hochstein, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib47" title="">2019</a>; Kuhn, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib57" title="">1961</a>; Hacking, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib41" title="">1983</a>)</cite>. Early in the development of a field, certain norms and design choices can help scope feasible research questions, providing an important starting point for researchers and scientists. However, over time, these initial frameworks become so deeply ingrained that they limit the scope of inquiry and exclude alternative perspectives.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">US-centrism and binary logic represent two such frameworks. Current praxis advances a myth of algorithmic fairness as a fungible, abstract, and universal phenomena. However, this myth breaks apart when confronted with the dynamism of identity and the plurality of experience around the world.
To move beyond this myth, the fairness research community must chart new paths forward, guided by the principles of inclusivity, representativeness, and cultural specificity.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The AIES and FAccT conferences reflect two important sites of dialogue for the algorithmic fairness community field. As a result, this meta-analysis offers a valuable snapshot of current trends in fairness research. In the future, expanding the scope of inquiry to other sites of study will help enrich and contextualize our understanding of contemporary norms and practices in fairness research. Promising sites include generalist conference venues (e.g., the Conference and Workshop on Neural Information Processing Systems), pre-print repositories (e.g., arXiv), and journals (e.g., <span class="ltx_text ltx_font_italic" id="S5.p4.1.1">Big Data &amp; Society</span>). Future work should also extend beyond the examination of academic discourse. An important step for this line of research will be to investigate how developers conceptualize and implement fairness within the systems they deploy to the real world. A critical examination of the social and structural factors shaping research practices across these sites will help identify biases and encourage a more inclusive and representative vision of algorithmic fairness.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Researchers and ethicists have proposed several solutions in this direction. Many prioritize transparency and self-reflection, advocating for explicit disclosure of the values embedded in technical work and operational definitions (e.g. <cite class="ltx_cite ltx_citemacro_citep">van Berkel et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib88" title="">2023</a></cite>). Others emphasize diversifying representation across datasets (e.g. <cite class="ltx_cite ltx_citemacro_citep">Sambasivan, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib75" title="">2021</a></cite>), within research teams (e.g. <cite class="ltx_cite ltx_citemacro_citep">Laufer et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib59" title="">2022</a></cite>), and through participatory initiatives (e.g. <cite class="ltx_cite ltx_citemacro_citep">Gadiraju et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib35" title="">2023</a></cite>). A final set of solutions focuses on grounding research, calling for greater engagement with real-world issues (e.g. <cite class="ltx_cite ltx_citemacro_citep">Birhane et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib11" title="">2022b</a></cite>) and rigorous ethnographic research (e.g. <cite class="ltx_cite ltx_citemacro_citep">Hagerty and Rubinov, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib42" title="">2019</a>; Marda and Narayan, <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib65" title="">2021</a>; Martin Jr et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib66" title="">2020</a></cite>).</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The current focus on the US and on binary representations of sensitive attributes threatens to perpetuate a narrow and misleading perspective on algorithmic fairness. To overcome these limitations, the research community must expand its scope to encompass diverse social and cultural settings, particularly those outside the Global North. This expansion also requires moving beyond simplistic binary classifications and engaging with the involute, intersectional realities of bias, identity, and community <cite class="ltx_cite ltx_citemacro_citep">(Selbst et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib78" title="">2019</a>)</cite>. Qualitative methods and participatory approaches <cite class="ltx_cite ltx_citemacro_citep">(Birhane et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib10" title="">2022a</a>; Martin Jr et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#bib.bib66" title="">2020</a>)</cite> will be crucial for developing datasets and fairness frameworks tailored to these conditions. Overall, researchers should seek to challenge the homogenizing effect of purely technical solutions and recenter the complex lived experiences of marginalized communities.</p>
</div>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our meta-analysis of algorithmic fairness papers reveals a field grappling with several fundamental tensions. While researchers strive to develop fair and unbiased systems, current research practices often default to US-centric perspectives and binary representations of sensitive attributes. As a result, current praxis not only can compromise the credibility of research insights, but also may lead to policy solutions that are poorly aligned with the needs of diverse communities. Our findings underscore the urgent need for fairness researchers to reflect on the norms embedded in their work, echoing and expanding calls for more inclusive, representative, and context-specific approaches to algorithmic fairness.</p>
</div>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Positionality Statement</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper emerges from a collective effort by researchers situated at the intersection of technology and marginalized communities. Our research backgrounds include the fields of algorithmic fairness, human-computer interaction, and critical algorithmic studies, with prior experience studying communities outside the Global North as well as issues afflicting LGBTQ+ populations. These experiences motivate our critical perspective on the existing AI ethics landscape and its inherent biases towards the US and the Global North. The lead author, for example, is a US-based doctoral student holding multiple intersecting identities—some marginalized, and others affording greater privilege. Navigating these complexities shapes their perspective on power dynamics within AI ethics discourse. Thus, we draw on our own identities and experiences to understand and motivate our research. While this personal perspective provides valuable insights, we acknowledge that it also introduces potential biases and welcome constructive engagement and alternative perspectives to challenge and enrich out understanding.</p>
</div>
</section>
<section class="ltx_section" id="S8" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We would like to thank Seliem El-Sayed, Shakir Mohamed, and Simon Osindero for reviewing this work. We would also like to thank Iason Gabriel for his comments, insights, and advice throughout the process.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdu et al. (2023)</span>
<span class="ltx_bibblock">
A. A. Abdu, I. V. Pasquetto, and A. Z. Jacobs.

</span>
<span class="ltx_bibblock">An empirical analysis of racial categories in the algorithmic
fairness literature.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2023 ACM Conference on Fairness,
Accountability, and Transparency</em>, pages 1324–1333, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acuna and Liang (2021)</span>
<span class="ltx_bibblock">
D. E. Acuna and L. Liang.

</span>
<span class="ltx_bibblock">Are ai ethics conferences different and more diverse compared to
traditional computer science conferences?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 307–315, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agich (2003)</span>
<span class="ltx_bibblock">
G. Agich.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Dependence and autonomy in old age: An ethical framework for
long-term care</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agyemang et al. (2005)</span>
<span class="ltx_bibblock">
C. Agyemang, R. Bhopal, and M. Bruijnzeels.

</span>
<span class="ltx_bibblock">Negro, black, black african, african caribbean, african american or
what? labelling african origin populations in the health arena in the 21st
century.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Epidemiology &amp; Community Health</em>, 59(12):1014–1018, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awad et al. (2021)</span>
<span class="ltx_bibblock">
G. H. Awad, H. Hashem, and H. Nguyen.

</span>
<span class="ltx_bibblock">Identity and ethnic/racial self-labeling among americans of arab or
middle eastern and north african descent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Identity</em>, 21(2):115–130, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barocas et al. (2023)</span>
<span class="ltx_bibblock">
S. Barocas, M. Hardt, and A. Narayanan.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Fairness and machine learning: Limitations and opportunities</em>.

</span>
<span class="ltx_bibblock">MIT Press, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benjamin (2020)</span>
<span class="ltx_bibblock">
R. Benjamin.

</span>
<span class="ltx_bibblock">Race after technology: Abolitionist tools for the new jim code, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhabha (2008)</span>
<span class="ltx_bibblock">
J. Bhabha.

</span>
<span class="ltx_bibblock">Independent children, inconsistent adults: International child
migration and the legal framework.

</span>
<span class="ltx_bibblock">2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binns (2020)</span>
<span class="ltx_bibblock">
R. Binns.

</span>
<span class="ltx_bibblock">On the apparent conflict between individual and group fairness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2020 conference on fairness,
accountability, and transparency</em>, pages 514–524, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane et al. (2022a)</span>
<span class="ltx_bibblock">
A. Birhane, W. Isaac, V. Prabhakaran, M. Diaz, M. C. Elish, I. Gabriel, and
S. Mohamed.

</span>
<span class="ltx_bibblock">Power to the people? opportunities and challenges for participatory
ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Equity and Access in Algorithms, Mechanisms, and Optimization</em>,
pages 1–8, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane et al. (2022b)</span>
<span class="ltx_bibblock">
A. Birhane, E. Ruane, T. Laurent, M. S. Brown, J. Flowers, A. Ventresque, and
C. L. Dancy.

</span>
<span class="ltx_bibblock">The forgotten margins of ai ethics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, pages 948–958, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boni-Saenz (2022)</span>
<span class="ltx_bibblock">
A. A. Boni-Saenz.

</span>
<span class="ltx_bibblock">Legal age.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">BCL Rev.</em>, 63:521, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borenstein et al. (2021)</span>
<span class="ltx_bibblock">
M. Borenstein, L. V. Hedges, J. P. Higgins, and H. R. Rothstein.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Introduction to meta-analysis</em>.

</span>
<span class="ltx_bibblock">John Wiley &amp; Sons, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bunge (2018)</span>
<span class="ltx_bibblock">
M. Bunge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Critical approaches to science and philosophy</em>.

</span>
<span class="ltx_bibblock">Routledge, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buolamwini and Gebru (2018)</span>
<span class="ltx_bibblock">
J. Buolamwini and T. Gebru.

</span>
<span class="ltx_bibblock">Gender shades: Intersectional accuracy disparities in commercial
gender classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Conference on fairness, accountability and transparency</em>,
pages 77–91. PMLR, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caslin (2024)</span>
<span class="ltx_bibblock">
S. Caslin.

</span>
<span class="ltx_bibblock">Trans feminism and the women’s liberation movement in britain, c.
1970–1980.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Gender &amp; History</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro Torres and Alburez-Gutierrez (2022)</span>
<span class="ltx_bibblock">
A. F. Castro Torres and D. Alburez-Gutierrez.

</span>
<span class="ltx_bibblock">North and south: Naming practices and the hidden dimension of global
disparities in knowledge production.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the National Academy of Sciences</em>, 119(10):e2119373119, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaturvedi and Painter (2007)</span>
<span class="ltx_bibblock">
S. Chaturvedi and J. Painter.

</span>
<span class="ltx_bibblock">Whose world, whose order? spatiality, geopolitics and the limits of
the world order concept.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Cooperation and Conflict</em>, 42(4):375–395,
2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherns (1976)</span>
<span class="ltx_bibblock">
A. Cherns.

</span>
<span class="ltx_bibblock">The principles of sociotechnical design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Human relations</em>, 29(8):783–792, 1976.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chien and Danks (2023)</span>
<span class="ltx_bibblock">
J. Chien and D. Danks.

</span>
<span class="ltx_bibblock">Fairness vs. personalization: Towards equity in epistemic utility.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2309.11503</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cole and Bertenthal (2017)</span>
<span class="ltx_bibblock">
S. A. Cole and A. Bertenthal.

</span>
<span class="ltx_bibblock">Science, technology, society, and law.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Annual Review of Law and Social Science</em>, 13:351–371, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collins (1992)</span>
<span class="ltx_bibblock">
P. H. Collins.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Black feminist thought: Knowledge, consciousness, and the
politics of empowerment</em>.

</span>
<span class="ltx_bibblock">routledge, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crenshaw (1989)</span>
<span class="ltx_bibblock">
K. Crenshaw.

</span>
<span class="ltx_bibblock">Demarginalizing the intersection of race and sex: A black feminist
critique of antidiscrimination doctrine, feminist theory and antiracist
politics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">University of Chicago Legal Forum</em>, volume 1989, page 8,
1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dailey and Rosenbury (2018)</span>
<span class="ltx_bibblock">
A. C. Dailey and L. A. Rosenbury.

</span>
<span class="ltx_bibblock">The new law of the child.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">The Yale Law Journal</em>, pages 1448–1537, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeCuir-Gunby (2014)</span>
<span class="ltx_bibblock">
J. T. DeCuir-Gunby.

</span>
<span class="ltx_bibblock">“proving your skin is white, you can have everything”: Race,
racial identity, and property rights in whiteness in the supreme court case
of josephine decuir.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Critical race theory in education</em>, pages 103–126.
Routledge, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denton and Massey (1989)</span>
<span class="ltx_bibblock">
N. A. Denton and D. S. Massey.

</span>
<span class="ltx_bibblock">Racial identity among caribbean hispanics: The effect of double
minority status on residential segregation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">American Sociological Review</em>, pages 790–808, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’ignazio and Klein (2020)</span>
<span class="ltx_bibblock">
C. D’ignazio and L. F. Klein.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Data feminism</em>.

</span>
<span class="ltx_bibblock">MIT press, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dixon et al. (2018)</span>
<span class="ltx_bibblock">
L. Dixon, J. Li, J. Sorensen, N. Thain, and L. Vasserman.

</span>
<span class="ltx_bibblock">Measuring and mitigating unintended bias in text classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 67–73, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolata et al. (2022)</span>
<span class="ltx_bibblock">
M. Dolata, S. Feuerriegel, and G. Schwabe.

</span>
<span class="ltx_bibblock">A sociotechnical view of algorithmic fairness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Information Systems Journal</em>, 32(4):754–818, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2012)</span>
<span class="ltx_bibblock">
C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel.

</span>
<span class="ltx_bibblock">Fairness through awareness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 3rd innovations in theoretical computer
science conference</em>, pages 214–226, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekstrand et al. (2018)</span>
<span class="ltx_bibblock">
M. D. Ekstrand, M. Tian, I. M. Azpiazu, J. D. Ekstrand, O. Anuyah, D. McNeill,
and M. S. Pera.

</span>
<span class="ltx_bibblock">All the cool kids, how do they fit in?: Popularity and demographic
biases in recommender evaluation and effectiveness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Conference on fairness, accountability and transparency</em>,
pages 172–186. PMLR, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eubanks (2018)</span>
<span class="ltx_bibblock">
V. Eubanks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Automating inequality: How high-tech tools profile, police, and
punish the poor</em>.

</span>
<span class="ltx_bibblock">St. Martin’s Press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fahs (2024)</span>
<span class="ltx_bibblock">
B. Fahs.

</span>
<span class="ltx_bibblock">The urgent need for radical feminism today.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Signs: Journal of Women in Culture and Society</em>, 49(2):479–497, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fook (1999)</span>
<span class="ltx_bibblock">
J. Fook.

</span>
<span class="ltx_bibblock">Reflexivity as method.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Annual Review of Health Social Science</em>, 9(1):11–20, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadiraju et al. (2023)</span>
<span class="ltx_bibblock">
V. Gadiraju, S. Kane, S. Dev, A. Taylor, D. Wang, E. Denton, and R. Brewer.

</span>
<span class="ltx_bibblock">“i wouldn’t say offensive but…”: Disability-centered
perspectives on large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2023 ACM Conference on Fairness,
Accountability, and Transparency</em>, pages 205–216, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gastner et al. (2018)</span>
<span class="ltx_bibblock">
M. T. Gastner, V. Seguy, and P. More.

</span>
<span class="ltx_bibblock">Fast flow-based algorithm for creating density-equalizing map
projections.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the National Academy of Sciences</em>, 115(10):E2156–E2164, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilman and Peters (1999)</span>
<span class="ltx_bibblock">
S. L. Gilman and W. Peters.

</span>
<span class="ltx_bibblock">Making the body beautiful: a cultural history of aesthetic surgery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Canadian Medical Association. Journal</em>, 161(12):1565, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein (2006)</span>
<span class="ltx_bibblock">
E. L. Goldstein.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">The price of whiteness: Jews, race, and American identity</em>.

</span>
<span class="ltx_bibblock">Princeton University Press, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gooren and Gijs (2015)</span>
<span class="ltx_bibblock">
L. Gooren and L. Gijs.

</span>
<span class="ltx_bibblock">Medicalization of homosexuality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">The International Encyclopedia of Human Sexuality</em>, pages
721–817, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guillemin and Gillam (2004)</span>
<span class="ltx_bibblock">
M. Guillemin and L. Gillam.

</span>
<span class="ltx_bibblock">Ethics, reflexivity, and “ethically important moments” in
research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Qualitative inquiry</em>, 10(2):261–280, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hacking (1983)</span>
<span class="ltx_bibblock">
I. Hacking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Representing and intervening: Introductory topics in the
philosophy of natural science</em>.

</span>
<span class="ltx_bibblock">Cambridge university press, 1983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hagerty and Rubinov (2019)</span>
<span class="ltx_bibblock">
A. Hagerty and I. Rubinov.

</span>
<span class="ltx_bibblock">Global ai ethics: a review of the social impacts and ethical
implications of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:1907.07892</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton (2016)</span>
<span class="ltx_bibblock">
V. E. Hamilton.

</span>
<span class="ltx_bibblock">Adulthood in law and culture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Tul. L. Rev.</em>, 91:55, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haraway (1988)</span>
<span class="ltx_bibblock">
D. Haraway.

</span>
<span class="ltx_bibblock">Situated knowledges: The science question in feminism and the
privilege of partial perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Feminist Studies</em>, 14(3):575–599, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harding (1991)</span>
<span class="ltx_bibblock">
S. Harding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Whose science? Whose knowledge?: Thinking from women’s lives</em>.

</span>
<span class="ltx_bibblock">Cornell University Press, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harris (1993)</span>
<span class="ltx_bibblock">
C. I. Harris.

</span>
<span class="ltx_bibblock">Whiteness as property.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Harvard law review</em>, pages 1707–1791, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochstein (2019)</span>
<span class="ltx_bibblock">
E. Hochstein.

</span>
<span class="ltx_bibblock">How metaphysical commitments shape the study of psychological
mechanisms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Theory &amp; Psychology</em>, 29(5):579–600,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hollinger (2005)</span>
<span class="ltx_bibblock">
D. A. Hollinger.

</span>
<span class="ltx_bibblock">The one drop rule &amp; the one hate rule.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Daedalus</em>, 134(1):18–28, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iliadis and Russo (2016)</span>
<span class="ltx_bibblock">
A. Iliadis and F. Russo.

</span>
<span class="ltx_bibblock">Critical data studies: An introduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Big Data &amp; Society</em>, 3(2):2053951716674238, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irani et al. (2010)</span>
<span class="ltx_bibblock">
L. Irani, J. Vertesi, P. Dourish, K. Philip, and R. E. Grinter.

</span>
<span class="ltx_bibblock">Postcolonial computing: a lens on design and development.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the SIGCHI conference on human factors in
computing systems</em>, pages 1311–1320, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones (2015)</span>
<span class="ltx_bibblock">
V. Jones.

</span>
<span class="ltx_bibblock">The black-white dichotomy of race: Influence of a predominantly white
environment on multiracial identity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Higher Education in Review</em>, 12, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamin and Omari (1998)</span>
<span class="ltx_bibblock">
L. J. Kamin and S. Omari.

</span>
<span class="ltx_bibblock">Race, head size, and intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">South African Journal of Psychology</em>, 28(3):119–128, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keyes (2019)</span>
<span class="ltx_bibblock">
O. Keyes.

</span>
<span class="ltx_bibblock">Counting the countless: Why data science is a profound threat for
queer people, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://reallifemag.com/counting-the-countless/" title="">https://reallifemag.com/counting-the-countless/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitano et al. (1984)</span>
<span class="ltx_bibblock">
H. H. Kitano, W.-T. Yeung, L. Chai, and H. Hatanaka.

</span>
<span class="ltx_bibblock">Asian-american interracial marriage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Journal of Marriage and the Family</em>, pages 179–190, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kleanthous et al. (2022)</span>
<span class="ltx_bibblock">
S. Kleanthous, M. Kasinidou, P. Barlas, and J. Otterbacher.

</span>
<span class="ltx_bibblock">Perception of fairness in algorithmic decisions: future developers’
perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Patterns</em>, 3(1), 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koch et al. (2021)</span>
<span class="ltx_bibblock">
B. Koch, E. Denton, A. Hanna, and J. G. Foster.

</span>
<span class="ltx_bibblock">Reduced, reused and recycled: The life of a dataset in machine
learning research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2112.01716</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn (1961)</span>
<span class="ltx_bibblock">
T. S. Kuhn.

</span>
<span class="ltx_bibblock">The function of measurement in modern physical science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Isis</em>, 52(2):161–193, 1961.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landale and Oropesa (2002)</span>
<span class="ltx_bibblock">
N. S. Landale and R. S. Oropesa.

</span>
<span class="ltx_bibblock">White, black, or puerto rican? racial self-identification among
mainland and island puerto ricans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Social Forces</em>, 81(1):231–254, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laufer et al. (2022)</span>
<span class="ltx_bibblock">
B. Laufer, S. Jain, A. F. Cooper, J. Kleinberg, and H. Heidari.

</span>
<span class="ltx_bibblock">Four years of facct: A reflexive, mixed-methods analysis of research
contributions, shortcomings, and future prospects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, pages 401–426, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lepri et al. (2018)</span>
<span class="ltx_bibblock">
B. Lepri, N. Oliver, E. Letouzé, A. Pentland, and P. Vinck.

</span>
<span class="ltx_bibblock">Fair, transparent, and accountable algorithmic decision-making
processes: The premise, the proposed solutions, and the open challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Philosophy &amp; Technology</em>, 31(4):611–627,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lips (2020)</span>
<span class="ltx_bibblock">
H. M. Lips.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Sex and gender: An introduction</em>.

</span>
<span class="ltx_bibblock">Waveland Press, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
C. Lu, J. Kay, and K. McKee.

</span>
<span class="ltx_bibblock">Subverting machines, fluctuating identities: Re-learning human
categorization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, FAccT ’22, page 1005–1015, New York, NY, USA, 2022.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450393522.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1145/3531146.3533161" title="">10.1145/3531146.3533161</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3531146.3533161" title="">https://doi.org/10.1145/3531146.3533161</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lum and Isaac (2016)</span>
<span class="ltx_bibblock">
K. Lum and W. Isaac.

</span>
<span class="ltx_bibblock">To predict and serve?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Signif. (Oxf.)</em>, 13(5):14–19, Oct. 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malone and Rudner (2011)</span>
<span class="ltx_bibblock">
K. Malone and J. Rudner.

</span>
<span class="ltx_bibblock">Global perspectives on children’s independent mobility: a
socio-cultural comparison and theoretical discussion of children’s lives in
four countries in asia and africa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Global Studies of Childhood</em>, 1(3):243–259, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marda and Narayan (2021)</span>
<span class="ltx_bibblock">
V. Marda and S. Narayan.

</span>
<span class="ltx_bibblock">On the importance of ethnographic methods in ai research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Nature Machine Intelligence</em>, 3(3):187–189, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin Jr et al. (2020)</span>
<span class="ltx_bibblock">
D. Martin Jr, V. Prabhakaran, J. Kuhlberg, A. Smart, and W. S. Isaac.

</span>
<span class="ltx_bibblock">Extending the machine learning abstraction boundary: A complex
systems approach to incorporate societal context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2006.09663</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCabe and Holmes (2009)</span>
<span class="ltx_bibblock">
J. L. McCabe and D. Holmes.

</span>
<span class="ltx_bibblock">Reflexivity, critical qualitative research and emancipation: A
foucauldian perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Journal of advanced nursing</em>, 65(7):1518–1526, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabi et al. (2021)</span>
<span class="ltx_bibblock">
N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan.

</span>
<span class="ltx_bibblock">A survey on bias and fairness in machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">ACM Computing Surveys (CSUR)</em>, 54(6):1–35,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OECD (2009)</span>
<span class="ltx_bibblock">
OECD.

</span>
<span class="ltx_bibblock">Social policy division: Directorate of employment, labour and social
affairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">OECD family database</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OECD (2021)</span>
<span class="ltx_bibblock">
OECD.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Pensions at a glance 2021: OECD and G20 indicators</em>.

</span>
<span class="ltx_bibblock">Organisation for Economic Co-operation and Development OECD, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Overmyer-Velázquez (2013)</span>
<span class="ltx_bibblock">
M. Overmyer-Velázquez.

</span>
<span class="ltx_bibblock">Good neighbors and white mexicans: Constructing race and nation on
the mexico-us border.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Journal of American Ethnic History</em>, 33(1):5–34, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2023)</span>
<span class="ltx_bibblock">
J. Pfeiffer, J. Gutschow, C. Haas, F. Möslein, O. Maspfuhl, F. Borgers, and
S. Alpsancar.

</span>
<span class="ltx_bibblock">Algorithmic fairness in ai: An interdisciplinary view.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Business &amp; Information Systems Engineering</em>, 65(2):209–222, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puri (2016)</span>
<span class="ltx_bibblock">
J. Puri.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Sexual states</em>.

</span>
<span class="ltx_bibblock">Duke University Press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Said (1993)</span>
<span class="ltx_bibblock">
E. Said.

</span>
<span class="ltx_bibblock">Culture and imperialism (london: Chatto and windus).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Culture and Imperialism</em>, pages 1–28, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sambasivan (2021)</span>
<span class="ltx_bibblock">
N. Sambasivan.

</span>
<span class="ltx_bibblock">Seeing like a dataset from the global south.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Interactions</em>, 28(4):76–78, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sambasivan et al. (2021)</span>
<span class="ltx_bibblock">
N. Sambasivan, E. Arnesen, B. Hutchinson, T. Doshi, and V. Prabhakaran.

</span>
<span class="ltx_bibblock">Re-imagining algorithmic fairness in india and beyond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency</em>, pages 315–328, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schoeffer et al. (2022)</span>
<span class="ltx_bibblock">
J. Schoeffer, N. Kuehl, and Y. Machowski.

</span>
<span class="ltx_bibblock">“there is not enough information”: On the effects of explanations
on perceptions of informational fairness and trustworthiness in automated
decision-making.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Proceedings of the 2022 ACM Conference on Fairness,
Accountability, and Transparency</em>, pages 1616–1628, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selbst et al. (2019)</span>
<span class="ltx_bibblock">
A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi.

</span>
<span class="ltx_bibblock">Fairness and abstraction in sociotechnical systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the conference on fairness, accountability,
and transparency</em>, pages 59–68, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Septiandri et al. (2023)</span>
<span class="ltx_bibblock">
A. A. Septiandri, M. Constantinides, M. Tahaei, and D. Quercia.

</span>
<span class="ltx_bibblock">Weird faccts: How western, educated, industrialized, rich, and
democratic is facct?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2305.06415</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serano (2016)</span>
<span class="ltx_bibblock">
J. Serano.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Whipping girl: A transsexual woman on sexism and the
scapegoating of femininity</em>.

</span>
<span class="ltx_bibblock">Hachette UK, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shabbar and Roberts (2017)</span>
<span class="ltx_bibblock">
S. Shabbar and N. Roberts.

</span>
<span class="ltx_bibblock">Authoritative points of references as the grounding for innovation
and progress.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">International Institute for Islamic Thought</em>, pages 86–92,
2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suresh et al. (2022)</span>
<span class="ltx_bibblock">
H. Suresh, R. Movva, A. L. Dogan, R. Bhargava, I. Cruxen, Á. M. Cuba,
G. Taurino, W. So, and C. D’Ignazio.

</span>
<span class="ltx_bibblock">Towards intersectional feminist and participatory ml: A case study in
supporting feminicide counterdata collection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, pages 667–678, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tasca et al. (2012)</span>
<span class="ltx_bibblock">
C. Tasca, M. Rapetti, M. G. Carta, and B. Fadda.

</span>
<span class="ltx_bibblock">Women and hysteria in the history of mental health.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Clinical practice and epidemiology in mental health: CP &amp;
EMH</em>, 8:110, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomasev et al. (2021)</span>
<span class="ltx_bibblock">
N. Tomasev, K. R. McKee, J. Kay, and S. Mohamed.

</span>
<span class="ltx_bibblock">Fairness for unobserved characteristics: Insights from technological
impacts on queer communities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 254–265, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turner and Wiber (2023)</span>
<span class="ltx_bibblock">
B. Turner and M. G. Wiber.

</span>
<span class="ltx_bibblock">Legal pluralism and science and technology studies: Exploring sources
of the legal pluriverse.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Science, Technology, &amp; Human Values</em>, 48(3):457–474, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unger (1979)</span>
<span class="ltx_bibblock">
R. K. Unger.

</span>
<span class="ltx_bibblock">Toward a redefinition of sex and gender.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">American psychologist</em>, 34(11):1085, 1979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Usunier et al. (2022)</span>
<span class="ltx_bibblock">
N. Usunier, V. Do, and E. Dohmatob.

</span>
<span class="ltx_bibblock">Fast online ranking with fairness of exposure.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Proceedings of the 2022 ACM Conference on Fairness,
Accountability, and Transparency</em>, pages 2157–2167, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Berkel et al. (2023)</span>
<span class="ltx_bibblock">
N. van Berkel, Z. Sarsenbayeva, and J. Goncalves.

</span>
<span class="ltx_bibblock">The methodology of studying fairness perceptions in artificial
intelligence: Contrasting chi and facct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">International Journal of Human-Computer Studies</em>, 170:102954, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Y. Yang, A. Gupta, J. Feng, P. Singhal, V. Yadav, Y. Wu, P. Natarajan,
V. Hedau, and J. Joo.

</span>
<span class="ltx_bibblock">Enhancing fairness in face detection in computer vision systems by
demographic bias mitigation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics,
and Society</em>, pages 813–822, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zarsky (2016)</span>
<span class="ltx_bibblock">
T. Zarsky.

</span>
<span class="ltx_bibblock">The trouble with algorithmic decisions: An analytic road map to
examine efficiency and fairness in automated and opaque decision making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Science, Technology, &amp; Human Values</em>, 41(1):118–132, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou and Khern-am nuai (2023)</span>
<span class="ltx_bibblock">
L. Zou and W. Khern-am nuai.

</span>
<span class="ltx_bibblock">Ai and housing discrimination: the case of mortgage applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">AI and Ethics</em>, 3(4):1271–1281, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Expanded Results</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Data and Authorship Provenance</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.F6" title="Figure 6 ‣ A.1 Data and Authorship Provenance ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></span></a> shows the geographic distribution of authorship affiliations over time.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="A1.F6.g1" src="x6.png" width="448"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="A1.F6.3.2" style="font-size:90%;">Geographic Representation in Authorship Affiliations Over Time. The US consistently contributes the largest share of affiliations over all years included in our sample.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Study Design</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Fairness research papers can be categorized by the methods they apply to their proposed fairness definitions: empirical experiments that collect new data with a deployed intervention (<span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.1">prospective</span>); empirical simulations with existing datasets (<span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.2">retrospective</span>); or proofs using mathematical guarantees (<span class="ltx_text ltx_font_italic" id="A1.SS2.p1.1.3">theory</span>). Examining our sample through the lens of these categories reveals a clear trend: papers most frequently rely on retrospective analyses to study algorithmic fairness (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.F7" title="Figure 7 ‣ A.2 Study Design ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></span></a>a). This approach predominates across all years in our sample. In terms of dataset type, we observe a notable shift towards using synthetic data after 2018. The most common non-synthetic datasets (e.g., COMPAS, UCI Adult, and German Credit) disclose a focus on applications within financial and criminal justice contexts, though the exact pattern varies from year to year (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.F7" title="Figure 7 ‣ A.2 Study Design ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></span></a>b).</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">We next analyze aspects of the algorithmic interventions proposed. Our results indicate a clear preference for in-processing interventions, which directly modify the algorithm during the learning process (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.F8" title="Figure 8 ‣ A.2 Study Design ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></span></a>a). In-processing approaches tend to overshadow pre-processing interventions, which focus on data adjustments before training, and post-processing interventions, which address fairness concerns by modifying algorithmic outputs. Across the years we examine, we observe a growing ambiguity regarding the inherent trade-off between fairness and performance (<a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A1.F8" title="Figure 8 ‣ A.2 Study Design ‣ Appendix A Expanded Results ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis">Figure <span class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></span></a>b).</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="A1.F7.g1" src="x7.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="A1.F7.3.2" style="font-size:90%;">Prevalence of Study Types and Datasets Over Time. (a) The majority of fairness papers engage in retrospective analysis of existing datasets and algorithms. (b) Studies most frequently leverage synthetic datasets. The most studied individual dataset is COMPAS, with other non-synthetic datasets like the German Credit and UCI Adult datasets also seeing considerable use.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="A1.F8.g1" src="x8.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="A1.F8.3.2" style="font-size:90%;">Trends in Intervention Points and Acknowledgement of Performance Trade-offs Over Time. (a) In-processing consistently emerges as the most common point for implementing fairness interventions, with the exception of 2020. (b) Notably, we observe an upward trend in the number of papers that do not address whether improving fairness metrics comes at the cost of performance.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Fairness Definitions</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A2.F9" title="Figure 9 ‣ Appendix B Fairness Definitions ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">9</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A2.F10" title="Figure 10 ‣ Appendix B Fairness Definitions ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">10</span></a> illustrate the prevalence of different fairness metrics in our sample over time, presenting the top ten most frequently used metrics and the full list, respectively. A single study might explore multiple metrics. For instance, an individual paper might examine both demographic parity and equalized false positive rates across demographic groups.</p>
</div>
<figure class="ltx_figure" id="A2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="301" id="A2.F9.g1" src="x9.png" width="448"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="A2.F9.3.2" style="font-size:90%;">Popularity of Top Fairness Metrics Over Time. While a wide range of metrics exist for assessing fairness, a small subset sees disproportionate use within the literature. This barplot depicts the ten most common fairness metrics from our sample. Demographic parity demonstrates consistent popularity over the years.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="874" id="A2.F10.g1" src="x10.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="A2.F10.3.2" style="font-size:90%;">Prevalence of All Fairness Metrics Over Time. This barplot depicts the use of fairness metrics across papers in our sample.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Sampled Papers</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Tables <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A3.T2" title="Table 2 ‣ Appendix C Sampled Papers ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.16895v1#A3.T3" title="Table 3 ‣ Appendix C Sampled Papers ‣ (Unfair) Norms in Fairness Research: A Meta-Analysis"><span class="ltx_text ltx_ref_tag">3</span></a>
provide titles for the full sample of papers selected and reviewed for our meta-analysis.</p>
</div>
<figure class="ltx_table" id="A3.T2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_square" height="960" id="A3.T2.g1" src="x11.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="A3.T2.3.2" style="font-size:90%;">Sampled Papers, from 2018 to 2020.</span></figcaption>
</figure>
<figure class="ltx_table" id="A3.T3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_portrait" height="995" id="A3.T3.g1" src="x12.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A3.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="A3.T3.3.2" style="font-size:90%;">Sampled Papers, from 2021 to 2022.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 17 17:12:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
