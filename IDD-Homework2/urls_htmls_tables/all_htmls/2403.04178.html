<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation</title>
<!--Generated on Thu Mar  7 03:16:47 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content=" speech-to-speech machine translation,  stress detection,  text-to-speech,  speech synthesis" lang="en" name="keywords"/>
<base href="/html/2403.04178v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S1" title="I Introduction ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2" title="II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Proposed Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.SS1" title="II-A Stress Dataset ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Stress Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.SS2" title="II-B Stress Detection Models ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Stress Detection Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.SS3" title="II-C TTS ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">TTS</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3" title="III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3.SS1" title="III-A Stress Dataset ‚Ä£ III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Stress Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3.SS2" title="III-B Feature Extraction and Stress Detection Models ‚Ä£ III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Feature Extraction and Stress Detection Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3.SS3" title="III-C SSMT Pipeline ‚Ä£ III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">SSMT Pipeline</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S4" title="IV Results ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S5" title="V Conclusion ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#Sx1" title="Acknowledgment ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_title">Acknowledgment</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.04178v1 [cs.CL] 07 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_italic" id="id1.1.id1">Sai Akarsh, Vamshi Raghusimha, Anindita Mondal, Anil Vuppala
<br class="ltx_break"/></span>LTRC, International Institute of Information Technology - Hyderabad
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2" style="font-size:80%;">{sai.akarsh, narasinga.vamshi, anindita.mondal}@research.iiit.ac.in, anil.vuppala@iiit.ac.in</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">The language diversity in India‚Äôs education sector poses a significant challenge, hindering inclusivity. Despite the democratization of knowledge through online educational content, the dominance of English, as the internet‚Äôs lingua franca, limits accessibility, emphasizing the crucial need for translation into Indian languages. Despite existing Speech-to-Speech Machine Translation (SSMT) technologies, the lack of intonation in these systems gives monotonous translations, leading to a loss of audience interest and disengagement from the content. To address this, our paper introduces a dataset with stress annotations in Indian English and also a Text-to-Speech (TTS) architecture capable of incorporating stress into synthesized speech. This dataset is used for training a stress detection model, which is then used in the SSMT system for detecting stress in the source speech and transferring it into the target language speech. The TTS architecture is based on FastPitch and can modify the variances based on stressed words given. We present an Indian English-to-Hindi SSMT system that can transfer stress and aim to enhance the overall quality and engagement of educational content.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6> speech-to-speech machine translation, stress detection, text-to-speech, speech synthesis
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">SSMT stands at the forefront of linguistic innovation, aiming to bridge communication gaps by offering real-time translation of spoken language. In a globalized society, effective communication across diverse linguistic boundaries is indispensable. SSMT, as a cutting-edge technology, emerges as a pivotal solution and is composed of three primary components: Automatic Speech Recognition (ASR), Machine Translation (MT), and Text-to-Speech synthesis (TTS). These components involve language and prosodic understanding and hence, there are several challenges, stemming from the complex nature of spoken language and the intricacies involved in real-time translation.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Prosody encompasses elements such as stress, rhythm, and intonation, which play a crucial role in conveying the emotional and expressive aspects of spoken language. Ensuring the faithful transfer of prosody from the source to the target language is essential for producing natural and human-like translations. Prosody often varies based on the context of the conversation, such as asking a question, making a statement, or expressing surprise. The inability to transfer prosodic features accurately results in robotic or unnatural-sounding translations, affecting the overall intelligibility and expressiveness of the translated speech.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Traditional speech translation systems overlook paralinguistic cues, but there has been a recent focus on systems that transfer linguistic content along with emphasis information. In contrast to conventional SSMT systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx8" title="">8</a>]</cite>, proposed methods to translate paralinguistic information, focusing specifically on stress/emphasis where source speech was English and target speech was Japanese. Utilizing DNN-HMM ASR, Statistical Machine Translation, and an HSMM-based TTS model, the approach in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx7" title="">7</a>]</cite> involves the use of linear-regression hidden semi-Markov models (LRHSMMs). These models estimate a sequence of real-numbered emphasis values for each word in an utterance.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Numerous studies have been conducted within the linguistic domains of Spanish, English, and Catalan such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx1" title="">1</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx12" title="">12</a>]</cite> reflecting a comprehensive exploration of prosody transfer techniques between these languages. Moreover, recent research initiatives have broadened their focus to incorporate the intricate linguistic environment prevalent in the Indian context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx16" title="">16</a>]</cite>. While existing speech datasets offer valuable resources for speech processing and synthesis research, they often lack annotations for aspects like prosodic emphasis, particularly in domains such as educational settings. To address this gap, this paper undertook an in-depth exploration of an Indian English dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx11" title="">11</a>]</cite>, meticulously annotated the gathered data, devised a stress detection model, and subsequently formulated a methodology to integrate this model into TTS systems which can generate target language speech in Hindi enriched with stress patterns for enhanced naturalness and expressiveness.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Proposed Methodology</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The fidelity of SSMT hinges not just on semantic accuracy, but also on capturing the speaker‚Äôs intent conveyed through subtle prosodic cues like stress. While existing systems excel at translating meaning, they often stumble when replicating the emotional weight and emphasis embedded in the source speech. In this section, we discuss in detail the methodology for bridging this gap.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Stress Dataset</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recognizing the limitations of existing speech datasets in capturing the subtleties of stress, especially within the distinct domain of Indian English video lectures, we sought to build a resource tailored to this specific niche. We leveraged the existing strengths of the IED-IIITH dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx11" title="">11</a>]</cite>, which features high-quality speech recordings from the NPTEL educational lectures. This dataset was chosen as it contains a wide variety of speakers and speaking styles, and also captures common disfluencies that occur in spoken speech. This is important as our stress detection model which is set in the context of spoken speech in educational lectures needs to be robust against disfluencies and shouldn‚Äôt necessarily classify them as stress regions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In the context of lectures, the lecturer naturally incorporates stress while explaining specific topics, emphasizing certain words to add meaning and subtlety. Such stress patterns often occur at the beginning or ending of sentences, and feature prolonged duration, with potential pauses. This dataset was manually annotated by trained annotators who identified the stressed regions, relying on both acoustic cues like pitch and duration and indicators like waveform and the above-explained reasoning.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S2.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Sample from the annotated dataset that contains a stress region (orange)</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Label Studio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx20" title="">20</a>]</cite>, an open-source data labelling platform, was used to perform stress region annotation and generate the timestamps for each audio file as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.F1" title="Figure 1 ‚Ä£ II-A Stress Dataset ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> (with the transcript for reference). An audio file may have multiple stressed regions or none at all. The audio data was distributed among ten annotators in such a way that every audio sample had labels from at least 3 different annotators. This was done to address the subjective nature of stress and emphasis as everyone has a different perception of such nuances. We use Fleiss Kappa inter-annotator agreement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx9" title="">9</a>]</cite> to aggregate multiple annotations to get the final labels. This effort resulted in a unique stress-annotated dataset specifically tailored to Indian English (NPTEL) video lectures, advancing the availability of resources for stress detection.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Stress Detection Models</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Stress detection in speech is achieved by training classifiers on several acoustic features calculated from the data collected. The classifier is trained to predict a stress label for each frame indicating if a frame is stressed or not. The model uses features from a window of frames rather than one frame for more contextual information. In speech, the stressed regions occupy a small region when compared to the non-stressed regions, hence the labels given to the classifier are usually skewed giving rise to more false positives and need to be balanced.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">A post-processing technique is employed, where the word alignments with speech are calculated using an ASR system. With these alignments, the frame-level predictions are converted to word-level predictions. A word is classified as stressed if the region containing the word has a majority of frames predicted as stress. This technique not only filters out a considerable amount of false positives but also gives stress detection on the word level which is used in SSMT.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">For every detected stress word, additional information, indicating how the pitch and energy variances change in that stressed region with respect to the rest of the speech, is denoted as a scaling factor and is quite useful in SSMT as a metric for the TTS system to modify variances in order to introduce stress in the target speech.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">TTS</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Once the stressed words are known, a way of incorporating stress in synthesized speech is presented based on FastPitch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx15" title="">15</a>]</cite> which is a two-stage TTS architecture. The first stage is comprised of a Transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx21" title="">21</a>]</cite> text encoder, and a CNN-based pitch and duration variance predictor. The second stage combines the text encoding with pitch embeddings and duplicates it based on the predicted duration, which is then sent to another Transformer to finally give the mel spectrogram.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In the proposed TTS system, several changes were made to the base FastPitch architecture to condition the synthesized speech on stress cues. First, both text and stress cues are taken as input, where the text is handled as character tokens and stress cues are given as stressed word and scale factor pairs that were generated using the stress detection model. Second, an energy prediction block is added since stressed words have a strong correlation with energy and can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.F2" title="Figure 2 ‚Ä£ II-C TTS ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> (a). Third, a Pitch-Duration-Energy (PDE) Modifier block as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S2.F2" title="Figure 2 ‚Ä£ II-C TTS ‚Ä£ II Proposed Methodology ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> (b) is introduced, which modifies the predicted variances using the stress cues. Such modifications enable the training of the TTS on neutral speech-text datasets which are already available. Since TTS requires very high-quality clean speech data, procuring such amounts of stressed data is not trivial and we overcome this issue by using the PDE Modifier block only during inference.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S2.F1.sf1.g1" src="x2.png" width="505"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F1.sf1.3.2" style="font-size:90%;">Proposed TTS architecture</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="189" id="S2.F1.sf2.g1" src="x3.png" width="208"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F1.sf2.3.2" style="font-size:90%;">PDE Modifier</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F2.4.2" style="font-size:90%;">Architecture for proposed TTS<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.1"> follows FastPitch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx15" title="">15</a>]</cite>. The predicted variances are modified by the PDE Modifier based on the stress cues to introduce stress in the synthesized speech.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.6">Let <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p3.1.m1.1"><semantics id="S2.SS3.p3.1.m1.1a"><mi id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><ci id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.1.m1.1d">italic_x</annotation></semantics></math> be input character tokens and <math alttext="s" class="ltx_Math" display="inline" id="S2.SS3.p3.2.m2.1"><semantics id="S2.SS3.p3.2.m2.1a"><mi id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><ci id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.2.m2.1d">italic_s</annotation></semantics></math> be input stress cues. The token embeddings for the text are generated using an embedding layer and are passed to a feed-forward Transformer (FFT) encoder stack that produces the hidden representation <math alttext="h=\text{FFT}_{e}\text{(Embedding}(x))" class="ltx_math_unparsed" display="inline" id="S2.SS3.p3.3.m3.1"><semantics id="S2.SS3.p3.3.m3.1a"><mrow id="S2.SS3.p3.3.m3.1b"><mi id="S2.SS3.p3.3.m3.1.2">h</mi><mo id="S2.SS3.p3.3.m3.1.3">=</mo><msub id="S2.SS3.p3.3.m3.1.4"><mtext id="S2.SS3.p3.3.m3.1.4.2">FFT</mtext><mi id="S2.SS3.p3.3.m3.1.4.3">e</mi></msub><mtext id="S2.SS3.p3.3.m3.1.5">(Embedding</mtext><mrow id="S2.SS3.p3.3.m3.1.6"><mo id="S2.SS3.p3.3.m3.1.6.1" stretchy="false">(</mo><mi id="S2.SS3.p3.3.m3.1.1">x</mi><mo id="S2.SS3.p3.3.m3.1.6.2" stretchy="false">)</mo></mrow><mo id="S2.SS3.p3.3.m3.1.7" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">h=\text{FFT}_{e}\text{(Embedding}(x))</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.3.m3.1d">italic_h = FFT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT (Embedding ( italic_x ) )</annotation></semantics></math>. This hidden representation is then used to predict each input character token‚Äôs duration <math alttext="\hat{d}" class="ltx_Math" display="inline" id="S2.SS3.p3.4.m4.1"><semantics id="S2.SS3.p3.4.m4.1a"><mover accent="true" id="S2.SS3.p3.4.m4.1.1" xref="S2.SS3.p3.4.m4.1.1.cmml"><mi id="S2.SS3.p3.4.m4.1.1.2" xref="S2.SS3.p3.4.m4.1.1.2.cmml">d</mi><mo id="S2.SS3.p3.4.m4.1.1.1" xref="S2.SS3.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.m4.1b"><apply id="S2.SS3.p3.4.m4.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1"><ci id="S2.SS3.p3.4.m4.1.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1.1">^</ci><ci id="S2.SS3.p3.4.m4.1.1.2.cmml" xref="S2.SS3.p3.4.m4.1.1.2">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.m4.1c">\hat{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.4.m4.1d">over^ start_ARG italic_d end_ARG</annotation></semantics></math>, pitch <math alttext="\hat{p}" class="ltx_Math" display="inline" id="S2.SS3.p3.5.m5.1"><semantics id="S2.SS3.p3.5.m5.1a"><mover accent="true" id="S2.SS3.p3.5.m5.1.1" xref="S2.SS3.p3.5.m5.1.1.cmml"><mi id="S2.SS3.p3.5.m5.1.1.2" xref="S2.SS3.p3.5.m5.1.1.2.cmml">p</mi><mo id="S2.SS3.p3.5.m5.1.1.1" xref="S2.SS3.p3.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.5.m5.1b"><apply id="S2.SS3.p3.5.m5.1.1.cmml" xref="S2.SS3.p3.5.m5.1.1"><ci id="S2.SS3.p3.5.m5.1.1.1.cmml" xref="S2.SS3.p3.5.m5.1.1.1">^</ci><ci id="S2.SS3.p3.5.m5.1.1.2.cmml" xref="S2.SS3.p3.5.m5.1.1.2">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.5.m5.1c">\hat{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.5.m5.1d">over^ start_ARG italic_p end_ARG</annotation></semantics></math> and energy <math alttext="\hat{e}" class="ltx_Math" display="inline" id="S2.SS3.p3.6.m6.1"><semantics id="S2.SS3.p3.6.m6.1a"><mover accent="true" id="S2.SS3.p3.6.m6.1.1" xref="S2.SS3.p3.6.m6.1.1.cmml"><mi id="S2.SS3.p3.6.m6.1.1.2" xref="S2.SS3.p3.6.m6.1.1.2.cmml">e</mi><mo id="S2.SS3.p3.6.m6.1.1.1" xref="S2.SS3.p3.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.6.m6.1b"><apply id="S2.SS3.p3.6.m6.1.1.cmml" xref="S2.SS3.p3.6.m6.1.1"><ci id="S2.SS3.p3.6.m6.1.1.1.cmml" xref="S2.SS3.p3.6.m6.1.1.1">^</ci><ci id="S2.SS3.p3.6.m6.1.1.2.cmml" xref="S2.SS3.p3.6.m6.1.1.2">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.6.m6.1c">\hat{e}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.6.m6.1d">over^ start_ARG italic_e end_ARG</annotation></semantics></math> using a CNN stack. The pitch, duration and energy values are then sent to the PDE Modifier block, where each of the variances is modified based on the stressed word region and the scaling factors provided. This is done by scaling the predicted variances of every input token corresponding to a stressed word using the scaling factors.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.8">Next, the modified pitch <math alttext="p_{m}" class="ltx_Math" display="inline" id="S2.SS3.p4.1.m1.1"><semantics id="S2.SS3.p4.1.m1.1a"><msub id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml"><mi id="S2.SS3.p4.1.m1.1.1.2" xref="S2.SS3.p4.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS3.p4.1.m1.1.1.3" xref="S2.SS3.p4.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><apply id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.1.m1.1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p4.1.m1.1.1.2.cmml" xref="S2.SS3.p4.1.m1.1.1.2">ùëù</ci><ci id="S2.SS3.p4.1.m1.1.1.3.cmml" xref="S2.SS3.p4.1.m1.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">p_{m}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> and energy <math alttext="e_{m}" class="ltx_Math" display="inline" id="S2.SS3.p4.2.m2.1"><semantics id="S2.SS3.p4.2.m2.1a"><msub id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml"><mi id="S2.SS3.p4.2.m2.1.1.2" xref="S2.SS3.p4.2.m2.1.1.2.cmml">e</mi><mi id="S2.SS3.p4.2.m2.1.1.3" xref="S2.SS3.p4.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><apply id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.2.m2.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p4.2.m2.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.2">ùëí</ci><ci id="S2.SS3.p4.2.m2.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">e_{m}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.2.m2.1d">italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> are projected to match the dimensionality of the hidden representation <math alttext="h" class="ltx_Math" display="inline" id="S2.SS3.p4.3.m3.1"><semantics id="S2.SS3.p4.3.m3.1a"><mi id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><ci id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.3.m3.1d">italic_h</annotation></semantics></math> using an embedding layer and are then added to <math alttext="h" class="ltx_Math" display="inline" id="S2.SS3.p4.4.m4.1"><semantics id="S2.SS3.p4.4.m4.1a"><mi id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.1b"><ci id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.4.m4.1d">italic_h</annotation></semantics></math>. The resulting sum <math alttext="g" class="ltx_Math" display="inline" id="S2.SS3.p4.5.m5.1"><semantics id="S2.SS3.p4.5.m5.1a"><mi id="S2.SS3.p4.5.m5.1.1" xref="S2.SS3.p4.5.m5.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.5.m5.1b"><ci id="S2.SS3.p4.5.m5.1.1.cmml" xref="S2.SS3.p4.5.m5.1.1">ùëî</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.5.m5.1c">g</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.5.m5.1d">italic_g</annotation></semantics></math> which is at the resolution of input character tokens is then discretely upsampled using the modified durations <math alttext="d_{m}" class="ltx_Math" display="inline" id="S2.SS3.p4.6.m6.1"><semantics id="S2.SS3.p4.6.m6.1a"><msub id="S2.SS3.p4.6.m6.1.1" xref="S2.SS3.p4.6.m6.1.1.cmml"><mi id="S2.SS3.p4.6.m6.1.1.2" xref="S2.SS3.p4.6.m6.1.1.2.cmml">d</mi><mi id="S2.SS3.p4.6.m6.1.1.3" xref="S2.SS3.p4.6.m6.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.6.m6.1b"><apply id="S2.SS3.p4.6.m6.1.1.cmml" xref="S2.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.6.m6.1.1.1.cmml" xref="S2.SS3.p4.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.p4.6.m6.1.1.2.cmml" xref="S2.SS3.p4.6.m6.1.1.2">ùëë</ci><ci id="S2.SS3.p4.6.m6.1.1.3.cmml" xref="S2.SS3.p4.6.m6.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.6.m6.1c">d_{m}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.6.m6.1d">italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> to give <math alttext="f" class="ltx_Math" display="inline" id="S2.SS3.p4.7.m7.1"><semantics id="S2.SS3.p4.7.m7.1a"><mi id="S2.SS3.p4.7.m7.1.1" xref="S2.SS3.p4.7.m7.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.7.m7.1b"><ci id="S2.SS3.p4.7.m7.1.1.cmml" xref="S2.SS3.p4.7.m7.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.7.m7.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.7.m7.1d">italic_f</annotation></semantics></math> which is at the resolution of output frames. <math alttext="f" class="ltx_Math" display="inline" id="S2.SS3.p4.8.m8.1"><semantics id="S2.SS3.p4.8.m8.1a"><mi id="S2.SS3.p4.8.m8.1.1" xref="S2.SS3.p4.8.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.8.m8.1b"><ci id="S2.SS3.p4.8.m8.1.1.cmml" xref="S2.SS3.p4.8.m8.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.8.m8.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.8.m8.1d">italic_f</annotation></semantics></math> is then passed to an FFT decoder stack to produce the output mel-spectrogram sequence.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">The output mel-spectrogram <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S2.SS3.p5.1.m1.1"><semantics id="S2.SS3.p5.1.m1.1a"><mover accent="true" id="S2.SS3.p5.1.m1.1.1" xref="S2.SS3.p5.1.m1.1.1.cmml"><mi id="S2.SS3.p5.1.m1.1.1.2" xref="S2.SS3.p5.1.m1.1.1.2.cmml">y</mi><mo id="S2.SS3.p5.1.m1.1.1.1" xref="S2.SS3.p5.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.1.m1.1b"><apply id="S2.SS3.p5.1.m1.1.1.cmml" xref="S2.SS3.p5.1.m1.1.1"><ci id="S2.SS3.p5.1.m1.1.1.1.cmml" xref="S2.SS3.p5.1.m1.1.1.1">^</ci><ci id="S2.SS3.p5.1.m1.1.1.2.cmml" xref="S2.SS3.p5.1.m1.1.1.2">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.1.m1.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p5.1.m1.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> is generated using the modified variances from the PDE Modifier block, thus incorporating stress in the synthesized speech. Since the PDE Modifier block is used only during inference, the training loss is based on mean-square error (MSE), similar to that of FastPitch.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S2.F3.g1" src="x4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F3.4.2" style="font-size:90%;">The overall architecture for the proposed SSMT pipeline<span class="ltx_text ltx_font_medium" id="S2.F3.4.2.1">. The MT Aligner takes English stress words, generates the English-Hindi word alignments and converts them into stress cues: Hindi stress words along with scaling factors</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experimental Setup</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we discuss the datasets, features and models that were used for the experiments.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Stress Dataset</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The stress dataset consists of nearly 10 hours of spoken speech in the context of educational video lectures from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx11" title="">11</a>]</cite>. It features recordings from 60 speakers (30 male and 30 female), where each speaker has around 10 minutes of speech. These 10-minute recordings are further split into smaller audio files of 8 to 12 seconds, with a sampling rate of 16000 Hz. The dataset comprises 3329 audio files where each audio file is identified using a unique ID and has information about the speaker and the stressed region labels. Each audio sample can have multiple stressed regions or none at all. There are a total of 3807 occurrences of stressed regions with an average duration of about 0.53 seconds.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Features</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.2.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.2.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S3.T1.2.1.1.3">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">Window Size = 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S3.T1.2.1.1.4">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.4.1">Window Size = 5</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3" id="S3.T1.2.1.1.5">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.5.1">Window Size = 7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.1.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.2.1">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.3.1">Post Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.4.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.5.1">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.6.1">Post Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.7.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.8.1">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.9.1">Post Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.3.3.1">F0 and Energy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.2">LPA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.3">60.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.4">60.45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.5">71.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.6.1">68.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.7.1">68.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.8.1">81.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.9.1">75.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.10"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.10.1">75.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.3.3.11"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.3.11.1">86.18</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.4">
<td class="ltx_td ltx_border_l ltx_border_r" id="S3.T1.2.4.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.2">RFC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.3">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.4">58.06</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.5">69.52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.6">68.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.7">67.59</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.8">76.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.9">72.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.10">71.79</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.4.4.11">82</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.5">
<td class="ltx_td ltx_border_l ltx_border_r" id="S3.T1.2.5.5.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.2">SVC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.5.5.3.1">61.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.5.5.4.1">60.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.5.5.5.1">73.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.6">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.7">66.11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.8">80.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.9">71.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.10">70.46</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.5.5.11">83.83</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.6.6.1">F0, Energy, MFCC, SDC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.2">LPA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.3">63.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.4">61.09</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.5">75.28</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.6">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.7">58.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.8">74.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.9"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.9.1">82.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.10"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.10.1">80.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.6.6.11"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.6.11.1">90.36</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S3.T1.2.7.7.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.2">RFC</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.3">64.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.4">63.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.5">77.37</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.7.7.6.1">71.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.7"><span class="ltx_text ltx_font_bold" id="S3.T1.2.7.7.7.1">70.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.8"><span class="ltx_text ltx_font_bold" id="S3.T1.2.7.7.8.1">81.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.9">78.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.10">78.27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.7.7.11">85.29</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8.8">
<td class="ltx_td ltx_border_b ltx_border_l ltx_border_r" id="S3.T1.2.8.8.1"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.2">SVC</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.8.8.3.1">66.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.8.8.4.1">66.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.8.8.5.1">78.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.6">70.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.7">70.09</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.8">80.12</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.9">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.10">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.8.8.11">83.82</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Evaluation of stress detection model with different features and varying window sizes.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Feature Extraction and Stress Detection Models</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Several acoustic features such as F0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx10" title="">10</a>]</cite>, energy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx14" title="">14</a>]</cite>, Mel Frequency Cepstral Coefficients (MFCC) and Shifted Delta Coefficients (SDC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx11" title="">11</a>]</cite> were used for the task of stress detection. 13 MFCC features were extracted by taking a frame length of 1024 and hop length of 256. 52 SDC features were calculated using <math alttext="d=1,p=5,k=3" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.2"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.2.2" xref="S3.SS2.p1.1.m1.2.2.3.cmml"><mrow id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.1.1.2.cmml">d</mi><mo id="S3.SS2.p1.1.m1.1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p1.1.m1.2.2.2.3" xref="S3.SS2.p1.1.m1.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p1.1.m1.2.2.2.2.2" xref="S3.SS2.p1.1.m1.2.2.2.2.3.cmml"><mrow id="S3.SS2.p1.1.m1.2.2.2.2.1.1" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.cmml"><mi id="S3.SS2.p1.1.m1.2.2.2.2.1.1.2" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.2.cmml">p</mi><mo id="S3.SS2.p1.1.m1.2.2.2.2.1.1.1" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.2.2.2.2.1.1.3" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.3.cmml">5</mn></mrow><mo id="S3.SS2.p1.1.m1.2.2.2.2.2.3" xref="S3.SS2.p1.1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p1.1.m1.2.2.2.2.2.2" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.2.cmml">k</mi><mo id="S3.SS2.p1.1.m1.2.2.2.2.2.2.1" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.3.cmml">3</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><apply id="S3.SS2.p1.1.m1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.3a.cmml" xref="S3.SS2.p1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.1"></eq><ci id="S3.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1.2">ùëë</ci><cn id="S3.SS2.p1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.2.2.3a.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p1.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1"><eq id="S3.SS2.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.1"></eq><ci id="S3.SS2.p1.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.2">ùëù</ci><cn id="S3.SS2.p1.1.m1.2.2.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.2.2.2.2.1.1.3">5</cn></apply><apply id="S3.SS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2"><eq id="S3.SS2.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.1"></eq><ci id="S3.SS2.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.2">ùëò</ci><cn id="S3.SS2.p1.1.m1.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.2.2.2.2.2.2.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">d=1,p=5,k=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.2d">italic_d = 1 , italic_p = 5 , italic_k = 3</annotation></semantics></math> as they gave better results. The F0 and energy contours were averaged to get frame-level features. All the features were then mean-variance normalized, and different subsets of features were considered to get the best performance. We combine different features on the frame level and then stack up those features from frames before and after to give wider contextual information.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The data imbalance needs to be tackled as the frames corresponding to stressed regions are much fewer than that of non-stressed regions. This would lead the model to be biased and hence not desirable. We used SMOTE (Synthetic Minority Oversampling Technique) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx4" title="">4</a>]</cite> based oversampling on the data while training each model.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Using the features extracted above, various classifiers were trained to perform the task of stress detection. The following three classifiers were chosen to be tabulated as they gave better results: Label Propagation Algorithm (LPA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx22" title="">22</a>]</cite> with radial basis function kernel and 7 neighbours, Random Forest Classifier (RFC) with 100 trees in the ensemble and Support Vector Classifier (SVC) with a radial basis function kernel and penalty factor of 0.8. Due to the limited availability of data, a DNN-based model was not deemed suitable to achieve the desired level of accuracy.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">SSMT Pipeline</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">ASR:</span> WhisperX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx3" title="">3</a>]</cite> ASR model was used to process the source speech in Indian English to give both the transcript and word alignments. The ASR is built upon a pre-trained Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx17" title="">17</a>]</cite> English large-v2 model and delivers accurate word-level transcriptions with high confidence scores. Word-level alignments in post-processing boost stress detection model accuracy and reliability. They also aid in identifying stressed words in the target language by mapping them to their equivalents in the source language.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">MT and Aligner:</span> English-Hindi MT model was taken from the open source Helsinki OPUS MT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx19" title="">19</a>]</cite> project. This is used to convert the English transcript from ASR into a Hindi transcript. MT word aligner from simalign <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx13" title="">13</a>]</cite> was used to map the translated Hindi words to their corresponding English words. It uses a pre-trained mBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx5" title="">5</a>]</cite> multilingual language model to achieve this mapping.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">TTS:</span> To train the TTS model, we have used the latest version of the Hindi Male speaker‚Äôs database from the ULCA Bhashini IndicTTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx2" title="">2</a>]</cite> which has nearly 10 hours of recorded and transcribed speech at a sampling rate of 22050 Hz. The utterances of more than 15 seconds were ignored. The TTS model used an FFT stack of 6 layers and a pre-trained Tacotron2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx18" title="">18</a>]</cite> model for calculating the durations of ground truth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#bib.bibx15" title="">15</a>]</cite>. It was trained for 2000 epochs on a single NVIDIA A100 80GB Tensor Core GPU with a batch size of 128.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The performance of the stress detection models is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3.T1" title="TABLE I ‚Ä£ III-A Stress Dataset ‚Ä£ III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a> based on the classification of frame-level stress labels. The experiments were carried out on two feature sets: (F0, Energy) and (F0, Energy, MFCC, SDC) using different classifiers. From Table <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S3.T1" title="TABLE I ‚Ä£ III-A Stress Dataset ‚Ä£ III Experimental Setup ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">I</span></a>, it can be seen that LPA having a window size of 7 with the addition of the post-processing stage improves the accuracy and f1 score for both feature sets with an absolute improvement of around 2-4%. RFC gave better results compared to LPA and SVC in the case of all features when the window size is 5 with an absolute improvement of around 5%. SVC gave better results compared to LPA and RFC in both cases when the window size is 3 with an absolute improvement of around 2-3%.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.1.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.2.1">MOS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.1.1.1">TTS Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.1.2.1">Without Stress</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.1.3.1">With Stress</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.3.2.1">Pitch</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.3.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.3.2.2.1">4.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.3.2.3">3.95</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.4.3.1">Pitch and Energy</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.4.3.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.4.3.2.1">4.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.4.3.3">4.21</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Evaluation of TTS models in SSMT using Mean Opinion Score (MOS)</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Due to the lack of automated evaluation metrics for SSMT, we rely on subjective evaluation through a survey where 30 participants, fluent in both English and Hindi were asked to give performance metrics. Every participant was given 15 samples<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://sativus04.github.io/</span></span></span>, each containing the original English speech, translated Hindi speech with and without stress. They gave two performance metrics, the first is the quality of the synthesized Hindi speech for both with and without stress as seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S4.T2" title="TABLE II ‚Ä£ IV Results ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">II</span></a>. A score between 0 and 5 is given, where 0 is very bad and 5 is very good. The Mean Opinion Score (MOS) is derived by averaging all the scores. Notably, TTS without stress outperforms its stressed counterpart due to potential instability in spectrogram generation when directly altering variances. The model with both Pitch and Energy exhibits enhanced flexibility in speech generation.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.1">SSMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.2.1">st-MOS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.2.1.1">Pitch</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.1.2">3.73</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.3.2.1">Pitch and Energy</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.3.2.2.1">4.09</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Evaluation of SSMT models using st-MOS (Stress Transfer MOS)</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The second performance metric is the measure of how well the stress has been incorporated as seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.04178v1#S4.T3" title="TABLE III ‚Ä£ IV Results ‚Ä£ Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation"><span class="ltx_text ltx_ref_tag">III</span></a>. The participant uses the original speech and synthesized speech without stress as a reference to gauge how well the stress has been added to the synthesized speech with stress. This is used to calculate the st-MOS (stress transfer MOS) score, where 0 means that no stress was transferred and 5 means that all the stress regions were transferred. The average score of 3.96 indicates effective stress integration, particularly with both pitch and energy, underlining the significance of energy in stress modelling. These results serve as the baseline for this task, given the absence of a baseline setup.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">SSMT systems keep improving with rapid strides. We curated a dataset with stress annotations for Indian English and trained stress detection models using it. We modified an existing TTS architecture for the addition of stress in synthesized speech using existing speech corpora. Evaluation was done on the stress detection models, TTS and SSMT systems and the best configurations were identified. Several directions of work can be seen from here: other TTS models need to be compared in this task, preserving the speaker‚Äôs voice in target speech, and designing better subjective and objective metrics for such tasks in SSMT. We aim to address some of these issues in future works and bridge the gap between the source and target speech in SSMT.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the reviewers for their insightful comments. This undertaking is funded by the Ministry of
Electronics and Information Technology, Government of India
, as evidenced by the Sanction Order:
11(1)/2022-HCC(TDIL)-Part(2)/A/B/C and the Administrative Approval: 11(1)/2022-HCC(TDIL)-
Part(2).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">P.D. Aguero, J. Adell and A. Bonafonte
</span>
<span class="ltx_bibblock">‚ÄúProsody Generation for Speech-to-Speech Translation‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx1.1.1">2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</em> <span class="ltx_text ltx_font_bold" id="bib.bibx1.2.2">1</span>, 2006, pp. I‚ÄìI
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/ICASSP.2006.1660081" title="">10.1109/ICASSP.2006.1660081</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Arun Baby, Anju Thomas, Nishanthi L and TTS Consortium
</span>
<span class="ltx_bibblock">‚ÄúResources for Indian languages‚Äù, 2016
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Max Bain, Jaesung Huh, Tengda Han and Andrew Zisserman
</span>
<span class="ltx_bibblock">‚ÄúWhisperX: Time-Accurate Speech Transcription of Long-Form Audio‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx3.1.1">INTERSPEECH 2023</em>, 2023
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">N.V. Chawla, K.W. Bowyer, L.O. Hall and W.P. Kegelmeyer
</span>
<span class="ltx_bibblock">‚ÄúSMOTE: Synthetic Minority Over-sampling Technique‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx4.1.1">Journal of Artificial Intelligence Research</em> <span class="ltx_text ltx_font_bold" id="bib.bibx4.2.2">16</span>
</span>
<span class="ltx_bibblock">AI Access Foundation, 2002, pp. 321‚Äì357
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1613/jair.953" title="">10.1613/jair.953</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova
</span>
<span class="ltx_bibblock">‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù, 2019
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1810.04805" title="">1810.04805 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Quoc Truong Do, Sakriani Sakti and Satoshi Nakamura
</span>
<span class="ltx_bibblock">‚ÄúSequence-to-Sequence Models for Emphasis Speech Translation‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx6.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> <span class="ltx_text ltx_font_bold" id="bib.bibx6.2.2">26.10</span>, 2018, pp. 1873‚Äì1883
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/TASLP.2018.2846402" title="">10.1109/TASLP.2018.2846402</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Quoc Truong Do et al.
</span>
<span class="ltx_bibblock">‚ÄúPreserving Word-Level Emphasis in Speech-to-Speech Translation‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx7.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> <span class="ltx_text ltx_font_bold" id="bib.bibx7.2.2">25.3</span>, 2017, pp. 544‚Äì556
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/TASLP.2016.2643280" title="">10.1109/TASLP.2016.2643280</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Quoc Truong Do, Sakriani Sakti, Graham Neubig and Satoshi Nakamura
</span>
<span class="ltx_bibblock">‚ÄúTransferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx8.1.1">Proc. Interspeech 2016</em>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">JL Fleiss
</span>
<span class="ltx_bibblock">‚ÄúMeasuring nominal scale agreement among many raters‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx9.1.1">Psychological bulletin</em>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Hiroya Fujisaki
</span>
<span class="ltx_bibblock">‚ÄúInformation, prosody, and modeling-with emphasis on tonal features of speech‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx10.1.1">Scientific Programming - SP</em>, 2004
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Sparsh Garg, Utkarsh Mehrotra, Gurugubelli Krishna and Anil Kumar Vuppala
</span>
<span class="ltx_bibblock">‚ÄúTowards a Database For Detection of Multiple Speech Disfluencies in Indian English‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx11.1.1">2021 National Conference on Communications (NCC)</em>, 2021, pp. 1‚Äì6
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/NCC52529.2021.9530043" title="">10.1109/NCC52529.2021.9530043</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Wen-Chin Huang et al.
</span>
<span class="ltx_bibblock">‚ÄúA Holistic Cascade System, benchmark, and Human Evaluation Protocol for Expressive Speech-to-Speech Translation‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2301.10606" title="">2301.10606 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Masoud Jalili Sabet, Philipp Dufter, Francois Yvon and Hinrich Schutze
</span>
<span class="ltx_bibblock">‚ÄúSimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx13.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>
</span>
<span class="ltx_bibblock">Association for Computational Linguistics
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Takatomo Kano and Sakriani Sakti
</span>
<span class="ltx_bibblock">‚ÄúA method for translation of paralinguistic information‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx14.1.1">Proceedings of the 9th International Workshop on Spoken Language Translation: Papers</em>, 2012, pp. 158‚Äì163
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Adrian ≈Åa≈Ñcucki
</span>
<span class="ltx_bibblock">‚ÄúFastpitch: Parallel Text-to-Speech with Pitch Prediction‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx15.1.1">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021, pp. 6588‚Äì6592
</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_href" href="https://dx.doi.org/10.1109/ICASSP39728.2021.9413889" title="">10.1109/ICASSP39728.2021.9413889</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Shivam Mhaskar et al.
</span>
<span class="ltx_bibblock">‚ÄúVAKTA-SETU: A Speech-to-Speech Machine Translation Service in Select Indic Languages‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.12518" title="">2305.12518 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Alec Radford et al.
</span>
<span class="ltx_bibblock">‚ÄúRobust Speech Recognition via Large-Scale Weak Supervision‚Äù, 2022
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.04356" title="">2212.04356 [eess.AS]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Jonathan Shen et al.
</span>
<span class="ltx_bibblock">‚ÄúNatural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions‚Äù, 2018
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1712.05884" title="">1712.05884 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">J√∂rg Tiedemann and Santhosh Thottingal
</span>
<span class="ltx_bibblock">‚ÄúOPUS-MT ‚Äî Building open translation services for the World‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx19.1.1">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</em>, 2020
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk and Nikolai Liubimov
</span>
<span class="ltx_bibblock">‚ÄúLabel Studio: Data labeling software‚Äù Open source software available from https://github.com/heartexlabs/label-studio, 2022
</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/heartexlabs/label-studio" title="">https://github.com/heartexlabs/label-studio</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Ashish Vaswani et al.
</span>
<span class="ltx_bibblock">‚ÄúAttention Is All You Need‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1706.03762" title="">1706.03762 [cs.CL]</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Dengyong Zhou et al.
</span>
<span class="ltx_bibblock">‚ÄúLearning with Local and Global Consistency‚Äù
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bibx22.1.1">Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bibx22.2.2">16</span>
</span>
<span class="ltx_bibblock">MIT Press, 2003
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Mar  7 03:16:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
