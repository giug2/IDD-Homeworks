<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems</title>
<!--Generated on Wed Jul  3 08:27:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.02914v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S1" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2.SS1" title="In 2 Background and Related Work ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Energy Efficiency of ML-Enabled Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2.SS2" title="In 2 Background and Related Work ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Ensemble Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2.SS3" title="In 2 Background and Related Work ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Related Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS1" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Objective and Research Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS2" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>ML Algorithms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS3" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS4" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Fusion Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS5" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Dataset Partitioning Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS6" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Experiment Variables</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS7" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Experiment Execution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S3.SS8" title="In 3 Experiment Design ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.8 </span>Data Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.SS1" title="In 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Impact of Ensemble Size (RQ1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.SS2" title="In 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Impact of Majority Voting vs. Meta-Model Fusion (RQ2)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.SS3" title="In 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Impact of Whole-Dataset vs. Subset-Based Training (RQ3)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S5" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S6" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Threats to Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S7" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S8" title="In The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgement</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rafiullah Omar
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Justus Bogner
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henry Muccini
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patricia Lago
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Silverio Martínez-Fernández
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xavier Franch
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1"><span class="ltx_text ltx_font_bold" id="id7.id1.1">Background:</span>
Machine learning (ML) model composition is a popular technique to mitigate shortcomings of a single ML model and to design more effective ML-enabled systems.
Even though ensemble learning, i.e., forwarding the same request to several models and fusing their predictions, has been studied extensively regarding accuracy, we have insufficient knowledge about how to design energy-efficient ensembles.</p>
<p class="ltx_p" id="id8.id2"><span class="ltx_text ltx_font_bold" id="id8.id2.1">Objective:</span>
We therefore wanted to analyze three different types of design decisions for ensemble learning regarding a potential trade-off between accuracy and energy consumption:
a) ensemble size, i.e., the number of models in the ensemble, b) fusion methods (majority voting vs. a meta-model), and c) partitioning methods (whole-dataset vs. subset-based training).</p>
<p class="ltx_p" id="id3.3"><span class="ltx_text ltx_font_bold" id="id3.3.1">Methods:</span>
By combining four popular ML algorithms for classification in different ensembles, we conducted a full factorial experiment with 11 ensembles <math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> 4 datasets <math alttext="\times" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">×</annotation></semantics></math> 2 fusion methods <math alttext="\times" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mo id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><times id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">×</annotation></semantics></math> 2 partitioning methods (176 combinations).
For each combination, we measured accuracy (F1-score) and energy consumption in J (for both training and inference).</p>
<p class="ltx_p" id="id6.6"><span class="ltx_text ltx_font_bold" id="id6.6.1">Results:</span>
While a larger ensemble size significantly increased energy consumption (size 2 ensembles consumed 37.49% less energy than size 3 ensembles, which in turn consumed 26.96%
less energy than the size 4 ensembles), it did not significantly increase accuracy.
Furthermore, majority voting outperformed meta-model fusion both in terms of accuracy (Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="id4.4.m1.1"><semantics id="id4.4.m1.1a"><mi id="id4.4.m1.1.1" xref="id4.4.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="id4.4.m1.1b"><ci id="id4.4.m1.1.1.cmml" xref="id4.4.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="id4.4.m1.1d">italic_d</annotation></semantics></math> of 0.38) and energy consumption (Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="id5.5.m2.1"><semantics id="id5.5.m2.1a"><mi id="id5.5.m2.1.1" xref="id5.5.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="id5.5.m2.1b"><ci id="id5.5.m2.1.1.cmml" xref="id5.5.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="id5.5.m2.1d">italic_d</annotation></semantics></math> of 0.92).
Lastly, subset-based training led to significantly lower energy consumption (Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="id6.6.m3.1"><semantics id="id6.6.m3.1a"><mi id="id6.6.m3.1.1" xref="id6.6.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="id6.6.m3.1b"><ci id="id6.6.m3.1.1.cmml" xref="id6.6.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="id6.6.m3.1d">italic_d</annotation></semantics></math> of 0.91), while training on the whole dataset did not increase accuracy significantly.</p>
<p class="ltx_p" id="id9.id3"><span class="ltx_text ltx_font_bold" id="id9.id3.1">Limitations:</span>
Our results cannot be easily generalized to deep learning, non-tabular ML use cases, or ensembles of substantially larger size.</p>
<p class="ltx_p" id="id10.id4"><span class="ltx_text ltx_font_bold" id="id10.id4.1">Conclusions:</span>
From a Green AI perspective, we recommend designing ensembles of small size (2 or maximum 3 models), using subset-based training, majority voting, and energy-efficient ML algorithms like decision trees, Naive Bayes, or KNN. Our results help to understand design decisions that impact the architectural quality concerns of environmental sustainability and functional correctness in ML-enabled systems.
ML researchers and practitioners can use our findings to guide their design decisions for ML-enabled systems based on ensembles.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
energy efficiency, machine learning, ensemble learning, design trade-offs, controlled experiment, Green AI

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Journal of Systems and Software</span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\affiliation</span>
<p class="ltx_p" id="p1.2">[org1]
organization = FrAmeLab, University of L’Aquila,
city = L’Aquila,
country = Italy

<span class="ltx_ERROR undefined" id="p1.2.1">\affiliation</span>[org2]
organization = Vrije Universiteit Amsterdam,
city = Amsterdam,
country = The Netherlands

<span class="ltx_ERROR undefined" id="p1.2.2">\affiliation</span>[org3]
organization = Universitat Politècnica de Catalunya - BarcelonaTech,
city = Barcelona,
country = Spain</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">ML-enabled systems, i.e., systems that use machine learning (ML) models for parts of their functionality, have experienced a rapid rise in popularity across various domains, from healthcare to finance to autonomous systems <cite class="ltx_cite ltx_citemacro_citep">(Injadat et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib20" title="">2021</a>)</cite>.
However, the inherent nature of ML models poses several challenges.
These challenges include the significant computational resources required for training and inference <cite class="ltx_cite ltx_citemacro_citep">(Schwartz et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib45" title="">2020</a>)</cite>, potential overfitting, and the limited generalizability of model predictions for unseen data <cite class="ltx_cite ltx_citemacro_citep">(Santos and Papa, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib43" title="">2022</a>)</cite>.
Moreover, the deployment of these models in real-world applications often leads to substantial energy consumption, raising concerns about the sustainability and cost-effectiveness of ML solutions <cite class="ltx_cite ltx_citemacro_citep">(Strubell et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib49" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To alleviate some of these issues, <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">ML model composition</span> has emerged as a promising technique.
According to <cite class="ltx_cite ltx_citemacro_citet">Apel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib3" title="">2022</a>)</cite>, composing multiple ML models to solve complex problems can provide a structured approach to address the lack of specifications.
For instance, instead of relying on a single monolithic model, the task of automatic image captioning can be decomposed into several steps, each handled by a different model.
This divide-and-conquer approach allows for independent development and testing of models, potentially reusing models or training data from other domains.
However, this approach is not without its own challenges, as interactions between models can lead to unexpected behaviors, necessitating a careful design to manage these interactions effectively.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">From a software architecture perspective, ML model composition is an interesting consideration towards more modular ML-enabled systems.
<cite class="ltx_cite ltx_citemacro_citet">Heyn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib19" title="">2023</a>)</cite> discuss the importance of adopting a compositional approach to creating architecture frameworks, particularly in distributed AI systems.
This perspective emphasizes the need for modularity and systematic design to manage the complexity of systems composed of multiple ML components.
Effective architectural frameworks can help in isolating components, designing communication channels, and implementing coordination logic to handle interactions between models.
This approach not only enhances the reliability and maintainability of ML-enabled systems but also aligns with software engineering best practices related to modularity that go back as far as to <cite class="ltx_cite ltx_citemacro_citet">Parnas (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib36" title="">1979</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The most common form of ML model composition is <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">ensemble learning</span>.
In an ensemble, multiple ML models are trained for the same task and then their predictions on the same input are fused to improve overall accuracy <cite class="ltx_cite ltx_citemacro_citep">(Sarkar and Natarajan, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib44" title="">2019</a>)</cite>.
Ensemble learning methods, such as bagging, boosting, and stacking <cite class="ltx_cite ltx_citemacro_citep">(Kunapuli, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite>, aim to leverage the strengths of individual models while mitigating their weaknesses.
This approach has been widely studied for its ability to enhance the accuracy of ML predictions<cite class="ltx_cite ltx_citemacro_citep">(Sagi and Rokach, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib41" title="">2018</a>; Kunapuli, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite>.
However, while many empirical studies have focused on the accuracy of different types of ensembles, the energy efficiency of ensemble learning has not been thoroughly explored.
Understanding a potential trade-off between accuracy and energy consumption is crucial, especially in resource-constrained environments.
Knowing how different design decisions regarding ensembles impact these quality attributes can inform the design and development of more sustainable and effective ML-enabled systems.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we therefore present a controlled experiment to investigate how three types of design decisions influence the energy efficiency of ensembles:
a) the number of models in the ensemble, b) the fusion method, and c) the partition method for the training data.
Our goal is to support practitioners in navigating potential design trade-offs in ensemble learning.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Energy Efficiency of ML-Enabled Systems</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The energy and carbon footprints associated with ML models and their integrated systems have become a growing concern.
For instance, training a standard natural language processing (NLP) model based on transformers can result in greenhouse gas emissions comparable to those produced by several cars over their lifetimes <cite class="ltx_cite ltx_citemacro_citep">(Strubell et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib49" title="">2019</a>)</cite>.
Consequently, <cite class="ltx_cite ltx_citemacro_citet">Schwartz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib45" title="">2020</a>)</cite> introduced the distinction between <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Green AI</span> and <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">Red AI</span>.
Traditionally, AI development focused primarily on achieving high prediction quality with little regard for energy efficiency (Red AI).
In contrast, Green AI development emphasizes minimizing energy consumption and carbon emissions while maintaining high accuracy.
While accuracy and energy consumption are often viewed as a tradeoff <cite class="ltx_cite ltx_citemacro_citep">(Brownlee et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib7" title="">2021</a>)</cite>, studies have demonstrated that various techniques can significantly reduce energy consumption with minimal impact on accuracy <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>; Del Rey et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib13" title="">2023</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib56" title="">2023</a>)</cite>.
Today, Green AI is an active research field, with a recently published review <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib55" title="">2023</a>)</cite> with data collection in 2022 identifying 98 primary studies on the topic.
Since then, Green AI research has substantially increased.
The initial development phase of ML models is typically the most energy-intensive stage within the life cycle of ML-enabled systems <cite class="ltx_cite ltx_citemacro_citep">(Kaack et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib23" title="">2022</a>)</cite>, thus making ML training more energy-efficient has received considerable attention.
However, other phases, such as the operation of ML-enabled systems, also offer substantial opportunities for reducing their environmental impact.
This paper focuses on the energy consumption of the training and testing phases of ensemble learning, which is an understudied area.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Ensemble Learning</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Ensemble learning, a machine learning technique that combines multiple base learners to create an ensemble learner, aims to achieve superior generalization of learning systems <cite class="ltx_cite ltx_citemacro_citep">(Dietterich, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib14" title="">2000</a>; Zhang and Ma, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib63" title="">2012</a>)</cite>.
It is renowned for its ability to enhance model accuracy, drawing on the concept of the <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S2.SS2.p1.1.1">“Wisdom of the Crowd”</span> <cite class="ltx_cite ltx_citemacro_citep">(Kunapuli, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>; Sagi and Rokach, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib41" title="">2018</a>)</cite>.
A simple analogy of ensemble learning can be illustrated by Dr. Randy Forrest’s approach (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2.F1" title="Figure 1 ‣ 2.2 Ensemble Learning ‣ 2 Background and Related Work ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">1</span></a>), where he consults three doctors to provide their opinions on a patient’s cancer diagnosis, ultimately selecting the majority consensus as the final diagnostic <cite class="ltx_cite ltx_citemacro_citep">(Kunapuli, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S2.F1.g1" src="extracted/5707683/pics/doctors_ensemble.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of an ensemble of doctors, taken from <cite class="ltx_cite ltx_citemacro_citet">Kunapuli (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The basic architecture of ensemble learning, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S2.F2" title="Figure 2 ‣ 2.2 Ensemble Learning ‣ 2 Background and Related Work ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">2</span></a>, involves individual models trained as <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">base learners</span> (also called <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">weak learners</span>).
Ensemble learning can consist of either homogeneous ensembles, constructed from a single model type, or heterogeneous ensembles, which incorporate different model types to introduce diversity <cite class="ltx_cite ltx_citemacro_citep">(Sarkar and Natarajan, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib44" title="">2019</a>)</cite>.
Diverse models are typically more popular because they enhance the ensemble’s robustness.
The individual models can be trained using either the entire original dataset or subsets of it.
Various methods are available for partitioning the dataset into subsets, including boosting, random forest, and cross-validation <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Ma, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib63" title="">2012</a>; Wolpert, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib58" title="">1992</a>)</cite>.
We chose cross-validation to partition the dataset into subsets because it allows us to obtain representative and mutually exclusive subsets with small equal sizes.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The final component of the ensemble learning architecture involves the aggregation of the base learner outputs, which is called <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">fusion</span>.
Two primary methods are employed to fuse the outputs for classification tasks: majority voting and meta-model fusion.
In majority voting, the final decision is based on the most popular option.
Potential ties are typically resolved by using the accuracy of individual models as weights <cite class="ltx_cite ltx_citemacro_citep">(Kokkinos and Margaritis, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib24" title="">2014</a>)</cite>.
In contrast, meta-model fusion involves training another ML model on the outputs of the base learners, with the final prediction made by the trained meta-model. Based on these different choices, we will have various combinations of the architectural components of the ensembles, leading to multiple architectural options. In our study, we will compare the accuracy and energy efficiency of these different ensemble architectures.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S2.F2.g1" src="extracted/5707683/pics/ensemble_basic_architecture.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Basic architecture of ensemble learning, taken from <cite class="ltx_cite ltx_citemacro_citet">Dietterich (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib14" title="">2000</a>)</cite></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Related Work</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Research on the energy consumption of software systems spans various domains, including frameworks <cite class="ltx_cite ltx_citemacro_citep">(Calero et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib8" title="">2021</a>)</cite>, data structures <cite class="ltx_cite ltx_citemacro_citep">(Oliveira et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib34" title="">2019</a>)</cite>, and programming languages <cite class="ltx_cite ltx_citemacro_citep">(Pereira et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib37" title="">2021</a>; Georgiou et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib16" title="">2018</a>)</cite>.
This body of work, known as Green Software, aims to guide developers in creating more energy-efficient systems <cite class="ltx_cite ltx_citemacro_citep">(Anwar et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib2" title="">2020</a>; Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib54" title="">2017</a>; Ribeiro et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib40" title="">2021</a>; Cruz and Abreu, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib12" title="">2019</a>)</cite>.
However, the energy efficiency of AI-based systems, particularly machine learning models, remains largely underexplored <cite class="ltx_cite ltx_citemacro_citep">(Van Wynsberghe, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib52" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Strubell et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib49" title="">2019</a>)</cite> drew attention to the significant carbon emissions resulting from training large NLP models, prompting further investigation into AI’s energy consumption.
<cite class="ltx_cite ltx_citemacro_citet">Schwartz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib45" title="">2020</a>)</cite> introduced the concept of Green AI, advocating for algorithms that balance accuracy with computational costs.
<cite class="ltx_cite ltx_citemacro_citet">Bender et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib4" title="">2021</a>)</cite> emphasized the importance of curating datasets and assessing risks to mitigate the environmental impact of expanding NLP models.
Other studies have examined the impact of model size and dataset downsampling on accuracy, emphasizing AI sustainability <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib59" title="">2022</a>; Zogaj et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib66" title="">2021</a>)</cite>.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Garcia-Martin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib15" title="">2017</a>)</cite> achieved significant energy savings with minimal accuracy loss using the Very Fast Decision Tree (VFDT) algorithm.
<cite class="ltx_cite ltx_citemacro_citet">Georgiou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib17" title="">2022</a>)</cite> found that TensorFlow is more energy-efficient for training, whereas PyTorch is better for inference. <cite class="ltx_cite ltx_citemacro_citet">Omar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib35" title="">2024</a>)</cite> has investigated the energy efficiency of various concept drift detectors to efficiently facilitate updating ML models as needed rather than periodically<cite class="ltx_cite ltx_citemacro_citep">(Järvenpää et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib22" title="">2024</a>; Poenaru-Olaru et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib38" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">The use of multiple ML models to improve accuracy naturally leads to a higher energy consumption in ensemble learning.
<cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib65" title="">2002</a>)</cite> introduced GASEN, an approach to optimize neural network weights to select effective subsets of models.
They demonstrated that a full ensemble is not always necessary.
However, their approach itself consumes more energy because of its repetitive nature.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib29" title="">2023</a>)</cite> proposed the IRENE method to reduce inference costs through a learnable selector and early halting, although this method is limited to sequential processing. <cite class="ltx_cite ltx_citemacro_citet">Nijkamp et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib33" title="">2024</a>)</cite> introduced model selection strategies in ensemble learning to optimize performance and reduce resource consumption during the inference phase in real-time production environments.
<cite class="ltx_cite ltx_citemacro_citet">Kotary et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib25" title="">2022</a>)</cite> presented a combinatorial optimization framework for model selection. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Cordeiro et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib10" title="">2023</a>)</cite> proposed PS-DES, which evaluates ensembles for each query instance, thereby reducing the number of models during inference in AI pipelines.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">To the best of our knowledge, none of the existing work has studied the comparative analysis of different fusion methods, the comparison of training strategies for individual models (whole dataset vs. subset), and the combination of different sizes, fusion methods, and training strategies.
Our research focuses on these aspects and analyzes the trade-offs between energy consumption and accuracy in machine learning ensembles.
By investigating the impact of ensemble size, fusion methods, and training strategies, we aim to optimize energy consumption without compromising accuracy.
This contributes to the broader goals of Green AI and sustainable machine learning practices, providing valuable insights for both researchers and practitioners.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To address this gap, we designed a controlled experiment <cite class="ltx_cite ltx_citemacro_citep">(Wohlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib57" title="">2012</a>)</cite>.
In this section, we describe the most important details associated with this research method, namely our experiment objects (ML algorithms, datasets, fusion methods, and training data allocation methods), experiment variables, and experiment execution.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objective and Research Questions</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Driven by the need for ML-enabled systems that are both accurate and sustainable, this research investigates a potential energy consumption vs. accuracy trade-off in ensemble learning.
By analyzing the impact of ensemble size, fusion methods, and dataset partitioning, we aim to understand how these factors influence the balance between energy consumption and predictive performance.
Ultimately, accumulating evidence in this area will pave the way for guidelines that practitioners can use to navigate these trade-offs when designing ML-enabled systems.
We will achieve our research goal by answering the following research questions:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">RQ1:</span> How does the number of models in an ensemble influence energy consumption and accuracy?</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The more models you add to an ensemble, the more energy their training and inference usage will consume.
However, adding additional models to increase diversity may also increase the overall accuracy of the ensemble<cite class="ltx_cite ltx_citemacro_cite">Kunapuli (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite>, it is plausible that a trade-off exists between these two quality concerns.
We want to analyze how quickly we arrive at a point of diminishing returns, i.e., when adding another model only leads to marginal gains in accuracy while consuming substantially more energy.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">RQ2:</span> How do the ensemble fusion methods <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">majority voting</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.3">meta-model</span> compare in terms of energy consumption and accuracy?</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">We assume that using <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.1">majority voting</span> in the fusion step does not require much energy due to the nature of the operation, but it is also fairly simplistic.
On the other hand, using a specifically trained additional ML model as a <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.2">meta-model</span> is a more complex approach that can lead to accuracy improvements <cite class="ltx_cite ltx_citemacro_citep">(Bonissone et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib5" title="">2011</a>)</cite>, but may require more energy due to an additional ML model being trained.
We want to analyze how strongly this decision influences the potential overall trade-off,
especially since, with increasing ensemble size, the meta-model will be trained on more features.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">RQ3:</span> How do the ensemble training methods <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">whole-dataset</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.3">subset-based</span> training compare in terms of energy consumption and accuracy?</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">Another design decision is whether to train each individual model in the ensemble on the <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.1">whole dataset</span> or only on a <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.2">subset</span>.
The first will likely lead to more accuracy <cite class="ltx_cite ltx_citemacro_citep">(Prusa et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib39" title="">2015</a>)</cite> but consumes more energy <cite class="ltx_cite ltx_citemacro_citep">(Salehi and Schmeink, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib42" title="">2023</a>)</cite>, while the second consumes less energy but will likely not have the same prediction performance.
We want to understand how strong the differences between these two options are, and how variation in the ensemble size moderates this relationship.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ML Algorithms</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To increase comparability between the different experiment configurations and to limit the experiment space, we only focused on supervised classification tasks and traditional ML algorithms (no deep learning).
Various ML algorithms are available for this, each with its own strengths and weaknesses.
Combining diverse models in an ensemble may therefore yield different effects on the accuracy and energy consumption of the ensemble.
Since the choice of ML framework can also impact the energy consumption of models <cite class="ltx_cite ltx_citemacro_citep">(Georgiou et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib17" title="">2022</a>)</cite>, we exclusively used the widely adopted <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">scikit-learn</span> framework<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://scikit-learn.org/stable/supervised_learning.html" title="">https://scikit-learn.org/stable/supervised_learning.html</a></span></span></span> for comparability.
From this framework, we selected popular implemented traditional ML algorithms for classification.
Note that we excluded algorithms that are ensembles by design, e.g., random forests or the gradient boosting classifier.
The four selected ML algorithms are:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Support Vector Machine (SVM):</span> SVMs are kernel-based supervised models that focus on maximizing the <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.I1.i1.p1.1.2">“margin”</span> in classification tasks, i.e., the widest gap between the decision boundary (represented by a hyperplane) and the closest data points from each class. This margin maximization has been shown to reduce the potential for errors when the model encounters new, unseen data, i.e., it improves generalization <cite class="ltx_cite ltx_citemacro_citep">(Cristianini and Shawe-Taylor, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib11" title="">2000</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Decision Tree (DT):</span> DTs are supervised learning models based on tree-like structures that can be used for various classification tasks <cite class="ltx_cite ltx_citemacro_citep">(Suthaharan, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib50" title="">2016</a>)</cite>. They classify instances by sorting them based on feature values. Each node in the tree represents a feature in an instance to be classified, and each branch represents a possible value for that feature. During classification, instances start at the root node and are sorted down the tree based on their features, reaching the appropriate leaf node and its corresponding class prediction <cite class="ltx_cite ltx_citemacro_citep">(Kotsiantis et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib27" title="">2007</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Naive Bayes (NB):</span> NB classifiers, known for their simplicity and efficiency, are probabilistic models based on Bayes’ theorem. They operate under the assumption that each feature within a dataset contributes independently and equally to the likelihood of a sample belonging to a specific class. This means that features are mutually exclusive and have the same influence in determining the class outcome. This <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S3.I1.i3.p1.1.2">“naive”</span> assumption, while not always true in reality, allows the NB classifier to be computationally fast even on large, high-dimensional datasets <cite class="ltx_cite ltx_citemacro_citep">(Misra et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib31" title="">2019</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">k-Nearest Neighbors (KNN):</span> the similarity-based KNN algorithm classifies new data points using the majority vote of their closest neighbors in the training data, i.e., similar data points are likely to have a similar class. It is a simple and effective method, especially for large datasets <cite class="ltx_cite ltx_citemacro_citep">(Kotsiantis et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib27" title="">2007</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We used <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">scikit-learn</span> in version 1.4.1 for all ML models in the default configuration without hyperparameter tuning.
Our primary focus is the energy consumption of ensembles, and individual model optimization via hyperparameter tuning is beyond the scope of this study.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To increase the generalizability of our experiment results for classification tasks, we selected four different datasets with varying sizes and data types.
These datasets are available via the UC Irvine Machine Learning Repository, a public repository of ML training datasets that are widely used in research.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">TUNADROMD:</span> this IT security dataset<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://archive.ics.uci.edu/dataset/813/tunadromd" title="">https://archive.ics.uci.edu/dataset/813/tunadromd</a></span></span></span> contains 4,465 software instances and 241 attributes <cite class="ltx_cite ltx_citemacro_citep">(Borah et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib6" title="">2020</a>)</cite>. The target attribute for classification is a binary category that indicates whether an executable is malicious or not (malware vs. goodware).</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Default of Credit Card Clients:</span> this finance dataset<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients" title="">https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients</a></span></span></span>, contains 30,000 instances and 23 attributes on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005 <cite class="ltx_cite ltx_citemacro_citep">(Yeh, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib62" title="">2016</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">RT-IoT2022:</span> this Internet of Things (IoT) dataset<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://archive.ics.uci.edu/dataset/942/rt-iot2022" title="">https://archive.ics.uci.edu/dataset/942/rt-iot2022</a></span></span></span>, derived from a real-time IoT infrastructure, contains 123,117 instances and 83 features. It simulates real-world IoT environments by combining data from various devices and mimicking cyberattacks <cite class="ltx_cite ltx_citemacro_citep">(Sharmila and Nagapadma, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib47" title="">2023</a>)</cite>. The target attribute for the classification is a categorical variable called attack type.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Heart Disease:</span> this medical dataset<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://archive.ics.uci.edu/dataset/45/heart+disease" title="">https://archive.ics.uci.edu/dataset/45/heart+disease</a></span></span></span> contains 13 features collected from 303 patients related to their cardiac features including age, gender, blood pressure, cholesterol levels, electrocardiographic (ECG) features, and more <cite class="ltx_cite ltx_citemacro_citep">(Janosi et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib21" title="">1988</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Fusion Methods</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Output fusion is the step in ensemble learning where the individual predictions from the multiple base models are aggregated into one final output.
We compared two approaches within our classification scenarios:</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">Majority voting:</span> this method is the simplest weighting method for classification problems, as the selected class is the one with the most votes <cite class="ltx_cite ltx_citemacro_citep">(Sagi and Rokach, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib41" title="">2018</a>)</cite>. For ties, we use the accuracy of individual models as weights to force a decision <cite class="ltx_cite ltx_citemacro_citep">(Gungor et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib18" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">Meta-model:</span> in this meta-learning approach, the ensemble consists of multiple learning stages. The base models form the first stage, with their individual predictions feeding into a meta-model that generates the final output. This approach is advantageous when base models display varying effectiveness across different data subsets.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">We compared these two methods in terms of energy consumption and accuracy.
For the meta-model, we chose KNN for its lower energy consumption compared to other models <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>)</cite>. For selecting the value of k, we followed an accepted rule of thumb that is both simple and computationally efficient: using the square root of the number of data points in the training set <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib64" title="">2017</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Dataset Partitioning Methods</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Since data-centric methods show promising potential to improve the energy efficiency of ML training <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>; Alswaitti et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib1" title="">2024</a>)</cite>, we also included basic dataset partitioning into the experiment.
We selected two variations of <span class="ltx_text ltx_font_italic" id="S3.SS5.p1.1.1">stacking</span> for creating the ensemble <cite class="ltx_cite ltx_citemacro_citep">(Kunapuli, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib28" title="">2023</a>)</cite>, i.e., each individual model was first trained using the full dataset and then using a randomly selected subset of the original dataset.
To create these subsets, we used <span class="ltx_text ltx_font_italic" id="S3.SS5.p1.1.2">horizontal partitioning</span> <cite class="ltx_cite ltx_citemacro_citep">(Sagi and Rokach, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib41" title="">2018</a>)</cite>.
This involves dividing the dataset into disjoint subsets of equal size, where the number of subsets is equal to the number of models participating in the ensemble <cite class="ltx_cite ltx_citemacro_citep">(Ting and Witten, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib51" title="">1997</a>)</cite>.
We chose horizontal partitioning because no single partitioning method generally outperforms others in terms of accuracy <cite class="ltx_cite ltx_citemacro_citep">(Kotsianti and Kanellopoulos, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib26" title="">2007</a>)</cite>.
However, reducing the size of the dataset affects the energy consumption of models during training <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>)</cite>.
Partitioning the dataset into horizontally disjoint subsets is a simple way of reducing the dataset size for each individual participating model.
This choice will help us understand the impact of dataset partitioning on a potential trade-off between energy consumption and accuracy in ensembles.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Experiment Variables</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">In our ensemble experiments, we analyze the trade-off between two <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.1">dependent variables</span>:</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I4.i1.p1.1.1">Ensemble energy consumption</span> measured in Joule (J); this includes the energy consumed for training each individual model in the ensemble (plus training the optional meta-model), but also the energy consumed during the inference to include the different fusion methods.</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I4.i2.p1.1.1">Ensemble accuracy</span> measured in <math alttext="F_{1}" class="ltx_Math" display="inline" id="S3.I4.i2.p1.1.m1.1"><semantics id="S3.I4.i2.p1.1.m1.1a"><msub id="S3.I4.i2.p1.1.m1.1.1" xref="S3.I4.i2.p1.1.m1.1.1.cmml"><mi id="S3.I4.i2.p1.1.m1.1.1.2" xref="S3.I4.i2.p1.1.m1.1.1.2.cmml">F</mi><mn id="S3.I4.i2.p1.1.m1.1.1.3" xref="S3.I4.i2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.1.m1.1b"><apply id="S3.I4.i2.p1.1.m1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I4.i2.p1.1.m1.1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I4.i2.p1.1.m1.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.1.1.2">𝐹</ci><cn id="S3.I4.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I4.i2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.1.m1.1c">F_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> score, i.e., the harmonic mean between precision and recall</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">Additionally, we defined several <span class="ltx_text ltx_font_bold" id="S3.SS6.p3.1.1">independent variables</span> that we consciously manipulated to see how they influence the dependent variables:</p>
<ul class="ltx_itemize" id="S3.I5">
<li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I5.i1.p1">
<p class="ltx_p" id="S3.I5.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I5.i1.p1.1.1">Ensemble size:</span> using our four ML algorithms, we studied ensembles of increasing size (from 1 to 4 models) for all possible combinations</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I5.i2.p1">
<p class="ltx_p" id="S3.I5.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I5.i2.p1.1.1">Fusion methods:</span> using majority voting vs. a meta-model</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I5.i3.p1">
<p class="ltx_p" id="S3.I5.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I5.i3.p1.1.1">Datasets:</span> using our four datasets that differ in number of instances and features</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I5.i4.p1">
<p class="ltx_p" id="S3.I5.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I5.i4.p1.1.1">Dataset partitioning:</span> using the whole dataset for training vs. horizontal partitioning</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Experiment Execution</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">By combining the different independent variables, we created a <span class="ltx_text ltx_font_italic" id="S3.SS7.p1.1.1">full factorial design</span> <cite class="ltx_cite ltx_citemacro_citep">(Wohlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib57" title="">2012</a>)</cite>.
For the ensemble size, we needed to take all potential combinations of the four base models into account:</p>
</div>
<div class="ltx_para" id="S3.SS7.p2">
<ul class="ltx_itemize" id="S3.I6">
<li class="ltx_item" id="S3.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I6.i1.p1">
<p class="ltx_p" id="S3.I6.i1.p1.1">4 individual models on their own</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i2" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I6.i2.p1">
<p class="ltx_p" id="S3.I6.i2.p1.1"><math alttext="\binom{4}{2}=6" class="ltx_Math" display="inline" id="S3.I6.i2.p1.1.m1.2"><semantics id="S3.I6.i2.p1.1.m1.2a"><mrow id="S3.I6.i2.p1.1.m1.2.3" xref="S3.I6.i2.p1.1.m1.2.3.cmml"><mrow id="S3.I6.i2.p1.1.m1.2.2.4" xref="S3.I6.i2.p1.1.m1.2.2.3.cmml"><mo id="S3.I6.i2.p1.1.m1.2.2.4.1" xref="S3.I6.i2.p1.1.m1.2.2.3.1.cmml">(</mo><mfrac id="S3.I6.i2.p1.1.m1.2.2.2.2" linethickness="0pt" xref="S3.I6.i2.p1.1.m1.2.2.3.cmml"><mn id="S3.I6.i2.p1.1.m1.1.1.1.1.1.1" xref="S3.I6.i2.p1.1.m1.1.1.1.1.1.1.cmml">4</mn><mn id="S3.I6.i2.p1.1.m1.2.2.2.2.2.1" xref="S3.I6.i2.p1.1.m1.2.2.2.2.2.1.cmml">2</mn></mfrac><mo id="S3.I6.i2.p1.1.m1.2.2.4.2" xref="S3.I6.i2.p1.1.m1.2.2.3.1.cmml">)</mo></mrow><mo id="S3.I6.i2.p1.1.m1.2.3.1" xref="S3.I6.i2.p1.1.m1.2.3.1.cmml">=</mo><mn id="S3.I6.i2.p1.1.m1.2.3.2" xref="S3.I6.i2.p1.1.m1.2.3.2.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.i2.p1.1.m1.2b"><apply id="S3.I6.i2.p1.1.m1.2.3.cmml" xref="S3.I6.i2.p1.1.m1.2.3"><eq id="S3.I6.i2.p1.1.m1.2.3.1.cmml" xref="S3.I6.i2.p1.1.m1.2.3.1"></eq><apply id="S3.I6.i2.p1.1.m1.2.2.3.cmml" xref="S3.I6.i2.p1.1.m1.2.2.4"><csymbol cd="latexml" id="S3.I6.i2.p1.1.m1.2.2.3.1.cmml" xref="S3.I6.i2.p1.1.m1.2.2.4.1">binomial</csymbol><cn id="S3.I6.i2.p1.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.I6.i2.p1.1.m1.1.1.1.1.1.1">4</cn><cn id="S3.I6.i2.p1.1.m1.2.2.2.2.2.1.cmml" type="integer" xref="S3.I6.i2.p1.1.m1.2.2.2.2.2.1">2</cn></apply><cn id="S3.I6.i2.p1.1.m1.2.3.2.cmml" type="integer" xref="S3.I6.i2.p1.1.m1.2.3.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.i2.p1.1.m1.2c">\binom{4}{2}=6</annotation><annotation encoding="application/x-llamapun" id="S3.I6.i2.p1.1.m1.2d">( FRACOP start_ARG 4 end_ARG start_ARG 2 end_ARG ) = 6</annotation></semantics></math> ensembles with 2 models</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i3" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I6.i3.p1">
<p class="ltx_p" id="S3.I6.i3.p1.1"><math alttext="\binom{4}{3}=4" class="ltx_Math" display="inline" id="S3.I6.i3.p1.1.m1.2"><semantics id="S3.I6.i3.p1.1.m1.2a"><mrow id="S3.I6.i3.p1.1.m1.2.3" xref="S3.I6.i3.p1.1.m1.2.3.cmml"><mrow id="S3.I6.i3.p1.1.m1.2.2.4" xref="S3.I6.i3.p1.1.m1.2.2.3.cmml"><mo id="S3.I6.i3.p1.1.m1.2.2.4.1" xref="S3.I6.i3.p1.1.m1.2.2.3.1.cmml">(</mo><mfrac id="S3.I6.i3.p1.1.m1.2.2.2.2" linethickness="0pt" xref="S3.I6.i3.p1.1.m1.2.2.3.cmml"><mn id="S3.I6.i3.p1.1.m1.1.1.1.1.1.1" xref="S3.I6.i3.p1.1.m1.1.1.1.1.1.1.cmml">4</mn><mn id="S3.I6.i3.p1.1.m1.2.2.2.2.2.1" xref="S3.I6.i3.p1.1.m1.2.2.2.2.2.1.cmml">3</mn></mfrac><mo id="S3.I6.i3.p1.1.m1.2.2.4.2" xref="S3.I6.i3.p1.1.m1.2.2.3.1.cmml">)</mo></mrow><mo id="S3.I6.i3.p1.1.m1.2.3.1" xref="S3.I6.i3.p1.1.m1.2.3.1.cmml">=</mo><mn id="S3.I6.i3.p1.1.m1.2.3.2" xref="S3.I6.i3.p1.1.m1.2.3.2.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.i3.p1.1.m1.2b"><apply id="S3.I6.i3.p1.1.m1.2.3.cmml" xref="S3.I6.i3.p1.1.m1.2.3"><eq id="S3.I6.i3.p1.1.m1.2.3.1.cmml" xref="S3.I6.i3.p1.1.m1.2.3.1"></eq><apply id="S3.I6.i3.p1.1.m1.2.2.3.cmml" xref="S3.I6.i3.p1.1.m1.2.2.4"><csymbol cd="latexml" id="S3.I6.i3.p1.1.m1.2.2.3.1.cmml" xref="S3.I6.i3.p1.1.m1.2.2.4.1">binomial</csymbol><cn id="S3.I6.i3.p1.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.I6.i3.p1.1.m1.1.1.1.1.1.1">4</cn><cn id="S3.I6.i3.p1.1.m1.2.2.2.2.2.1.cmml" type="integer" xref="S3.I6.i3.p1.1.m1.2.2.2.2.2.1">3</cn></apply><cn id="S3.I6.i3.p1.1.m1.2.3.2.cmml" type="integer" xref="S3.I6.i3.p1.1.m1.2.3.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.i3.p1.1.m1.2c">\binom{4}{3}=4</annotation><annotation encoding="application/x-llamapun" id="S3.I6.i3.p1.1.m1.2d">( FRACOP start_ARG 4 end_ARG start_ARG 3 end_ARG ) = 4</annotation></semantics></math> ensembles with 3 models</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i4" style="list-style-type:none;padding-top:3.0pt;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I6.i4.p1">
<p class="ltx_p" id="S3.I6.i4.p1.1">1 ensemble with 4 models</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS7.p3">
<p class="ltx_p" id="S3.SS7.p3.4">This resulted in 11 ensemble combinations.
For the individual models, we covered an experiment space of 4 ML algorithms <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS7.p3.1.m1.1"><semantics id="S3.SS7.p3.1.m1.1a"><mo id="S3.SS7.p3.1.m1.1.1" xref="S3.SS7.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.1.m1.1b"><times id="S3.SS7.p3.1.m1.1.1.cmml" xref="S3.SS7.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.1.m1.1d">×</annotation></semantics></math> 4 datasets (16 combinations).
For the ensembles, we covered an experiment space of 11 ensembles <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS7.p3.2.m2.1"><semantics id="S3.SS7.p3.2.m2.1a"><mo id="S3.SS7.p3.2.m2.1.1" xref="S3.SS7.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.2.m2.1b"><times id="S3.SS7.p3.2.m2.1.1.cmml" xref="S3.SS7.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.2.m2.1d">×</annotation></semantics></math> 4 datasets <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS7.p3.3.m3.1"><semantics id="S3.SS7.p3.3.m3.1a"><mo id="S3.SS7.p3.3.m3.1.1" xref="S3.SS7.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.3.m3.1b"><times id="S3.SS7.p3.3.m3.1.1.cmml" xref="S3.SS7.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.3.m3.1d">×</annotation></semantics></math> 2 fusion methods <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS7.p3.4.m4.1"><semantics id="S3.SS7.p3.4.m4.1a"><mo id="S3.SS7.p3.4.m4.1.1" xref="S3.SS7.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS7.p3.4.m4.1b"><times id="S3.SS7.p3.4.m4.1.1.cmml" xref="S3.SS7.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p3.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p3.4.m4.1d">×</annotation></semantics></math> 2 partitioning methods (176 combinations).
For training the models on the entire dataset, we followed the steps outlined below for each of the 11 ensembles and 4 dataset combinations.
These four steps constitute one iteration.
We repeated each iteration 30 times to improve the reliability of the energy consumption measurements, to reduce effects of potential fluctuations in the experiment infrastructure, and to sample different parts of the dataset for training and testing for the accuracy evaluations.</p>
</div>
<div class="ltx_para" id="S3.SS7.p4">
<ol class="ltx_enumerate" id="S3.I7">
<li class="ltx_item" id="S3.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I7.i1.p1">
<p class="ltx_p" id="S3.I7.i1.p1.1">Randomly divide the dataset into three sets using a 60/20/20 split. The 60% split set is used to train the base models, the first 20% split set is used to test the base models, and the second 20% split set is used to test the ensemble.</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I7.i2.p1">
<p class="ltx_p" id="S3.I7.i2.p1.1">Train each individual model on the training set while measuring energy consumption.</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I7.i3.p1">
<p class="ltx_p" id="S3.I7.i3.p1.1">Fuse the model outputs using weighting and evaluate ensemble accuracy on the testing set while measuring energy consumption.</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I7.i4.p1">
<p class="ltx_p" id="S3.I7.i4.p1.1">Train a meta-model and fuse the model outputs with it, then evaluate ensemble accuracy on the testing set, all while measuring energy consumption.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS7.p5">
<p class="ltx_p" id="S3.SS7.p5.1">In the second stage, each model was trained on a subset of the dataset using horizontal partitioning.
A similar set of steps as outlined above was followed, with the difference that the training set was randomly split into subsets of equal size according to the number of models in the ensemble.
Each model was then trained on such a randomly chosen subset instead of on the whole training set.</p>
</div>
<div class="ltx_para" id="S3.SS7.p6">
<p class="ltx_p" id="S3.SS7.p6.1">To instrument the experiment execution, we developed several Python scripts.
As a scripting language, Python is well suited to this task, and it is the de facto standard language for ML practitioners.
The process described above was completely automated.
To estimate the energy consumption, we used <span class="ltx_text ltx_font_typewriter" id="S3.SS7.p6.1.1">CodeCarbon<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote6.1.1.1">6</span></span><a class="ltx_ref ltx_url" href="https://codecarbon.io" title="">https://codecarbon.io</a></span></span></span></span>, a popular Python package.
While this is not as precise as hardware-based measurements, <span class="ltx_text ltx_font_typewriter" id="S3.SS7.p6.1.2">CodeCarbon</span> underreports the actual consumed energy only by a few percentage points <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib60" title="">2023</a>)</cite>, with a very strong correlation between both measurements (<math alttext="\rho=0.94" class="ltx_Math" display="inline" id="S3.SS7.p6.1.m1.1"><semantics id="S3.SS7.p6.1.m1.1a"><mrow id="S3.SS7.p6.1.m1.1.1" xref="S3.SS7.p6.1.m1.1.1.cmml"><mi id="S3.SS7.p6.1.m1.1.1.2" xref="S3.SS7.p6.1.m1.1.1.2.cmml">ρ</mi><mo id="S3.SS7.p6.1.m1.1.1.1" xref="S3.SS7.p6.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS7.p6.1.m1.1.1.3" xref="S3.SS7.p6.1.m1.1.1.3.cmml">0.94</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.p6.1.m1.1b"><apply id="S3.SS7.p6.1.m1.1.1.cmml" xref="S3.SS7.p6.1.m1.1.1"><eq id="S3.SS7.p6.1.m1.1.1.1.cmml" xref="S3.SS7.p6.1.m1.1.1.1"></eq><ci id="S3.SS7.p6.1.m1.1.1.2.cmml" xref="S3.SS7.p6.1.m1.1.1.2">𝜌</ci><cn id="S3.SS7.p6.1.m1.1.1.3.cmml" type="float" xref="S3.SS7.p6.1.m1.1.1.3">0.94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p6.1.m1.1c">\rho=0.94</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p6.1.m1.1d">italic_ρ = 0.94</annotation></semantics></math>).
Using tools like <span class="ltx_text ltx_font_typewriter" id="S3.SS7.p6.1.3">CodeCarbon</span> is therefore accepted practice when the experiment goal is to compare differences in energy consumption between several variants.
The complete reproducible code is publicly available in our artifact repository.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/doi/10.5281/zenodo.11664165" title="">https://zenodo.org/doi/10.5281/zenodo.11664165</a></span></span></span>
We executed the code on dedicated experiment infrastructure for software energy experiments located at the VU Amsterdam.
This infrastructure is based on a server equipped with 36 TB HDD, 384 GB RAM, and an Intel Xeon CPU including 16 cores with hyper-threading running at 2.1GHz (i.e., 32 vCPUs).
Running the experiment took approximately 50 hours.
To prevent unwanted additional load, we restricted access to the server during the experiment execution.
Before beginning the experiment, we executed a warm-up function to make the system more stable before starting data collection.
Moreover, we incorporated a 5-seconds sleep interval between each iteration to allow the infrastructure to return closer to its start state.
Together with repeating each configuration 30 times, these practices improved the reliability of our experiment and enabled a thorough and dependable assessment of the energy consumption and accuracy of the different ensembles.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>Data Analysis</h3>
<div class="ltx_para" id="S3.SS8.p1">
<p class="ltx_p" id="S3.SS8.p1.1">As an initial step, we assessed the normality of the distribution of results with the Shapiro-Wilk test <cite class="ltx_cite ltx_citemacro_citep">(Shapiro and Wilk, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib46" title="">1965</a>)</cite>.
The obtained result indicated that the data exhibits non-normal distribution characteristics, as evidenced by a p-value of 9.08e-18.
To evaluate if a correlation exists between size of ensemble and energy and accuracy of ensemble, we leverage the calculation of the one-tailed Spearman’s rank correlation coefficient <cite class="ltx_cite ltx_citemacro_citep">(Myers and Well, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib32" title="">2002</a>)</cite>.
To identify significant differences in the energy consumption and accuracy among meta-model vs. majority voting fusion and subset vs. whole-set training, we employed the Mann-Whitney U test <cite class="ltx_cite ltx_citemacro_citep">(Mann and Whitney, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib30" title="">1947</a>)</cite>, which can handle data which is not normally distributed.
To compare the magnitude of effects, we computed the percentage difference between the mean energy consumption values of different ensembles.
Furthermore, Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S3.SS8.p1.1.m1.1"><semantics id="S3.SS8.p1.1.m1.1a"><mi id="S3.SS8.p1.1.m1.1.1" xref="S3.SS8.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.1.m1.1b"><ci id="S3.SS8.p1.1.m1.1.1.cmml" xref="S3.SS8.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.p1.1.m1.1d">italic_d</annotation></semantics></math> values were computed for a more comprehensive understanding of the effect size <cite class="ltx_cite ltx_citemacro_citep">(Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib9" title="">1988</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present the experiment results according
to the research questions.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Impact of Ensemble Size (RQ1)</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We investigated the accuracy and energy consumption of ensembles of different sizes, specifically ensembles of sizes 2, 3, and 4.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.T1" title="Table 1 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F4" title="Figure 4 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">4</span></a>, average energy consumption clearly increases with increasing ensemble size, from 478.4 J for size 2 ensembles to 765.4 J for size 3, and up to 1,047.7 J for size 4.
The Spearman correlation test further supports these findings, yielding a significant p-value of 5.78e-35, indicating a correlation.
Specifically, an ensemble of size 2 consumes 37.49% less energy compared to an ensemble of size 3, and an ensemble of size 3 consumes 26.96% less energy than an ensemble of size 4.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F3.g1" src="extracted/5707683/pics/rq1_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Energy consumption of ensembles of different sizes</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In contrast, the relationship between ensemble size and accuracy is different.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.T1" title="Table 1 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F4" title="Figure 4 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">4</span></a>, adding arbitrary models to an existing ensemble did not always increase accuracy.
In most of our studied cases, ensembles of size 2 performed better than those of sizes 3 and 4.
The Spearman correlation test yielded a p-value of 0.90 for the relationship between ensemble size and F1-score, indicating no statistically significant correlation.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S4.F4.g1" src="extracted/5707683/pics/rq1_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Accuracy of ensembles of different sizes</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">This suggests that variations in the number of models within an ensemble do not meaningfully impact accuracy, as measured by the F1-score.
The accuracy difference between the worst ensemble (KNN, DT: 0.754) and the best (SVM, NB: 0.796) was only 0.042, with both having a size of 2.
The average F1-scores for ensemble sizes 2, 3, and 4 were 0.782, 0.774, and 0.780 respectively, demonstrating that increasing ensemble size does, on average, not positively impact accuracy.
However, increasing the number of models in an ensemble substantially increases energy consumption.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average energy consumption vs. accuracy, Max and Min F1 are referred to Max and Min F1 of individual models in the ensemble.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Ensemble Size</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Energy (J)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">F1</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Max F1</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">Min F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.2.1.1">2</th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.2.1.2">478.4</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.2.1.3">0.782</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.2.1.4">0.822</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.2.1.5">0.632</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.1.3.2.1">3</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.2">765.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">0.774</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.4">0.829</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.5">0.546</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.4.3.1">4</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.4.3.2">1047.7</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.4.3.3">0.780</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.4.3.4">0.835</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.4.3.5">0.474</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<svg class="ltx_picture" height="103.59" id="S4.SS1.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,103.59) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 0 L 0 103.59 L 600 103.59 L 600 0 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 101.62 L 598.03 101.62 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="76.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS1.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.pic1.1.1.1.1.1.1.1">Answer to RQ1:</span></span>
<span class="ltx_p" id="S4.SS1.p4.pic1.1.1.1.1.1.2">On average, adding more models to an ensemble does <span class="ltx_text ltx_font_bold" id="S4.SS1.p4.pic1.1.1.1.1.1.2.1">not</span> improve accuracy.
While larger ensembles consume more energy, they are not automatically more accurate compared to smaller ones.
Deliberate model selection to optimize energy efficiency within ensembles is therefore critical.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Impact of Majority Voting vs. Meta-Model Fusion (RQ2)</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For RQ2, we compared the ensemble fusion methods <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">majority voting</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">meta-model</span> regarding their impact on energy consumption and accuracy.
As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F5" title="Figure 5 ‣ 4.2 Impact of Majority Voting vs. Meta-Model Fusion (RQ2) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">5</span></a>, meta-model ensembles consumed more energy than majority voting ones in <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">all</span> cases.
The energy consumption for the meta-model method ranged from 119 J to 1,225 J, while the majority voting method ranged from 66 J to 869 J.
These findings are further supported by a Mann-Whitney U Test, which yielded a p-value of 2.66e-13 for energy consumption between these two methods, indicating a significant difference in energy consumption between the two fusion methods.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S4.F5.g1" src="extracted/5707683/pics/rq2_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Energy consumption of meta-model fusion vs. majority voting fusion</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">To quantify this difference, we calculated Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_d</annotation></semantics></math> and the percentage difference as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.T2" title="Table 2 ‣ 4.2 Impact of Majority Voting vs. Meta-Model Fusion (RQ2) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.
The Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_d</annotation></semantics></math> for energy consumption was 0.92, suggesting a large effect size, with majority voting requiring significantly less energy than the meta-model approach.
The minimum individual percentage difference starts from 22.02% in the (SVM, DT) ensemble and increases to a maximum of 51.11% in the (KNN, DT, NB) ensemble.
Averaging the differences for ensembles of the same size, we saw a 36.16% difference for ensembles of size 2, 33.16% for ensembles of size 3, and 29.06% for ensembles of size 4.
As the number of models increased in the ensembles, this difference slightly decreased.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F6.g1" src="extracted/5707683/pics/rq2_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy of meta-model fusion vs. majority voting fusion</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Majority voting also consistently performed better than meta-model in terms of accuracy, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F6" title="Figure 6 ‣ 4.2 Impact of Majority Voting vs. Meta-Model Fusion (RQ2) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">6</span></a>.
The best performing meta-model ensemble (SVM, NB) had an F1-score of 0.792 and the worst (KNN, DT) of 0.716, while for majority voting, the minimum was 0.793 (KNN, DT) and the maximum 0.828 (SVM, KNN, DT, NB).
These findings are further supported by a Mann-Whitney U Test, which returned a p-value of 1.33e-36, indicating a significant difference in accuracy.
To understand the effect size, we calculated Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_d</annotation></semantics></math> for the F1-score, which was 0.3764, indicating a small effect size.
The difference in accuracy ranged from 0.007 to 0.096.
This finding contradicts previous work <cite class="ltx_cite ltx_citemacro_cite">Bonissone et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib5" title="">2011</a>)</cite>, and we will discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S5" title="5 Discussion ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">5</span></a> why this might be the case.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of energy consumption and accuracy of fusion methods and partition methods sorted by Fusion (J)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td" id="S4.T2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Fusion (J)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Fusion F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.1.1">Ensemble</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.2.1">Meta</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.3.1">M Voting</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.4.1">Diff (%)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.5.1">Meta</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.6.1">Majority voting</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.7.1">Diff</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.3.3.1">(’KNN’, ’NB’)</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.2">225</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.3">110</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.4">51.11</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.5">0.750</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.6">0.808</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.3.3.7">0.058</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.4.1">(’KNN’, ’DT’, ’NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.2">255</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.3">136</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.4">46.67</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.5">0.721</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.6">0.815</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.4.7">0.094</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.5.1">(’KNN’, ’DT’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.2">227</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.3">123</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.4">45.81</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.5">0.716</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.6">0.793</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.5.5.7">0.077</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.6.1">(’DT’, ’NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.2">119</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.3">66</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.4">44.54</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.5">0.777</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.6">0.804</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.6.7">0.027</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.7.1">(’SVM’, ’KNN’, ’NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.2">1,166</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.3">810</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.4">30.53</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.5">0.737</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.6">0.813</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.7.7">0.076</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<td class="ltx_td ltx_align_left" id="S4.T2.1.8.8.1">(’SVM’, ’KNN’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.2">1,048</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.3">731</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.4">30.25</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.5">0.746</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.6">0.822</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.8.8.7">0.076</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.9.1">(’SVM’, ’KNN’, ’DT’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.2">1,167</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.3">825</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.4">29.31</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.5">0.733</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.6">0.814</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.9.7">0.081</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.10.1">(’SVM’, ’KNN’, ’DT’, ’NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.2">1,225</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.3">869</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.4">29.06</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.5">0.732</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.6">0.828</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.10.7">0.096</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<td class="ltx_td ltx_align_left" id="S4.T2.1.11.11.1">(’SVM’, ’DT’, ’NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.2">1,018</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.3">751</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.4">26.23</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.5">0.743</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.6">0.812</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.11.11.7">0.069</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.12">
<td class="ltx_td ltx_align_left" id="S4.T2.1.12.12.1">(’SVM’, ’ NB’)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.2">869</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.3">667</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.4">23.25</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.5">0.792</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.6">0.799</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.12.7">0.007 6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.13.13.1">(’SVM’, ’DT’)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.2">881</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.3">687</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.4">22.02</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.5">0.755</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.6">0.822</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.13.13.7">0.067</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<svg class="ltx_picture" height="89.67" id="S4.SS2.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 0 L 0 89.67 L 600 89.67 L 600 0 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 87.7 L 598.03 87.7 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS2.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS2.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.pic1.1.1.1.1.1.1.1">Answer to RQ2:</span></span>
<span class="ltx_p" id="S4.SS2.p4.pic1.1.1.1.1.1.2">Majority voting emerged as the more energy-efficient fusion method in comparison to meta-model.
It not only required on average 34.43% less energy but also increased accuracy by 0.066.
In our experiment, majority voting was always preferable to meta-model fusion.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Impact of Whole-Dataset vs. Subset-Based Training (RQ3)</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Regarding training set partition methods, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F7" title="Figure 7 ‣ 4.3 Impact of Whole-Dataset vs. Subset-Based Training (RQ3) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">7</span></a> reveals clear differences.
Training the ensemble models on the whole dataset consistently consumed more energy than training on subsets.
This was further supported by a Mann-Whitney U test with a p-value of 2.3e-17, which indicates a significant difference.
To understand the effect size, we calculated Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_d</annotation></semantics></math> and obtained 0.91, indicating a large effect.
Additionally, we calculated the percentage difference: for individual ensembles, the difference starts from 24.04% and increases to 64.35%.
On average, in ensembles of size 2, training the models on subsets saved 45.68% energy; in ensembles of size 3, subsets saved 45.94%; and in ensembles of size 4, training on subsets saved 44.78% energy.
For all ensemble sizes, training on subsets saved on average 45.7% energy.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary of energy consumption and accuracy of fusion methods and partition methods sorted by Partition (J)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Partition (J)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">Partition F1</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.1.1">Ensemble</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.2.1">Subset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.3.1">Whole</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.4.1">Diff (%)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.5.1">Subset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.6.1">Whole</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.7.1">Diff</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.1.3.1.1">(’SVM’, ’DT’)</th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.2">416</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.3">1,167</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.4">64.35</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.5">0.779</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.6">0.801</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.3.1.7">0.022</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.2.1">(’SVM’, ’ NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.2">417</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.3">1,120</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.4">62.77</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.5">0.788</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.6">0.804</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.4.2.7">0.016</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.3.1">(’SVM’, ’KNN’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.2">487</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.3">1,287</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.4">62.16</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.5">0.778</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.6">0.791</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.5.3.7">0.013</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.4.1">(’SVM’, ’DT’, ’NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.2">563</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.3">1,201</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.4">53.12</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.5">0.776</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.6">0.781</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.6.4.7">0.005</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.7.5.1">(’SVM’, ’KNN’, ’NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.2">642</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.3">1,334</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.4">51.87</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.5">0.775</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.6">0.775</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.7.5.7">0.000</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.8.6.1">(’SVM’, ’KNN’, ’DT’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.2">647</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.3">1,342</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.4">51.79</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.5">0.773</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.6">0.774</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.8.6.7">0.001</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.9.7.1">(’SVM’, ’KNN’, ’DT’, ’NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.2">746</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.3">1,351</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.4">44.78</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.5">0.783</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.6">0.778</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.9.7.7">-0.005</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.10.8.1">(’KNN’, ’DT’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.2">142</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.3">209</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.4">32.06</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.5">0.747</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.6">0.762</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.10.8.7">0.015</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.11.9.1">(’KNN’, ’NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.2">139</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.3">195</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.4">28.72</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.5">0.772</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.6">0.788</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.11.9.7">0.016</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.12.10.1">(’KNN’, ’DT’, ’NB’)</th>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.2">165</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.3">226</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.4">26.99</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.5">0.767</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.6">0.768</td>
<td class="ltx_td ltx_align_right" id="S4.T3.1.12.10.7">0.001</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.13.11.1">(’DT’, ’NB’)</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.2">79</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.3">104</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.4">24.04</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.5">0.781</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.6">0.802</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.1.13.11.7">0.021</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S4.F7.g1" src="extracted/5707683/pics/rq3_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Energy consumption of whole-dataset vs. subset-based training</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">When comparing the F1-scores, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F8" title="Figure 8 ‣ 4.3 Impact of Whole-Dataset vs. Subset-Based Training (RQ3) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">8</span></a> shows that training on the whole dataset either gave the same or higher accuracy compared to subsets.
The only exception was for ensembles of size 4, where subsets offered an average increase of 0.005 in accuracy compared to the whole dataset.
Even though this difference in accuracy is very small, the Mann-Whitney U test still led to a significant p-value of 0.2e-3 due to the large sample size.
However, the Cohen’s <math alttext="d" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_d</annotation></semantics></math> value of 0.02, shows that the effect size is close to negligible.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">To quantify the difference, we calculated the differences as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.T3" title="Table 3 ‣ 4.3 Impact of Whole-Dataset vs. Subset-Based Training (RQ3) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, and found that the difference in accuracy starts from 0 and increases to 0.022.
On average, training on the whole dataset offered a 0.0095 increase in accuracy compared to training the model on subsets.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F8.g1" src="extracted/5707683/pics/rq3_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Accuracy of whole-dataset vs. subset-based training</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<svg class="ltx_picture" height="74.6" id="S4.SS3.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,74.6) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 0 L 0 74.6 L 600 74.6 L 600 0 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 72.64 L 598.03 72.64 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS3.p4.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.SS3.p4.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.pic1.1.1.1.1.1.1.1">Answer to RQ3:</span></span>
<span class="ltx_p" id="S4.SS3.p4.pic1.1.1.1.1.1.2">Whole-dataset training consumed on average 45.7% more energy, but offered only a negligible (0.0095) increase in accuracy compared to subset-based training.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We summarize and visualize our results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S5.F9" title="Figure 9 ‣ 5 Discussion ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">9</span></a>, which categorizes our ensembles into four groups based on the four combinations of fusion and partition methods.
These groups also have clear indications for accuracy and energy consumption.
The size of the shapes is adjusted based on the size of the ensembles; the larger the ensemble size, the bigger the shape.</p>
</div>
<div class="ltx_para" id="S5.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Meta-Model, Whole-Dataset (triangle shape)</span>: Ensembles trained on the whole dataset and fused with a meta-model consume more energy and offer lower accuracy compared to other categories. They are mostly located in the top-left quadrant of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S5.F9" title="Figure 9 ‣ 5 Discussion ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Majority voting, Whole-Dataset (square shape)</span>: Ensembles trained on the whole dataset and fused using majority voting consume less energy than the triangle shape but more than the circle and diamond shapes. However, they offer the highest accuracy. These ensembles are mostly in the top-right quadrant.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Meta-Model, Subset (circle shape)</span>: Ensembles trained on subsets of the dataset and fused with a meta-model consume less energy than the triangle and square shapes but offer the lowest accuracy of all. They are mostly located in the bottom-left quadrant.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Majority voting, Subset (diamond shape)</span>: Ensembles trained on subsets and fused using majority voting consume the least energy and offer higher accuracy than the triangle and circle shapes but slightly less than the square shape. They are all located in the bottom-right quadrant.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S5.F9.g1" src="extracted/5707683/pics/discussion.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Ensemble trade-offs: accuracy vs. energy consumption</figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">From a Green AI perspective, the ensembles shown as diamond shapes (Majority voting, Subset) are the most preferable.
This category consumed the least energy and offered only a marginally lower accuracy than the (Majority voting, Whole) category, i.e., an average difference of less than 0.02, which is negligible in several cases.
We see that ensembles are clustered based on fusion and partitioning methods rather than ensemble size, indicating that these factors are more influential.
However, within these clusters, we still see the impact of size on energy consumption, i.e., ensembles with more models tend to consume more energy.
While size has a positive effect on the accuracy of ensembles only in categories 2 and 4, it is notable that in both categories, the fusion method is majority voting.
Interestingly, it has a more positive effect on accuracy in category 4 compared to category 2.
In category 4, the average ensemble of size 2 offers 0.802 accuracy, an ensemble of size 3 offers 0.813 accuracy, and an ensemble of size 4 offers 0.830 accuracy.
This shows that an ensemble of size 4 offers a 0.017 increase in accuracy compared to size 3 and a 0.028 increase in accuracy compared to size 2.
Additionally, size 3 offers a 0.011 increase in accuracy compared to an ensemble of size 2.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Some of the ensembles in category 2 with smaller sizes offer almost the same accuracy as the size 4 ensembles in category 4, even the two ensembles (SVM, DT) and (SVM, KNN) in category 2 with smaller sizes that offer equal (SVM, KNN) accuracy or very close (SVM, DT) to the ensemble of size 4 in category 4 consume more energy because the models in category 2 are trained on the whole dataset.
However, there are exceptions, as seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F3" title="Figure 3 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">3</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S4.F4" title="Figure 4 ‣ 4.1 Impact of Ensemble Size (RQ1) ‣ 4 Results ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">4</span></a>.
For example, a (KNN, DT, NB) ensemble consumes less energy than some size 2 ensembles, such as (SVM, NB), because the energy consumption depends on the individual models.
SVM consumes more energy than KNN and NB combined, illustrating the importance of model selection.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Regarding accuracy, ensembles of the same size offer different accuracy levels.
Analyzing the correlation between individual accuracy and its effect on the overall accuracy of the model, we observe that there is no direct correlation.
The average F1-score is 0.516 for Naive Bayes, 0.773 for the decision tree, 0.796 for k-nearest neighbors, and 0.800 for the support vector machine.
Naive Bayes and the decision tree offer less accuracy individually compared to the other two models, but the (NB, DT) ensemble surprisingly offers <span class="ltx_text ltx_font_italic" id="S5.p5.1.1">higher</span> accuracy than the (SVM, KNN) ensemble.
Deciding which models to include in the ensemble is another important issue that affects both the energy consumption and accuracy of the ensemble.
Different methods are proposed to select the right number of models in an ensemble, which can be roughly divided into clustering-based methods, ranking-based methods, and selection-based methods <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib61" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">As discussed in RQ2, meta-model fusion consumes more energy compared to majority majority voting, and the energy difference decreases as the number of models increases. On average, meta-model consume 36.16% more energy than majority voting for size 2 ensembles, 33.18% more energy for size 3, and 29.06% more energy for size 4. As the number of models increases in the ensemble, the percentage difference decreases.
This is because the energy consumed by the fusion process becomes a smaller part of the total energy consumption as the number of models increases.
Conversely, the absolute energy difference increases due to the greater number of features in the meta-model, which consumes more energy to train.
The choice of meta-model impacts the energy and accuracy of ensembles.
We selected K-nearest neighbors (KNN) as it is the most energy-efficient model among the four selected models and offers the second-highest accuracy with the selected datasets.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">As KNN is one of the most energy-efficient models among the available traditional ML models for classification tasks <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>)</cite>, we can generalize that the meta-model approach consumes more energy than the majority voting method.
However, the accuracy of a model is entirely dependent on the datasets it uses; there is no inherently good or bad model <cite class="ltx_cite ltx_citemacro_citep">(Sheth et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib48" title="">2022</a>)</cite>.
Even though meta-model fusion led to accuracy improvements in previous work <cite class="ltx_cite ltx_citemacro_cite">Bonissone et al. (<a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib5" title="">2011</a>)</cite>, this was not the case in our experiments, most likely because of the small number of models in the ensemble.
This limited the number of features the meta-model could use for predictions, and therefore rendered it ineffective.
Moreover, parameter tuning can also affect the meta-model’s efficiency, which was beyond the scope of our study.
Therefore, we can generalize that, with a limited number of models, the meta-model option could be referred to as a never-recommended option because of its higher energy consumption and lower accuracy compared to majority voting.</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">As seen in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#S5.F9" title="Figure 9 ‣ 5 Discussion ‣ The More the Merrier? Navigating Accuracy vs. Energy Efficiency Design Trade-Offs in Ensemble Learning Systems"><span class="ltx_text ltx_ref_tag">9</span></a>, there is an obvious difference between the energy consumption of ensembles trained on the whole dataset vs. a subset.
As training on a subset can have different effects on different models <cite class="ltx_cite ltx_citemacro_citep">(Verdecchia et al., <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib53" title="">2022</a>)</cite> and different models have different energy requirements, that is likely of the reasons why we saw different energy savings in different ensembles of the same size.
The method for subset selection is another factor that may also affect the energy consumption and accuracy of the ensemble.
We selected the subsets based on a cross-validation partition of the training data <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Ma, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib63" title="">2012</a>; Wolpert, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib58" title="">1992</a>)</cite>.
We divided the dataset into mutually exclusive subsets while ensuring that each training instance is used at least once in training an individual model.
There are other methods also used for training the ensemble, such as random sampling with replacement and random sampling without replacement <cite class="ltx_cite ltx_citemacro_citep">(Sagi and Rokach, <a class="ltx_ref" href="https://arxiv.org/html/2407.02914v1#bib.bib41" title="">2018</a>)</cite>.
Further investigation is needed to compare these different methods.</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1"><span class="ltx_text ltx_font_bold" id="S5.p9.1.1">Implications and Recommendations:</span>
The findings from this study emphasize that it is possible to design ensemble learning components that consume substantially less energy while simultaneously offering comparable or only slightly decreased accuracy.
Increasing the number of models and using whole-dataset training can lead to higher energy costs with only marginal accuracy improvements.
The majority voting fusion method emerged as a particularly effective approach, balancing energy consumption and accuracy.
For practical applications, it is recommended to use majority voting fusion and subset-based training methods to reduce energy consumption without sacrificing accuracy.
Additionally, limiting the ensemble size to what is necessary to achieve acceptable performance and consciously selecting suitable model types can further enhance energy efficiency.
As energy-efficient ML algorithms, we especially recommend experimenting with decision tree, Naive Bayes, and KNN, as ensembles with these algorithms required very little energy in our experiment, while still providing convincing accuracy.
Future work could explore this impact of different types of models within ensembles further, and investigate more sophisticated fusion methods that may offer even better energy efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p10">
<svg class="ltx_picture" height="102.62" id="S5.p10.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,102.62) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 0 L 0 102.62 L 600 102.62 L 600 0 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 100.66 L 598.03 100.66 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="75.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.p10.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S5.p10.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.p10.pic1.1.1.1.1.1.1.1">Summary:</span></span>
<span class="ltx_p" id="S5.p10.pic1.1.1.1.1.1.2">Increasing ensemble size and using whole-dataset training methods significantly increase energy consumption with negligible accuracy improvements. The majority voting fusion method and subset-based training are recommended for their balance of energy consumption and accuracy.</span>
</span></foreignobject></g></g></svg>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Threats to Validity</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">A potential threat to <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">internal validity</span>, associated with historical factors, may have emerged in our experiment due to the impact of executing successive iterations on our measurements, such as increasing hardware temperatures.
To mitigate this concern, we introduced a 5-second sleep operation before each experimental iteration, ensuring more uniform hardware conditions for all runs.
Additionally, a warm-up operation was performed to ensure that the initial iteration occurred under conditions similar to subsequent ones, reducing potential measurement influences.
Regarding the energy measures, the presence of background tasks during the experiment could have acted as confounding factors that affected energy measurements.
To address this, we terminated non-essential processes and restricted access to the infrastructure.
Furthermore, we conducted each experiment iteration 30 times to minimize the impact of any unforeseen background processes.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Concerning <span class="ltx_text ltx_font_bold" id="S6.p2.1.1">external validity</span>, i.e., the generalizability of our findings, we carefully selected tabular classification datasets with different characteristics to have some variety.
We also included four commonly used base classifiers to enhance the study’s diversity.
This deliberate selection of datasets and classifiers contributes to a more robust evaluation of the generalizability and effectiveness of the examined methods.
Nonetheless, our results cannot be easily transferred to, e.g., deep learning, regression, or larger number of models in the ensemble.
Additional research is required to address this.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Regarding <span class="ltx_text ltx_font_bold" id="S6.p3.1.1">reliability</span>, we have made a replication package available online to ensure the reproducibility of our study.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/doi/10.5281/zenodo.11664165" title="">https://zenodo.org/doi/10.5281/zenodo.11664165</a></span></span></span>
Additionally, conducting the experiments on different hardware yielded consistent results, reinforcing the reliability of our findings and ensuring the robustness of the outcomes.
The exclusive use of CodeCarbon to estimate energy consumption could threaten the construct validity of our experiment.
To mitigate this risk, we took two steps.
First, we leveraged its open-source nature to examine its implementation and verify its use of RAPL on Linux for gathering energy data.
Second, we conducted the experiment on two different systems to confirm consistent results.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our controlled experiment provided a comprehensive analysis of the impact of several design decisions on energy consumption and accuracy in ensemble learning.
Increasing the number of models in an ensemble led to significantly higher energy consumption, with no substantial gains in accuracy.
This highlights the inefficiency of enlarging ensembles indiscriminately.
Comparative analysis of fusion methods revealed that the majority voting approach is notably less energy-hungry than the meta-model, while also providing slightly higher accuracy.
Further, the comparison of training methods showed that while whole-dataset training consumes considerably more energy, it does not offer meaningful improvements in accuracy over subset-based training.
This underscores the potential for significant energy savings through data-centric approaches.
Overall, these findings emphasize the need for careful consideration of energy efficiency in the design and implementation of ensemble learning systems.
From a Green AI perspective, we recommend building ensembles of small size that use majority voting for fusion, subset-based training, and rely on energy-efficient ML algorithms like decision trees, Naive Bayes, or KNN.
However, future research is required to investigate other types of ensembles and forms of machine learning, such as deep learning, to continue exploring innovative ways to optimize energy efficiency of ML-enabled systems.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgement</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This work is partially supported by the PON scholarship of Italian government.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alswaitti et al. (2024)</span>
<span class="ltx_bibblock">
Alswaitti, M., Verdecchia, R., Danoy, G., Bouvry, P., Pecero, J., 2024.

</span>
<span class="ltx_bibblock">Training green ai models using elite samples.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2402.12010 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al. (2020)</span>
<span class="ltx_bibblock">
Anwar, H., Demirer, B., Pfahl, D., Srirama, S., 2020.

</span>
<span class="ltx_bibblock">Should energy consumption influence the choice of android third-party http libraries?, in: Proceedings of the IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems, pp. 87–97.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apel et al. (2022)</span>
<span class="ltx_bibblock">
Apel, S., Kastner, C., Kang, E., 2022.

</span>
<span class="ltx_bibblock">Feature Interactions on Steroids: On the Composition of ML Models.

</span>
<span class="ltx_bibblock">IEEE Software 39, 120–124.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/9758568/" title="">https://ieeexplore.ieee.org/document/9758568/</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/MS.2021.3134386" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/MS.2021.3134386</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al. (2021)</span>
<span class="ltx_bibblock">
Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?, in: Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610–623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonissone et al. (2011)</span>
<span class="ltx_bibblock">
Bonissone, P.P., Xue, F., Subbu, R., 2011.

</span>
<span class="ltx_bibblock">Fast meta-models for local fusion of multiple predictive models.

</span>
<span class="ltx_bibblock">Applied Soft Computing 11, 1529–1539.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1568494608000410" title="">https://www.sciencedirect.com/science/article/pii/S1568494608000410</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/https://doi.org/10.1016/j.asoc.2008.03.006" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.asoc.2008.03.006</span></a>. the Impact of Soft Computing for the Progress of Artificial Intelligence.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borah et al. (2020)</span>
<span class="ltx_bibblock">
Borah, P., Bhattacharyya, D., Kalita, J., 2020.

</span>
<span class="ltx_bibblock">Malware dataset generation and evaluation, in: 2020 IEEE 4th Conference on Information &amp; Communication Technology (CICT), IEEE. pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brownlee et al. (2021)</span>
<span class="ltx_bibblock">
Brownlee, A.E., Adair, J., Haraldsson, S.O., Jabbo, J., 2021.

</span>
<span class="ltx_bibblock">Exploring the Accuracy – Energy Trade-off in Machine Learning, in: 2021 IEEE/ACM International Workshop on Genetic Improvement (GI), IEEE, Madrid, Spain. pp. 11–18.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/GI52543.2021.00011" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/GI52543.2021.00011</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calero et al. (2021)</span>
<span class="ltx_bibblock">
Calero, C., Polo, M., Moraga, M.Á., 2021.

</span>
<span class="ltx_bibblock">Investigating the impact on execution time and energy consumption of developing with spring.

</span>
<span class="ltx_bibblock">Sustainable Computing: Informatics and Systems 32, 100603.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen (1988)</span>
<span class="ltx_bibblock">
Cohen, J., 1988.

</span>
<span class="ltx_bibblock">Statistical Power Analysis for the Behavioral Sciences.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.4324/9780203771587" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.4324/9780203771587</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cordeiro et al. (2023)</span>
<span class="ltx_bibblock">
Cordeiro, P.R., Cavalcanti, G.D., Cruz, R.M., 2023.

</span>
<span class="ltx_bibblock">A post-selection algorithm for improving dynamic ensemble selection methods, in: 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE. pp. 1142–1147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cristianini and Shawe-Taylor (2000)</span>
<span class="ltx_bibblock">
Cristianini, N., Shawe-Taylor, J., 2000.

</span>
<span class="ltx_bibblock">An introduction to support vector machines and other kernel-based learning methods.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cruz and Abreu (2019)</span>
<span class="ltx_bibblock">
Cruz, L., Abreu, R., 2019.

</span>
<span class="ltx_bibblock">Emaas: Energy measurements as a service for mobile applications, in: 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), IEEE. pp. 101–104.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Del Rey et al. (2023)</span>
<span class="ltx_bibblock">
Del Rey, S., Martínez-Fernández, S., Cruz, L., Franch, X., 2023.

</span>
<span class="ltx_bibblock">Do DL models and training environments have an impact on energy consumption?, in: 2023 49th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), IEEE, Durres, Albania. pp. 150–158.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/SEAA60479.2023.00031" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/SEAA60479.2023.00031</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dietterich (2000)</span>
<span class="ltx_bibblock">
Dietterich, T.G., 2000.

</span>
<span class="ltx_bibblock">Ensemble methods in machine learning, in: International workshop on multiple classifier systems, Springer. pp. 1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia-Martin et al. (2017)</span>
<span class="ltx_bibblock">
Garcia-Martin, E., Lavesson, N., Grahn, H., 2017.

</span>
<span class="ltx_bibblock">Identification of energy hotspots: A case study of the very fast decision tree, in: Green, Pervasive, and Cloud Computing: 12th International Conference, GPC 2017, Cetara, Italy, May 11-14, 2017, Proceedings 12, Springer. pp. 267–281.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Georgiou et al. (2018)</span>
<span class="ltx_bibblock">
Georgiou, S., Kechagia, M., Louridas, P., Spinellis, D., 2018.

</span>
<span class="ltx_bibblock">What are your programming language’s energy-delay implications?, in: Proceedings of the 15th International Conference on Mining Software Repositories, pp. 303–313.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Georgiou et al. (2022)</span>
<span class="ltx_bibblock">
Georgiou, S., Kechagia, M., Sharma, T., Sarro, F., Zou, Y., 2022.

</span>
<span class="ltx_bibblock">Green ai: do deep learning frameworks have different costs?, in: Proceedings of the 44th International Conference on Software Engineering, Association for Computing Machinery, New York, NY, USA. p. 1082–1094.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3510003.3510221" title="">https://doi.org/10.1145/3510003.3510221</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3510003.3510221" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3510003.3510221</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gungor et al. (2021)</span>
<span class="ltx_bibblock">
Gungor, O., Rosing, T., Aksanli, B., 2021.

</span>
<span class="ltx_bibblock">Enfes: Ensemble few-shot learning for intelligent fault diagnosis with limited data, in: 2021 IEEE Sensors, IEEE. pp. 1–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heyn et al. (2023)</span>
<span class="ltx_bibblock">
Heyn, H.M., Knauss, E., Pelliccione, P., 2023.

</span>
<span class="ltx_bibblock">A compositional approach to creating architecture frameworks with an application to distributed AI systems.

</span>
<span class="ltx_bibblock">Journal of Systems and Software 198, 111604.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://linkinghub.elsevier.com/retrieve/pii/S0164121222002801" title="">https://linkinghub.elsevier.com/retrieve/pii/S0164121222002801</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1016/j.jss.2022.111604" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1016/j.jss.2022.111604</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Injadat et al. (2021)</span>
<span class="ltx_bibblock">
Injadat, M., Moubayed, A., Nassif, A.B., Shami, A., 2021.

</span>
<span class="ltx_bibblock">Machine learning towards intelligent systems: applications, challenges, and opportunities.

</span>
<span class="ltx_bibblock">Artificial Intelligence Review 54, 3299–3348.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Janosi et al. (1988)</span>
<span class="ltx_bibblock">
Janosi, A., Steinbrunn, W., Pfisterer, M., Detrano, R., 1988.

</span>
<span class="ltx_bibblock">Heart Disease.

</span>
<span class="ltx_bibblock">UCI Machine Learning Repository.

</span>
<span class="ltx_bibblock">DOI: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.24432/C52P4X" title="">https://doi.org/10.24432/C52P4X</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Järvenpää et al. (2024)</span>
<span class="ltx_bibblock">
Järvenpää, H., Lago, P., Bogner, J., Lewis, G., Muccini, H., Ozkaya, I., 2024.

</span>
<span class="ltx_bibblock">A synthesis of green architectural tactics for ml-enabled systems, in: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society, pp. 130–141.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaack et al. (2022)</span>
<span class="ltx_bibblock">
Kaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D., 2022.

</span>
<span class="ltx_bibblock">Aligning Artificial Intelligence with Climate Change Mitigation.

</span>
<span class="ltx_bibblock">Nature Climate Change 12, 518–527.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kokkinos and Margaritis (2014)</span>
<span class="ltx_bibblock">
Kokkinos, Y., Margaritis, K.G., 2014.

</span>
<span class="ltx_bibblock">Breaking ties of plurality voting in ensembles of distributed neural network classifiers using soft max accumulations, in: Artificial Intelligence Applications and Innovations: 10th IFIP WG 12.5 International Conference, AIAI 2014, Rhodes, Greece, September 19-21, 2014. Proceedings 10, Springer. pp. 20–28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotary et al. (2022)</span>
<span class="ltx_bibblock">
Kotary, J., Di Vito, V., Fioretto, F., 2022.

</span>
<span class="ltx_bibblock">Differentiable model selection for ensemble learning.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2211.00251 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotsianti and Kanellopoulos (2007)</span>
<span class="ltx_bibblock">
Kotsianti, S., Kanellopoulos, D., 2007.

</span>
<span class="ltx_bibblock">Combining bagging, boosting and dagging for classification problems, in: Knowledge-Based Intelligent Information and Engineering Systems: 11th International Conference, KES 2007, XVII Italian Workshop on Neural Networks, Vietri sul Mare, Italy, September 12-14, 2007. Proceedings, Part II 11, Springer. pp. 493–500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotsiantis et al. (2007)</span>
<span class="ltx_bibblock">
Kotsiantis, S.B., Zaharakis, I., Pintelas, P., et al., 2007.

</span>
<span class="ltx_bibblock">Supervised machine learning: A review of classification techniques.

</span>
<span class="ltx_bibblock">Emerging artificial intelligence applications in computer engineering 160, 3–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kunapuli (2023)</span>
<span class="ltx_bibblock">
Kunapuli, G., 2023.

</span>
<span class="ltx_bibblock">Ensemble Methods for Machine Learning.

</span>
<span class="ltx_bibblock">Simon and Schuster.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Li, Z., Ren, K., Yang, Y., Jiang, X., Yang, Y., Li, D., 2023.

</span>
<span class="ltx_bibblock">Towards inference efficient deep ensemble learning, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8711–8719.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mann and Whitney (1947)</span>
<span class="ltx_bibblock">
Mann, H., Whitney, D., 1947.

</span>
<span class="ltx_bibblock">On a test of whether one of two random variables is stochastically larger than the other.

</span>
<span class="ltx_bibblock">Annals of Mathematical Statistics, Vol. 18, No. 1 , 50–60doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1214/aoms/1177730491" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1214/aoms/1177730491</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra et al. (2019)</span>
<span class="ltx_bibblock">
Misra, S., Li, H., He, J., 2019.

</span>
<span class="ltx_bibblock">Machine learning for subsurface characterization.

</span>
<span class="ltx_bibblock">Gulf Professional Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Myers and Well (2002)</span>
<span class="ltx_bibblock">
Myers, J.L., Well, A.D., 2002.

</span>
<span class="ltx_bibblock">Research Design &amp; Statistical Analysis.

</span>
<span class="ltx_bibblock">Lawrence Erlbaum Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nijkamp et al. (2024)</span>
<span class="ltx_bibblock">
Nijkamp, N., Sallou, J., van der Heijden, N., Cruz, L., 2024.

</span>
<span class="ltx_bibblock">Green ai in action: Strategic model selection for ensembles in production.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2405.17451 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oliveira et al. (2019)</span>
<span class="ltx_bibblock">
Oliveira, W., Oliveira, R., Castor, F., Fernandes, B., Pinto, G., 2019.

</span>
<span class="ltx_bibblock">Recommending energy-efficient java collections, in: 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), IEEE. pp. 160–170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Omar et al. (2024)</span>
<span class="ltx_bibblock">
Omar, R., Bogner, J., Leest, J., Stoico, V., Lago, P., Muccini, H., 2024.

</span>
<span class="ltx_bibblock">How to sustainably monitor ml-enabled systems? accuracy and energy efficiency tradeoffs in concept drift detection.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2404.19452 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parnas (1979)</span>
<span class="ltx_bibblock">
Parnas, D., 1979.

</span>
<span class="ltx_bibblock">Designing Software for Ease of Extension and Contraction.

</span>
<span class="ltx_bibblock">IEEE Transactions on Software Engineering SE-5, 128–138.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/1702607/" title="">http://ieeexplore.ieee.org/document/1702607/</a>, doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1109/TSE.1979.234169" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TSE.1979.234169</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pereira et al. (2021)</span>
<span class="ltx_bibblock">
Pereira, R., Couto, M., Ribeiro, F., Rua, R., Cunha, J., Fernandes, J.P., Saraiva, J., 2021.

</span>
<span class="ltx_bibblock">Ranking programming languages by energy efficiency.

</span>
<span class="ltx_bibblock">Science of Computer Programming 205, 102609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poenaru-Olaru et al. (2023)</span>
<span class="ltx_bibblock">
Poenaru-Olaru, L., Sallou, J., Cruz, L., Rellermeyer, J.S., Van Deursen, A., 2023.

</span>
<span class="ltx_bibblock">Retrain ai systems responsibly! use sustainable concept drift adaptation techniques, in: 2023 IEEE/ACM 7th International Workshop on Green And Sustainable Software (GREENS), IEEE. pp. 17–18.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prusa et al. (2015)</span>
<span class="ltx_bibblock">
Prusa, J., Khoshgoftaar, T.M., Seliya, N., 2015.

</span>
<span class="ltx_bibblock">The effect of dataset size on training tweet sentiment classifiers, in: 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA), IEEE. pp. 96–102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al. (2021)</span>
<span class="ltx_bibblock">
Ribeiro, A., Ferreira, J.F., Mendes, A., 2021.

</span>
<span class="ltx_bibblock">Ecoandroid: An android studio plugin for developing energy-efficient java mobile applications, in: 2021 IEEE 21st international conference on software quality, reliability and security (QRS), IEEE. pp. 62–69.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sagi and Rokach (2018)</span>
<span class="ltx_bibblock">
Sagi, O., Rokach, L., 2018.

</span>
<span class="ltx_bibblock">Ensemble learning: A survey.

</span>
<span class="ltx_bibblock">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8, e1249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salehi and Schmeink (2023)</span>
<span class="ltx_bibblock">
Salehi, S., Schmeink, A., 2023.

</span>
<span class="ltx_bibblock">Data-centric green artificial intelligence: A survey.

</span>
<span class="ltx_bibblock">IEEE Transactions on Artificial Intelligence .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santos and Papa (2022)</span>
<span class="ltx_bibblock">
Santos, C.F.G.D., Papa, J.P., 2022.

</span>
<span class="ltx_bibblock">Avoiding overfitting: A survey on regularization methods for convolutional neural networks.

</span>
<span class="ltx_bibblock">ACM Computing Surveys (CSUR) 54, 1–25.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarkar and Natarajan (2019)</span>
<span class="ltx_bibblock">
Sarkar, D., Natarajan, V., 2019.

</span>
<span class="ltx_bibblock">Ensemble Machine Learning Cookbook: Over 35 practical recipes to explore ensemble machine learning techniques using Python.

</span>
<span class="ltx_bibblock">Packt Publishing Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz et al. (2020)</span>
<span class="ltx_bibblock">
Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O., 2020.

</span>
<span class="ltx_bibblock">Green AI.

</span>
<span class="ltx_bibblock">Communications of the ACM 63, 54–63.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3381831" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3381831</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shapiro and Wilk (1965)</span>
<span class="ltx_bibblock">
Shapiro, S.S., Wilk, M.B., 1965.

</span>
<span class="ltx_bibblock">An Analysis of Variance Test for Normality (Complete Samples).

</span>
<span class="ltx_bibblock">Biometrika 52, 591.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.2307/2333709" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.2307/2333709</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharmila and Nagapadma (2023)</span>
<span class="ltx_bibblock">
Sharmila, B., Nagapadma, R., 2023.

</span>
<span class="ltx_bibblock">Quantized autoencoder (qae) intrusion detection system for anomaly detection in resource-constrained iot devices using rt-iot2022 dataset.

</span>
<span class="ltx_bibblock">Cybersecurity 6, 41.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheth et al. (2022)</span>
<span class="ltx_bibblock">
Sheth, V., Tripathi, U., Sharma, A., 2022.

</span>
<span class="ltx_bibblock">A comparative analysis of machine learning algorithms for classification purpose.

</span>
<span class="ltx_bibblock">Procedia Computer Science 215, 422–431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et al. (2019)</span>
<span class="ltx_bibblock">
Strubell, E., Ganesh, A., McCallum, A., 2019.

</span>
<span class="ltx_bibblock">Energy and Policy Considerations for Deep Learning in NLP, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Florence, Italy. pp. 3645–3650.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.18653/v1/P19-1355" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.18653/v1/P19-1355</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suthaharan (2016)</span>
<span class="ltx_bibblock">
Suthaharan, S., 2016.

</span>
<span class="ltx_bibblock">Machine learning models and algorithms for big data classification.

</span>
<span class="ltx_bibblock">Integr. Ser. Inf. Syst 36, 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ting and Witten (1997)</span>
<span class="ltx_bibblock">
Ting, K.M., Witten, I.H., 1997.

</span>
<span class="ltx_bibblock">Stacking bagged and dagged models .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Wynsberghe (2021)</span>
<span class="ltx_bibblock">
Van Wynsberghe, A., 2021.

</span>
<span class="ltx_bibblock">Sustainable ai: Ai for sustainability and the sustainability of ai.

</span>
<span class="ltx_bibblock">AI and Ethics 1, 213–218.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verdecchia et al. (2022)</span>
<span class="ltx_bibblock">
Verdecchia, R., Cruz, L., Sallou, J., Lin, M., Wickenden, J., Hotellier, E., 2022.

</span>
<span class="ltx_bibblock">Data-centric green ai an exploratory empirical study, in: 2022 international conference on ICT for sustainability (ICT4S), IEEE. pp. 35–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verdecchia et al. (2017)</span>
<span class="ltx_bibblock">
Verdecchia, R., Procaccianti, G., Malavolta, I., Lago, P., Koedijk, J., 2017.

</span>
<span class="ltx_bibblock">Estimating energy impact of software releases and deployment strategies: The kpmg case study, in: 2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), IEEE. pp. 257–266.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verdecchia et al. (2023)</span>
<span class="ltx_bibblock">
Verdecchia, R., Sallou, J., Cruz, L., 2023.

</span>
<span class="ltx_bibblock">A systematic review of Green AI.

</span>
<span class="ltx_bibblock">WIREs Data Mining and Knowledge Discovery 13, e1507.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1002/widm.1507" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1002/widm.1507</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Wei, X., Gonugondla, S.K., Wang, S., Ahmad, W., Ray, B., Qian, H., Li, X., Kumar, V., Wang, Z., Tian, Y., Sun, Q., Athiwaratkun, B., Shang, M., Ramanathan, M.K., Bhatia, P., Xiang, B., 2023.

</span>
<span class="ltx_bibblock">Towards Greener Yet Powerful Code Generation via Quantization: An Empirical Study, in: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ACM, San Francisco CA USA. pp. 224–236.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1145/3611643.3616302" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1145/3611643.3616302</span></a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wohlin et al. (2012)</span>
<span class="ltx_bibblock">
Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., 2012.

</span>
<span class="ltx_bibblock">Experimentation in Software Engineering. volume 9783642290.

</span>
<span class="ltx_bibblock">Springer Berlin Heidelberg, Berlin, Heidelberg.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="http://dx.doi.org/10.1007/978-3-642-29044-2" title=""><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/978-3-642-29044-2</span></a>. publication Title: Experimentation in Software Engineering.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolpert (1992)</span>
<span class="ltx_bibblock">
Wolpert, D.H., 1992.

</span>
<span class="ltx_bibblock">Stacked generalization.

</span>
<span class="ltx_bibblock">Neural networks 5, 241–259.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2022)</span>
<span class="ltx_bibblock">
Wu, C.J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K., Chang, G., Aga, F., Huang, J., Bai, C., et al., 2022.

</span>
<span class="ltx_bibblock">Sustainable ai: Environmental implications, challenges and opportunities.

</span>
<span class="ltx_bibblock">Proceedings of Machine Learning and Systems 4, 795–813.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Xu, Y., Martínez-Fernández, S., Martinez, M., Franch, X., 2023.

</span>
<span class="ltx_bibblock">Energy Efficiency of Training Neural Network Architectures: An Empirical Study, in: Proceedings of the 56th Hawaii International Conference on System Sciences.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Yang, Y., Lv, H., Chen, N., 2023.

</span>
<span class="ltx_bibblock">A survey on ensemble learning under the era of deep learning.

</span>
<span class="ltx_bibblock">Artificial Intelligence Review 56, 5545–5589.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeh (2016)</span>
<span class="ltx_bibblock">
Yeh, I.C., 2016.

</span>
<span class="ltx_bibblock">Default of Credit Card Clients.

</span>
<span class="ltx_bibblock">UCI Machine Learning Repository.

</span>
<span class="ltx_bibblock">DOI: https://doi.org/10.24432/C55S3H.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Ma (2012)</span>
<span class="ltx_bibblock">
Zhang, C., Ma, Y., 2012.

</span>
<span class="ltx_bibblock">Ensemble machine learning: methods and applications.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2017)</span>
<span class="ltx_bibblock">
Zhang, S., Li, X., Zong, M., Zhu, X., Cheng, D., 2017.

</span>
<span class="ltx_bibblock">Learning k for knn classification.

</span>
<span class="ltx_bibblock">ACM Transactions on Intelligent Systems and Technology (TIST) 8, 1–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2002)</span>
<span class="ltx_bibblock">
Zhou, Z.H., Wu, J., Tang, W., 2002.

</span>
<span class="ltx_bibblock">Ensembling neural networks: many could be better than all.

</span>
<span class="ltx_bibblock">Artificial intelligence 137, 239–263.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zogaj et al. (2021)</span>
<span class="ltx_bibblock">
Zogaj, F., Cambronero, J.P., Rinard, M.C., Cito, J., 2021.

</span>
<span class="ltx_bibblock">Doing more with less: characterizing dataset downsampling for automl.

</span>
<span class="ltx_bibblock">Proceedings of the VLDB Endowment 14, 2059–2072.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul  3 08:27:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
