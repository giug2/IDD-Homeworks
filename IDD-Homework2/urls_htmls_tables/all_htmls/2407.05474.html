<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.05474] Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses</title><meta property="og:description" content="Detecting hallucinations in large language model (LLM) outputs is pivotal, yet traditional fine-tuning for this classification task is impeded by the expensive and quickly outdated annotation process, especially across…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.05474">

<!--Generated on Mon Aug  5 16:10:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dongxu Zhang, Varun Gangal, Barrett Martin Lattimer, Yi Yang
<br class="ltx_break">ASAPP, Inc.
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{dzhang,vgangal,blattimer,yyang}@asapp.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Detecting hallucinations in large language model (LLM) outputs is pivotal, yet traditional fine-tuning for this classification task is impeded by the expensive and quickly outdated annotation process, especially across numerous vertical domains and in the face of rapid LLM advancements. In this study, we introduce an approach that automatically generates both faithful and hallucinated outputs by rewriting system responses. Experimental findings demonstrate that a T5-base model, fine-tuned on our generated dataset, surpasses state-of-the-art zero-shot detectors and existing synthetic generation methods in both accuracy and latency, indicating efficacy of our approach.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span id="p1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Dongxu Zhang, Varun Gangal, Barrett Martin Lattimer, Yi Yang</span></span></span>
<span id="p1.1.2.1.1.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.1" class="ltx_td ltx_align_center">ASAPP, Inc.</span></span>
<span id="p1.1.2.1.1.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.3.1.1" class="ltx_text ltx_font_typewriter">{dzhang,vgangal,blattimer,yyang}@asapp.com</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large Language Models (LLMs) tend to produce hallucinations, wherein the generated text either contradicts the given source knowledge (intrinsic hallucination) or cannot be verified against it (extrinsic hallucination) <cite class="ltx_cite ltx_citemacro_cite">Maynez et al. (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>); Rawte et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. Despite the burgeoning enthusiasm for deploying Generative AI and LLMs in real-world applications, the issue of hallucinations poses significant concerns for downstream users. Consequently, the detection of hallucinations is paramount in enhancing the safety of LLM applications and in fostering trust among users of these technologies.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An effective hallucination detection system should be accurate, fast, and affordable. Cost-effectiveness is crucial because every check for hallucinations adds extra cost to the use of large language models (LLMs), which may already be substantially high. Moreover, the system must possess the flexibility to adapt to the rapidly evolving landscape of LLMs. As shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, newer iterations of LLMs generally exhibit enhanced capabilities in mitigating hallucinations, thereby escalating the complexity of the detection challenge. Unfortunately, many current methodologies are either i) costly in terms of compute <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Manakul et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>
or ii) depend on out-of-domain/external resources such as QA <cite class="ltx_cite ltx_citemacro_cite">Honovich et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Fabbri et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> or NLI annotation  <cite class="ltx_cite ltx_citemacro_cite">Laban et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); Honovich et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, potentially compromising performance.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance evaluation of a GPT-3.5-based zero-shot hallucination detector across different generations of LLMs (see Appendix §E for prompt). This table illustrates a notable decline in detection efficacy when transitioning from older to more recent LLM iterations.</figcaption>
<table id="S1.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.4.5" class="ltx_tr">
<td id="S1.T1.4.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Hallucination data</td>
<td id="S1.T1.4.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">LLMs used in the data</td>
<td id="S1.T1.4.5.3" class="ltx_td ltx_align_right ltx_border_tt">F1</td>
</tr>
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">MNBM (’20)</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">GPT, Bert, Rnn, ConvNet</td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="0.780" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mn id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">0.780</mn><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><cn type="float" id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1">0.780</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">0.780</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_r">FRANK (’21)</td>
<td id="S1.T1.2.2.3" class="ltx_td ltx_align_right ltx_border_r">PointerNet, bertS2S, Bart</td>
<td id="S1.T1.2.2.1" class="ltx_td ltx_align_right"><math id="S1.T1.2.2.1.m1.1" class="ltx_Math" alttext="0.694" display="inline"><semantics id="S1.T1.2.2.1.m1.1a"><mn id="S1.T1.2.2.1.m1.1.1" xref="S1.T1.2.2.1.m1.1.1.cmml">0.694</mn><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.1.m1.1b"><cn type="float" id="S1.T1.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.1.m1.1.1">0.694</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.1.m1.1c">0.694</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_left ltx_border_r">Seahorse (early ’23)</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_align_right ltx_border_r">T5, MT5, PALM</td>
<td id="S1.T1.3.3.1" class="ltx_td ltx_align_right"><math id="S1.T1.3.3.1.m1.1" class="ltx_Math" alttext="0.576" display="inline"><semantics id="S1.T1.3.3.1.m1.1a"><mn id="S1.T1.3.3.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.cmml">0.576</mn><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.1.m1.1b"><cn type="float" id="S1.T1.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1">0.576</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.1.m1.1c">0.576</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.4.4" class="ltx_tr">
<td id="S1.T1.4.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">ScreenEval (late ’23)</td>
<td id="S1.T1.4.4.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">GPT-4, longformer</td>
<td id="S1.T1.4.4.1" class="ltx_td ltx_align_right ltx_border_bb"><math id="S1.T1.4.4.1.m1.1" class="ltx_Math" alttext="0.130" display="inline"><semantics id="S1.T1.4.4.1.m1.1a"><mn id="S1.T1.4.4.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.cmml">0.130</mn><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.1.m1.1b"><cn type="float" id="S1.T1.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1">0.130</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.1.m1.1c">0.130</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this study, we introduce a simple yet effective approach for automatically generating synthetic annotations to train hallucination detectors. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of our approach. The core of our method involves prompting a rewriting LLM to transform a given system response from the target LLM into both faithful and hallucinated versions, respectively. This technique distinguishes itself from existing methods <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>); Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>); Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>); Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite> in three significant ways. First, unlike traditional methods that rely on human-annotated examples of faithfulness, our strategy is entirely automated, eliminating need for manual annotation. Second, by directly altering responses from the target LLM, our trained detector aligns more closely with the response distribution of the target LLM, facilitating seamless adaptation to new LLMs. Lastly, while previous approaches require predefined information about the types of hallucinations for their generation process, our method operates without such assumptions. This allows for the creation of a broader spectrum of hallucination types, enhancing the coverage and diversity of generated hallucinations.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.05474/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="212" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our automatic hallucination generation pipeline. Red and green highlights hallucinated and faithful claims.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our experimental evaluations span two hallucination detection datasets, OpenDialKG <cite class="ltx_cite ltx_citemacro_cite">Moon et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> and BEGIN <cite class="ltx_cite ltx_citemacro_cite">Dziri et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022b</a>)</cite>, where a T5-base model, fine-tuned with our novel data generation approach, significantly surpasses GPT-4 based methods in performance while achieving a tenfold increase in speed. Further analysis of the generated hallucinations uncovers previously unreported patterns, such as "adding attributes", expanding the discourse on hallucination beyond existing literature. Our code and data will be available at <a target="_blank" href="https://github.com/asappresearch/halugen" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/asappresearch/halugen</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we detail our methodology for generating synthetic hallucinations that closely mimic those encountered in real-world applications of Large Language Models (LLMs). Prior approaches to hallucination generation have primarily relied on rewriting human-authored texts <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>); Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite> or introducing perturbations to the knowledge source <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>); Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>); Zhang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. However, these methods often yield outputs that diverge significantly from those produced by LLM systems, leading to a substantial discrepancy between the synthetic hallucinations and the genuine hallucinations observed in practice. To address this gap, our approach involves prompting a rewriting LLM to perturb the responses of the LLM system itself, rather than those written by humans. This strategy draws inspiration from the “Minor perturbation” technique described by <cite class="ltx_cite ltx_citemacro_citet">Lucas et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, adapted to our context to ensure the synthetic hallucinations closely align with the expected data in real-world deployments.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To effectively train a hallucination detector, it is imperative to have access to both hallucinated and faithful responses. Unlike previous studies, where human-curated outputs served as the benchmark for faithful system outputs <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>); Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>); Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite>, the responses obtained directly from the target LLM system may contain a considerable proportion of non-faithful responses. To overcome this challenge, we employ the rewriting LLM to adjust the system’s responses in a manner that promotes the generation of faithful outputs. The specific prompts utilized for inducing both hallucination and faithfulness are presented in Appendix §<a href="#A1" title="Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. It is important to note that our process for generating hallucinations did not involve biasing the system with predefined categories of hallucination within the prompt, ensuring a more authentic and unbiased generation process.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>These prompts have been designed with versatility in mind, allowing for straightforward adaptation to other NLP tasks such as question answering and summarization. However, our current investigation is focused exclusively on knowledge-grounded dialogues.</span></span></span> For the rewriting LLM, we selected GPT-4 <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We use gpt-4-1106-preview for our experiments.</span></span></span> due to its robust capabilities in text rewriting <cite class="ltx_cite ltx_citemacro_cite">Madaan et al. (<a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite>. Leveraging a powerful rewriting LLM like GPT-4 enables the exploration of a wider array of hallucination categories, thereby enhancing the coverage of hallucinations that are likely to be encountered in real-world scenarios.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OpenDialKG</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">is a dialogue dataset that was adopted by HaluEval <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>, a recent benchmark for hallucination detection. OpenDialKG features human-generated dialogues exclusively with supporting knowledge sources from Freebase <cite class="ltx_cite ltx_citemacro_cite">Bollacker et al. (<a href="#bib.bib1" title="" class="ltx_ref">2008</a>)</cite>. In order to leverage the dataset for hallucination detection, we simulate a chatbot system by employing GPT-4 to generate responses grounded in both the provided knowledge and the preceding dialogue context. The specifics of the prompt template utilized for this simulation are detailed in Appendix <a href="#A2" title="Appendix B Prompt Template for Simulating Chatbot on OpenDialKG ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. To create a evaluation set on the generated responses, we employ Amazon Mechanical Turk annotators to evaluate whether the responses from the simulated chatbot system were fully supported by the dialogue history and the provided knowledge (for detailed annotation guidelines and interface, refer to Appendix D). Our collection (OpenDialKG-Eval) comprises 402 annotated responses. We designated responses with high-confidence labels as our test set and utilized the remainder for development purposes, resulting in 312 test responses and 90 for development. More details of OpenDialKG-Eval can be found in Appendix  <a href="#A5" title="Appendix E Statistics of OpenDialKG-Eval ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. In addition, we simulate another 2000 responses from OpenDialKG for synthetic generation purpose.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BEGIN</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">is a knowledge-grounded dialog dataset featuring 12k responses from four dialogue systems distributed over 3 document-scale knowledge domains – Wizard of Wikipedia <cite class="ltx_cite ltx_citemacro_cite">Dinan et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, TopicalChat <cite class="ltx_cite ltx_citemacro_cite">Gopalakrishnan et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> and DoG <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> — all with mean knowledge snippets longer than OpenDialKG.
In addition, there are three response categories in BEGIN: Fully attributable, Not fully attributable, Generic. Generic category refers to response that are vague and do not provide any new information. Therefore, in addition to faithful and hallucination generation, we also ask LLM to generate responses under "Generic" category. The detailed prompt can be found in Appendix <a href="#A1" title="Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> Table <a href="#A1.T9" title="Table 9 ‣ Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
Since BEGIN only released the Dev and Test split, we adopt 1,228 system responses from Dev for both synthetic generation and development while reporting results on Test split.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baselines</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot Detection</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We compare with SelfCheckGPT <cite class="ltx_cite ltx_citemacro_cite">Manakul et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023a</a>)</cite>, a consistency-based approach which samples system responses multiple times in temperature 1.0 and then leverage scores from NLI or QA to measure whether the target response is consistent with these samples.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">Another baseline is G-Eval <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, which prompts GPT-4 with an annotation-rubric style prompt describing target variable and furthermore draws multiple samples at a higher temperature; emulating diverse multi-annotation by humans.
Since both G-Eval and SelfCheckGPT can only output scores between 0 and 1 and BEGIN data has three output categories, we compare GPT-4 (Internal), our self-devised zero-shot detector, which prompts GPT-4 with an intuitive prompt to enable three-way outputs(Appendix§F) and does greedy decodes to generate a binary/ternary answer.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p3.1" class="ltx_p">The last zero-shot baseline we compare with is SCALE <cite class="ltx_cite ltx_citemacro_cite">Lattimer et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, NLI-based approach which first decomposes the supporting context into chunks, calculate NLI scores on the chunk level using FlanT5 <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, then use the maximum score as the final prediction of factual consistency.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Detection with End-to-end Finetuning</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We use T5-base, an encoder-decoder LM with 223M parameters, as the base model of the detector and fine-tune it on multiple synthetic datasets.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For more experimental details, please refer to Appendix <a href="#A8" title="Appendix H Experimental Details ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a>.</span></span></span>
We make our best efforts to conduct apple-to-apple comparison among different synthetic data.
On OpenDialKG-Eval benchmark, we compare with FADE <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite> and HaluEval <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>, where we adopted their existing synthetic hallucinations as negative and human written responses from OpenDialKG as positive data for training.
On BEGIN dataset, we compare with AugWOW <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> and BEGIN-Adv. <cite class="ltx_cite ltx_citemacro_cite">Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite>, both are synthetic generation baselines and their performances on BEGIN has been reported by  <cite class="ltx_cite ltx_citemacro_cite">Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite>. For more details of these synthetic data generation baselines, please refer to Section <a href="#S5" title="5 Related Work ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Results</h3>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Macro-F1 and latency of hallucination detection methods over OpenDialKG-Eval. </figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">F1</td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_right ltx_border_tt">Latency</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Zero-shot Detection</td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SelfCheckGPT (QA) <cite class="ltx_cite ltx_citemacro_cite">Manakul et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.536</td>
<td id="S3.T2.1.4.3" class="ltx_td ltx_align_right ltx_border_t">60.59 sec</td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_left ltx_border_r">SelfCheckGPT (NLI) <cite class="ltx_cite ltx_citemacro_cite">Manakul et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_right ltx_border_r">0.579</td>
<td id="S3.T2.1.5.3" class="ltx_td ltx_align_right">0.93 sec</td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_align_left ltx_border_r">G-Eval <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_align_right ltx_border_r">0.608</td>
<td id="S3.T2.1.6.3" class="ltx_td ltx_align_right">2.79 sec</td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_r">SCALE<sub id="S3.T2.1.1.1.1" class="ltx_sub"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">XL</span></sub> <cite class="ltx_cite ltx_citemacro_cite">Lattimer et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_right ltx_border_r">0.687</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_right">0.22 sec</td>
</tr>
<tr id="S3.T2.1.7" class="ltx_tr">
<td id="S3.T2.1.7.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3">T5-base Finetuned over Synthetic Data</td>
</tr>
<tr id="S3.T2.1.8" class="ltx_tr">
<td id="S3.T2.1.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FADE <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S3.T2.1.8.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.625</td>
<td id="S3.T2.1.8.3" class="ltx_td ltx_align_right ltx_border_t">0.20 sec</td>
</tr>
<tr id="S3.T2.1.9" class="ltx_tr">
<td id="S3.T2.1.9.1" class="ltx_td ltx_align_left ltx_border_r">HaluEval <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T2.1.9.2" class="ltx_td ltx_align_right ltx_border_r">0.702</td>
<td id="S3.T2.1.9.3" class="ltx_td ltx_align_right">0.20 sec</td>
</tr>
<tr id="S3.T2.1.10" class="ltx_tr">
<td id="S3.T2.1.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Our approach</td>
<td id="S3.T2.1.10.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">0.762</td>
<td id="S3.T2.1.10.3" class="ltx_td ltx_align_right ltx_border_bb">0.20 sec</td>
</tr>
</table>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.3 Results ‣ 3 Experiments ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance of hallucination detection and latency per response on OpenDialKG-Eval. Latencies are profiled over AWS g5.xlarge instances with no batching sae for G-Eval which requires OpenAI API access.
From the results, our approach not only out-performs T5 detectors finetuned over previous hallucination generation baselines, but more interestingly, it out-performs state-of-the-art zero-shot detection methods. Besides performance, finetuned models achieve significantly lower latency than all zero-shot baselines. We also show the results on BEGIN data. The results can be found in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Results ‣ 3 Experiments ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where similar observation can be found.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Macro-F1 and latency of hallucination detection over BEGIN test split with three-class classification.
</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">F1</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Latency</td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Zero-shot Detection</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GPT-4 (Internal)</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.323</td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_align_right ltx_border_t">1.13 sec</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2">T5-base Finetuned over Synthetic Data</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AugWow <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.378</td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_align_right ltx_border_t">0.20 sec</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_left ltx_border_r">BEGIN-Adv. <cite class="ltx_cite ltx_citemacro_cite">Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_right ltx_border_r">0.459</td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_align_right">0.20 sec</td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Our approach</td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">0.473</td>
<td id="S3.T3.1.7.3" class="ltx_td ltx_align_right ltx_border_bb">0.20 sec</td>
</tr>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Lastly, average cost per synthetic response generation is 0.008 USD on OpenDialKG and 0.006 USD on BEGIN, using <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">gpt-4-1106-preview</em>. In comparison, average cost of human annotation per example for OpenDialKG-Eval is 0.20 USD.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Ablation Study</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To analyze the significance of both hallucination and faithful response generation, we conduct an ablation study to replace one of the generation using system response. Results are shown in Table <a href="#S3.T4" title="Table 4 ‣ 3.4 Ablation Study ‣ 3 Experiments ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Results show that both categories of synthetic data are necessary to effectively fine-tune the detector.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of ablation study. “pos-F1” and “neg-F1” represents F1 performance over faithful and Hallucination labels separately.
</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Approach</td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">pos-F1</td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">neg-F1</td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">F1</td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Our approach</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.812</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.713</td>
<td id="S3.T4.1.2.4" class="ltx_td ltx_align_right ltx_border_t">0.762</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_r">w/o faithful generation</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_right ltx_border_r">0.747</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_right ltx_border_r">0.618</td>
<td id="S3.T4.1.3.4" class="ltx_td ltx_align_right">0.683</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">w/o hallucination generation</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">0.517</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">0.502</td>
<td id="S3.T4.1.4.4" class="ltx_td ltx_align_right ltx_border_bb">0.509</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Hallucination Pattern Analysis</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Hallucination Pattern Analysis</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Previous work usually predefined hallucination patterns such as replacing or swapping entities <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib3" title="" class="ltx_ref">2022a</a>); Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>. We randomly sample 144 hallucinations generated by our method over OpenDialKG dataset, and manually annotate these into a taxonomy of 6 distinct pattern-driven categories characterizing the pattern surfaced in the hallucination, further described in Appendix §C.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Hallucination patterns appeared in OpenDialKG-Eval and our synthetic generated data for finetuning.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Pattern name</td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">System</td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">HaluEval</td>
<td id="S4.T5.1.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">FADE</td>
<td id="S4.T5.1.2.5" class="ltx_td ltx_align_right ltx_border_tt">Ours #</td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Adding attribute to an entity</td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.540</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T5.1.3.3.1" class="ltx_text ltx_font_bold">0.435</span></td>
<td id="S4.T5.1.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.156</td>
<td id="S4.T5.1.3.5" class="ltx_td ltx_align_right ltx_border_t">0.530</td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_left ltx_border_r">Adding or updating relation</td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_right ltx_border_r">0.070</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_right ltx_border_r">0.150</td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_right ltx_border_r">0.099</td>
<td id="S4.T5.1.4.5" class="ltx_td ltx_align_right">0.220</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_align_left ltx_border_r">Addding new entities</td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_right ltx_border_r">0.050</td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T5.1.5.3.1" class="ltx_text ltx_font_bold">0.370</span></td>
<td id="S4.T5.1.5.4" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T5.1.5.4.1" class="ltx_text ltx_font_bold">0.675</span></td>
<td id="S4.T5.1.5.5" class="ltx_td ltx_align_right">0.160</td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_align_left ltx_border_r">Overclaim knowledge/affordance</td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_right ltx_border_r">0.027</td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_right ltx_border_r">0.011</td>
<td id="S4.T5.1.6.4" class="ltx_td ltx_align_right ltx_border_r">0.010</td>
<td id="S4.T5.1.6.5" class="ltx_td ltx_align_right">0.025</td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr">
<td id="S4.T5.1.7.1" class="ltx_td ltx_align_left ltx_border_r">Inference error beyond above</td>
<td id="S4.T5.1.7.2" class="ltx_td ltx_align_right ltx_border_r">0.004</td>
<td id="S4.T5.1.7.3" class="ltx_td ltx_align_right ltx_border_r">0.011</td>
<td id="S4.T5.1.7.4" class="ltx_td ltx_align_right ltx_border_r">0.018</td>
<td id="S4.T5.1.7.5" class="ltx_td ltx_align_right">0.010</td>
</tr>
<tr id="S4.T5.1.8" class="ltx_tr">
<td id="S4.T5.1.8.1" class="ltx_td ltx_align_left ltx_border_r">None of the above</td>
<td id="S4.T5.1.8.2" class="ltx_td ltx_align_right ltx_border_r">0.310</td>
<td id="S4.T5.1.8.3" class="ltx_td ltx_align_right ltx_border_r">0.016</td>
<td id="S4.T5.1.8.4" class="ltx_td ltx_align_right ltx_border_r">0.042</td>
<td id="S4.T5.1.8.5" class="ltx_td ltx_align_right">0.050</td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><math id="S4.T5.1.1.1.m1.2" class="ltx_Math" alttext="\text{KL}(\bullet,\text{System})" display="inline"><semantics id="S4.T5.1.1.1.m1.2a"><mrow id="S4.T5.1.1.1.m1.2.3" xref="S4.T5.1.1.1.m1.2.3.cmml"><mtext id="S4.T5.1.1.1.m1.2.3.2" xref="S4.T5.1.1.1.m1.2.3.2a.cmml">KL</mtext><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.m1.2.3.1" xref="S4.T5.1.1.1.m1.2.3.1.cmml">​</mo><mrow id="S4.T5.1.1.1.m1.2.3.3.2" xref="S4.T5.1.1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S4.T5.1.1.1.m1.2.3.3.2.1" xref="S4.T5.1.1.1.m1.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">∙</mo><mo id="S4.T5.1.1.1.m1.2.3.3.2.2" xref="S4.T5.1.1.1.m1.2.3.3.1.cmml">,</mo><mtext id="S4.T5.1.1.1.m1.2.2" xref="S4.T5.1.1.1.m1.2.2a.cmml">System</mtext><mo stretchy="false" id="S4.T5.1.1.1.m1.2.3.3.2.3" xref="S4.T5.1.1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.2b"><apply id="S4.T5.1.1.1.m1.2.3.cmml" xref="S4.T5.1.1.1.m1.2.3"><times id="S4.T5.1.1.1.m1.2.3.1.cmml" xref="S4.T5.1.1.1.m1.2.3.1"></times><ci id="S4.T5.1.1.1.m1.2.3.2a.cmml" xref="S4.T5.1.1.1.m1.2.3.2"><mtext id="S4.T5.1.1.1.m1.2.3.2.cmml" xref="S4.T5.1.1.1.m1.2.3.2">KL</mtext></ci><interval closure="open" id="S4.T5.1.1.1.m1.2.3.3.1.cmml" xref="S4.T5.1.1.1.m1.2.3.3.2"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">∙</ci><ci id="S4.T5.1.1.1.m1.2.2a.cmml" xref="S4.T5.1.1.1.m1.2.2"><mtext id="S4.T5.1.1.1.m1.2.2.cmml" xref="S4.T5.1.1.1.m1.2.2">System</mtext></ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.2c">\text{KL}(\bullet,\text{System})</annotation></semantics></math></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">-</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">0.671</td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">1.527</td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">0.340</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Pattern distributions are both listed in Table 
<a href="#S4.T5" title="Table 5 ‣ 4.1 Hallucination Pattern Analysis ‣ 4 Hallucination Pattern Analysis ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Hallucination Pattern Analysis ‣ 4 Hallucination Pattern Analysis ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. From the pattern distribution, it is interesting to see that our method has fewer hallucinations from entity replacing/swapping, the most dominant hallucination type is adding unverifiable attributes to an entity. This indicates that our methods generate responses which conform tighter to the real hallucination distribution in contrast to prior approaches. The KL Divergence between the categorical pattern distribution of our method and the system response based distribution is 0.3395, compared to the much greater 0.6706 (and 1.52) between the distribution of HaluEval (and FADE) vs the latter.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2407.05474/assets/Figures/newPlot755BoundariesFinal.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Spiderplot spider-web traces visualizing how the synthesized hallucinations from our approach (in <span id="S4.F2.11.1" class="ltx_text ltx_font_italic">green</span>) + two baselines (<span id="S4.F2.12.2" class="ltx_text ltx_font_smallcaps">HaluEval,Fade</span>, in <span id="S4.F2.13.3" class="ltx_text ltx_font_italic">red</span> and <span id="S4.F2.14.4" class="ltx_text ltx_font_italic">blue</span>) as well as the system response distribution (<span id="S4.F2.15.5" class="ltx_text ltx_font_smallcaps">System</span>,in <span id="S4.F2.16.6" class="ltx_text ltx_font_italic">purple</span>) distribute over the 6 qualitative categories as laid out in §<a href="#S4.SS1" title="4.1 Hallucination Pattern Analysis ‣ 4 Hallucination Pattern Analysis ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Both <span id="S4.F2.17.7" class="ltx_text ltx_font_smallcaps">HaluEval</span> (blue) and <span id="S4.F2.18.8" class="ltx_text ltx_font_smallcaps">FADE</span> (red) show a marked skew towards "Add new entity", while <span id="S4.F2.19.9" class="ltx_text ltx_font_smallcaps">Ours</span> (green) shows a closer alignment with the <span id="S4.F2.20.10" class="ltx_text ltx_font_smallcaps">System</span> (purple).</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quality Analysis of Synthetic Data Generation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To more closely evaluate the effectiveness of rewriting, we did human annotation over 100 randomly sampled system responses along with our synthetically generated responses based on these system responses. Table  <a href="#S4.T6" title="Table 6 ‣ 4.2 Quality Analysis of Synthetic Data Generation ‣ 4 Hallucination Pattern Analysis ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the portion of faithful data within each type of responses:</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Human analysis of faithfulness of our generated synthetic responses in comparison to system outputs.
</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">Faithfulness</td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">System output</td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_align_right ltx_border_t">41%</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Faithful generation</td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_align_right ltx_border_t">51%</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Hallucination generation</td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_align_right ltx_border_bb">5%</td>
</tr>
</table>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Upon reviewing the annotations, our method demonstrates a significant reduction in the generation of unfaithful responses (hallucinations) compared to system outputs. Our approach also yields a higher number of faithful responses compared to the baseline system outputs, aligning with our objectives. Specifically, out of 59 instances of hallucinations identified in system outputs, our faithful generator converts 11 into faithful responses, achieving a conversion rate of approximately 19%. Conversely, among the 41 faithful responses generated by the system, our method inadvertently transformed only one into a hallucination.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Research on generating synthetic annotations for hallucination detection has explored various strategies. Some approaches, like FADE <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022b</a>)</cite> and HaluEval <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023a</a>)</cite>, manipulate human-written texts by altering entities or applying predefined hallucination criteria, respectively. These methods
only focus on introducing hallucinations and ignore faithfulness augmentation, assuming human-generated content to be inherently accurate, which maybe untrue.
Other studies focus on modifying the knowledge source before response generation. AugWow <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> introduces hallucinations by using irrelevant or no evidence, while BEGIN-Adv <cite class="ltx_cite ltx_citemacro_cite">Dziri et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022a</a>)</cite> alters subjects, objects, named entities, or verbs in the source material, prompting a GPT2-based system <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> for response regeneration. These techniques, however, might lead to predictable hallucination patterns due to their reliance on predefined rules.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">More recently,
ICD <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> mitigates LLM hallucinations by finetuning a model on non-factual samples, aiming to down-weight factually weak predictions. Despite its novelty, the reliance on entity perturbation for generating non-factual samples could limit the coverage of detected hallucinations.
HaluEval-Wild <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite> aims to evaluate LLM hallucination in human-LLM interactions. Their approach first collects challenging user queries which can lead to hallucinated LLM responses. Faithful reference responses are generated using GPT4 with retrieval augmentation.
While the generated data is challenging, it is not obvious how to adapt the approach for customized tasks.
Interestingly,  <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023b</a>)</cite> observed that the effectiveness of the LLM-generated synthetic data in supporting model training is negatively correlated with the subjectivity of the target task.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we aim to address the prevalent challenge of training data for hallucination detection being either unavailable or expensive to curate. We hypothesize that this can be addressed via a framework that automatically synthesizes both hallucinated and faithful responses using a prompt-based method. Our experimental results on two datasets verify effectiveness of our approach and show it compares favourably against several baselines, including those using prompt-based synthesis.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, the quality of the synthetically generated data is partially determined by the capability of prompted LLM. However, this issue is not severe since our goal is to facilitate the fine-tuning process of the hallucination detection model rather than using the data for evaluation.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In addition, note that hallucinations generated in this work, may still be different from those exist unintentionally in system outputs. One promising future work is to explore unintentional hallucination/faithful output generation, such as evaluating output faithfulness via sampling, or perturbing the prompt so that it becomes more or less likely to induce hallucinations.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Lastly, since we are encouraging the LLM to generate hallucinations, there is a risk of introducing misinformation into the real world data, which is also a common issue for large language model generation in general. We encourage people to follow policies and strategies with regarding to data sourcing, fact checking, etc. in order to mitigate such issue.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bollacker et al. (2008)</span>
<span class="ltx_bibblock">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring human knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</em>, pages 1247–1250.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2022a)</span>
<span class="ltx_bibblock">
Souvik Das, Sougata Saha, and Rohini Srihari. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-emnlp.48" title="" class="ltx_ref ltx_href">Diving deep into modes of fact hallucinations in dialogue systems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 684–699, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2022b)</span>
<span class="ltx_bibblock">
Souvik Das, Sougata Saha, and Rohini K Srihari. 2022b.

</span>
<span class="ltx_bibblock">Diving deep into modes of fact hallucinations in dialogue systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 684–699.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et al. (2018)</span>
<span class="ltx_bibblock">
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018.

</span>
<span class="ltx_bibblock">Wizard of wikipedia: Knowledge-powered conversational agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dziri et al. (2022a)</span>
<span class="ltx_bibblock">
Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00506" title="" class="ltx_ref ltx_href">Evaluating attribution in dialogue systems: The BEGIN benchmark</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:1066–1083.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dziri et al. (2022b)</span>
<span class="ltx_bibblock">
Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022b.

</span>
<span class="ltx_bibblock">Evaluating attribution in dialogue systems: The begin benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:1066–1083.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al. (2022)</span>
<span class="ltx_bibblock">
Alexander Richard Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022.

</span>
<span class="ltx_bibblock">Qafacteval: Improved qa-based factual consistency evaluation for summarization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2587–2601.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fleiss et al. (1981)</span>
<span class="ltx_bibblock">
Joseph L Fleiss, B Levin, and MC Paik. 1981.

</span>
<span class="ltx_bibblock">Statistical methods for rates and proportions. john wiley &amp; sons.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">New York</em>, 870.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopalakrishnan et al. (2023)</span>
<span class="ltx_bibblock">
Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023.

</span>
<span class="ltx_bibblock">Topical-chat: Towards knowledge-grounded open-domain conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11995</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2021)</span>
<span class="ltx_bibblock">
Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021.

</span>
<span class="ltx_bibblock">Dialfact: A benchmark for fact-checking in dialogue.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08222</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al. (2022)</span>
<span class="ltx_bibblock">
Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022.

</span>
<span class="ltx_bibblock">True: Re-evaluating factual consistency evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3905–3920.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al. (2021)</span>
<span class="ltx_bibblock">
Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.619" title="" class="ltx_ref ltx_href"><math id="bib.bib13.1.1.m1.1" class="ltx_Math" alttext="q^{2}" display="inline"><semantics id="bib.bib13.1.1.m1.1a"><msup id="bib.bib13.1.1.m1.1.1" xref="bib.bib13.1.1.m1.1.1.cmml"><mi id="bib.bib13.1.1.m1.1.1.2" xref="bib.bib13.1.1.m1.1.1.2.cmml">q</mi><mn id="bib.bib13.1.1.m1.1.1.3" xref="bib.bib13.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib13.1.1.m1.1b"><apply id="bib.bib13.1.1.m1.1.1.cmml" xref="bib.bib13.1.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib13.1.1.m1.1.1.1.cmml" xref="bib.bib13.1.1.m1.1.1">superscript</csymbol><ci id="bib.bib13.1.1.m1.1.1.2.cmml" xref="bib.bib13.1.1.m1.1.1.2">𝑞</ci><cn type="integer" id="bib.bib13.1.1.m1.1.1.3.cmml" xref="bib.bib13.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.1.1.m1.1c">q^{2}</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 7856–7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laban et al. (2022)</span>
<span class="ltx_bibblock">
Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00453" title="" class="ltx_ref ltx_href">SummaC: Re-visiting NLI-based models for inconsistency detection in summarization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:163–177.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lattimer et al. (2023)</span>
<span class="ltx_bibblock">
Barrett Lattimer, Patrick CHen, Xinyuan Zhang, and Yi Yang. 2023.

</span>
<span class="ltx_bibblock">Fast and accurate factual inconsistency detection over long documents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 1691–1703.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.397" title="" class="ltx_ref ltx_href">HaluEval: A large-scale hallucination evaluation benchmark for large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 6449–6464, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. 2023b.

</span>
<span class="ltx_bibblock">Synthetic data generation with large language models for text classification: Potential and limitations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 10443–10461.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.153" title="" class="ltx_ref ltx_href">G-eval: NLG evaluation using gpt-4 with better human alignment</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 2511–2522, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2018)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2018.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucas et al. (2023)</span>
<span class="ltx_bibblock">
Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, and Dongwon Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.883" title="" class="ltx_ref ltx_href">Fighting fire with fire: The dual role of LLMs in crafting and detecting elusive disinformation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 14279–14305, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2024)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manakul et al. (2023a)</span>
<span class="ltx_bibblock">
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.557" title="" class="ltx_ref ltx_href">SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 9004–9017, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manakul et al. (2023b)</span>
<span class="ltx_bibblock">
Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023b.

</span>
<span class="ltx_bibblock">Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pages arXiv–2303.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynez et al. (2020)</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.

</span>
<span class="ltx_bibblock">On faithfulness and factuality in abstractive summarization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 1906–1919.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2019)</span>
<span class="ltx_bibblock">
Seungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1081" title="" class="ltx_ref ltx_href">OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 845–854, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawte et al. (2023)</span>
<span class="ltx_bibblock">
Vipula Rawte, Amit Sheth, and Amitava Das. 2023.

</span>
<span class="ltx_bibblock">A survey of hallucination in large foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05922</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. 2023.

</span>
<span class="ltx_bibblock">Alleviating hallucinations of large language models through induced hallucinations.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15710</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018.

</span>
<span class="ltx_bibblock">A dataset for document grounded conversations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 708–713.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Zhiying Zhu, Yiming Yang, and Zhiqing Sun. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2403.04307" title="" class="ltx_ref ltx_href">Halueval-wild: Evaluating hallucinations of language models in the wild</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompt Template for Synthetic Response Generation</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> include the prompt templates to generate hallucinated responses and faithful responses.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">For BEGIN dataset, we also create a prompt to generate "Generic" responses, as shown in Table <a href="#A1.T9" title="Table 9 ‣ Appendix A Prompt Template for Synthetic Response Generation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a></p>
</div>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Prompt template to generate hallucinated responses.
</figcaption>
<table id="A1.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T7.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Take a deep breath and work on this problem step by step.</td>
</tr>
<tr id="A1.T7.1.2" class="ltx_tr">
<td id="A1.T7.1.2.1" class="ltx_td ltx_align_left">I want you act as a chatbot in a conversation with human. Your job is to edit a detail in the True Response and generate</td>
</tr>
<tr id="A1.T7.1.3" class="ltx_tr">
<td id="A1.T7.1.3.1" class="ltx_td ltx_align_left">a Hallucinated Response that is inconsistent with the Dialogue History and Knowledge.</td>
</tr>
<tr id="A1.T7.1.4" class="ltx_tr">
<td id="A1.T7.1.4.1" class="ltx_td ltx_align_left">- Valid edit actions include removing, replacing or adding a short piece of information to the True</td>
</tr>
<tr id="A1.T7.1.5" class="ltx_tr">
<td id="A1.T7.1.5.1" class="ltx_td ltx_align_left">Response.</td>
</tr>
<tr id="A1.T7.1.6" class="ltx_tr">
<td id="A1.T7.1.6.1" class="ltx_td ltx_align_left">- If the True Response is faithful, please edit it to generate a Hallucinated Response.</td>
</tr>
<tr id="A1.T7.1.7" class="ltx_tr">
<td id="A1.T7.1.7.1" class="ltx_td ltx_align_left">- If the True Response has already contained hallucination, please edit it to generate an adversarial Hallucinated Response</td>
</tr>
<tr id="A1.T7.1.8" class="ltx_tr">
<td id="A1.T7.1.8.1" class="ltx_td ltx_align_left">that are more difficult to be detected.</td>
</tr>
<tr id="A1.T7.1.9" class="ltx_tr">
<td id="A1.T7.1.9.1" class="ltx_td ltx_align_left">- The generated Hallucinated Response should be ambiguous or complex or non-trivially implicit to be detected by a</td>
</tr>
<tr id="A1.T7.1.10" class="ltx_tr">
<td id="A1.T7.1.10.1" class="ltx_td ltx_align_left">human who has access to all the Knowledge and Dialogue History.</td>
</tr>
<tr id="A1.T7.1.11" class="ltx_tr">
<td id="A1.T7.1.11.1" class="ltx_td ltx_align_left">- The generated Hallucinated Response should contain similar number of words as the True Response. Do not make it</td>
</tr>
<tr id="A1.T7.1.12" class="ltx_tr">
<td id="A1.T7.1.12.1" class="ltx_td ltx_align_left">lengthy.</td>
</tr>
<tr id="A1.T7.1.13" class="ltx_tr">
<td id="A1.T7.1.13.1" class="ltx_td ltx_align_left">#Knowledge#: {Instructional prompt for target system}</td>
</tr>
<tr id="A1.T7.1.14" class="ltx_tr">
<td id="A1.T7.1.14.1" class="ltx_td ltx_align_left">#Dialogue History#: {dialogue history}</td>
</tr>
<tr id="A1.T7.1.15" class="ltx_tr">
<td id="A1.T7.1.15.1" class="ltx_td ltx_align_left">#True Response#: {system output}</td>
</tr>
<tr id="A1.T7.1.16" class="ltx_tr">
<td id="A1.T7.1.16.1" class="ltx_td ltx_align_left">Now, please generate your hallucinated response:</td>
</tr>
<tr id="A1.T7.1.17" class="ltx_tr">
<td id="A1.T7.1.17.1" class="ltx_td ltx_align_left ltx_border_bb">#Hallucinated Response#:</td>
</tr>
</table>
</figure>
<figure id="A1.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Prompt template to generate faithful responses.
</figcaption>
<table id="A1.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T8.1.1" class="ltx_tr">
<td id="A1.T8.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Take a deep breath and work on this problem step by step.</td>
</tr>
<tr id="A1.T8.1.2" class="ltx_tr">
<td id="A1.T8.1.2.1" class="ltx_td ltx_align_left">I want you act as a chatbot in a conversation with human.</td>
</tr>
<tr id="A1.T8.1.3" class="ltx_tr">
<td id="A1.T8.1.3.1" class="ltx_td ltx_align_left">Given a Response that contains hallucination, your job is to edit the Response lightly and generate a faithful Response</td>
</tr>
<tr id="A1.T8.1.4" class="ltx_tr">
<td id="A1.T8.1.4.1" class="ltx_td ltx_align_left">that is fully supported by with the Dialogue History and Knowledge.</td>
</tr>
<tr id="A1.T8.1.5" class="ltx_tr">
<td id="A1.T8.1.5.1" class="ltx_td ltx_align_left">- Valid edit actions include removing or replacing a short piece of information to the Response.</td>
</tr>
<tr id="A1.T8.1.6" class="ltx_tr">
<td id="A1.T8.1.6.1" class="ltx_td ltx_align_left">- Every token of the generated Response should be strictly verifiable by the Knowledge and Dialogue History. Even</td>
</tr>
<tr id="A1.T8.1.7" class="ltx_tr">
<td id="A1.T8.1.7.1" class="ltx_td ltx_align_left">commonsense information needs to be verifiable.</td>
</tr>
<tr id="A1.T8.1.8" class="ltx_tr">
<td id="A1.T8.1.8.1" class="ltx_td ltx_align_left">- Please keep the similar writing style as the Response. Do not make your response lengthy.</td>
</tr>
<tr id="A1.T8.1.9" class="ltx_tr">
<td id="A1.T8.1.9.1" class="ltx_td ltx_align_left">#Knowledge#: {Instructional prompt for target system}</td>
</tr>
<tr id="A1.T8.1.10" class="ltx_tr">
<td id="A1.T8.1.10.1" class="ltx_td ltx_align_left">#Dialogue History#: {dialogue history}</td>
</tr>
<tr id="A1.T8.1.11" class="ltx_tr">
<td id="A1.T8.1.11.1" class="ltx_td ltx_align_left">#Response#:{system output}</td>
</tr>
<tr id="A1.T8.1.12" class="ltx_tr">
<td id="A1.T8.1.12.1" class="ltx_td ltx_align_left">Now, please generate your faithful response:</td>
</tr>
<tr id="A1.T8.1.13" class="ltx_tr">
<td id="A1.T8.1.13.1" class="ltx_td ltx_align_left ltx_border_bb">#Faithful Response#:</td>
</tr>
</table>
</figure>
<figure id="A1.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Prompt template to generate ’Generic’ responses for BEGIN dataset.
</figcaption>
<table id="A1.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T9.1.1" class="ltx_tr">
<td id="A1.T9.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Take a deep breath and work on this problem step by step.</td>
</tr>
<tr id="A1.T9.1.2" class="ltx_tr">
<td id="A1.T9.1.2.1" class="ltx_td ltx_align_left">I want you act as a chatbot in a conversation with human.</td>
</tr>
<tr id="A1.T9.1.3" class="ltx_tr">
<td id="A1.T9.1.3.1" class="ltx_td ltx_align_left">Given a Response, your job is to rewrite it such that it is ostensibly about the same topic as the Response but becomes</td>
</tr>
<tr id="A1.T9.1.4" class="ltx_tr">
<td id="A1.T9.1.4.1" class="ltx_td ltx_align_left">vague and does not contain any factual statement.</td>
</tr>
<tr id="A1.T9.1.5" class="ltx_tr">
<td id="A1.T9.1.5.1" class="ltx_td ltx_align_left">Examples of rewritten Response includes but not limited to back-channeling, expressing uncertainty, or diverting the</td>
</tr>
<tr id="A1.T9.1.6" class="ltx_tr">
<td id="A1.T9.1.6.1" class="ltx_td ltx_align_left">conversation from ambiguous or controversial topics.
Do not make your response lengthy.</td>
</tr>
<tr id="A1.T9.1.7" class="ltx_tr">
<td id="A1.T9.1.7.1" class="ltx_td ltx_align_left">#Knowledge#: {Instructional prompt for target system}</td>
</tr>
<tr id="A1.T9.1.8" class="ltx_tr">
<td id="A1.T9.1.8.1" class="ltx_td ltx_align_left">#Dialogue History#: {dialogue history}</td>
</tr>
<tr id="A1.T9.1.9" class="ltx_tr">
<td id="A1.T9.1.9.1" class="ltx_td ltx_align_left">#Response#: {system output}</td>
</tr>
<tr id="A1.T9.1.10" class="ltx_tr">
<td id="A1.T9.1.10.1" class="ltx_td ltx_align_left">Now, please generate your faithful response:</td>
</tr>
<tr id="A1.T9.1.11" class="ltx_tr">
<td id="A1.T9.1.11.1" class="ltx_td ltx_align_left ltx_border_bb">#Rewritten Response#:</td>
</tr>
</table>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompt Template for Simulating Chatbot on OpenDialKG</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Table <a href="#A2.T10" title="Table 10 ‣ Appendix B Prompt Template for Simulating Chatbot on OpenDialKG ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> contains the prompt template that we use to prompt GPT-4 for system responses on OpenDialKG.</p>
</div>
<figure id="A2.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Prompt template to simulate the chatbot system for OpenDialKG.
</figcaption>
<table id="A2.T10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T10.1.1" class="ltx_tr">
<td id="A2.T10.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Take a deep breath and work on this problem step by step.</td>
</tr>
<tr id="A2.T10.1.2" class="ltx_tr">
<td id="A2.T10.1.2.1" class="ltx_td ltx_align_left">Given a Dialogue History and Knowledge, your job is to follow instructions in the Knowledge and generate a faithful</td>
</tr>
<tr id="A2.T10.1.3" class="ltx_tr">
<td id="A2.T10.1.3.1" class="ltx_td ltx_align_left">Response based on the Knowledge and Dialogue History.</td>
</tr>
<tr id="A2.T10.1.4" class="ltx_tr">
<td id="A2.T10.1.4.1" class="ltx_td ltx_align_left">#Knowledge#:</td>
</tr>
<tr id="A2.T10.1.5" class="ltx_tr">
<td id="A2.T10.1.5.1" class="ltx_td ltx_align_left">You are a chatbot. Your goal is to continue the conversation by responding to user’s last utterance.</td>
</tr>
<tr id="A2.T10.1.6" class="ltx_tr">
<td id="A2.T10.1.6.1" class="ltx_td ltx_align_left">You have the following knowledge that can be used to generate your response:</td>
</tr>
<tr id="A2.T10.1.7" class="ltx_tr">
<td id="A2.T10.1.7.1" class="ltx_td ltx_align_left">{KG knowledge}</td>
</tr>
<tr id="A2.T10.1.8" class="ltx_tr">
<td id="A2.T10.1.8.1" class="ltx_td ltx_align_left">#Dialogue History#:</td>
</tr>
<tr id="A2.T10.1.9" class="ltx_tr">
<td id="A2.T10.1.9.1" class="ltx_td ltx_align_left">{dialogue history}</td>
</tr>
<tr id="A2.T10.1.10" class="ltx_tr">
<td id="A2.T10.1.10.1" class="ltx_td ltx_align_left">Now, please generate your response:</td>
</tr>
<tr id="A2.T10.1.11" class="ltx_tr">
<td id="A2.T10.1.11.1" class="ltx_td ltx_align_left ltx_border_bb">#Response#:</td>
</tr>
</table>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Rubric/Typology for Qualitative Annotation</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">For the qualitative annotation in Table 4 of the main body, we use the rough definitions/guidelines below. We formulate these types based on prior work on hallucination and hallucination typology such as FRANK.</p>
</div>
<div id="A3.p2" class="ltx_para">
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p">Type No 1 : Adding attribute to entity, or adding new value to a known entity.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p">Type No 2 : Changing or misspecifying the relation between two entities, or interchanging and swapping their roles w.r.t the same relation.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p id="A3.I1.i3.p1.1" class="ltx_p">Type No 3 : Adding new entities in place of an existing entity, or even otherwise, and mentioning any information about them leaving aside one that purely expresses a no-information stance</p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p id="A3.I1.i4.p1.1" class="ltx_p">Type No 4 : Mistakenly claiming knowledge or committing to action about something that the model doesnt really know or cannot act upon</p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i5.p1" class="ltx_para">
<p id="A3.I1.i5.p1.1" class="ltx_p">Type No 5 : A genuine error in the logic and inference beyond just new entities, misattributed or swapped roles and relations.</p>
</div>
</li>
<li id="A3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i6.p1" class="ltx_para">
<p id="A3.I1.i6.p1.1" class="ltx_p">Type No 6: Definitely none of the above, it is something else</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>AMT Annotation Guidelines, Setup and Template</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">This section describes the AMT annotation guidelines for OpenDialKG-Eval.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">A snapshot of the template instructions as seen for an actual example can be viewed in Figure <a href="#A4.F3" title="Figure 3 ‣ Appendix D AMT Annotation Guidelines, Setup and Template ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Furthermore, we enclose the complete annotation template [including rules and illustrative examples in its contents] in the form of a single .html file included in the Supplementary Materials along with this submission.</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.1" class="ltx_p">Annotators were restricted to be from Anglophone countries (USA, UK, Australia and New Zealand) to ensure a good likelihood of them being native speakers. Further, annotators were restricted to be from among those with a prior approval rate of atleast 98%.</p>
</div>
<div id="A4.p4" class="ltx_para">
<p id="A4.p4.2" class="ltx_p">Annotators were compensated fairly at a rate of 9.3<math id="A4.p4.1.m1.1" class="ltx_Math" alttext="\$" display="inline"><semantics id="A4.p4.1.m1.1a"><mo id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">$</mo><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><csymbol cd="latexml" id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">currency-dollar</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">\$</annotation></semantics></math> per HIT per hour which is well over the minimum wage of 7.25<math id="A4.p4.2.m2.1" class="ltx_Math" alttext="\$" display="inline"><semantics id="A4.p4.2.m2.1a"><mo id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">$</mo><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><csymbol cd="latexml" id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">currency-dollar</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">\$</annotation></semantics></math> per hour in the U.S.A as per Department of Labour estimates for 2023.</p>
</div>
<div id="A4.p5" class="ltx_para">
<p id="A4.p5.1" class="ltx_p">We also provide due warning to the annotators not to even inadvertently share any PII or personal information and this is in no way required for our task. We also assure them that time taken etc [nothing beyond the task pertinent annotation] will be used or shared. The disclaimer we include in the template is "Important Disclaimer: Please avoid sharing any personal details or information including PII or demographics anywhere in this study. We will also not be sharing how much time you took to solve this, or what your individual experience profile was. We will merely be using the judgements made about aspects of generated output in relation to input. No other data implicitly or explicitly collected will be shared."</p>
</div>
<figure id="A4.F3" class="ltx_figure"><img src="/html/2407.05474/assets/Figures/TemplateSnap.png" id="A4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A snapshot of how the initial instructions and examples section of the template would appear to an annotator doing a HIT for our annotation task. </figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Statistics of OpenDialKG-Eval</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">OpenDialKG-Eval comprises 180 faithful generations and 132 hallucinations, while the development set contains 39 faithful generations and 51 hallucinations. The inter-annotator agreement, measured by Cohen’s, stands at 0.583, indicating a moderate level of agreement according to Fleiss’s guidelines <cite class="ltx_cite ltx_citemacro_cite">Fleiss et al. (<a href="#bib.bib9" title="" class="ltx_ref">1981</a>)</cite> on interpreting Cohen Kappa magnitude.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">For the annotation process, we utilized a scale ranging from -2 to +2, excluding 0. Here, -2 represents strong hallucination, and +2 signifies strong faithfulness. Based on this scale, instances scoring greater than 1 or less than -1 were allocated to the test set, with the remaining instances assigned to the development set.</p>
</div>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Prompt for Motivating Zero-Shot Detector Experiment in Intro Table 1</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">Prompts are shown in Table <a href="#A6.T11" title="Table 11 ‣ Appendix F Prompt for Motivating Zero-Shot Detector Experiment in Intro Table 1 ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="A6.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Prompt template of zero-shot hallucination detector GPT-3.5-turbo for Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</figcaption>
<table id="A6.T11.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A6.T11.1.1" class="ltx_tr">
<td id="A6.T11.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">&lt;DocumentGivenToAISystem&gt;: {Input/Document}&lt;/DocumentGivenToAISystem&gt;</td>
</tr>
<tr id="A6.T11.1.2" class="ltx_tr">
<td id="A6.T11.1.2.1" class="ltx_td ltx_align_left">&lt;SummaryByAISystem&gt;: {System Output}&lt;/SummaryByAISystem&gt;</td>
</tr>
<tr id="A6.T11.1.3" class="ltx_tr">
<td id="A6.T11.1.3.1" class="ltx_td ltx_align_left">Is the output Summary generated by the AI System Faithful to the Document given to it?</td>
</tr>
<tr id="A6.T11.1.4" class="ltx_tr">
<td id="A6.T11.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">Or is it Hallucinated? (Answer with +1 for Faithful or -1 for Hallucinated):</td>
</tr>
</table>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Prompt for GPT-4 (Internal) Zeroshot Approach</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">Prompts are shown in Table <a href="#A7.T12" title="Table 12 ‣ Appendix G Prompt for GPT-4 (Internal) Zeroshot Approach ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a></p>
</div>
<figure id="A7.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Prompt for GPT-4 (Internal) Zeroshot Approach (The Ternary version with Generic, the binary one omits the part concerned with Generic class)
</figcaption>
<table id="A7.T12.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A7.T12.1.1" class="ltx_tr">
<td id="A7.T12.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">&lt;PromptGivenToExtBot&gt;: {Knowledge}&lt;/PromptGivenToExtBot&gt;</td>
</tr>
<tr id="A7.T12.1.2" class="ltx_tr">
<td id="A7.T12.1.2.1" class="ltx_td ltx_align_left">&lt;ConvHistoryBetweenUserAndExtBot&gt;: {System Output}&lt;/ConvHistoryBetweenUserAndExtBot&gt;</td>
</tr>
<tr id="A7.T12.1.3" class="ltx_tr">
<td id="A7.T12.1.3.1" class="ltx_td ltx_align_left">&lt;ResponseByExtBot&gt;: {System Output}&lt;/ResponseByExtBot&gt;</td>
</tr>
<tr id="A7.T12.1.4" class="ltx_tr">
<td id="A7.T12.1.4.1" class="ltx_td ltx_align_left">The Response here can be either Faithful to the Context (Prompt and ConvHistory) OR it
ecan be hallucinated/contain hallucinations</td>
</tr>
<tr id="A7.T12.1.5" class="ltx_tr">
<td id="A7.T12.1.5.1" class="ltx_td ltx_align_left">(says something that is contradictory or not</td>
</tr>
<tr id="A7.T12.1.6" class="ltx_tr">
<td id="A7.T12.1.6.1" class="ltx_td ltx_align_left">entirely or close to likely supported by the context).</td>
</tr>
<tr id="A7.T12.1.7" class="ltx_tr">
<td id="A7.T12.1.7.1" class="ltx_td ltx_align_left">A third possibility is that it says something really generic and not really having a relevant truth value or sufficient relatibility to context,</td>
</tr>
<tr id="A7.T12.1.8" class="ltx_tr">
<td id="A7.T12.1.8.1" class="ltx_td ltx_align_left">such as smalltalk, obviously</td>
</tr>
<tr id="A7.T12.1.9" class="ltx_tr">
<td id="A7.T12.1.9.1" class="ltx_td ltx_align_left">true statements amongst other things.</td>
</tr>
<tr id="A7.T12.1.10" class="ltx_tr">
<td id="A7.T12.1.10.1" class="ltx_td ltx_align_left">Thus a Response can be Faithful, Hallucinated or Generic w.r.t the Prompt given to it and the ConvHistory.</td>
</tr>
<tr id="A7.T12.1.11" class="ltx_tr">
<td id="A7.T12.1.11.1" class="ltx_td ltx_align_left">Is the output Response given by the ExtBot Faithful to the Prompt given to it and the ConvHistory between User and ExtBot so far?</td>
</tr>
<tr id="A7.T12.1.12" class="ltx_tr">
<td id="A7.T12.1.12.1" class="ltx_td ltx_align_left">Or is it Hallucinated? Or is it Generic?</td>
</tr>
<tr id="A7.T12.1.13" class="ltx_tr">
<td id="A7.T12.1.13.1" class="ltx_td ltx_align_left ltx_border_bb">(Answer with 2 for Faithful, 1 for Generic or 0 for Hallucinated):</td>
</tr>
</table>
</figure>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Experimental Details</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.5" class="ltx_p">During finetuning, we use batchsize = 4, apply AdamW gradient descent  <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> and tune learning rates from the range of <math id="A8.p1.1.m1.1" class="ltx_Math" alttext="[" display="inline"><semantics id="A8.p1.1.m1.1a"><mo stretchy="false" id="A8.p1.1.m1.1.1" xref="A8.p1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A8.p1.1.m1.1b"><ci id="A8.p1.1.m1.1.1.cmml" xref="A8.p1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.p1.1.m1.1c">[</annotation></semantics></math>1e-3, 1e-4, 1e-5<math id="A8.p1.2.m2.1" class="ltx_Math" alttext="]" display="inline"><semantics id="A8.p1.2.m2.1a"><mo stretchy="false" id="A8.p1.2.m2.1.1" xref="A8.p1.2.m2.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A8.p1.2.m2.1b"><ci id="A8.p1.2.m2.1.1.cmml" xref="A8.p1.2.m2.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.p1.2.m2.1c">]</annotation></semantics></math>. We run 5 epochs for OpenDialKG and 1 epoch over the BEGIN dataset.
We evaluate our model, HaluEval-based and FADE-based finetuned models using Dev set, choose the best performing learning rate and report the
performances on test set. In addition, we adopt Low Rank Adaptation <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> with r=16, <math id="A8.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="A8.p1.3.m3.1a"><mi id="A8.p1.3.m3.1.1" xref="A8.p1.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A8.p1.3.m3.1b"><ci id="A8.p1.3.m3.1.1.cmml" xref="A8.p1.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.p1.3.m3.1c">\alpha</annotation></semantics></math>=32, and target_modules=<math id="A8.p1.4.m4.1" class="ltx_Math" alttext="[" display="inline"><semantics id="A8.p1.4.m4.1a"><mo stretchy="false" id="A8.p1.4.m4.1.1" xref="A8.p1.4.m4.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A8.p1.4.m4.1b"><ci id="A8.p1.4.m4.1.1.cmml" xref="A8.p1.4.m4.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.p1.4.m4.1c">[</annotation></semantics></math>"q", "v"<math id="A8.p1.5.m5.1" class="ltx_Math" alttext="]" display="inline"><semantics id="A8.p1.5.m5.1a"><mo stretchy="false" id="A8.p1.5.m5.1.1" xref="A8.p1.5.m5.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A8.p1.5.m5.1b"><ci id="A8.p1.5.m5.1.1.cmml" xref="A8.p1.5.m5.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.p1.5.m5.1c">]</annotation></semantics></math> during optimization. Our experiments are base on</p>
</div>
<div id="A8.p2" class="ltx_para">
<p id="A8.p2.1" class="ltx_p">BEGIN-Adv. has 8k unreleased data for training, while there are only 1.2k data in BEGIN dev that we can leverage for fine-tuning. In order to generate similar amount of training data, we generate 3 synthetic responses per category for each example in BEGIN Dev set. We adopt temperature 0.5 to avoid repeat generation.</p>
</div>
<div id="A8.p3" class="ltx_para">
<p id="A8.p3.1" class="ltx_p">Wherever pertinent, we provide mean results over two random runs.</p>
</div>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Additional Arguments for Cost Efficiency of our Method</h2>

<div id="A9.p1" class="ltx_para">
<p id="A9.p1.1" class="ltx_p">We re-emphasize some key additional points here regarding relative cost efficiency of our approach:</p>
</div>
<div id="A9.p2" class="ltx_para">
<ol id="A9.I1" class="ltx_enumerate">
<li id="A9.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A9.I1.i1.p1" class="ltx_para">
<p id="A9.I1.i1.p1.1" class="ltx_p">Inference Time Efficiency: Our findings, presented in Table 2 and Table 3, demonstrate that a fine-tuned T5-base classifier offers significantly lower latency compared to other baseline methods. This is crucial for applications where high latency is not viable.</p>
</div>
</li>
<li id="A9.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A9.I1.i2.p1" class="ltx_para">
<p id="A9.I1.i2.p1.1" class="ltx_p">An additional pragmatic aspect bolstering our relative cost efficiency angle in the long term is the steadily decreasing per-token costs of API-based colossal LLMs like GPT-4, driven by widening adoption, faster accelerators, better compression/distillation and quantization etc. Specifically for GPT-4, we have in the past year seen three major price decrease events, each by a factor of 2+, on June 13th, November 11th and Jan 25th, causing a price decrease of over 8 fold. In contrast, human annotation costs in dollar terms rise albeit very gradually.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A10" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Out of Domain Evaluation</h2>

<div id="A10.p1" class="ltx_para">
<p id="A10.p1.1" class="ltx_p">We conducted additional experiments to evaluate our model’s performance in out-of-distribution scenario. Specifically, we utilized the best-performing T5 models, which were finetuned using our dataset generated from OpenDialKG, and applied the model to the BEGIN test set. And we compared with our internal GPT-4 baseline.</p>
</div>
<figure id="A10.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>Detection Results on Out-of-Distribution Data.
</figcaption>
<table id="A10.T13.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A10.T13.1.1" class="ltx_tr">
<td id="A10.T13.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Approach</td>
<td id="A10.T13.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">Macro-F1</td>
</tr>
<tr id="A10.T13.1.2" class="ltx_tr">
<td id="A10.T13.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Random baseline</td>
<td id="A10.T13.1.2.2" class="ltx_td ltx_align_right ltx_border_t">0.455</td>
</tr>
<tr id="A10.T13.1.3" class="ltx_tr">
<td id="A10.T13.1.3.1" class="ltx_td ltx_align_left ltx_border_r">Our approach</td>
<td id="A10.T13.1.3.2" class="ltx_td ltx_align_right">0.518</td>
</tr>
<tr id="A10.T13.1.4" class="ltx_tr">
<td id="A10.T13.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">GPT-4 (Internal)</td>
<td id="A10.T13.1.4.2" class="ltx_td ltx_align_right ltx_border_bb">0.543</td>
</tr>
</table>
</figure>
<div id="A10.p2" class="ltx_para">
<p id="A10.p2.1" class="ltx_p">Given that the BEGIN dataset features significantly longer knowledge contexts in natural language text, as opposed to the list of triplets from the Freebase knowledge graph found in OpenDialKG, this setup simulates a substantial out-of-distribution evaluation. To adapt our binary classifiers for use with BEGIN, we collapse the “generic” and “not fully attributable” labels into a single “hallucination” category, resulting in a modified binarized test set. The results are as follows in Table <a href="#A10.T13" title="Table 13 ‣ Appendix J Out of Domain Evaluation ‣ Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, which indicates that our approach leads to signifantly better performance in comparison to a naive random baseline, and shows competitive performance with GPT-4 based zero-shot detection in out-of-distribution scenario, emphasizing its generalization ability.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.05473" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.05474" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.05474">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.05474" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.05475" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 16:10:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
